{
    "author": "cyyever",
    "message": "More PYUP fixes (#38883)\n\nMore pyup fixes\n\nSigned-off-by: cyy <cyyever@outlook.com>",
    "sha": "1fc67a25c6d6080467fae4f35efcefe5f13b7409",
    "files": [
        {
            "sha": "b4478bcbac014ba2b48b72d115096feba624782a",
            "filename": "src/transformers/generation/candidate_generator.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -710,8 +710,8 @@ def __init__(\n         assistant_model: Optional[\"PreTrainedModel\"] = None,\n         assistant_prune_lm_head: bool = False,\n     ):\n-        self._target_tokenizer: \"PreTrainedTokenizerBase\" = target_tokenizer\n-        self._assistant_tokenizer: \"PreTrainedTokenizerBase\" = assistant_tokenizer\n+        self._target_tokenizer: PreTrainedTokenizerBase = target_tokenizer\n+        self._assistant_tokenizer: PreTrainedTokenizerBase = assistant_tokenizer\n         self._assistant_model_device: str = (\n             assistant_model_device if assistant_model is None else assistant_model.device\n         )"
        },
        {
            "sha": "399ac2a3f5b7d86022900865302e221fe3a6a225",
            "filename": "src/transformers/generation/streamers.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fgeneration%2Fstreamers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fgeneration%2Fstreamers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fstreamers.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -72,7 +72,7 @@ class TextStreamer(BaseStreamer):\n         ```\n     \"\"\"\n \n-    def __init__(self, tokenizer: \"AutoTokenizer\", skip_prompt: bool = False, **decode_kwargs):\n+    def __init__(self, tokenizer: AutoTokenizer, skip_prompt: bool = False, **decode_kwargs):\n         self.tokenizer = tokenizer\n         self.skip_prompt = skip_prompt\n         self.decode_kwargs = decode_kwargs\n@@ -206,7 +206,7 @@ class TextIteratorStreamer(TextStreamer):\n     \"\"\"\n \n     def __init__(\n-        self, tokenizer: \"AutoTokenizer\", skip_prompt: bool = False, timeout: Optional[float] = None, **decode_kwargs\n+        self, tokenizer: AutoTokenizer, skip_prompt: bool = False, timeout: Optional[float] = None, **decode_kwargs\n     ):\n         super().__init__(tokenizer, skip_prompt, **decode_kwargs)\n         self.text_queue = Queue()\n@@ -284,7 +284,7 @@ class AsyncTextIteratorStreamer(TextStreamer):\n     \"\"\"\n \n     def __init__(\n-        self, tokenizer: \"AutoTokenizer\", skip_prompt: bool = False, timeout: Optional[float] = None, **decode_kwargs\n+        self, tokenizer: AutoTokenizer, skip_prompt: bool = False, timeout: Optional[float] = None, **decode_kwargs\n     ):\n         super().__init__(tokenizer, skip_prompt, **decode_kwargs)\n         self.text_queue = asyncio.Queue()"
        },
        {
            "sha": "e1dc9cf1248ef988307f1d0573d368a1032d6188",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -4723,7 +4723,7 @@ def _constrained_beam_search(\n                 )\n \n             if return_dict_in_generate and output_scores:\n-                beam_indices = tuple((beam_indices[beam_idx[i]] + (beam_idx[i],) for i in range(len(beam_indices))))\n+                beam_indices = tuple(beam_indices[beam_idx[i]] + (beam_idx[i],) for i in range(len(beam_indices)))\n \n             # increase cur_len\n             cur_len = cur_len + 1"
        },
        {
            "sha": "11274a60b486b0d282b1e27911becb038cf8f2d1",
            "filename": "src/transformers/integrations/integration_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -1626,8 +1626,8 @@ def _log_model_checkpoint(self, source_directory: str, checkpoint: str):\n                 target_path = consistent_checkpoint_path\n             except OSError as e:\n                 logger.warning(\n-                    \"NeptuneCallback was unable to made a copy of checkpoint due to I/O exception: '{}'. \"\n-                    \"Could fail trying to upload.\".format(e)\n+                    f\"NeptuneCallback was unable to made a copy of checkpoint due to I/O exception: '{e}'. \"\n+                    \"Could fail trying to upload.\"\n                 )\n \n         self._metadata_namespace[self._target_checkpoints_namespace].upload_files(target_path)\n@@ -1976,9 +1976,7 @@ def on_save(self, args, state, control, **kwargs):\n                     )\n                 except Exception as e:\n                     logger.warning(\n-                        \"Could not remove checkpoint `{}` after going over the `save_total_limit`. Error is: {}\".format(\n-                            self._checkpoints_saved[0].name, e\n-                        )\n+                        f\"Could not remove checkpoint `{self._checkpoints_saved[0].name}` after going over the `save_total_limit`. Error is: {e}\"\n                     )\n                     break\n                 self._checkpoints_saved = self._checkpoints_saved[1:]"
        },
        {
            "sha": "b64e3569459ffdc1efb49cdd9322d6a483bbb96d",
            "filename": "src/transformers/modeling_tf_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodeling_tf_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodeling_tf_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_tf_utils.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -1409,10 +1409,10 @@ def _save_checkpoint(self, checkpoint_dir, epoch):\n \n     def prepare_tf_dataset(\n         self,\n-        dataset: \"datasets.Dataset\",  # noqa:F821\n+        dataset: datasets.Dataset,  # noqa:F821\n         batch_size: int = 8,\n         shuffle: bool = True,\n-        tokenizer: Optional[\"PreTrainedTokenizerBase\"] = None,\n+        tokenizer: Optional[PreTrainedTokenizerBase] = None,\n         collate_fn: Optional[Callable] = None,\n         collate_fn_args: Optional[dict[str, Any]] = None,\n         drop_remainder: Optional[bool] = None,"
        },
        {
            "sha": "4bde1f4451c0636e22f3b2bf887679e80ec6e5cf",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -4424,10 +4424,8 @@ def from_pretrained(\n                 raise ValueError(\"DeepSpeed Zero-3 is not compatible with passing a `device_map`.\")\n             if not is_accelerate_available():\n                 raise ValueError(\n-                    (\n-                        \"Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` \"\n-                        \"requires `accelerate`. You can install it with `pip install accelerate`\"\n-                    )\n+                    \"Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` \"\n+                    \"requires `accelerate`. You can install it with `pip install accelerate`\"\n                 )\n \n         # handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation."
        },
        {
            "sha": "011ad689edbdb10f53694eb9c774604d922a0d73",
            "filename": "src/transformers/models/albert/tokenization_albert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -203,7 +203,7 @@ def _tokenize(self, text: str) -> list[str]:\n         pieces = self.sp_model.encode(text, out_type=str)\n         new_pieces = []\n         for piece in pieces:\n-            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\n+            if len(piece) > 1 and piece[-1] == \",\" and piece[-2].isdigit():\n                 # Logic to handle special cases see https://github.com/google-research/bert/blob/master/README.md#tokenization\n                 # `9,9` -> ['â–9', ',', '9'] instead of [`_9,`, '9']\n                 cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))"
        },
        {
            "sha": "c66dc73a96cac02a5e679f7ff98360d0cabd5686",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -830,7 +830,7 @@ def torch_forward(\n \n             # 2. Compute the state for each intra-chunk\n             # (right term of low-rank factorization of off-diagonal blocks; B terms)\n-            decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum))\n+            decay_states = torch.exp(A_cumsum[:, :, :, -1:] - A_cumsum)\n             B_decay = B * decay_states.permute(0, -2, -1, 1)[..., None]\n             states = (B_decay[..., None, :] * hidden_states[..., None]).sum(dim=2)\n "
        },
        {
            "sha": "3fec4ad35cfca49a2642c85d88a7b58c4cebc4c9",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -632,7 +632,7 @@ def torch_forward(\n \n             # 2. Compute the state for each intra-chunk\n             # (right term of low-rank factorization of off-diagonal blocks; B terms)\n-            decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum))\n+            decay_states = torch.exp(A_cumsum[:, :, :, -1:] - A_cumsum)\n             B_decay = B * decay_states.permute(0, -2, -1, 1)[..., None]\n             states = (B_decay[..., None, :] * hidden_states[..., None]).sum(dim=2)\n "
        },
        {
            "sha": "f674afe1a4126781ef64f8b5e96436b0025490c2",
            "filename": "src/transformers/models/bart/tokenization_bart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fbart%2Ftokenization_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fbart%2Ftokenization_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Ftokenization_bart.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -32,7 +32,7 @@\n # See all BART models at https://huggingface.co/models?filter=bart\n \n \n-@lru_cache()\n+@lru_cache\n def bytes_to_unicode():\n     \"\"\"\n     Returns list of utf-8 byte and a mapping to unicode strings. We specifically avoids mapping to whitespace/control"
        },
        {
            "sha": "086e62561fd0d93eeb89b0ea7c439a3ad0a202c2",
            "filename": "src/transformers/models/beit/modeling_beit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -110,7 +110,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return drop_path(hidden_states, self.drop_prob, self.training)\n \n     def extra_repr(self) -> str:\n-        return \"p={}\".format(self.drop_prob)\n+        return f\"p={self.drop_prob}\"\n \n \n # Based on timm implementation, which can be found here:\n@@ -513,8 +513,8 @@ def __init__(self, config: BeitConfig, window_size: Optional[tuple] = None, drop\n \n         init_values = config.layer_scale_init_value\n         if init_values > 0:\n-            self.lambda_1 = nn.Parameter(init_values * torch.ones((config.hidden_size)), requires_grad=True)\n-            self.lambda_2 = nn.Parameter(init_values * torch.ones((config.hidden_size)), requires_grad=True)\n+            self.lambda_1 = nn.Parameter(init_values * torch.ones(config.hidden_size), requires_grad=True)\n+            self.lambda_2 = nn.Parameter(init_values * torch.ones(config.hidden_size), requires_grad=True)\n         else:\n             self.lambda_1, self.lambda_2 = None, None\n "
        },
        {
            "sha": "002011795c4ee9b1208de49e44644e0748306ea0",
            "filename": "src/transformers/models/bert_japanese/tokenization_bert_japanese.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fbert_japanese%2Ftokenization_bert_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fbert_japanese%2Ftokenization_bert_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_japanese%2Ftokenization_bert_japanese.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -934,7 +934,7 @@ def tokenize(self, text):\n         pieces = self.sp_model.encode(text, out_type=str)\n         new_pieces = []\n         for piece in pieces:\n-            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\n+            if len(piece) > 1 and piece[-1] == \",\" and piece[-2].isdigit():\n                 cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\n                 if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n                     if len(cur_pieces[0]) == 1:"
        },
        {
            "sha": "8da189b1b308db55bd2ea98e627e3d940ca72807",
            "filename": "src/transformers/models/biogpt/convert_biogpt_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fconvert_biogpt_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fconvert_biogpt_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fconvert_biogpt_original_pytorch_checkpoint_to_pytorch.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -115,7 +115,7 @@ def add_from_file(self, f):\n             except FileNotFoundError as fnfe:\n                 raise fnfe\n             except UnicodeError:\n-                raise Exception(\"Incorrect encoding detected in {}, please rebuild the dataset\".format(f))\n+                raise Exception(f\"Incorrect encoding detected in {f}, please rebuild the dataset\")\n             return\n \n         lines = f.readlines()\n@@ -133,11 +133,11 @@ def add_from_file(self, f):\n                 word = line\n                 if word in self and not overwrite:\n                     raise RuntimeError(\n-                        \"Duplicate word found when loading Dictionary: '{}'. \"\n+                        f\"Duplicate word found when loading Dictionary: '{word}'. \"\n                         \"Duplicate words can overwrite earlier ones by adding the \"\n                         \"#fairseq:overwrite flag at the end of the corresponding row \"\n                         \"in the dictionary file. If using the Camembert model, please \"\n-                        \"download an updated copy of the model file.\".format(word)\n+                        \"download an updated copy of the model file.\"\n                     )\n                 self.add_symbol(word, n=count, overwrite=overwrite)\n             except ValueError:"
        },
        {
            "sha": "1a7a016f9cd851efd5a2e97dc54f0244e9616157",
            "filename": "src/transformers/models/bit/modeling_bit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -310,7 +310,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return drop_path(hidden_states, self.drop_prob, self.training)\n \n     def extra_repr(self) -> str:\n-        return \"p={}\".format(self.drop_prob)\n+        return f\"p={self.drop_prob}\"\n \n \n def make_div(value, divisor=8):"
        },
        {
            "sha": "76719fa254948b7fa4a9ea151bfc2fcdba07131c",
            "filename": "src/transformers/models/blenderbot/tokenization_blenderbot.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fblenderbot%2Ftokenization_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fblenderbot%2Ftokenization_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Ftokenization_blenderbot.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -35,7 +35,7 @@\n }\n \n \n-@lru_cache()\n+@lru_cache\n # Copied from transformers.models.roberta.tokenization_roberta.bytes_to_unicode\n def bytes_to_unicode():\n     \"\"\""
        },
        {
            "sha": "2dac6b3493fce9c842bab9066e4a657433a2196d",
            "filename": "src/transformers/models/blip/modeling_blip_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -641,9 +641,7 @@ def get_extended_attention_mask(\n                 extended_attention_mask = attention_mask[:, None, None, :]\n         else:\n             raise ValueError(\n-                \"Wrong shape for input_ids (shape {}) or attention_mask (shape {})\".format(\n-                    input_shape, attention_mask.shape\n-                )\n+                f\"Wrong shape for input_ids (shape {input_shape}) or attention_mask (shape {attention_mask.shape})\"\n             )\n \n         # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n@@ -723,7 +721,7 @@ def forward(\n         past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n \n         if attention_mask is None:\n-            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length))).to(device)\n+            attention_mask = torch.ones((batch_size, seq_length + past_key_values_length)).to(device)\n \n         # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n         # ourselves in which case we just need to make it broadcastable to all heads."
        },
        {
            "sha": "3d0a480fe623108b38e68fe21721299e63a9b1e8",
            "filename": "src/transformers/models/blip/modeling_tf_blip_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_tf_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_tf_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_tf_blip_text.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -800,9 +800,7 @@ def get_extended_attention_mask(\n                 extended_attention_mask = attention_mask[:, None, None, :]\n         else:\n             raise ValueError(\n-                \"Wrong shape for input_ids (shape {}) or attention_mask (shape {})\".format(\n-                    input_shape, attention_mask.shape\n-                )\n+                f\"Wrong shape for input_ids (shape {input_shape}) or attention_mask (shape {attention_mask.shape})\"\n             )\n \n         # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n@@ -881,7 +879,7 @@ def call(\n         past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n \n         if attention_mask is None:\n-            attention_mask = tf.ones(((batch_size, seq_length + past_key_values_length)))\n+            attention_mask = tf.ones((batch_size, seq_length + past_key_values_length))\n \n         # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n         # ourselves in which case we just need to make it broadcastable to all heads."
        },
        {
            "sha": "4382296969e4012a23fa9c7d67484227a792add4",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -1144,9 +1144,7 @@ def get_extended_attention_mask(\n             extended_attention_mask = attention_mask[:, None, None, :]\n         else:\n             raise ValueError(\n-                \"Wrong shape for input_ids (shape {}) or attention_mask (shape {})\".format(\n-                    input_shape, attention_mask.shape\n-                )\n+                f\"Wrong shape for input_ids (shape {input_shape}) or attention_mask (shape {attention_mask.shape})\"\n             )\n \n         # Since attention_mask is 1.0 for positions we want to attend and 0.0 for"
        },
        {
            "sha": "b9c9b27bd3c1a5a9037e58a9738e4d2212cd0585",
            "filename": "src/transformers/models/bloom/convert_bloom_original_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fbloom%2Fconvert_bloom_original_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fbloom%2Fconvert_bloom_original_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fconvert_bloom_original_checkpoint_to_pytorch.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -98,7 +98,7 @@ def convert_bloom_checkpoint_to_pytorch(\n         config = BloomConfig()\n \n         for j, file in enumerate(file_names):\n-            print(\"Processing file: {}\".format(file))\n+            print(f\"Processing file: {file}\")\n             tensors = None\n \n             for i in range(pretraining_tp):\n@@ -132,16 +132,16 @@ def convert_bloom_checkpoint_to_pytorch(\n                 tensors,\n                 os.path.join(\n                     pytorch_dump_folder_path,\n-                    \"pytorch_model_{}-of-{}.bin\".format(str(j + 1).zfill(5), str(len(file_names)).zfill(5)),\n+                    f\"pytorch_model_{str(j + 1).zfill(5)}-of-{str(len(file_names)).zfill(5)}.bin\",\n                 ),\n             )\n \n             for key in tensors.keys():\n                 value = tensors[key]\n                 total_size += value.numel() * get_dtype_size(value.dtype)\n                 if key not in index_dict[\"weight_map\"]:\n-                    index_dict[\"weight_map\"][key] = \"pytorch_model_{}-of-{}.bin\".format(\n-                        str(j + 1).zfill(5), str(len(file_names)).zfill(5)\n+                    index_dict[\"weight_map\"][key] = (\n+                        f\"pytorch_model_{str(j + 1).zfill(5)}-of-{str(len(file_names)).zfill(5)}.bin\"\n                     )\n \n         config = BloomConfig()"
        },
        {
            "sha": "6a44e36ade43fb2f4738668dfa6b0978787141b3",
            "filename": "src/transformers/models/clap/modeling_clap.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -610,7 +610,7 @@ def get_attn_mask(self, height, width, dtype, device):\n             mask_windows = window_partition(img_mask, self.window_size)\n             mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n             attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n-            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n+            attn_mask = attn_mask.masked_fill(attn_mask != 0, -100.0).masked_fill(attn_mask == 0, 0.0)\n         else:\n             attn_mask = None\n         return attn_mask"
        },
        {
            "sha": "2db3fd4a9af08a291e6cadd4f30700f4a213412f",
            "filename": "src/transformers/models/clip/tokenization_clip.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fclip%2Ftokenization_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fclip%2Ftokenization_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Ftokenization_clip.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -34,7 +34,7 @@\n }\n \n \n-@lru_cache()\n+@lru_cache\n def bytes_to_unicode():\n     \"\"\"\n     Returns list of utf-8 byte and a mapping to unicode strings. We specifically avoids mapping to whitespace/control\n@@ -488,7 +488,7 @@ def convert_tokens_to_string(self, tokens):\n \n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         if not os.path.isdir(save_directory):\n-            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n+            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n             return\n         vocab_file = os.path.join(\n             save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n@@ -506,8 +506,8 @@ def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] =\n             for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):\n                 if index != token_index:\n                     logger.warning(\n-                        \"Saving vocabulary to {}: BPE merge indices are not consecutive.\"\n-                        \" Please check that the tokenizer is not corrupted!\".format(merge_file)\n+                        f\"Saving vocabulary to {merge_file}: BPE merge indices are not consecutive.\"\n+                        \" Please check that the tokenizer is not corrupted!\"\n                     )\n                     index = token_index\n                 writer.write(\" \".join(bpe_tokens) + \"\\n\")"
        },
        {
            "sha": "6f4a60737bf26e80770963ec5670640d95e2cb54",
            "filename": "src/transformers/models/clipseg/convert_clipseg_original_pytorch_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconvert_clipseg_original_pytorch_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconvert_clipseg_original_pytorch_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconvert_clipseg_original_pytorch_to_hf.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -181,7 +181,7 @@ def convert_clipseg_checkpoint(model_name, checkpoint_path, pytorch_dump_folder_\n     missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n \n     if missing_keys != [\"clip.text_model.embeddings.position_ids\", \"clip.vision_model.embeddings.position_ids\"]:\n-        raise ValueError(\"Missing keys that are not expected: {}\".format(missing_keys))\n+        raise ValueError(f\"Missing keys that are not expected: {missing_keys}\")\n     if unexpected_keys != [\"decoder.reduce.weight\", \"decoder.reduce.bias\"]:\n         raise ValueError(f\"Unexpected keys: {unexpected_keys}\")\n "
        },
        {
            "sha": "61bd5964ecdf7e7dc837fda7cdb012861a3a5377",
            "filename": "src/transformers/models/clvp/tokenization_clvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fclvp%2Ftokenization_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fclvp%2Ftokenization_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Ftokenization_clvp.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -34,7 +34,7 @@\n }\n \n \n-@lru_cache()\n+@lru_cache\n # Copied from transformers.models.gpt2.tokenization_gpt2.bytes_to_unicode\n def bytes_to_unicode():\n     \"\"\""
        },
        {
            "sha": "152b1a84fc37d5ed6613072284133565ebc86cbf",
            "filename": "src/transformers/models/codegen/tokenization_codegen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -42,7 +42,7 @@\n }\n \n \n-@lru_cache()\n+@lru_cache\n def bytes_to_unicode():\n     \"\"\"\n     Returns list of utf-8 byte and a mapping to unicode strings. We specifically avoids mapping to whitespace/control"
        },
        {
            "sha": "81e262be04150bbed51cd12323623a03fd4bf86f",
            "filename": "src/transformers/models/convnext/modeling_convnext.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_convnext.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -70,7 +70,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return drop_path(hidden_states, self.drop_prob, self.training)\n \n     def extra_repr(self) -> str:\n-        return \"p={}\".format(self.drop_prob)\n+        return f\"p={self.drop_prob}\"\n \n \n class ConvNextLayerNorm(nn.Module):\n@@ -149,7 +149,7 @@ def __init__(self, config, dim, drop_path=0):\n         self.act = ACT2FN[config.hidden_act]\n         self.pwconv2 = nn.Linear(4 * dim, dim)\n         self.layer_scale_parameter = (\n-            nn.Parameter(config.layer_scale_init_value * torch.ones((dim)), requires_grad=True)\n+            nn.Parameter(config.layer_scale_init_value * torch.ones(dim), requires_grad=True)\n             if config.layer_scale_init_value > 0\n             else None\n         )"
        },
        {
            "sha": "c2b3372c97a0c3d677bb4ae8e2b745c40026771e",
            "filename": "src/transformers/models/convnextv2/modeling_convnextv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_convnextv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_convnextv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_convnextv2.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -70,7 +70,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return drop_path(hidden_states, self.drop_prob, self.training)\n \n     def extra_repr(self) -> str:\n-        return \"p={}\".format(self.drop_prob)\n+        return f\"p={self.drop_prob}\"\n \n \n class ConvNextV2GRN(nn.Module):"
        },
        {
            "sha": "c045c41eb4e3449d6fb272c82be5462b4b056e86",
            "filename": "src/transformers/models/cpm/tokenization_cpm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fcpm%2Ftokenization_cpm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fcpm%2Ftokenization_cpm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcpm%2Ftokenization_cpm.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -207,7 +207,7 @@ def _tokenize(self, text: str) -> list[str]:\n         pieces = self.sp_model.encode(text, out_type=str)\n         new_pieces = []\n         for piece in pieces:\n-            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\n+            if len(piece) > 1 and piece[-1] == \",\" and piece[-2].isdigit():\n                 cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\n                 if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n                     if len(cur_pieces[0]) == 1:"
        },
        {
            "sha": "691a246753591130bc63845ef9650a195632e8c7",
            "filename": "src/transformers/models/cvt/modeling_cvt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -86,7 +86,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return drop_path(hidden_states, self.drop_prob, self.training)\n \n     def extra_repr(self) -> str:\n-        return \"p={}\".format(self.drop_prob)\n+        return f\"p={self.drop_prob}\"\n \n \n class CvtEmbeddings(nn.Module):"
        },
        {
            "sha": "7d8f988430d0e4dce775f8d0b8888a68696ad449",
            "filename": "src/transformers/models/d_fine/modeling_d_fine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -187,7 +187,7 @@ def forward(\n             sampling_locations = reference_points[:, :, None, :, :2] + offset\n         else:\n             raise ValueError(\n-                \"Last dim of reference_points must be 2 or 4, but get {} instead.\".format(reference_points.shape[-1])\n+                f\"Last dim of reference_points must be 2 or 4, but get {reference_points.shape[-1]} instead.\"\n             )\n \n         output = self.ms_deformable_attn_core("
        },
        {
            "sha": "83b31cf5659d7915d70ad5145591666f265c70cc",
            "filename": "src/transformers/models/d_fine/modular_d_fine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -517,7 +517,7 @@ def forward(\n             sampling_locations = reference_points[:, :, None, :, :2] + offset\n         else:\n             raise ValueError(\n-                \"Last dim of reference_points must be 2 or 4, but get {} instead.\".format(reference_points.shape[-1])\n+                f\"Last dim of reference_points must be 2 or 4, but get {reference_points.shape[-1]} instead.\"\n             )\n \n         output = self.ms_deformable_attn_core("
        },
        {
            "sha": "c977f4b923b8d635d4097525ecafa523756afa76",
            "filename": "src/transformers/models/dab_detr/modeling_dab_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -384,7 +384,7 @@ def gen_sine_position_embeddings(pos_tensor, hidden_size=256):\n \n         pos = torch.cat((pos_y, pos_x, pos_w, pos_h), dim=2)\n     else:\n-        raise ValueError(\"Unknown pos_tensor shape(-1):{}\".format(pos_tensor.size(-1)))\n+        raise ValueError(f\"Unknown pos_tensor shape(-1):{pos_tensor.size(-1)}\")\n     return pos\n \n \n@@ -1254,7 +1254,7 @@ def __init__(self, config: DabDetrConfig):\n \n         self.num_patterns = config.num_patterns\n         if not isinstance(self.num_patterns, int):\n-            logger.warning(\"num_patterns should be int but {}\".format(type(self.num_patterns)))\n+            logger.warning(f\"num_patterns should be int but {type(self.num_patterns)}\")\n             self.num_patterns = 0\n         if self.num_patterns > 0:\n             self.patterns = nn.Embedding(self.num_patterns, self.hidden_size)"
        },
        {
            "sha": "3608d3b4a9fe103644e1759999c4dde1a6aac23b",
            "filename": "src/transformers/models/dac/convert_dac_checkpoint.py",
            "status": "modified",
            "additions": 3,
            "deletions": 15,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fdac%2Fconvert_dac_checkpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fdac%2Fconvert_dac_checkpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdac%2Fconvert_dac_checkpoint.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -157,24 +157,12 @@ def recursively_load_weights(orig_dict, hf_model, model_name):\n                 elif len(mapped_key) == 3:\n                     integers = re.findall(r\"\\b\\d+\\b\", name)\n                     if mapped_key[0][0] == \"d\":\n-                        mapped_key = \"{}.{}.{}{}.{}\".format(\n-                            mapped_key[0],\n-                            str(int(integers[0]) - 1),\n-                            mapped_key[1],\n-                            str(int(integers[1]) - 1),\n-                            mapped_key[2],\n-                        )\n+                        mapped_key = f\"{mapped_key[0]}.{str(int(integers[0]) - 1)}.{mapped_key[1]}{str(int(integers[1]) - 1)}.{mapped_key[2]}\"\n                     else:\n-                        mapped_key = \"{}.{}.{}{}.{}\".format(\n-                            mapped_key[0],\n-                            str(int(integers[0]) - 1),\n-                            mapped_key[1],\n-                            str(int(integers[1]) + 1),\n-                            mapped_key[2],\n-                        )\n+                        mapped_key = f\"{mapped_key[0]}.{str(int(integers[0]) - 1)}.{mapped_key[1]}{str(int(integers[1]) + 1)}.{mapped_key[2]}\"\n                 elif len(mapped_key) == 2:\n                     integers = re.findall(r\"\\b\\d+\\b\", name)\n-                    mapped_key = \"{}.{}.{}\".format(mapped_key[0], str(int(integers[0]) - 1), mapped_key[1])\n+                    mapped_key = f\"{mapped_key[0]}.{str(int(integers[0]) - 1)}.{mapped_key[1]}\"\n \n                 is_used = True\n                 if \"weight_g\" in name:"
        },
        {
            "sha": "910e1fc8e240bac51efd1d9f829766b121a8ebcf",
            "filename": "src/transformers/models/data2vec/convert_data2vec_vision_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconvert_data2vec_vision_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconvert_data2vec_vision_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconvert_data2vec_vision_original_pytorch_checkpoint_to_pytorch.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -185,18 +185,12 @@ def load(module, prefix=\"\"):\n         missing_keys = warn_missing_keys\n \n         if len(missing_keys) > 0:\n-            print(\n-                \"Weights of {} not initialized from pretrained model: {}\".format(\n-                    model.__class__.__name__, missing_keys\n-                )\n-            )\n+            print(f\"Weights of {model.__class__.__name__} not initialized from pretrained model: {missing_keys}\")\n         if len(unexpected_keys) > 0:\n-            print(\"Weights from pretrained model not used in {}: {}\".format(model.__class__.__name__, unexpected_keys))\n+            print(f\"Weights from pretrained model not used in {model.__class__.__name__}: {unexpected_keys}\")\n         if len(ignore_missing_keys) > 0:\n             print(\n-                \"Ignored weights of {} not initialized from pretrained model: {}\".format(\n-                    model.__class__.__name__, ignore_missing_keys\n-                )\n+                f\"Ignored weights of {model.__class__.__name__} not initialized from pretrained model: {ignore_missing_keys}\"\n             )\n         if len(error_msgs) > 0:\n             print(\"\\n\".join(error_msgs))"
        },
        {
            "sha": "c48782d247715d065c77a532da588607184fc14b",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_vision.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -101,7 +101,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return drop_path(hidden_states, self.drop_prob, self.training)\n \n     def extra_repr(self) -> str:\n-        return \"p={}\".format(self.drop_prob)\n+        return f\"p={self.drop_prob}\"\n \n \n # Copied from transformers.models.beit.modeling_beit.BeitEmbeddings with Beit->Data2VecVision\n@@ -515,8 +515,8 @@ def __init__(\n \n         init_values = config.layer_scale_init_value\n         if init_values > 0:\n-            self.lambda_1 = nn.Parameter(init_values * torch.ones((config.hidden_size)), requires_grad=True)\n-            self.lambda_2 = nn.Parameter(init_values * torch.ones((config.hidden_size)), requires_grad=True)\n+            self.lambda_1 = nn.Parameter(init_values * torch.ones(config.hidden_size), requires_grad=True)\n+            self.lambda_2 = nn.Parameter(init_values * torch.ones(config.hidden_size), requires_grad=True)\n         else:\n             self.lambda_1, self.lambda_2 = None, None\n "
        },
        {
            "sha": "766c127cd49fc6c006fd2e9a5990e29f596c80fe",
            "filename": "src/transformers/models/data2vec/modeling_tf_data2vec_vision.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_tf_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_tf_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_tf_data2vec_vision.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -306,7 +306,7 @@ def call(\n         hidden_states: tf.Tensor,\n         head_mask: tf.Tensor,\n         output_attentions: bool,\n-        relative_position_bias: Optional[\"TFData2VecVisionRelativePositionBias\"] = None,\n+        relative_position_bias: Optional[TFData2VecVisionRelativePositionBias] = None,\n         training: bool = False,\n     ) -> tuple[tf.Tensor]:\n         batch_size = shape_list(hidden_states)[0]\n@@ -416,7 +416,7 @@ def call(\n         input_tensor: tf.Tensor,\n         head_mask: tf.Tensor,\n         output_attentions: bool,\n-        relative_position_bias: Optional[\"TFData2VecVisionRelativePositionBias\"] = None,\n+        relative_position_bias: Optional[TFData2VecVisionRelativePositionBias] = None,\n         training: bool = False,\n     ) -> tuple[tf.Tensor]:\n         self_outputs = self.attention(\n@@ -538,8 +538,8 @@ def build(self, input_shape: tf.TensorShape = None):\n                 trainable=True,\n                 name=\"lambda_2\",\n             )\n-            self.lambda_1.assign(self.init_values * tf.ones((self.config.hidden_size)))\n-            self.lambda_2.assign(self.init_values * tf.ones((self.config.hidden_size)))\n+            self.lambda_1.assign(self.init_values * tf.ones(self.config.hidden_size))\n+            self.lambda_2.assign(self.init_values * tf.ones(self.config.hidden_size))\n         else:\n             self.lambda_1, self.lambda_2 = None, None\n \n@@ -570,7 +570,7 @@ def call(\n         hidden_states: tf.Tensor,\n         head_mask: tf.Tensor,\n         output_attentions: bool,\n-        relative_position_bias: Optional[\"TFData2VecVisionRelativePositionBias\"] = None,\n+        relative_position_bias: Optional[TFData2VecVisionRelativePositionBias] = None,\n         training: bool = False,\n     ) -> tuple[tf.Tensor]:\n         self_attention_outputs = self.attention("
        },
        {
            "sha": "6eb506218918392ea7e7a9c2cb585c20311db198",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -113,7 +113,7 @@ def __init__(self, config):\n         self.norm_topk_prob = config.norm_topk_prob\n \n         self.weight = nn.Parameter(torch.empty((self.n_routed_experts, config.hidden_size)))\n-        self.register_buffer(\"e_score_correction_bias\", torch.zeros((self.n_routed_experts)))\n+        self.register_buffer(\"e_score_correction_bias\", torch.zeros(self.n_routed_experts))\n \n     @torch.no_grad()\n     def get_topk_indices(self, scores):"
        },
        {
            "sha": "73f3acc2047e2d185d5acabc1b2e4f906d8c8ad6",
            "filename": "src/transformers/models/deepseek_v3/modular_deepseek_v3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -110,7 +110,7 @@ def __init__(self, config):\n         self.norm_topk_prob = config.norm_topk_prob\n \n         self.weight = nn.Parameter(torch.empty((self.n_routed_experts, config.hidden_size)))\n-        self.register_buffer(\"e_score_correction_bias\", torch.zeros((self.n_routed_experts)))\n+        self.register_buffer(\"e_score_correction_bias\", torch.zeros(self.n_routed_experts))\n \n     @torch.no_grad()\n     def get_topk_indices(self, scores):"
        },
        {
            "sha": "5e1a0cdf9aa6b1fb76cd35a58bfc34b53cc76ea8",
            "filename": "src/transformers/models/deprecated/efficientformer/modeling_efficientformer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fmodeling_efficientformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fmodeling_efficientformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fmodeling_efficientformer.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -270,7 +270,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return drop_path(hidden_states, self.drop_prob, self.training)\n \n     def extra_repr(self) -> str:\n-        return \"p={}\".format(self.drop_prob)\n+        return f\"p={self.drop_prob}\"\n \n \n class EfficientFormerFlat(nn.Module):\n@@ -303,8 +303,8 @@ def __init__(self, config: EfficientFormerConfig, dim: int, drop_path: float = 0\n         self.drop_path = EfficientFormerDropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n         self.use_layer_scale = config.use_layer_scale\n         if config.use_layer_scale:\n-            self.layer_scale_1 = nn.Parameter(config.layer_scale_init_value * torch.ones((dim)), requires_grad=True)\n-            self.layer_scale_2 = nn.Parameter(config.layer_scale_init_value * torch.ones((dim)), requires_grad=True)\n+            self.layer_scale_1 = nn.Parameter(config.layer_scale_init_value * torch.ones(dim), requires_grad=True)\n+            self.layer_scale_2 = nn.Parameter(config.layer_scale_init_value * torch.ones(dim), requires_grad=True)\n \n     def forward(self, hidden_states: torch.Tensor, output_attentions: bool = False) -> tuple[torch.Tensor]:\n         self_attention_outputs = self.token_mixer(self.layernorm1(hidden_states), output_attentions)\n@@ -370,8 +370,8 @@ def __init__(self, config: EfficientFormerConfig, dim: int, drop_path: float = 0\n         self.drop_path = EfficientFormerDropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n         self.use_layer_scale = config.use_layer_scale\n         if config.use_layer_scale:\n-            self.layer_scale_1 = nn.Parameter(config.layer_scale_init_value * torch.ones((dim)), requires_grad=True)\n-            self.layer_scale_2 = nn.Parameter(config.layer_scale_init_value * torch.ones((dim)), requires_grad=True)\n+            self.layer_scale_1 = nn.Parameter(config.layer_scale_init_value * torch.ones(dim), requires_grad=True)\n+            self.layer_scale_2 = nn.Parameter(config.layer_scale_init_value * torch.ones(dim), requires_grad=True)\n \n     def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor]:\n         outputs = self.token_mixer(hidden_states)"
        },
        {
            "sha": "4c33abd043e6d170485e6d01298788d461275aa6",
            "filename": "src/transformers/models/deprecated/ernie_m/tokenization_ernie_m.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Ftokenization_ernie_m.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Ftokenization_ernie_m.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Ftokenization_ernie_m.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"Tokenization classes for Ernie-M.\"\"\"\n \n-import io\n import os\n import unicodedata\n from typing import Any, Optional\n@@ -172,7 +171,7 @@ def __setstate__(self, d):\n \n     def clean_text(self, text):\n         \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n-        return \"\".join((self.SP_CHAR_MAPPING.get(c, c) for c in text))\n+        return \"\".join(self.SP_CHAR_MAPPING.get(c, c) for c in text)\n \n     def _tokenize(self, text, enable_sampling=False, nbest_size=64, alpha=0.1):\n         \"\"\"Tokenize a string.\"\"\"\n@@ -373,7 +372,7 @@ def is_whitespace(self, char):\n \n     def load_vocab(self, filepath):\n         token_to_idx = {}\n-        with io.open(filepath, \"r\", encoding=\"utf-8\") as f:\n+        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n             for index, line in enumerate(f):\n                 token = line.rstrip(\"\\n\")\n                 token_to_idx[token] = int(index)"
        },
        {
            "sha": "c31dc90928460d4578d1bd4b444a79a9276f3043",
            "filename": "src/transformers/models/deprecated/mega/modeling_mega.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -118,7 +118,7 @@ def __init__(self, config: MegaConfig):\n \n     def forward(self, seq_len):\n         if seq_len > self.max_positions:\n-            raise ValueError(\"Sequence length {} going beyond max length {}\".format(seq_len, self.max_positions))\n+            raise ValueError(f\"Sequence length {seq_len} going beyond max length {self.max_positions}\")\n \n         # seq_len * 2 - 1\n         bias = self.rel_pos_bias[(self.max_positions - seq_len) : (self.max_positions + seq_len - 1)]\n@@ -298,7 +298,7 @@ def __init__(self, norm_type, embedding_dim, eps=1e-5, affine=True, export=False\n         elif norm_type == \"syncbatchnorm\":\n             self.norm = nn.SyncBatchNorm(embedding_dim, eps=eps, affine=affine)\n         else:\n-            raise ValueError(\"Unknown norm type: {}\".format(norm_type))\n+            raise ValueError(f\"Unknown norm type: {norm_type}\")\n \n     def forward(self, input):\n         if isinstance(self.norm, nn.modules.batchnorm._BatchNorm):\n@@ -563,7 +563,7 @@ def __init__(self, config: MegaConfig):\n         elif self.config.relative_positional_bias == \"rotary\":\n             self.rel_pos_bias = MegaRotaryRelativePositionalBias(config)\n         else:\n-            raise ValueError(\"unknown relative position bias: {}\".format(self.config.relative_positional_bias))\n+            raise ValueError(f\"unknown relative position bias: {self.config.relative_positional_bias}\")\n \n         self.softmax = nn.Softmax(dim=-1)\n "
        },
        {
            "sha": "15c7dc62b350c999056130e0ede205a6aa41606a",
            "filename": "src/transformers/models/deprecated/nat/modeling_nat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fmodeling_nat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fmodeling_nat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fmodeling_nat.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -287,7 +287,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return drop_path(hidden_states, self.drop_prob, self.training)\n \n     def extra_repr(self) -> str:\n-        return \"p={}\".format(self.drop_prob)\n+        return f\"p={self.drop_prob}\"\n \n \n class NeighborhoodAttention(nn.Module):"
        },
        {
            "sha": "4ad87dd4e7a4cd1825ff0b231d8928aeef616da2",
            "filename": "src/transformers/models/deprecated/tapex/tokenization_tapex.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftapex%2Ftokenization_tapex.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftapex%2Ftokenization_tapex.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftapex%2Ftokenization_tapex.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -99,7 +99,7 @@ class TapexTruncationStrategy(ExplicitEnum):\n \"\"\"\n \n \n-@lru_cache()\n+@lru_cache\n def bytes_to_unicode():\n     \"\"\"\n     Returns list of utf-8 byte and a mapping to unicode strings. We specifically avoids mapping to whitespace/control"
        },
        {
            "sha": "2d9917164d952d10e5ca3707e3278c9b8a50fdd2",
            "filename": "src/transformers/models/deprecated/van/modeling_van.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fmodeling_van.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fmodeling_van.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fmodeling_van.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -79,7 +79,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return drop_path(hidden_states, self.drop_prob, self.training)\n \n     def extra_repr(self) -> str:\n-        return \"p={}\".format(self.drop_prob)\n+        return f\"p={self.drop_prob}\"\n \n \n class VanOverlappingPatchEmbedder(nn.Module):\n@@ -204,7 +204,7 @@ class VanLayerScaling(nn.Module):\n \n     def __init__(self, hidden_size: int, initial_value: float = 1e-2):\n         super().__init__()\n-        self.weight = nn.Parameter(initial_value * torch.ones((hidden_size)), requires_grad=True)\n+        self.weight = nn.Parameter(initial_value * torch.ones(hidden_size), requires_grad=True)\n \n     def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n         # unsqueezing for broadcasting"
        },
        {
            "sha": "2afe614dc974900f7fff9a9ef0b39fb9e84559f2",
            "filename": "src/transformers/models/dinat/modeling_dinat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -275,7 +275,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return drop_path(hidden_states, self.drop_prob, self.training)\n \n     def extra_repr(self) -> str:\n-        return \"p={}\".format(self.drop_prob)\n+        return f\"p={self.drop_prob}\"\n \n \n class NeighborhoodAttention(nn.Module):"
        },
        {
            "sha": "bd35abb941aca449e95331a2049d8f1e3091462b",
            "filename": "src/transformers/models/dinov2/modeling_dinov2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -343,7 +343,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return drop_path(hidden_states, self.drop_prob, self.training)\n \n     def extra_repr(self) -> str:\n-        return \"p={}\".format(self.drop_prob)\n+        return f\"p={self.drop_prob}\"\n \n \n class Dinov2MLP(nn.Module):"
        },
        {
            "sha": "c2eeb197021b2da36327c6d18ff7ce37bc967497",
            "filename": "src/transformers/models/dinov2_with_registers/modeling_dinov2_with_registers.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -360,7 +360,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return drop_path(hidden_states, self.drop_prob, self.training)\n \n     def extra_repr(self) -> str:\n-        return \"p={}\".format(self.drop_prob)\n+        return f\"p={self.drop_prob}\"\n \n \n class Dinov2WithRegistersMLP(nn.Module):"
        },
        {
            "sha": "a63b0d3f0f57f472577549aee80f9580a2b8ea45",
            "filename": "src/transformers/models/donut/modeling_donut_swin.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -393,7 +393,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return drop_path(hidden_states, self.drop_prob, self.training)\n \n     def extra_repr(self) -> str:\n-        return \"p={}\".format(self.drop_prob)\n+        return f\"p={self.drop_prob}\"\n \n \n # Copied from transformers.models.swin.modeling_swin.SwinSelfAttention with Swin->DonutSwin\n@@ -625,7 +625,7 @@ def get_attn_mask(self, height, width, dtype, device):\n             mask_windows = window_partition(img_mask, self.window_size)\n             mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n             attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n-            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n+            attn_mask = attn_mask.masked_fill(attn_mask != 0, -100.0).masked_fill(attn_mask == 0, 0.0)\n         else:\n             attn_mask = None\n         return attn_mask"
        },
        {
            "sha": "3338b3d50073d0d34a8d02fab92ed4a89e71f66a",
            "filename": "src/transformers/models/esm/modeling_esmfold.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -1414,7 +1414,7 @@ def __init__(self, config):\n \n         self.linear_b = EsmFoldLinear(c_z, config.num_heads_ipa)\n \n-        self.head_weights = nn.Parameter(torch.zeros((config.num_heads_ipa)))\n+        self.head_weights = nn.Parameter(torch.zeros(config.num_heads_ipa))\n \n         concat_out_dim = config.num_heads_ipa * (c_z + config.ipa_dim + config.num_v_points * 4)\n         self.linear_out = EsmFoldLinear(concat_out_dim, c_s, init=\"final\")"
        },
        {
            "sha": "9af36d7db74e04bb02c793199946a7afaa4f0c37",
            "filename": "src/transformers/models/esm/openfold_utils/residue_constants.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fesm%2Fopenfold_utils%2Fresidue_constants.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fesm%2Fopenfold_utils%2Fresidue_constants.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fopenfold_utils%2Fresidue_constants.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -398,7 +398,7 @@ def map_structure_with_atom_order(in_list: list, first_call: bool = True) -> lis\n     return in_list\n \n \n-@functools.lru_cache(maxsize=None)\n+@functools.cache\n def load_stereo_chemical_props() -> tuple[\n     Mapping[str, list[Bond]],\n     Mapping[str, list[Bond]],"
        },
        {
            "sha": "8d86151c4b06ef3e3654fa6a8cb43745f06ebac8",
            "filename": "src/transformers/models/esm/openfold_utils/rigid_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fesm%2Fopenfold_utils%2Frigid_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fesm%2Fopenfold_utils%2Frigid_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fopenfold_utils%2Frigid_utils.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -16,7 +16,7 @@\n from __future__ import annotations\n \n from collections.abc import Sequence\n-from functools import lru_cache\n+from functools import cache\n from typing import Any, Callable, Optional\n \n import numpy as np\n@@ -75,7 +75,7 @@ def rot_vec_mul(r: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n     )\n \n \n-@lru_cache(maxsize=None)\n+@cache\n def identity_rot_mats(\n     batch_dims: tuple[int, ...],\n     dtype: Optional[torch.dtype] = None,\n@@ -90,7 +90,7 @@ def identity_rot_mats(\n     return rots\n \n \n-@lru_cache(maxsize=None)\n+@cache\n def identity_trans(\n     batch_dims: tuple[int, ...],\n     dtype: Optional[torch.dtype] = None,\n@@ -101,7 +101,7 @@ def identity_trans(\n     return trans\n \n \n-@lru_cache(maxsize=None)\n+@cache\n def identity_quats(\n     batch_dims: tuple[int, ...],\n     dtype: Optional[torch.dtype] = None,\n@@ -220,7 +220,7 @@ def rot_to_quat(rot: torch.Tensor) -> torch.Tensor:\n }\n \n \n-@lru_cache(maxsize=None)\n+@cache\n def _get_quat(quat_key: str, dtype: torch.dtype, device: torch.device) -> torch.Tensor:\n     return torch.tensor(_CACHED_QUATS[quat_key], dtype=dtype, device=device)\n \n@@ -1070,7 +1070,7 @@ def from_3_points(\n         e0 = [c / denom for c in e0]\n         dot = sum((c1 * c2 for c1, c2 in zip(e0, e1)))\n         e1 = [c2 - c1 * dot for c1, c2 in zip(e0, e1)]\n-        denom = torch.sqrt(sum((c * c for c in e1)) + eps * torch.ones_like(e1[0]))\n+        denom = torch.sqrt(sum(c * c for c in e1) + eps * torch.ones_like(e1[0]))\n         e1 = [c / denom for c in e1]\n         e2 = [\n             e0[1] * e1[2] - e0[2] * e1[1],"
        },
        {
            "sha": "eb75d8f2b80af45e20f8ea4becc5c4c05f7297dd",
            "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -949,7 +949,7 @@ def torch_forward(\n \n             # 2. Compute the state for each intra-chunk\n             # (right term of low-rank factorization of off-diagonal blocks; B terms)\n-            decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum))\n+            decay_states = torch.exp(A_cumsum[:, :, :, -1:] - A_cumsum)\n             B_decay = B * decay_states.permute(0, -2, -1, 1)[..., None]\n             states = (B_decay[..., None, :] * hidden_states[..., None]).sum(dim=2)\n "
        },
        {
            "sha": "3572d8eb4547186f253dcc167ff2831f56715288",
            "filename": "src/transformers/models/falcon_h1/modular_falcon_h1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -748,7 +748,7 @@ def torch_forward(\n \n             # 2. Compute the state for each intra-chunk\n             # (right term of low-rank factorization of off-diagonal blocks; B terms)\n-            decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum))\n+            decay_states = torch.exp(A_cumsum[:, :, :, -1:] - A_cumsum)\n             B_decay = B * decay_states.permute(0, -2, -1, 1)[..., None]\n             states = (B_decay[..., None, :] * hidden_states[..., None]).sum(dim=2)\n "
        },
        {
            "sha": "fc8f47d87ce95a751c8ae6dff3b65969f2ee6331",
            "filename": "src/transformers/models/flava/image_processing_flava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -318,7 +318,7 @@ def from_dict(cls, image_processor_dict: dict[str, Any], **kwargs):\n             image_processor_dict[\"codebook_crop_size\"] = kwargs.pop(\"codebook_crop_size\")\n         return super().from_dict(image_processor_dict, **kwargs)\n \n-    @lru_cache()\n+    @lru_cache\n     def masking_generator(\n         self,\n         input_size_patches,"
        },
        {
            "sha": "cab61081cf4773cea00bf69764d4282ce2c8e2f2",
            "filename": "src/transformers/models/flava/image_processing_flava_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava_fast.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -273,7 +273,7 @@ def from_dict(cls, image_processor_dict: dict[str, Any], **kwargs):\n             image_processor_dict[\"codebook_crop_size\"] = kwargs.pop(\"codebook_crop_size\")\n         return super().from_dict(image_processor_dict, **kwargs)\n \n-    @lru_cache()\n+    @lru_cache\n     def masking_generator(\n         self,\n         input_size_patches,"
        },
        {
            "sha": "3bd7b45d0dc8b2f9e9ad7cec37fb09def7710a88",
            "filename": "src/transformers/models/flava/modeling_flava.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -1446,7 +1446,7 @@ def __init__(\n                 param.requires_grad = False\n \n     def get_codebook_indices(self, pixel_values: torch.Tensor) -> torch.Tensor:\n-        \"\"\"\n+        f\"\"\"\n         Args:\n             pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n                 Pixel values. Codebook pixel values can be obtained using [`AutoImageProcessor`] by passing\n@@ -1458,8 +1458,8 @@ def get_codebook_indices(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         >>> import requests\n         >>> from transformers import AutoImageProcessor, FlavaImageCodebook\n \n-        >>> model = FlavaImageCodebook.from_pretrained(\"{0}\")\n-        >>> image_processor = AutoImageProcessor.from_pretrained(\"{0}\")\n+        >>> model = FlavaImageCodebook.from_pretrained(\"{_CHECKPOINT_FOR_CODEBOOK_DOC}\")\n+        >>> image_processor = AutoImageProcessor.from_pretrained(\"{_CHECKPOINT_FOR_CODEBOOK_DOC}\")\n \n         >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n         >>> image = Image.open(requests.get(url, stream=True).raw)\n@@ -1469,7 +1469,7 @@ def get_codebook_indices(self, pixel_values: torch.Tensor) -> torch.Tensor:\n \n         >>> outputs = model.get_codebook_indices(**inputs)\n         ```\n-        \"\"\".format(_CHECKPOINT_FOR_CODEBOOK_DOC)\n+        \"\"\"\n         z_logits = self.blocks(pixel_values)\n         return torch.argmax(z_logits, axis=1)\n \n@@ -1478,7 +1478,7 @@ def get_codebook_probs(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         return nn.Softmax(dim=1)(z_logits)\n \n     def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n-        \"\"\"\n+        f\"\"\"\n         Args:\n             pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n                 Pixel values. Codebook pixel values can be obtained using [`AutoImageProcessor`] by passing\n@@ -1491,8 +1491,8 @@ def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n         >>> import requests\n         >>> from transformers import AutoImageProcessor, FlavaImageCodebook\n \n-        >>> model = FlavaImageCodebook.from_pretrained(\"{0}\")\n-        >>> image_processor = AutoImageProcessor.from_pretrained(\"{0}\")\n+        >>> model = FlavaImageCodebook.from_pretrained(\"{_CHECKPOINT_FOR_CODEBOOK_DOC}\")\n+        >>> image_processor = AutoImageProcessor.from_pretrained(\"{_CHECKPOINT_FOR_CODEBOOK_DOC}\")\n \n         >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n         >>> image = Image.open(requests.get(url, stream=True).raw)\n@@ -1504,7 +1504,7 @@ def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n         >>> print(outputs.shape)\n         (1, 196)\n         ```\n-        \"\"\".format(_CHECKPOINT_FOR_CODEBOOK_DOC)\n+        \"\"\"\n         if len(pixel_values.shape) != 4:\n             raise ValueError(f\"input shape {pixel_values.shape} is not 4d\")\n         if pixel_values.shape[1] != self.input_channels:"
        },
        {
            "sha": "72aa202612e0bd451c413d071e84443c8c384e6e",
            "filename": "src/transformers/models/fnet/tokenization_fnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Ffnet%2Ftokenization_fnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Ffnet%2Ftokenization_fnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffnet%2Ftokenization_fnet.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -177,7 +177,7 @@ def _tokenize(self, text: str) -> list[str]:\n         pieces = self.sp_model.encode(text, out_type=str)\n         new_pieces = []\n         for piece in pieces:\n-            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\n+            if len(piece) > 1 and piece[-1] == \",\" and piece[-2].isdigit():\n                 cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\n                 if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n                     if len(cur_pieces[0]) == 1:"
        },
        {
            "sha": "232f1e6ed1fa45c6a446668c0b8b257e7bb6a620",
            "filename": "src/transformers/models/focalnet/modeling_focalnet.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -293,7 +293,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return drop_path(hidden_states, self.drop_prob, self.training)\n \n     def extra_repr(self) -> str:\n-        return \"p={}\".format(self.drop_prob)\n+        return f\"p={self.drop_prob}\"\n \n \n class FocalNetModulation(nn.Module):\n@@ -431,8 +431,8 @@ def __init__(self, config, index, dim, input_resolution, drop_path=0.0):\n         self.gamma_1 = 1.0\n         self.gamma_2 = 1.0\n         if config.use_layerscale:\n-            self.gamma_1 = nn.Parameter(config.layerscale_value * torch.ones((dim)), requires_grad=True)\n-            self.gamma_2 = nn.Parameter(config.layerscale_value * torch.ones((dim)), requires_grad=True)\n+            self.gamma_1 = nn.Parameter(config.layerscale_value * torch.ones(dim), requires_grad=True)\n+            self.gamma_2 = nn.Parameter(config.layerscale_value * torch.ones(dim), requires_grad=True)\n \n     def forward(self, hidden_state, input_dimensions):\n         height, width = input_dimensions"
        },
        {
            "sha": "8715a09613a3aaa052a0f551b4c3a12f68d8873c",
            "filename": "src/transformers/models/glpn/modeling_glpn.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fglpn%2Fmodeling_glpn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fglpn%2Fmodeling_glpn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglpn%2Fmodeling_glpn.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -65,7 +65,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return drop_path(hidden_states, self.drop_prob, self.training)\n \n     def extra_repr(self) -> str:\n-        return \"p={}\".format(self.drop_prob)\n+        return f\"p={self.drop_prob}\"\n \n \n # Copied from transformers.models.segformer.modeling_segformer.SegformerOverlapPatchEmbeddings"
        },
        {
            "sha": "608164ef2d83ab15bf7f99d33f9c6eb56ed1fcff",
            "filename": "src/transformers/models/gpt2/tokenization_gpt2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fgpt2%2Ftokenization_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fgpt2%2Ftokenization_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Ftokenization_gpt2.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -33,7 +33,7 @@\n }\n \n \n-@lru_cache()\n+@lru_cache\n def bytes_to_unicode():\n     \"\"\"\n     Returns list of utf-8 byte and a mapping to unicode strings. We specifically avoids mapping to whitespace/control"
        },
        {
            "sha": "62af013448cefbd3bfce4a1074204d33c2b47101",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -757,7 +757,7 @@ def torch_forward(\n \n             # 2. Compute the state for each intra-chunk\n             # (right term of low-rank factorization of off-diagonal blocks; B terms)\n-            decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum))\n+            decay_states = torch.exp(A_cumsum[:, :, :, -1:] - A_cumsum)\n             B_decay = B * decay_states.permute(0, -2, -1, 1)[..., None]\n             states = (B_decay[..., None, :] * hidden_states[..., None]).sum(dim=2)\n "
        },
        {
            "sha": "200158d2ccbd516ac7d086dcc686ab98cf15bfd6",
            "filename": "src/transformers/models/grounding_dino/modeling_grounding_dino.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -921,7 +921,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return drop_path(hidden_states, self.drop_prob, self.training)\n \n     def extra_repr(self) -> str:\n-        return \"p={}\".format(self.drop_prob)\n+        return f\"p={self.drop_prob}\"\n \n \n class GroundingDinoFusionLayer(nn.Module):\n@@ -937,8 +937,8 @@ def __init__(self, config):\n         # add layer scale for training stability\n         self.drop_path = GroundingDinoDropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n         init_values = 1e-4\n-        self.vision_param = nn.Parameter(init_values * torch.ones((config.d_model)), requires_grad=True)\n-        self.text_param = nn.Parameter(init_values * torch.ones((config.d_model)), requires_grad=True)\n+        self.vision_param = nn.Parameter(init_values * torch.ones(config.d_model), requires_grad=True)\n+        self.text_param = nn.Parameter(init_values * torch.ones(config.d_model), requires_grad=True)\n \n     def forward(\n         self,"
        },
        {
            "sha": "2fadde33211e9ff3f22c3ed0a810999d182fbe34",
            "filename": "src/transformers/models/hiera/modeling_hiera.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -459,7 +459,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return drop_path(hidden_states, self.drop_prob, self.training)\n \n     def extra_repr(self) -> str:\n-        return \"p={}\".format(self.drop_prob)\n+        return f\"p={self.drop_prob}\"\n \n \n class HieraMlp(nn.Module):"
        },
        {
            "sha": "8d31e5e5c3be520944efd2ecd1de1702df73c93b",
            "filename": "src/transformers/models/hubert/convert_hubert_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fhubert%2Fconvert_hubert_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fhubert%2Fconvert_hubert_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fconvert_hubert_original_pytorch_checkpoint_to_pytorch.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -203,7 +203,7 @@ def convert_hubert_checkpoint(\n             config.vocab_size = len(target_dict.symbols)\n             vocab_path = os.path.join(pytorch_dump_folder_path, \"vocab.json\")\n             if not os.path.isdir(pytorch_dump_folder_path):\n-                logger.error(\"--pytorch_dump_folder_path ({}) should be a directory\".format(pytorch_dump_folder_path))\n+                logger.error(f\"--pytorch_dump_folder_path ({pytorch_dump_folder_path}) should be a directory\")\n                 return\n             os.makedirs(pytorch_dump_folder_path, exist_ok=True)\n             with open(vocab_path, \"w\", encoding=\"utf-8\") as vocab_handle:"
        },
        {
            "sha": "ff47c6457b9183fc66c51bb01b2b0600ee9db5af",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 2,
            "deletions": 13,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -300,12 +300,7 @@ def forward(self, input_ids):\n         return full_vector\n \n     def extra_repr(self) -> str:\n-        return \"num_embeddings={}, num_additional_embeddings={}, embedding_dim={}, partially_freeze={}\".format(\n-            self.num_embeddings,\n-            self.num_additional_embeddings,\n-            self.embedding_dim,\n-            self.partially_freeze,\n-        )\n+        return f\"num_embeddings={self.num_embeddings}, num_additional_embeddings={self.num_additional_embeddings}, embedding_dim={self.embedding_dim}, partially_freeze={self.partially_freeze}\"\n \n \n class IdeficsDecoupledLinear(nn.Linear):\n@@ -364,13 +359,7 @@ def forward(self, input: torch.Tensor) -> torch.Tensor:\n \n     def extra_repr(self) -> str:\n         \"\"\"Overwriting `nn.Linear.extra_repr` to include new parameters.\"\"\"\n-        return \"in_features={}, out_features={}, out_additional_features={}, bias={}, partially_freeze={}\".format(\n-            self.in_features,\n-            self.out_features,\n-            self.out_additional_features,\n-            self.bias is not None,\n-            self.partially_freeze,\n-        )\n+        return f\"in_features={self.in_features}, out_features={self.out_features}, out_additional_features={self.out_additional_features}, bias={self.bias is not None}, partially_freeze={self.partially_freeze}\"\n \n \n # this was adapted from LlamaRMSNorm"
        },
        {
            "sha": "405dee0d8f8b0b52fb7534dd46140e56757792c6",
            "filename": "src/transformers/models/idefics/modeling_tf_idefics.py",
            "status": "modified",
            "additions": 2,
            "deletions": 13,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_tf_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_tf_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_tf_idefics.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -362,12 +362,7 @@ def call(self, input_ids):\n         return full_vector\n \n     def extra_repr(self) -> str:\n-        return \"num_embeddings={}, num_additional_embeddings={}, embedding_dim={}, partially_freeze={}\".format(\n-            self.num_embeddings,\n-            self.num_additional_embeddings,\n-            self.output_dim,\n-            self.partially_freeze,\n-        )\n+        return f\"num_embeddings={self.num_embeddings}, num_additional_embeddings={self.num_additional_embeddings}, embedding_dim={self.output_dim}, partially_freeze={self.partially_freeze}\"\n \n \n class TFIdeficsDecoupledLinear(tf.keras.layers.Layer):\n@@ -431,13 +426,7 @@ def get_config(self):\n \n     def extra_repr(self) -> str:\n         \"\"\"Overwriting `nn.Linear.extra_repr` to include new parameters.\"\"\"\n-        return \"in_features={}, out_features={}, out_additional_features={}, bias={}, partially_freeze={}\".format(\n-            self.in_features,\n-            self.out_features,\n-            self.out_additional_features,\n-            self.bias is not None,\n-            self.partially_freeze,\n-        )\n+        return f\"in_features={self.in_features}, out_features={self.out_features}, out_additional_features={self.out_additional_features}, bias={self.bias is not None}, partially_freeze={self.partially_freeze}\"\n \n     @classmethod\n     def from_config(cls, config):"
        },
        {
            "sha": "db5ae763aadd74b7fd421f2d4010d871908501f4",
            "filename": "src/transformers/models/imagegpt/modeling_imagegpt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -60,14 +60,14 @@ def load_tf_weights_in_imagegpt(model, config, imagegpt_checkpoint_path):\n         )\n         raise\n     tf_path = os.path.abspath(imagegpt_checkpoint_path)\n-    logger.info(\"Converting TensorFlow checkpoint from {}\".format(tf_path))\n+    logger.info(f\"Converting TensorFlow checkpoint from {tf_path}\")\n     # Load weights from TF model\n     init_vars = tf.train.list_variables(tf_path)\n     names = []\n     arrays = []\n \n     for name, shape in init_vars:\n-        logger.info(\"Loading TF weight {} with shape {}\".format(name, shape))\n+        logger.info(f\"Loading TF weight {name} with shape {shape}\")\n         array = tf.train.load_variable(tf_path, name)\n         names.append(name)\n         arrays.append(array.squeeze())\n@@ -129,7 +129,7 @@ def load_tf_weights_in_imagegpt(model, config, imagegpt_checkpoint_path):\n                 e.args += (pointer.shape, array.shape)\n                 raise\n \n-        logger.info(\"Initialize PyTorch weight {}\".format(name))\n+        logger.info(f\"Initialize PyTorch weight {name}\")\n \n         if name[-1] == \"q_proj\":\n             pointer.data[:, : config.n_embd] = torch.from_numpy(array.reshape(config.n_embd, config.n_embd)).T"
        },
        {
            "sha": "485adea836307900caa7ad41d39434442053cc09",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -397,8 +397,8 @@ def __init__(self, config: InternVLVisionConfig) -> None:\n         self.layernorm_after = NORM2FN[config.norm_type](config.hidden_size, eps=config.layer_norm_eps)\n \n         init_values = config.layer_scale_init_value\n-        self.lambda_1 = nn.Parameter(init_values * torch.ones((config.hidden_size)), requires_grad=True)\n-        self.lambda_2 = nn.Parameter(init_values * torch.ones((config.hidden_size)), requires_grad=True)\n+        self.lambda_1 = nn.Parameter(init_values * torch.ones(config.hidden_size), requires_grad=True)\n+        self.lambda_2 = nn.Parameter(init_values * torch.ones(config.hidden_size), requires_grad=True)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n \n     def forward("
        },
        {
            "sha": "90576676b3cb0519583b3d541efea067fa1bebf9",
            "filename": "src/transformers/models/internvl/modular_internvl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -348,8 +348,8 @@ def __init__(self, config: InternVLVisionConfig) -> None:\n         self.layernorm_after = NORM2FN[config.norm_type](config.hidden_size, eps=config.layer_norm_eps)\n \n         init_values = config.layer_scale_init_value\n-        self.lambda_1 = nn.Parameter(init_values * torch.ones((config.hidden_size)), requires_grad=True)\n-        self.lambda_2 = nn.Parameter(init_values * torch.ones((config.hidden_size)), requires_grad=True)\n+        self.lambda_1 = nn.Parameter(init_values * torch.ones(config.hidden_size), requires_grad=True)\n+        self.lambda_2 = nn.Parameter(init_values * torch.ones(config.hidden_size), requires_grad=True)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n \n     def forward("
        },
        {
            "sha": "b69fc57b1743c4b0349f33b3bc1f4a3d35120df3",
            "filename": "src/transformers/models/layoutlmv3/tokenization_layoutlmv3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -140,7 +140,7 @@\n \"\"\"\n \n \n-@lru_cache()\n+@lru_cache\n # Copied from transformers.models.roberta.tokenization_roberta.bytes_to_unicode\n def bytes_to_unicode():\n     \"\"\""
        },
        {
            "sha": "d110ac30d969e1958a872e8bdc186df0822a25b5",
            "filename": "src/transformers/models/led/tokenization_led.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fled%2Ftokenization_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fled%2Ftokenization_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Ftokenization_led.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -34,7 +34,7 @@\n # See all LED models at https://huggingface.co/models?filter=LED\n \n \n-@lru_cache()\n+@lru_cache\n # Copied from transformers.models.bart.tokenization_bart.bytes_to_unicode\n def bytes_to_unicode():\n     \"\"\""
        },
        {
            "sha": "5a7b969b94a6d885187d48a9bb436089cf6001c8",
            "filename": "src/transformers/models/lightglue/modeling_lightglue.py",
            "status": "modified",
            "additions": 8,
            "deletions": 12,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -797,22 +797,18 @@ def _match_image_pair(\n                     # Remove image pairs that have been early stopped from the forward pass\n                     num_points_per_pair = num_points_per_pair[~early_stopped_pairs]\n                     descriptors, keypoints_0, keypoint_1, mask, image_indices = tuple(\n-                        (\n-                            tensor[~early_stops]\n-                            for tensor in [descriptors, keypoints[0], keypoints[1], mask, image_indices]\n-                        )\n+                        tensor[~early_stops]\n+                        for tensor in [descriptors, keypoints[0], keypoints[1], mask, image_indices]\n                     )\n                     keypoints = (keypoints_0, keypoint_1)\n                     if do_keypoint_pruning:\n                         pruned_keypoints_indices, pruned_keypoints_iterations, keypoint_confidences = tuple(\n-                            (\n-                                tensor[~early_stops]\n-                                for tensor in [\n-                                    pruned_keypoints_indices,\n-                                    pruned_keypoints_iterations,\n-                                    keypoint_confidences,\n-                                ]\n-                            )\n+                            tensor[~early_stops]\n+                            for tensor in [\n+                                pruned_keypoints_indices,\n+                                pruned_keypoints_iterations,\n+                                keypoint_confidences,\n+                            ]\n                         )\n                 # If all pairs of images are early stopped, we stop the forward pass through the transformer\n                 # layers for all pairs of images."
        },
        {
            "sha": "bbdc595330b31d67c41b68f73eba3f7d8b138fb7",
            "filename": "src/transformers/models/lightglue/modular_lightglue.py",
            "status": "modified",
            "additions": 8,
            "deletions": 12,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -871,22 +871,18 @@ def _match_image_pair(\n                     # Remove image pairs that have been early stopped from the forward pass\n                     num_points_per_pair = num_points_per_pair[~early_stopped_pairs]\n                     descriptors, keypoints_0, keypoint_1, mask, image_indices = tuple(\n-                        (\n-                            tensor[~early_stops]\n-                            for tensor in [descriptors, keypoints[0], keypoints[1], mask, image_indices]\n-                        )\n+                        tensor[~early_stops]\n+                        for tensor in [descriptors, keypoints[0], keypoints[1], mask, image_indices]\n                     )\n                     keypoints = (keypoints_0, keypoint_1)\n                     if do_keypoint_pruning:\n                         pruned_keypoints_indices, pruned_keypoints_iterations, keypoint_confidences = tuple(\n-                            (\n-                                tensor[~early_stops]\n-                                for tensor in [\n-                                    pruned_keypoints_indices,\n-                                    pruned_keypoints_iterations,\n-                                    keypoint_confidences,\n-                                ]\n-                            )\n+                            tensor[~early_stops]\n+                            for tensor in [\n+                                pruned_keypoints_indices,\n+                                pruned_keypoints_iterations,\n+                                keypoint_confidences,\n+                            ]\n                         )\n                 # If all pairs of images are early stopped, we stop the forward pass through the transformer\n                 # layers for all pairs of images."
        },
        {
            "sha": "4d9609cb63a77655e1a44cb1f6f16242f6b6cd09",
            "filename": "src/transformers/models/llava/convert_llava_weights_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fllava%2Fconvert_llava_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fllava%2Fconvert_llava_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fconvert_llava_weights_to_hf.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -161,13 +161,11 @@ def convert_llava_llama_to_hf(text_model_id, vision_model_id, output_hub_path, o\n     vocab_size = config.text_config.vocab_size\n     model.resize_token_embeddings(config.text_config.vocab_size + 2, pad_shape)\n     model.language_model.model.embed_tokens.weight.data[vocab_size:] = torch.stack(\n-        tuple(\n-            (dist.sample() for _ in range(model.language_model.model.embed_tokens.weight.data[vocab_size:].shape[0]))\n-        ),\n+        tuple(dist.sample() for _ in range(model.language_model.model.embed_tokens.weight.data[vocab_size:].shape[0])),\n         dim=0,\n     )\n     model.language_model.lm_head.weight.data[vocab_size:] = torch.stack(\n-        tuple((dist.sample() for _ in range(model.language_model.lm_head.weight.data[vocab_size:].shape[0]))),\n+        tuple(dist.sample() for _ in range(model.language_model.lm_head.weight.data[vocab_size:].shape[0])),\n         dim=0,\n     )\n "
        },
        {
            "sha": "8aee180dd777ad159785443a4f23ce31117acc79",
            "filename": "src/transformers/models/llava_next/convert_llava_next_weights_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fllava_next%2Fconvert_llava_next_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fllava_next%2Fconvert_llava_next_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fconvert_llava_next_weights_to_hf.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -175,15 +175,12 @@ def convert_llava_to_hf(model_id, pytorch_dump_folder_path, push_to_hub=False):\n         model.resize_token_embeddings(num_tokens, pad_to_multiple_of=pad_shape)\n         model.language_model.model.embed_tokens.weight.data[vocab_size:] = torch.stack(\n             tuple(\n-                (\n-                    dist.sample()\n-                    for _ in range(model.language_model.model.embed_tokens.weight.data[vocab_size:].shape[0])\n-                )\n+                dist.sample() for _ in range(model.language_model.model.embed_tokens.weight.data[vocab_size:].shape[0])\n             ),\n             dim=0,\n         )\n         model.language_model.lm_head.weight.data[vocab_size:] = torch.stack(\n-            tuple((dist.sample() for _ in range(model.language_model.lm_head.weight.data[vocab_size:].shape[0]))),\n+            tuple(dist.sample() for _ in range(model.language_model.lm_head.weight.data[vocab_size:].shape[0])),\n             dim=0,\n         )\n "
        },
        {
            "sha": "265e543cb55730342d9c6f0a4d301bd86cd30b1a",
            "filename": "src/transformers/models/llava_next_video/convert_llava_next_video_weights_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconvert_llava_next_video_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconvert_llava_next_video_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconvert_llava_next_video_weights_to_hf.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -227,13 +227,11 @@ def convert_llava_to_hf(model_id, pytorch_dump_folder_path, push_to_hub=False):\n     num_tokens = vocab_size + 3\n     model.resize_token_embeddings(num_tokens, pad_to_multiple_of=pad_shape)\n     model.language_model.model.embed_tokens.weight.data[vocab_size:] = torch.stack(\n-        tuple(\n-            (dist.sample() for _ in range(model.language_model.model.embed_tokens.weight.data[vocab_size:].shape[0]))\n-        ),\n+        tuple(dist.sample() for _ in range(model.language_model.model.embed_tokens.weight.data[vocab_size:].shape[0])),\n         dim=0,\n     )\n     model.language_model.lm_head.weight.data[vocab_size:] = torch.stack(\n-        tuple((dist.sample() for _ in range(model.language_model.lm_head.weight.data[vocab_size:].shape[0]))),\n+        tuple(dist.sample() for _ in range(model.language_model.lm_head.weight.data[vocab_size:].shape[0])),\n         dim=0,\n     )\n "
        },
        {
            "sha": "79bcad09ce13534fac130b1bd383fb6923795548",
            "filename": "src/transformers/models/llava_onevision/convert_llava_onevision_weights_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconvert_llava_onevision_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconvert_llava_onevision_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconvert_llava_onevision_weights_to_hf.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -176,13 +176,11 @@ def convert_llava_to_hf(model_id, pytorch_dump_folder_path, push_to_hub=False):\n     num_tokens = vocab_size + 2\n     model.resize_token_embeddings(num_tokens, pad_to_multiple_of=pad_shape)\n     model.language_model.model.embed_tokens.weight.data[vocab_size:] = torch.stack(\n-        tuple(\n-            (dist.sample() for _ in range(model.language_model.model.embed_tokens.weight.data[vocab_size:].shape[0]))\n-        ),\n+        tuple(dist.sample() for _ in range(model.language_model.model.embed_tokens.weight.data[vocab_size:].shape[0])),\n         dim=0,\n     )\n     model.language_model.lm_head.weight.data[vocab_size:] = torch.stack(\n-        tuple((dist.sample() for _ in range(model.language_model.lm_head.weight.data[vocab_size:].shape[0]))),\n+        tuple(dist.sample() for _ in range(model.language_model.lm_head.weight.data[vocab_size:].shape[0])),\n         dim=0,\n     )\n "
        },
        {
            "sha": "104bdd7a9b99f5a4809557dcee97cda4873cea12",
            "filename": "src/transformers/models/longformer/tokenization_longformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Flongformer%2Ftokenization_longformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Flongformer%2Ftokenization_longformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongformer%2Ftokenization_longformer.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -30,7 +30,7 @@\n VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.json\", \"merges_file\": \"merges.txt\"}\n \n \n-@lru_cache()\n+@lru_cache\n # Copied from transformers.models.roberta.tokenization_roberta.bytes_to_unicode\n def bytes_to_unicode():\n     \"\"\""
        },
        {
            "sha": "5e0e461862a8b32be02330e7c5897f030f04a137",
            "filename": "src/transformers/models/luke/convert_luke_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fluke%2Fconvert_luke_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fluke%2Fconvert_luke_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fluke%2Fconvert_luke_original_pytorch_checkpoint_to_pytorch.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -127,7 +127,7 @@ def convert_luke_checkpoint(checkpoint_path, metadata_path, entity_vocab_path, p\n         raise ValueError\n \n     # Finally, save our PyTorch model and tokenizer\n-    print(\"Saving PyTorch model to {}\".format(pytorch_dump_folder_path))\n+    print(f\"Saving PyTorch model to {pytorch_dump_folder_path}\")\n     model.save_pretrained(pytorch_dump_folder_path)\n \n "
        },
        {
            "sha": "8b3305587781bcbf25b42e8bd0eb7784b3a74f64",
            "filename": "src/transformers/models/luke/tokenization_luke.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fluke%2Ftokenization_luke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fluke%2Ftokenization_luke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fluke%2Ftokenization_luke.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -130,7 +130,7 @@\n \"\"\"\n \n \n-@lru_cache()\n+@lru_cache\n # Copied from transformers.models.roberta.tokenization_roberta.bytes_to_unicode\n def bytes_to_unicode():\n     \"\"\""
        },
        {
            "sha": "17925c5acc03221657cb178a703edb668501b4c5",
            "filename": "src/transformers/models/mamba2/modeling_mamba2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -606,7 +606,7 @@ def torch_forward(\n \n             # 2. Compute the state for each intra-chunk\n             # (right term of low-rank factorization of off-diagonal blocks; B terms)\n-            decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum))\n+            decay_states = torch.exp(A_cumsum[:, :, :, -1:] - A_cumsum)\n             B_decay = B * decay_states.permute(0, -2, -1, 1)[..., None]\n             states = (B_decay[..., None, :] * hidden_states[..., None]).sum(dim=2)\n "
        },
        {
            "sha": "a090e11ec36dff560134c91ed8053423fe35b7bb",
            "filename": "src/transformers/models/markuplm/tokenization_markuplm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -89,7 +89,7 @@\n \"\"\"\n \n \n-@lru_cache()\n+@lru_cache\n def bytes_to_unicode():\n     \"\"\"\n     Returns list of utf-8 byte and a mapping to unicode strings. We specifically avoids mapping to whitespace/control"
        },
        {
            "sha": "f8448936419b2c487f9f5d152555c210744c8364",
            "filename": "src/transformers/models/markuplm/tokenization_markuplm_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm_fast.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -44,7 +44,7 @@\n VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.json\", \"merges_file\": \"merges.txt\", \"tokenizer_file\": \"tokenizer.json\"}\n \n \n-@lru_cache()\n+@lru_cache\n def bytes_to_unicode():\n     \"\"\"\n     Returns list of utf-8 byte and a mapping to unicode strings. We specifically avoids mapping to whitespace/control"
        },
        {
            "sha": "3eb559dfcdb4b7fd838d0c8c301f0ceb9465b6b0",
            "filename": "src/transformers/models/mask2former/modeling_mask2former.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -1247,8 +1247,8 @@ def __init__(self, config: Mask2FormerConfig, feature_channels):\n                 nn.GroupNorm(32, feature_dim),\n                 nn.ReLU(),\n             )\n-            self.add_module(\"adapter_{}\".format(idx + 1), lateral_conv)\n-            self.add_module(\"layer_{}\".format(idx + 1), output_conv)\n+            self.add_module(f\"adapter_{idx + 1}\", lateral_conv)\n+            self.add_module(f\"layer_{idx + 1}\", output_conv)\n \n             lateral_convs.append(lateral_conv)\n             output_convs.append(output_conv)"
        },
        {
            "sha": "b7505aa6748ee125e3bb6ef68db9c2e389b45bc7",
            "filename": "src/transformers/models/maskformer/modeling_maskformer_swin.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -333,7 +333,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return drop_path(hidden_states, self.drop_prob, self.training)\n \n     def extra_repr(self) -> str:\n-        return \"p={}\".format(self.drop_prob)\n+        return f\"p={self.drop_prob}\"\n \n \n # Copied from transformers.models.swin.modeling_swin.SwinSelfAttention with Swin->MaskFormerSwin\n@@ -556,7 +556,7 @@ def get_attn_mask(self, input_resolution):\n             mask_windows = window_partition(img_mask, self.window_size)\n             mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n             attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n-            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n+            attn_mask = attn_mask.masked_fill(attn_mask != 0, -100.0).masked_fill(attn_mask == 0, 0.0)\n         else:\n             attn_mask = None\n         return attn_mask"
        },
        {
            "sha": "d22b1536081bc2e2c203915cdbd098ae89701d04",
            "filename": "src/transformers/models/megatron_bert/modeling_megatron_bert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -62,7 +62,7 @@ def load_tf_weights_in_megatron_bert(model, config, tf_checkpoint_path):\n         )\n         raise\n     tf_path = os.path.abspath(tf_checkpoint_path)\n-    logger.info(\"Converting TensorFlow checkpoint from {}\".format(tf_path))\n+    logger.info(f\"Converting TensorFlow checkpoint from {tf_path}\")\n     # Load weights from TF model\n     init_vars = tf.train.list_variables(tf_path)\n     names = []\n@@ -112,7 +112,7 @@ def load_tf_weights_in_megatron_bert(model, config, tf_checkpoint_path):\n             array = np.transpose(array)\n         if pointer.shape != array.shape:\n             raise ValueError(f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\")\n-        logger.info(\"Initialize PyTorch weight {}\".format(name))\n+        logger.info(f\"Initialize PyTorch weight {name}\")\n         pointer.data = torch.from_numpy(array)\n     return model\n "
        },
        {
            "sha": "f422ce24ef97bb09bf124c93b676790c6ca42c28",
            "filename": "src/transformers/models/mgp_str/modeling_mgp_str.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fmodeling_mgp_str.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fmodeling_mgp_str.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fmodeling_mgp_str.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -65,7 +65,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return drop_path(hidden_states, self.drop_prob, self.training)\n \n     def extra_repr(self) -> str:\n-        return \"p={}\".format(self.drop_prob)\n+        return f\"p={self.drop_prob}\"\n \n \n @dataclass"
        },
        {
            "sha": "f29b8a8348ee3771210df17de79be6c30a65c34a",
            "filename": "src/transformers/models/mgp_str/tokenization_mgp_str.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fmgp_str%2Ftokenization_mgp_str.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fmgp_str%2Ftokenization_mgp_str.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmgp_str%2Ftokenization_mgp_str.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -89,7 +89,7 @@ def _convert_id_to_token(self, index):\n \n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         if not os.path.isdir(save_directory):\n-            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n+            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n             return\n         vocab_file = os.path.join(\n             save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]"
        },
        {
            "sha": "f918908c3d13df21a1d8523f41633639062c4f48",
            "filename": "src/transformers/models/mllama/image_processing_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fmllama%2Fimage_processing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fmllama%2Fimage_processing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fimage_processing_mllama.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -487,7 +487,7 @@ def to_channel_dimension_format(\n     elif target_channel_dim == ChannelDimension.LAST:\n         image = image.transpose((1, 2, 0))\n     else:\n-        raise ValueError(\"Unsupported channel dimension format: {}\".format(channel_dim))\n+        raise ValueError(f\"Unsupported channel dimension format: {channel_dim}\")\n \n     return image\n "
        },
        {
            "sha": "1881e26e1555332ac46f82ff886557c7d7365b0f",
            "filename": "src/transformers/models/mluke/convert_mluke_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fmluke%2Fconvert_mluke_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fmluke%2Fconvert_mluke_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmluke%2Fconvert_mluke_original_pytorch_checkpoint_to_pytorch.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -179,7 +179,7 @@ def convert_luke_checkpoint(checkpoint_path, metadata_path, entity_vocab_path, p\n     assert [e for e in multilingual_predicted_entities if e.startswith(\"en:\")][0] == \"en:Japan\"\n \n     # Finally, save our PyTorch model and tokenizer\n-    print(\"Saving PyTorch model to {}\".format(pytorch_dump_folder_path))\n+    print(f\"Saving PyTorch model to {pytorch_dump_folder_path}\")\n     model.save_pretrained(pytorch_dump_folder_path)\n \n "
        },
        {
            "sha": "1fc473e0933abc0c26a2f8eaf05d4f766a5e0365",
            "filename": "src/transformers/models/mobilevitv2/convert_mlcvnets_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fconvert_mlcvnets_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fconvert_mlcvnets_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fconvert_mlcvnets_to_pytorch.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -60,7 +60,7 @@ def flatten_yaml_as_dict(d, parent_key=\"\", sep=\".\"):\n             for k, v in flat_cfg.items():\n                 setattr(config, k, v)\n         except yaml.YAMLError as exc:\n-            logger.error(\"Error while loading config file: {}. Error message: {}\".format(orig_cfg_file, str(exc)))\n+            logger.error(f\"Error while loading config file: {orig_cfg_file}. Error message: {str(exc)}\")\n     return config\n \n "
        },
        {
            "sha": "f6039df2dc02564378f6f83ef1770ded9000d31f",
            "filename": "src/transformers/models/mvp/tokenization_mvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fmvp%2Ftokenization_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fmvp%2Ftokenization_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2Ftokenization_mvp.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -32,7 +32,7 @@\n # See all MVP models at https://huggingface.co/models?filter=mvp\n \n \n-@lru_cache()\n+@lru_cache\n def bytes_to_unicode():\n     \"\"\"\n     Returns list of utf-8 byte and a mapping to unicode strings. We specifically avoids mapping to whitespace/control"
        },
        {
            "sha": "3380118dd9efab3ff1f81a455e9a61270ee8ea44",
            "filename": "src/transformers/models/omdet_turbo/modeling_omdet_turbo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -1142,7 +1142,7 @@ def get_class_similarity(class_distance_type, cls_feature, class_proj):\n     elif class_distance_type == \"dot\":\n         class_logits = torch.bmm(cls_feature, class_proj)\n     else:\n-        raise Exception(\"Unknown class_distance_type {}\".format(class_distance_type))\n+        raise Exception(f\"Unknown class_distance_type {class_distance_type}\")\n     return class_logits\n \n "
        },
        {
            "sha": "d400e08cd18a217c95ae8720400823ca4c69d7b4",
            "filename": "src/transformers/models/oneformer/modeling_oneformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -1345,8 +1345,8 @@ def __init__(self, config: OneFormerConfig, feature_channels):\n                 nn.GroupNorm(32, config.conv_dim),\n                 nn.ReLU(),\n             )\n-            self.add_module(\"adapter_{}\".format(idx + 1), lateral_conv)\n-            self.add_module(\"layer_{}\".format(idx + 1), output_conv)\n+            self.add_module(f\"adapter_{idx + 1}\", lateral_conv)\n+            self.add_module(f\"layer_{idx + 1}\", output_conv)\n \n             lateral_convs.append(lateral_conv)\n             output_convs.append(output_conv)"
        },
        {
            "sha": "62619ea1d39d00225df5af2cc1ba7019e561be77",
            "filename": "src/transformers/models/paligemma/convert_paligemma2_weights_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconvert_paligemma2_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconvert_paligemma2_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconvert_paligemma2_weights_to_hf.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -346,13 +346,11 @@ def convert_paligemma2_checkpoint(\n         # We add an image token so we resize the model\n         model.resize_token_embeddings(config.text_config.vocab_size + 2, pad_shape)\n         model.language_model.model.embed_tokens.weight.data[257152:] = torch.stack(\n-            tuple(\n-                (dist.sample() for _ in range(model.language_model.model.embed_tokens.weight.data[257152:].shape[0]))\n-            ),\n+            tuple(dist.sample() for _ in range(model.language_model.model.embed_tokens.weight.data[257152:].shape[0])),\n             dim=0,\n         )\n         model.language_model.lm_head.weight.data[257152:] = torch.stack(\n-            tuple((dist.sample() for _ in range(model.language_model.lm_head.weight.data[257152:].shape[0]))),\n+            tuple(dist.sample() for _ in range(model.language_model.lm_head.weight.data[257152:].shape[0])),\n             dim=0,\n         )\n         # convert to needed precision"
        },
        {
            "sha": "b46300f1bb2616bb3c2db6abce849957688f5721",
            "filename": "src/transformers/models/paligemma/convert_paligemma_weights_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconvert_paligemma_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconvert_paligemma_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconvert_paligemma_weights_to_hf.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -279,11 +279,11 @@ def convert_paligemma_checkpoint(\n     # We add an image token so we resize the model\n     model.resize_token_embeddings(config.text_config.vocab_size + 2, pad_shape)\n     model.language_model.model.embed_tokens.weight.data[257152:] = torch.stack(\n-        tuple((dist.sample() for _ in range(model.language_model.model.embed_tokens.weight.data[257152:].shape[0]))),\n+        tuple(dist.sample() for _ in range(model.language_model.model.embed_tokens.weight.data[257152:].shape[0])),\n         dim=0,\n     )\n     model.language_model.lm_head.weight.data[257152:] = torch.stack(\n-        tuple((dist.sample() for _ in range(model.language_model.lm_head.weight.data[257152:].shape[0]))),\n+        tuple(dist.sample() for _ in range(model.language_model.lm_head.weight.data[257152:].shape[0])),\n         dim=0,\n     )\n "
        },
        {
            "sha": "bec4cfa688a3cc4e01442f3ce46e0ac15edc9a58",
            "filename": "src/transformers/models/pix2struct/convert_pix2struct_original_pytorch_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fconvert_pix2struct_original_pytorch_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fconvert_pix2struct_original_pytorch_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fconvert_pix2struct_original_pytorch_to_hf.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -139,7 +139,7 @@ def convert_pix2struct_original_pytorch_checkpoint_to_hf(\n     model.save_pretrained(pytorch_dump_folder_path)\n     processor.save_pretrained(pytorch_dump_folder_path)\n \n-    print(\"Model saved in {}\".format(pytorch_dump_folder_path))\n+    print(f\"Model saved in {pytorch_dump_folder_path}\")\n \n \n if __name__ == \"__main__\":"
        },
        {
            "sha": "ea37c22e755759dbd8f61cb74b55de063ebcdca6",
            "filename": "src/transformers/models/poolformer/image_processing_poolformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -185,7 +185,7 @@ def resize(\n                 else:\n                     scale_size = (int(size[\"height\"] / crop_pct), int(size[\"width\"] / crop_pct))\n             else:\n-                raise ValueError(\"Invalid size for resize: {}\".format(size))\n+                raise ValueError(f\"Invalid size for resize: {size}\")\n \n             output_size = get_resize_output_image_size(\n                 image, size=scale_size, default_to_square=False, input_data_format=input_data_format\n@@ -198,7 +198,7 @@ def resize(\n             elif \"height\" in size and \"width\" in size:\n                 output_size = (size[\"height\"], size[\"width\"])\n             else:\n-                raise ValueError(\"Invalid size for resize: {}\".format(size))\n+                raise ValueError(f\"Invalid size for resize: {size}\")\n \n         return resize(\n             image,"
        },
        {
            "sha": "4219aa6b576e3d1517b2d8c8427ce440c5ac48a2",
            "filename": "src/transformers/models/poolformer/image_processing_poolformer_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer_fast.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -136,7 +136,7 @@ def resize(\n                 else:\n                     scale_size = (int(size.height / crop_pct), int(size.width / crop_pct))\n             else:\n-                raise ValueError(\"Invalid size for resize: {}\".format(size))\n+                raise ValueError(f\"Invalid size for resize: {size}\")\n \n             new_size = get_resize_output_image_size(\n                 image,"
        },
        {
            "sha": "8e01e39872025e42e485494623c21b8aa6276f06",
            "filename": "src/transformers/models/poolformer/modeling_poolformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fmodeling_poolformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fmodeling_poolformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fmodeling_poolformer.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -65,7 +65,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return drop_path(hidden_states, self.drop_prob, self.training)\n \n     def extra_repr(self) -> str:\n-        return \"p={}\".format(self.drop_prob)\n+        return f\"p={self.drop_prob}\"\n \n \n class PoolFormerEmbeddings(nn.Module):\n@@ -142,10 +142,10 @@ def __init__(self, config, num_channels, pool_size, hidden_size, intermediate_si\n         self.use_layer_scale = config.use_layer_scale\n         if config.use_layer_scale:\n             self.layer_scale_1 = nn.Parameter(\n-                config.layer_scale_init_value * torch.ones((num_channels)), requires_grad=True\n+                config.layer_scale_init_value * torch.ones(num_channels), requires_grad=True\n             )\n             self.layer_scale_2 = nn.Parameter(\n-                config.layer_scale_init_value * torch.ones((num_channels)), requires_grad=True\n+                config.layer_scale_init_value * torch.ones(num_channels), requires_grad=True\n             )\n \n     def forward(self, hidden_states):"
        },
        {
            "sha": "bf0c1afdfaa9874a9d7fd1d5f4ff3a3cf29256d4",
            "filename": "src/transformers/models/pop2piano/tokenization_pop2piano.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fpop2piano%2Ftokenization_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fpop2piano%2Ftokenization_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Ftokenization_pop2piano.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -404,7 +404,7 @@ def encode_plus(\n         notes = np.round(notes).astype(np.int32)\n         max_time_idx = notes[:, :2].max()\n \n-        times = [[] for i in range((max_time_idx + 1))]\n+        times = [[] for i in range(max_time_idx + 1)]\n         for onset, offset, pitch, velocity in notes:\n             times[onset].append([pitch, velocity])\n             times[offset].append([pitch, 0])"
        },
        {
            "sha": "5fb4f8269e4b895fc07ce65ac69fda712d579bd6",
            "filename": "src/transformers/models/pvt/modeling_pvt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fpvt%2Fmodeling_pvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fpvt%2Fmodeling_pvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpvt%2Fmodeling_pvt.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -71,7 +71,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return drop_path(hidden_states, self.drop_prob, self.training)\n \n     def extra_repr(self) -> str:\n-        return \"p={}\".format(self.drop_prob)\n+        return f\"p={self.drop_prob}\"\n \n \n class PvtPatchEmbeddings(nn.Module):"
        },
        {
            "sha": "7c2f48bd5806756fa8fe5849c2f52ac4ac8a234f",
            "filename": "src/transformers/models/pvt_v2/modeling_pvt_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fpvt_v2%2Fmodeling_pvt_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fpvt_v2%2Fmodeling_pvt_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpvt_v2%2Fmodeling_pvt_v2.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -69,7 +69,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return drop_path(hidden_states, self.drop_prob, self.training)\n \n     def extra_repr(self) -> str:\n-        return \"p={}\".format(self.drop_prob)\n+        return f\"p={self.drop_prob}\"\n \n \n class PvtV2OverlapPatchEmbeddings(nn.Module):"
        },
        {
            "sha": "be121adb54423069ff7454678f71a22ca2b76bc1",
            "filename": "src/transformers/models/qwen2/tokenization_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fqwen2%2Ftokenization_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fqwen2%2Ftokenization_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Ftokenization_qwen2.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -39,7 +39,7 @@\n PRETOKENIZE_REGEX = r\"\"\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\"\"\n \n \n-@lru_cache()\n+@lru_cache\n # Copied from transformers.models.gpt2.tokenization_gpt2.bytes_to_unicode\n def bytes_to_unicode():\n     \"\"\""
        },
        {
            "sha": "c4e151e9ce163f0b0936149d6f42e1eda339cdbf",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -3866,7 +3866,7 @@ def load_speakers(self, path):\n         check_torch_load_is_safe()\n         for key, value in torch.load(path, weights_only=True).items():\n             self.speaker_map[key] = value\n-        logger.info(\"Speaker {} loaded\".format(list(self.speaker_map.keys())))\n+        logger.info(f\"Speaker {list(self.speaker_map.keys())} loaded\")\n \n     def disable_talker(self):\n         if hasattr(self, \"talker\"):"
        },
        {
            "sha": "10edb4e6a4396e5d462f8cc2fc7532ee23328513",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -4134,7 +4134,7 @@ def load_speakers(self, path):\n         check_torch_load_is_safe()\n         for key, value in torch.load(path, weights_only=True).items():\n             self.speaker_map[key] = value\n-        logger.info(\"Speaker {} loaded\".format(list(self.speaker_map.keys())))\n+        logger.info(f\"Speaker {list(self.speaker_map.keys())} loaded\")\n \n     def disable_talker(self):\n         if hasattr(self, \"talker\"):"
        },
        {
            "sha": "369388c540f99c7138cfec1fcafe6d01fa4c862d",
            "filename": "src/transformers/models/rembert/convert_rembert_tf_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Frembert%2Fconvert_rembert_tf_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Frembert%2Fconvert_rembert_tf_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Fconvert_rembert_tf_checkpoint_to_pytorch.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -28,14 +28,14 @@\n def convert_rembert_tf_checkpoint_to_pytorch(tf_checkpoint_path, bert_config_file, pytorch_dump_path):\n     # Initialise PyTorch model\n     config = RemBertConfig.from_json_file(bert_config_file)\n-    print(\"Building PyTorch model from configuration: {}\".format(str(config)))\n+    print(f\"Building PyTorch model from configuration: {str(config)}\")\n     model = RemBertModel(config)\n \n     # Load weights from tf checkpoint\n     load_tf_weights_in_rembert(model, config, tf_checkpoint_path)\n \n     # Save pytorch-model\n-    print(\"Save PyTorch model to {}\".format(pytorch_dump_path))\n+    print(f\"Save PyTorch model to {pytorch_dump_path}\")\n     torch.save(model.state_dict(), pytorch_dump_path)\n \n "
        },
        {
            "sha": "c45844047f621775bc0524891804935541795700",
            "filename": "src/transformers/models/rembert/modeling_tf_rembert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_tf_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_tf_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_tf_rembert.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -535,7 +535,7 @@ def __init__(self, config: RemBertConfig, **kwargs):\n             kernel_initializer=get_initializer(config.initializer_range),\n             name=\"embedding_hidden_mapping_in\",\n         )\n-        self.layer = [TFRemBertLayer(config, name=\"layer_._{}\".format(i)) for i in range(config.num_hidden_layers)]\n+        self.layer = [TFRemBertLayer(config, name=f\"layer_._{i}\") for i in range(config.num_hidden_layers)]\n \n     def call(\n         self,"
        },
        {
            "sha": "cf27a7b3bae66ba8b7ce43027bee3a277f7877d2",
            "filename": "src/transformers/models/rembert/tokenization_rembert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Frembert%2Ftokenization_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Frembert%2Ftokenization_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Ftokenization_rembert.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -218,7 +218,7 @@ def get_special_tokens_mask(\n \n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         if not os.path.isdir(save_directory):\n-            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n+            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n             return\n         out_vocab_file = os.path.join(\n             save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]"
        },
        {
            "sha": "fb358746e6d2da1054142b1472efe50815f8abf8",
            "filename": "src/transformers/models/rembert/tokenization_rembert_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Frembert%2Ftokenization_rembert_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Frembert%2Ftokenization_rembert_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Ftokenization_rembert_fast.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -183,7 +183,7 @@ def get_special_tokens_mask(\n \n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         if not os.path.isdir(save_directory):\n-            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n+            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n             return\n         out_vocab_file = os.path.join(\n             save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]"
        },
        {
            "sha": "67cdcbbf488a53abcd51db017de963cea48ea647",
            "filename": "src/transformers/models/roberta/tokenization_roberta.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Froberta%2Ftokenization_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Froberta%2Ftokenization_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Ftokenization_roberta.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -33,7 +33,7 @@\n }\n \n \n-@lru_cache()\n+@lru_cache\n def bytes_to_unicode():\n     \"\"\"\n     Returns list of utf-8 byte and a mapping to unicode strings. We specifically avoids mapping to whitespace/control"
        },
        {
            "sha": "a9088958a8f4612e388cb733280600a718406f03",
            "filename": "src/transformers/models/sam/modeling_sam.py",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -1442,21 +1442,19 @@ def forward(\n         if input_points is not None and len(input_points.shape) != 4:\n             raise ValueError(\n                 \"The input_points must be a 4D tensor. Of shape `batch_size`, `point_batch_size`, `nb_points_per_image`, `2`.\",\n-                \" got {}.\".format(input_points.shape),\n+                f\" got {input_points.shape}.\",\n             )\n         if input_boxes is not None and len(input_boxes.shape) != 3:\n             raise ValueError(\n                 \"The input_points must be a 3D tensor. Of shape `batch_size`, `nb_boxes`, `4`.\",\n-                \" got {}.\".format(input_boxes.shape),\n+                f\" got {input_boxes.shape}.\",\n             )\n         if input_points is not None and input_boxes is not None:\n             point_batch_size = input_points.shape[1]\n             box_batch_size = input_boxes.shape[1]\n             if point_batch_size != box_batch_size:\n                 raise ValueError(\n-                    \"You should provide as many bounding boxes as input points per box. Got {} and {}.\".format(\n-                        point_batch_size, box_batch_size\n-                    )\n+                    f\"You should provide as many bounding boxes as input points per box. Got {point_batch_size} and {box_batch_size}.\"\n                 )\n \n         image_positional_embeddings = self.get_image_wide_positional_embeddings()\n@@ -1486,7 +1484,7 @@ def forward(\n         if input_points is not None and image_embeddings.shape[0] != input_points.shape[0]:\n             raise ValueError(\n                 \"The batch size of the image embeddings and the input points must be the same. \",\n-                \"Got {} and {} respectively.\".format(image_embeddings.shape[0], input_points.shape[0]),\n+                f\"Got {image_embeddings.shape[0]} and {input_points.shape[0]} respectively.\",\n                 \" if you want to pass multiple points for the same image, make sure that you passed \",\n                 \" input_points of shape (batch_size, point_batch_size, num_points_per_image, 3) and \",\n                 \" input_labels of shape (batch_size, point_batch_size, num_points_per_image)\","
        },
        {
            "sha": "69afaf2b89c8eec434acd75dc7c94000df9012fe",
            "filename": "src/transformers/models/sam/modeling_tf_sam.py",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_tf_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_tf_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_tf_sam.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -1597,21 +1597,19 @@ def call(\n         if input_points is not None and len(input_points.shape) != 4:\n             raise ValueError(\n                 \"The input_points must be a 4D tensor. Of shape `batch_size`, `point_batch_size`, `nb_points_per_image`, `2`.\",\n-                \" got {}.\".format(input_points.shape),\n+                f\" got {input_points.shape}.\",\n             )\n         if input_boxes is not None and len(input_boxes.shape) != 3:\n             raise ValueError(\n                 \"The input_points must be a 3D tensor. Of shape `batch_size`, `nb_boxes`, `4`.\",\n-                \" got {}.\".format(input_boxes.shape),\n+                f\" got {input_boxes.shape}.\",\n             )\n         if input_points is not None and input_boxes is not None:\n             point_batch_size = shape_list(input_points)[1]\n             box_batch_size = shape_list(input_boxes)[1]\n             if point_batch_size != box_batch_size:\n                 raise ValueError(\n-                    \"You should provide as many bounding boxes as input points per box. Got {} and {}.\".format(\n-                        point_batch_size, box_batch_size\n-                    )\n+                    f\"You should provide as many bounding boxes as input points per box. Got {point_batch_size} and {box_batch_size}.\"\n                 )\n         if pixel_values is not None:\n             # Ensures that later checks pass even with an all-None shape from the serving signature\n@@ -1653,7 +1651,7 @@ def call(\n         if input_points is not None and image_embeddings.shape[0] != input_points.shape[0]:\n             raise ValueError(\n                 \"The batch size of the image embeddings and the input points must be the same. \",\n-                \"Got {} and {} respectively.\".format(image_embeddings.shape[0], input_points.shape[0]),\n+                f\"Got {image_embeddings.shape[0]} and {input_points.shape[0]} respectively.\",\n                 \" if you want to pass multiple points for the same image, make sure that you passed \",\n                 \" input_points of shape (batch_size, point_batch_size, num_points_per_image, 3) and \",\n                 \" input_labels of shape (batch_size, point_batch_size, num_points_per_image)\","
        },
        {
            "sha": "2039116985563ca43273f5bdd72b46ea6f626052",
            "filename": "src/transformers/models/sam_hq/modeling_sam_hq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -1587,9 +1587,7 @@ def forward(\n             box_batch_size = input_boxes.shape[1]\n             if point_batch_size != box_batch_size:\n                 raise ValueError(\n-                    \"You should provide as many bounding boxes as input points per box. Got {} and {}.\".format(\n-                        point_batch_size, box_batch_size\n-                    )\n+                    f\"You should provide as many bounding boxes as input points per box. Got {point_batch_size} and {box_batch_size}.\"\n                 )\n \n         image_positional_embeddings = self.get_image_wide_positional_embeddings()"
        },
        {
            "sha": "a78ce712cc0d3a85c8fda481b471864951e910a4",
            "filename": "src/transformers/models/sam_hq/modular_sam_hq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodular_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodular_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodular_sam_hq.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -615,9 +615,7 @@ def forward(\n             box_batch_size = input_boxes.shape[1]\n             if point_batch_size != box_batch_size:\n                 raise ValueError(\n-                    \"You should provide as many bounding boxes as input points per box. Got {} and {}.\".format(\n-                        point_batch_size, box_batch_size\n-                    )\n+                    f\"You should provide as many bounding boxes as input points per box. Got {point_batch_size} and {box_batch_size}.\"\n                 )\n \n         image_positional_embeddings = self.get_image_wide_positional_embeddings()"
        },
        {
            "sha": "24f02c3e6b4cf67efcca328cb923891077160a19",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -2434,7 +2434,7 @@ def forward(\n         lang = self.language_embedding(lang_id).transpose(1, 2)\n \n         log_dur_pred = self.dur_predictor(hidden_states.transpose(1, 2))\n-        dur_out = torch.clamp(torch.round((torch.expm1(log_dur_pred))).long(), min=1)\n+        dur_out = torch.clamp(torch.round(torch.expm1(log_dur_pred)).long(), min=1)\n         # B x C x T\n         if hidden_states.size(0) == 1:\n             hidden_states = torch.repeat_interleave(hidden_states, dur_out.view(-1), dim=2)"
        },
        {
            "sha": "95c586bfd76139b5d50ed300e2794476f0093e0e",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -2110,7 +2110,7 @@ def forward(\n \n         # predict duration\n         log_dur_pred = self.duration_predictor(char_hidden_states, padding_mask=char_padding_mask)\n-        dur_out = torch.clamp(torch.round((torch.expm1(log_dur_pred))).long(), min=1)\n+        dur_out = torch.clamp(torch.round(torch.expm1(log_dur_pred)).long(), min=1)\n         dur_out = dur_out.masked_fill(~char_padding_mask.bool(), 0.0)\n \n         # upsample char hidden states according to predicted duration\n@@ -2675,7 +2675,7 @@ def forward(\n         lang = self.language_embedding(lang_id).transpose(1, 2)\n \n         log_dur_pred = self.dur_predictor(hidden_states.transpose(1, 2))\n-        dur_out = torch.clamp(torch.round((torch.expm1(log_dur_pred))).long(), min=1)\n+        dur_out = torch.clamp(torch.round(torch.expm1(log_dur_pred)).long(), min=1)\n         # B x C x T\n         if hidden_states.size(0) == 1:\n             hidden_states = torch.repeat_interleave(hidden_states, dur_out.view(-1), dim=2)"
        },
        {
            "sha": "81c220446103bb28deaad7c39d57cb32496f3bd6",
            "filename": "src/transformers/models/segformer/modeling_segformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_segformer.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -93,7 +93,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return drop_path(hidden_states, self.drop_prob, self.training)\n \n     def extra_repr(self) -> str:\n-        return \"p={}\".format(self.drop_prob)\n+        return f\"p={self.drop_prob}\"\n \n \n class SegformerOverlapPatchEmbeddings(nn.Module):"
        },
        {
            "sha": "69c5ce88f7a9657f21b54944fed982e736d6dc32",
            "filename": "src/transformers/models/seggpt/modeling_seggpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -392,7 +392,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return drop_path(hidden_states, self.drop_prob, self.training)\n \n     def extra_repr(self) -> str:\n-        return \"p={}\".format(self.drop_prob)\n+        return f\"p={self.drop_prob}\"\n \n \n class SegGptLayer(nn.Module):"
        },
        {
            "sha": "be8b507b60bfbb1091a2d7adcf5cde938d2ff3d0",
            "filename": "src/transformers/models/sew/convert_sew_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fsew%2Fconvert_sew_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fsew%2Fconvert_sew_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Fconvert_sew_original_pytorch_checkpoint_to_pytorch.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -263,7 +263,7 @@ def convert_sew_checkpoint(\n             config.vocab_size = len(target_dict.symbols)\n             vocab_path = os.path.join(pytorch_dump_folder_path, \"vocab.json\")\n             if not os.path.isdir(pytorch_dump_folder_path):\n-                logger.error(\"--pytorch_dump_folder_path ({}) should be a directory\".format(pytorch_dump_folder_path))\n+                logger.error(f\"--pytorch_dump_folder_path ({pytorch_dump_folder_path}) should be a directory\")\n                 return\n             os.makedirs(pytorch_dump_folder_path, exist_ok=True)\n             with open(vocab_path, \"w\", encoding=\"utf-8\") as vocab_handle:"
        },
        {
            "sha": "b4dfd7bd2ad267a27cab3f919c41edd25409d3f7",
            "filename": "src/transformers/models/sew_d/convert_sew_d_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fsew_d%2Fconvert_sew_d_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fsew_d%2Fconvert_sew_d_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew_d%2Fconvert_sew_d_original_pytorch_checkpoint_to_pytorch.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -275,7 +275,7 @@ def convert_sew_checkpoint(\n             config.vocab_size = len(target_dict.symbols)\n             vocab_path = os.path.join(pytorch_dump_folder_path, \"vocab.json\")\n             if not os.path.isdir(pytorch_dump_folder_path):\n-                logger.error(\"--pytorch_dump_folder_path ({}) should be a directory\".format(pytorch_dump_folder_path))\n+                logger.error(f\"--pytorch_dump_folder_path ({pytorch_dump_folder_path}) should be a directory\")\n                 return\n             os.makedirs(pytorch_dump_folder_path, exist_ok=True)\n             with open(vocab_path, \"w\", encoding=\"utf-8\") as vocab_handle:"
        },
        {
            "sha": "c63426468d2d6543261fd9753f01c4a01e4a3df6",
            "filename": "src/transformers/models/speecht5/modeling_speecht5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -404,7 +404,7 @@ class SpeechT5ScaledPositionalEncoding(nn.Module):\n     def __init__(self, dropout, dim, max_len=5000):\n         pe = torch.zeros(max_len, dim)\n         position = torch.arange(0, max_len).unsqueeze(1)\n-        div_term = torch.exp((torch.arange(0, dim, 2, dtype=torch.int64).float() * -(math.log(10000.0) / dim)))\n+        div_term = torch.exp(torch.arange(0, dim, 2, dtype=torch.int64).float() * -(math.log(10000.0) / dim))\n         pe[:, 0::2] = torch.sin(position.float() * div_term)\n         pe[:, 1::2] = torch.cos(position.float() * div_term)\n         pe = pe.unsqueeze(0)"
        },
        {
            "sha": "5d9707dea829547d8c47aab7917c782bb5c1c8fe",
            "filename": "src/transformers/models/swiftformer/modeling_swiftformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fswiftformer%2Fmodeling_swiftformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fswiftformer%2Fmodeling_swiftformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswiftformer%2Fmodeling_swiftformer.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -91,7 +91,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return drop_path(hidden_states, self.drop_prob, self.training)\n \n     def extra_repr(self) -> str:\n-        return \"p={}\".format(self.drop_prob)\n+        return f\"p={self.drop_prob}\"\n \n \n class SwiftFormerEmbeddings(nn.Module):"
        },
        {
            "sha": "a8c29e84785abb8f86cdbec8c88a737ac99583fc",
            "filename": "src/transformers/models/swin/modeling_swin.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -433,7 +433,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return drop_path(hidden_states, self.drop_prob, self.training)\n \n     def extra_repr(self) -> str:\n-        return \"p={}\".format(self.drop_prob)\n+        return f\"p={self.drop_prob}\"\n \n \n class SwinSelfAttention(nn.Module):\n@@ -659,7 +659,7 @@ def get_attn_mask(self, height, width, dtype, device):\n             mask_windows = window_partition(img_mask, self.window_size)\n             mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n             attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n-            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n+            attn_mask = attn_mask.masked_fill(attn_mask != 0, -100.0).masked_fill(attn_mask == 0, 0.0)\n         else:\n             attn_mask = None\n         return attn_mask"
        },
        {
            "sha": "b5dd36f1cac2e2f06f240bc853a5160c338a2d1d",
            "filename": "src/transformers/models/swin/modeling_tf_swin.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_tf_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_tf_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_tf_swin.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -795,8 +795,8 @@ def get_attn_mask(self, height: int, width: int, window_size: int, shift_size: i\n         mask_windows = window_partition(img_mask, window_size)\n         mask_windows = tf.reshape(mask_windows, (-1, window_size * window_size))\n         attn_mask = tf.expand_dims(mask_windows, 1) - tf.expand_dims(mask_windows, 2)\n-        attn_mask = tf.where(attn_mask != 0, float(-100.0), attn_mask)\n-        attn_mask = tf.where(attn_mask == 0, float(0.0), attn_mask)\n+        attn_mask = tf.where(attn_mask != 0, -100.0, attn_mask)\n+        attn_mask = tf.where(attn_mask == 0, 0.0, attn_mask)\n         return attn_mask\n \n     def maybe_pad("
        },
        {
            "sha": "192e58d9db06d4aab8c06400341927f5da26c23b",
            "filename": "src/transformers/models/swin2sr/convert_swin2sr_original_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fconvert_swin2sr_original_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fconvert_swin2sr_original_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fconvert_swin2sr_original_to_pytorch.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -170,7 +170,7 @@ def convert_swin2sr_checkpoint(checkpoint_url, pytorch_dump_folder_path, push_to\n     missing_keys, unexpected_keys = model.load_state_dict(new_state_dict, strict=False)\n \n     if len(missing_keys) > 0:\n-        raise ValueError(\"Missing keys when converting: {}\".format(missing_keys))\n+        raise ValueError(f\"Missing keys when converting: {missing_keys}\")\n     for key in unexpected_keys:\n         if not (\"relative_position_index\" in key or \"relative_coords_table\" in key or \"self_mask\" in key):\n             raise ValueError(f\"Unexpected key {key} in state_dict\")"
        },
        {
            "sha": "c63579a014f58ab675d348cf2dc64774b4e11177",
            "filename": "src/transformers/models/swin2sr/modeling_swin2sr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -117,7 +117,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return drop_path(hidden_states, self.drop_prob, self.training)\n \n     def extra_repr(self) -> str:\n-        return \"p={}\".format(self.drop_prob)\n+        return f\"p={self.drop_prob}\"\n \n \n class Swin2SREmbeddings(nn.Module):\n@@ -521,7 +521,7 @@ def get_attn_mask(self, height, width, dtype):\n             mask_windows = window_partition(img_mask, self.window_size)\n             mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n             attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n-            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n+            attn_mask = attn_mask.masked_fill(attn_mask != 0, -100.0).masked_fill(attn_mask == 0, 0.0)\n         else:\n             attn_mask = None\n         return attn_mask"
        },
        {
            "sha": "050e8d3fd271aa639bd48d7ac1909f68a2ce1c96",
            "filename": "src/transformers/models/swinv2/modeling_swinv2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -250,7 +250,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return drop_path(hidden_states, self.drop_prob, self.training)\n \n     def extra_repr(self) -> str:\n-        return \"p={}\".format(self.drop_prob)\n+        return f\"p={self.drop_prob}\"\n \n \n # Copied from transformers.models.swin.modeling_swin.SwinEmbeddings with Swin->Swinv2\n@@ -716,7 +716,7 @@ def get_attn_mask(self, height, width, dtype):\n             mask_windows = window_partition(img_mask, self.window_size)\n             mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n             attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n-            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n+            attn_mask = attn_mask.masked_fill(attn_mask != 0, -100.0).masked_fill(attn_mask == 0, 0.0)\n         else:\n             attn_mask = None\n         return attn_mask"
        },
        {
            "sha": "60dc8fb7a55a155de941ec9a3fddec523b0f2aac",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -307,7 +307,7 @@ def forward(self, hidden_states):\n             0\n         ].tolist()  # length: number of \"activated\" expert / value: index\n         for idx in idx_mask:\n-            next_states[router_mask[:, :, idx]] = getattr(self.experts, \"expert_{}\".format(idx))(\n+            next_states[router_mask[:, :, idx]] = getattr(self.experts, f\"expert_{idx}\")(\n                 hidden_states[router_mask[:, :, idx]]\n             )\n "
        },
        {
            "sha": "00592039a920c615edbc6f872479bb4b49f6ebab",
            "filename": "src/transformers/models/timesformer/modeling_timesformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Ftimesformer%2Fmodeling_timesformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Ftimesformer%2Fmodeling_timesformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimesformer%2Fmodeling_timesformer.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -179,7 +179,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return drop_path(hidden_states, self.drop_prob, self.training)\n \n     def extra_repr(self) -> str:\n-        return \"p={}\".format(self.drop_prob)\n+        return f\"p={self.drop_prob}\"\n \n \n # Adapted from https://github.com/facebookresearch/TimeSformer/blob/a5ef29a7b7264baff199a30b3306ac27de901133/timesformer/models/vit.py#L57\n@@ -309,7 +309,7 @@ def __init__(self, config: TimesformerConfig, layer_index: int) -> None:\n         self.config = config\n         self.attention_type = attention_type\n         if attention_type not in [\"divided_space_time\", \"space_only\", \"joint_space_time\"]:\n-            raise ValueError(\"Unknown attention type: {}\".format(attention_type))\n+            raise ValueError(f\"Unknown attention type: {attention_type}\")\n \n         # Temporal Attention Parameters\n         if self.attention_type == \"divided_space_time\":"
        },
        {
            "sha": "79c5c2ca399b8aece501e066441b3d8173a44910",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -1219,7 +1219,7 @@ def forward(\n         batch_size, seq_length = input_shape\n \n         if use_cache is True:\n-            assert self.is_decoder, \"`use_cache` can only be set to `True` if {} is used as a decoder\".format(self)\n+            assert self.is_decoder, f\"`use_cache` can only be set to `True` if {self} is used as a decoder\"\n \n         # initialize past_key_values\n         return_legacy_cache = False"
        },
        {
            "sha": "b17d304a1e48558ab3d60a7cdbe931423cde8111",
            "filename": "src/transformers/models/unispeech/convert_unispeech_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Funispeech%2Fconvert_unispeech_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Funispeech%2Fconvert_unispeech_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fconvert_unispeech_original_pytorch_checkpoint_to_pytorch.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -210,7 +210,7 @@ def convert_unispeech_checkpoint(\n             config.vocab_size = len(target_dict.symbols)\n             vocab_path = os.path.join(pytorch_dump_folder_path, \"vocab.json\")\n             if not os.path.isdir(pytorch_dump_folder_path):\n-                logger.error(\"--pytorch_dump_folder_path ({}) should be a directory\".format(pytorch_dump_folder_path))\n+                logger.error(f\"--pytorch_dump_folder_path ({pytorch_dump_folder_path}) should be a directory\")\n                 return\n             os.makedirs(pytorch_dump_folder_path, exist_ok=True)\n             vocab_dict = target_dict.indices"
        },
        {
            "sha": "4f0f56f21bff5224505ba4d10efb4bdad9a2b91f",
            "filename": "src/transformers/models/video_llava/convert_video_llava_weights_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fconvert_video_llava_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fconvert_video_llava_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fconvert_video_llava_weights_to_hf.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -116,11 +116,11 @@ def convert_video_llava_llama_to_hf(text_model_id, vision_model_id, output_hub_p\n     # We add an image and video token so we resize the model\n     model.resize_token_embeddings(config.text_config.vocab_size + 3, pad_shape)\n     model.language_model.model.embed_tokens.weight.data[32000:] = torch.stack(\n-        tuple((dist.sample() for _ in range(model.language_model.model.embed_tokens.weight.data[32000:].shape[0]))),\n+        tuple(dist.sample() for _ in range(model.language_model.model.embed_tokens.weight.data[32000:].shape[0])),\n         dim=0,\n     )\n     model.language_model.lm_head.weight.data[32000:] = torch.stack(\n-        tuple((dist.sample() for _ in range(model.language_model.lm_head.weight.data[32000:].shape[0]))),\n+        tuple(dist.sample() for _ in range(model.language_model.lm_head.weight.data[32000:].shape[0])),\n         dim=0,\n     )\n "
        },
        {
            "sha": "47f58cc6e10a5d3ce30e0fb5663dd934d0e116d4",
            "filename": "src/transformers/models/vipllava/convert_vipllava_weights_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fvipllava%2Fconvert_vipllava_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fvipllava%2Fconvert_vipllava_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fconvert_vipllava_weights_to_hf.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -92,11 +92,11 @@ def convert_vipllava_llama_to_hf(text_model_id, vision_model_id, output_hub_path\n     # We add an image token so we resize the model\n     model.resize_token_embeddings(config.text_config.vocab_size + 2, pad_shape)\n     model.language_model.model.embed_tokens.weight.data[32000:] = torch.stack(\n-        tuple((dist.sample() for _ in range(model.language_model.model.embed_tokens.weight.data[32000:].shape[0]))),\n+        tuple(dist.sample() for _ in range(model.language_model.model.embed_tokens.weight.data[32000:].shape[0])),\n         dim=0,\n     )\n     model.language_model.lm_head.weight.data[32000:] = torch.stack(\n-        tuple((dist.sample() for _ in range(model.language_model.lm_head.weight.data[32000:].shape[0]))),\n+        tuple(dist.sample() for _ in range(model.language_model.lm_head.weight.data[32000:].shape[0])),\n         dim=0,\n     )\n "
        },
        {
            "sha": "e13e36d08e2967ab906b70b654abb9b04d91c93b",
            "filename": "src/transformers/models/vitdet/modeling_vitdet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fvitdet%2Fmodeling_vitdet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fvitdet%2Fmodeling_vitdet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitdet%2Fmodeling_vitdet.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -294,7 +294,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return drop_path(hidden_states, self.drop_prob, self.training)\n \n     def extra_repr(self) -> str:\n-        return \"p={}\".format(self.drop_prob)\n+        return f\"p={self.drop_prob}\"\n \n \n class VitDetLayerNorm(nn.Module):"
        },
        {
            "sha": "107d3819f4106cff2d7a2a77731322a6b5a4ebd7",
            "filename": "src/transformers/models/vjepa2/modeling_vjepa2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -402,7 +402,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return drop_path(hidden_states, self.drop_prob, self.training)\n \n     def extra_repr(self) -> str:\n-        return \"p={}\".format(self.drop_prob)\n+        return f\"p={self.drop_prob}\"\n \n \n class VJEPA2MLP(nn.Module):"
        },
        {
            "sha": "1e0f00f4775c4e2c8cf3b88e227e9d9e95b83014",
            "filename": "src/transformers/models/wav2vec2/convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fconvert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fconvert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fconvert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -307,7 +307,7 @@ def convert_wav2vec2_checkpoint(\n             config.vocab_size = len(target_dict.symbols)\n             vocab_path = os.path.join(pytorch_dump_folder_path, \"vocab.json\")\n             if not os.path.isdir(pytorch_dump_folder_path):\n-                logger.error(\"--pytorch_dump_folder_path ({}) should be a directory\".format(pytorch_dump_folder_path))\n+                logger.error(f\"--pytorch_dump_folder_path ({pytorch_dump_folder_path}) should be a directory\")\n                 return\n             os.makedirs(pytorch_dump_folder_path, exist_ok=True)\n             vocab_dict = target_dict.indices"
        },
        {
            "sha": "ce54a1756ab376230b4a9a9ee304e413e98300dc",
            "filename": "src/transformers/models/wav2vec2_conformer/convert_wav2vec2_conformer_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fconvert_wav2vec2_conformer_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fconvert_wav2vec2_conformer_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fconvert_wav2vec2_conformer_original_pytorch_checkpoint_to_pytorch.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -243,7 +243,7 @@ def convert_wav2vec2_conformer_checkpoint(\n             config.vocab_size = len(target_dict.symbols)\n             vocab_path = os.path.join(pytorch_dump_folder_path, \"vocab.json\")\n             if not os.path.isdir(pytorch_dump_folder_path):\n-                logger.error(\"--pytorch_dump_folder_path ({}) should be a directory\".format(pytorch_dump_folder_path))\n+                logger.error(f\"--pytorch_dump_folder_path ({pytorch_dump_folder_path}) should be a directory\")\n                 return\n             os.makedirs(pytorch_dump_folder_path, exist_ok=True)\n             vocab_dict = target_dict.indices"
        },
        {
            "sha": "41db6f5ce8549d11cacabd1413cd4b16d99ee014",
            "filename": "src/transformers/models/x_clip/modeling_x_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -421,7 +421,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return drop_path(hidden_states, self.drop_prob, self.training)\n \n     def extra_repr(self) -> str:\n-        return \"p={}\".format(self.drop_prob)\n+        return f\"p={self.drop_prob}\"\n \n \n class XCLIPVisionEncoderLayer(nn.Module):"
        },
        {
            "sha": "9186db33d788e2766755c0d1ec51614e95173a0f",
            "filename": "src/transformers/models/xlnet/tokenization_xlnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fxlnet%2Ftokenization_xlnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fxlnet%2Ftokenization_xlnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlnet%2Ftokenization_xlnet.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -211,7 +211,7 @@ def _tokenize(self, text: str) -> list[str]:\n         pieces = self.sp_model.encode(text, out_type=str)\n         new_pieces = []\n         for piece in pieces:\n-            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\n+            if len(piece) > 1 and piece[-1] == \",\" and piece[-2].isdigit():\n                 cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\n                 if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n                     if len(cur_pieces[0]) == 1:"
        },
        {
            "sha": "ea832692b7b98e3a0030d7a040e8ef1668d07ccf",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -332,12 +332,10 @@ def __init__(self, config: ZambaConfig, layer_idx):\n         # weight associated to the selective projection used to make dt, B and C input dependent\n         # each mamba head is processed independently\n         self.x_proj_weight = nn.Parameter(\n-            (\n-                torch.zeros(\n-                    self.n_mamba_heads,\n-                    self.time_step_rank + self.ssm_state_size * 2,\n-                    self.mamba_head_dim,\n-                )\n+            torch.zeros(\n+                self.n_mamba_heads,\n+                self.time_step_rank + self.ssm_state_size * 2,\n+                self.mamba_head_dim,\n             )\n         )\n         # time step projection (discretization)"
        },
        {
            "sha": "ecd0abcb0263e0ed399334f8206a0d5695a73a59",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -881,7 +881,7 @@ def torch_forward(self, input_states, cache_params: Optional[Zamba2HybridDynamic\n \n             # (right term of low-rank factorization of off-diagonal blocks; B terms)\n \n-            decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum))\n+            decay_states = torch.exp(A_cumsum[:, :, :, -1:] - A_cumsum)\n             B_decay_contraction = B * decay_states.permute(0, 2, 3, 1)[..., None]\n             # permute back B * decay states\n             states = (B_decay_contraction.permute(0, 1, 3, 2, 4)[..., None]  * hidden_states.permute(0, 1, 3, 2, 4)[..., None, :]).sum(dim=3).permute(0, 1, 2, 4, 3)"
        },
        {
            "sha": "a89ab2729f21679941f50c165a06dc3cbe69d3df",
            "filename": "src/transformers/models/zamba2/modular_zamba2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -660,7 +660,7 @@ def torch_forward(self, input_states, cache_params: Optional[Zamba2HybridDynamic\n \n             # (right term of low-rank factorization of off-diagonal blocks; B terms)\n \n-            decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum))\n+            decay_states = torch.exp(A_cumsum[:, :, :, -1:] - A_cumsum)\n             B_decay_contraction = B * decay_states.permute(0, 2, 3, 1)[..., None]\n             # permute back B * decay states\n             states = (B_decay_contraction.permute(0, 1, 3, 2, 4)[..., None]  * hidden_states.permute(0, 1, 3, 2, 4)[..., None, :]).sum(dim=3).permute(0, 1, 2, 4, 3)"
        },
        {
            "sha": "760f092f0e2b46813d100e46e099290a5affeccd",
            "filename": "src/transformers/pipelines/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fpipelines%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fpipelines%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2F__init__.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -16,7 +16,7 @@\n import os\n import warnings\n from pathlib import Path\n-from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Union\n+from typing import TYPE_CHECKING, Any, Optional, Union\n \n from huggingface_hub import model_info\n "
        },
        {
            "sha": "c23f1c544cb7479b3f3e557880d12139289e34a2",
            "filename": "src/transformers/pipelines/zero_shot_classification.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fpipelines%2Fzero_shot_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Fpipelines%2Fzero_shot_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fzero_shot_classification.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -27,10 +27,8 @@ def __call__(self, sequences, labels, hypothesis_template):\n             raise ValueError(\"You must include at least one label and at least one sequence.\")\n         if hypothesis_template.format(labels[0]) == hypothesis_template:\n             raise ValueError(\n-                (\n-                    'The provided hypothesis_template \"{}\" was not able to be formatted with the target labels. '\n-                    \"Make sure the passed template includes formatting syntax such as {{}} where the label should go.\"\n-                ).format(hypothesis_template)\n+                f'The provided hypothesis_template \"{hypothesis_template}\" was not able to be formatted with the target labels. '\n+                \"Make sure the passed template includes formatting syntax such as {} where the label should go.\"\n             )\n \n         if isinstance(sequences, str):"
        },
        {
            "sha": "b3a2933a2edb04e40108fb59be03f13a79248b34",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -31,10 +31,10 @@\n import tempfile\n import time\n import warnings\n-from collections.abc import Mapping\n+from collections.abc import Iterator, Mapping\n from functools import partial\n from pathlib import Path\n-from typing import TYPE_CHECKING, Any, Callable, Iterator, Optional, Union\n+from typing import TYPE_CHECKING, Any, Callable, Optional, Union\n \n \n # Integrations must be imported before ML frameworks:"
        },
        {
            "sha": "cdf39d8a5f94caeb2a196534ca339fddac19ad11",
            "filename": "src/transformers/utils/args_doc.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Futils%2Fargs_doc.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Futils%2Fargs_doc.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fargs_doc.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -861,7 +861,7 @@ def format_args_docstring(args, model_name):\n     deducted from the model name and the auto modules.\n     \"\"\"\n     # first check if there are any placeholders in the args, if not return them as is\n-    placeholders = set(re.findall(r\"{(.*?)}\", \"\".join((args[arg][\"description\"] for arg in args))))\n+    placeholders = set(re.findall(r\"{(.*?)}\", \"\".join(args[arg][\"description\"] for arg in args)))\n     if not placeholders:\n         return args\n "
        },
        {
            "sha": "6ae1b36d0bb14c53d2d3ccd5d74fef5119586c48",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -590,7 +590,7 @@ def is_torch_bf16_available():\n     return is_torch_bf16_gpu_available()\n \n \n-@lru_cache()\n+@lru_cache\n def is_torch_fp16_available_on_device(device):\n     if not is_torch_available():\n         return False\n@@ -622,7 +622,7 @@ def is_torch_fp16_available_on_device(device):\n     return True\n \n \n-@lru_cache()\n+@lru_cache\n def is_torch_bf16_available_on_device(device):\n     if not is_torch_available():\n         return False\n@@ -731,14 +731,14 @@ def is_torch_xla_available(check_is_tpu=False, check_is_gpu=False):\n     return True\n \n \n-@lru_cache()\n+@lru_cache\n def is_torch_neuroncore_available(check_device=True):\n     if importlib.util.find_spec(\"torch_neuronx\") is not None:\n         return is_torch_xla_available()\n     return False\n \n \n-@lru_cache()\n+@lru_cache\n def is_torch_npu_available(check_device=False):\n     \"Checks if `torch_npu` is installed and potentially if a NPU is in the environment\"\n     if not _torch_available or importlib.util.find_spec(\"torch_npu\") is None:\n@@ -757,7 +757,7 @@ def is_torch_npu_available(check_device=False):\n     return hasattr(torch, \"npu\") and torch.npu.is_available()\n \n \n-@lru_cache()\n+@lru_cache\n def is_torch_mlu_available(check_device=False):\n     \"\"\"\n     Checks if `mlu` is available via an `cndev-based` check which won't trigger the drivers and leave mlu\n@@ -782,7 +782,7 @@ def is_torch_mlu_available(check_device=False):\n     return available\n \n \n-@lru_cache()\n+@lru_cache\n def is_torch_musa_available(check_device=False):\n     \"Checks if `torch_musa` is installed and potentially if a MUSA is in the environment\"\n     if not _torch_available or importlib.util.find_spec(\"torch_musa\") is None:\n@@ -1020,7 +1020,7 @@ def is_torch_xpu_available(check_device=False):\n     return hasattr(torch, \"xpu\") and torch.xpu.is_available()\n \n \n-@lru_cache()\n+@lru_cache\n def is_bitsandbytes_available(check_library_only=False) -> bool:\n     if not _bitsandbytes_available:\n         return False\n@@ -1075,23 +1075,23 @@ def is_flash_attn_2_available():\n         return False\n \n \n-@lru_cache()\n+@lru_cache\n def is_flash_attn_greater_or_equal_2_10():\n     if not _is_package_available(\"flash_attn\"):\n         return False\n \n     return version.parse(importlib.metadata.version(\"flash_attn\")) >= version.parse(\"2.1.0\")\n \n \n-@lru_cache()\n+@lru_cache\n def is_flash_attn_greater_or_equal(library_version: str):\n     if not _is_package_available(\"flash_attn\"):\n         return False\n \n     return version.parse(importlib.metadata.version(\"flash_attn\")) >= version.parse(library_version)\n \n \n-@lru_cache()\n+@lru_cache\n def is_torch_greater_or_equal(library_version: str, accept_dev: bool = False):\n     \"\"\"\n     Accepts a library version and returns True if the current version of the library is greater than or equal to the\n@@ -1109,7 +1109,7 @@ def is_torch_greater_or_equal(library_version: str, accept_dev: bool = False):\n         return version.parse(importlib.metadata.version(\"torch\")) >= version.parse(library_version)\n \n \n-@lru_cache()\n+@lru_cache\n def is_huggingface_hub_greater_or_equal(library_version: str, accept_dev: bool = False):\n     if not _is_package_available(\"huggingface_hub\"):\n         return False\n@@ -2160,7 +2160,7 @@ def from_string(version_string: str) -> \"VersionComparison\":\n         return string_to_operator[version_string]\n \n \n-@lru_cache()\n+@lru_cache\n def split_package_version(package_version_str) -> tuple[str, str, str]:\n     pattern = r\"([a-zA-Z0-9_-]+)([!<>=~]+)([0-9.]+)\"\n     match = re.match(pattern, package_version_str)\n@@ -2275,7 +2275,7 @@ def fetch__all__(file_content):\n         return _all\n \n \n-@lru_cache()\n+@lru_cache\n def create_import_structure_from_path(module_path):\n     \"\"\"\n     This method takes the path to a file/a folder and returns the import structure.\n@@ -2603,7 +2603,7 @@ def flatten_dict(_dict, previous_key=None):\n     return flattened_import_structure\n \n \n-@lru_cache()\n+@lru_cache\n def define_import_structure(module_path: str, prefix: Optional[str] = None) -> IMPORT_STRUCTURE_T:\n     \"\"\"\n     This method takes a module_path as input and creates an import structure digestible by a _LazyModule."
        },
        {
            "sha": "8f063575fd7af2ae65d3d55b054b856ca8b77fe8",
            "filename": "src/transformers/utils/sentencepiece_model_pb2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Futils%2Fsentencepiece_model_pb2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/src%2Ftransformers%2Futils%2Fsentencepiece_model_pb2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fsentencepiece_model_pb2.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -342,7 +342,7 @@\n             cpp_type=6,\n             label=1,\n             has_default_value=True,\n-            default_value=float(0.9995),\n+            default_value=0.9995,\n             message_type=None,\n             enum_type=None,\n             containing_type=None,\n@@ -456,7 +456,7 @@\n             cpp_type=6,\n             label=1,\n             has_default_value=True,\n-            default_value=float(0.75),\n+            default_value=0.75,\n             message_type=None,\n             enum_type=None,\n             containing_type=None,"
        },
        {
            "sha": "74f81cfe3624e3adf1d8cacf5e5baf50d68066ce",
            "filename": "tests/utils/test_video_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/tests%2Futils%2Ftest_video_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/tests%2Futils%2Ftest_video_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_video_utils.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -45,7 +45,7 @@\n \n def get_random_video(height, width, num_frames=8, return_torch=False):\n     random_frame = np.random.randint(0, 256, (height, width, 3), dtype=np.uint8)\n-    video = np.array(([random_frame] * num_frames))\n+    video = np.array([random_frame] * num_frames)\n     if return_torch:\n         # move channel first\n         return torch.from_numpy(video).permute(0, 3, 1, 2)"
        },
        {
            "sha": "d53d1fc9ca1eaa2a133fe48614362e29fb0807af",
            "filename": "utils/check_copies.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fc67a25c6d6080467fae4f35efcefe5f13b7409/utils%2Fcheck_copies.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fc67a25c6d6080467fae4f35efcefe5f13b7409/utils%2Fcheck_copies.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_copies.py?ref=1fc67a25c6d6080467fae4f35efcefe5f13b7409",
            "patch": "@@ -1023,7 +1023,7 @@ def _rep(match):\n \n     sorted_index = sorted(localized_model_index.items(), key=lambda x: x[0].lower())\n \n-    return readmes_match, \"\\n\".join((x[1] for x in sorted_index)) + \"\\n\"\n+    return readmes_match, \"\\n\".join(x[1] for x in sorted_index) + \"\\n\"\n \n \n # Map a model name with the name it has in the README for the check_readme check"
        }
    ],
    "stats": {
        "total": 628,
        "additions": 273,
        "deletions": 355
    }
}