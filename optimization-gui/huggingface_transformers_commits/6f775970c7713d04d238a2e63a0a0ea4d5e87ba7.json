{
    "author": "HiDolen",
    "message": "avoid errors when the size of `input_ids` passed to `PrefixConstrainedLogitsProcessor` is zero (#36489)\n\n* avoid errors when the size of `input_ids` passed to PrefixConstrainedLogitsProcessor is zero\n\n* use more reasonable process\n\n* avoid early return\n\n---------\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",
    "sha": "6f775970c7713d04d238a2e63a0a0ea4d5e87ba7",
    "files": [
        {
            "sha": "16e69538b621e00371d3ce0c4c6f022c2447ff2f",
            "filename": "src/transformers/generation/logits_process.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f775970c7713d04d238a2e63a0a0ea4d5e87ba7/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f775970c7713d04d238a2e63a0a0ea4d5e87ba7/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Flogits_process.py?ref=6f775970c7713d04d238a2e63a0a0ea4d5e87ba7",
            "patch": "@@ -1353,8 +1353,11 @@ def __init__(self, prefix_allowed_tokens_fn: Callable[[int, torch.Tensor], List[\n     @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n     def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n         mask = torch.full_like(scores, -math.inf)\n-        for batch_id, beam_sent in enumerate(input_ids.view(-1, self._num_beams, input_ids.shape[-1])):\n-            for beam_id, sent in enumerate(beam_sent):\n+        batch_size = input_ids.shape[0] // self._num_beams\n+\n+        for batch_id in range(batch_size):\n+            for beam_id in range(self._num_beams):\n+                sent = input_ids[batch_id * self._num_beams + beam_id]\n                 prefix_allowed_tokens = self._prefix_allowed_tokens_fn(batch_id, sent)\n                 if len(prefix_allowed_tokens) == 0:\n                     raise ValueError("
        }
    ],
    "stats": {
        "total": 7,
        "additions": 5,
        "deletions": 2
    }
}