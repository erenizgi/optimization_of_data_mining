{
    "author": "Cyrilvallez",
    "message": "[modular] Simplify logic and docstring handling (#39185)\n\n* simplify a lot\n\n* Update modular_model_converter.py\n\n* finalize\n\n* remove outdated functions\n\n* apply it\n\n* and examples",
    "sha": "056fa73fae97f0db277939d89859139566dc4f81",
    "files": [
        {
            "sha": "ff359fa416b3456c0fd204b8ecce530adc463dbb",
            "filename": "examples/modular-transformers/configuration_my_new_model.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/056fa73fae97f0db277939d89859139566dc4f81/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/056fa73fae97f0db277939d89859139566dc4f81/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py?ref=056fa73fae97f0db277939d89859139566dc4f81",
            "patch": "@@ -125,8 +125,6 @@ class MyNewModelConfig(PretrainedConfig):\n     >>> # Accessing the model configuration\n     >>> configuration = model.config\n     ```\n-        new_param (`int`, *optional*, defaults to `False`):\n-            A fun new parameter\n     \"\"\"\n \n     model_type = \"my_new_model\""
        },
        {
            "sha": "c116b55d4d579d9af749c2eee4e20799104008b0",
            "filename": "examples/modular-transformers/modeling_new_task_model.py",
            "status": "modified",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/056fa73fae97f0db277939d89859139566dc4f81/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/056fa73fae97f0db277939d89859139566dc4f81/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py?ref=056fa73fae97f0db277939d89859139566dc4f81",
            "patch": "@@ -437,32 +437,6 @@ def forward(\n         num_logits_to_keep: int = 0,\n     ) -> Union[tuple, NewTaskModelCausalLMOutputWithPast]:\n         r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-            config.text_config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.text_config.vocab_size]`.\n-\n-        Example:\n-\n-        ```python\n-        >>> from PIL import Image\n-        >>> import requests\n-        >>> from transformers import AutoProcessor, NewTaskModelForNewTask\n-\n-        >>> model = NewTaskModelForNewTask.from_pretrained(\"google/new_task_model2-3b-mix-224\")\n-        >>> processor = AutoProcessor.from_pretrained(\"google/new_task_model2-3b-mix-224\")\n-\n-        >>> prompt = \"Where is the cat standing?\"\n-        >>> url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n-\n-        >>> inputs = processor(images=image, text=prompt,  return_tensors=\"pt\")\n-\n-        >>> # Generate\n-        >>> generate_ids = model.generate(**inputs,)\n-        >>> processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n-        \"Where is the cat standing?\\nsnow\"\n-        ```\n         Returns:\n         \"\"\"\n         vlm_outputs = super().forward("
        },
        {
            "sha": "58b74cd7eb13a8a62c9b1ff247cf5caf30b1f720",
            "filename": "examples/modular-transformers/modular_my_new_model.py",
            "status": "modified",
            "additions": 114,
            "deletions": 3,
            "changes": 117,
            "blob_url": "https://github.com/huggingface/transformers/blob/056fa73fae97f0db277939d89859139566dc4f81/examples%2Fmodular-transformers%2Fmodular_my_new_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/056fa73fae97f0db277939d89859139566dc4f81/examples%2Fmodular-transformers%2Fmodular_my_new_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_my_new_model.py?ref=056fa73fae97f0db277939d89859139566dc4f81",
            "patch": "@@ -2,11 +2,122 @@\n \n \n # Example where we only want to only add a new config argument and new arg doc\n-# here there is no `ARG` so we are gonna take parent doc\n class MyNewModelConfig(LlamaConfig):\n     r\"\"\"\n-    new_param (`int`, *optional*, defaults to `False`):\n-        A fun new parameter\n+    This is the configuration class to store the configuration of a [`MyNewModelModel`]. It is used to instantiate an MyNewModel\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to that of the MyNewModel-7B.\n+    e.g. [meta-my_new_model/MyNewModel-2-7b-hf](https://huggingface.co/meta-my_new_model/MyNewModel-2-7b-hf)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 32000):\n+            Vocabulary size of the MyNewModel model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`MyNewModelModel`]\n+        hidden_size (`int`, *optional*, defaults to 4096):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 11008):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 32):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details, check out [this\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to\n+            `num_attention_heads`.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 2048):\n+            The maximum sequence length that this model might ever be used with. MyNewModel 1 supports up to 2048 tokens,\n+            MyNewModel 2 up to 4096, CodeLlama up to 16384.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        pad_token_id (`int`, *optional*):\n+            Padding token id.\n+        bos_token_id (`int`, *optional*, defaults to 1):\n+            Beginning of stream token id.\n+        eos_token_id (`int`, *optional*, defaults to 2):\n+            End of stream token id.\n+        pretraining_tp (`int`, *optional*, defaults to 1):\n+            Experimental feature. Tensor parallelism rank used during pretraining. Please refer to [this\n+            document](https://huggingface.co/docs/transformers/main/perf_train_gpu_many#tensor-parallelism) to\n+            understand more about it. This value is necessary to ensure exact reproducibility of the pretraining\n+            results. Please refer to [this issue](https://github.com/pytorch/pytorch/issues/76232).\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether to tie weight embeddings\n+        rope_theta (`float`, *optional*, defaults to 10000.0):\n+            The base period of the RoPE embeddings.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'my_new_model3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'my_new_model3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`list[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`list[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'my_new_model3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'my_new_model3'. Scaling factor applied to high frequency components of the RoPE\n+        attention_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        mlp_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use a bias in up_proj, down_proj and gate_proj layers in the MLP layers.\n+        head_dim (`int`, *optional*):\n+            The attention head dimension. If None, it will default to hidden_size // num_attention_heads\n+\n+    ```python\n+    >>> from transformers import MyNewModelModel, MyNewModelConfig\n+\n+    >>> # Initializing a MyNewModel my_new_model-7b style configuration\n+    >>> configuration = MyNewModelConfig()\n+\n+    >>> # Initializing a model from the my_new_model-7b style configuration\n+    >>> model = MyNewModelModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\n     \"\"\"\n \n     def __init__(self, mlp_bias=True, new_param=0, **super_kwargs):"
        },
        {
            "sha": "acadb5287252424987074d6cf7cd4a01481e7280",
            "filename": "src/transformers/models/d_fine/modeling_d_fine.py",
            "status": "modified",
            "additions": 3,
            "deletions": 16,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py?ref=056fa73fae97f0db277939d89859139566dc4f81",
            "patch": "@@ -1674,21 +1674,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], DFineObjectDetectionOutput]:\n-        r\"\"\"\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing the flattened feature map (output of the backbone + projection layer), you\n-            can choose to directly pass a flattened representation of an image.\n-        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n-            Optionally, instead of initializing the queries with a tensor of zeros, you can choose to directly pass an\n-            embedded representation.\n-        labels (`list[Dict]` of len `(batch_size,)`, *optional*):\n-            Labels for computing the bipartite matching loss. List of dicts, each dictionary containing at least the\n-            following 2 keys: 'class_labels' and 'boxes' (the class labels and bounding boxes of an image in the batch\n-            respectively). The class labels themselves should be a `torch.LongTensor` of len `(number of bounding boxes\n-            in the image,)` and the boxes a `torch.FloatTensor` of shape `(number of bounding boxes in the image, 4)`.\n-\n-        Examples:\n-\n+        \"\"\"\n         ```python\n         >>> import torch\n         >>> from transformers.image_utils import load_image\n@@ -1729,7 +1715,8 @@ def forward(\n         Detected cat with confidence 0.956 at location [11.71, 53.52, 316.64, 472.33]\n         Detected remote with confidence 0.947 at location [40.46, 73.7, 175.62, 117.57]\n         Detected sofa with confidence 0.918 at location [0.59, 1.88, 640.25, 474.74]\n-        ```\"\"\"\n+        ```\n+        \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states"
        },
        {
            "sha": "4c36cc0f85d40207ae417f2f24b276002933dca6",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=056fa73fae97f0db277939d89859139566dc4f81",
            "patch": "@@ -729,11 +729,6 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n         Example:\n \n         ```python"
        },
        {
            "sha": "eaa2fd313f63883a1b6234c89b208bbd3bf15b39",
            "filename": "src/transformers/models/dinov2_with_registers/modeling_dinov2_with_registers.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py?ref=056fa73fae97f0db277939d89859139566dc4f81",
            "patch": "@@ -746,7 +746,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> BackboneOutput:\n-        r\"\"\"\n+        \"\"\"\n         Examples:\n \n         ```python"
        },
        {
            "sha": "24a26f377712013a21f430a8cf86cba24d9a30de",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=056fa73fae97f0db277939d89859139566dc4f81",
            "patch": "@@ -1292,11 +1292,6 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n         Example:\n \n         ```python"
        },
        {
            "sha": "d156dcc94ee9f97d84c39d4b4dc381362af9cbc9",
            "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py?ref=056fa73fae97f0db277939d89859139566dc4f81",
            "patch": "@@ -1530,11 +1530,6 @@ def forward(\n         **kwargs,\n     ) -> Union[tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n         Example:\n \n         ```python"
        },
        {
            "sha": "85d20faf527f89ce112bee51616f83352ab5f928",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=056fa73fae97f0db277939d89859139566dc4f81",
            "patch": "@@ -479,11 +479,6 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n         Example:\n \n         ```python"
        },
        {
            "sha": "25bdfb7ab8aa4781c77ee953f40d5cd99d20df2c",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=056fa73fae97f0db277939d89859139566dc4f81",
            "patch": "@@ -553,11 +553,6 @@ def forward(\n         **kwargs,\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n         Example:\n \n         ```python"
        },
        {
            "sha": "38b108bc87f3268c961fb7559196d1c07f358d8a",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=056fa73fae97f0db277939d89859139566dc4f81",
            "patch": "@@ -658,11 +658,6 @@ def forward(\n         **kwargs,\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n         Example:\n \n         ```python"
        },
        {
            "sha": "1c98bc5000cff5d489accffaa96639222fa2d9ca",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=056fa73fae97f0db277939d89859139566dc4f81",
            "patch": "@@ -1830,11 +1830,6 @@ def forward(\n         **kwargs,\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n         Example:\n \n         ```python"
        },
        {
            "sha": "534f744fc3149217cc7239c9748b249b06fa60a8",
            "filename": "src/transformers/models/glm4v/processing_glm4v.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Fglm4v%2Fprocessing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Fglm4v%2Fprocessing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fprocessing_glm4v.py?ref=056fa73fae97f0db277939d89859139566dc4f81",
            "patch": "@@ -39,13 +39,13 @@ class Glm4vImagesKwargs(ImagesKwargs):\n \n \n class Glm4vProcessorKwargs(ProcessingKwargs, total=False):\n-    images_kwargs: Glm4vImagesKwargs\n-    videos_kwargs: Glm4vVideosProcessorKwargs\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding\": False,\n         },\n     }\n+    images_kwargs: Glm4vImagesKwargs\n+    videos_kwargs: Glm4vVideosProcessorKwargs\n \n \n class Glm4vProcessor(ProcessorMixin):"
        },
        {
            "sha": "51fc1f07e90a7f6b01c839ed563492c39a0f3f4c",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=056fa73fae97f0db277939d89859139566dc4f81",
            "patch": "@@ -472,11 +472,6 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n         Example:\n \n         ```python"
        },
        {
            "sha": "30023430ec9fad3135b2814955c232f34b92c2c9",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 35,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=056fa73fae97f0db277939d89859139566dc4f81",
            "patch": "@@ -1501,41 +1501,6 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, InstructBlipVideoForConditionalGenerationModelOutput]:\n         r\"\"\"\n-        qformer_input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of input sequence tokens in the vocabulary of the Q-Former. Input tokens can optionally be provided\n-            to serve as text prompt, which the Q-Former model will encode.\n-\n-            Indices can be obtained using [`InstructBlipVideoProcessor`]. See [`InstructBlipVideoProcessor.__call__`] for\n-            details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        qformer_attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of input sequence tokens in the vocabulary of the language model. Input tokens can optionally be\n-            provided to serve as text prompt, which the language model can continue.\n-\n-            Indices can be obtained using [`InstructBlipVideoProcessor`]. See [`InstructBlipVideoProcessor.__call__`] for\n-            details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        decoder_attention_mask (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n-            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n-            be used by default.\n-\n-            Only relevant in case an encoder-decoder language model (like T5) is used.\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the language modeling loss. Indices should be in `[-100, 0, ..., config.vocab_size -\n-            1]`. All labels set to `-100` are ignored (masked), the loss is only computed for labels in `[0, ...,\n-            config.vocab_size]`\n-\n-        Examples:\n-\n         ```python\n         >>> from transformers import InstructBlipVideoProcessor, InstructBlipVideoForConditionalGeneration\n         >>> import torch"
        },
        {
            "sha": "718300161f214d5b49ff13fffd1306ffcc0bda14",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=056fa73fae97f0db277939d89859139566dc4f81",
            "patch": "@@ -901,11 +901,6 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, InternVLCausalLMOutputWithPast]:\n         r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n         Example:\n \n         ```python"
        },
        {
            "sha": "73d3bf940238e727802c60640e155ebdfef08442",
            "filename": "src/transformers/models/mlcd/modeling_mlcd.py",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py?ref=056fa73fae97f0db277939d89859139566dc4f81",
            "patch": "@@ -223,12 +223,11 @@ def apply_rotary_pos_emb_vision(\n \n \n class MLCDAttention(nn.Module):\n-    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\n-    Multi-headed attention with RoPE. Refer to papers:\n-        - Attention is all you need:\n-            https://huggingface.co/papers/1706.03762\n-        - RoFormer: Enhanced Transformer with Rotary Position Embedding:\n-            https://huggingface.co/papers/2104.09864\n+    \"\"\"Multi-headed attention with RoPE. Refer to papers:\n+    - Attention is all you need:\n+        https://huggingface.co/papers/1706.03762\n+    - RoFormer: Enhanced Transformer with Rotary Position Embedding:\n+        https://huggingface.co/papers/2104.09864\n     \"\"\"\n \n     def __init__(self, config: MLCDVisionConfig):"
        },
        {
            "sha": "7987510b88ed99a9a479734953e107f4fdb3ff8c",
            "filename": "src/transformers/models/sam_hq/configuration_sam_hq.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fconfiguration_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fconfiguration_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fconfiguration_sam_hq.py?ref=056fa73fae97f0db277939d89859139566dc4f81",
            "patch": "@@ -221,8 +221,6 @@ class SamHQMaskDecoderConfig(PretrainedConfig):\n             The dimensionality of the hidden states in the IoU head module.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n             The epsilon used by the layer normalization layers.\n-\n-\n         vit_dim (`int`, *optional*, defaults to 768):\n             Dimensionality of the Vision Transformer (ViT) used in the `SamHQMaskDecoder` module.\n     \"\"\""
        },
        {
            "sha": "67772cb6c4c9710ef26acdf48fb328f43f0c2459",
            "filename": "src/transformers/models/sam_hq/modular_sam_hq.py",
            "status": "modified",
            "additions": 31,
            "deletions": 2,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodular_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodular_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodular_sam_hq.py?ref=056fa73fae97f0db277939d89859139566dc4f81",
            "patch": "@@ -77,8 +77,37 @@ class SamHQVisionConfig(SamVisionConfig):\n \n class SamHQMaskDecoderConfig(SamMaskDecoderConfig):\n     r\"\"\"\n-    vit_dim (`int`, *optional*, defaults to 768):\n-        Dimensionality of the Vision Transformer (ViT) used in the `SamHQMaskDecoder` module.\n+    This is the configuration class to store the configuration of a [`SamHQMaskDecoder`]. It is used to instantiate a SAM_HQ\n+    mask decoder to the specified arguments, defining the model architecture. Instantiating a configuration defaults\n+    will yield a similar configuration to that of the SAM_HQ-vit-h\n+    [facebook/sam_hq-vit-huge](https://huggingface.co/facebook/sam_hq-vit-huge) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 256):\n+            Dimensionality of the hidden states.\n+        hidden_act (`str`, *optional*, defaults to `\"relu\"`):\n+            The non-linear activation function used inside the `SamHQMaskDecoder` module.\n+        mlp_dim (`int`, *optional*, defaults to 2048):\n+            Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n+        num_hidden_layers (`int`, *optional*, defaults to 2):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        attention_downsample_rate (`int`, *optional*, defaults to 2):\n+            The downsampling rate of the attention layer.\n+        num_multimask_outputs (`int`, *optional*, defaults to 3):\n+            The number of outputs from the `SamHQMaskDecoder` module. In the Segment Anything paper, this is set to 3.\n+        iou_head_depth (`int`, *optional*, defaults to 3):\n+            The number of layers in the IoU head module.\n+        iou_head_hidden_dim (`int`, *optional*, defaults to 256):\n+            The dimensionality of the hidden states in the IoU head module.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the layer normalization layers.\n+        vit_dim (`int`, *optional*, defaults to 768):\n+            Dimensionality of the Vision Transformer (ViT) used in the `SamHQMaskDecoder` module.\n     \"\"\"\n \n     def __init__("
        },
        {
            "sha": "bbe5a98154d07b012a060420e0e86c640aa9e5e9",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=056fa73fae97f0db277939d89859139566dc4f81",
            "patch": "@@ -874,16 +874,6 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, SmolVLMCausalLMOutputWithPast]:\n         r\"\"\"\n-        pixel_attention_mask (`torch.Tensor` of shape `(batch_size, image_size, image_size)`, *optional*):\n-            Mask to avoid performing attention on padding pixel indices.\n-        image_hidden_states (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n-            The hidden states of the image encoder after modality projection.\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-            config.vocab_size]` or `model.image_token_id` (where `model` is your instance of `SmolVLMForConditionalGeneration`).\n-            Tokens with indices set to `model.image_token_id` are ignored (masked), the loss is only\n-            computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n         Example:\n \n         ```python"
        },
        {
            "sha": "ff51537d81e1ef484628b04836101dbdaa6951a3",
            "filename": "src/transformers/models/t5gemma/configuration_t5gemma.py",
            "status": "modified",
            "additions": 73,
            "deletions": 74,
            "changes": 147,
            "blob_url": "https://github.com/huggingface/transformers/blob/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fconfiguration_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fconfiguration_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fconfiguration_t5gemma.py?ref=056fa73fae97f0db277939d89859139566dc4f81",
            "patch": "@@ -26,81 +26,80 @@\n \n class T5GemmaModuleConfig(PretrainedConfig):\n     r\"\"\"\n-        This is the configuration class to store the configuration of a [`T5GemmaModuleModel`]. It is used to instantiate an T5GemmaModule\n-        model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n-        defaults will yield a similar configuration to that of the T5GemmaModule-7B.\n-        e.g. [google/t5_gemma_module-7b](https://huggingface.co/google/t5_gemma_module-7b)\n-        Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-        documentation from [`PretrainedConfig`] for more information.\n-        Args:\n-            vocab_size (`int`, *optional*, defaults to 256000):\n-                Vocabulary size of the T5GemmaModule model. Defines the number of different tokens that can be represented by the\n-                `inputs_ids` passed when calling [`T5GemmaModuleModel`]\n-            hidden_size (`int`, *optional*, defaults to 2304):\n-                Dimension of the hidden representations.\n-            intermediate_size (`int`, *optional*, defaults to 9216):\n-                Dimension of the MLP representations.\n-            num_hidden_layers (`int`, *optional*, defaults to 26):\n-                Number of hidden layers in the Transformer decoder.\n-            num_attention_heads (`int`, *optional*, defaults to 8):\n-                Number of attention heads for each attention layer in the Transformer decoder.\n-            num_key_value_heads (`int`, *optional*, defaults to 4):\n-                This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n-                `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n-                `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n-                converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-                by meanpooling all the original heads within that group. For more details, check out [this\n-                paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to\n-                `num_attention_heads`.\n-            head_dim (`int`, *optional*, defaults to 256):\n-                The attention head dimension.\n-            hidden_activation (`str` or `function`, *optional*, defaults to `\"gelu_pytorch_tanh\"`):\n-                The non-linear activation function (function or string) in the decoder. Will default to `\"gelu_pytorch_tanh\"`\n-                if not specified. `\"gelu_pytorch_tanh\"` uses an approximation of the `\"gelu\"` activation function.\n-            max_position_embeddings (`int`, *optional*, defaults to 8192):\n-                The maximum sequence length that this model might ever be used with.\n-            initializer_range (`float`, *optional*, defaults to 0.02):\n-                The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n-            rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n-                The epsilon used by the rms normalization layers.\n-            use_cache (`bool`, *optional*, defaults to `True`):\n-                Whether or not the model should return the last key/values attentions (not used by all models). Only\n-                relevant if `config.is_decoder=True`.\n-            pad_token_id (`int`, *optional*, defaults to 0):\n-                Padding token id.\n-            eos_token_id (`int`, *optional*, defaults to 1):\n-                End of stream token id.\n-            bos_token_id (`int`, *optional*, defaults to 2):\n-                Beginning of stream token id.\n-            tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n-                Whether to tie weight embeddings\n-            rope_theta (`float`, *optional*, defaults to 10000.0):\n-                The base period of the RoPE embeddings.\n-            attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n-                Whether to use a bias in the query, key, value and output projection layers during self-attention.\n-            attention_dropout (`float`, *optional*, defaults to 0.0):\n-                The dropout ratio for the attention probabilities.\n-            query_pre_attn_scalar (`float`, *optional*, defaults to 256):\n-                scaling factor used on the attention scores\n-            sliding_window (`int`, *optional*, defaults to 4096):\n-                in T5GemmaModule, every other layer uses sliding window attention. This is the size of the sliding window.\n-            layer_types (`list`, *optional*):\n-                Attention pattern for each layer.\n-            final_logit_softcapping (`float`, *optional*, defaults to 30.0):\n-                scaling factor when applying tanh softcapping on the logits.\n-            attn_logit_softcapping (`float`, *optional*, defaults to 50.0):\n-                scaling factor when applying tanh softcapping on the attention scores.\n+    This is the configuration class to store the configuration of a [`T5GemmaModuleModel`]. It is used to instantiate an T5GemmaModule\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to that of the T5GemmaModule-7B.\n+    e.g. [google/t5_gemma_module-7b](https://huggingface.co/google/t5_gemma_module-7b)\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 256000):\n+            Vocabulary size of the T5GemmaModule model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`T5GemmaModuleModel`]\n+        hidden_size (`int`, *optional*, defaults to 2304):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 9216):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 26):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*, defaults to 4):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details, check out [this\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to\n+            `num_attention_heads`.\n+        head_dim (`int`, *optional*, defaults to 256):\n+            The attention head dimension.\n+        hidden_activation (`str` or `function`, *optional*, defaults to `\"gelu_pytorch_tanh\"`):\n+            The non-linear activation function (function or string) in the decoder. Will default to `\"gelu_pytorch_tanh\"`\n+            if not specified. `\"gelu_pytorch_tanh\"` uses an approximation of the `\"gelu\"` activation function.\n+        max_position_embeddings (`int`, *optional*, defaults to 8192):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        pad_token_id (`int`, *optional*, defaults to 0):\n+            Padding token id.\n+        eos_token_id (`int`, *optional*, defaults to 1):\n+            End of stream token id.\n+        bos_token_id (`int`, *optional*, defaults to 2):\n+            Beginning of stream token id.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n+            Whether to tie weight embeddings\n+        rope_theta (`float`, *optional*, defaults to 10000.0):\n+            The base period of the RoPE embeddings.\n+        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        query_pre_attn_scalar (`float`, *optional*, defaults to 256):\n+            scaling factor used on the attention scores\n+        sliding_window (`int`, *optional*, defaults to 4096):\n+            in T5GemmaModule, every other layer uses sliding window attention. This is the size of the sliding window.\n+        layer_types (`list`, *optional*):\n+            Attention pattern for each layer.\n+        final_logit_softcapping (`float`, *optional*, defaults to 30.0):\n+            scaling factor when applying tanh softcapping on the logits.\n+        attn_logit_softcapping (`float`, *optional*, defaults to 50.0):\n+            scaling factor when applying tanh softcapping on the attention scores.\n \n-        ```python\n-        >>> from transformers import T5GemmaModuleModel, T5GemmaModuleConfig\n-        >>> # Initializing a T5GemmaModule t5_gemma_module-7b style configuration\n-        >>> configuration = T5GemmaModuleConfig()\n-        >>> # Initializing a model from the t5_gemma_module-7b style configuration\n-        >>> model = T5GemmaModuleModel(configuration)\n-        >>> # Accessing the model configuration\n-        >>> configuration = model.config\n-        ```\n-    Module config (encoder or decoder): the same as Gemma2Config.\"\"\"\n+    ```python\n+    >>> from transformers import T5GemmaModuleModel, T5GemmaModuleConfig\n+    >>> # Initializing a T5GemmaModule t5_gemma_module-7b style configuration\n+    >>> configuration = T5GemmaModuleConfig()\n+    >>> # Initializing a model from the t5_gemma_module-7b style configuration\n+    >>> model = T5GemmaModuleModel(configuration)\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n \n     model_type = \"t5_gemma_module\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]"
        },
        {
            "sha": "56533855ee32cc385672e107ebcb0f378d4748fd",
            "filename": "src/transformers/models/t5gemma/modular_t5gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py?ref=056fa73fae97f0db277939d89859139566dc4f81",
            "patch": "@@ -68,10 +68,7 @@\n \n \n class T5GemmaModuleConfig(Gemma2Config):\n-    \"\"\"Module config (encoder or decoder): the same as Gemma2Config.\"\"\"\n-\n-    def __init__(self, **super_kwargs):\n-        super().__init__(**super_kwargs)\n+    pass\n \n \n class T5GemmaConfig(PretrainedConfig):"
        },
        {
            "sha": "c565808363b26fb072fab728cd124c75130e1b56",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/056fa73fae97f0db277939d89859139566dc4f81/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=056fa73fae97f0db277939d89859139566dc4f81",
            "patch": "@@ -319,17 +319,6 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n \n class Zamba2Attention(nn.Module):\n     \"\"\"\n-    Multi-headed attention from 'Attention Is All You Need' paper. Modified to use sliding window attention: Longformer\n-    and \"Generating Long Sequences with Sparse Transformers\".\n-\n-    Adapted from transformers.models.mistral.modeling_mistral.MistralAttention:\n-    The input dimension here is attention_hidden_size = 2 * hidden_size, and head_dim = attention_hidden_size // num_heads.\n-    The extra factor of 2 comes from the input being the concatenation of original_hidden_states with the output of the previous (mamba) layer\n-    (see fig. 2 in https://huggingface.co/papers/2405.16712).\n-    Additionally, replaced\n-    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim) with\n-    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim/2)\n-\n     Multi-headed attention from 'Attention Is All You Need' paper.\n \n     Adapted from transformers.models.mistral.modeling_mistral.MistralAttention:"
        },
        {
            "sha": "265873c604cfddf2e44e77374dd41a2427c57ce7",
            "filename": "tests/repo_utils/modular/test_conversion_order.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/056fa73fae97f0db277939d89859139566dc4f81/tests%2Frepo_utils%2Fmodular%2Ftest_conversion_order.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/056fa73fae97f0db277939d89859139566dc4f81/tests%2Frepo_utils%2Fmodular%2Ftest_conversion_order.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Frepo_utils%2Fmodular%2Ftest_conversion_order.py?ref=056fa73fae97f0db277939d89859139566dc4f81",
            "patch": "@@ -23,7 +23,6 @@\n     os.path.join(MODEL_ROOT, \"rt_detr\", \"modular_rt_detr.py\"),\n     os.path.join(MODEL_ROOT, \"qwen2\", \"modular_qwen2.py\"),\n     os.path.join(MODEL_ROOT, \"qwen3\", \"modular_qwen3.py\"),\n-    os.path.join(MODEL_ROOT, \"qwen3\", \"modular_qwen3_moe.py\"),\n     os.path.join(MODEL_ROOT, \"llava_next_video\", \"modular_llava_next_video.py\"),\n     os.path.join(MODEL_ROOT, \"cohere2\", \"modular_cohere2.py\"),\n     os.path.join(MODEL_ROOT, \"modernbert\", \"modular_modernbert.py\"),"
        },
        {
            "sha": "7585695e65895fb2670366c57e5ee02d8a411599",
            "filename": "utils/modular_model_converter.py",
            "status": "modified",
            "additions": 151,
            "deletions": 226,
            "changes": 377,
            "blob_url": "https://github.com/huggingface/transformers/blob/056fa73fae97f0db277939d89859139566dc4f81/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/056fa73fae97f0db277939d89859139566dc4f81/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=056fa73fae97f0db277939d89859139566dc4f81",
            "patch": "@@ -249,90 +249,13 @@ def leave_Call(self, original_node: cst.Call, updated_node: cst.Call) -> cst.CST\n         return updated_node\n \n \n-def get_docstring_indent(docstring):\n-    # Match the first line after the opening triple quotes\n-    match = re.search(r'(?:\"\"\"|\\'\\'\\'|```)\\n(\\s+)', docstring)\n-    if match:\n-        # Return the indentation spaces captured\n-        return len(match.group(1))\n-    return 0\n-\n-\n-def is_full_docstring(original_docstring: str, new_docstring: str, original_level: int) -> bool:\n-    \"\"\"Check if `new_docstring` is a full docstring, or if it is only part of a docstring that should then\n-    be merged with the existing old one.\n-    \"\"\"\n-    # libcst returns the docstrinbgs with literal `r\"\"\"` quotes in front\n-    new_docstring = new_docstring.split('\"\"\"', 1)[1]\n-    # The docstring contains Args definition, so it is self-contained\n-    if re.search(r\"\\n\\s*Args:\\n\", new_docstring):\n-        return True\n-    elif re.search(r\"\\n\\s*Args:\\n\", original_docstring):\n-        return False\n-    # Check if the docstring contains args docstring (meaning it is self contained):\n-    param_pattern = re.compile(\n-        # |--- Group 1 ---|| Group 2 ||- Group 3 -||---------- Group 4 ----------|\n-        rf\"^\\s{{0,{original_level}}}(\\w+)\\s*\\(\\s*([^, \\)]*)(\\s*.*?)\\s*\\)\\s*:\\s*((?:(?!\\n^\\s{{0,{original_level}}}\\w+\\s*\\().)*)\",\n-        re.DOTALL | re.MULTILINE,\n-    )\n-    match_object = param_pattern.search(new_docstring)\n-    if match_object is not None:\n-        return True\n-    # If it contains Returns, but starts with text indented with an additional 4 spaces before, it is self-contained\n-    # (this is the scenario when using `@add_start_docstrings_to_model_forward`, but adding more args to docstring)\n-    match_object = re.search(r\"\\n([^\\S\\n]*)Returns:\\n\", new_docstring)\n-    if match_object is not None:\n-        full_indent = match_object.group(1)\n-        striped_doc = new_docstring.strip(\"\\n\")\n-        if striped_doc.startswith(full_indent + \" \" * 4) or striped_doc.startswith(full_indent + \"\\t\"):\n-            return True\n-    return False\n-\n-\n-def merge_docstrings(original_docstring, updated_docstring):\n-    original_level = get_docstring_indent(original_docstring)\n-    if not is_full_docstring(original_docstring, updated_docstring, original_level):\n-        # Split the docstring at the example section, assuming `\"\"\"` is used to define the docstring\n-        parts = original_docstring.split(\"```\")\n-        if \"```\" in updated_docstring and len(parts) > 1:\n-            updated_docstring = updated_docstring.lstrip('r\"')\n-            new_parts = updated_docstring.split(\"```\")\n-            if len(new_parts) != 3:\n-                raise ValueError(\"There should only be one example, and it should have opening and closing '```'\")\n-            parts[1] = new_parts[1]\n-            updated_docstring = \"\".join(\n-                [\n-                    f\"\\n{original_level * ' '}```\",\n-                    parts[1],\n-                    \"```\",\n-                    parts[2],\n-                ]\n-            )\n-            docstring_opening, original_start_docstring = parts[0].rstrip(\" \\n\").split('\"\"\"')[:2]\n-            new_start_docstring = new_parts[0].rstrip(\" \\n\")\n-            docstring_opening += '\"\"\"'\n-            if new_start_docstring.startswith(original_start_docstring):\n-                updated_docstring = new_start_docstring + \"\\n\" + updated_docstring\n-            elif original_start_docstring.endswith(new_start_docstring):\n-                updated_docstring = original_start_docstring + \"\\n\" + updated_docstring\n-            else:\n-                updated_docstring = original_start_docstring + \"\\n\" + new_start_docstring + \"\\n\" + updated_docstring\n-            updated_docstring = docstring_opening + updated_docstring\n-        elif updated_docstring not in original_docstring:\n-            # add tabulation if we are at the lowest level.\n-            if re.search(r\"\\n\\s*.*\\(.*\\)\\:\\n\\s*\\w\", updated_docstring):\n-                updated_docstring = updated_docstring.replace(\"\\n    \", \"\\n        \")\n-            updated_docstring = original_docstring.rstrip('\"') + \"\\n\" + updated_docstring.lstrip('r\"\\n')\n-    return updated_docstring\n-\n-\n class SuperTransformer(cst.CSTTransformer):\n     METADATA_DEPENDENCIES = (ParentNodeProvider,)\n \n-    def __init__(self, python_module: cst.Module, original_methods, updated_methods, all_bases=None):\n+    def __init__(self, python_module: cst.Module, original_modeling_methods, modular_methods, all_bases=None):\n         self.python_module = python_module\n-        self.original_methods = original_methods\n-        self.updated_methods = updated_methods\n+        self.original_modeling_methods = original_modeling_methods\n+        self.modular_methods = modular_methods\n         self.all_assign_target = {}\n         self.deleted_targets = {}  # child node can delete some arguments\n         self.all_bases = all_bases or []\n@@ -414,53 +337,39 @@ def _fix_init_location(self, new_body):\n                 break\n         return new_body\n \n-    def replace_super_calls(self, node: cst.IndentedBlock, func_name: str) -> cst.CSTNode:\n+    def replace_super_calls(self, node: cst.BaseSuite, func_name: str) -> cst.BaseSuite:\n         \"\"\"Updates the body of the input `node`'s `func_name` function by replacing calls\n         to super().func_name() with the source code of the parent class' `func_name`.\n         It keeps everything that is defined before `super().func_name()`.\n         \"\"\"\n-        self.has_docstring = False\n-        parent_has_docstring = False\n-        if func_name in self.original_methods:\n-            parent_has_docstring = m.matches(self.original_methods[func_name].body.body[0], DOCSTRING_NODE)\n         new_body = []\n-        has_super_call = False\n+        modular_node_body = node.body\n \n-        for i, expr in enumerate(node.body):\n+        for i, expr in enumerate(modular_node_body):\n             if is_call_to_super(expr, func_name):\n-                has_super_call = True\n-                new_body.extend(self.update_body(self.original_methods[func_name].body.body, node.body[i + 1 :]))\n+                original_modeling_method_body = self.original_modeling_methods[func_name].body.body\n+                new_body.extend(self.update_body(original_modeling_method_body, modular_node_body[i + 1 :]))\n                 new_body = self._fix_init_location(new_body)\n+                return node.with_changes(body=new_body)\n             else:\n                 expr = expr.visit(self.transformer)\n-            if m.matches(expr, DOCSTRING_NODE):\n-                self.has_docstring = True\n-                if parent_has_docstring:  # actually here we ought to de-duplicate?\n-                    original_docstring = self.original_methods[func_name].body.body[0].body[0].value.value\n-                    updated_docstring = expr.body[0].value.value\n-                    merged_doc = merge_docstrings(original_docstring, updated_docstring)\n-                    new_node = [expr.with_changes(body=[cst.Expr(value=cst.SimpleString(value=merged_doc))])]\n-                else:\n-                    new_node = [expr]\n-                new_body.extend(new_node)\n-            elif not m.matches(expr, m.SimpleStatementLine(body=[m.Del()])) and not has_super_call:\n+            if not m.matches(expr, m.SimpleStatementLine(body=[m.Del()])):\n                 new_body.append(expr)\n-        if not self.has_docstring and parent_has_docstring:\n-            new_body = [self.original_methods[func_name].body.body[0]] + new_body\n+\n         return node.with_changes(body=new_body)\n \n-    def leave_FunctionDef(self, original_node: cst.Call, updated_node: cst.Call) -> cst.CSTNode:\n-        if updated_node.name.value in self.updated_methods:\n-            name = updated_node.name.value\n+    def leave_FunctionDef(self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef) -> cst.FunctionDef:\n+        name = updated_node.name.value\n+        if name in self.modular_methods:\n             new_body = self.replace_super_calls(updated_node.body, name)\n             return updated_node.with_changes(body=new_body, params=updated_node.params)\n         return updated_node\n \n-    def leave_Return(self, original_node: cst.Return, updated_node: cst.Return) -> cst.CSTNode:\n+    def leave_Return(self, original_node: cst.Return, updated_node: cst.Return) -> cst.Return:\n         \"\"\" \"When a return statement is reached, it is replaced with the unrolled super code\"\"\"\n         if m.matches(updated_node.value, m.Call(func=m.Attribute(attr=m.Name(\"super\")))):\n             func_def = self.get_metadata(ParentNodeProvider, original_node)\n-            if m.matched(func_def, m.FunctionDef()) and func_def.name.value in self.original_methods:\n+            if m.matched(func_def, m.FunctionDef()) and func_def.name.value in self.original_modeling_methods:\n                 updated_return_value = updated_node.value.with_changes(\n                     args=[\n                         cst.Arg(\n@@ -979,55 +888,52 @@ def common_partial_suffix(str1: str, str2: str) -> str:\n \n \n def replace_class_node(\n-    mapper: ModelFileMapper, class_node: cst.ClassDef, renamed_super_class: str, original_super_class: str\n-):\n+    mapper: ModelFileMapper, modular_class_node: cst.ClassDef, renamed_super_class: str, original_super_class: str\n+) -> cst.ClassDef:\n     \"\"\"\n     Replace a class node which inherits from another modeling class. This function works in the following way:\n-    - start from the base class node of the inherited class (a cst.Node)\n-    - replace all methods of the base node with the methods defined in the child class\n-    - append all new methods defined in the child class\n+    - start from the methods and class attributes of the original modeling code node, and replace their definition\n+    if overriden in the modular\n+    - append all new methods and class attributes defined in the child class\n+    - all potential method/class docstrings and decorators use the ones found in modular if any, else in original modeling\n     - replace all calls to super() with the unravelled code\n \n-                    |    ```python                          |               |    ```python\n-                    |    class GemmaModel(LlamaModel):      |               |       class GemmaModel(nn.Module):\n-                    |        def __init__(self):            |               |           def __init__(self):\n-    Going from:     |            super().__init__()         |       to:     |               super().__init__(config)\n-                    |            self.dropout = 0.2         |               |               self.dropout = 0.2\n-                    |     ```                               |               |               self.padding_idx = config.pad_token_id\n-                                                                            |               self.vocab_size = config.vocab_size\n-                                                                            |               self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n-                                                                            |               self.layers = nn.ModuleList(\n-                                                                            |                   [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n-                                                                            |               )\n-                                                                            |               self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-                                                                            |               self.gradient_checkpointing = False\n-                                                                            |               # Initialize weights and apply final processing\n-                                                                            |               self.post_init()\n-                                                                            |     ```\n+    Args:\n+        mapper (`ModelFileMapper`):\n+            The mapper corresponding to the visited file from which the modular class node inherits.\n+        modular_class_node (`cst.ClassDef`):\n+            The class node as found in the modular file.\n+        renamed_super_class (`str`):\n+            The name of the class from which `modular_class_node` inherits after automatic renaming.\n+        original_super_class (`str`):\n+            The name of the class from which `modular_class_node` inherits before automatic renaming.\n+\n+    Returns:\n+        A new class node corresponding to the modular definition.\n     \"\"\"\n-    all_bases = [get_full_attribute_name(k.value) for k in class_node.bases]\n+    all_bases = [get_full_attribute_name(k.value) for k in modular_class_node.bases]\n     if any(base is None for base in all_bases):\n-        raise ValueError(f\"Could not parse the name of the bases for {class_node.name.value}\")\n+        raise ValueError(f\"Could not parse the name of the bases for {modular_class_node.name.value}\")\n \n-    original_node = mapper.classes[renamed_super_class]\n+    original_modeling_node = mapper.classes[renamed_super_class]\n     # Always use the new name of the class (in case we use e.g. `ColPaliForRetrieval` inheriting from `PaliGemmaForConditionalGeneration`)\n-    new_name = class_node.name\n+    new_class_name = modular_class_node.name\n \n     # If the new class name is different from the renamed super class name, we need to update the docstrings/comments accordingly\n-    if new_name.value != renamed_super_class:\n-        common_suffix = common_partial_suffix(new_name.value, renamed_super_class)\n+    if new_class_name.value != renamed_super_class:\n+        common_suffix = common_partial_suffix(new_class_name.value, renamed_super_class)\n         # Note that this works even without common prefix, in which case it does not replace anything\n-        old, new = renamed_super_class.replace(common_suffix, \"\"), new_name.value.replace(common_suffix, \"\")\n-        temp_module = cst.Module(body=[original_node])\n-        original_node = temp_module.visit(\n+        old, new = renamed_super_class.replace(common_suffix, \"\"), new_class_name.value.replace(common_suffix, \"\")\n+        temp_module = cst.Module(body=[original_modeling_node])\n+        original_modeling_node = temp_module.visit(\n             ReplaceNameTransformer(get_lowercase_name(old), get_lowercase_name(new), only_doc=True)\n         ).body[0]\n \n     # If we explicitly passed a new base with common suffix to an old base, it is for switching the prefix\n     # e.g. if the \"natural\" parent class is `PreTrainedModel` but we wanted to rename it to `PreTrainedVisionModel`\n     additional_bases = [base for base in all_bases if base != original_super_class]\n-    new_bases = []\n-    for original_base in original_node.bases:\n+    new_class_bases = []\n+    for original_base in original_modeling_node.bases:\n         new_base = original_base\n         # we only potentially switch base for Name-based bases, not Attribute\n         if m.matches(original_base.value, m.Name()):\n@@ -1038,106 +944,125 @@ def replace_class_node(\n                     new_name_node = original_base.value.with_changes(value=additional_base_name)\n                     new_base = original_base.with_changes(value=new_name_node)\n                     break\n-        new_bases.append(new_base)\n+        new_class_bases.append(new_base)\n+\n+    # Use class decorators redefined in modular file if any\n+    new_class_decorators = (\n+        modular_class_node.decorators if len(modular_class_node.decorators) > 0 else original_modeling_node.decorators\n+    )\n \n-    original_methods = {\n-        f.name.value if hasattr(f, \"name\") else mapper.python_module.code_for_node(f): f\n-        for f in original_node.body.body\n+    # Compute new class docstring\n+    original_modeling_docstring = [\n+        node for node in original_modeling_node.body.body if m.matches(node, DOCSTRING_NODE)\n+    ]\n+    modular_docstring = [node for node in modular_class_node.body.body if m.matches(node, DOCSTRING_NODE)]\n+    # Use class docstring in modular if any, else original modeling code docstring\n+    new_class_docstring = modular_docstring if len(modular_docstring) > 0 else original_modeling_docstring\n+\n+    # Compute new class attributes\n+    original_modeling_class_attributes = {\n+        node.body[0].targets[0].target.value: node\n+        for node in original_modeling_node.body.body\n+        if m.matches(node, m.SimpleStatementLine(body=[m.Assign()]))\n     }\n-    updated_methods = {\n-        f.name.value if hasattr(f, \"name\") else mapper.python_module.code_for_node(f): f for f in class_node.body.body\n+    original_modeling_class_attributes.update(\n+        {\n+            node.body[0].target.value: node\n+            for node in original_modeling_node.body.body\n+            if m.matches(node, m.SimpleStatementLine(body=[m.AnnAssign()]))\n+        }\n+    )\n+    modular_class_attributes = {\n+        node.body[0].targets[0].target.value: node\n+        for node in modular_class_node.body.body\n+        if m.matches(node, m.SimpleStatementLine(body=[m.Assign()]))\n     }\n-    end_meth = []\n-\n-    assign_targets = {}\n-    docstring_node = []\n-    # Iterate directly from node.body as there can be property/setters with same names which are overwritten when we use a dict\n-    for func in original_node.body.body:\n-        name = func.name.value if hasattr(func, \"name\") else mapper.python_module.code_for_node(func)\n-        if m.matches(func, m.FunctionDef()) and name in updated_methods and updated_methods[name] is not None:\n-            new_params = updated_methods[name].params\n-            # Replace the method in the replacement class, preserving decorators\n-            kwarg_name = getattr(updated_methods[name].params, \"star_kwarg\", None)\n-            if kwarg_name and kwarg_name.name.value == \"super_kwargs\":\n-                parent_params = {k.name.value: k for k in func.params.params}\n-                parent_params.update({k.name.value: k for k in new_params.params[1:]})\n-                new_params = new_params.with_changes(\n-                    params=list(parent_params.values()), star_kwarg=func.params.star_kwarg\n-                )\n-            # Keep decorators in `modular_xxx.py` if any, else original decorators\n-            new_decorators = (\n-                updated_methods[name].decorators if len(updated_methods[name].decorators) > 0 else func.decorators\n-            )\n+    modular_class_attributes.update(\n+        {\n+            node.body[0].target.value: node\n+            for node in modular_class_node.body.body\n+            if m.matches(node, m.SimpleStatementLine(body=[m.AnnAssign()]))\n+        }\n+    )\n+    # Use all original modeling attributes, and potentially override some with values in the modular\n+    new_class_attributes = list({**original_modeling_class_attributes, **modular_class_attributes}.values())\n \n-            # Keep return annotation in `modular_xxx.py` if any, else original return annotation\n-            new_return_annotation = updated_methods[name].returns if updated_methods[name].returns else func.returns\n-\n-            if not re.match(\n-                r\"\\ndef .*\\(.*\\):\\n    raise.*Error\\(.*\",\n-                mapper.python_module.code_for_node(updated_methods[name]),\n-            ):\n-                func = func.with_changes(\n-                    body=updated_methods[name].body,\n-                    params=new_params,\n-                    decorators=new_decorators,\n-                    returns=new_return_annotation,\n-                )\n-            else:\n+    original_modeling_methods = {\n+        node.name.value: node for node in original_modeling_node.body.body if m.matches(node, m.FunctionDef())\n+    }\n+    modular_methods = {\n+        node.name.value: node for node in modular_class_node.body.body if m.matches(node, m.FunctionDef())\n+    }\n+\n+    new_class_methods = []\n+    # Iterate over the methods of the original modeling code, and add them to the list of methods to add\n+    for name, node in original_modeling_methods.items():\n+        # If the method was redefined in modular, make appropriate changes to the node\n+        if name in modular_methods:\n+            # Get the corresponding method node in modular\n+            modular_node = modular_methods[name]\n+\n+            # If we match the pattern, we should avoid inheriting the method\n+            if re.match(r\"\\ndef .*\\(.*\\):\\n    raise.*Error\\(.*\", mapper.python_module.code_for_node(modular_node)):\n                 continue\n \n-        if m.matches(func, m.SimpleStatementLine(body=[m.Assign()])):\n-            target = mapper.python_module.code_for_node(func.body[0].targets[0])\n-            assign_targets[target] = func\n-        elif m.matches(func, m.SimpleStatementLine(body=[m.AnnAssign()])):\n-            target = mapper.python_module.code_for_node(func.body[0].target)\n-            assign_targets[target] = func\n-        elif m.matches(func, DOCSTRING_NODE):\n-            docstring_node = [func]\n-        else:\n-            end_meth.append(func)\n+            # Compute new method docstring\n+            modeling_docstring = [node_ for node_ in node.body.body if m.matches(node_, DOCSTRING_NODE)]\n+            modular_docstring = [node_ for node_ in modular_node.body.body if m.matches(node_, DOCSTRING_NODE)]\n+            # Use method docstring in modular if any, else original modeling code docstring\n+            new_body = (\n+                modular_node.body.body\n+                if len(modular_docstring) > 0\n+                else modeling_docstring + list(modular_node.body.body)\n+            )\n+            new_body = modular_node.body.with_changes(body=new_body)\n+\n+            # Use arguments as defined in the modular\n+            new_params = modular_node.params\n+\n+            # If using the `**super_kwargs` syntax in modular, merge any existing modular arg with all the original modeling ones\n+            kwarg_name = getattr(modular_node.params, \"star_kwarg\", None)\n+            if kwarg_name and kwarg_name.name.value == \"super_kwargs\":\n+                original_modeling_params = {k.name.value: k for k in node.params.params}\n+                modular_params = {k.name.value: k for k in new_params.params[1:]}\n+                new_param_list = list({**original_modeling_params, **modular_params}.values())\n+                new_params = new_params.with_changes(params=new_param_list, star_kwarg=node.params.star_kwarg)\n+\n+            # Keep decorators in modular if any, else original decorators\n+            new_decorators = modular_node.decorators if len(modular_node.decorators) > 0 else node.decorators\n+\n+            # Keep return annotation in modular if any, else original return annotation\n+            new_return_annotation = modular_node.returns if modular_node.returns else node.returns\n+\n+            # Update the method node\n+            node = node.with_changes(\n+                body=new_body,\n+                params=new_params,\n+                decorators=new_decorators,\n+                returns=new_return_annotation,\n+            )\n+\n+        new_class_methods.append(node)\n \n     # Port new methods that are defined only in modular-file and append at the end\n-    for func in class_node.body.body:\n-        name = func.name.value if hasattr(func, \"name\") else mapper.python_module.code_for_node(func)\n-        if m.matches(func, DOCSTRING_NODE):  # This processes the docstring of the class!\n-            # Extract the original docstring\n-            updated_docstring = func.body[0].value.value\n-            if len(docstring_node) == 0:  # If the original docstring is empty, just create one from the updated.\n-                docstring_node = [\n-                    cst.SimpleStatementLine(body=[cst.Expr(value=cst.SimpleString(value=updated_docstring))])\n-                ]\n-            else:\n-                original_docstring = docstring_node[0].body[0].value.value\n-                merged_doc = merge_docstrings(original_docstring, updated_docstring)\n-                # Update the docstring in the original function\n-                docstring_node = [\n-                    docstring_node[0].with_changes(body=[cst.Expr(value=cst.SimpleString(value=merged_doc))])\n-                ]\n-        if name not in original_methods and func is not None and isinstance(func, cst.FunctionDef):\n-            end_meth.append(func)\n-        if m.matches(func, m.SimpleStatementLine(body=[m.Assign()])):\n-            # TODO we only use single assign might cause issues\n-            target = mapper.python_module.code_for_node(func.body[0].targets[0])\n-            assign_targets[target] = func\n-        if m.matches(func, m.SimpleStatementLine(body=[m.AnnAssign()])):\n-            target = mapper.python_module.code_for_node(func.body[0].target)\n-            assign_targets[target] = func\n-    end_meth = docstring_node + list(assign_targets.values()) + end_meth\n-\n-    # Replace the calls to `super()` with the unrolled code\n-    result_node = original_node.with_changes(body=cst.IndentedBlock(body=end_meth))\n+    for name, node in modular_methods.items():\n+        if name not in original_modeling_methods:\n+            new_class_methods.append(node)\n+\n+    # Recreate the whole new class body\n+    new_class_body = new_class_docstring + new_class_attributes + new_class_methods\n+\n+    # Replace the calls to `super()` of the redefined modular methods with the unrolled code\n+    result_node = original_modeling_node.with_changes(body=cst.IndentedBlock(body=new_class_body))\n     temp_module = cst.Module(body=[result_node])\n     new_module = MetadataWrapper(temp_module)\n     new_replacement_class = new_module.visit(\n-        SuperTransformer(temp_module, original_methods, updated_methods, all_bases)\n+        SuperTransformer(temp_module, original_modeling_methods, modular_methods, all_bases)\n     )\n-    new_replacement_body = new_replacement_class.body[0].body  # get the indented block\n-\n-    # Use decorators redefined in `modular_xxx.py` if any\n-    new_decorators = class_node.decorators if len(class_node.decorators) > 0 else original_node.decorators\n+    new_class_body = new_replacement_class.body[0].body  # get the indented block\n \n-    return original_node.with_changes(\n-        body=new_replacement_body, decorators=new_decorators, bases=new_bases, name=new_name\n+    return original_modeling_node.with_changes(\n+        body=new_class_body, decorators=new_class_decorators, bases=new_class_bases, name=new_class_name\n     )\n \n "
        }
    ],
    "stats": {
        "total": 847,
        "additions": 381,
        "deletions": 466
    }
}