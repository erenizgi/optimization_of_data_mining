{
    "author": "zucchini-nlp",
    "message": "[qwen2-vl] fix FA2 inference (#39121)\n\n* fix FA2\n\n* update is causal flag and remove mask for FA2\n\n* update for FA2 with varlen path\n\n* how the tests were passing with different devices?\n\n* add comment and ref to the PR\n\n* move mask preparation to base pretrained model\n\n* seq len is the first dim, not second\n\n* fix copies to fix GLM4V",
    "sha": "7a25f8dfdba4c710d278d8312ef2522c5996a894",
    "files": [
        {
            "sha": "13da327dab005e2ab2e24d40e328c4573436deea",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 17,
            "deletions": 8,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/7a25f8dfdba4c710d278d8312ef2522c5996a894/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7a25f8dfdba4c710d278d8312ef2522c5996a894/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=7a25f8dfdba4c710d278d8312ef2522c5996a894",
            "patch": "@@ -508,6 +508,22 @@ def _flash_attention_forward(\n         query_states, key_states, value_states, target_dtype\n     )\n \n+    # We will use `flash_attn_varlen_func` to prevent cross-example attention and also allow padding free approach\n+    # under two cases:\n+    # Case 1. If position_ids is provided and check all examples do not contain only 1 sequence, If tensor in increasing\n+    # then we probably have one sequence, otherwise it is packed. Additionally check we are in pre-fill/training stage.\n+    # Case 2. Some models pass directly pre-computed `cu_seqlens` so we don't need to infer it from position ids. It is safe to\n+    # use `flash_attn_varlen_func` knowing we already have all necessary the kwargs. NOTE: it is user's responsibility\n+    # to take care of flattenning `position_ids` if that's needed by the model. See #39121 for more information\n+    is_fa2_with_position_ids = (\n+        position_ids is not None\n+        and query_states.shape[0] == 1\n+        and (max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all()))\n+    )\n+    is_fa2_with_varlen_kwargs = all(\n+        kwarg is not None for kwarg in (cu_seq_lens_q, cu_seq_lens_k, max_length_q, max_length_k)\n+    )\n+\n     # Contains at least one padding token in the sequence\n     if attention_mask is not None:\n         batch_size = query_states.shape[0]\n@@ -531,14 +547,7 @@ def _flash_attention_forward(\n         )\n         attn_output = _pad_input(attn_output_unpad, indices_q, batch_size, query_length)\n \n-    # If position_ids is provided and check all examples do not contain only 1 sequence, If tensor in increasing\n-    # then we probably have one sequence, otherwise it is packed. Additionally check we are in pre-fill/training stage.\n-    # Use `flash_attn_varlen_func` to prevent cross-example attention and also allow padding free approach\n-    elif (\n-        position_ids is not None\n-        and query_states.shape[0] == 1\n-        and (max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all()))\n-    ):\n+    elif is_fa2_with_varlen_kwargs or is_fa2_with_position_ids:\n         batch_size = query_states.size(0)\n \n         if cu_seq_lens_q is None or cu_seq_lens_k is None:"
        },
        {
            "sha": "5f0e737b43de0286f18f5d70477c37ae37bb8bf1",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 57,
            "deletions": 22,
            "changes": 79,
            "blob_url": "https://github.com/huggingface/transformers/blob/7a25f8dfdba4c710d278d8312ef2522c5996a894/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7a25f8dfdba4c710d278d8312ef2522c5996a894/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=7a25f8dfdba4c710d278d8312ef2522c5996a894",
            "patch": "@@ -279,14 +279,15 @@ def eager_attention_forward(\n class Glm4vVisionAttention(nn.Module):\n     def __init__(self, config: Glm4vVisionConfig) -> None:\n         super().__init__()\n-        self.config = config\n+        self.dim = config.hidden_size\n         self.num_heads = config.num_heads\n-        self.head_dim = config.hidden_size // self.num_heads\n-        self.num_key_value_groups = 1\n-        self.scale = self.head_dim**-0.5\n-        self.attention_dropout = config.attention_dropout\n+        self.head_dim = self.dim // self.num_heads\n+        self.num_key_value_groups = 1  # needed for eager attention\n         self.qkv = nn.Linear(config.hidden_size, config.hidden_size * 3, bias=config.attention_bias)\n         self.proj = nn.Linear(config.hidden_size, config.hidden_size, bias=False)\n+        self.scaling = self.head_dim**-0.5\n+        self.config = config\n+        self.attention_dropout = config.attention_dropout\n         self.is_causal = False\n \n     def forward(\n@@ -295,23 +296,31 @@ def forward(\n         cu_seqlens: torch.Tensor,\n         rotary_pos_emb: Optional[torch.Tensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> torch.Tensor:\n         seq_length = hidden_states.shape[0]\n         query_states, key_states, value_states = (\n             self.qkv(hidden_states).reshape(seq_length, 3, self.num_heads, -1).permute(1, 0, 2, 3).unbind(0)\n         )\n-\n-        cos, sin = position_embeddings\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `rotary_pos_emb` (2D tensor of RoPE theta values), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.54 `rotary_pos_emb` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n+            cos = emb.cos()\n+            sin = emb.sin()\n+        else:\n+            cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb_vision(query_states, key_states, cos, sin)\n \n         query_states = query_states.transpose(0, 1).unsqueeze(0)\n         key_states = key_states.transpose(0, 1).unsqueeze(0)\n         value_states = value_states.transpose(0, 1).unsqueeze(0)\n-\n-        attention_mask = torch.zeros([1, 1, seq_length, seq_length], device=query_states.device, dtype=torch.bool)\n-        for i in range(1, len(cu_seqlens)):\n-            attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = True\n+        max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -322,13 +331,17 @@ def forward(\n             query_states,\n             key_states,\n             value_states,\n-            attention_mask,\n+            attention_mask=attention_mask,\n             dropout=0.0 if not self.training else self.attention_dropout,\n-            scaling=self.scale,\n-            is_causal=self.is_causal,\n+            scaling=self.scaling,\n+            cu_seq_lens_q=cu_seqlens,  # pass cu seq lens for FA2\n+            cu_seq_lens_k=cu_seqlens,\n+            max_length_q=max_seqlen,\n+            max_length_k=max_seqlen,\n+            is_causal=False,\n             **kwargs,\n         )\n-        attn_output = attn_output.squeeze(0)\n+\n         attn_output = attn_output.reshape(seq_length, -1).contiguous()\n         attn_output = self.proj(attn_output)\n         return attn_output\n@@ -348,13 +361,15 @@ def forward(\n         cu_seqlens: torch.Tensor,\n         rotary_pos_emb: Optional[torch.Tensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         hidden_states = hidden_states + self.attn(\n             self.norm1(hidden_states),\n             cu_seqlens=cu_seqlens,\n             rotary_pos_emb=rotary_pos_emb,\n             position_embeddings=position_embeddings,\n+            attention_mask=attention_mask,\n             **kwargs,\n         )\n         hidden_states = hidden_states + self.mlp(self.norm2(hidden_states))\n@@ -452,6 +467,25 @@ def rot_pos_emb(self, grid_thw):\n         rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)\n         return rotary_pos_emb, pos_ids\n \n+    def _prepare_attention_mask(self, inputs_tensor: torch.Tensor, cu_seqlens: torch.Tensor) -> torch.Tensor:\n+        # Flash Attention 2 doesn't need a 4D mask and relies on `cu_seqlens/max_seqlen`\n+        # NOTE: the created attention masl only approximates the ragged FA2 attention by\n+        # allowing bidirectional attention within `cu_seqlens` blocks, and not attending between\n+        # blocks. Though it will not be a 100% match for FA2's `varlen` path\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            return None\n+\n+        seq_length = inputs_tensor.shape[0]\n+        attention_mask = torch.full(\n+            [1, 1, seq_length, seq_length],\n+            torch.finfo(inputs_tensor.dtype).min,\n+            device=inputs_tensor.device,\n+            dtype=inputs_tensor.dtype,\n+        )\n+        for i in range(1, len(cu_seqlens)):\n+            attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = 0\n+        return attention_mask\n+\n     def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.Tensor:\n         \"\"\"\n         Args:\n@@ -481,14 +515,15 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.\n         cu_seqlens = F.pad(cu_seqlens, (1, 0), value=0)\n         seqlens = (cu_seqlens[1:] - cu_seqlens[:-1]).tolist()\n         hidden_states = self.embeddings(hidden_states, seqlens, grid_thw, image_type_ids[:, 0], image_type_ids[:, 1])\n+        attention_mask = self._prepare_attention_mask(hidden_states, cu_seqlens=cu_seqlens)\n \n         for blk in self.blocks:\n-            if self.gradient_checkpointing and self.training:\n-                hidden_states = self._gradient_checkpointing_func(\n-                    blk.__call__, hidden_states, cu_seqlens, None, position_embeddings\n-                )\n-            else:\n-                hidden_states = blk(hidden_states, cu_seqlens=cu_seqlens, position_embeddings=position_embeddings)\n+            hidden_states = blk(\n+                hidden_states,\n+                cu_seqlens=cu_seqlens,\n+                position_embeddings=position_embeddings,\n+                attention_mask=attention_mask,\n+            )\n \n         hidden_states = self.post_layernorm(hidden_states)\n "
        },
        {
            "sha": "d95e457fc4eceda98bc3bcf16417accfffbd15dc",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 28,
            "deletions": 58,
            "changes": 86,
            "blob_url": "https://github.com/huggingface/transformers/blob/7a25f8dfdba4c710d278d8312ef2522c5996a894/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7a25f8dfdba4c710d278d8312ef2522c5996a894/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=7a25f8dfdba4c710d278d8312ef2522c5996a894",
            "patch": "@@ -50,8 +50,8 @@\n     Qwen2_5_VLPreTrainedModel,\n     Qwen2_5_VLRotaryEmbedding,\n     Qwen2_5_VLTextModel,\n+    Qwen2_5_VLVisionAttention,\n     Qwen2_5_VLVisionBlock,\n-    apply_rotary_pos_emb_vision,\n )\n from ..qwen2_5_vl.processing_qwen2_5_vl import (\n     Qwen2_5_VLProcessor,\n@@ -505,62 +505,12 @@ def forward(self, embeddings, lengths, image_shapes, h_coords, w_coords) -> torc\n         return embeddings\n \n \n-class Glm4vVisionAttention(nn.Module):\n+class Glm4vVisionAttention(Qwen2_5_VLVisionAttention):\n     def __init__(self, config: Glm4vVisionConfig) -> None:\n         super().__init__()\n-        self.config = config\n-        self.num_heads = config.num_heads\n-        self.head_dim = config.hidden_size // self.num_heads\n-        self.num_key_value_groups = 1\n-        self.scale = self.head_dim**-0.5\n         self.attention_dropout = config.attention_dropout\n         self.qkv = nn.Linear(config.hidden_size, config.hidden_size * 3, bias=config.attention_bias)\n         self.proj = nn.Linear(config.hidden_size, config.hidden_size, bias=False)\n-        self.is_causal = False\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        cu_seqlens: torch.Tensor,\n-        rotary_pos_emb: Optional[torch.Tensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> torch.Tensor:\n-        seq_length = hidden_states.shape[0]\n-        query_states, key_states, value_states = (\n-            self.qkv(hidden_states).reshape(seq_length, 3, self.num_heads, -1).permute(1, 0, 2, 3).unbind(0)\n-        )\n-\n-        cos, sin = position_embeddings\n-        query_states, key_states = apply_rotary_pos_emb_vision(query_states, key_states, cos, sin)\n-\n-        query_states = query_states.transpose(0, 1).unsqueeze(0)\n-        key_states = key_states.transpose(0, 1).unsqueeze(0)\n-        value_states = value_states.transpose(0, 1).unsqueeze(0)\n-\n-        attention_mask = torch.zeros([1, 1, seq_length, seq_length], device=query_states.device, dtype=torch.bool)\n-        for i in range(1, len(cu_seqlens)):\n-            attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = True\n-\n-        attention_interface: Callable = eager_attention_forward\n-        if self.config._attn_implementation != \"eager\":\n-            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n-\n-        attn_output, _ = attention_interface(\n-            self,\n-            query_states,\n-            key_states,\n-            value_states,\n-            attention_mask,\n-            dropout=0.0 if not self.training else self.attention_dropout,\n-            scaling=self.scale,\n-            is_causal=self.is_causal,\n-            **kwargs,\n-        )\n-        attn_output = attn_output.squeeze(0)\n-        attn_output = attn_output.reshape(seq_length, -1).contiguous()\n-        attn_output = self.proj(attn_output)\n-        return attn_output\n \n \n class Glm4vVisionBlock(Qwen2_5_VLVisionBlock):\n@@ -653,6 +603,25 @@ def rot_pos_emb(self, grid_thw):\n         rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)\n         return rotary_pos_emb, pos_ids\n \n+    def _prepare_attention_mask(self, inputs_tensor: torch.Tensor, cu_seqlens: torch.Tensor) -> torch.Tensor:\n+        # Flash Attention 2 doesn't need a 4D mask and relies on `cu_seqlens/max_seqlen`\n+        # NOTE: the created attention masl only approximates the ragged FA2 attention by\n+        # allowing bidirectional attention within `cu_seqlens` blocks, and not attending between\n+        # blocks. Though it will not be a 100% match for FA2's `varlen` path\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            return None\n+\n+        seq_length = inputs_tensor.shape[0]\n+        attention_mask = torch.full(\n+            [1, 1, seq_length, seq_length],\n+            torch.finfo(inputs_tensor.dtype).min,\n+            device=inputs_tensor.device,\n+            dtype=inputs_tensor.dtype,\n+        )\n+        for i in range(1, len(cu_seqlens)):\n+            attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = 0\n+        return attention_mask\n+\n     def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.Tensor:\n         \"\"\"\n         Args:\n@@ -682,14 +651,15 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.\n         cu_seqlens = F.pad(cu_seqlens, (1, 0), value=0)\n         seqlens = (cu_seqlens[1:] - cu_seqlens[:-1]).tolist()\n         hidden_states = self.embeddings(hidden_states, seqlens, grid_thw, image_type_ids[:, 0], image_type_ids[:, 1])\n+        attention_mask = self._prepare_attention_mask(hidden_states, cu_seqlens=cu_seqlens)\n \n         for blk in self.blocks:\n-            if self.gradient_checkpointing and self.training:\n-                hidden_states = self._gradient_checkpointing_func(\n-                    blk.__call__, hidden_states, cu_seqlens, None, position_embeddings\n-                )\n-            else:\n-                hidden_states = blk(hidden_states, cu_seqlens=cu_seqlens, position_embeddings=position_embeddings)\n+            hidden_states = blk(\n+                hidden_states,\n+                cu_seqlens=cu_seqlens,\n+                position_embeddings=position_embeddings,\n+                attention_mask=attention_mask,\n+            )\n \n         hidden_states = self.post_layernorm(hidden_states)\n "
        },
        {
            "sha": "f7e5f5ba4a5dded33d2734cfec5bef5ed6f45229",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 76,
            "deletions": 35,
            "changes": 111,
            "blob_url": "https://github.com/huggingface/transformers/blob/7a25f8dfdba4c710d278d8312ef2522c5996a894/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7a25f8dfdba4c710d278d8312ef2522c5996a894/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=7a25f8dfdba4c710d278d8312ef2522c5996a894",
            "patch": "@@ -607,6 +607,7 @@ def __init__(\n                 f\" and `num_heads`: {self.num_heads}).\"\n             )\n         self.scaling = self.head_dim**-0.5\n+        self.attention_dropout = 0.0\n         self.is_decoder = False\n         self.is_causal = False\n \n@@ -619,6 +620,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         cu_seqlens: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n@@ -634,15 +636,6 @@ def forward(\n         value_states = value_states.transpose(0, 1).unsqueeze(0)\n         max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n \n-        attention_mask = torch.full(\n-            [1, 1, seq_length, key_states.shape[-2]],\n-            torch.finfo(query_states.dtype).min,\n-            device=query_states.device,\n-            dtype=query_states.dtype,\n-        )\n-        for i in range(1, len(cu_seqlens)):\n-            attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = 0\n-\n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n@@ -652,13 +645,13 @@ def forward(\n             query_states,\n             key_states,\n             value_states,\n-            attention_mask,\n-            dropout=0.0 if not self.training else self.dropout,\n+            attention_mask=attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n             scaling=self.scaling,\n-            cu_seqlens_q=cu_seqlens,  # pass cu seq lens for FA2\n-            cu_seqlens_k=cu_seqlens,\n-            max_seqlen_q=max_seqlen,\n-            max_seqlen_k=max_seqlen,\n+            cu_seq_lens_q=cu_seqlens,  # pass cu seq lens for FA2\n+            cu_seq_lens_k=cu_seqlens,\n+            max_length_q=max_seqlen,\n+            max_length_k=max_seqlen,\n             is_causal=False,\n             **kwargs,\n         )\n@@ -686,6 +679,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         cu_seqlens: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         \"\"\"\n@@ -704,6 +698,7 @@ def forward(\n         hidden_states = self.self_attn(\n             hidden_states=hidden_states,\n             cu_seqlens=cu_seqlens,\n+            attention_mask=attention_mask,\n             **kwargs,\n         )\n         hidden_states = residual + hidden_states\n@@ -785,6 +780,25 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value: nn.Module):\n         self.conv1 = value\n \n+    def _prepare_attention_mask(self, inputs_tensor: torch.Tensor, cu_seqlens: torch.Tensor) -> torch.Tensor:\n+        # Flash Attention 2 doesn't need a 4D mask and relies on `cu_seqlens/max_seqlen`\n+        # NOTE: the created attention masl only approximates the ragged FA2 attention by\n+        # allowing bidirectional attention within `cu_seqlens` blocks, and not attending between\n+        # blocks. Though it will not be a 100% match for FA2's `varlen` path\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            return None\n+\n+        seq_length = inputs_tensor.shape[0]\n+        attention_mask = torch.full(\n+            [1, 1, seq_length, seq_length],\n+            torch.finfo(inputs_tensor.dtype).min,\n+            device=inputs_tensor.device,\n+            dtype=inputs_tensor.dtype,\n+        )\n+        for i in range(1, len(cu_seqlens)):\n+            attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = 0\n+        return attention_mask\n+\n     @auto_docstring\n     def forward(\n         self,\n@@ -833,9 +847,15 @@ def forward(\n                 padded_mask_after_cnn.sum(1).cumsum(0),\n             )\n         ).to(torch.int32)\n+        attention_mask = self._prepare_attention_mask(hidden_states, cu_seqlens)\n \n         for encoder_layer in self.layers:\n-            layer_outputs = encoder_layer(hidden_states, cu_seqlens, **kwargs)\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                cu_seqlens=cu_seqlens,\n+                attention_mask=attention_mask,\n+                **kwargs,\n+            )\n             hidden_states = layer_outputs[0]\n \n         hidden_states_list = hidden_states.split(aftercnn_lens.tolist(), dim=0)\n@@ -928,12 +948,15 @@ def __init__(self, config: Qwen2_5OmniVisionEncoderConfig = None) -> None:\n         self.scaling = self.head_dim**-0.5\n         self.num_key_value_groups = 1  # needed for eager attention\n         self.config = config\n+        self.attention_dropout = 0.0\n+        self.is_causal = False\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         cu_seqlens: torch.Tensor,\n         rotary_pos_emb: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         seq_length = hidden_states.shape[0]\n@@ -943,18 +966,9 @@ def forward(\n         query_states = apply_rotary_pos_emb_vision(query_states.unsqueeze(0), rotary_pos_emb).squeeze(0)\n         key_states = apply_rotary_pos_emb_vision(key_states.unsqueeze(0), rotary_pos_emb).squeeze(0)\n \n-        attention_mask = torch.full(\n-            [1, 1, seq_length, seq_length],\n-            torch.finfo(query_states.dtype).min,\n-            device=query_states.device,\n-            dtype=query_states.dtype,\n-        )\n-        for i in range(1, len(cu_seqlens)):\n-            attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = 0\n-\n-        query_states = query_states.transpose(0, 1).unsqueeze(0)  # unsqueeze batch_dim\n-        key_states = key_states.transpose(0, 1).unsqueeze(0)  # unsqueeze batch_dim\n-        value_states = value_states.transpose(0, 1).unsqueeze(0)  # unsqueeze batch_dim\n+        query_states = query_states.transpose(0, 1).unsqueeze(0)\n+        key_states = key_states.transpose(0, 1).unsqueeze(0)\n+        value_states = value_states.transpose(0, 1).unsqueeze(0)\n         max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n \n         attention_interface: Callable = eager_attention_forward\n@@ -966,13 +980,13 @@ def forward(\n             query_states,\n             key_states,\n             value_states,\n-            attention_mask,\n-            dropout=0.0,\n+            attention_mask=attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n             scaling=self.scaling,\n-            cu_seqlens_q=cu_seqlens,  # pass cu seq lens for FA2\n-            cu_seqlens_k=cu_seqlens,\n-            max_seqlen_q=max_seqlen,\n-            max_seqlen_k=max_seqlen,\n+            cu_seq_lens_q=cu_seqlens,  # pass cu seq lens for FA2\n+            cu_seq_lens_k=cu_seqlens,\n+            max_length_q=max_seqlen,\n+            max_length_k=max_seqlen,\n             is_causal=False,\n             **kwargs,\n         )\n@@ -1009,10 +1023,15 @@ def forward(\n         hidden_states: torch.Tensor,\n         cu_seqlens: torch.Tensor,\n         rotary_pos_emb: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         hidden_states = hidden_states + self.attn(\n-            self.norm1(hidden_states), cu_seqlens=cu_seqlens, rotary_pos_emb=rotary_pos_emb, **kwargs\n+            self.norm1(hidden_states),\n+            cu_seqlens=cu_seqlens,\n+            rotary_pos_emb=rotary_pos_emb,\n+            attention_mask=attention_mask,\n+            **kwargs,\n         )\n         hidden_states = hidden_states + self.mlp(self.norm2(hidden_states))\n         return hidden_states\n@@ -1171,6 +1190,25 @@ def get_window_index(self, grid_thw):\n \n         return window_index, cu_window_seqlens\n \n+    def _prepare_attention_mask(self, inputs_tensor: torch.Tensor, cu_seqlens: torch.Tensor) -> torch.Tensor:\n+        # Flash Attention 2 doesn't need a 4D mask and relies on `cu_seqlens/max_seqlen`\n+        # NOTE: the created attention masl only approximates the ragged FA2 attention by\n+        # allowing bidirectional attention within `cu_seqlens` blocks, and not attending between\n+        # blocks. Though it will not be a 100% match for FA2's `varlen` path\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            return None\n+\n+        seq_length = inputs_tensor.shape[0]\n+        attention_mask = torch.full(\n+            [1, 1, seq_length, seq_length],\n+            torch.finfo(inputs_tensor.dtype).min,\n+            device=inputs_tensor.device,\n+            dtype=inputs_tensor.dtype,\n+        )\n+        for i in range(1, len(cu_seqlens)):\n+            attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = 0\n+        return attention_mask\n+\n     def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs) -> torch.Tensor:\n         \"\"\"\n         Args:\n@@ -1217,10 +1255,13 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs)\n                 cu_seqlens_now = cu_seqlens\n             else:\n                 cu_seqlens_now = cu_window_seqlens\n+\n+            attention_mask = self._prepare_attention_mask(hidden_states, cu_seqlens_now)\n             hidden_states = blk(\n                 hidden_states,\n                 cu_seqlens=cu_seqlens_now,\n                 rotary_pos_emb=rotary_pos_emb,\n+                attention_mask=attention_mask,\n                 **kwargs,\n             )\n         hidden_states = self.merger(hidden_states)"
        },
        {
            "sha": "ae7681543e317c6f61e2d2ba07404d33e1103582",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 76,
            "deletions": 35,
            "changes": 111,
            "blob_url": "https://github.com/huggingface/transformers/blob/7a25f8dfdba4c710d278d8312ef2522c5996a894/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7a25f8dfdba4c710d278d8312ef2522c5996a894/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=7a25f8dfdba4c710d278d8312ef2522c5996a894",
            "patch": "@@ -1611,6 +1611,7 @@ def __init__(\n                 f\" and `num_heads`: {self.num_heads}).\"\n             )\n         self.scaling = self.head_dim**-0.5\n+        self.attention_dropout = 0.0\n         self.is_decoder = False\n         self.is_causal = False\n \n@@ -1623,6 +1624,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         cu_seqlens: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n@@ -1638,15 +1640,6 @@ def forward(\n         value_states = value_states.transpose(0, 1).unsqueeze(0)\n         max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n \n-        attention_mask = torch.full(\n-            [1, 1, seq_length, key_states.shape[-2]],\n-            torch.finfo(query_states.dtype).min,\n-            device=query_states.device,\n-            dtype=query_states.dtype,\n-        )\n-        for i in range(1, len(cu_seqlens)):\n-            attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = 0\n-\n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n@@ -1656,13 +1649,13 @@ def forward(\n             query_states,\n             key_states,\n             value_states,\n-            attention_mask,\n-            dropout=0.0 if not self.training else self.dropout,\n+            attention_mask=attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n             scaling=self.scaling,\n-            cu_seqlens_q=cu_seqlens,  # pass cu seq lens for FA2\n-            cu_seqlens_k=cu_seqlens,\n-            max_seqlen_q=max_seqlen,\n-            max_seqlen_k=max_seqlen,\n+            cu_seq_lens_q=cu_seqlens,  # pass cu seq lens for FA2\n+            cu_seq_lens_k=cu_seqlens,\n+            max_length_q=max_seqlen,\n+            max_length_k=max_seqlen,\n             is_causal=False,\n             **kwargs,\n         )\n@@ -1682,13 +1675,15 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         cu_seqlens: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         residual = hidden_states\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n         hidden_states = self.self_attn(\n             hidden_states=hidden_states,\n             cu_seqlens=cu_seqlens,\n+            attention_mask=attention_mask,\n             **kwargs,\n         )\n         hidden_states = residual + hidden_states\n@@ -1770,6 +1765,25 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value: nn.Module):\n         self.conv1 = value\n \n+    def _prepare_attention_mask(self, inputs_tensor: torch.Tensor, cu_seqlens: torch.Tensor) -> torch.Tensor:\n+        # Flash Attention 2 doesn't need a 4D mask and relies on `cu_seqlens/max_seqlen`\n+        # NOTE: the created attention masl only approximates the ragged FA2 attention by\n+        # allowing bidirectional attention within `cu_seqlens` blocks, and not attending between\n+        # blocks. Though it will not be a 100% match for FA2's `varlen` path\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            return None\n+\n+        seq_length = inputs_tensor.shape[0]\n+        attention_mask = torch.full(\n+            [1, 1, seq_length, seq_length],\n+            torch.finfo(inputs_tensor.dtype).min,\n+            device=inputs_tensor.device,\n+            dtype=inputs_tensor.dtype,\n+        )\n+        for i in range(1, len(cu_seqlens)):\n+            attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = 0\n+        return attention_mask\n+\n     @auto_docstring\n     def forward(\n         self,\n@@ -1818,9 +1832,15 @@ def forward(\n                 padded_mask_after_cnn.sum(1).cumsum(0),\n             )\n         ).to(torch.int32)\n+        attention_mask = self._prepare_attention_mask(hidden_states, cu_seqlens)\n \n         for encoder_layer in self.layers:\n-            layer_outputs = encoder_layer(hidden_states, cu_seqlens, **kwargs)\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                cu_seqlens=cu_seqlens,\n+                attention_mask=attention_mask,\n+                **kwargs,\n+            )\n             hidden_states = layer_outputs[0]\n \n         hidden_states_list = hidden_states.split(aftercnn_lens.tolist(), dim=0)\n@@ -1906,12 +1926,15 @@ def __init__(self, config: Qwen2_5OmniVisionEncoderConfig = None) -> None:\n         self.scaling = self.head_dim**-0.5\n         self.num_key_value_groups = 1  # needed for eager attention\n         self.config = config\n+        self.attention_dropout = 0.0\n+        self.is_causal = False\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         cu_seqlens: torch.Tensor,\n         rotary_pos_emb: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         seq_length = hidden_states.shape[0]\n@@ -1921,18 +1944,9 @@ def forward(\n         query_states = apply_rotary_pos_emb_vision(query_states.unsqueeze(0), rotary_pos_emb).squeeze(0)\n         key_states = apply_rotary_pos_emb_vision(key_states.unsqueeze(0), rotary_pos_emb).squeeze(0)\n \n-        attention_mask = torch.full(\n-            [1, 1, seq_length, seq_length],\n-            torch.finfo(query_states.dtype).min,\n-            device=query_states.device,\n-            dtype=query_states.dtype,\n-        )\n-        for i in range(1, len(cu_seqlens)):\n-            attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = 0\n-\n-        query_states = query_states.transpose(0, 1).unsqueeze(0)  # unsqueeze batch_dim\n-        key_states = key_states.transpose(0, 1).unsqueeze(0)  # unsqueeze batch_dim\n-        value_states = value_states.transpose(0, 1).unsqueeze(0)  # unsqueeze batch_dim\n+        query_states = query_states.transpose(0, 1).unsqueeze(0)\n+        key_states = key_states.transpose(0, 1).unsqueeze(0)\n+        value_states = value_states.transpose(0, 1).unsqueeze(0)\n         max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n \n         attention_interface: Callable = eager_attention_forward\n@@ -1944,13 +1958,13 @@ def forward(\n             query_states,\n             key_states,\n             value_states,\n-            attention_mask,\n-            dropout=0.0,\n+            attention_mask=attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n             scaling=self.scaling,\n-            cu_seqlens_q=cu_seqlens,  # pass cu seq lens for FA2\n-            cu_seqlens_k=cu_seqlens,\n-            max_seqlen_q=max_seqlen,\n-            max_seqlen_k=max_seqlen,\n+            cu_seq_lens_q=cu_seqlens,  # pass cu seq lens for FA2\n+            cu_seq_lens_k=cu_seqlens,\n+            max_length_q=max_seqlen,\n+            max_length_k=max_seqlen,\n             is_causal=False,\n             **kwargs,\n         )\n@@ -1970,10 +1984,15 @@ def forward(\n         hidden_states: torch.Tensor,\n         cu_seqlens: torch.Tensor,\n         rotary_pos_emb: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         hidden_states = hidden_states + self.attn(\n-            self.norm1(hidden_states), cu_seqlens=cu_seqlens, rotary_pos_emb=rotary_pos_emb, **kwargs\n+            self.norm1(hidden_states),\n+            cu_seqlens=cu_seqlens,\n+            rotary_pos_emb=rotary_pos_emb,\n+            attention_mask=attention_mask,\n+            **kwargs,\n         )\n         hidden_states = hidden_states + self.mlp(self.norm2(hidden_states))\n         return hidden_states\n@@ -1987,6 +2006,25 @@ def __init__(self, config: Qwen2_5OmniVisionEncoderConfig, *inputs, **kwargs) ->\n         super().__init__(config, *inputs, **kwargs)\n         self.blocks = nn.ModuleList([Qwen2_5OmniVisionBlock(config) for _ in range(config.depth)])\n \n+    def _prepare_attention_mask(self, inputs_tensor: torch.Tensor, cu_seqlens: torch.Tensor) -> torch.Tensor:\n+        # Flash Attention 2 doesn't need a 4D mask and relies on `cu_seqlens/max_seqlen`\n+        # NOTE: the created attention masl only approximates the ragged FA2 attention by\n+        # allowing bidirectional attention within `cu_seqlens` blocks, and not attending between\n+        # blocks. Though it will not be a 100% match for FA2's `varlen` path\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            return None\n+\n+        seq_length = inputs_tensor.shape[0]\n+        attention_mask = torch.full(\n+            [1, 1, seq_length, seq_length],\n+            torch.finfo(inputs_tensor.dtype).min,\n+            device=inputs_tensor.device,\n+            dtype=inputs_tensor.dtype,\n+        )\n+        for i in range(1, len(cu_seqlens)):\n+            attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = 0\n+        return attention_mask\n+\n     def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs) -> torch.Tensor:\n         \"\"\"\n         Args:\n@@ -2033,10 +2071,13 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs)\n                 cu_seqlens_now = cu_seqlens\n             else:\n                 cu_seqlens_now = cu_window_seqlens\n+\n+            attention_mask = self._prepare_attention_mask(hidden_states, cu_seqlens_now)\n             hidden_states = blk(\n                 hidden_states,\n                 cu_seqlens=cu_seqlens_now,\n                 rotary_pos_emb=rotary_pos_emb,\n+                attention_mask=attention_mask,\n                 **kwargs,\n             )\n         hidden_states = self.merger(hidden_states)"
        },
        {
            "sha": "f5b40ac4a4544efeabce50fd3ea2ffd4033d5eba",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 40,
            "deletions": 19,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/7a25f8dfdba4c710d278d8312ef2522c5996a894/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7a25f8dfdba4c710d278d8312ef2522c5996a894/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=7a25f8dfdba4c710d278d8312ef2522c5996a894",
            "patch": "@@ -206,13 +206,16 @@ def __init__(self, config: Qwen2_5_VLVisionConfig) -> None:\n         self.proj = nn.Linear(self.dim, self.dim)\n         self.scaling = self.head_dim**-0.5\n         self.config = config\n+        self.attention_dropout = 0.0\n+        self.is_causal = False\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         cu_seqlens: torch.Tensor,\n         rotary_pos_emb: Optional[torch.Tensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         seq_length = hidden_states.shape[0]\n@@ -233,18 +236,9 @@ def forward(\n             cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb_vision(query_states, key_states, cos, sin)\n \n-        attention_mask = torch.full(\n-            [1, 1, seq_length, seq_length],\n-            torch.finfo(value_states.dtype).min,\n-            device=value_states.device,\n-            dtype=value_states.dtype,\n-        )\n-        for i in range(1, len(cu_seqlens)):\n-            attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = 0\n-\n-        query_states = query_states.transpose(0, 1).unsqueeze(0)  # unsqueeze batch_dim\n-        key_states = key_states.transpose(0, 1).unsqueeze(0)  # unsqueeze batch_dim\n-        value_states = value_states.transpose(0, 1).unsqueeze(0)  # unsqueeze batch_dim\n+        query_states = query_states.transpose(0, 1).unsqueeze(0)\n+        key_states = key_states.transpose(0, 1).unsqueeze(0)\n+        value_states = value_states.transpose(0, 1).unsqueeze(0)\n         max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n \n         attention_interface: Callable = eager_attention_forward\n@@ -256,13 +250,13 @@ def forward(\n             query_states,\n             key_states,\n             value_states,\n-            attention_mask,\n-            dropout=0.0,\n+            attention_mask=attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n             scaling=self.scaling,\n-            cu_seqlens_q=cu_seqlens,  # pass cu seq lens for FA2\n-            cu_seqlens_k=cu_seqlens,\n-            max_seqlen_q=max_seqlen,\n-            max_seqlen_k=max_seqlen,\n+            cu_seq_lens_q=cu_seqlens,  # pass cu seq lens for FA2\n+            cu_seq_lens_k=cu_seqlens,\n+            max_length_q=max_seqlen,\n+            max_length_k=max_seqlen,\n             is_causal=False,\n             **kwargs,\n         )\n@@ -286,13 +280,15 @@ def forward(\n         cu_seqlens: torch.Tensor,\n         rotary_pos_emb: Optional[torch.Tensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         hidden_states = hidden_states + self.attn(\n             self.norm1(hidden_states),\n             cu_seqlens=cu_seqlens,\n             rotary_pos_emb=rotary_pos_emb,\n             position_embeddings=position_embeddings,\n+            attention_mask=attention_mask,\n             **kwargs,\n         )\n         hidden_states = hidden_states + self.mlp(self.norm2(hidden_states))\n@@ -426,6 +422,25 @@ def get_window_index(self, grid_thw):\n \n         return window_index, cu_window_seqlens\n \n+    def _prepare_attention_mask(self, inputs_tensor: torch.Tensor, cu_seqlens: torch.Tensor) -> torch.Tensor:\n+        # Flash Attention 2 doesn't need a 4D mask and relies on `cu_seqlens/max_seqlen`\n+        # NOTE: the created attention masl only approximates the ragged FA2 attention by\n+        # allowing bidirectional attention within `cu_seqlens` blocks, and not attending between\n+        # blocks. Though it will not be a 100% match for FA2's `varlen` path\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            return None\n+\n+        seq_length = inputs_tensor.shape[0]\n+        attention_mask = torch.full(\n+            [1, 1, seq_length, seq_length],\n+            torch.finfo(inputs_tensor.dtype).min,\n+            device=inputs_tensor.device,\n+            dtype=inputs_tensor.dtype,\n+        )\n+        for i in range(1, len(cu_seqlens)):\n+            attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = 0\n+        return attention_mask\n+\n     def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs) -> torch.Tensor:\n         \"\"\"\n         Args:\n@@ -472,8 +487,14 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs)\n                 cu_seqlens_now = cu_seqlens\n             else:\n                 cu_seqlens_now = cu_window_seqlens\n+\n+            attention_mask = self._prepare_attention_mask(hidden_states, cu_seqlens_now)\n             hidden_states = blk(\n-                hidden_states, cu_seqlens=cu_seqlens_now, position_embeddings=position_embeddings, **kwargs\n+                hidden_states,\n+                cu_seqlens=cu_seqlens_now,\n+                position_embeddings=position_embeddings,\n+                attention_mask=attention_mask,\n+                **kwargs,\n             )\n \n         hidden_states = self.merger(hidden_states)"
        },
        {
            "sha": "30ddfbda098eeb82e44cb14c9e87fb2110dc20ae",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 28,
            "deletions": 1,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7a25f8dfdba4c710d278d8312ef2522c5996a894/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7a25f8dfdba4c710d278d8312ef2522c5996a894/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=7a25f8dfdba4c710d278d8312ef2522c5996a894",
            "patch": "@@ -159,13 +159,15 @@ def forward(\n         cu_seqlens: torch.Tensor,\n         rotary_pos_emb: Optional[torch.Tensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         hidden_states = hidden_states + self.attn(\n             self.norm1(hidden_states),\n             cu_seqlens=cu_seqlens,\n             rotary_pos_emb=rotary_pos_emb,\n             position_embeddings=position_embeddings,\n+            attention_mask=attention_mask,\n             **kwargs,\n         )\n         hidden_states = hidden_states + self.mlp(self.norm2(hidden_states))\n@@ -287,6 +289,25 @@ def get_window_index(self, grid_thw):\n \n         return window_index, cu_window_seqlens\n \n+    def _prepare_attention_mask(self, inputs_tensor: torch.Tensor, cu_seqlens: torch.Tensor) -> torch.Tensor:\n+        # Flash Attention 2 doesn't need a 4D mask and relies on `cu_seqlens/max_seqlen`\n+        # NOTE: the created attention masl only approximates the ragged FA2 attention by\n+        # allowing bidirectional attention within `cu_seqlens` blocks, and not attending between\n+        # blocks. Though it will not be a 100% match for FA2's `varlen` path\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            return None\n+\n+        seq_length = inputs_tensor.shape[0]\n+        attention_mask = torch.full(\n+            [1, 1, seq_length, seq_length],\n+            torch.finfo(inputs_tensor.dtype).min,\n+            device=inputs_tensor.device,\n+            dtype=inputs_tensor.dtype,\n+        )\n+        for i in range(1, len(cu_seqlens)):\n+            attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = 0\n+        return attention_mask\n+\n     def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs) -> torch.Tensor:\n         \"\"\"\n         Args:\n@@ -333,8 +354,14 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs)\n                 cu_seqlens_now = cu_seqlens\n             else:\n                 cu_seqlens_now = cu_window_seqlens\n+\n+            attention_mask = self._prepare_attention_mask(hidden_states, cu_seqlens_now)\n             hidden_states = blk(\n-                hidden_states, cu_seqlens=cu_seqlens_now, position_embeddings=position_embeddings, **kwargs\n+                hidden_states,\n+                cu_seqlens=cu_seqlens_now,\n+                position_embeddings=position_embeddings,\n+                attention_mask=attention_mask,\n+                **kwargs,\n             )\n \n         hidden_states = self.merger(hidden_states)"
        },
        {
            "sha": "a075de666fbe3d9e5c022aa1f3ac74be8ab1a722",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 39,
            "deletions": 19,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/7a25f8dfdba4c710d278d8312ef2522c5996a894/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7a25f8dfdba4c710d278d8312ef2522c5996a894/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=7a25f8dfdba4c710d278d8312ef2522c5996a894",
            "patch": "@@ -324,13 +324,16 @@ def __init__(self, config: Qwen2VLVisionConfig) -> None:\n         self.proj = nn.Linear(self.dim, self.dim)\n         self.scaling = self.head_dim**-0.5\n         self.config = config\n+        self.attention_dropout = 0.0\n+        self.is_causal = False\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         cu_seqlens: torch.Tensor,\n         rotary_pos_emb: Optional[torch.Tensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         seq_length = hidden_states.shape[0]\n@@ -351,18 +354,9 @@ def forward(\n             cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb_vision(query_states, key_states, cos, sin)\n \n-        attention_mask = torch.full(\n-            [1, 1, seq_length, seq_length],\n-            torch.finfo(value_states.dtype).min,\n-            device=value_states.device,\n-            dtype=value_states.dtype,\n-        )\n-        for i in range(1, len(cu_seqlens)):\n-            attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = 0\n-\n-        query_states = query_states.transpose(0, 1).unsqueeze(0)  # unsqueeze batch_dim\n-        key_states = key_states.transpose(0, 1).unsqueeze(0)  # unsqueeze batch_dim\n-        value_states = value_states.transpose(0, 1).unsqueeze(0)  # unsqueeze batch_dim\n+        query_states = query_states.transpose(0, 1).unsqueeze(0)\n+        key_states = key_states.transpose(0, 1).unsqueeze(0)\n+        value_states = value_states.transpose(0, 1).unsqueeze(0)\n         max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n \n         attention_interface: Callable = eager_attention_forward\n@@ -374,13 +368,13 @@ def forward(\n             query_states,\n             key_states,\n             value_states,\n-            attention_mask,\n-            dropout=0.0,\n+            attention_mask=attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n             scaling=self.scaling,\n-            cu_seqlens_q=cu_seqlens,  # pass cu seq lens for FA2\n-            cu_seqlens_k=cu_seqlens,\n-            max_seqlen_q=max_seqlen,\n-            max_seqlen_k=max_seqlen,\n+            cu_seq_lens_q=cu_seqlens,  # pass cu seq lens for FA2\n+            cu_seq_lens_k=cu_seqlens,\n+            max_length_q=max_seqlen,\n+            max_length_k=max_seqlen,\n             is_causal=False,\n             **kwargs,\n         )\n@@ -406,13 +400,15 @@ def forward(\n         cu_seqlens: torch.Tensor,\n         rotary_pos_emb: Optional[torch.Tensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         hidden_states = hidden_states + self.attn(\n             self.norm1(hidden_states),\n             cu_seqlens=cu_seqlens,\n             rotary_pos_emb=rotary_pos_emb,\n             position_embeddings=position_embeddings,\n+            attention_mask=attention_mask,\n             **kwargs,\n         )\n         hidden_states = hidden_states + self.mlp(self.norm2(hidden_states))\n@@ -725,6 +721,25 @@ def rot_pos_emb(self, grid_thw):\n         rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)\n         return rotary_pos_emb\n \n+    def _prepare_attention_mask(self, inputs_tensor: torch.Tensor, cu_seqlens: torch.Tensor) -> torch.Tensor:\n+        # Flash Attention 2 doesn't need a 4D mask and relies on `cu_seqlens/max_seqlen`\n+        # NOTE: the created attention masl only approximates the ragged FA2 attention by\n+        # allowing bidirectional attention within `cu_seqlens` blocks, and not attending between\n+        # blocks. Though it will not be a 100% match for FA2's `varlen` path\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            return None\n+\n+        seq_length = inputs_tensor.shape[0]\n+        attention_mask = torch.full(\n+            [1, 1, seq_length, seq_length],\n+            torch.finfo(inputs_tensor.dtype).min,\n+            device=inputs_tensor.device,\n+            dtype=inputs_tensor.dtype,\n+        )\n+        for i in range(1, len(cu_seqlens)):\n+            attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = 0\n+        return attention_mask\n+\n     @auto_docstring\n     def forward(\n         self,\n@@ -750,10 +765,15 @@ def forward(\n             dtype=grid_thw.dtype if torch.jit.is_tracing() else torch.int32,\n         )\n         cu_seqlens = F.pad(cu_seqlens, (1, 0), value=0)\n+        attention_mask = self._prepare_attention_mask(hidden_states, cu_seqlens)\n \n         for blk in self.blocks:\n             hidden_states = blk(\n-                hidden_states, cu_seqlens=cu_seqlens, position_embeddings=position_embeddings, **kwargs\n+                hidden_states,\n+                cu_seqlens=cu_seqlens,\n+                position_embeddings=position_embeddings,\n+                attention_mask=attention_mask,\n+                **kwargs,\n             )\n \n         return self.merger(hidden_states)"
        },
        {
            "sha": "9525f6415a21f3311ce3b6fe4ee0aaeea2030722",
            "filename": "tests/models/qwen2_5_vl/test_modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7a25f8dfdba4c710d278d8312ef2522c5996a894/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7a25f8dfdba4c710d278d8312ef2522c5996a894/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py?ref=7a25f8dfdba4c710d278d8312ef2522c5996a894",
            "patch": "@@ -184,7 +184,7 @@ def prepare_config_and_inputs_for_common(self):\n         input_ids[:, self.num_image_tokens - 1] = self.vision_start_token_id\n         inputs_dict = {\n             \"pixel_values\": pixel_values,\n-            \"image_grid_thw\": torch.tensor([[1, 1, 1]] * self.batch_size),\n+            \"image_grid_thw\": torch.tensor([[1, 1, 1]] * self.batch_size, device=torch_device),\n             \"input_ids\": input_ids,\n             \"attention_mask\": attention_mask,\n         }"
        },
        {
            "sha": "dff92fb588889ca4943fd16a99b25ee96085ac98",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7a25f8dfdba4c710d278d8312ef2522c5996a894/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7a25f8dfdba4c710d278d8312ef2522c5996a894/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=7a25f8dfdba4c710d278d8312ef2522c5996a894",
            "patch": "@@ -176,7 +176,7 @@ def prepare_config_and_inputs_for_common(self):\n \n         inputs_dict = {\n             \"pixel_values\": pixel_values,\n-            \"image_grid_thw\": torch.tensor([[1, 1, 1]] * self.batch_size),\n+            \"image_grid_thw\": torch.tensor([[1, 1, 1]] * self.batch_size, device=torch_device),\n             \"input_ids\": input_ids,\n             \"attention_mask\": attention_mask,\n         }"
        }
    ],
    "stats": {
        "total": 562,
        "additions": 363,
        "deletions": 199
    }
}