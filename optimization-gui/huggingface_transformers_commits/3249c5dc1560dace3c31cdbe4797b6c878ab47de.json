{
    "author": "qubvel",
    "message": "Refactor attention for SigLIP based models (#36981)\n\n* Update Siglip attention implementation\n\n* Update tests for Siglip\n\n* Remove one level of indentation\n\n* Update test to be more specific\n\n* Fixup\n\n* Idefics2\n\n* Idefics3\n\n* Emu3\n\n* SmolVLM\n\n* Phi4 (just init small update)\n\n* Idefics2 (test fix)\n\n* Update siglip2 tests\n\n* Update eager\n\n* trigger\n\n* Clean up\n\n* Transfer inputs to device in test\n\n* Fixing test\n\n* Fixing test\n\n* Revert contiguous\n\n* Remove unused is_flash_attn_2_available\n\n* Move flaky to specific models",
    "sha": "3249c5dc1560dace3c31cdbe4797b6c878ab47de",
    "files": [
        {
            "sha": "3322ce28f9fb0ff0bd56ee5c536e3f2c97d2889f",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 37,
            "deletions": 35,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/3249c5dc1560dace3c31cdbe4797b6c878ab47de/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3249c5dc1560dace3c31cdbe4797b6c878ab47de/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=3249c5dc1560dace3c31cdbe4797b6c878ab47de",
            "patch": "@@ -600,7 +600,7 @@ def forward(self, hidden_states: torch.Tensor, quant_channels: Optional[torch.Te\n class Emu3VQVAEAttentionBlock(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    def __init__(self, config):\n+    def __init__(self, config: Emu3VQVAEConfig):\n         super().__init__()\n         self.config = config\n         self.embed_dim = config.hidden_size\n@@ -613,12 +613,16 @@ def __init__(self, config):\n             )\n         self.scale = self.head_dim**-0.5\n         self.dropout = config.attention_dropout\n+        self.is_causal = False\n \n         self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n         self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n         self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n         self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n \n+        # for compatibility with the attention interface\n+        self.num_key_value_groups = 1\n+\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -627,48 +631,43 @@ def forward(\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n-        batch_size, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n+        batch_size, seq_length, embed_dim = hidden_states.shape\n \n-        query_states = query_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        queries = self.q_proj(hidden_states)\n+        keys = self.k_proj(hidden_states)\n+        values = self.v_proj(hidden_states)\n \n-        k_v_seq_len = key_states.shape[-2]\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) * self.scale\n-\n-        if attn_weights.size() != (batch_size, self.num_heads, q_len, k_v_seq_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(batch_size, self.num_heads, q_len, k_v_seq_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n+        queries = queries.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        keys = keys.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        values = values.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n \n-        if attention_mask is not None:\n-            if attention_mask.size() != (batch_size, 1, q_len, k_v_seq_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(batch_size, 1, q_len, k_v_seq_len)}, but is {attention_mask.size()}\"\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n                 )\n-            attn_weights = attn_weights + attention_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (batch_size, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(batch_size, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.reshape(batch_size, q_len, self.embed_dim)\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            queries,\n+            keys,\n+            values,\n+            attention_mask,\n+            is_causal=self.is_causal,\n+            scaling=self.scale,\n+            dropout=0.0 if not self.training else self.dropout,\n+        )\n \n+        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n+        if not output_attentions:\n+            attn_weights = None\n+\n         return attn_output, attn_weights\n \n \n@@ -1005,6 +1004,9 @@ class Emu3VQVAE(PreTrainedModel):\n     config_class = Emu3VQVAEConfig\n     base_model_prefix = \"emuvideovq\"\n     main_input_name = \"pixel_values\"\n+    _supports_sdpa = True\n+    _supports_flash_attn_2 = True\n+    _supports_flex_attn = True\n     _no_split_modules = [\n         \"Emu3VQVAETemporalResnetBlock\",\n         \"Emu3VQVAEAttentionBlock\","
        },
        {
            "sha": "8af5ec700a25e885e3511178a88257bfbaad18d8",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3249c5dc1560dace3c31cdbe4797b6c878ab47de/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3249c5dc1560dace3c31cdbe4797b6c878ab47de/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=3249c5dc1560dace3c31cdbe4797b6c878ab47de",
            "patch": "@@ -394,7 +394,11 @@ def forward(self, hidden_states: torch.Tensor, quant_channels: Optional[torch.Te\n \n \n class Emu3VQVAEAttentionBlock(SiglipAttention):\n-    pass\n+    def __init__(self, config: Emu3VQVAEConfig):\n+        super().__init__(config)\n+\n+        # for compatibility with the attention interface\n+        self.num_key_value_groups = 1\n \n \n class Emu3VQVAEGroupNorm(nn.GroupNorm):\n@@ -730,6 +734,9 @@ class Emu3VQVAE(PreTrainedModel):\n     config_class = Emu3VQVAEConfig\n     base_model_prefix = \"emuvideovq\"\n     main_input_name = \"pixel_values\"\n+    _supports_sdpa = True\n+    _supports_flash_attn_2 = True\n+    _supports_flex_attn = True\n     _no_split_modules = [\n         \"Emu3VQVAETemporalResnetBlock\",\n         \"Emu3VQVAEAttentionBlock\","
        },
        {
            "sha": "5ee9ec4cc77a09accf77f70b8eed8cf246d4ff46",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 85,
            "deletions": 310,
            "changes": 395,
            "blob_url": "https://github.com/huggingface/transformers/blob/3249c5dc1560dace3c31cdbe4797b6c878ab47de/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3249c5dc1560dace3c31cdbe4797b6c878ab47de/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=3249c5dc1560dace3c31cdbe4797b6c878ab47de",
            "patch": "@@ -14,9 +14,8 @@\n # limitations under the License.\n \"\"\"PyTorch Idefics2 model.\"\"\"\n \n-import math\n from dataclasses import dataclass\n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -27,9 +26,8 @@\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n-from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import BaseModelOutput, ModelOutput\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n@@ -41,10 +39,6 @@\n from .configuration_idefics2 import Idefics2Config, Idefics2PerceiverConfig, Idefics2VisionConfig\n \n \n-if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n-\n logger = logging.get_logger(__name__)\n \n _CONFIG_FOR_DOC = \"Idefics2Config\"\n@@ -185,6 +179,33 @@ def forward(self, pixel_values: torch.FloatTensor, patch_attention_mask: torch.B\n         return embeddings\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    if hasattr(module, \"num_key_value_groups\"):\n+        key = repeat_kv(key, module.num_key_value_groups)\n+        value = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n # Copied from transformers.models.siglip.modeling_siglip.SiglipAttention with Siglip->Idefics2Vision\n class Idefics2VisionAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n@@ -220,140 +241,38 @@ def forward(\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n-        batch_size, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n+        batch_size, seq_length, embed_dim = hidden_states.shape\n \n-        query_states = query_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        queries = self.q_proj(hidden_states)\n+        keys = self.k_proj(hidden_states)\n+        values = self.v_proj(hidden_states)\n \n-        k_v_seq_len = key_states.shape[-2]\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) * self.scale\n+        queries = queries.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        keys = keys.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        values = values.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n \n-        if attn_weights.size() != (batch_size, self.num_heads, q_len, k_v_seq_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(batch_size, self.num_heads, q_len, k_v_seq_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        if attention_mask is not None:\n-            if attention_mask.size() != (batch_size, 1, q_len, k_v_seq_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(batch_size, 1, q_len, k_v_seq_len)}, but is {attention_mask.size()}\"\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n                 )\n-            attn_weights = attn_weights + attention_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (batch_size, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(batch_size, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.reshape(batch_size, q_len, self.embed_dim)\n-\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, attn_weights\n-\n-\n-class Idefics2VisionFlashAttention2(Idefics2VisionAttention):\n-    \"\"\"\n-    Idefics2Vision flash attention module. This module inherits from `Idefics2VisionAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        **kwargs,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        output_attentions = False\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        # Flash attention requires the input to have the shape\n-        # batch_size x seq_length x head_dim x hidden_dim\n-        # therefore we just need to keep the original shape\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim)\n-        key_states = key_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-\n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n-\n-        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n-        # to be able to avoid many of these transpose/reshape/view.\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        dropout_rate = self.dropout if self.training else 0.0\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (Idefics2VisionRMSNorm handles it correctly)\n-\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n             else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        attn_output = _flash_attention_forward(\n-            query_states,\n-            key_states,\n-            value_states,\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            queries,\n+            keys,\n+            values,\n             attention_mask,\n-            q_len,\n-            dropout=dropout_rate,\n             is_causal=self.is_causal,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n+            scaling=self.scale,\n+            dropout=0.0 if not self.training else self.dropout,\n         )\n \n-        attn_output = attn_output.reshape(bsz, q_len, self.embed_dim).contiguous()\n+        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n         if not output_attentions:\n@@ -362,12 +281,6 @@ def forward(\n         return attn_output, attn_weights\n \n \n-IDEFICS_VISION_ATTENTION_CLASSES = {\n-    \"eager\": Idefics2VisionAttention,\n-    \"flash_attention_2\": Idefics2VisionFlashAttention2,\n-}\n-\n-\n # Copied from transformers.models.siglip.modeling_siglip.SiglipMLP with Siglip->Idefics2Vision\n class Idefics2VisionMLP(nn.Module):\n     def __init__(self, config):\n@@ -437,7 +350,7 @@ class Idefics2EncoderLayer(nn.Module):\n     def __init__(self, config: Idefics2VisionConfig):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n-        self.self_attn = IDEFICS_VISION_ATTENTION_CLASSES[config._attn_implementation](config)\n+        self.self_attn = Idefics2VisionAttention(config)\n         self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n         self.mlp = Idefics2VisionMLP(config)\n         self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n@@ -600,6 +513,7 @@ class Idefics2PreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n \n     def _init_weights(self, module):\n@@ -646,8 +560,10 @@ def _init_weights(self, module):\n     IDEFICS2_START_DOCSTRING,\n )\n class Idefics2VisionTransformer(Idefics2PreTrainedModel):\n-    _supports_sdpa = False\n     config_class = Idefics2VisionConfig\n+    _supports_sdpa = True\n+    _supports_flash_attention_2 = True\n+    _supports_flex_attn = True\n \n     def __init__(self, config: Idefics2VisionConfig):\n         super().__init__(config)\n@@ -761,14 +677,15 @@ class Idefics2PerceiverAttention(nn.Module):\n     def __init__(self, config, layer_idx: Optional[int] = None) -> None:\n         \"\"\"Perceiver Cross-Attention Module --> let long-form inputs be `context`, resampled embeddings be `latents`\"\"\"\n         super().__init__()\n-\n+        self.config = config\n         self.layer_idx = None\n         self.hidden_size = config.hidden_size\n         self.num_heads = config.resampler_n_heads\n         self.head_dim = config.resampler_head_dim\n         self.num_key_value_heads = config.num_key_value_heads\n         self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n         self.attention_dropout = config.attention_dropout\n+        self.scaling = self.head_dim**-0.5\n \n         self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n         self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n@@ -804,179 +721,41 @@ def forward(\n \n         hidden_states = torch.concat([context, latents], dim=-2)\n \n-        query_states = self.q_proj(latents)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n+        queries = self.q_proj(latents)\n+        keys = self.k_proj(hidden_states)\n+        values = self.v_proj(hidden_states)\n \n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, kv_seq_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, kv_seq_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        queries = queries.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        keys = keys.view(bsz, kv_seq_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        values = values.view(bsz, kv_seq_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n         past_key_value = getattr(self, \"past_key_value\", past_key_value)\n \n         if past_key_value is not None:\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx)\n-\n-        # repeat k/v heads if n_kv_heads < n_heads\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n-\n-        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n+            keys, values = past_key_value.update(keys, values, self.layer_idx)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n                 )\n-\n-            attn_weights = attn_weights + attention_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.reshape(bsz, q_len, self.num_heads * self.head_dim)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-# NO LONGER EXIST Copied from transformers.models.mistral.modeling_mistral.MistralFlashAttention2 with MistralAttention->Idefics2PerceiverAttention,MistralFlashAttention->Idefics2PerceiverFlashAttention,Mistral->Idefics2\n-# TODO cyril: modular\n-class Idefics2PerceiverFlashAttention2(Idefics2PerceiverAttention):\n-    \"\"\"\n-    Idefics2 flash attention module. This module inherits from `Idefics2PerceiverAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n-\n-    # Ignore copy\n-    def forward(\n-        self,\n-        latents: torch.Tensor,\n-        context: torch.Tensor,\n-        attention_mask: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        **kwargs,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        bsz, q_len, _ = latents.size()\n-        kv_seq_len = q_len + context.size()[1]\n-\n-        # Query, Key, Value Projections --> Note that in Flamingo, latents are *concatenated* with context prior to attn!\n-        #   Note: This results in queries w/ `seq = n_latents`, and keys, values with `seq = len(context) + n_latents`\n-        query_states = self.q_proj(latents)\n-        key_states = self.k_proj(torch.cat([context, latents], dim=-2))\n-        value_states = self.v_proj(torch.cat([context, latents], dim=-2))\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim)\n-        key_states = key_states.view(bsz, kv_seq_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, kv_seq_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            kv_seq_len += past_key_value[0].shape[-2]\n-\n-        if past_key_value is not None:\n-            # Activate slicing cache only if the config has a value `sliding_windows` attribute\n-            if hasattr(self.config, \"sliding_window\") and kv_seq_len > self.config.sliding_window:\n-                slicing_tokens = kv_seq_len - self.config.sliding_window\n-\n-                past_key = past_key_value[0]\n-                past_value = past_key_value[1]\n-\n-                past_key = past_key[:, :, slicing_tokens:, :].contiguous()\n-                past_value = past_value[:, :, slicing_tokens:, :].contiguous()\n-\n-                if past_key.shape[-2] != self.config.sliding_window - 1:\n-                    raise ValueError(\n-                        \"past key must have a shape of (`batch_size, num_heads, self.config.sliding_window-1,\"\n-                        f\" head_dim`), got {past_key.shape}\"\n-                    )\n-\n-                past_key_value = (past_key, past_value)\n-\n-                if attention_mask is not None:\n-                    attention_mask = attention_mask[:, slicing_tokens:]\n-                    attention_mask = torch.cat([attention_mask, torch.ones_like(attention_mask[:, -1:])], dim=-1)\n-\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n-\n-        past_key_value = (key_states, value_states) if use_cache else None\n-\n-        # repeat k/v heads if n_kv_heads < n_heads\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-        dropout_rate = 0.0 if not self.training else self.attention_dropout\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in float16 just to be sure everything works as expected.\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n             else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        # Reashape to the expected shape for Flash Attention\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        attn_output = _flash_attention_forward(\n-            query_states,\n-            key_states,\n-            value_states,\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            queries,\n+            keys,\n+            values,\n             attention_mask,\n-            q_len,\n-            dropout=dropout_rate,\n-            sliding_window=None,\n             is_causal=self.is_causal,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n+            scaling=self.scaling,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n         )\n \n-        attn_output = attn_output.reshape(bsz, q_len, self.num_heads * self.head_dim).contiguous()\n+        attn_output = attn_output.reshape(bsz, q_len, self.num_heads * self.head_dim)\n         attn_output = self.o_proj(attn_output)\n \n         if not output_attentions:\n@@ -985,12 +764,6 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-IDEFICS2_PERCEIVER_ATTENTION_CLASSES = {\n-    \"eager\": Idefics2PerceiverAttention,\n-    \"flash_attention_2\": Idefics2PerceiverFlashAttention2,\n-}\n-\n-\n class Idefics2PerceiverLayer(nn.Module):\n     def __init__(self, config, layer_idx: int):\n         super().__init__()\n@@ -1001,7 +774,7 @@ def __init__(self, config, layer_idx: int):\n \n         self.input_latents_norm = Idefics2RMSNorm(self.hidden_size, eps=self.rms_norm_eps)\n         self.input_context_norm = Idefics2RMSNorm(self.hidden_size, eps=self.rms_norm_eps)\n-        self.self_attn = IDEFICS2_PERCEIVER_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx=layer_idx)\n+        self.self_attn = Idefics2PerceiverAttention(config, layer_idx=layer_idx)\n         self.post_attention_layernorm = Idefics2RMSNorm(self.hidden_size, eps=self.rms_norm_eps)\n         self.mlp = Idefics2MLP(\n             hidden_size=config.hidden_size,\n@@ -1084,8 +857,10 @@ def forward(\n     IDEFICS2_START_DOCSTRING,\n )\n class Idefics2PerceiverResampler(Idefics2PreTrainedModel):\n-    _supports_sdpa = False\n     config_class = Idefics2PerceiverConfig\n+    _supports_sdpa = True\n+    _supports_flash_attention_2 = True\n+    _supports_flex_attn = True\n \n     def __init__(self, config) -> None:\n         super().__init__(config)"
        },
        {
            "sha": "3821bd3e7a236dbed4b353c2211ecc5c28935897",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 54,
            "deletions": 141,
            "changes": 195,
            "blob_url": "https://github.com/huggingface/transformers/blob/3249c5dc1560dace3c31cdbe4797b6c878ab47de/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3249c5dc1560dace3c31cdbe4797b6c878ab47de/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=3249c5dc1560dace3c31cdbe4797b6c878ab47de",
            "patch": "@@ -15,20 +15,19 @@\n \"\"\"PyTorch Idefics3 model.\"\"\"\n \n from dataclasses import dataclass\n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache\n+from ...cache_utils import DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n-from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import BaseModelOutput, ModelOutput\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n@@ -39,10 +38,6 @@\n from .configuration_idefics3 import Idefics3Config, Idefics3VisionConfig\n \n \n-if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n-\n logger = logging.get_logger(__name__)\n \n _CONFIG_FOR_DOC = \"Idefics3Config\"\n@@ -184,6 +179,30 @@ def forward(self, pixel_values: torch.FloatTensor, patch_attention_mask: torch.B\n         return embeddings\n \n \n+# Copied from transformers.models.siglip.modeling_siglip.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n # Copied from transformers.models.siglip.modeling_siglip.SiglipAttention with Siglip->Idefics3Vision\n class Idefics3VisionAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n@@ -219,141 +238,38 @@ def forward(\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n-        batch_size, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n+        batch_size, seq_length, embed_dim = hidden_states.shape\n \n-        query_states = query_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        queries = self.q_proj(hidden_states)\n+        keys = self.k_proj(hidden_states)\n+        values = self.v_proj(hidden_states)\n \n-        k_v_seq_len = key_states.shape[-2]\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) * self.scale\n-\n-        if attn_weights.size() != (batch_size, self.num_heads, q_len, k_v_seq_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(batch_size, self.num_heads, q_len, k_v_seq_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n+        queries = queries.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        keys = keys.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        values = values.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n \n-        if attention_mask is not None:\n-            if attention_mask.size() != (batch_size, 1, q_len, k_v_seq_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(batch_size, 1, q_len, k_v_seq_len)}, but is {attention_mask.size()}\"\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n                 )\n-            attn_weights = attn_weights + attention_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (batch_size, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(batch_size, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.reshape(batch_size, q_len, self.embed_dim)\n-\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, attn_weights\n-\n-\n-# Copied from transformers.models.idefics2.modeling_idefics2.Idefics2VisionFlashAttention2 with Idefics2->Idefics3\n-class Idefics3VisionFlashAttention2(Idefics3VisionAttention):\n-    \"\"\"\n-    Idefics3Vision flash attention module. This module inherits from `Idefics3VisionAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        **kwargs,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        output_attentions = False\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        # Flash attention requires the input to have the shape\n-        # batch_size x seq_length x head_dim x hidden_dim\n-        # therefore we just need to keep the original shape\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim)\n-        key_states = key_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-\n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n-\n-        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n-        # to be able to avoid many of these transpose/reshape/view.\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        dropout_rate = self.dropout if self.training else 0.0\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (Idefics3VisionRMSNorm handles it correctly)\n-\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n             else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        attn_output = _flash_attention_forward(\n-            query_states,\n-            key_states,\n-            value_states,\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            queries,\n+            keys,\n+            values,\n             attention_mask,\n-            q_len,\n-            dropout=dropout_rate,\n             is_causal=self.is_causal,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n+            scaling=self.scale,\n+            dropout=0.0 if not self.training else self.dropout,\n         )\n \n-        attn_output = attn_output.reshape(bsz, q_len, self.embed_dim).contiguous()\n+        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n         if not output_attentions:\n@@ -362,12 +278,6 @@ def forward(\n         return attn_output, attn_weights\n \n \n-IDEFICS_VISION_ATTENTION_CLASSES = {\n-    \"eager\": Idefics3VisionAttention,\n-    \"flash_attention_2\": Idefics3VisionFlashAttention2,\n-}\n-\n-\n # Copied from transformers.models.siglip.modeling_siglip.SiglipMLP with Siglip->Idefics3Vision\n class Idefics3VisionMLP(nn.Module):\n     def __init__(self, config):\n@@ -400,7 +310,7 @@ class Idefics3EncoderLayer(nn.Module):\n     def __init__(self, config: Idefics3VisionConfig):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n-        self.self_attn = IDEFICS_VISION_ATTENTION_CLASSES[config._attn_implementation](config)\n+        self.self_attn = Idefics3VisionAttention(config)\n         self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n         self.mlp = Idefics3VisionMLP(config)\n         self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n@@ -620,6 +530,7 @@ class Idefics3PreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n \n     # Copied from transformers.models.idefics2.modeling_idefics2.Idefics2PreTrainedModel._init_weights\n@@ -666,7 +577,9 @@ def _init_weights(self, module):\n )\n class Idefics3VisionTransformer(Idefics3PreTrainedModel):\n     config_class = Idefics3VisionConfig\n-    _supports_sdpa = False\n+    _supports_sdpa = True\n+    _supports_flash_attention_2 = True\n+    _supports_flex_attn = True\n \n     def __init__(self, config: Idefics3VisionConfig):\n         super().__init__(config)"
        },
        {
            "sha": "ea1a7384f58b2b855f3b4b3ab96d334aff38b18e",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/3249c5dc1560dace3c31cdbe4797b6c878ab47de/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3249c5dc1560dace3c31cdbe4797b6c878ab47de/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=3249c5dc1560dace3c31cdbe4797b6c878ab47de",
            "patch": "@@ -150,12 +150,11 @@ class Phi4MultimodalVisionEncoderLayer(nn.Module):\n     def __init__(self, config: Phi4MultimodalVisionConfig):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n-        self.self_attn = Phi4MultimodalVisionAttention(config)\n         self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n-        self.mlp = Phi4MultimodalVisionMLP(config)\n+        self.self_attn = Phi4MultimodalVisionAttention(config)\n         self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n+        self.mlp = Phi4MultimodalVisionMLP(config)\n \n-    # Ignore copy\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "f60feb2ea8d157c95a3f2f8eb1cef5f49257ce1e",
            "filename": "src/transformers/models/siglip/modeling_siglip.py",
            "status": "modified",
            "additions": 52,
            "deletions": 200,
            "changes": 252,
            "blob_url": "https://github.com/huggingface/transformers/blob/3249c5dc1560dace3c31cdbe4797b6c878ab47de/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3249c5dc1560dace3c31cdbe4797b6c878ab47de/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py?ref=3249c5dc1560dace3c31cdbe4797b6c878ab47de",
            "patch": "@@ -17,7 +17,7 @@\n import math\n import warnings\n from dataclasses import dataclass\n-from typing import Any, Optional, Tuple\n+from typing import Any, Callable, Optional, Tuple, Union\n \n import numpy as np\n import torch\n@@ -28,9 +28,8 @@\n \n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n-from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...utils import (\n     ModelOutput,\n     add_start_docstrings,\n@@ -43,10 +42,6 @@\n from .configuration_siglip import SiglipConfig, SiglipTextConfig, SiglipVisionConfig\n \n \n-if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n-\n logger = logging.get_logger(__name__)\n \n # General docstring\n@@ -360,11 +355,33 @@ def forward(\n         return embeddings\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class SiglipAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    # Copied from transformers.models.clip.modeling_clip.CLIPAttention.__init__\n-    def __init__(self, config):\n+    def __init__(self, config: Union[SiglipVisionConfig, SiglipTextConfig]):\n         super().__init__()\n         self.config = config\n         self.embed_dim = config.hidden_size\n@@ -377,6 +394,7 @@ def __init__(self, config):\n             )\n         self.scale = self.head_dim**-0.5\n         self.dropout = config.attention_dropout\n+        self.is_causal = False\n \n         self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n         self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n@@ -391,130 +409,38 @@ def forward(\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n-        batch_size, q_len, _ = hidden_states.size()\n+        batch_size, seq_length, embed_dim = hidden_states.shape\n \n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n+        queries = self.q_proj(hidden_states)\n+        keys = self.k_proj(hidden_states)\n+        values = self.v_proj(hidden_states)\n \n-        query_states = query_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        queries = queries.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        keys = keys.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        values = values.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n \n-        k_v_seq_len = key_states.shape[-2]\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) * self.scale\n-\n-        if attn_weights.size() != (batch_size, self.num_heads, q_len, k_v_seq_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(batch_size, self.num_heads, q_len, k_v_seq_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        if attention_mask is not None:\n-            if attention_mask.size() != (batch_size, 1, q_len, k_v_seq_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(batch_size, 1, q_len, k_v_seq_len)}, but is {attention_mask.size()}\"\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n                 )\n-            attn_weights = attn_weights + attention_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (batch_size, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(batch_size, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.reshape(batch_size, q_len, self.embed_dim)\n-\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, attn_weights\n-\n-\n-class SiglipFlashAttention2(SiglipAttention):\n-    \"\"\"\n-    SiglipAttention flash attention module. This module inherits from `SiglipAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    is_causal = False\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n-\n-    # Adapted from transformers.models.llama.modeling_llama.LlamaFlashAttention2.forward\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.LongTensor] = None,\n-        output_attentions: bool = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        output_attentions = False\n-\n-        batch_size, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        # Flash attention requires the input to have the shape\n-        # batch_size x seq_length x head_dim x hidden_dim\n-        # therefore we just need to keep the original shape\n-        query_states = query_states.view(batch_size, q_len, self.num_heads, self.head_dim)\n-        key_states = key_states.view(batch_size, q_len, self.num_heads, self.head_dim)\n-        value_states = value_states.view(batch_size, q_len, self.num_heads, self.head_dim)\n-\n-        dropout_rate = self.dropout if self.training else 0.0\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32.\n-\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n             else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output = _flash_attention_forward(\n-            query_states,\n-            key_states,\n-            value_states,\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            queries,\n+            keys,\n+            values,\n             attention_mask,\n-            q_len,\n-            dropout=dropout_rate,\n             is_causal=self.is_causal,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n+            scaling=self.scale,\n+            dropout=0.0 if not self.training else self.dropout,\n         )\n \n-        attn_output = attn_output.reshape(batch_size, q_len, self.embed_dim).contiguous()\n+        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n         if not output_attentions:\n@@ -523,79 +449,6 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class SiglipSdpaAttention(SiglipAttention):\n-    \"\"\"\n-    Siglip attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `SiglipAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n-    \"\"\"\n-\n-    is_causal = False\n-\n-    # Adapted from SiglipAttention.forward and transformers.models.llama.modeling_llama.LlamaSdpaAttention.forward\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"SiglipModel is using SiglipSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n-            )\n-\n-        batch_size, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and attention_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if self.is_causal and q_len > 1 else False\n-\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=attention_mask,\n-            dropout_p=self.dropout if self.training else 0.0,\n-            is_causal=is_causal,\n-        )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(batch_size, q_len, self.embed_dim)\n-\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, None\n-\n-\n-SIGLIP_ATTENTION_CLASSES = {\n-    \"eager\": SiglipAttention,\n-    \"flash_attention_2\": SiglipFlashAttention2,\n-    \"sdpa\": SiglipSdpaAttention,\n-}\n-\n-\n # Copied from transformers.models.clip.modeling_clip.CLIPMLP with CLIP->Siglip\n class SiglipMLP(nn.Module):\n     def __init__(self, config):\n@@ -613,15 +466,14 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n \n \n class SiglipEncoderLayer(nn.Module):\n-    def __init__(self, config: SiglipConfig):\n+    def __init__(self, config: Union[SiglipVisionConfig, SiglipTextConfig]):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n-        self.self_attn = SIGLIP_ATTENTION_CLASSES[config._attn_implementation](config=config)\n         self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n-        self.mlp = SiglipMLP(config)\n+        self.self_attn = SiglipAttention(config)\n         self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n+        self.mlp = SiglipMLP(config)\n \n-    # Ignore copy\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "15d9cffe7d7471e17662e3ed9d4f96a7e4c0e972",
            "filename": "src/transformers/models/siglip2/modeling_siglip2.py",
            "status": "modified",
            "additions": 52,
            "deletions": 199,
            "changes": 251,
            "blob_url": "https://github.com/huggingface/transformers/blob/3249c5dc1560dace3c31cdbe4797b6c878ab47de/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3249c5dc1560dace3c31cdbe4797b6c878ab47de/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py?ref=3249c5dc1560dace3c31cdbe4797b6c878ab47de",
            "patch": "@@ -21,7 +21,7 @@\n import math\n import warnings\n from dataclasses import dataclass\n-from typing import Any, Optional, Tuple\n+from typing import Any, Callable, Optional, Tuple, Union\n \n import numpy as np\n import torch\n@@ -32,9 +32,8 @@\n \n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n-from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...utils import (\n     ModelOutput,\n     add_start_docstrings,\n@@ -46,10 +45,6 @@\n from .configuration_siglip2 import Siglip2Config, Siglip2TextConfig, Siglip2VisionConfig\n \n \n-if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n-\n logger = logging.get_logger(__name__)\n \n # General docstring\n@@ -252,10 +247,33 @@ def forward(self, pixel_values: torch.FloatTensor, spatial_shapes: torch.LongTen\n         return embeddings\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class Siglip2Attention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    def __init__(self, config):\n+    def __init__(self, config: Union[Siglip2VisionConfig, Siglip2TextConfig]):\n         super().__init__()\n         self.config = config\n         self.embed_dim = config.hidden_size\n@@ -268,6 +286,7 @@ def __init__(self, config):\n             )\n         self.scale = self.head_dim**-0.5\n         self.dropout = config.attention_dropout\n+        self.is_causal = False\n \n         self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n         self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n@@ -282,130 +301,38 @@ def forward(\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n-        batch_size, q_len, _ = hidden_states.size()\n+        batch_size, seq_length, embed_dim = hidden_states.shape\n \n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n+        queries = self.q_proj(hidden_states)\n+        keys = self.k_proj(hidden_states)\n+        values = self.v_proj(hidden_states)\n \n-        query_states = query_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        queries = queries.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        keys = keys.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        values = values.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n \n-        k_v_seq_len = key_states.shape[-2]\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) * self.scale\n-\n-        if attn_weights.size() != (batch_size, self.num_heads, q_len, k_v_seq_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(batch_size, self.num_heads, q_len, k_v_seq_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        if attention_mask is not None:\n-            if attention_mask.size() != (batch_size, 1, q_len, k_v_seq_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(batch_size, 1, q_len, k_v_seq_len)}, but is {attention_mask.size()}\"\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n                 )\n-            attn_weights = attn_weights + attention_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (batch_size, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(batch_size, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.reshape(batch_size, q_len, self.embed_dim)\n-\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, attn_weights\n-\n-\n-class Siglip2FlashAttention2(Siglip2Attention):\n-    \"\"\"\n-    Siglip2Attention flash attention module. This module inherits from `Siglip2Attention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    is_causal = False\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n-\n-    # Adapted from transformers.models.llama.modeling_llama.LlamaFlashAttention2.forward\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.LongTensor] = None,\n-        output_attentions: bool = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        output_attentions = False\n-\n-        batch_size, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        # Flash attention requires the input to have the shape\n-        # batch_size x seq_length x head_dim x hidden_dim\n-        # therefore we just need to keep the original shape\n-        query_states = query_states.view(batch_size, q_len, self.num_heads, self.head_dim)\n-        key_states = key_states.view(batch_size, q_len, self.num_heads, self.head_dim)\n-        value_states = value_states.view(batch_size, q_len, self.num_heads, self.head_dim)\n-\n-        dropout_rate = self.dropout if self.training else 0.0\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32.\n-\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n             else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output = _flash_attention_forward(\n-            query_states,\n-            key_states,\n-            value_states,\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            queries,\n+            keys,\n+            values,\n             attention_mask,\n-            q_len,\n-            dropout=dropout_rate,\n             is_causal=self.is_causal,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n+            scaling=self.scale,\n+            dropout=0.0 if not self.training else self.dropout,\n         )\n \n-        attn_output = attn_output.reshape(batch_size, q_len, self.embed_dim).contiguous()\n+        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n         if not output_attentions:\n@@ -414,72 +341,6 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class Siglip2SdpaAttention(Siglip2Attention):\n-    \"\"\"\n-    Siglip2 attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `Siglip2Attention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n-    \"\"\"\n-\n-    is_causal = False\n-\n-    # Adapted from Siglip2Attention.forward and transformers.models.llama.modeling_llama.LlamaSdpaAttention.forward\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"Siglip2Model is using Siglip2SdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n-            )\n-\n-        batch_size, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and attention_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if self.is_causal and q_len > 1 else False\n-\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=attention_mask,\n-            dropout_p=self.dropout if self.training else 0.0,\n-            is_causal=is_causal,\n-        )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(batch_size, q_len, self.embed_dim)\n-\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, None\n-\n-\n class Siglip2MLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -495,23 +356,15 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-SIGLIP2_ATTENTION_CLASSES = {\n-    \"eager\": Siglip2Attention,\n-    \"flash_attention_2\": Siglip2FlashAttention2,\n-    \"sdpa\": Siglip2SdpaAttention,\n-}\n-\n-\n class Siglip2EncoderLayer(nn.Module):\n-    def __init__(self, config: Siglip2Config):\n+    def __init__(self, config: Union[Siglip2VisionConfig, Siglip2TextConfig]):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n-        self.self_attn = SIGLIP2_ATTENTION_CLASSES[config._attn_implementation](config=config)\n         self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n-        self.mlp = Siglip2MLP(config)\n+        self.self_attn = Siglip2Attention(config)\n         self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n+        self.mlp = Siglip2MLP(config)\n \n-    # Ignore copy\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "f9cad6fa20fc781f25fe03340cad3cf84ec563ee",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 53,
            "deletions": 140,
            "changes": 193,
            "blob_url": "https://github.com/huggingface/transformers/blob/3249c5dc1560dace3c31cdbe4797b6c878ab47de/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3249c5dc1560dace3c31cdbe4797b6c878ab47de/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=3249c5dc1560dace3c31cdbe4797b6c878ab47de",
            "patch": "@@ -20,19 +20,18 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n from dataclasses import dataclass\n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache\n+from ...cache_utils import DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n-from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import BaseModelOutput, ModelOutput\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n@@ -43,10 +42,6 @@\n from .configuration_smolvlm import SmolVLMConfig, SmolVLMVisionConfig\n \n \n-if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n-\n logger = logging.get_logger(__name__)\n \n _CONFIG_FOR_DOC = \"SmolVLMConfig\"\n@@ -81,6 +76,7 @@ class SmolVLMPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n \n     def _init_weights(self, module):\n@@ -161,6 +157,29 @@ def forward(self, pixel_values: torch.FloatTensor, patch_attention_mask: torch.B\n         return embeddings\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class SmolVLMVisionAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -194,140 +213,38 @@ def forward(\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n-        batch_size, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n+        batch_size, seq_length, embed_dim = hidden_states.shape\n \n-        query_states = query_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        queries = self.q_proj(hidden_states)\n+        keys = self.k_proj(hidden_states)\n+        values = self.v_proj(hidden_states)\n \n-        k_v_seq_len = key_states.shape[-2]\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) * self.scale\n-\n-        if attn_weights.size() != (batch_size, self.num_heads, q_len, k_v_seq_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(batch_size, self.num_heads, q_len, k_v_seq_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n+        queries = queries.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        keys = keys.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        values = values.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n \n-        if attention_mask is not None:\n-            if attention_mask.size() != (batch_size, 1, q_len, k_v_seq_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(batch_size, 1, q_len, k_v_seq_len)}, but is {attention_mask.size()}\"\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n                 )\n-            attn_weights = attn_weights + attention_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (batch_size, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(batch_size, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.reshape(batch_size, q_len, self.embed_dim)\n-\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, attn_weights\n-\n-\n-class SmolVLMVisionFlashAttention2(SmolVLMVisionAttention):\n-    \"\"\"\n-    SmolVLMVision flash attention module. This module inherits from `SmolVLMVisionAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        **kwargs,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        output_attentions = False\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        # Flash attention requires the input to have the shape\n-        # batch_size x seq_length x head_dim x hidden_dim\n-        # therefore we just need to keep the original shape\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim)\n-        key_states = key_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-\n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n-\n-        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n-        # to be able to avoid many of these transpose/reshape/view.\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        dropout_rate = self.dropout if self.training else 0.0\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (SmolVLMVisionRMSNorm handles it correctly)\n-\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n             else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        attn_output = _flash_attention_forward(\n-            query_states,\n-            key_states,\n-            value_states,\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            queries,\n+            keys,\n+            values,\n             attention_mask,\n-            q_len,\n-            dropout=dropout_rate,\n             is_causal=self.is_causal,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n+            scaling=self.scale,\n+            dropout=0.0 if not self.training else self.dropout,\n         )\n \n-        attn_output = attn_output.reshape(bsz, q_len, self.embed_dim).contiguous()\n+        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n         if not output_attentions:\n@@ -351,17 +268,11 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-IDEFICS_VISION_ATTENTION_CLASSES = {\n-    \"eager\": SmolVLMVisionAttention,\n-    \"flash_attention_2\": SmolVLMVisionFlashAttention2,\n-}\n-\n-\n class SmolVLMEncoderLayer(nn.Module):\n     def __init__(self, config: SmolVLMVisionConfig):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n-        self.self_attn = IDEFICS_VISION_ATTENTION_CLASSES[config._attn_implementation](config)\n+        self.self_attn = SmolVLMVisionAttention(config)\n         self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n         self.mlp = SmolVLMVisionMLP(config)\n         self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n@@ -516,7 +427,9 @@ def forward(\n )\n class SmolVLMVisionTransformer(SmolVLMPreTrainedModel):\n     config_class = SmolVLMVisionConfig\n-    _supports_sdpa = False\n+    _supports_sdpa = True\n+    _supports_flash_attention_2 = True\n+    _supports_flex_attn = True\n \n     def __init__(self, config: SmolVLMVisionConfig):\n         super().__init__(config)"
        },
        {
            "sha": "98a0b2d50bb6bc477c9bac7a93c5f452a0e3e3b4",
            "filename": "tests/models/idefics2/test_modeling_idefics2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/3249c5dc1560dace3c31cdbe4797b6c878ab47de/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3249c5dc1560dace3c31cdbe4797b6c878ab47de/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py?ref=3249c5dc1560dace3c31cdbe4797b6c878ab47de",
            "patch": "@@ -344,17 +344,15 @@ def test_sdpa_can_dispatch_composite_models(self):\n                 model_sdpa = model_class.from_pretrained(tmpdirname)\n                 model_sdpa = model_sdpa.eval().to(torch_device)\n \n-                vision_attn = None if model.vision_model._supports_sdpa else \"eager\"\n-                perceiver_attn = None if model.connector.perceiver_resampler._supports_sdpa else \"eager\"\n                 self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n-                self.assertTrue(model_sdpa.vision_model.config._attn_implementation == vision_attn)\n-                self.assertTrue(model_sdpa.connector.perceiver_resampler.config._attn_implementation == perceiver_attn)\n+                self.assertTrue(model_sdpa.vision_model.config._attn_implementation == \"sdpa\")\n+                self.assertTrue(model_sdpa.connector.perceiver_resampler.config._attn_implementation == \"sdpa\")\n \n                 model_eager = model_class.from_pretrained(tmpdirname, attn_implementation=\"eager\")\n                 model_eager = model_eager.eval().to(torch_device)\n                 self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n                 self.assertTrue(model_eager.vision_model.config._attn_implementation == \"eager\")\n-                self.assertTrue(model_sdpa.connector.perceiver_resampler.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.connector.perceiver_resampler.config._attn_implementation == \"eager\")\n \n                 for name, submodule in model_eager.named_modules():\n                     class_name = submodule.__class__.__name__"
        },
        {
            "sha": "10d57a5aebbcac032ce147d13c37166a4b97fe07",
            "filename": "tests/models/siglip/test_modeling_siglip.py",
            "status": "modified",
            "additions": 14,
            "deletions": 216,
            "changes": 230,
            "blob_url": "https://github.com/huggingface/transformers/blob/3249c5dc1560dace3c31cdbe4797b6c878ab47de/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3249c5dc1560dace3c31cdbe4797b6c878ab47de/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py?ref=3249c5dc1560dace3c31cdbe4797b6c878ab47de",
            "patch": "@@ -18,7 +18,6 @@\n import os\n import tempfile\n import unittest\n-from typing import Tuple\n \n import numpy as np\n import requests\n@@ -27,30 +26,28 @@\n \n from transformers import SiglipConfig, SiglipTextConfig, SiglipVisionConfig\n from transformers.testing_utils import (\n+    is_flaky,\n     require_flash_attn,\n     require_torch,\n     require_torch_gpu,\n-    require_torch_sdpa,\n     require_vision,\n     slow,\n     torch_device,\n )\n from transformers.utils import (\n     is_torch_available,\n-    is_torch_bf16_available_on_device,\n-    is_torch_fp16_available_on_device,\n-    is_torch_sdpa_available,\n     is_vision_available,\n )\n \n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import (\n+    TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION,\n     ModelTesterMixin,\n     _config_zero_init,\n     floats_tensor,\n     ids_tensor,\n-    is_flaky,\n     random_attention_mask,\n+    require_torch_sdpa,\n )\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n@@ -61,16 +58,14 @@\n \n     from transformers import SiglipForImageClassification, SiglipModel, SiglipTextModel, SiglipVisionModel\n \n-if is_torch_sdpa_available():\n-    from torch.nn.attention import SDPBackend, sdpa_kernel\n-\n if is_vision_available():\n     from PIL import Image\n \n     from transformers import SiglipProcessor\n \n \n class SiglipModelTesterMixin(ModelTesterMixin):\n+    @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         for model_class in self.all_model_classes:\n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n@@ -81,171 +76,24 @@ def test_sdpa_can_dispatch_composite_models(self):\n \n                 # Load the model with SDPA\n                 model_sdpa = model_class.from_pretrained(tmpdirname)\n-                model_sdpa = model_sdpa.eval().to(torch_device)\n \n                 # Load model with eager attention\n                 model_eager = model_class.from_pretrained(\n                     tmpdirname,\n                     attn_implementation=\"eager\",\n                 )\n-                model_eager = model_eager.eval().to(torch_device)\n-\n-            # SigLip has one shared cls attr for all models, so we assign both submodels heer\n-            vision_attn = text_attn = \"sdpa\" if model._supports_sdpa else \"eager\"\n \n-            if hasattr(model_sdpa, \"vision_model\") and hasattr(model_sdpa, \"text_model\"):\n-                self.assertTrue(model_sdpa.vision_model.config._attn_implementation == vision_attn)\n-                self.assertTrue(model_sdpa.text_model.config._attn_implementation == text_attn)\n+            if hasattr(model_sdpa, \"vision_model\"):\n+                self.assertTrue(model_sdpa.vision_model.config._attn_implementation == \"sdpa\")\n                 self.assertTrue(model_eager.vision_model.config._attn_implementation == \"eager\")\n+\n+            if hasattr(model_sdpa, \"text_model\"):\n+                self.assertTrue(model_sdpa.text_model.config._attn_implementation == \"sdpa\")\n                 self.assertTrue(model_eager.text_model.config._attn_implementation == \"eager\")\n \n             self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n             self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n \n-            for name, submodule in model_eager.named_modules():\n-                class_name = submodule.__class__.__name__\n-                if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n-                    raise ValueError(\"The eager model should not have SDPA attention layers\")\n-\n-            has_sdpa = False\n-            for name, submodule in model_sdpa.named_modules():\n-                class_name = submodule.__class__.__name__\n-                if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n-                    has_sdpa = True\n-                    break\n-            if not has_sdpa and model_sdpa.config.model_type != \"falcon\":\n-                raise ValueError(\"The SDPA model should have SDPA attention layers\")\n-\n-    def test_eager_matches_sdpa_inference(\n-        self,\n-        torch_dtype: str,\n-        use_attention_mask_options: Tuple[bool, ...] = (True, False),\n-        logit_keys: Tuple[str, ...] = (\"logits_per_image\", \"logits_per_text\", \"image_embeds\", \"text_embeds\"),\n-    ):\n-        if not self.all_model_classes[0]._supports_sdpa:\n-            self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n-\n-        if torch_dtype == \"float16\" and not is_torch_fp16_available_on_device(torch_device):\n-            self.skipTest(f\"float16 not supported on {torch_device} (on the specific device currently used)\")\n-\n-        if torch_dtype == \"bfloat16\" and not is_torch_bf16_available_on_device(torch_device):\n-            self.skipTest(\n-                f\"bfloat16 not supported on {torch_device} (on the specific device currently used, e.g. Nvidia T4 GPU)\"\n-            )\n-\n-        # Convert to torch dtype\n-        dtypes = {\n-            \"float16\": torch.float16,\n-            \"bfloat16\": torch.bfloat16,\n-            \"float32\": torch.float32,\n-        }\n-        torch_dtype = dtypes[torch_dtype]\n-\n-        atols = {\n-            torch.float32: 1e-5,\n-            torch.bfloat16: 3e-2,\n-            torch.float16: 5e-3,\n-        }\n-        rtols = {\n-            torch.float32: 1e-4,\n-            torch.bfloat16: 3e-2,\n-            torch.float16: 5e-3,\n-        }\n-\n-        atol = atols[torch_dtype]\n-        rtol = rtols[torch_dtype]\n-\n-        def get_mean_reldiff(msg, current_case, x, ref, atol, rtol):\n-            return f\"{msg} {current_case}: mean relative difference: {((x - ref).abs() / (ref.abs() + 1e-12)).mean():.3e}, torch atol = {atol}, torch rtol = {rtol}\"\n-\n-        for model_class in self.all_model_classes:\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-\n-                # Load the model with SDPA\n-                model_sdpa = model_class.from_pretrained(tmpdirname, torch_dtype=torch_dtype)\n-                model_sdpa = model_sdpa.eval().to(torch_device)\n-\n-                # Load model with eager attention\n-                model_eager = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch_dtype,\n-                    attn_implementation=\"eager\",\n-                )\n-                model_eager = model_eager.eval().to(torch_device)\n-\n-            # We use these for loops instead of parameterized.expand just for the interest of avoiding loading/saving the model each time,\n-            # but it would be nicer to have an efficient way to use parameterized.expand\n-            cases = [\n-                (use_mask, output_attentions, sdpa_backend, batch_size)\n-                for use_mask in use_attention_mask_options\n-                for output_attentions in [True, False]\n-                for sdpa_backend in [\n-                    SDPBackend.MATH,\n-                    [SDPBackend.FLASH_ATTENTION, SDPBackend.MATH],\n-                    [SDPBackend.EFFICIENT_ATTENTION, SDPBackend.MATH],\n-                    [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION, SDPBackend.MATH],\n-                ]\n-                for batch_size in [1, 5]\n-            ]\n-            fail_cases = []\n-\n-            for use_mask, output_attentions, sdpa_backend, batch_size in cases:\n-                processed_inputs = inputs_dict.copy()\n-\n-                # convert to torch_dtype\n-                if \"pixel_values\" in processed_inputs:\n-                    processed_inputs[\"pixel_values\"] = processed_inputs[\"pixel_values\"].to(torch_dtype)\n-\n-                # slice for different batch sizes\n-                for key in [\"pixel_values\", \"input_ids\", \"attention_mask\"]:\n-                    if key in processed_inputs:\n-                        processed_inputs[key] = processed_inputs[key][:batch_size]\n-\n-                # set attention mask with left padding\n-                if not use_mask:\n-                    processed_inputs.pop(\"attention_mask\", None)\n-                else:\n-                    dummy_attention_mask = processed_inputs[\"attention_mask\"]\n-                    dummy_attention_mask[:] = 1\n-                    dummy_attention_mask[:, :1] = 0\n-                    processed_inputs[\"attention_mask\"] = dummy_attention_mask\n-\n-                processed_inputs[\"output_attentions\"] = output_attentions\n-                processed_inputs[\"output_hidden_states\"] = True\n-\n-                current_case = (\n-                    f\"padding_side=left, use_mask={use_mask}, batch_size={batch_size}, sdpa_backend={sdpa_backend}\"\n-                )\n-\n-                prepared_inputs = self._prepare_for_class(processed_inputs, model_class)\n-\n-                with torch.no_grad():\n-                    try:\n-                        with sdpa_kernel(sdpa_backend):\n-                            outputs_eager = model_eager(**prepared_inputs)\n-                            outputs_sdpa = model_sdpa(**prepared_inputs)\n-                    except Exception as e:\n-                        fail_cases.append(f\"{current_case}: {e}\")\n-                        continue\n-\n-                for key in logit_keys:\n-                    eager_logits = outputs_eager[key]\n-                    sdpa_logits = outputs_sdpa[key]\n-\n-                    if use_mask:\n-                        eager_logits = eager_logits[:, 1:]\n-                        sdpa_logits = sdpa_logits[:, 1:]\n-\n-                    is_close = torch.allclose(eager_logits, sdpa_logits, atol=atol, rtol=rtol)\n-                    if not is_close:\n-                        fail_cases.append(get_mean_reldiff(key, current_case, sdpa_logits, eager_logits, atol, rtol))\n-\n-            self.assertTrue(len(fail_cases) == 0, \"\\n\".join(fail_cases))\n-\n \n class SiglipVisionModelTester:\n     def __init__(\n@@ -409,20 +257,12 @@ def test_model_from_pretrained(self):\n         model = SiglipVisionModel.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n+    @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n     @require_torch_sdpa\n-    @slow\n     @is_flaky()\n-    def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n-        super().test_eager_matches_sdpa_inference(\n-            torch_dtype=torch_dtype,\n-            logit_keys=(\"pooler_output\", \"last_hidden_state\"),\n-            use_attention_mask_options=(False,),\n-        )\n-\n-    @require_torch_sdpa\n-    def test_sdpa_can_dispatch_composite_models(self):\n-        super().test_sdpa_can_dispatch_composite_models()\n+    def test_eager_matches_sdpa_inference(self, *args):\n+        # adding only flaky decorator here and call the parent test method\n+        return getattr(ModelTesterMixin, self._testMethodName)(self)\n \n \n class SiglipTextModelTester:\n@@ -565,21 +405,6 @@ def test_model_from_pretrained(self):\n         model = SiglipTextModel.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n-    @require_torch_sdpa\n-    @slow\n-    @is_flaky()\n-    def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n-        super().test_eager_matches_sdpa_inference(\n-            torch_dtype=torch_dtype,\n-            logit_keys=(\"pooler_output\", \"last_hidden_state\"),\n-            use_attention_mask_options=(False, True),\n-        )\n-\n-    @require_torch_sdpa\n-    def test_sdpa_can_dispatch_composite_models(self):\n-        super().test_sdpa_can_dispatch_composite_models()\n-\n \n class SiglipModelTester:\n     def __init__(self, parent, text_kwargs=None, vision_kwargs=None, is_training=True):\n@@ -634,6 +459,7 @@ def prepare_config_and_inputs_for_common(self):\n \n @require_torch\n class SiglipModelTest(SiglipModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    additional_model_inputs = [\"pixel_values\"]\n     all_model_classes = (SiglipModel,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"feature-extraction\": SiglipModel} if is_torch_available() else {}\n     fx_compatible = False\n@@ -862,21 +688,6 @@ def test_flash_attn_2_inference_equivalence(self):\n     def test_flash_attn_2_inference_equivalence_right_padding(self):\n         self.skipTest(\"SigLIP does not support right padding\")\n \n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n-    @require_torch_sdpa\n-    @slow\n-    @is_flaky()\n-    def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n-        super().test_eager_matches_sdpa_inference(\n-            torch_dtype=torch_dtype,\n-            logit_keys=(\"logits_per_image\", \"logits_per_text\", \"image_embeds\", \"text_embeds\"),\n-            use_attention_mask_options=(False, True),\n-        )\n-\n-    @require_torch_sdpa\n-    def test_sdpa_can_dispatch_composite_models(self):\n-        super().test_sdpa_can_dispatch_composite_models()\n-\n \n class SiglipForImageClassificationModelTester(SiglipModelTester):\n     def __init__(self, parent):\n@@ -943,19 +754,6 @@ def test_training_gradient_checkpointing_use_reentrant_false(self):\n     def test_initialization(self):\n         pass\n \n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n-    @require_torch_sdpa\n-    @slow\n-    @is_flaky()\n-    def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n-        super().test_eager_matches_sdpa_inference(\n-            torch_dtype=torch_dtype, logit_keys=(\"logits\",), use_attention_mask_options=(False,)\n-        )\n-\n-    @require_torch_sdpa\n-    def test_sdpa_can_dispatch_composite_models(self):\n-        super().test_sdpa_can_dispatch_composite_models()\n-\n \n # We will verify our results on an image of cute cats\n def prepare_img():"
        },
        {
            "sha": "885f57751ba46d807eb4567109489aa9de1b887e",
            "filename": "tests/models/siglip2/test_modeling_siglip2.py",
            "status": "modified",
            "additions": 20,
            "deletions": 216,
            "changes": 236,
            "blob_url": "https://github.com/huggingface/transformers/blob/3249c5dc1560dace3c31cdbe4797b6c878ab47de/tests%2Fmodels%2Fsiglip2%2Ftest_modeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3249c5dc1560dace3c31cdbe4797b6c878ab47de/tests%2Fmodels%2Fsiglip2%2Ftest_modeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsiglip2%2Ftest_modeling_siglip2.py?ref=3249c5dc1560dace3c31cdbe4797b6c878ab47de",
            "patch": "@@ -17,37 +17,34 @@\n import inspect\n import tempfile\n import unittest\n-from typing import Tuple\n \n import numpy as np\n from parameterized import parameterized\n from pytest import mark\n \n from transformers import Siglip2Config, Siglip2TextConfig, Siglip2VisionConfig\n from transformers.testing_utils import (\n+    is_flaky,\n     require_flash_attn,\n     require_torch,\n     require_torch_gpu,\n-    require_torch_sdpa,\n     require_vision,\n     slow,\n     torch_device,\n )\n from transformers.utils import (\n     is_torch_available,\n-    is_torch_bf16_available_on_device,\n-    is_torch_fp16_available_on_device,\n-    is_torch_sdpa_available,\n     is_vision_available,\n )\n \n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import (\n+    TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION,\n     ModelTesterMixin,\n     floats_tensor,\n     ids_tensor,\n-    is_flaky,\n     random_attention_mask,\n+    require_torch_sdpa,\n )\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n@@ -58,16 +55,14 @@\n \n     from transformers import Siglip2ForImageClassification, Siglip2Model, Siglip2TextModel, Siglip2VisionModel\n \n-if is_torch_sdpa_available():\n-    from torch.nn.attention import SDPBackend, sdpa_kernel\n-\n if is_vision_available():\n     from PIL import Image, ImageDraw\n \n     from transformers import Siglip2Processor\n \n \n class Siglip2ModelTesterMixin(ModelTesterMixin):\n+    @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         for model_class in self.all_model_classes:\n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n@@ -78,171 +73,24 @@ def test_sdpa_can_dispatch_composite_models(self):\n \n                 # Load the model with SDPA\n                 model_sdpa = model_class.from_pretrained(tmpdirname)\n-                model_sdpa = model_sdpa.eval().to(torch_device)\n \n                 # Load model with eager attention\n                 model_eager = model_class.from_pretrained(\n                     tmpdirname,\n                     attn_implementation=\"eager\",\n                 )\n-                model_eager = model_eager.eval().to(torch_device)\n-\n-            # SigLip has one shared cls attr for all models, so we assign both submodels heer\n-            vision_attn = text_attn = \"sdpa\" if model._supports_sdpa else \"eager\"\n \n-            if hasattr(model_sdpa, \"vision_model\") and hasattr(model_sdpa, \"text_model\"):\n-                self.assertTrue(model_sdpa.vision_model.config._attn_implementation == vision_attn)\n-                self.assertTrue(model_sdpa.text_model.config._attn_implementation == text_attn)\n+            if hasattr(model_sdpa, \"vision_model\"):\n+                self.assertTrue(model_sdpa.vision_model.config._attn_implementation == \"sdpa\")\n                 self.assertTrue(model_eager.vision_model.config._attn_implementation == \"eager\")\n+\n+            if hasattr(model_sdpa, \"text_model\"):\n+                self.assertTrue(model_sdpa.text_model.config._attn_implementation == \"sdpa\")\n                 self.assertTrue(model_eager.text_model.config._attn_implementation == \"eager\")\n \n             self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n             self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n \n-            for name, submodule in model_eager.named_modules():\n-                class_name = submodule.__class__.__name__\n-                if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n-                    raise ValueError(\"The eager model should not have SDPA attention layers\")\n-\n-            has_sdpa = False\n-            for name, submodule in model_sdpa.named_modules():\n-                class_name = submodule.__class__.__name__\n-                if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n-                    has_sdpa = True\n-                    break\n-            if not has_sdpa and model_sdpa.config.model_type != \"falcon\":\n-                raise ValueError(\"The SDPA model should have SDPA attention layers\")\n-\n-    def test_eager_matches_sdpa_inference(\n-        self,\n-        torch_dtype: str,\n-        use_attention_mask_options: Tuple[bool, ...] = (True, False),\n-        logit_keys: Tuple[str, ...] = (\"logits_per_image\", \"logits_per_text\", \"image_embeds\", \"text_embeds\"),\n-    ):\n-        if not self.all_model_classes[0]._supports_sdpa:\n-            self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n-\n-        if torch_dtype == \"float16\" and not is_torch_fp16_available_on_device(torch_device):\n-            self.skipTest(f\"float16 not supported on {torch_device} (on the specific device currently used)\")\n-\n-        if torch_dtype == \"bfloat16\" and not is_torch_bf16_available_on_device(torch_device):\n-            self.skipTest(\n-                f\"bfloat16 not supported on {torch_device} (on the specific device currently used, e.g. Nvidia T4 GPU)\"\n-            )\n-\n-        # Convert to torch dtype\n-        dtypes = {\n-            \"float16\": torch.float16,\n-            \"bfloat16\": torch.bfloat16,\n-            \"float32\": torch.float32,\n-        }\n-        torch_dtype = dtypes[torch_dtype]\n-\n-        atols = {\n-            torch.float32: 1e-5,\n-            torch.bfloat16: 3e-2,\n-            torch.float16: 5e-3,\n-        }\n-        rtols = {\n-            torch.float32: 1e-4,\n-            torch.bfloat16: 3e-2,\n-            torch.float16: 5e-3,\n-        }\n-\n-        atol = atols[torch_dtype]\n-        rtol = rtols[torch_dtype]\n-\n-        def get_mean_reldiff(msg, current_case, x, ref, atol, rtol):\n-            return f\"{msg} {current_case}: mean relative difference: {((x - ref).abs() / (ref.abs() + 1e-12)).mean():.3e}, torch atol = {atol}, torch rtol = {rtol}\"\n-\n-        for model_class in self.all_model_classes:\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-\n-                # Load the model with SDPA\n-                model_sdpa = model_class.from_pretrained(tmpdirname, torch_dtype=torch_dtype)\n-                model_sdpa = model_sdpa.eval().to(torch_device)\n-\n-                # Load model with eager attention\n-                model_eager = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch_dtype,\n-                    attn_implementation=\"eager\",\n-                )\n-                model_eager = model_eager.eval().to(torch_device)\n-\n-            # We use these for loops instead of parameterized.expand just for the interest of avoiding loading/saving the model each time,\n-            # but it would be nicer to have an efficient way to use parameterized.expand\n-            cases = [\n-                (use_mask, output_attentions, sdpa_backend, batch_size)\n-                for use_mask in use_attention_mask_options\n-                for output_attentions in [True, False]\n-                for sdpa_backend in [\n-                    SDPBackend.MATH,\n-                    [SDPBackend.FLASH_ATTENTION, SDPBackend.MATH],\n-                    [SDPBackend.EFFICIENT_ATTENTION, SDPBackend.MATH],\n-                    [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION, SDPBackend.MATH],\n-                ]\n-                for batch_size in [1, 5]\n-            ]\n-            fail_cases = []\n-\n-            for use_mask, output_attentions, sdpa_backend, batch_size in cases:\n-                processed_inputs = inputs_dict.copy()\n-\n-                # convert to torch_dtype\n-                if \"pixel_values\" in processed_inputs:\n-                    processed_inputs[\"pixel_values\"] = processed_inputs[\"pixel_values\"].to(torch_dtype)\n-\n-                # slice for different batch sizes\n-                for key in processed_inputs.keys():\n-                    if isinstance(processed_inputs[key], (torch.Tensor, list, tuple)):\n-                        processed_inputs[key] = processed_inputs[key][:batch_size]\n-\n-                # set attention mask with left padding\n-                if not use_mask:\n-                    processed_inputs.pop(\"attention_mask\", None)\n-                else:\n-                    dummy_attention_mask = processed_inputs[\"attention_mask\"]\n-                    dummy_attention_mask[:] = 1\n-                    dummy_attention_mask[:, :1] = 0\n-                    processed_inputs[\"attention_mask\"] = dummy_attention_mask\n-\n-                processed_inputs[\"output_attentions\"] = output_attentions\n-                processed_inputs[\"output_hidden_states\"] = True\n-\n-                current_case = (\n-                    f\"padding_side=left, use_mask={use_mask}, batch_size={batch_size}, sdpa_backend={sdpa_backend}\"\n-                )\n-\n-                prepared_inputs = self._prepare_for_class(processed_inputs, model_class)\n-\n-                with torch.no_grad():\n-                    try:\n-                        with sdpa_kernel(sdpa_backend):\n-                            outputs_eager = model_eager(**prepared_inputs)\n-                            outputs_sdpa = model_sdpa(**prepared_inputs)\n-                    except Exception as e:\n-                        fail_cases.append(f\"{current_case}: {e}\")\n-                        continue\n-\n-                for key in logit_keys:\n-                    eager_logits = outputs_eager[key]\n-                    sdpa_logits = outputs_sdpa[key]\n-\n-                    if use_mask:\n-                        eager_logits = eager_logits[:, 1:]\n-                        sdpa_logits = sdpa_logits[:, 1:]\n-\n-                    is_close = torch.allclose(eager_logits, sdpa_logits, atol=atol, rtol=rtol)\n-                    if not is_close:\n-                        fail_cases.append(get_mean_reldiff(key, current_case, sdpa_logits, eager_logits, atol, rtol))\n-\n-            self.assertTrue(len(fail_cases) == 0, \"\\n\".join(fail_cases))\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @mark.flash_attn_test\n@@ -422,6 +270,7 @@ class Siglip2VisionModelTest(Siglip2ModelTesterMixin, unittest.TestCase):\n     \"\"\"\n \n     all_model_classes = (Siglip2VisionModel,) if is_torch_available() else ()\n+    additional_model_inputs = [\"pixel_attention_mask\", \"spatial_shapes\"]\n     fx_compatible = False\n     test_pruning = False\n     test_resize_embeddings = False\n@@ -497,20 +346,12 @@ def test_model_from_pretrained(self):\n         model = Siglip2VisionModel.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n+    @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n     @require_torch_sdpa\n-    @slow\n     @is_flaky()\n-    def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n-        super().test_eager_matches_sdpa_inference(\n-            torch_dtype=torch_dtype,\n-            logit_keys=(\"pooler_output\", \"last_hidden_state\"),\n-            use_attention_mask_options=(False,),\n-        )\n-\n-    @require_torch_sdpa\n-    def test_sdpa_can_dispatch_composite_models(self):\n-        super().test_sdpa_can_dispatch_composite_models()\n+    def test_eager_matches_sdpa_inference(self, *args):\n+        # adding only flaky decorator here and call the parent test method\n+        return getattr(ModelTesterMixin, self._testMethodName)(self)\n \n \n class Siglip2TextModelTester:\n@@ -648,21 +489,6 @@ def test_model_from_pretrained(self):\n         model = Siglip2TextModel.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n-    @require_torch_sdpa\n-    @slow\n-    @is_flaky()\n-    def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n-        super().test_eager_matches_sdpa_inference(\n-            torch_dtype=torch_dtype,\n-            logit_keys=(\"pooler_output\", \"last_hidden_state\"),\n-            use_attention_mask_options=(False, True),\n-        )\n-\n-    @require_torch_sdpa\n-    def test_sdpa_can_dispatch_composite_models(self):\n-        super().test_sdpa_can_dispatch_composite_models()\n-\n \n class Siglip2ModelTester:\n     def __init__(self, parent, text_kwargs=None, vision_kwargs=None, is_training=True):\n@@ -725,6 +551,11 @@ def prepare_config_and_inputs_for_common(self):\n class Siglip2ModelTest(Siglip2ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (Siglip2Model,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"feature-extraction\": Siglip2Model} if is_torch_available() else {}\n+    additional_model_inputs = [\n+        \"pixel_values\",\n+        \"pixel_attention_mask\",\n+        \"spatial_shapes\",\n+    ]\n     fx_compatible = False\n     test_head_masking = False\n     test_pruning = False\n@@ -796,21 +627,6 @@ def test_model_from_pretrained(self):\n     def test_flash_attn_2_inference_equivalence_right_padding(self):\n         self.skipTest(\"Siglip2 does not support right padding\")\n \n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n-    @require_torch_sdpa\n-    @slow\n-    @is_flaky()\n-    def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n-        super().test_eager_matches_sdpa_inference(\n-            torch_dtype=torch_dtype,\n-            logit_keys=(\"logits_per_image\", \"logits_per_text\", \"image_embeds\", \"text_embeds\"),\n-            use_attention_mask_options=(False, True),\n-        )\n-\n-    @require_torch_sdpa\n-    def test_sdpa_can_dispatch_composite_models(self):\n-        super().test_sdpa_can_dispatch_composite_models()\n-\n \n class Siglip2ForImageClassificationModelTester(Siglip2ModelTester):\n     def __init__(self, parent):\n@@ -841,6 +657,7 @@ def prepare_config_and_inputs_for_common(self):\n class Siglip2ForImageClassificationModelTest(Siglip2ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (Siglip2ForImageClassification,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"image-classification\": Siglip2ForImageClassification} if is_torch_available() else {}\n+    additional_model_inputs = [\"pixel_values\", \"pixel_attention_mask\", \"spatial_shapes\"]\n     fx_compatible = False\n     test_head_masking = False\n     test_pruning = False\n@@ -881,19 +698,6 @@ def test_training_gradient_checkpointing_use_reentrant_false(self):\n     def test_initialization(self):\n         pass\n \n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n-    @require_torch_sdpa\n-    @slow\n-    @is_flaky()\n-    def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n-        super().test_eager_matches_sdpa_inference(\n-            torch_dtype=torch_dtype, logit_keys=(\"logits\",), use_attention_mask_options=(False,)\n-        )\n-\n-    @require_torch_sdpa\n-    def test_sdpa_can_dispatch_composite_models(self):\n-        super().test_sdpa_can_dispatch_composite_models()\n-\n \n # Draw a circle on an images with different aspect ratios\n def prepare_images():"
        },
        {
            "sha": "ce8921b3337b52c8fe93474a7583695eb2a17ac6",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 183,
            "deletions": 176,
            "changes": 359,
            "blob_url": "https://github.com/huggingface/transformers/blob/3249c5dc1560dace3c31cdbe4797b6c878ab47de/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3249c5dc1560dace3c31cdbe4797b6c878ab47de/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=3249c5dc1560dace3c31cdbe4797b6c878ab47de",
            "patch": "@@ -3457,7 +3457,7 @@ def test_eager_matches_sdpa_inference(\n     ):\n         # TODO: we shouldn't need to do this skip, i.e. the test would be composable from the model tester. CLIP-like\n         # models have a custom mixin, which we detect to skip this test.\n-        if not any(\".ModelTesterMixin\" in str(base) for base in self.__class__.__bases__):\n+        if any(\".CLIPModelTesterMixin\" in str(base) for base in self.__class__.__bases__):\n             self.skipTest(reason=\"CLIP-like models have a different `test_eager_matches_sdpa_inference`\")\n \n         if not self.has_attentions:\n@@ -3549,206 +3549,213 @@ def test_eager_matches_sdpa_inference(\n                 model_eager = model_class.from_pretrained(**model_from_pretrained_kwargs, attn_implementation=\"eager\")\n                 model_eager = model_eager.eval().to(torch_device, dtype=torch_dtype)\n \n-                set_model_for_less_flaky_test(model_eager)\n-                set_model_for_less_flaky_test(model_sdpa)\n+            set_model_for_less_flaky_test(model_eager)\n+            set_model_for_less_flaky_test(model_sdpa)\n \n-                can_output_attn = \"output_attentions\" in inspect.signature(model_sdpa.forward).parameters\n-                if not (self.has_attentions and can_output_attn) and output_attentions:\n-                    self.skipTest(reason=\"Model does not support output_attentions\")\n+            can_output_attn = \"output_attentions\" in inspect.signature(model_sdpa.forward).parameters\n+            if not (self.has_attentions and can_output_attn) and output_attentions:\n+                self.skipTest(reason=\"Model does not support output_attentions\")\n \n-                # TODO: if we can also check with `batch_size=1` without being flaky?\n-                for batch_size in [7]:\n-                    # musicgen decoder models; TODO: find better abstraction\n-                    if hasattr(self.model_tester, \"num_codebooks\") and not hasattr(model_eager, \"text_encoder\"):\n-                        input_data_batch_size = batch_size * self.model_tester.num_codebooks\n-                    else:\n-                        input_data_batch_size = batch_size\n+            # TODO: if we can also check with `batch_size=1` without being flaky?\n+            for batch_size in [7]:\n+                # musicgen decoder models; TODO: find better abstraction\n+                if hasattr(self.model_tester, \"num_codebooks\") and not hasattr(model_eager, \"text_encoder\"):\n+                    input_data_batch_size = batch_size * self.model_tester.num_codebooks\n+                else:\n+                    input_data_batch_size = batch_size\n \n-                    dummy_input = inputs_dict[model.main_input_name]\n+                processed_inputs = {}\n+                processed_inputs[model.main_input_name] = inputs_dict[model.main_input_name]\n \n-                    if dummy_input.dtype in [torch.float32, torch.bfloat16, torch.float16]:\n-                        dummy_input = dummy_input.to(torch_dtype)\n+                for key in getattr(self, \"additional_model_inputs\", []):\n+                    processed_inputs[key] = inputs_dict[key]\n \n-                    dummy_input = dummy_input[:input_data_batch_size]\n-                    if dummy_input.shape[0] != input_data_batch_size:\n-                        if dummy_input.dtype in [torch.float32, torch.bfloat16, torch.float16]:\n-                            extension = torch.rand(\n-                                input_data_batch_size - dummy_input.shape[0],\n-                                *dummy_input.shape[1:],\n-                                dtype=torch_dtype,\n-                                device=torch_device,\n-                            )\n-                            dummy_input = torch.cat((dummy_input, extension), dim=0).to(torch_device)\n+                for key, value in processed_inputs.items():\n+                    if torch.is_floating_point(value):\n+                        value = value.to(torch_dtype)\n+\n+                    # extend value to have at least `input_data_batch_size` elements\n+                    if value.shape[0] < input_data_batch_size:\n+                        size = (input_data_batch_size - value.shape[0], *value.shape[1:])\n+                        if torch.is_floating_point(value):\n+                            extension = torch.rand(size=size, dtype=value.dtype, device=torch_device)\n                         else:\n-                            extension = torch.randint(\n-                                high=5,\n-                                size=(input_data_batch_size - dummy_input.shape[0], *dummy_input.shape[1:]),\n-                                dtype=dummy_input.dtype,\n-                                device=torch_device,\n-                            )\n-                            dummy_input = torch.cat((dummy_input, extension), dim=0).to(torch_device)\n+                            extension = torch.randint(high=5, size=size, dtype=value.dtype, device=torch_device)\n+                        value = torch.cat((value, extension), dim=0).to(torch_device)\n \n-                    if not use_attention_mask:\n-                        dummy_attention_mask = None\n-                    else:\n-                        dummy_attention_mask = inputs_dict.get(\"attention_mask\", None)\n-                        if dummy_attention_mask is None:\n-                            if is_encoder_decoder:\n-                                seqlen = inputs_dict.get(\"decoder_input_ids\", dummy_input).shape[-1]\n-                            else:\n-                                seqlen = dummy_input.shape[-1]\n-                            dummy_attention_mask = torch.ones(batch_size, seqlen).to(torch.int64).to(torch_device)\n-\n-                        dummy_attention_mask = dummy_attention_mask[:batch_size]\n-                        if dummy_attention_mask.shape[0] != batch_size:\n-                            extension = torch.ones(\n-                                batch_size - dummy_attention_mask.shape[0],\n-                                *dummy_attention_mask.shape[1:],\n-                                dtype=dummy_attention_mask.dtype,\n-                                device=torch_device,\n-                            )\n-                            dummy_attention_mask = torch.cat((dummy_attention_mask, extension), dim=0)\n-                            dummy_attention_mask = dummy_attention_mask.to(torch_device)\n-\n-                        dummy_attention_mask[:] = 1\n-                        if padding_side == \"left\":\n-                            dummy_attention_mask[-1, :2] = 0\n-                            dummy_attention_mask[-1, 2:] = 1\n-                        elif padding_side == \"right\":\n-                            dummy_attention_mask[-1, -2:] = 0\n-                            dummy_attention_mask[-1, :-2] = 1\n-\n-                    if is_encoder_decoder:\n-                        # musicgen encoder-decoder models; TODO: find better abstraction\n-                        if hasattr(self.model_tester, \"num_codebooks\"):\n-                            input_data_batch_size = batch_size * self.model_tester.num_codebooks\n+                    processed_inputs[key] = value[:input_data_batch_size]\n+\n+                if not use_attention_mask:\n+                    dummy_attention_mask = None\n+                else:\n+                    dummy_attention_mask = inputs_dict.get(\"attention_mask\", None)\n+                    if dummy_attention_mask is None:\n+                        if is_encoder_decoder:\n+                            seqlen = inputs_dict.get(\n+                                \"decoder_input_ids\", processed_inputs[model.main_input_name]\n+                            ).shape[-1]\n                         else:\n-                            input_data_batch_size = batch_size\n-\n-                        decoder_input_ids = inputs_dict.get(\"decoder_input_ids\", dummy_input)[:input_data_batch_size]\n-                        if decoder_input_ids.shape[0] != input_data_batch_size:\n-                            extension = torch.ones(\n-                                input_data_batch_size - decoder_input_ids.shape[0],\n-                                *decoder_input_ids.shape[1:],\n-                                dtype=decoder_input_ids.dtype,\n-                                device=torch_device,\n-                            )\n-                            decoder_input_ids = torch.cat((decoder_input_ids, extension), dim=0)\n-                            decoder_input_ids = decoder_input_ids.to(torch_device)\n+                            seqlen = processed_inputs[model.main_input_name].shape[-1]\n+                        dummy_attention_mask = torch.ones(batch_size, seqlen).to(torch.int64).to(torch_device)\n+\n+                    # extend dummy_attention_mask to have at least `batch_size` elements\n+                    if dummy_attention_mask.shape[0] < batch_size:\n+                        size = (batch_size - dummy_attention_mask.shape[0], *dummy_attention_mask.shape[1:])\n+                        extension = torch.ones(size=size, dtype=dummy_attention_mask.dtype, device=torch_device)\n+                        dummy_attention_mask = torch.cat((dummy_attention_mask, extension), dim=0)\n+\n+                    dummy_attention_mask = dummy_attention_mask[:batch_size].to(torch_device)\n+\n+                    dummy_attention_mask[:] = 1\n+                    if padding_side == \"left\":\n+                        dummy_attention_mask[-1, :2] = 0\n+                        dummy_attention_mask[-1, 2:] = 1\n+                    elif padding_side == \"right\":\n+                        dummy_attention_mask[-1, -2:] = 0\n+                        dummy_attention_mask[-1, :-2] = 1\n+\n+                if is_encoder_decoder:\n+                    # musicgen encoder-decoder models; TODO: find better abstraction\n+                    if hasattr(self.model_tester, \"num_codebooks\"):\n+                        input_data_batch_size = batch_size * self.model_tester.num_codebooks\n+                    else:\n+                        input_data_batch_size = batch_size\n \n-                        # TODO: never an `attention_mask` arg here?\n-                        processed_inputs = {\n-                            model.main_input_name: dummy_input,\n+                    decoder_input_ids = inputs_dict.get(\"decoder_input_ids\", processed_inputs[model.main_input_name])\n+                    decoder_input_ids = decoder_input_ids[:input_data_batch_size]\n+                    if decoder_input_ids.shape[0] != input_data_batch_size:\n+                        extension = torch.ones(\n+                            input_data_batch_size - decoder_input_ids.shape[0],\n+                            *decoder_input_ids.shape[1:],\n+                            dtype=decoder_input_ids.dtype,\n+                            device=torch_device,\n+                        )\n+                        decoder_input_ids = torch.cat((decoder_input_ids, extension), dim=0)\n+                        decoder_input_ids = decoder_input_ids.to(torch_device)\n+\n+                    # TODO: never an `attention_mask` arg here?\n+                    processed_inputs.update(\n+                        {\n                             \"decoder_input_ids\": decoder_input_ids,\n                             \"decoder_attention_mask\": dummy_attention_mask,\n                             \"output_hidden_states\": True,\n                         }\n-                    else:\n-                        processed_inputs = {\n-                            model.main_input_name: dummy_input,\n+                    )\n+                else:\n+                    processed_inputs.update(\n+                        {\n                             \"output_hidden_states\": True,\n                         }\n+                    )\n \n-                        # Otherwise fails for e.g. WhisperEncoderModel\n-                        if \"attention_mask\" in inspect.signature(model_eager.forward).parameters:\n-                            processed_inputs[\"attention_mask\"] = dummy_attention_mask\n+                    # Otherwise fails for e.g. WhisperEncoderModel\n+                    if \"attention_mask\" in inspect.signature(model_eager.forward).parameters:\n+                        processed_inputs[\"attention_mask\"] = dummy_attention_mask\n \n-                        if (\n-                            self.has_attentions\n-                            and \"output_attentions\" in inspect.signature(model_sdpa.forward).parameters\n-                        ):\n-                            processed_inputs[\"output_attentions\"] = output_attentions\n-                    if \"bool_masked_pos\" in inspect.signature(model_eager.forward).parameters:\n-                        dummy_mask = torch.ones((self.model_tester.num_masks,))\n-\n-                        # In case of additional token (like class) we define a custom `mask_length`\n-                        if hasattr(self.model_tester, \"mask_length\"):\n-                            mask_length = self.model_tester.mask_length - dummy_mask.size(0)\n-                        else:\n-                            mask_length = self.model_tester.seq_length - dummy_mask.size(0)\n-                        dummy_mask = torch.cat([dummy_mask, torch.zeros(mask_length)])\n-                        dummy_bool_masked_pos = dummy_mask.expand(batch_size, -1).bool()\n-                        processed_inputs[\"bool_masked_pos\"] = dummy_bool_masked_pos.to(torch_device)\n-\n-                    if \"noise\" in inspect.signature(model_eager.forward).parameters:\n-                        np.random.seed(2)\n-                        num_patches = int((self.model_tester.image_size // self.model_tester.patch_size) ** 2)\n-                        noise = np.random.uniform(size=(batch_size, num_patches))\n-                        processed_inputs[\"noise\"] = torch.from_numpy(noise)\n-\n-                    # TODO: test gradients as well (& for FA2 as well!)\n-                    with torch.no_grad():\n-                        with sdpa_kernel(\n-                            enable_flash=enable_kernels,\n-                            enable_math=True,\n-                            enable_mem_efficient=enable_kernels,\n-                        ):\n-                            prepared_inputs = self._prepare_for_class(processed_inputs, model_class)\n-                            outputs_eager = model_eager(**prepared_inputs)\n-                            outputs_sdpa = model_sdpa(**prepared_inputs)\n-\n-                    # TODO: rename logits -> hidden_states\n-                    if hasattr(outputs_eager, \"vision_hidden_states\"):\n-                        logits_eager = outputs_eager.vision_hidden_states[-1]\n-                        logits_sdpa = outputs_sdpa.vision_hidden_states[-1]\n-                    elif hasattr(outputs_eager, \"audio_values\"):\n-                        logits_eager = outputs_eager.audio_values\n-                        logits_sdpa = outputs_sdpa.audio_values\n-                    else:\n-                        logits_eager = (\n-                            outputs_eager.decoder_hidden_states[-1]\n-                            if hasattr(outputs_eager, \"decoder_hidden_states\")\n-                            else outputs_eager.hidden_states[-1]\n-                        )\n-                        logits_sdpa = (\n-                            outputs_sdpa.decoder_hidden_states[-1]\n-                            if hasattr(outputs_sdpa, \"decoder_hidden_states\")\n-                            else outputs_sdpa.hidden_states[-1]\n-                        )\n+                    if self.has_attentions and \"output_attentions\" in inspect.signature(model_sdpa.forward).parameters:\n+                        processed_inputs[\"output_attentions\"] = output_attentions\n+                if \"bool_masked_pos\" in inspect.signature(model_eager.forward).parameters:\n+                    dummy_mask = torch.ones((self.model_tester.num_masks,))\n \n-                    if torch_device in [\"cpu\", \"cuda\"]:\n-                        atol = atols[torch_device, enable_kernels, torch_dtype]\n-                        rtol = rtols[torch_device, enable_kernels, torch_dtype]\n-                    elif torch_device == \"xpu\":\n-                        # As of PyTorch 2.5 XPU backend supports only torch.nn.attention.SDPBackend.MATH\n-                        # which is implemented on PyTorch level using aten operators and is\n-                        # device agnostic with respect to implementation of each aten operator.\n-                        atol = atols[\"cuda\", False, torch_dtype]\n-                        rtol = rtols[\"cuda\", False, torch_dtype]\n+                    # In case of additional token (like class) we define a custom `mask_length`\n+                    if hasattr(self.model_tester, \"mask_length\"):\n+                        mask_length = self.model_tester.mask_length - dummy_mask.size(0)\n                     else:\n-                        atol = 1e-7\n-                        rtol = 1e-4\n+                        mask_length = self.model_tester.seq_length - dummy_mask.size(0)\n+                    dummy_mask = torch.cat([dummy_mask, torch.zeros(mask_length)])\n+                    dummy_bool_masked_pos = dummy_mask.expand(batch_size, -1).bool()\n+                    processed_inputs[\"bool_masked_pos\"] = dummy_bool_masked_pos.to(torch_device)\n+\n+                if \"noise\" in inspect.signature(model_eager.forward).parameters:\n+                    np.random.seed(2)\n+                    num_patches = int((self.model_tester.image_size // self.model_tester.patch_size) ** 2)\n+                    noise = np.random.uniform(size=(batch_size, num_patches))\n+                    processed_inputs[\"noise\"] = torch.from_numpy(noise)\n+\n+                # TODO: test gradients as well (& for FA2 as well!)\n+                with torch.no_grad():\n+                    with sdpa_kernel(\n+                        enable_flash=enable_kernels,\n+                        enable_math=True,\n+                        enable_mem_efficient=enable_kernels,\n+                    ):\n+                        prepared_inputs = self._prepare_for_class(processed_inputs, model_class)\n+                        prepared_inputs = {\n+                            k: v.to(torch_device) if isinstance(v, torch.Tensor) else v\n+                            for k, v in prepared_inputs.items()\n+                        }\n+                        outputs_eager = model_eager(**prepared_inputs)\n+                        outputs_sdpa = model_sdpa(**prepared_inputs)\n+\n+                if \"logits_per_text\" in outputs_eager:\n+                    key = \"logits_per_text\"\n+                elif \"vision_hidden_states\" in outputs_eager:\n+                    key = \"vision_hidden_states\"\n+                elif \"audio_values\" in outputs_eager:\n+                    key = \"audio_values\"\n+                elif \"decoder_hidden_states\" in outputs_eager:\n+                    key = \"decoder_hidden_states\"\n+                elif \"logits\" in outputs_eager and \"Classification\" in model_class.__name__:\n+                    key = \"logits\"\n+                else:\n+                    key = \"hidden_states\"\n+\n+                # TODO: rename logits -> hidden_states\n+                logits_eager = outputs_eager[key]\n+                logits_sdpa = outputs_sdpa[key]\n+\n+                if key in [\"vision_hidden_states\", \"decoder_hidden_states\", \"hidden_states\"]:\n+                    logits_eager = logits_eager[-1]\n+                    logits_sdpa = logits_sdpa[-1]\n+\n+                if key == \"logits_per_text\":\n+                    nan_mask = torch.isnan(logits_eager)\n+                    logits_eager[nan_mask] = 0\n+                    logits_sdpa[nan_mask] = 0\n+\n+                if torch_device in [\"cpu\", \"cuda\"]:\n+                    atol = atols[torch_device, enable_kernels, torch_dtype]\n+                    rtol = rtols[torch_device, enable_kernels, torch_dtype]\n+                elif torch_device == \"xpu\":\n+                    # As of PyTorch 2.5 XPU backend supports only torch.nn.attention.SDPBackend.MATH\n+                    # which is implemented on PyTorch level using aten operators and is\n+                    # device agnostic with respect to implementation of each aten operator.\n+                    atol = atols[\"cuda\", False, torch_dtype]\n+                    rtol = rtols[\"cuda\", False, torch_dtype]\n+                else:\n+                    atol = 1e-7\n+                    rtol = 1e-4\n \n-                    # Masked tokens output slightly deviates - we don't mind that.\n-                    if use_attention_mask:\n-                        _logits_sdpa = torch.zeros_like(input=logits_sdpa)\n-                        _logits_eager = torch.zeros_like(input=logits_eager)\n+                # Masked tokens output slightly deviates - we don't mind that.\n+                if use_attention_mask:\n+                    _logits_sdpa = torch.zeros_like(input=logits_sdpa)\n+                    _logits_eager = torch.zeros_like(input=logits_eager)\n \n-                        _logits_sdpa[:-1] = logits_sdpa[:-1]\n-                        _logits_eager[:-1] = logits_eager[:-1]\n+                    _logits_sdpa[:-1] = logits_sdpa[:-1]\n+                    _logits_eager[:-1] = logits_eager[:-1]\n \n-                        if padding_side == \"left\":\n-                            _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, 2:]\n-                            _logits_eager[-1:, 2:] = logits_eager[-1:, 2:]\n+                    if padding_side == \"left\":\n+                        _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, 2:]\n+                        _logits_eager[-1:, 2:] = logits_eager[-1:, 2:]\n \n-                        elif padding_side == \"right\":\n-                            _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, :-2]\n-                            _logits_eager[-1:, 2:] = logits_eager[-1:, :-2]\n+                    elif padding_side == \"right\":\n+                        _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, :-2]\n+                        _logits_eager[-1:, 2:] = logits_eager[-1:, :-2]\n \n-                        logits_sdpa = _logits_sdpa\n-                        logits_eager = _logits_eager\n+                    logits_sdpa = _logits_sdpa\n+                    logits_eager = _logits_eager\n \n-                    results = [\n-                        torch.allclose(_logits_sdpa, _logits_eager, atol=atol, rtol=rtol)\n-                        for (_logits_sdpa, _logits_eager) in zip(logits_sdpa, logits_eager)\n-                    ]\n-                    # If 80% batch elements have matched results, it's fine\n-                    if np.mean(results) < 0.8:\n-                        mean_relative_diff = ((logits_sdpa - logits_eager).abs() / (logits_eager.abs() + 1e-12)).mean()\n-                        raise ValueError(\n-                            f\"mean relative difference: {mean_relative_diff:.3e}, torch atol = {atol}, torch rtol = \"\n-                            f\"{rtol}\"\n-                        )\n+                results = [\n+                    torch.allclose(_logits_sdpa, _logits_eager, atol=atol, rtol=rtol)\n+                    for (_logits_sdpa, _logits_eager) in zip(logits_sdpa, logits_eager)\n+                ]\n+                # If 80% batch elements have matched results, it's fine\n+                if np.mean(results) < 0.8:\n+                    mean_relative_diff = ((logits_sdpa - logits_eager).abs() / (logits_eager.abs() + 1e-12)).mean()\n+                    raise ValueError(\n+                        f\"mean relative difference for {key}: {mean_relative_diff:.3e}, torch atol = {atol}, torch rtol = \"\n+                        f\"{rtol}\"\n+                    )\n \n     @require_torch_sdpa\n     @require_torch_gpu"
        }
    ],
    "stats": {
        "total": 2205,
        "additions": 563,
        "deletions": 1642
    }
}