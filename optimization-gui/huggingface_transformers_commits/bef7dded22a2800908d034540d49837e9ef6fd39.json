{
    "author": "mtreinik",
    "message": "Replace deprecated batch_size with max_batch_size when using HybridCache (#35498)\n\n* Replace deprecated batch_size with max_batch_size\r\n\r\n- Functionality remains the same, because property getter batch_size(self) returned max_batch_size anyways.\r\n- This change just avoids an unnecessary warning about deprecation.\r\n\r\n* Use max_batch_size instead of deprecated batch_size with HybridCache\r\n\r\n* Use max_batch_size instead of deprecated batch_size with HybridCache\r\n\r\n- Change generated code to match original source",
    "sha": "bef7dded22a2800908d034540d49837e9ef6fd39",
    "files": [
        {
            "sha": "822692315cd6810869f3f260632298e9a85bd19e",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bef7dded22a2800908d034540d49837e9ef6fd39/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bef7dded22a2800908d034540d49837e9ef6fd39/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=bef7dded22a2800908d034540d49837e9ef6fd39",
            "patch": "@@ -1640,9 +1640,9 @@ def __init__(\n         )\n         self.key_cache: List[torch.Tensor] = []\n         self.value_cache: List[torch.Tensor] = []\n-        global_cache_shape = (self.batch_size, self.num_key_value_heads, max_cache_len, self.head_dim)\n+        global_cache_shape = (self.max_batch_size, self.num_key_value_heads, max_cache_len, self.head_dim)\n         sliding_cache_shape = (\n-            self.batch_size,\n+            self.max_batch_size,\n             self.num_key_value_heads,\n             min(config.sliding_window, max_cache_len),\n             self.head_dim,"
        },
        {
            "sha": "1dde941fe0c9c0df4ca1311c6bdd82915efe6e46",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bef7dded22a2800908d034540d49837e9ef6fd39/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bef7dded22a2800908d034540d49837e9ef6fd39/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=bef7dded22a2800908d034540d49837e9ef6fd39",
            "patch": "@@ -579,7 +579,7 @@ def forward(\n             batch_size, seq_len, _ = inputs_embeds.shape\n             past_key_values = HybridCache(\n                 self.config,\n-                batch_size=batch_size,\n+                max_batch_size=batch_size,\n                 max_cache_len=seq_len,\n                 device=self.device,\n                 dtype=inputs_embeds.dtype,"
        },
        {
            "sha": "78419e78c08b5c0444fa0773854714ca66b5ae05",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bef7dded22a2800908d034540d49837e9ef6fd39/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bef7dded22a2800908d034540d49837e9ef6fd39/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=bef7dded22a2800908d034540d49837e9ef6fd39",
            "patch": "@@ -459,7 +459,7 @@ def forward(\n             batch_size, seq_len, _ = inputs_embeds.shape\n             past_key_values = HybridCache(\n                 self.config,\n-                batch_size=batch_size,\n+                max_batch_size=batch_size,\n                 max_cache_len=seq_len,\n                 device=self.device,\n                 dtype=inputs_embeds.dtype,"
        },
        {
            "sha": "e64559b266509f0599716b91bc58e65b8b2c0fe4",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bef7dded22a2800908d034540d49837e9ef6fd39/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bef7dded22a2800908d034540d49837e9ef6fd39/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=bef7dded22a2800908d034540d49837e9ef6fd39",
            "patch": "@@ -577,7 +577,7 @@ def forward(\n             batch_size, seq_len, _ = inputs_embeds.shape\n             past_key_values = HybridCache(\n                 self.config,\n-                batch_size=batch_size,\n+                max_batch_size=batch_size,\n                 max_cache_len=seq_len,\n                 device=self.device,\n                 dtype=inputs_embeds.dtype,"
        },
        {
            "sha": "5f21fc6bfffd61e46da2d7a519d2afce7d2c2400",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bef7dded22a2800908d034540d49837e9ef6fd39/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bef7dded22a2800908d034540d49837e9ef6fd39/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=bef7dded22a2800908d034540d49837e9ef6fd39",
            "patch": "@@ -403,7 +403,7 @@ def forward(\n             batch_size, seq_len, _ = inputs_embeds.shape\n             past_key_values = HybridCache(\n                 self.config,\n-                batch_size=batch_size,\n+                max_batch_size=batch_size,\n                 max_cache_len=seq_len,\n                 device=self.device,\n                 dtype=inputs_embeds.dtype,"
        }
    ],
    "stats": {
        "total": 12,
        "additions": 6,
        "deletions": 6
    }
}