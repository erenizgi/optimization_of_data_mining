{
    "author": "7amim",
    "message": "Bugfix/remove emojis from print (#42091)\n\n* Removed initial batch of emojis from strings and print statements.\n\n* Removed many of the emojis that are printed to the client.\n\n* Removed the Hugging Face emoji from the arg-parse help descriptions.\n\n* Removed even more emojis.\n\n* Used cursor to do a pass on top of my manual pass to remove run-time emojis. I did a manual verification of the results.\n\n* Reverting some changes that were not necessary.\n\n* Used ruff to format files.",
    "sha": "31839d741a30d7fe5daf982d9f2d9cc549ec6181",
    "files": [
        {
            "sha": "a7523e8edc9d4861dcef4c1283632475e8ae8646",
            "filename": "examples/pytorch/language-modeling/run_clm_no_trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm_no_trainer.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -127,7 +127,7 @@ def parse_args():\n     parser.add_argument(\n         \"--use_slow_tokenizer\",\n         action=\"store_true\",\n-        help=\"If passed, will use a slow tokenizer (not backed by the ü§ó Tokenizers library).\",\n+        help=\"If passed, will use a slow tokenizer (not backed by the Hugging Face Tokenizers library).\",\n     )\n     parser.add_argument(\n         \"--per_device_train_batch_size\","
        },
        {
            "sha": "c66ebcf3cf7b7070730650680311cb886427db3a",
            "filename": "examples/pytorch/language-modeling/run_fim_no_trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim_no_trainer.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -132,7 +132,7 @@ def parse_args():\n     parser.add_argument(\n         \"--use_slow_tokenizer\",\n         action=\"store_true\",\n-        help=\"If passed, will use a slow tokenizer (not backed by the ü§ó Tokenizers library).\",\n+        help=\"If passed, will use a slow tokenizer (not backed by the Tokenizers library).\",\n     )\n     parser.add_argument(\n         \"--per_device_train_batch_size\","
        },
        {
            "sha": "6bec1ab7f963960ab166d3f11976cffd7e7b3d96",
            "filename": "examples/pytorch/language-modeling/run_mlm_no_trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/examples%2Fpytorch%2Flanguage-modeling%2Frun_mlm_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/examples%2Fpytorch%2Flanguage-modeling%2Frun_mlm_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_mlm_no_trainer.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -130,7 +130,7 @@ def parse_args():\n     parser.add_argument(\n         \"--use_slow_tokenizer\",\n         action=\"store_true\",\n-        help=\"If passed, will use a slow tokenizer (not backed by the ü§ó Tokenizers library).\",\n+        help=\"If passed, will use a slow tokenizer (not backed by the Hugging Face Tokenizers library).\",\n     )\n     parser.add_argument(\n         \"--per_device_train_batch_size\","
        },
        {
            "sha": "9983832fa5b702488799454dde443534e01cff9b",
            "filename": "examples/pytorch/multiple-choice/run_swag_no_trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/examples%2Fpytorch%2Fmultiple-choice%2Frun_swag_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/examples%2Fpytorch%2Fmultiple-choice%2Frun_swag_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fmultiple-choice%2Frun_swag_no_trainer.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -128,7 +128,7 @@ def parse_args():\n     parser.add_argument(\n         \"--use_slow_tokenizer\",\n         action=\"store_true\",\n-        help=\"If passed, will use a slow tokenizer (not backed by the ü§ó Tokenizers library).\",\n+        help=\"If passed, will use a slow tokenizer (not backed by the HuggingFace Tokenizers library).\",\n     )\n     parser.add_argument(\n         \"--per_device_train_batch_size\","
        },
        {
            "sha": "0fd4058bd2ff965561d2c208343e7e11c8e24b0a",
            "filename": "examples/pytorch/question-answering/run_qa_no_trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_no_trainer.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -151,7 +151,7 @@ def parse_args():\n     parser.add_argument(\n         \"--use_slow_tokenizer\",\n         action=\"store_true\",\n-        help=\"If passed, will use a slow tokenizer (not backed by the ü§ó Tokenizers library).\",\n+        help=\"If passed, will use a slow tokenizer (not backed by the Hugging Face Tokenizers library).\",\n     )\n     parser.add_argument(\n         \"--per_device_train_batch_size\","
        },
        {
            "sha": "fdd1a098e2df2d13d7151dfb6c58425c94d345f3",
            "filename": "examples/pytorch/summarization/run_summarization_no_trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/examples%2Fpytorch%2Fsummarization%2Frun_summarization_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/examples%2Fpytorch%2Fsummarization%2Frun_summarization_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fsummarization%2Frun_summarization_no_trainer.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -223,7 +223,7 @@ def parse_args():\n     parser.add_argument(\n         \"--use_slow_tokenizer\",\n         action=\"store_true\",\n-        help=\"If passed, will use a slow tokenizer (not backed by the ü§ó Tokenizers library).\",\n+        help=\"If passed, will use a slow tokenizer (not backed by the Hugging Face Tokenizers library).\",\n     )\n     parser.add_argument(\n         \"--per_device_train_batch_size\","
        },
        {
            "sha": "9a1d5aea111db1451473fdec377c0c3bfd8c1aac",
            "filename": "examples/pytorch/text-classification/run_glue_no_trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/examples%2Fpytorch%2Ftext-classification%2Frun_glue_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/examples%2Fpytorch%2Ftext-classification%2Frun_glue_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftext-classification%2Frun_glue_no_trainer.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -120,7 +120,7 @@ def parse_args():\n     parser.add_argument(\n         \"--use_slow_tokenizer\",\n         action=\"store_true\",\n-        help=\"If passed, will use a slow tokenizer (not backed by the ü§ó Tokenizers library).\",\n+        help=\"If passed, will use a slow tokenizer (not backed by the Hugging Face Tokenizers library).\",\n     )\n     parser.add_argument(\n         \"--per_device_train_batch_size\","
        },
        {
            "sha": "a760045d09723d2d894b0338ac9a54e13e495f19",
            "filename": "examples/pytorch/translation/run_translation_no_trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/examples%2Fpytorch%2Ftranslation%2Frun_translation_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/examples%2Fpytorch%2Ftranslation%2Frun_translation_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftranslation%2Frun_translation_no_trainer.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -212,7 +212,7 @@ def parse_args():\n     parser.add_argument(\n         \"--use_slow_tokenizer\",\n         action=\"store_true\",\n-        help=\"If passed, will use a slow tokenizer (not backed by the ü§ó Tokenizers library).\",\n+        help=\"If passed, will use a slow tokenizer (not backed by the Hugging Face Tokenizers library).\",\n     )\n     parser.add_argument(\n         \"--per_device_train_batch_size\","
        },
        {
            "sha": "39ea926c98882299ea729e27308fd345c7cdb5ea",
            "filename": "src/transformers/data/datasets/glue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fdata%2Fdatasets%2Fglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fdata%2Fdatasets%2Fglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fdatasets%2Fglue.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -82,7 +82,7 @@ def __init__(\n         cache_dir: Optional[str] = None,\n     ):\n         warnings.warn(\n-            \"This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets \"\n+            \"This dataset will be removed from the library soon, preprocessing should be handled with the Hugging Face Datasets \"\n             \"library. You can have a look at this example script for pointers: \"\n             \"https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\",\n             FutureWarning,"
        },
        {
            "sha": "f8a48ef7dcde7b02bd846c46045b60e54e165e5f",
            "filename": "src/transformers/data/metrics/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fdata%2Fmetrics%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fdata%2Fmetrics%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fmetrics%2F__init__.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -21,7 +21,7 @@\n \n \n DEPRECATION_WARNING = (\n-    \"This metric will be removed from the library soon, metrics should be handled with the ü§ó Evaluate \"\n+    \"This metric will be removed from the library soon, metrics should be handled with the Hugging Face Evaluate \"\n     \"library. You can have a look at this example script for pointers: \"\n     \"https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\"\n )"
        },
        {
            "sha": "2dc97760563893c4fe75abafff335d981e59b42f",
            "filename": "src/transformers/data/processors/glue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fdata%2Fprocessors%2Fglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fdata%2Fprocessors%2Fglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fprocessors%2Fglue.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -28,7 +28,7 @@\n logger = logging.get_logger(__name__)\n \n DEPRECATION_WARNING = (\n-    \"This {0} will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets \"\n+    \"This {0} will be removed from the library soon, preprocessing should be handled with the Hugging Face Datasets \"\n     \"library. You can have a look at this example script for pointers: \"\n     \"https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\"\n )"
        },
        {
            "sha": "79a705df804aa18104442e587c5baf0b5e074099",
            "filename": "src/transformers/integrations/integration_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -38,7 +38,7 @@\n \n \n if os.getenv(\"WANDB_MODE\") == \"offline\":\n-    print(\"‚öôÔ∏è  Running in WANDB offline mode\")\n+    print(\"[INFO] Running in WANDB offline mode\")\n \n from .. import PreTrainedModel, TrainingArguments\n from .. import __version__ as version"
        },
        {
            "sha": "c2b6867c45776c25a051264379192c910490ff47",
            "filename": "src/transformers/models/audio_spectrogram_transformer/convert_audio_spectrogram_transformer_original_to_pytorch.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fconvert_audio_spectrogram_transformer_original_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fconvert_audio_spectrogram_transformer_original_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fconvert_audio_spectrogram_transformer_original_to_pytorch.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -272,7 +272,9 @@ def convert_audio_spectrogram_transformer_checkpoint(model_name, pytorch_dump_fo\n         \"--pytorch_dump_folder_path\", default=None, type=str, help=\"Path to the output PyTorch model directory.\"\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n \n     args = parser.parse_args()"
        },
        {
            "sha": "2579fbcf62f5811a87ce68589a4476e0df7912d6",
            "filename": "src/transformers/models/bros/convert_bros_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fbros%2Fconvert_bros_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fbros%2Fconvert_bros_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbros%2Fconvert_bros_to_pytorch.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -138,7 +138,7 @@ def convert_bros_checkpoint(model_name, pytorch_dump_folder_path=None, push_to_h\n     parser.add_argument(\n         \"--push_to_hub\",\n         action=\"store_true\",\n-        help=\"Whether or not to push the converted model and processor to the ü§ó hub.\",\n+        help=\"Whether or not to push the converted model and processor to the Hugging Face hub.\",\n     )\n \n     args = parser.parse_args()"
        },
        {
            "sha": "1192fe796655f9971aec64cc2aef2fa0f248fc3e",
            "filename": "src/transformers/models/clipseg/convert_clipseg_original_pytorch_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconvert_clipseg_original_pytorch_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconvert_clipseg_original_pytorch_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconvert_clipseg_original_pytorch_to_hf.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -257,7 +257,9 @@ def convert_clipseg_checkpoint(model_name, checkpoint_path, pytorch_dump_folder_\n         \"--pytorch_dump_folder_path\", default=None, type=str, help=\"Path to the output PyTorch model directory.\"\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n \n     args = parser.parse_args()"
        },
        {
            "sha": "4b73318d88a45509aedcb7f5e4446aca17de8a5a",
            "filename": "src/transformers/models/dac/convert_dac_checkpoint.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fdac%2Fconvert_dac_checkpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fdac%2Fconvert_dac_checkpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdac%2Fconvert_dac_checkpoint.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -274,7 +274,7 @@ def convert_checkpoint(\n         \"--pytorch_dump_folder_path\", required=True, default=None, type=str, help=\"Path to the output PyTorch model.\"\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", default=None, type=str, help=\"Where to upload the converted model on the ü§ó hub.\"\n+        \"--push_to_hub\", default=None, type=str, help=\"Where to upload the converted model on the Hugging Face hub.\"\n     )\n     parser.add_argument(\"--sample_rate\", default=None, type=str, help=\"Sample rate used by DacFeatureExtractor\")\n     args = parser.parse_args()"
        },
        {
            "sha": "a19c9b16e3c77b3ef4d00ae0f88ed46990b68748",
            "filename": "src/transformers/models/data2vec/convert_data2vec_audio_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconvert_data2vec_audio_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconvert_data2vec_audio_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconvert_data2vec_audio_original_pytorch_checkpoint_to_pytorch.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -258,7 +258,7 @@ def load_data2vec(path):\n     max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()\n     print(f\"max_absolute_diff = {max_absolute_diff}\")  # ~ 1e-7\n     success = torch.allclose(our_output, their_output, atol=1e-3)\n-    print(\"Do both models output the same tensors?\", \"üî•\" if success else \"üí©\")\n+    print(\"Do both models output the same tensors?\", \"[PASS]\" if success else \"[FAIL]\")\n     if not success:\n         raise Exception(\"Something went wRoNg\")\n "
        },
        {
            "sha": "13d334085bc7250a28512422c68a8543a00ffeb5",
            "filename": "src/transformers/models/data2vec/convert_data2vec_text_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconvert_data2vec_text_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconvert_data2vec_text_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconvert_data2vec_text_original_pytorch_checkpoint_to_pytorch.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -180,7 +180,7 @@ def convert_data2vec_checkpoint_to_pytorch(\n     max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()\n     print(f\"max_absolute_diff = {max_absolute_diff}\")  # ~ 1e-7\n     success = torch.allclose(our_output, their_output, atol=1e-3)\n-    print(\"Do both models output the same tensors?\", \"üî•\" if success else \"üí©\")\n+    print(\"Do both models output the same tensors?\", \"[PASS]\" if success else \"[FAIL]\")\n     if not success:\n         raise Exception(\"Something went wRoNg\")\n "
        },
        {
            "sha": "d53f5da90b2fa74da7c48d3726236e95c9cd9c71",
            "filename": "src/transformers/models/data2vec/convert_data2vec_vision_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconvert_data2vec_vision_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconvert_data2vec_vision_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconvert_data2vec_vision_original_pytorch_checkpoint_to_pytorch.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -341,7 +341,7 @@ def main():\n \n     print(f\"max_absolute_diff = {max_absolute_diff}\")\n     success = torch.allclose(hf_output, orig_model_output, atol=1e-3)\n-    print(\"Do both models output the same tensors?\", \"üî•\" if success else \"üí©\")\n+    print(\"Do both models output the same tensors?\", \"[PASS]\" if success else \"[FAIL]\")\n     if not success:\n         raise Exception(\"Something went wRoNg\")\n "
        },
        {
            "sha": "8110f87e14ffc2eec74020a49e383b8412f9b635",
            "filename": "src/transformers/models/deformable_detr/convert_deformable_detr_to_pytorch.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fconvert_deformable_detr_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fconvert_deformable_detr_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fconvert_deformable_detr_to_pytorch.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -222,7 +222,9 @@ def convert_deformable_detr_checkpoint(\n         help=\"Path to the folder to output PyTorch model.\",\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n     args = parser.parse_args()\n     convert_deformable_detr_checkpoint("
        },
        {
            "sha": "425438de25ecee986842904c932d16065649394e",
            "filename": "src/transformers/models/deprecated/bort/convert_bort_original_gluonnlp_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fbort%2Fconvert_bort_original_gluonnlp_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fbort%2Fconvert_bort_original_gluonnlp_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fbort%2Fconvert_bort_original_gluonnlp_checkpoint_to_pytorch.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -299,9 +299,9 @@ def check_and_map_params(hf_param, gluon_param):\n     success = np.allclose(gluon_layer, hf_layer, atol=1e-3)\n \n     if success:\n-        print(\"‚úîÔ∏è Both model do output the same tensors\")\n+        print(\"[SUCCESS] Both models do output the same tensors\")\n     else:\n-        print(\"‚ùå Both model do **NOT** output the same tensors\")\n+        print(\"[FAIL] Both models do **NOT** output the same tensors\")\n         print(\"Absolute difference is:\", max_absolute_diff)\n \n "
        },
        {
            "sha": "6023be50854c5b3f9384ccb437bae956f2b77b7e",
            "filename": "src/transformers/models/deprecated/deta/convert_deta_resnet_to_pytorch.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fconvert_deta_resnet_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fconvert_deta_resnet_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fconvert_deta_resnet_to_pytorch.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -313,7 +313,9 @@ def convert_deta_checkpoint(model_name, pytorch_dump_folder_path, push_to_hub):\n         help=\"Path to the folder to output PyTorch model.\",\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n     args = parser.parse_args()\n     convert_deta_checkpoint(args.model_name, args.pytorch_dump_folder_path, args.push_to_hub)"
        },
        {
            "sha": "cc95883c891fdbff867e72ad24beff1260da2cbe",
            "filename": "src/transformers/models/deprecated/deta/convert_deta_swin_to_pytorch.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fconvert_deta_swin_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fconvert_deta_swin_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fconvert_deta_swin_to_pytorch.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -320,7 +320,9 @@ def convert_deta_checkpoint(model_name, pytorch_dump_folder_path, push_to_hub):\n         help=\"Path to the folder to output PyTorch model.\",\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n     args = parser.parse_args()\n     convert_deta_checkpoint(args.model_name, args.pytorch_dump_folder_path, args.push_to_hub)"
        },
        {
            "sha": "55231cddc2aeba40a68e9828c39426f4dffbcdf8",
            "filename": "src/transformers/models/dinov2/convert_dinov2_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fdinov2%2Fconvert_dinov2_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fdinov2%2Fconvert_dinov2_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2%2Fconvert_dinov2_to_hf.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -278,7 +278,9 @@ def convert_dinov2_checkpoint(model_name, pytorch_dump_folder_path, push_to_hub=\n         \"--pytorch_dump_folder_path\", default=None, type=str, help=\"Path to the output PyTorch model directory.\"\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n \n     args = parser.parse_args()"
        },
        {
            "sha": "d3abb19ea9c94fc4093cbbc24048f1f0ccfae7ff",
            "filename": "src/transformers/models/dinov2_with_registers/convert_dinov2_with_registers_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fconvert_dinov2_with_registers_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fconvert_dinov2_with_registers_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fconvert_dinov2_with_registers_to_hf.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -284,7 +284,9 @@ def convert_dinov2_with_registers_checkpoint(model_name, pytorch_dump_folder_pat\n         \"--pytorch_dump_folder_path\", default=None, type=str, help=\"Path to the output PyTorch model directory.\"\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n \n     args = parser.parse_args()"
        },
        {
            "sha": "813f26c1338c734dbff4b824c80ca6bf510497e4",
            "filename": "src/transformers/models/donut/convert_donut_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fdonut%2Fconvert_donut_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fdonut%2Fconvert_donut_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fconvert_donut_to_pytorch.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -227,7 +227,7 @@ def convert_donut_checkpoint(model_name, pytorch_dump_folder_path=None, push_to_\n     parser.add_argument(\n         \"--push_to_hub\",\n         action=\"store_true\",\n-        help=\"Whether or not to push the converted model and processor to the ü§ó hub.\",\n+        help=\"Whether or not to push the converted model and processor to the Hugging Face hub.\",\n     )\n \n     args = parser.parse_args()"
        },
        {
            "sha": "cb7c588abd7399bf39d2210d42cecd7a7be6102a",
            "filename": "src/transformers/models/encodec/convert_encodec_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fencodec%2Fconvert_encodec_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fencodec%2Fconvert_encodec_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencodec%2Fconvert_encodec_checkpoint_to_pytorch.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -352,7 +352,7 @@ def convert_checkpoint(\n         \"--pytorch_dump_folder_path\", required=True, default=None, type=str, help=\"Path to the output PyTorch model.\"\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", default=None, type=str, help=\"Where to upload the converted model on the ü§ó hub.\"\n+        \"--push_to_hub\", default=None, type=str, help=\"Where to upload the converted model on the Hugging Face hub.\"\n     )\n \n     args = parser.parse_args()"
        },
        {
            "sha": "9e1f72c93be481b295f39f627e40b8895bc65b1c",
            "filename": "src/transformers/models/esm/convert_esm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fesm%2Fconvert_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fesm%2Fconvert_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fconvert_esm.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -316,7 +316,7 @@ def convert_esm_checkpoint_to_pytorch(\n         hf_tokens = hf_tokenizer([row[1] for row in sample_data], return_tensors=\"pt\", padding=True)\n         success = torch.all(hf_tokens[\"input_ids\"] == batch_tokens)\n \n-    print(\"Do both models tokenizers output the same tokens?\", \"üî•\" if success else \"üí©\")\n+    print(\"Do both models tokenizers output the same tokens?\", \"[PASS]\" if success else \"[FAIL]\")\n     if not success:\n         raise Exception(\"Tokenization does not match!\")\n \n@@ -348,7 +348,7 @@ def convert_esm_checkpoint_to_pytorch(\n             success = torch.allclose(our_output, their_output, atol=1e-5)\n \n         print(f\"max_absolute_diff = {max_absolute_diff}\")  # ~ 1e-5\n-        print(\"Do both models output the same tensors?\", \"üî•\" if success else \"üí©\")\n+        print(\"Do both models output the same tensors?\", \"[PASS]\" if success else \"[FAIL]\")\n \n         if not success:\n             raise Exception(\"Something went wRoNg\")\n@@ -362,7 +362,7 @@ def convert_esm_checkpoint_to_pytorch(\n \n             print(\"Contact prediction testing:\")\n             print(f\"max_absolute_diff = {max_absolute_diff}\")  # ~ 1e-5\n-            print(\"Do both models output the same tensors?\", \"üî•\" if success else \"üí©\")\n+            print(\"Do both models output the same tensors?\", \"[PASS]\" if success else \"[FAIL]\")\n \n             if not success:\n                 raise Exception(\"Something went wRoNg\")"
        },
        {
            "sha": "78cc416e760a981c963c32892b6b8e7d80eab01a",
            "filename": "src/transformers/models/fastspeech2_conformer/convert_fastspeech2_conformer_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fconvert_fastspeech2_conformer_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fconvert_fastspeech2_conformer_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fconvert_fastspeech2_conformer_original_pytorch_checkpoint_to_pytorch.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -198,7 +198,7 @@ def convert_FastSpeech2ConformerModel_checkpoint(\n         \"--pytorch_dump_folder_path\", required=True, default=None, type=str, help=\"Path to the output PyTorch model.\"\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", default=None, type=str, help=\"Where to upload the converted model on the ü§ó hub.\"\n+        \"--push_to_hub\", default=None, type=str, help=\"Where to upload the converted model on the Hugging Face hub.\"\n     )\n \n     args = parser.parse_args()"
        },
        {
            "sha": "d4d1c3c405e1889920dc2448448c834f1ddb56dc",
            "filename": "src/transformers/models/fastspeech2_conformer/convert_hifigan.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fconvert_hifigan.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fconvert_hifigan.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fconvert_hifigan.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -122,7 +122,7 @@ def convert_hifigan_checkpoint(\n         \"--pytorch_dump_folder_path\", required=True, default=None, type=str, help=\"Path to the output PyTorch model.\"\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", default=None, type=str, help=\"Where to upload the converted model on the ü§ó hub.\"\n+        \"--push_to_hub\", default=None, type=str, help=\"Where to upload the converted model on the Hugging Face hub.\"\n     )\n \n     args = parser.parse_args()"
        },
        {
            "sha": "93f9a8f8155d94354673ea3ff1e0f3d530426385",
            "filename": "src/transformers/models/fastspeech2_conformer/convert_model_with_hifigan.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fconvert_model_with_hifigan.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fconvert_model_with_hifigan.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fconvert_model_with_hifigan.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -89,7 +89,7 @@ def convert_FastSpeech2ConformerWithHifiGan_checkpoint(\n         help=\"Path to the output `FastSpeech2ConformerModel` PyTorch model.\",\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", default=None, type=str, help=\"Where to upload the converted model on the ü§ó hub.\"\n+        \"--push_to_hub\", default=None, type=str, help=\"Where to upload the converted model on the Hugging Face hub.\"\n     )\n \n     args = parser.parse_args()"
        },
        {
            "sha": "599ca381d804687c53750121fd93884e7304b815",
            "filename": "src/transformers/models/got_ocr2/convert_got_ocr2_weights_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fconvert_got_ocr2_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fconvert_got_ocr2_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fconvert_got_ocr2_weights_to_hf.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -250,7 +250,9 @@ def main():\n     )\n \n     parser.add_argument(\n-        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n     args = parser.parse_args()\n     write_tokenizer("
        },
        {
            "sha": "4697e9f4e0375c58890d7eb92b0c6691fa6b8058",
            "filename": "src/transformers/models/grounding_dino/convert_grounding_dino_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fconvert_grounding_dino_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fconvert_grounding_dino_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fconvert_grounding_dino_to_hf.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -481,7 +481,9 @@ def convert_grounding_dino_checkpoint(args):\n         \"--pytorch_dump_folder_path\", default=None, type=str, help=\"Path to the output PyTorch model directory.\"\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n     parser.add_argument(\n         \"--verify_logits\", action=\"store_false\", help=\"Whether or not to verify logits after conversion.\""
        },
        {
            "sha": "c53e12c89153b8ca736145abb1afecb9acee62d7",
            "filename": "src/transformers/models/groupvit/convert_groupvit_nvlab_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fconvert_groupvit_nvlab_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fconvert_groupvit_nvlab_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fconvert_groupvit_nvlab_to_hf.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -210,7 +210,7 @@ def convert_groupvit_checkpoint(\n     parser.add_argument(\n         \"--push_to_hub\",\n         action=\"store_true\",\n-        help=\"Whether or not to push the converted model and processor to the ü§ó hub using the provided `model_name`.\",\n+        help=\"Whether or not to push the converted model and processor to the Hugging Face hub using the provided `model_name`.\",\n     )\n     args = parser.parse_args()\n "
        },
        {
            "sha": "c65b07ccc761399056aa81572ed9d8c9245a91b8",
            "filename": "src/transformers/models/hiera/convert_hiera_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fhiera%2Fconvert_hiera_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fhiera%2Fconvert_hiera_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhiera%2Fconvert_hiera_to_hf.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -353,7 +353,9 @@ def convert_hiera_checkpoint(args):\n         help=\"Whether or not to verify the logits against the original implementation.\",\n     )\n     parser.add_argument(\n-        \"--push-to-hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+        \"--push-to-hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n     parser.add_argument(\n         \"--base-model\","
        },
        {
            "sha": "4c06f9d202e305ad7ad82eccf5bd38333031988d",
            "filename": "src/transformers/models/ijepa/convert_ijepa_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fijepa%2Fconvert_ijepa_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fijepa%2Fconvert_ijepa_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fijepa%2Fconvert_ijepa_to_hf.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -253,7 +253,7 @@ def main():\n     parser.add_argument(\n         \"--push_to_hub\",\n         action=\"store_true\",\n-        help=\"Whether or not to push the model to the ü§ó Hub.\",\n+        help=\"Whether or not to push the model to the Hugging Face Hub.\",\n     )\n     parser.add_argument(\n         \"--verify_logits\", action=\"store_false\", help=\"Whether or not to verify logits after conversion.\""
        },
        {
            "sha": "ec7b4ded56a0ae90853433827d972c9a0f18f1cc",
            "filename": "src/transformers/models/internvl/convert_internvl_weights_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Finternvl%2Fconvert_internvl_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Finternvl%2Fconvert_internvl_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fconvert_internvl_weights_to_hf.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -433,7 +433,9 @@ def main():\n     )\n \n     parser.add_argument(\n-        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n     args = parser.parse_args()\n     write_tokenizer("
        },
        {
            "sha": "b6d12cf0704f7acc0ef511842a0c0401a1e52f23",
            "filename": "src/transformers/models/llava_next/convert_llava_next_weights_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fllava_next%2Fconvert_llava_next_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fllava_next%2Fconvert_llava_next_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fconvert_llava_next_weights_to_hf.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -387,7 +387,9 @@ def convert_llava_to_hf(model_id, pytorch_dump_folder_path, push_to_hub=False):\n         \"--pytorch_dump_folder_path\", type=str, required=True, help=\"Path to the output PyTorch model directory.\"\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n     args = parser.parse_args()\n "
        },
        {
            "sha": "77df44e9d1eb23c5e5973357b61f02e4cf8b85ad",
            "filename": "src/transformers/models/llava_next_video/convert_llava_next_video_weights_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconvert_llava_next_video_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconvert_llava_next_video_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconvert_llava_next_video_weights_to_hf.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -267,7 +267,9 @@ def convert_llava_to_hf(model_id, pytorch_dump_folder_path, push_to_hub=False):\n         \"--pytorch_dump_folder_path\", default=None, type=str, help=\"Path to the output PyTorch model directory.\"\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n     args = parser.parse_args()\n "
        },
        {
            "sha": "bfe67fd443fe68526c22ac38128db9a17f066f42",
            "filename": "src/transformers/models/llava_onevision/convert_llava_onevision_weights_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconvert_llava_onevision_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconvert_llava_onevision_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconvert_llava_onevision_weights_to_hf.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -379,7 +379,9 @@ def convert_llava_to_hf(model_id, pytorch_dump_folder_path, push_to_hub=False):\n         \"--pytorch_dump_folder_path\", type=str, required=True, help=\"Path to the output PyTorch model directory.\"\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n     args = parser.parse_args()\n "
        },
        {
            "sha": "d10662daf61478aa7dbed2a00062f09a56d8564a",
            "filename": "src/transformers/models/mask2former/convert_mask2former_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fmask2former%2Fconvert_mask2former_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fmask2former%2Fconvert_mask2former_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fconvert_mask2former_original_pytorch_checkpoint_to_pytorch.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -826,7 +826,7 @@ def using_dirs(checkpoints_dir: Path, config_dir: Path) -> Iterator[tuple[object\n         checkpoints: list[Path] = checkpoints_dir.glob(\"**/*.pkl\")\n \n         for checkpoint in checkpoints:\n-            logger.info(f\"üí™ Converting {checkpoint.stem}\")\n+            logger.info(f\"Converting {checkpoint.stem}\")\n             # find associated config file\n \n             # dataset_name e.g 'coco'\n@@ -902,7 +902,7 @@ def test(\n             \"The predicted masks are not the same.\"\n         )\n \n-        logger.info(\"‚úÖ Test passed!\")\n+        logger.info(\"Test passed!\")\n \n \n def get_model_name(checkpoint_file: Path):\n@@ -1012,9 +1012,9 @@ def get_model_name(checkpoint_file: Path):\n         if model_name in high_tolerance_models:\n             tolerance = 3e-1\n \n-        logger.info(f\"ü™Ñ Testing {model_name}...\")\n+        logger.info(f\"Testing {model_name}...\")\n         test(original_model, mask2former_for_segmentation, image_processor, tolerance)\n-        logger.info(f\"ü™Ñ Pushing {model_name} to hub...\")\n+        logger.info(f\"Pushing {model_name} to hub...\")\n \n         image_processor.push_to_hub(model_name)\n         mask2former_for_segmentation.push_to_hub(model_name)"
        },
        {
            "sha": "6632bb0a171628b20a4f73570fb1ba011e4661ca",
            "filename": "src/transformers/models/maskformer/convert_maskformer_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fconvert_maskformer_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fconvert_maskformer_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fconvert_maskformer_original_pytorch_checkpoint_to_pytorch.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -548,7 +548,7 @@ def using_dirs(checkpoints_dir: Path, config_dir: Path) -> Iterator[tuple[object\n         checkpoints: list[Path] = checkpoints_dir.glob(\"**/*.pkl\")\n \n         for checkpoint in checkpoints:\n-            logger.info(f\"üí™ Converting {checkpoint.stem}\")\n+            logger.info(f\"Converting {checkpoint.stem}\")\n             # find associated config file\n             config: Path = config_dir / checkpoint.parents[0].stem / \"swin\" / f\"{checkpoint.stem}.yaml\"\n \n@@ -607,7 +607,7 @@ def test(original_model, our_model: MaskFormerForInstanceSegmentation, image_pro\n             \"The segmentation image is not the same.\"\n         )\n \n-        logger.info(\"‚úÖ Test passed!\")\n+        logger.info(\"Test passed!\")\n \n \n def get_name(checkpoint_file: Path):\n@@ -715,7 +715,7 @@ def get_name(checkpoint_file: Path):\n         test(original_model, mask_former_for_instance_segmentation, image_processor)\n \n         model_name = get_name(checkpoint_file)\n-        logger.info(f\"ü™Ñ Saving {model_name}\")\n+        logger.info(f\"Saving {model_name}\")\n \n         image_processor.save_pretrained(save_directory / model_name)\n         mask_former_for_instance_segmentation.save_pretrained(save_directory / model_name)"
        },
        {
            "sha": "98e8ad63c32af8a8449541bfa1906bf7f282f53c",
            "filename": "src/transformers/models/maskformer/convert_maskformer_resnet_to_pytorch.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fconvert_maskformer_resnet_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fconvert_maskformer_resnet_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fconvert_maskformer_resnet_to_pytorch.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -391,7 +391,9 @@ def convert_maskformer_checkpoint(\n         \"--pytorch_dump_folder_path\", default=None, type=str, help=\"Path to the output PyTorch model directory.\"\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n \n     args = parser.parse_args()"
        },
        {
            "sha": "1084e6b2bf60a6692d1832d4d606e8551592f07c",
            "filename": "src/transformers/models/maskformer/convert_maskformer_swin_to_pytorch.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fconvert_maskformer_swin_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fconvert_maskformer_swin_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fconvert_maskformer_swin_to_pytorch.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -334,7 +334,9 @@ def convert_maskformer_checkpoint(\n         \"--pytorch_dump_folder_path\", default=None, type=str, help=\"Path to the output PyTorch model directory.\"\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n \n     args = parser.parse_args()"
        },
        {
            "sha": "b086a5844b345aa5e6cefca84d30054b03fa4f50",
            "filename": "src/transformers/models/metaclip_2/convert_metaclip_2_to_hf.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fconvert_metaclip_2_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fconvert_metaclip_2_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fconvert_metaclip_2_to_hf.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -349,10 +349,10 @@ def verify_conversion(\n \n     # Check if they're close\n     if orig_logits.shape == hf_logits.shape and torch.allclose(orig_logits, hf_logits, atol=1e-4):\n-        print(\"‚úÖ Conversion verified! Outputs match.\")\n+        print(\"[SUCCESS] Conversion verified! Outputs match.\")\n         return True\n     else:\n-        print(\"‚ùå Conversion failed! Outputs don't match.\")\n+        print(\"[FAIL] Conversion failed! Outputs don't match.\")\n         if orig_logits.numel() > 0 and hf_logits.numel() > 0:\n             print(f\"Max difference: {(orig_logits - hf_logits).abs().max()}\")\n         return False\n@@ -365,9 +365,9 @@ def push_to_hub(hf_model: MetaClip2Model, processor: CLIPProcessor, repo_name: s\n     try:\n         hf_model.push_to_hub(repo_name)\n         processor.push_to_hub(repo_name)\n-        print(f\"‚úÖ Successfully pushed to {repo_name}\")\n+        print(f\"[SUCCESS] Successfully pushed to {repo_name}\")\n     except Exception as e:\n-        print(f\"‚ùå Failed to push to hub: {e}\")\n+        print(f\"[FAIL] Failed to push to hub: {e}\")\n \n \n def main():"
        },
        {
            "sha": "b2b124a4f2cd0c257cb680ded12996e761e58494",
            "filename": "src/transformers/models/mimi/convert_mimi_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fmimi%2Fconvert_mimi_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fmimi%2Fconvert_mimi_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fconvert_mimi_checkpoint_to_pytorch.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -186,7 +186,7 @@ def convert_checkpoint(\n         \"--pytorch_dump_folder_path\", required=True, default=None, type=str, help=\"Path to the output PyTorch model.\"\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", default=None, type=str, help=\"Where to upload the converted model on the ü§ó hub.\"\n+        \"--push_to_hub\", default=None, type=str, help=\"Where to upload the converted model on the Hugging Face hub.\"\n     )\n \n     args = parser.parse_args()"
        },
        {
            "sha": "5081426c69ac530e4ccf42328ccd5a4f0201e63f",
            "filename": "src/transformers/models/mlcd/convert_mlcd_weights_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fmlcd%2Fconvert_mlcd_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fmlcd%2Fconvert_mlcd_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmlcd%2Fconvert_mlcd_weights_to_hf.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -327,7 +327,9 @@ def convert_mlcd_checkpoint(model_name, input_dir, output_dir, verify_hidden_sta\n         help=\"Whether to verify hidden_state against the original implementation.\",\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n \n     args = parser.parse_args()"
        },
        {
            "sha": "933ae1746c0a9278e38b094a7a2aeee9474e4e1e",
            "filename": "src/transformers/models/mobilenet_v1/convert_original_tf_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fconvert_original_tf_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fconvert_original_tf_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fconvert_original_tf_checkpoint_to_pytorch.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -234,7 +234,9 @@ def convert_movilevit_checkpoint(model_name, checkpoint_path, pytorch_dump_folde\n         \"--pytorch_dump_folder_path\", required=True, type=str, help=\"Path to the output PyTorch model directory.\"\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n \n     args = parser.parse_args()"
        },
        {
            "sha": "9ea0da9f068b1f13c4dab380c505089732832fb4",
            "filename": "src/transformers/models/mobilenet_v2/convert_original_tf_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fconvert_original_tf_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fconvert_original_tf_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fconvert_original_tf_checkpoint_to_pytorch.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -336,7 +336,9 @@ def convert_movilevit_checkpoint(model_name, checkpoint_path, pytorch_dump_folde\n         \"--pytorch_dump_folder_path\", required=True, type=str, help=\"Path to the output PyTorch model directory.\"\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n \n     args = parser.parse_args()"
        },
        {
            "sha": "1f472720c1de0c5d2e142a21e0e58be5459e860f",
            "filename": "src/transformers/models/mobilevit/convert_mlcvnets_to_pytorch.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fconvert_mlcvnets_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fconvert_mlcvnets_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fconvert_mlcvnets_to_pytorch.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -302,7 +302,9 @@ def convert_movilevit_checkpoint(mobilevit_name, checkpoint_path, pytorch_dump_f\n         \"--pytorch_dump_folder_path\", required=True, type=str, help=\"Path to the output PyTorch model directory.\"\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n \n     args = parser.parse_args()"
        },
        {
            "sha": "995f2f54893bcd44aed416defe1ef1c175c4cde9",
            "filename": "src/transformers/models/moshi/convert_moshi_transformers.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fmoshi%2Fconvert_moshi_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fmoshi%2Fconvert_moshi_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fconvert_moshi_transformers.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -268,7 +268,7 @@ def convert_checkpoint(\n         \"--pytorch_dump_folder_path\", required=True, default=None, type=str, help=\"Path to the output PyTorch model.\"\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", default=None, type=str, help=\"Where to upload the converted model on the ü§ó hub.\"\n+        \"--push_to_hub\", default=None, type=str, help=\"Where to upload the converted model on the Hugging Face hub.\"\n     )\n \n     args = parser.parse_args()"
        },
        {
            "sha": "1572d06ba6755f2a587b3b15ffa1047c684c32ef",
            "filename": "src/transformers/models/musicgen/convert_musicgen_transformers.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fconvert_musicgen_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fconvert_musicgen_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fconvert_musicgen_transformers.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -221,7 +221,7 @@ def convert_musicgen_checkpoint(\n         help=\"Path to the output PyTorch model directory.\",\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", default=None, type=str, help=\"Where to upload the converted model on the ü§ó hub.\"\n+        \"--push_to_hub\", default=None, type=str, help=\"Where to upload the converted model on the Hugging Face hub.\"\n     )\n     parser.add_argument(\n         \"--device\", default=\"cpu\", type=str, help=\"Torch device to run the conversion, either cpu or cuda.\""
        },
        {
            "sha": "34a36d64ec65d735646866c791aa4548b7f176ae",
            "filename": "src/transformers/models/musicgen_melody/convert_musicgen_melody_transformers.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fconvert_musicgen_melody_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fconvert_musicgen_melody_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fconvert_musicgen_melody_transformers.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -254,7 +254,7 @@ def convert_musicgen_melody_checkpoint(\n         \"--push_to_hub\",\n         default=\"musicgen-melody\",\n         type=str,\n-        help=\"Where to upload the converted model on the ü§ó hub.\",\n+        help=\"Where to upload the converted model on the Hugging Face hub.\",\n     )\n     parser.add_argument(\n         \"--device\", default=\"cpu\", type=str, help=\"Torch device to run the conversion, either cpu or cuda.\""
        },
        {
            "sha": "847d79c5e886020c2dfc421a3950b9458c0abbd5",
            "filename": "src/transformers/models/nougat/convert_nougat_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fnougat%2Fconvert_nougat_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fnougat%2Fconvert_nougat_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Fconvert_nougat_to_hf.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -275,7 +275,7 @@ def convert_nougat_checkpoint(model_tag, pytorch_dump_folder_path=None, push_to_\n     parser.add_argument(\n         \"--push_to_hub\",\n         action=\"store_true\",\n-        help=\"Whether or not to push the converted model and processor to the ü§ó hub.\",\n+        help=\"Whether or not to push the converted model and processor to the Hugging Face hub.\",\n     )\n \n     args = parser.parse_args()"
        },
        {
            "sha": "bc3a761389895859646751355f9f168caaafc7df",
            "filename": "src/transformers/models/omdet_turbo/convert_omdet_turbo_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fconvert_omdet_turbo_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fconvert_omdet_turbo_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fconvert_omdet_turbo_to_hf.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -339,7 +339,9 @@ def convert_omdet_turbo_checkpoint(args):\n         \"--pytorch_dump_folder_path\", default=None, type=str, help=\"Path to the output PyTorch model directory.\"\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n     parser.add_argument(\n         \"--use_timm_backbone\", action=\"store_true\", help=\"Whether or not to use timm backbone for vision backbone.\""
        },
        {
            "sha": "2c879e2a8821574ecb71070f00d4688457b620f7",
            "filename": "src/transformers/models/oneformer/convert_to_hf_oneformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Foneformer%2Fconvert_to_hf_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Foneformer%2Fconvert_to_hf_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fconvert_to_hf_oneformer.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -926,7 +926,7 @@ def using_dirs(checkpoints_dir: Path, config_dir: Path) -> Iterator[tuple[object\n         checkpoints: list[Path] = checkpoints_dir.glob(\"**/*.pth\")\n \n         for checkpoint in checkpoints:\n-            logger.info(f\"üí™ Converting {checkpoint.stem}\")\n+            logger.info(f\"Converting {checkpoint.stem}\")\n             # find associated config file\n             config: Path = config_dir / f\"{checkpoint.stem}.yaml\"\n \n@@ -1054,7 +1054,7 @@ def _preprocess_text(text_list=None, max_length=77):\n             \"The segmentation image is not the same.\"\n         )\n \n-        logger.info(\"‚úÖ Test passed!\")\n+        logger.info(\"Test passed!\")\n \n \n def get_name(checkpoint_file: Path):\n@@ -1175,7 +1175,7 @@ def get_name(checkpoint_file: Path):\n         )\n \n         model_name = get_name(checkpoint_file)\n-        logger.info(f\"ü™Ñ Saving {model_name}\")\n+        logger.info(f\"Saving {model_name}\")\n \n         processor.save_pretrained(save_directory / model_name)\n         oneformer_for_universal_segmentation.save_pretrained(save_directory / model_name)"
        },
        {
            "sha": "8edcf6508f282973dce37a011ca674afd0dd8713",
            "filename": "src/transformers/models/rag/tokenization_rag.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Frag%2Ftokenization_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Frag%2Ftokenization_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Ftokenization_rag.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -86,7 +86,7 @@ def prepare_seq2seq_batch(\n         **kwargs,\n     ) -> BatchEncoding:\n         warnings.warn(\n-            \"`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of ü§ó Transformers. Use the \"\n+            \"`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of Hugging Face Transformers. Use the \"\n             \"regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` \"\n             \"context manager to prepare your targets. See the documentation of your specific tokenizer for more \"\n             \"details\","
        },
        {
            "sha": "7921421c391dd826340356b11a97dd2602b04ffa",
            "filename": "src/transformers/models/roberta/convert_roberta_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Froberta%2Fconvert_roberta_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Froberta%2Fconvert_roberta_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fconvert_roberta_original_pytorch_checkpoint_to_pytorch.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -150,7 +150,7 @@ def convert_roberta_checkpoint_to_pytorch(\n     max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()\n     print(f\"max_absolute_diff = {max_absolute_diff}\")  # ~ 1e-7\n     success = torch.allclose(our_output, their_output, atol=1e-3)\n-    print(\"Do both models output the same tensors?\", \"üî•\" if success else \"üí©\")\n+    print(\"Do both models output the same tensors?\", \"[PASS]\" if success else \"[FAIL]\")\n     if not success:\n         raise Exception(\"Something went wRoNg\")\n "
        },
        {
            "sha": "e3266a8b33ea8c81b09b9b28f7a788f1aac881ef",
            "filename": "src/transformers/models/seggpt/convert_seggpt_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fseggpt%2Fconvert_seggpt_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fseggpt%2Fconvert_seggpt_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseggpt%2Fconvert_seggpt_to_hf.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -214,7 +214,9 @@ def convert_seggpt_checkpoint(args):\n         help=\"Whether or not to verify the logits against the original implementation.\",\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n \n     args = parser.parse_args()"
        },
        {
            "sha": "0e61e745fbef10a29c18d6df239e9c9055fe9fce",
            "filename": "src/transformers/models/siglip/convert_siglip_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fsiglip%2Fconvert_siglip_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fsiglip%2Fconvert_siglip_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fconvert_siglip_to_hf.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -525,7 +525,9 @@ def convert_siglip_checkpoint(model_name, pytorch_dump_folder_path, verify_logit\n         help=\"Whether to verify logits against the original implementation.\",\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n \n     args = parser.parse_args()"
        },
        {
            "sha": "657d8d9e37f01cda704f9f6cac5673e358004851",
            "filename": "src/transformers/models/siglip2/convert_siglip2_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fconvert_siglip2_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fconvert_siglip2_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fconvert_siglip2_to_hf.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -431,7 +431,9 @@ def convert_siglip2_checkpoint(model_name, pytorch_dump_folder_path, verify_logi\n         help=\"Whether to verify logits against the original implementation.\",\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n \n     args = parser.parse_args()"
        },
        {
            "sha": "11e92c98e6d0c0c355d0601fb2fd1735095ca930",
            "filename": "src/transformers/models/speecht5/convert_hifigan.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fconvert_hifigan.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fconvert_hifigan.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fconvert_hifigan.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -95,7 +95,7 @@ def convert_hifigan_checkpoint(\n         \"--pytorch_dump_folder_path\", required=True, default=None, type=str, help=\"Path to the output PyTorch model.\"\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", default=None, type=str, help=\"Where to upload the converted model on the ü§ó hub.\"\n+        \"--push_to_hub\", default=None, type=str, help=\"Where to upload the converted model on the Hugging Face hub.\"\n     )\n \n     args = parser.parse_args()"
        },
        {
            "sha": "10a834f11a9e67ed5292f307139e3a1b69987cb7",
            "filename": "src/transformers/models/speecht5/convert_speecht5_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fconvert_speecht5_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fconvert_speecht5_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fconvert_speecht5_original_pytorch_checkpoint_to_pytorch.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -387,7 +387,7 @@ def convert_speecht5_checkpoint(\n         \"--pytorch_dump_folder_path\", required=True, default=None, type=str, help=\"Path to the output PyTorch model.\"\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", default=None, type=str, help=\"Where to upload the converted model on the ü§ó hub.\"\n+        \"--push_to_hub\", default=None, type=str, help=\"Where to upload the converted model on the Hugging Face hub.\"\n     )\n \n     args = parser.parse_args()"
        },
        {
            "sha": "6995b1c9f9b68cdea2d3179049253a4ec3adddd8",
            "filename": "src/transformers/models/swin/convert_swin_simmim_to_pytorch.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fswin%2Fconvert_swin_simmim_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fswin%2Fconvert_swin_simmim_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin%2Fconvert_swin_simmim_to_pytorch.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -175,7 +175,9 @@ def convert_swin_checkpoint(model_name, checkpoint_path, pytorch_dump_folder_pat\n         \"--pytorch_dump_folder_path\", default=None, type=str, help=\"Path to the output PyTorch model directory.\"\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n \n     args = parser.parse_args()"
        },
        {
            "sha": "32abb7eeeff4dc7d6b60e047735c8409db3beb11",
            "filename": "src/transformers/models/table_transformer/convert_table_transformer_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fconvert_table_transformer_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fconvert_table_transformer_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fconvert_table_transformer_to_hf.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -311,7 +311,9 @@ def convert_table_transformer_checkpoint(checkpoint_url, pytorch_dump_folder_pat\n         \"--pytorch_dump_folder_path\", default=None, type=str, help=\"Path to the folder to output PyTorch model.\"\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n     args = parser.parse_args()\n     convert_table_transformer_checkpoint(args.checkpoint_url, args.pytorch_dump_folder_path, args.push_to_hub)"
        },
        {
            "sha": "6efe0868f7a86906e7044d125914767d5e384222",
            "filename": "src/transformers/models/table_transformer/convert_table_transformer_to_hf_no_timm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fconvert_table_transformer_to_hf_no_timm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fconvert_table_transformer_to_hf_no_timm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fconvert_table_transformer_to_hf_no_timm.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -428,7 +428,9 @@ def convert_table_transformer_checkpoint(checkpoint_url, pytorch_dump_folder_pat\n         \"--pytorch_dump_folder_path\", default=None, type=str, help=\"Path to the folder to output PyTorch model.\"\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n     args = parser.parse_args()\n     convert_table_transformer_checkpoint(args.checkpoint_url, args.pytorch_dump_folder_path, args.push_to_hub)"
        },
        {
            "sha": "5eacd11bc31b9d76a15ff1089d9d07f5744c059d",
            "filename": "src/transformers/models/timesformer/convert_timesformer_to_pytorch.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Ftimesformer%2Fconvert_timesformer_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Ftimesformer%2Fconvert_timesformer_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimesformer%2Fconvert_timesformer_to_pytorch.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -244,7 +244,9 @@ def convert_timesformer_checkpoint(checkpoint_url, pytorch_dump_folder_path, mod\n     )\n     parser.add_argument(\"--model_name\", default=\"timesformer-base-finetuned-k400\", type=str, help=\"Name of the model.\")\n     parser.add_argument(\n-        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n \n     args = parser.parse_args()"
        },
        {
            "sha": "0404af925154423def2fbfaafc6bcde3ee75ddfd",
            "filename": "src/transformers/models/udop/convert_udop_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fudop%2Fconvert_udop_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fudop%2Fconvert_udop_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fconvert_udop_to_hf.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -217,7 +217,9 @@ def convert_udop_checkpoint(model_name, pytorch_dump_folder_path=None, push_to_h\n         \"--pytorch_dump_folder_path\", default=None, type=str, help=\"Path to the output PyTorch model directory.\"\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n \n     args = parser.parse_args()"
        },
        {
            "sha": "ee798ecceb105a7559d1e70bcbace6515a254827",
            "filename": "src/transformers/models/univnet/convert_univnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Funivnet%2Fconvert_univnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Funivnet%2Fconvert_univnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funivnet%2Fconvert_univnet.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -141,7 +141,7 @@ def main():\n         \"--pytorch_dump_folder_path\", required=True, default=None, type=str, help=\"Path to the output PyTorch model.\"\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", default=None, type=str, help=\"Where to upload the converted model on the ü§ó hub.\"\n+        \"--push_to_hub\", default=None, type=str, help=\"Where to upload the converted model on the Hugging Face hub.\"\n     )\n     parser.add_argument(\n         \"--safe_serialization\", action=\"store_true\", help=\"Whether to save the model using `safetensors`.\""
        },
        {
            "sha": "35b8cda2919ed49707f9cf1787f2914dc653515a",
            "filename": "src/transformers/models/upernet/convert_convnext_upernet_to_pytorch.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fupernet%2Fconvert_convnext_upernet_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fupernet%2Fconvert_convnext_upernet_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fupernet%2Fconvert_convnext_upernet_to_pytorch.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -207,7 +207,9 @@ def convert_upernet_checkpoint(model_name, pytorch_dump_folder_path, push_to_hub\n         \"--pytorch_dump_folder_path\", default=None, type=str, help=\"Path to the output PyTorch model directory.\"\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n \n     args = parser.parse_args()"
        },
        {
            "sha": "fb715a579d421320a15b22fa4b1e3d54f853b0c7",
            "filename": "src/transformers/models/upernet/convert_swin_upernet_to_pytorch.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fupernet%2Fconvert_swin_upernet_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fupernet%2Fconvert_swin_upernet_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fupernet%2Fconvert_swin_upernet_to_pytorch.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -290,7 +290,9 @@ def convert_upernet_checkpoint(model_name, pytorch_dump_folder_path, push_to_hub\n         \"--pytorch_dump_folder_path\", default=None, type=str, help=\"Path to the output PyTorch model directory.\"\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n \n     args = parser.parse_args()"
        },
        {
            "sha": "67af82f3a4d10f4fa58ca133f048447cb506c9f2",
            "filename": "src/transformers/models/videomae/convert_videomae_to_pytorch.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fvideomae%2Fconvert_videomae_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fvideomae%2Fconvert_videomae_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideomae%2Fconvert_videomae_to_pytorch.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -317,7 +317,9 @@ def convert_videomae_checkpoint(checkpoint_url, pytorch_dump_folder_path, model_\n     )\n     parser.add_argument(\"--model_name\", default=\"videomae-base\", type=str, help=\"Name of the model.\")\n     parser.add_argument(\n-        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n \n     args = parser.parse_args()"
        },
        {
            "sha": "33cc0f21787aa6bc3b7c898aea5f421868d3f25d",
            "filename": "src/transformers/models/vitmatte/convert_vitmatte_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fconvert_vitmatte_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fconvert_vitmatte_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fconvert_vitmatte_to_hf.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -163,7 +163,9 @@ def convert_vitmatte_checkpoint(model_name, pytorch_dump_folder_path, push_to_hu\n         \"--pytorch_dump_folder_path\", default=None, type=str, help=\"Path to the output PyTorch model directory.\"\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n \n     args = parser.parse_args()"
        },
        {
            "sha": "1f35f9c27c374744ead3c5c79d1c4b9e4d652477",
            "filename": "src/transformers/models/vitpose/convert_vitpose_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fvitpose%2Fconvert_vitpose_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fvitpose%2Fconvert_vitpose_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose%2Fconvert_vitpose_to_hf.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -410,7 +410,9 @@ def main():\n         \"--pytorch_dump_folder_path\", default=None, type=str, help=\"Path to store the converted model.\"\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n     parser.add_argument(\n         \"--check_logits\", action=\"store_false\", help=\"Whether or not to verify the logits of the converted model.\""
        },
        {
            "sha": "4a96bf12c64f8ff770e7df2f23ab109f77b51de8",
            "filename": "src/transformers/models/vits/convert_original_checkpoint.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fvits%2Fconvert_original_checkpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fvits%2Fconvert_original_checkpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvits%2Fconvert_original_checkpoint.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -374,7 +374,7 @@ def convert_checkpoint(\n         \"--pytorch_dump_folder_path\", required=True, default=None, type=str, help=\"Path to the output PyTorch model.\"\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", default=None, type=str, help=\"Where to upload the converted model on the ü§ó hub.\"\n+        \"--push_to_hub\", default=None, type=str, help=\"Where to upload the converted model on the Hugging Face hub.\"\n     )\n \n     args = parser.parse_args()"
        },
        {
            "sha": "7970e074208400973817dff1bf4b5ac37017ded3",
            "filename": "src/transformers/models/vjepa2/convert_vjepa2_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fconvert_vjepa2_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fconvert_vjepa2_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fconvert_vjepa2_to_hf.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -336,7 +336,7 @@ def convert_and_test_vjepa2_checkpoint(model_name, pytorch_dump_folder_path, pus\n     parser.add_argument(\n         \"--push_to_hub\",\n         action=\"store_true\",\n-        help=\"Whether or not to push the converted model to the ü§ó hub.\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n     parser.add_argument(\"--upload_original\", action=\"store_true\", help=\"upload the original checkpoint\")\n "
        },
        {
            "sha": "807fafb6441e7abae02ff140e951581d2af20e26",
            "filename": "src/transformers/models/x_clip/convert_x_clip_original_pytorch_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fx_clip%2Fconvert_x_clip_original_pytorch_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fx_clip%2Fconvert_x_clip_original_pytorch_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fx_clip%2Fconvert_x_clip_original_pytorch_to_hf.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -379,7 +379,9 @@ def convert_xclip_checkpoint(model_name, pytorch_dump_folder_path=None, push_to_\n         \"--pytorch_dump_folder_path\", default=None, type=str, help=\"Path to the output PyTorch model directory.\"\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n \n     args = parser.parse_args()"
        },
        {
            "sha": "6694d037bc9fb347c0a34f046e3a871ff0a3ad39",
            "filename": "src/transformers/models/xcodec/convert_xcodec_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fxcodec%2Fconvert_xcodec_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fxcodec%2Fconvert_xcodec_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxcodec%2Fconvert_xcodec_weights_to_hf.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -332,7 +332,7 @@ def convert_checkpoint(checkpoint_path, config_path, pytorch_dump_folder_path=No\n     )\n     parser.add_argument(\"--pytorch_dump_folder_path\", default=None, type=str, help=\"Path to the output PyTorch model.\")\n     parser.add_argument(\n-        \"--push_to_hub\", default=None, type=str, help=\"Where to upload the converted model on the ü§ó hub.\"\n+        \"--push_to_hub\", default=None, type=str, help=\"Where to upload the converted model on the Hugging Face hub.\"\n     )\n \n     args = parser.parse_args()"
        },
        {
            "sha": "95c98f98ea64a34dc67c5369e2e482d866df40dd",
            "filename": "src/transformers/models/xlm_roberta_xl/convert_xlm_roberta_xl_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fconvert_xlm_roberta_xl_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fconvert_xlm_roberta_xl_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fconvert_xlm_roberta_xl_original_pytorch_checkpoint_to_pytorch.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -156,7 +156,7 @@ def convert_xlm_roberta_xl_checkpoint_to_pytorch(\n     max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()\n     print(f\"max_absolute_diff = {max_absolute_diff}\")  # ~ 1e-7\n     success = torch.allclose(our_output, their_output, atol=1e-3)\n-    print(\"Do both models output the same tensors?\", \"üî•\" if success else \"üí©\")\n+    print(\"Do both models output the same tensors?\", \"[PASS]\" if success else \"[FAIL]\")\n     if not success:\n         raise Exception(\"Something went wRoNg\")\n "
        },
        {
            "sha": "0b9c72b0003ab3fee009275cd31aa58642389da2",
            "filename": "src/transformers/models/xmod/convert_xmod_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fxmod%2Fconvert_xmod_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fxmod%2Fconvert_xmod_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxmod%2Fconvert_xmod_original_pytorch_checkpoint_to_pytorch.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -185,7 +185,7 @@ def convert_xmod_checkpoint_to_pytorch(\n     max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()\n     print(f\"max_absolute_diff = {max_absolute_diff}\")  # ~ 1e-7\n     success = torch.allclose(our_output, their_output, atol=1e-3)\n-    print(\"Do both models output the same tensors?\", \"üî•\" if success else \"üí©\")\n+    print(\"Do both models output the same tensors?\", \"[PASS]\" if success else \"[FAIL]\")\n     if not success:\n         raise Exception(\"Something went wRoNg\")\n "
        },
        {
            "sha": "f971550ad9105f78bc5a0132c79c19990c8694ca",
            "filename": "src/transformers/models/yolos/convert_yolos_to_pytorch.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fyolos%2Fconvert_yolos_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fmodels%2Fyolos%2Fconvert_yolos_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fconvert_yolos_to_pytorch.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -260,7 +260,9 @@ def convert_yolos_checkpoint(\n         \"--pytorch_dump_folder_path\", default=None, type=str, help=\"Path to the output PyTorch model directory.\"\n     )\n     parser.add_argument(\n-        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the converted model to the Hugging Face hub.\",\n     )\n \n     args = parser.parse_args()"
        },
        {
            "sha": "eeeb8ac3c26b26714450e9f1b96adab86a5a189e",
            "filename": "src/transformers/pipelines/image_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fpipelines%2Fimage_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Fpipelines%2Fimage_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fimage_to_text.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -157,7 +157,7 @@ def preprocess(self, image, prompt=None, timeout=None):\n         if prompt is not None:\n             logger.warning_once(\n                 \"Passing `prompt` to the `image-to-text` pipeline is deprecated and will be removed in version 4.48\"\n-                \" of ü§ó Transformers. Use the `image-text-to-text` pipeline instead\",\n+                \" of Hugging Face Transformers. Use the `image-text-to-text` pipeline instead\",\n             )\n             if not isinstance(prompt, str):\n                 raise ValueError("
        },
        {
            "sha": "6d21445cf1387af44a46adc5bad466a34e1bd7f7",
            "filename": "src/transformers/utils/auto_docstring.py",
            "status": "modified",
            "additions": 11,
            "deletions": 9,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Futils%2Fauto_docstring.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/src%2Ftransformers%2Futils%2Fauto_docstring.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fauto_docstring.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -1126,7 +1126,7 @@ def get_model_name(obj):\n         if file_name.startswith(start) and file_name.endswith(end):\n             model_name_lowercase = file_name[len(start) : -len(end)]\n             return model_name_lowercase\n-    print(f\"üö® Something went wrong trying to find the model name in the path: {path}\")\n+    print(f\"[ERROR] Something went wrong trying to find the model name in the path: {path}\")\n     return \"model\"\n \n \n@@ -1273,7 +1273,7 @@ def _get_model_info(func, parent_class):\n             else:\n                 config_class = \"ModelConfig\"\n                 print(\n-                    f\"üö® Config not found for {model_name_lowercase}. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/auto_docstring.py\"\n+                    f\"[ERROR] Config not found for {model_name_lowercase}. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/auto_docstring.py\"\n                 )\n \n     return model_name_lowercase, class_name, config_class\n@@ -1300,7 +1300,7 @@ def _process_parameter_type(param, param_name, func):\n         else:\n             if False:\n                 print(\n-                    f\"üö® {param_type} for {param_name} of {func.__qualname__} in file {func.__code__.co_filename} has an invalid type\"\n+                    f\"[ERROR] {param_type} for {param_name} of {func.__qualname__} in file {func.__code__.co_filename} has an invalid type\"\n                 )\n         if \"ForwardRef\" in param_type:\n             param_type = re.sub(r\"ForwardRef\\('([\\w.]+)'\\)\", r\"\\1\", param_type)\n@@ -1409,7 +1409,7 @@ def _process_regular_parameters(\n                 else:\n                     param_type = f\"[`{param_type.split('.')[-1]}`]\"\n             # elif param_type == \"\" and False:  # TODO: Enforce typing for all parameters\n-            #     print(f\"üö® {param_name} for {func.__qualname__} in file {func.__code__.co_filename} has no type\")\n+            #     print(f\"[ERROR] {param_name} for {func.__qualname__} in file {func.__code__.co_filename} has no type\")\n             param_type = param_type if \"`\" in param_type else f\"`{param_type}`\"\n             # Format the parameter docstring\n             if additional_info:\n@@ -1431,7 +1431,7 @@ def _process_regular_parameters(\n                 \"default\": param_default,\n             }\n             undocumented_parameters.append(\n-                f\"üö® `{param_name}` is part of {func.__qualname__}'s signature, but not documented. Make sure to add it to the docstring of the function in {func.__code__.co_filename}.\"\n+                f\"[ERROR] `{param_name}` is part of {func.__qualname__}'s signature, but not documented. Make sure to add it to the docstring of the function in {func.__code__.co_filename}.\"\n             )\n \n     return docstring, missing_args\n@@ -1524,7 +1524,7 @@ def _process_kwargs_parameters(sig, func, parent_class, documented_kwargs, inden\n                     # Check if type is missing\n                     if param_type == \"\":\n                         print(\n-                            f\"üö® {param_name} for {kwarg_param.annotation.__args__[0].__qualname__} in file {func.__code__.co_filename} has no type\"\n+                            f\"[ERROR] {param_name} for {kwarg_param.annotation.__args__[0].__qualname__} in file {func.__code__.co_filename} has no type\"\n                         )\n                     param_type = param_type if \"`\" in param_type else f\"`{param_type}`\"\n                     # Format the parameter docstring\n@@ -1540,7 +1540,7 @@ def _process_kwargs_parameters(sig, func, parent_class, documented_kwargs, inden\n                         )\n                 else:\n                     undocumented_parameters.append(\n-                        f\"üö® `{param_name}` is part of {kwarg_param.annotation.__args__[0].__qualname__}, but not documented. Make sure to add it to the docstring of the function in {func.__code__.co_filename}.\"\n+                        f\"[ERROR] `{param_name}` is part of {kwarg_param.annotation.__args__[0].__qualname__}, but not documented. Make sure to add it to the docstring of the function in {func.__code__.co_filename}.\"\n                     )\n \n     return docstring\n@@ -1692,7 +1692,7 @@ def _process_example_section(\n                 example_docstring = set_min_indent(example_annotation, indent_level + 4)\n             else:\n                 print(\n-                    f\"üö® No checkpoint found for {class_name}.{func.__name__}. Please add a `checkpoint` arg to `auto_docstring` or add one in {config_class}'s docstring\"\n+                    f\"[ERROR] No checkpoint found for {class_name}.{func.__name__}. Please add a `checkpoint` arg to `auto_docstring` or add one in {config_class}'s docstring\"\n                 )\n         else:\n             # Check if the model is in a pipeline to get an example\n@@ -1865,7 +1865,9 @@ def auto_class_docstring(cls, custom_intro=None, custom_args=None, checkpoint=No\n                 if is_documented:\n                     # Check if type is missing\n                     if param_type == \"\":\n-                        print(f\"üö® {param_name} for {cls.__qualname__} in file {cls.__code__.co_filename} has no type\")\n+                        print(\n+                            f\"[ERROR] {param_name} for {cls.__qualname__} in file {cls.__code__.co_filename} has no type\"\n+                        )\n                     param_type = param_type if \"`\" in param_type else f\"`{param_type}`\"\n                     # Format the parameter docstring\n                     if additional_info:"
        },
        {
            "sha": "f67e8e367d33320e56b25045a7609a157eb40f17",
            "filename": "templates/adding_a_new_example_script/{{cookiecutter.directory_name}}/run_{{cookiecutter.example_shortcut}}.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/templates%2Fadding_a_new_example_script%2F%7B%7Bcookiecutter.directory_name%7D%7D%2Frun_%7B%7Bcookiecutter.example_shortcut%7D%7D.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/templates%2Fadding_a_new_example_script%2F%7B%7Bcookiecutter.directory_name%7D%7D%2Frun_%7B%7Bcookiecutter.example_shortcut%7D%7D.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/templates%2Fadding_a_new_example_script%2F%7B%7Bcookiecutter.directory_name%7D%7D%2Frun_%7B%7Bcookiecutter.example_shortcut%7D%7D.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -596,7 +596,7 @@ def parse_args():\n     parser.add_argument(\n         \"--use_slow_tokenizer\",\n         action=\"store_true\",\n-        help=\"If passed, will use a slow tokenizer (not backed by the ü§ó Tokenizers library).\",\n+        help=\"If passed, will use a slow tokenizer (not backed by the Hugging Face Tokenizers library).\",\n     )\n     parser.add_argument(\n         \"--per_device_train_batch_size\","
        },
        {
            "sha": "e6521b33b1c93474cb0228814d0940d8b1c5755d",
            "filename": "tests/trainer/test_trainer_tpu.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/tests%2Ftrainer%2Ftest_trainer_tpu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/tests%2Ftrainer%2Ftest_trainer_tpu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer_tpu.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -115,7 +115,7 @@ def compute_metrics(p: EvalPrediction) -> dict:\n \n         trainer.args.eval_accumulation_steps = None\n \n-    logger.info(\"üî• All distributed tests successful\")\n+    logger.info(\"All distributed tests successful\")\n \n \n def _mp_fn(index):"
        },
        {
            "sha": "ba5b85f8f2b23bea26dd1fff0a480c23c6bd2cb6",
            "filename": "utils/check_copies.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/utils%2Fcheck_copies.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/utils%2Fcheck_copies.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_copies.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -97,7 +97,7 @@\n         ),\n     },\n     \"README_ja.md\": {\n-        \"start_prompt\": \"ü§óTransformers„ÅØÁèæÂú®„ÄÅ‰ª•‰∏ã„ÅÆ„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„ÇíÊèê‰æõ„Åó„Å¶„ÅÑ„Åæ„Åô\",\n+        \"start_prompt\": \"ü§ó Transformers„ÅØÁèæÂú®„ÄÅ‰ª•‰∏ã„ÅÆ„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„ÇíÊèê‰æõ„Åó„Å¶„ÅÑ„Åæ„Åô\",\n         \"end_prompt\": \"1. Êñ∞„Åó„ÅÑ„É¢„Éá„É´„ÇíÊäïÁ®ø„Åó„Åü„ÅÑ„Åß„Åô„ÅãÔºü\",\n         \"format_model_list\": (\n             \"**[{title}]({model_link})** ({paper_affiliations} „Åã„Çâ) {paper_authors}.{supplements} „Åã„ÇâÂÖ¨Èñã„Åï„Çå„ÅüÁ†îÁ©∂Ë´ñÊñá\""
        },
        {
            "sha": "8584b80dcafa86542d051c6852e92cb6b50a8671",
            "filename": "utils/check_docstrings.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/utils%2Fcheck_docstrings.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/utils%2Fcheck_docstrings.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_docstrings.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -1363,19 +1363,19 @@ def check_auto_docstrings(overwrite: bool = False, check_all: bool = False):\n                 print(\n                     \"Some docstrings are missing. Run `make fix-copies` or `python utils/check_docstrings.py --fix_and_overwrite` to generate the docstring templates where needed.\"\n                 )\n-            print(f\"üö® Missing docstring for the following arguments in {candidate_file}:\")\n+            print(f\"[ERROR] Missing docstring for the following arguments in {candidate_file}:\")\n             for warning in missing_docstring_args_warnings:\n                 print(warning)\n         if docstring_args_ro_remove_warnings:\n             if not overwrite:\n                 print(\n                     \"Some docstrings are redundant with the ones in `auto_docstring.py` and will be removed. Run `make fix-copies` or `python utils/check_docstrings.py --fix_and_overwrite` to remove the redundant docstrings.\"\n                 )\n-            print(f\"üö® Redundant docstring for the following arguments in {candidate_file}:\")\n+            print(f\"[ERROR] Redundant docstring for the following arguments in {candidate_file}:\")\n             for warning in docstring_args_ro_remove_warnings:\n                 print(warning)\n         if fill_docstring_args_warnings:\n-            print(f\"üö® Docstring needs to be filled for the following arguments in {candidate_file}:\")\n+            print(f\"[ERROR] Docstring needs to be filled for the following arguments in {candidate_file}:\")\n             for warning in fill_docstring_args_warnings:\n                 print(warning)\n "
        },
        {
            "sha": "18cb6e9dd08b7a48da126627168d5c9cc47cea6b",
            "filename": "utils/notification_service.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/utils%2Fnotification_service.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/utils%2Fnotification_service.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnotification_service.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -227,7 +227,7 @@ def no_failures(self) -> dict:\n             \"type\": \"section\",\n             \"text\": {\n                 \"type\": \"plain_text\",\n-                \"text\": f\"üåû There were no failures: all {self.n_tests} tests passed. The suite ran in {self.time}.\",\n+                \"text\": f\"[SUCCESS] There were no failures: all {self.n_tests} tests passed. The suite ran in {self.time}.\",\n                 \"emoji\": True,\n             },\n             \"accessory\": {\n@@ -245,7 +245,7 @@ def failures(self) -> dict:\n                 \"type\": \"plain_text\",\n                 \"text\": (\n                     f\"There were {self.n_failures} failures, out of {self.n_tests} tests.\\n\"\n-                    f\"üö® There were {self.n_jobs_errored_out} jobs errored out (not producing test output files).\\n\"\n+                    f\"[ERROR] There were {self.n_jobs_errored_out} jobs errored out (not producing test output files).\\n\"\n                     f\"The suite ran in {self.time}.\"\n                 ),\n                 \"emoji\": True,\n@@ -712,16 +712,16 @@ def error_out(title, ci_title=\"\", runner_not_available=False, runner_failed=Fals\n \n         offline_runners = []\n         if runner_not_available:\n-            text = \"üíî CI runners are not available! Tests are not run. üò≠\"\n+            text = \"[FAIL] CI runners are not available! Tests are not run.\"\n             result = os.environ.get(\"OFFLINE_RUNNERS\")\n             if result is not None:\n                 offline_runners = json.loads(result)\n         elif runner_failed:\n-            text = \"üíî CI runners have problems! Tests are not run. üò≠\"\n+            text = \"[FAIL] CI runners have problems! Tests are not run.\"\n         elif setup_failed:\n-            text = \"üíî Setup job failed. Tests are not run. üò≠\"\n+            text = \"[FAIL] Setup job failed. Tests are not run.\"\n         else:\n-            text = \"üíî There was an issue running the tests. üò≠\"\n+            text = \"[FAIL] There was an issue running the tests.\"\n \n         error_block_1 = {\n             \"type\": \"header\",\n@@ -735,7 +735,7 @@ def error_out(title, ci_title=\"\", runner_not_available=False, runner_failed=Fals\n         if len(offline_runners) > 0:\n             text = \"\\n  ‚Ä¢ \" + \"\\n  ‚Ä¢ \".join(offline_runners)\n             text = f\"The following runners are offline:\\n{text}\\n\\n\"\n-        text += \"üôè Let's fix it ASAP! üôè\"\n+        text += \"Let's fix it ASAP!\"\n \n         error_block_2 = {\n             \"type\": \"section\",\n@@ -1120,7 +1120,7 @@ def pop_default(l: list[Any], i: int, default: Any) -> Any:\n         ci_title = \"\"\n \n     # `title` will be updated at the end before calling `Message()`.\n-    title = f\"ü§ó Results of {ci_event}\"\n+    title = f\"[INFO] Results of {ci_event}\"\n     if runner_not_available or runner_failed or setup_failed:\n         Message.error_out(title, ci_title, runner_not_available, runner_failed, setup_failed)\n         exit(0)\n@@ -1604,7 +1604,7 @@ def pop_default(l: list[Any], i: int, default: Any) -> Any:\n     if job_name in job_to_test_map:\n         ci_name_in_report = job_to_test_map[job_name]\n \n-    title = f\"ü§ó Results of {ci_event}: {ci_name_in_report}\"\n+    title = f\"[INFO] Results of {ci_event}: {ci_name_in_report}\"\n \n     message = Message(\n         title,"
        },
        {
            "sha": "001cf7f57beffb663e40ea8d755813c08c5e4a22",
            "filename": "utils/notification_service_doc_tests.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/utils%2Fnotification_service_doc_tests.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/utils%2Fnotification_service_doc_tests.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnotification_service_doc_tests.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -379,6 +379,6 @@ def add_path(self, path: str):\n     with open(\"doc_test_results/doc_test_results.json\", \"w\", encoding=\"UTF-8\") as fp:\n         json.dump(doc_test_results, fp, ensure_ascii=False, indent=4)\n \n-    message = Message(\"ü§ó Results of the doc tests.\", doc_test_results)\n+    message = Message(\"[INFO] Results of the doc tests.\", doc_test_results)\n     message.post()\n     message.post_reply()"
        },
        {
            "sha": "add6dc466027d23f137c5cd9d07df45f8be43f9f",
            "filename": "utils/patch_helper.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/utils%2Fpatch_helper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/utils%2Fpatch_helper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fpatch_helper.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -64,9 +64,9 @@ def checkout_branch(branch):\n     \"\"\"Checkout the target branch.\"\"\"\n     try:\n         subprocess.run([\"git\", \"checkout\", branch], check=True)\n-        print(f\"‚úÖ Checked out branch: {branch}\")\n+        print(f\"[SUCCESS] Checked out branch: {branch}\")\n     except subprocess.CalledProcessError:\n-        print(f\"‚ùå Failed to checkout branch: {branch}. Does it exist?\")\n+        print(f\"[FAIL] Failed to checkout branch: {branch}. Does it exist?\")\n         exit(1)\n \n \n@@ -108,9 +108,9 @@ def cherry_pick_commit(sha):\n     \"\"\"Cherry-pick a given commit SHA.\"\"\"\n     try:\n         subprocess.run([\"git\", \"cherry-pick\", sha], check=True)\n-        print(f\"‚úÖ Cherry-picked commit {sha}\")\n+        print(f\"[SUCCESS] Cherry-picked commit {sha}\")\n     except subprocess.CalledProcessError:\n-        print(f\"‚ö†Ô∏è Failed to cherry-pick {sha}. Manual intervention required.\")\n+        print(f\"[WARNING] Failed to cherry-pick {sha}. Manual intervention required.\")\n \n \n def commit_in_history(commit_sha, base_branch=\"HEAD\"):\n@@ -135,8 +135,8 @@ def main(verbose=False):\n             pr[\"timestamp\"] = get_commit_timestamp(sha)\n         else:\n             print(\"\\n\" + \"=\" * 80)\n-            print(f\"‚ö†Ô∏è  WARNING: PR #{pr['number']} ({sha}) is NOT in main!\")\n-            print(\"‚ö†Ô∏è  A core maintainer must review this before cherry-picking.\")\n+            print(f\"[WARNING] PR #{pr['number']} ({sha}) is NOT in main!\")\n+            print(\"[WARNING] A core maintainer must review this before cherry-picking.\")\n             print(\"=\" * 80 + \"\\n\")\n     # Sort by commit timestamp (ascending)\n     prs = [pr for pr in prs if pr.get(\"timestamp\") is not None]\n@@ -146,9 +146,9 @@ def main(verbose=False):\n         if sha:\n             if commit_in_history(sha):\n                 if verbose:\n-                    print(f\"üîÅ PR #{pr['number']} ({pr['title']}) already in history. Skipping.\")\n+                    print(f\"[INFO] PR #{pr['number']} ({pr['title']}) already in history. Skipping.\")\n             else:\n-                print(f\"üöÄ PR #{pr['number']} ({pr['title']}) not in history. Cherry-picking...\")\n+                print(f\"[INFO] PR #{pr['number']} ({pr['title']}) not in history. Cherry-picking...\")\n                 cherry_pick_commit(sha)\n \n "
        },
        {
            "sha": "d01f4e69a02a07e9d4ab8fdce33d7c4efda2cc5c",
            "filename": "utils/scan_skipped_tests.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/31839d741a30d7fe5daf982d9f2d9cc549ec6181/utils%2Fscan_skipped_tests.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31839d741a30d7fe5daf982d9f2d9cc549ec6181/utils%2Fscan_skipped_tests.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fscan_skipped_tests.py?ref=31839d741a30d7fe5daf982d9f2d9cc549ec6181",
            "patch": "@@ -135,7 +135,7 @@ def summarize_all_tests(\n     total_models = len(model_names)\n     test_names = list(tests_with_origin)\n \n-    print(f\"üìù Aggregating {len(test_names)} tests...\")\n+    print(f\"[INFO] Aggregating {len(test_names)} tests...\")\n     for index, test_fn in enumerate(test_names, 1):\n         print(f\"  ({index}/{len(test_names)}) {test_fn}\", end=\"\\r\")\n         models_ran, models_skipped, reasons_for_skipping = [], [], []\n@@ -155,7 +155,7 @@ def summarize_all_tests(\n             \"skipped_proportion\": round(skipped_ratio, 4),\n             \"reasons_skipped\": sorted(reasons_for_skipping),\n         }\n-    print(\"\\n‚úÖ Scan complete.\")\n+    print(\"\\n[INFO] Scan complete.\")\n     return results\n \n \n@@ -182,7 +182,7 @@ def main() -> None:\n         tests_with_origin = {test_method_name: tests_with_origin.get(test_method_name, \"unknown\")}\n \n     model_names, model_test_files = get_models_and_test_files(MODELS_DIR)\n-    print(f\"üî¨ Parsing {len(model_test_files)} model test files once each...\")\n+    print(f\"[INFO] Parsing {len(model_test_files)} model test files once each...\")\n     model_overrides = build_model_overrides(model_test_files)\n \n     if test_method_name:\n@@ -192,7 +192,7 @@ def main() -> None:\n         data = summarize_all_tests(tests_with_origin, model_names, model_overrides)\n         json_path = output_dir / \"all_tests_scan_result.json\"\n     save_json(data, json_path)\n-    print(f\"\\nüìÑ JSON saved to {json_path.resolve()}\")\n+    print(f\"\\n[INFO] JSON saved to {json_path.resolve()}\")\n \n \n if __name__ == \"__main__\":"
        }
    ],
    "stats": {
        "total": 338,
        "additions": 206,
        "deletions": 132
    }
}