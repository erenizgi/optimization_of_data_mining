{
    "author": "zucchini-nlp",
    "message": "Compile compatibilty for decoder-only models (#32617)\n\n* squash into one commit\r\n\r\n* add qwen2-vl for rope standardization\r\n\r\n* fix mistral compile\r\n\r\n* fix qwen2-vl\r\n\r\n* fix-copies",
    "sha": "65bb28444849976f853063edb958b3ef3dd59d12",
    "files": [
        {
            "sha": "b966d72c643380f6de70cfb46b84ad162955aba5",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -1629,13 +1629,14 @@ def _tensor_or_none(token, device=None):\n \n         # Set pad token if unset (and there are conditions to do so)\n         if pad_token_tensor is None and eos_token_tensor is not None:\n-            if kwargs_has_attention_mask is not None and not kwargs_has_attention_mask:\n-                logger.warning(\n-                    \"The attention mask and the pad token id were not set. As a consequence, you may observe \"\n-                    \"unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\"\n-                )\n+            if not is_torchdynamo_compiling():\n+                if kwargs_has_attention_mask is not None and not kwargs_has_attention_mask:\n+                    logger.warning(\n+                        \"The attention mask and the pad token id were not set. As a consequence, you may observe \"\n+                        \"unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\"\n+                    )\n+                logger.warning(f\"Setting `pad_token_id` to `eos_token_id`:{pad_token_tensor} for open-end generation.\")\n             pad_token_tensor = eos_token_tensor[0]\n-            logger.warning(f\"Setting `pad_token_id` to `eos_token_id`:{pad_token_tensor} for open-end generation.\")\n \n         # Sanity checks/warnings\n         if self.config.is_encoder_decoder and decoder_start_token_tensor is None:"
        },
        {
            "sha": "d32ab95f51c7da419d7220569a54361e1c7f797d",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 25,
            "deletions": 10,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -326,24 +326,21 @@ def forward(\n \n         # reshape qkv for further computations\n         query_layer = query_layer.reshape(batch_size * self.num_heads, -1, self.head_dim)\n-        key_layer = key_layer.reshape(batch_size * self.num_heads, -1, self.head_dim).transpose(1, 2)\n+        key_layer = key_layer.reshape(batch_size * self.num_heads, -1, self.head_dim).transpose(-1, -2)\n         value_layer = value_layer.reshape(batch_size * self.num_heads, -1, self.head_dim)\n \n-        kv_length = cache_position[-1] + 1  # cache position is 0-indexed while length should start from 1\n-\n         # [batch_size * num_heads, q_length, kv_length]\n-        # we use `torch.Tensor.baddbmm` instead of `torch.baddbmm` as the latter isn't supported by TorchScript v1.11\n-        matmul_result = alibi.baddbmm(\n+        attention_scores = alibi.baddbmm(\n             batch1=query_layer,\n             batch2=key_layer,\n             beta=self.beta,\n             alpha=self.inv_norm_factor,\n         )\n \n         # change view to [batch_size, num_heads, q_length, kv_length]\n-        attn_weights = matmul_result.view(batch_size, self.num_heads, q_length, kv_length)\n+        attn_weights = attention_scores.view(batch_size, self.num_heads, q_length, -1)\n         if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, :kv_length]\n+            causal_mask = attention_mask[:, :, :, : key_layer.shape[-1]]\n             attn_weights = attn_weights + causal_mask\n \n         # cast attention scores to fp32, compute scaled softmax and cast back to initial dtype\n@@ -356,7 +353,7 @@ def forward(\n             attention_probs = attention_probs * head_mask\n \n         # change view [batch_size x num_heads, q_length, kv_length]\n-        attention_probs_reshaped = attention_probs.view(batch_size * self.num_heads, q_length, kv_length)\n+        attention_probs_reshaped = attention_probs.view(batch_size * self.num_heads, q_length, -1)\n \n         # matmul: [batch_size * num_heads, q_length, head_dim]\n         context_layer = torch.bmm(attention_probs_reshaped, value_layer)\n@@ -496,6 +493,8 @@ class BloomPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"BloomBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_cache_class = True\n+    _supports_static_cache = True\n+    _supports_quantized_cache = True\n \n     def __init__(self, *inputs, **kwargs):\n         super().__init__(*inputs, **kwargs)\n@@ -895,9 +894,25 @@ def prepare_inputs_for_generation(\n \n         # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n         if inputs_embeds is not None and cache_position[0] == 0:\n-            model_inputs = {\"inputs_embeds\": inputs_embeds}\n+            model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": None}\n         else:\n-            model_inputs = {\"input_ids\": input_ids.contiguous()}  # `contiguous()` needed for compilation use cases\n+            # This `clone` call is needed to avoid recapturing cuda graphs with `torch.compile`'s  `mode=\"reduce-overhead`, as otherwise the\n+            # input `position_ids` would have various stride during the decoding. Here, simply using `.contiguous()` is not sufficient as in\n+            # the batch size = 1 case, `position_ids` is already contiguous but with varying stride which retriggers a capture.\n+            model_inputs = {\"input_ids\": input_ids.clone(memory_format=torch.contiguous_format), \"inputs_embeds\": None}\n+\n+        # This part differs from other models because BLOOM needs a 2D mask to construct alibi tensor\n+        # The only difference is the usage of 2D instead of 4D mask, but the shape will be static\n+        if isinstance(past_key_values, StaticCache) and attention_mask is not None:\n+            target_length = past_key_values.get_max_length()\n+            batch_size, seq_length = attention_mask.shape\n+            diff = target_length - seq_length\n+\n+            new_attn_mask = torch.zeros(batch_size, diff, device=attention_mask.device, dtype=attention_mask.dtype)\n+            attention_mask = torch.cat(\n+                [attention_mask, new_attn_mask],\n+                dim=-1,\n+            )\n \n         model_inputs.update(\n             {"
        },
        {
            "sha": "9f5f8f793ce891d9dd578b10071ea451e3897d2d",
            "filename": "src/transformers/models/falcon/configuration_falcon.py",
            "status": "modified",
            "additions": 36,
            "deletions": 31,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Ffalcon%2Fconfiguration_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Ffalcon%2Fconfiguration_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fconfiguration_falcon.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -77,13 +77,42 @@ class FalconConfig(PretrainedConfig):\n         rope_theta (`float`, *optional*, defaults to 10000.0):\n             The base period of the RoPE embeddings.\n         rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n-            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n-            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n-            `max_position_embeddings` to the expected new maximum. See the following thread for more information on how\n-            these scaling strategies behave:\n-            https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/. This is an\n-            experimental feature, subject to breaking API changes in future versions.\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n         bos_token_id (`int`, *optional*, defaults to 11):\n             The id of the \"beginning-of-sequence\" token.\n         eos_token_id (`int`, *optional*, defaults to 11):\n@@ -167,7 +196,6 @@ def __init__(\n             self.ffn_hidden_size = hidden_size * 4\n         else:\n             self.ffn_hidden_size = ffn_hidden_size\n-        self._rope_scaling_validation()\n \n         super().__init__(bos_token_id=bos_token_id, eos_token_id=eos_token_id, **kwargs)\n \n@@ -178,26 +206,3 @@ def head_dim(self):\n     @property\n     def rotary(self):\n         return not self.alibi\n-\n-    def _rope_scaling_validation(self):\n-        \"\"\"\n-        Validate the `rope_scaling` configuration.\n-        \"\"\"\n-        if self.rope_scaling is None:\n-            return\n-\n-        if self.alibi:\n-            raise ValueError(\"`rope_scaling` is not supported when `alibi` is `True`.\")\n-\n-        if not isinstance(self.rope_scaling, dict) or len(self.rope_scaling) != 2:\n-            raise ValueError(\n-                \"`rope_scaling` must be a dictionary with two fields, `type` and `factor`, \" f\"got {self.rope_scaling}\"\n-            )\n-        rope_scaling_type = self.rope_scaling.get(\"type\", None)\n-        rope_scaling_factor = self.rope_scaling.get(\"factor\", None)\n-        if rope_scaling_type is None or rope_scaling_type not in [\"linear\", \"dynamic\"]:\n-            raise ValueError(\n-                f\"`rope_scaling`'s type field must be one of ['linear', 'dynamic'], got {rope_scaling_type}\"\n-            )\n-        if rope_scaling_factor is None or not isinstance(rope_scaling_factor, float) or rope_scaling_factor <= 1.0:\n-            raise ValueError(f\"`rope_scaling`'s factor field must be a float > 1, got {rope_scaling_factor}\")"
        },
        {
            "sha": "73e8806352ebbdf25a4312aef5d929162038603d",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 143,
            "deletions": 129,
            "changes": 272,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -35,6 +35,7 @@\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import is_torch_greater_or_equal_than_2_0\n from ...utils import (\n@@ -133,18 +134,17 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n-# Copied from transformers.models.mixtral.modeling_mixtral.apply_rotary_pos_emb\n-def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n+# Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n     Args:\n         q (`torch.Tensor`): The query tensor.\n         k (`torch.Tensor`): The key tensor.\n         cos (`torch.Tensor`): The cosine part of the rotary embedding.\n         sin (`torch.Tensor`): The sine part of the rotary embedding.\n-        position_ids (`torch.Tensor`):\n-            The position indices of the tokens corresponding to the query and key tensors. For example, this can be\n-            used to pass offsetted position ids when working with a KV-cache.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n         unsqueeze_dim (`int`, *optional*, defaults to 1):\n             The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n             sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n@@ -155,97 +155,126 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n     Returns:\n         `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n     \"\"\"\n-    cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n-    sin = sin[position_ids].unsqueeze(unsqueeze_dim)\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n     q_embed = (q * cos) + (rotate_half(q) * sin)\n     k_embed = (k * cos) + (rotate_half(k) * sin)\n     return q_embed, k_embed\n \n \n-# Copied from transformers.models.mixtral.modeling_mixtral.MixtralRotaryEmbedding with Mixtral->Falcon\n+# Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Falcon\n class FalconRotaryEmbedding(nn.Module):\n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n+    def __init__(\n+        self,\n+        dim=None,\n+        max_position_embeddings=2048,\n+        base=10000,\n+        device=None,\n+        scaling_factor=1.0,\n+        rope_type=\"default\",\n+        config: Optional[FalconConfig] = None,\n+    ):\n         super().__init__()\n+        # TODO (joao): remove the `if` below, only used for BC\n+        self.rope_kwargs = {}\n+        if config is None:\n+            logger.warning_once(\n+                \"`FalconRotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n+                \"`config` argument. All other arguments will be removed in v4.46\"\n+            )\n+            self.rope_kwargs = {\n+                \"rope_type\": rope_type,\n+                \"factor\": scaling_factor,\n+                \"dim\": dim,\n+                \"base\": base,\n+                \"max_position_embeddings\": max_position_embeddings,\n+            }\n+            self.rope_type = rope_type\n+            self.max_seq_len_cached = max_position_embeddings\n+            self.original_max_seq_len = max_position_embeddings\n+        else:\n+            # BC: \"rope_type\" was originally \"type\"\n+            if config.rope_scaling is not None:\n+                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+            else:\n+                self.rope_type = \"default\"\n+            self.max_seq_len_cached = config.max_position_embeddings\n+            self.original_max_seq_len = config.max_position_embeddings\n \n-        self.dim = dim\n-        self.max_position_embeddings = max_position_embeddings\n-        self.base = base\n-        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-\n-        # Build here to make `torch.jit.trace` work.\n-        self._set_cos_sin_cache(\n-            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n-        )\n-\n-    def _set_cos_sin_cache(self, seq_len, device, dtype):\n-        self.max_seq_len_cached = seq_len\n-        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n-\n-        freqs = torch.outer(t, self.inv_freq)\n-        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n-        emb = torch.cat((freqs, freqs), dim=-1)\n-        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n-        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n-\n-    def forward(self, x, seq_len=None):\n-        # x: [bs, num_attention_heads, seq_len, head_size]\n-        if seq_len > self.max_seq_len_cached:\n-            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n-\n-        return (\n-            self.cos_cached[:seq_len].to(dtype=x.dtype),\n-            self.sin_cached[:seq_len].to(dtype=x.dtype),\n-        )\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n \n-# copied from transformers.models.llama.modeling_llama.LlamaLinearScalingRotaryEmbedding with Llama->Falcon\n-# TODO @joao no longer copied from LLama after static cache, fix me (copied -> Copied)\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(\n+                self.config, device, seq_len=seq_len, **self.rope_kwargs\n+            )\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n+\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n+\n+    @torch.no_grad()\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n+\n+        # Core RoPE block\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n+        device_type = x.device.type\n+        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos()\n+            sin = emb.sin()\n+\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+# Copied from transformers.models.llama.modeling_llama.LlamaLinearScalingRotaryEmbedding with Llama->Falcon\n class FalconLinearScalingRotaryEmbedding(FalconRotaryEmbedding):\n     \"\"\"FalconRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n \n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n-        self.scaling_factor = scaling_factor\n-        super().__init__(dim, max_position_embeddings, base, device)\n-\n-    def _set_cos_sin_cache(self, seq_len, device, dtype):\n-        self.max_seq_len_cached = seq_len\n-        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n-        t = t / self.scaling_factor\n-\n-        freqs = torch.outer(t, self.inv_freq)\n-        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n-        emb = torch.cat((freqs, freqs), dim=-1)\n-        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n-        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n+    def __init__(self, *args, **kwargs):\n+        logger.warning_once(\n+            \"`FalconLinearScalingRotaryEmbedding` is deprecated an will be removed in v4.46. Please use \"\n+            \"`FalconRotaryEmbedding`, which now also does linear scaling (simply pass the model config to __init__).\"\n+        )\n+        kwargs[\"rope_type\"] = \"linear\"\n+        super().__init__(*args, **kwargs)\n \n \n-# copied from transformers.models.llama.modeling_llama.LlamaDynamicNTKScalingRotaryEmbedding with Llama->Falcon\n-# TODO @joao no longer copied from LLama after static cache, fix me (copied -> Copied)\n+# Copied from transformers.models.llama.modeling_llama.LlamaDynamicNTKScalingRotaryEmbedding with Llama->Falcon\n class FalconDynamicNTKScalingRotaryEmbedding(FalconRotaryEmbedding):\n     \"\"\"FalconRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n \n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n-        self.scaling_factor = scaling_factor\n-        super().__init__(dim, max_position_embeddings, base, device)\n-\n-    def _set_cos_sin_cache(self, seq_len, device, dtype):\n-        self.max_seq_len_cached = seq_len\n-\n-        if seq_len > self.max_position_embeddings:\n-            base = self.base * (\n-                (self.scaling_factor * seq_len / self.max_position_embeddings) - (self.scaling_factor - 1)\n-            ) ** (self.dim / (self.dim - 2))\n-            inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-\n-        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n-\n-        freqs = torch.outer(t, self.inv_freq)\n-        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n-        emb = torch.cat((freqs, freqs), dim=-1)\n-        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n-        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n+    def __init__(self, *args, **kwargs):\n+        logger.warning_once(\n+            \"`FalconDynamicNTKScalingRotaryEmbedding` is deprecated an will be removed in v4.46. Please use \"\n+            \"`FalconRotaryEmbedding`, which now also does dynamic ntk scaling (simply pass the model config to \"\n+            \"__init__).\"\n+        )\n+        kwargs[\"rope_type\"] = \"dynamic\"\n+        super().__init__(*args, **kwargs)\n \n \n def build_alibi_tensor(attention_mask: torch.Tensor, num_heads: int, dtype: torch.dtype) -> torch.Tensor:\n@@ -324,9 +353,6 @@ def __init__(self, config: FalconConfig, layer_idx=None):\n                 f\" {self.num_heads}).\"\n             )\n \n-        if config.rotary:\n-            self._init_rope()\n-\n         # Layer-wise attention scaling\n         self.inv_norm_factor = 1.0 / math.sqrt(self.head_dim)\n         self.beta = self.inv_norm_factor\n@@ -343,32 +369,9 @@ def __init__(self, config: FalconConfig, layer_idx=None):\n         self.attention_dropout = nn.Dropout(config.attention_dropout)\n         self.num_kv_heads = config.num_kv_heads if (self.new_decoder_architecture or not self.multi_query) else 1\n \n-    def _init_rope(self):\n-        if self.config.rope_scaling is None:\n-            self.rotary_emb = FalconRotaryEmbedding(\n-                self.head_dim,\n-                max_position_embeddings=self.max_position_embeddings,\n-                base=self.rope_theta,\n-            )\n-        else:\n-            scaling_type = self.config.rope_scaling[\"type\"]\n-            scaling_factor = self.config.rope_scaling[\"factor\"]\n-            if scaling_type == \"linear\":\n-                self.rotary_emb = FalconLinearScalingRotaryEmbedding(\n-                    self.head_dim,\n-                    max_position_embeddings=self.max_position_embeddings,\n-                    scaling_factor=scaling_factor,\n-                    base=self.rope_theta,\n-                )\n-            elif scaling_type == \"dynamic\":\n-                self.rotary_emb = FalconDynamicNTKScalingRotaryEmbedding(\n-                    self.head_dim,\n-                    max_position_embeddings=self.max_position_embeddings,\n-                    scaling_factor=scaling_factor,\n-                    base=self.rope_theta,\n-                )\n-            else:\n-                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n+        # TODO (raushan): remove in v4.46 (RoPE is computed in the model, not in the decoder layers)\n+        if config.rotary:\n+            self.rotary_emb = FalconRotaryEmbedding(config=self.config)\n \n     def _split_heads(self, fused_qkv: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n         \"\"\"\n@@ -438,6 +441,7 @@ def forward(\n         use_cache: bool = False,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n     ):\n         fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]\n         num_kv_heads = self.num_heads if self.new_decoder_architecture else self.num_kv_heads\n@@ -450,18 +454,18 @@ def forward(\n         key_layer = key_layer.transpose(1, 2).reshape(batch_size, num_kv_heads, query_length, self.head_dim)\n         value_layer = value_layer.transpose(1, 2).reshape(batch_size, num_kv_heads, query_length, self.head_dim)\n \n-        kv_seq_len = key_layer.shape[-2]\n-        if layer_past is not None:\n-            if self.layer_idx is None:\n-                raise ValueError(\n-                    f\"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} \"\n-                    \"for auto-regressive decoding with k/v caching, please make sure to initialize the attention class \"\n-                    \"with a layer index.\"\n-                )\n-            kv_seq_len += layer_past.get_seq_length(self.layer_idx)\n         if alibi is None:\n-            cos, sin = self.rotary_emb(value_layer, seq_len=kv_seq_len)\n-            query_layer, key_layer = apply_rotary_pos_emb(query_layer, key_layer, cos, sin, position_ids)\n+            if position_embeddings is None:\n+                logger.warning_once(\n+                    \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                    \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                    \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n+                    \"removed and `position_embeddings` will be mandatory.\"\n+                )\n+                cos, sin = self.rotary_emb(value_layer, position_ids)\n+            else:\n+                cos, sin = position_embeddings\n+            query_layer, key_layer = apply_rotary_pos_emb(query_layer, key_layer, cos, sin)\n \n         if layer_past is not None:\n             cache_kwargs = {\"cache_position\": cache_position}\n@@ -597,6 +601,7 @@ def forward(\n         use_cache: bool = False,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n     ):\n         fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]\n         num_kv_heads = self.num_heads if self.new_decoder_architecture else self.num_kv_heads\n@@ -609,18 +614,18 @@ def forward(\n         key_layer = key_layer.transpose(1, 2).reshape(batch_size, num_kv_heads, query_length, self.head_dim)\n         value_layer = value_layer.transpose(1, 2).reshape(batch_size, num_kv_heads, query_length, self.head_dim)\n \n-        kv_seq_len = key_layer.shape[-2]\n-        if layer_past is not None:\n-            if self.layer_idx is None:\n-                raise ValueError(\n-                    f\"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} \"\n-                    \"for auto-regressive decoding with k/v caching, please make sure to initialize the attention class \"\n-                    \"with a layer index.\"\n-                )\n-            kv_seq_len += layer_past.get_seq_length(self.layer_idx)\n         if alibi is None:\n-            cos, sin = self.rotary_emb(value_layer, seq_len=kv_seq_len)\n-            query_layer, key_layer = apply_rotary_pos_emb(query_layer, key_layer, cos, sin, position_ids)\n+            if position_embeddings is None:\n+                logger.warning_once(\n+                    \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                    \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                    \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n+                    \"removed and `position_embeddings` will be mandatory.\"\n+                )\n+                cos, sin = self.rotary_emb(value_layer, position_ids)\n+            else:\n+                cos, sin = position_embeddings\n+            query_layer, key_layer = apply_rotary_pos_emb(query_layer, key_layer, cos, sin)\n \n         if layer_past is not None:\n             cache_kwargs = {\"cache_position\": cache_position}\n@@ -743,6 +748,7 @@ def forward(\n         use_cache: bool = False,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n         **kwargs,\n     ):\n         residual = hidden_states\n@@ -764,6 +770,7 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n         )\n \n         attention_output = attn_outputs[0]\n@@ -969,6 +976,8 @@ def __init__(self, config: FalconConfig):\n         # Final Layer Norm\n         self.ln_f = LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)\n \n+        self.rotary_emb = FalconRotaryEmbedding(config=config)\n+\n         self.gradient_checkpointing = False\n \n         # Initialize weights and apply final processing\n@@ -1065,6 +1074,9 @@ def forward(\n         head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n         hidden_states = inputs_embeds\n \n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n         next_decoder_cache = None\n         all_self_attentions = () if output_attentions else None\n         all_hidden_states = () if output_hidden_states else None\n@@ -1085,6 +1097,7 @@ def forward(\n                     use_cache,\n                     output_attentions,\n                     cache_position,\n+                    position_embeddings,\n                 )\n             else:\n                 outputs = block(\n@@ -1097,6 +1110,7 @@ def forward(\n                     output_attentions=output_attentions,\n                     alibi=alibi,\n                     cache_position=cache_position,\n+                    position_embeddings=position_embeddings,\n                 )\n \n             hidden_states = outputs[0]"
        },
        {
            "sha": "07514a37c6f2facce578428442d04296d7c0f200",
            "filename": "src/transformers/models/gpt_neox/configuration_gpt_neox.py",
            "status": "modified",
            "additions": 44,
            "deletions": 28,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fconfiguration_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fconfiguration_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fconfiguration_gpt_neox.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -15,6 +15,7 @@\n \"\"\"GPTNeoX model configuration\"\"\"\n \n from ...configuration_utils import PretrainedConfig\n+from ...modeling_rope_utils import rope_config_validation\n from ...utils import logging\n \n \n@@ -74,13 +75,42 @@ class GPTNeoXConfig(PretrainedConfig):\n             Whether to use a \"parallel\" formulation in each Transformer layer, which can provide a slight training\n             speedup at large scales (e.g. 20B).\n         rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n-            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n-            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n-            `max_position_embeddings` to the expected new maximum. See the following thread for more information on how\n-            these scaling strategies behave:\n-            https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/. This is an\n-            experimental feature, subject to breaking API changes in future versions.\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n         attention_bias (`bool`, *optional*, defaults to `True`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n \n@@ -136,7 +166,9 @@ def __init__(\n         self.intermediate_size = intermediate_size\n         self.hidden_act = hidden_act\n         self.rotary_pct = rotary_pct\n+        self.partial_rotary_factor = rotary_pct\n         self.rotary_emb_base = rotary_emb_base\n+        self.rope_theta = rotary_emb_base\n         self.attention_dropout = attention_dropout\n         self.hidden_dropout = hidden_dropout\n         self.classifier_dropout = classifier_dropout\n@@ -147,29 +179,13 @@ def __init__(\n         self.use_parallel_residual = use_parallel_residual\n         self.rope_scaling = rope_scaling\n         self.attention_bias = attention_bias\n-        self._rope_scaling_validation()\n+        # Validate the correctness of rotary position embeddings parameters\n+        # BC: if there is a 'type' field, move it to 'rope_type'.\n+        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n+            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_config_validation(self)\n \n         if self.hidden_size % self.num_attention_heads != 0:\n             raise ValueError(\n                 \"The hidden size is not divisble by the number of attention heads! Make sure to update them!\"\n             )\n-\n-    def _rope_scaling_validation(self):\n-        \"\"\"\n-        Validate the `rope_scaling` configuration.\n-        \"\"\"\n-        if self.rope_scaling is None:\n-            return\n-\n-        if not isinstance(self.rope_scaling, dict) or len(self.rope_scaling) != 2:\n-            raise ValueError(\n-                \"`rope_scaling` must be a dictionary with two fields, `type` and `factor`, \" f\"got {self.rope_scaling}\"\n-            )\n-        rope_scaling_type = self.rope_scaling.get(\"type\", None)\n-        rope_scaling_factor = self.rope_scaling.get(\"factor\", None)\n-        if rope_scaling_type is None or rope_scaling_type not in [\"linear\", \"dynamic\"]:\n-            raise ValueError(\n-                f\"`rope_scaling`'s type field must be one of ['linear', 'dynamic'], got {rope_scaling_type}\"\n-            )\n-        if rope_scaling_factor is None or not isinstance(rope_scaling_factor, float) or rope_scaling_factor <= 1.0:\n-            raise ValueError(f\"`rope_scaling`'s factor field must be a float > 1, got {rope_scaling_factor}\")"
        },
        {
            "sha": "e88302efa7bb04fe89016fb0ca8f0d35c80bf614",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 147,
            "deletions": 121,
            "changes": 268,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -38,8 +38,14 @@\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n-from ...utils import get_torch_version, is_flash_attn_2_available, is_flash_attn_greater_or_equal_2_10, logging\n+from ...utils import (\n+    get_torch_version,\n+    is_flash_attn_2_available,\n+    is_flash_attn_greater_or_equal_2_10,\n+    logging,\n+)\n from .configuration_gpt_neox import GPTNeoXConfig\n \n \n@@ -151,10 +157,11 @@ def __init__(self, config, layer_idx=None):\n             )\n         self.head_size = self.hidden_size // self.num_attention_heads\n         self.rotary_ndims = int(self.head_size * config.rotary_pct)\n+        self.rope_theta = config.rotary_emb_base\n         self._init_bias(config.max_position_embeddings)\n \n         self.register_buffer(\"masked_bias\", torch.tensor(-1e9), persistent=False)\n-        self._init_rope()\n+        self.rotary_emb = GPTNeoXRotaryEmbedding(config=self.config)\n \n         if layer_idx is None:\n             logger.warning_once(\n@@ -180,31 +187,6 @@ def _init_bias(self, max_positions, device=None):\n         if device is not None:\n             self.bias = self.bias.to(device)\n \n-    def _init_rope(self):\n-        if self.config.rope_scaling is None:\n-            self.rotary_emb = GPTNeoXRotaryEmbedding(\n-                self.rotary_ndims, self.config.max_position_embeddings, base=self.config.rotary_emb_base\n-            )\n-        else:\n-            scaling_type = self.config.rope_scaling[\"type\"]\n-            scaling_factor = self.config.rope_scaling[\"factor\"]\n-            if scaling_type == \"linear\":\n-                self.rotary_emb = GPTNeoXLinearScalingRotaryEmbedding(\n-                    self.rotary_ndims,\n-                    self.config.max_position_embeddings,\n-                    base=self.config.rotary_emb_base,\n-                    scaling_factor=scaling_factor,\n-                )\n-            elif scaling_type == \"dynamic\":\n-                self.rotary_emb = GPTNeoXDynamicNTKScalingRotaryEmbedding(\n-                    self.rotary_ndims,\n-                    self.config.max_position_embeddings,\n-                    base=self.config.rotary_emb_base,\n-                    scaling_factor=scaling_factor,\n-                )\n-            else:\n-                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n-\n     def forward(\n         self,\n         hidden_states: torch.FloatTensor,\n@@ -216,10 +198,15 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         padding_mask: Optional[torch.Tensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n     ):\n         # Apply attention-specific projections and rope\n         query, key, value, present = self._attn_projections_and_rope(\n-            hidden_states=hidden_states, position_ids=position_ids, layer_past=layer_past, use_cache=use_cache\n+            hidden_states=hidden_states,\n+            position_ids=position_ids,\n+            layer_past=layer_past,\n+            use_cache=use_cache,\n+            position_embeddings=position_embeddings,\n         )\n \n         # Compute attention\n@@ -267,6 +254,7 @@ def _attn_projections_and_rope(\n         layer_past: Optional[Tuple[torch.Tensor]] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n     ):\n         # Compute QKV\n         # Attention heads [batch, seq_len, hidden_size]\n@@ -289,19 +277,17 @@ def _attn_projections_and_rope(\n         key_rot = key[..., : self.rotary_ndims]\n         key_pass = key[..., self.rotary_ndims :]\n \n-        # Compute token offset for rotary embeddings (when decoding)\n-        seq_len = key.shape[-2]\n-        if layer_past is not None:\n-            if self.layer_idx is None:\n-                raise ValueError(\n-                    f\"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} \"\n-                    \"for auto-regressive decoding with k/v caching, please make sure to initialize the attention class \"\n-                    \"with a layer index.\"\n-                )\n-            seq_len += layer_past.get_seq_length(self.layer_idx)\n-\n-        cos, sin = self.rotary_emb(value, seq_len=seq_len)\n-        query, key = apply_rotary_pos_emb(query_rot, key_rot, cos, sin, position_ids)\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n+        query, key = apply_rotary_pos_emb(query_rot, key_rot, cos, sin)\n         query = torch.cat((query, query_pass), dim=-1)\n         key = torch.cat((key, key_pass), dim=-1)\n \n@@ -310,7 +296,7 @@ def _attn_projections_and_rope(\n             cache_kwargs = {\n                 \"sin\": sin,\n                 \"cos\": cos,\n-                \"partial_rotation_size\": self.rotary_emb.dim,\n+                \"partial_rotation_size\": self.rotary_ndims,\n                 \"cache_position\": cache_position,\n             }\n             key, value = layer_past.update(key, value, self.layer_idx, cache_kwargs)\n@@ -395,6 +381,7 @@ def forward(\n         use_cache: Optional[bool] = False,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n     ):\n         # Apply attention-specific projections and rope\n         query, key, value, present = self._attn_projections_and_rope(\n@@ -403,6 +390,7 @@ def forward(\n             layer_past=layer_past,\n             use_cache=use_cache,\n             cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n         )\n \n         query_length = query.shape[-2]\n@@ -496,6 +484,7 @@ def forward(\n         use_cache: Optional[bool] = False,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n     ):\n         if output_attentions or head_mask is not None:\n             logger.warning_once(\n@@ -524,6 +513,7 @@ def forward(\n             layer_past=layer_past,\n             use_cache=use_cache,\n             cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n         )\n \n         causal_mask = attention_mask\n@@ -570,90 +560,119 @@ def attention_mask_func(attention_scores, ltor_mask):\n     return attention_scores\n \n \n+# Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->GPTNeoX\n class GPTNeoXRotaryEmbedding(nn.Module):\n-    # Copied from transformers.models.mixtral.modeling_mixtral.MixtralRotaryEmbedding.__init__\n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n+    def __init__(\n+        self,\n+        dim=None,\n+        max_position_embeddings=2048,\n+        base=10000,\n+        device=None,\n+        scaling_factor=1.0,\n+        rope_type=\"default\",\n+        config: Optional[GPTNeoXConfig] = None,\n+    ):\n         super().__init__()\n+        # TODO (joao): remove the `if` below, only used for BC\n+        self.rope_kwargs = {}\n+        if config is None:\n+            logger.warning_once(\n+                \"`GPTNeoXRotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n+                \"`config` argument. All other arguments will be removed in v4.46\"\n+            )\n+            self.rope_kwargs = {\n+                \"rope_type\": rope_type,\n+                \"factor\": scaling_factor,\n+                \"dim\": dim,\n+                \"base\": base,\n+                \"max_position_embeddings\": max_position_embeddings,\n+            }\n+            self.rope_type = rope_type\n+            self.max_seq_len_cached = max_position_embeddings\n+            self.original_max_seq_len = max_position_embeddings\n+        else:\n+            # BC: \"rope_type\" was originally \"type\"\n+            if config.rope_scaling is not None:\n+                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+            else:\n+                self.rope_type = \"default\"\n+            self.max_seq_len_cached = config.max_position_embeddings\n+            self.original_max_seq_len = config.max_position_embeddings\n \n-        self.dim = dim\n-        self.max_position_embeddings = max_position_embeddings\n-        self.base = base\n-        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-\n-        # Build here to make `torch.jit.trace` work.\n-        self._set_cos_sin_cache(\n-            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n-        )\n-\n-    def _set_cos_sin_cache(self, seq_len, device, dtype):\n-        self.max_seq_len_cached = seq_len\n-        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n-\n-        freqs = torch.outer(t, self.inv_freq)\n-        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n-        emb = torch.cat((freqs, freqs), dim=-1)\n-        self.register_buffer(\"cos_cached\", emb.cos(), persistent=False)\n-        self.register_buffer(\"sin_cached\", emb.sin(), persistent=False)\n-\n-    def forward(self, x, seq_len=None):\n-        # x: [bs, num_attention_heads, seq_len, head_size]\n-        if seq_len > self.max_seq_len_cached:\n-            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n-\n-        return (\n-            self.cos_cached[:seq_len],\n-            self.sin_cached[:seq_len],\n-        )\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n \n-# copied from transformers.models.llama.modeling_llama.LlamaLinearScalingRotaryEmbedding.__init__\n-# TODO @gante bring compatibility back\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(\n+                self.config, device, seq_len=seq_len, **self.rope_kwargs\n+            )\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n+\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n+\n+    @torch.no_grad()\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n+\n+        # Core RoPE block\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n+        device_type = x.device.type\n+        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos()\n+            sin = emb.sin()\n+\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+# Copied from transformers.models.llama.modeling_llama.LlamaLinearScalingRotaryEmbedding with Llama->GPTNeoX\n class GPTNeoXLinearScalingRotaryEmbedding(GPTNeoXRotaryEmbedding):\n     \"\"\"GPTNeoXRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n \n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n-        self.scaling_factor = scaling_factor\n-        super().__init__(dim, max_position_embeddings, base, device)\n-\n-    def _set_cos_sin_cache(self, seq_len, device, dtype):\n-        self.max_seq_len_cached = seq_len\n-        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n-        t = t / self.scaling_factor\n-\n-        freqs = torch.outer(t, self.inv_freq)\n-        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n-        emb = torch.cat((freqs, freqs), dim=-1)\n-        self.register_buffer(\"cos_cached\", emb.cos(), persistent=False)\n-        self.register_buffer(\"sin_cached\", emb.sin(), persistent=False)\n+    def __init__(self, *args, **kwargs):\n+        logger.warning_once(\n+            \"`GPTNeoXLinearScalingRotaryEmbedding` is deprecated an will be removed in v4.46. Please use \"\n+            \"`GPTNeoXRotaryEmbedding`, which now also does linear scaling (simply pass the model config to __init__).\"\n+        )\n+        kwargs[\"rope_type\"] = \"linear\"\n+        super().__init__(*args, **kwargs)\n \n \n+# Copied from transformers.models.llama.modeling_llama.LlamaDynamicNTKScalingRotaryEmbedding with Llama->GPTNeoX\n class GPTNeoXDynamicNTKScalingRotaryEmbedding(GPTNeoXRotaryEmbedding):\n     \"\"\"GPTNeoXRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n \n-    # copied from transformers.models.llama.modeling_llama.LlamaDynamicNTKScalingRotaryEmbedding.__init__\n-    # TODO @gante no longer copied from\n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n-        self.scaling_factor = scaling_factor\n-        super().__init__(dim, max_position_embeddings, base, device)\n-\n-    def _set_cos_sin_cache(self, seq_len, device, dtype):\n-        self.max_seq_len_cached = seq_len\n-\n-        if seq_len > self.max_position_embeddings:\n-            base = self.base * (\n-                (self.scaling_factor * seq_len / self.max_position_embeddings) - (self.scaling_factor - 1)\n-            ) ** (self.dim / (self.dim - 2))\n-            inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-\n-        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n-\n-        freqs = torch.outer(t, self.inv_freq)\n-        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n-        emb = torch.cat((freqs, freqs), dim=-1)\n-        self.register_buffer(\"cos_cached\", emb.cos(), persistent=False)\n-        self.register_buffer(\"sin_cached\", emb.sin(), persistent=False)\n+    def __init__(self, *args, **kwargs):\n+        logger.warning_once(\n+            \"`GPTNeoXDynamicNTKScalingRotaryEmbedding` is deprecated an will be removed in v4.46. Please use \"\n+            \"`GPTNeoXRotaryEmbedding`, which now also does dynamic ntk scaling (simply pass the model config to \"\n+            \"__init__).\"\n+        )\n+        kwargs[\"rope_type\"] = \"dynamic\"\n+        super().__init__(*args, **kwargs)\n \n \n def rotate_half(x):\n@@ -663,18 +682,17 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n-# Copied from transformers.models.mixtral.modeling_mixtral.apply_rotary_pos_emb\n-def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n+# Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n     Args:\n         q (`torch.Tensor`): The query tensor.\n         k (`torch.Tensor`): The key tensor.\n         cos (`torch.Tensor`): The cosine part of the rotary embedding.\n         sin (`torch.Tensor`): The sine part of the rotary embedding.\n-        position_ids (`torch.Tensor`):\n-            The position indices of the tokens corresponding to the query and key tensors. For example, this can be\n-            used to pass offsetted position ids when working with a KV-cache.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n         unsqueeze_dim (`int`, *optional*, defaults to 1):\n             The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n             sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n@@ -685,8 +703,8 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n     Returns:\n         `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n     \"\"\"\n-    cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n-    sin = sin[position_ids].unsqueeze(unsqueeze_dim)\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n     q_embed = (q * cos) + (rotate_half(q) * sin)\n     k_embed = (k * cos) + (rotate_half(k) * sin)\n     return q_embed, k_embed\n@@ -734,6 +752,7 @@ def forward(\n         layer_past: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n     ):\n         attention_layer_outputs = self.attention(\n             self.input_layernorm(hidden_states),\n@@ -744,6 +763,7 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n         )\n         attn_output = attention_layer_outputs[0]  # output_attn: attn_output, present, (attn_weights)\n         attn_output = self.post_attention_dropout(attn_output)\n@@ -860,6 +880,7 @@ def __init__(self, config):\n         self.emb_dropout = nn.Dropout(config.hidden_dropout)\n         self.layers = nn.ModuleList([GPTNeoXLayer(config, i) for i in range(config.num_hidden_layers)])\n         self.final_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.rotary_emb = GPTNeoXRotaryEmbedding(config=config)\n \n         self._attn_implementation = config._attn_implementation\n \n@@ -952,6 +973,9 @@ def forward(\n         head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n         hidden_states = self.emb_dropout(inputs_embeds)\n \n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n         next_decoder_cache = None\n         all_attentions = () if output_attentions else None\n         all_hidden_states = () if output_hidden_states else None\n@@ -972,6 +996,7 @@ def forward(\n                     None,\n                     output_attentions,\n                     cache_position,\n+                    position_embeddings,\n                 )\n             else:\n                 outputs = layer(\n@@ -983,6 +1008,7 @@ def forward(\n                     use_cache=use_cache,\n                     output_attentions=output_attentions,\n                     cache_position=cache_position,\n+                    position_embeddings=position_embeddings,\n                 )\n             hidden_states = outputs[0]\n             if use_cache is True:\n@@ -1183,7 +1209,7 @@ def forward(\n             attentions=outputs.attentions,\n         )\n \n-    # can't be copied from llama, gpt-neox has emebd_out and not lm_head\n+    # can't be copied from llama, gpt-neox has embed_out and not lm_head\n     def prepare_inputs_for_generation(\n         self,\n         input_ids,"
        },
        {
            "sha": "e305bd28f2fbf469c67e013799fa7dba3934d103",
            "filename": "src/transformers/models/gpt_neox_japanese/configuration_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 47,
            "deletions": 0,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fconfiguration_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fconfiguration_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fconfiguration_gpt_neox_japanese.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -15,6 +15,7 @@\n \"\"\"GPTNeoX Japanese model configuration\"\"\"\n \n from ...configuration_utils import PretrainedConfig\n+from ...modeling_rope_utils import rope_config_validation\n from ...utils import logging\n \n \n@@ -59,6 +60,43 @@ class GPTNeoXJapaneseConfig(PretrainedConfig):\n         use_cache (`bool`, *optional*, defaults to `True`):\n             Whether or not the model should return the last key/values attentions (not used by all models). Only\n             relevant if `config.is_decoder=True`.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n         attention_dropout (`float`, *optional*, defaults to 0.1):\n             The dropout ratio for the attention.\n         hidden_dropout (`float`, *optional*, defaults to 0.0):\n@@ -96,6 +134,7 @@ def __init__(\n         use_cache=True,\n         bos_token_id=31996,\n         eos_token_id=31999,\n+        rope_scaling=None,\n         attention_dropout=0.1,\n         hidden_dropout=0.0,\n         **kwargs,\n@@ -109,9 +148,17 @@ def __init__(\n         self.intermediate_multiple_size = intermediate_multiple_size\n         self.hidden_act = hidden_act\n         self.rotary_pct = rotary_pct\n+        self.partial_rotary_factor = rotary_pct\n         self.rotary_emb_base = rotary_emb_base\n+        self.rope_theta = rotary_emb_base\n         self.initializer_range = initializer_range\n         self.layer_norm_eps = layer_norm_eps\n         self.use_cache = use_cache\n+        self.rope_scaling = rope_scaling\n         self.attention_dropout = attention_dropout\n         self.hidden_dropout = hidden_dropout\n+        # Validate the correctness of rotary position embeddings parameters\n+        # BC: if there is a 'type' field, move it to 'rope_type'.\n+        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n+            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_config_validation(self)"
        },
        {
            "sha": "bf832195b4efc334c1499d1657e07ac833d2350b",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 440,
            "deletions": 171,
            "changes": 611,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -14,6 +14,7 @@\n # limitations under the License.\n \"\"\"PyTorch GPTNeoX model.\"\"\"\n \n+import math\n from typing import Optional, Tuple, Union\n \n import torch\n@@ -22,8 +23,11 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...file_utils import add_start_docstrings, add_start_docstrings_to_model_forward, replace_return_docstrings\n+from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n from ...utils import logging\n from .configuration_gpt_neox_japanese import GPTNeoXJapaneseConfig\n@@ -35,6 +39,60 @@\n _CONFIG_FOR_DOC = \"GPTNeoXJapaneseConfig\"\n \n \n+# Copied from transformers.models.llama.modeling_llama._prepare_4d_causal_attention_mask_with_cache_position\n+def _prepare_4d_causal_attention_mask_with_cache_position(\n+    attention_mask: torch.Tensor,\n+    sequence_length: int,\n+    target_length: int,\n+    dtype: torch.dtype,\n+    device: torch.device,\n+    min_dtype: float,\n+    cache_position: torch.Tensor,\n+    batch_size: int,\n+):\n+    \"\"\"\n+    Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+    `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+    Args:\n+        attention_mask (`torch.Tensor`):\n+            A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n+        sequence_length (`int`):\n+            The sequence length being processed.\n+        target_length (`int`):\n+            The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n+        dtype (`torch.dtype`):\n+            The dtype to use for the 4D attention mask.\n+        device (`torch.device`):\n+            The device to plcae the 4D attention mask on.\n+        min_dtype (`float`):\n+            The minimum value representable with the dtype `dtype`.\n+        cache_position (`torch.Tensor`):\n+            Indices depicting the position of the input sequence tokens in the sequence.\n+        batch_size (`torch.Tensor`):\n+            Batch size.\n+    \"\"\"\n+    if attention_mask is not None and attention_mask.dim() == 4:\n+        # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+        causal_mask = attention_mask\n+    else:\n+        causal_mask = torch.full((sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device)\n+        if sequence_length != 1:\n+            causal_mask = torch.triu(causal_mask, diagonal=1)\n+        causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+        causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+        if attention_mask is not None:\n+            causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+            mask_length = attention_mask.shape[-1]\n+            padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+            padding_mask = padding_mask == 0\n+            causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                padding_mask, min_dtype\n+            )\n+\n+    return causal_mask\n+\n+\n class GPTNeoXJapanesePreTrainedModel(PreTrainedModel):\n     \"\"\"\n     An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n@@ -45,6 +103,9 @@ class GPTNeoXJapanesePreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"gpt_neox_japanese\"\n     _no_split_modules = [\"GPTNeoXJapaneseLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n+    _supports_cache_class = True\n+    _supports_quantized_cache = True\n+    _supports_static_cache = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n@@ -62,19 +123,24 @@ def _init_weights(self, module):\n \n \n class GPTNeoXJapaneseAttention(nn.Module):\n-    def __init__(self, config, use_bias=False):\n+    def __init__(self, config, use_bias=False, layer_idx=None):\n         super().__init__()\n         self.num_attention_heads = config.num_attention_heads\n         self.hidden_size = config.hidden_size\n         self.head_size = self.hidden_size // self.num_attention_heads\n+        if layer_idx is None:\n+            logger.warning_once(\n+                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n+                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n \n+        self.layer_idx = layer_idx\n         self.rotary_ndims = int(self.head_size * config.rotary_pct)\n-        self.rotary_emb = RotaryEmbedding(\n-            self.rotary_ndims, config.max_position_embeddings, base=config.rotary_emb_base\n-        )\n-        self.max_positions = config.max_position_embeddings\n+        self.rope_theta = config.rotary_emb_base\n+        self.rotary_emb = GPTNeoXJapaneseRotaryEmbedding(config=config)\n         self.attention_dropout = nn.Dropout(config.attention_dropout)\n-        self.norm_factor = torch.sqrt(torch.tensor(self.head_size, dtype=torch.float32)).to(torch.get_default_dtype())\n+        self.norm_factor = math.sqrt(self.head_size)\n \n         self.query_key_value = nn.Linear(config.hidden_size, 3 * config.hidden_size, bias=False)\n         self.dense = nn.Linear(config.hidden_size, config.hidden_size, bias=False)\n@@ -84,15 +150,16 @@ def __init__(self, config, use_bias=False):\n \n     def forward(\n         self,\n-        hidden_states,\n-        attention_mask,\n-        head_mask=None,\n-        layer_past=None,\n-        use_cache=False,\n-        output_attentions=False,\n+        hidden_states: torch.FloatTensor,\n+        attention_mask: torch.FloatTensor,\n+        position_ids: torch.LongTensor,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        layer_past: Optional[Cache] = None,\n+        use_cache: Optional[bool] = False,\n+        output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n     ):\n-        has_layer_past = layer_past is not None and layer_past[0].numel() > 0\n-\n         # Compute QKV\n         # Attention heads [batch, seq_len, hidden_size]\n         #   --> [batch, seq_len, (np * 3 * head_size)]\n@@ -114,24 +181,29 @@ def forward(\n         key_rot = key[..., : self.rotary_ndims]\n         key_pass = key[..., self.rotary_ndims :]\n \n-        # Compute token offset for rotary embeddings (when decoding)\n-        seq_len = key.shape[-2]\n-        offset = 0\n-        if has_layer_past:\n-            offset = layer_past[0].shape[-2]\n-            seq_len += offset\n-        cos, sin = self.rotary_emb(value, seq_len=seq_len)\n-        query, key = apply_rotary_pos_emb(query_rot, key_rot, cos, sin, offset=offset)\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n+        query, key = apply_rotary_pos_emb(query_rot, key_rot, cos, sin)\n         query = torch.cat((query, query_pass), dim=-1)\n         key = torch.cat((key, key_pass), dim=-1)\n \n         # Cache QKV values\n-        if has_layer_past:\n-            past_key = layer_past[0]\n-            past_value = layer_past[1]\n-            key = torch.cat((past_key, key), dim=-2)\n-            value = torch.cat((past_value, value), dim=-2)\n-        present = (key, value) if use_cache else None\n+        if layer_past is not None:\n+            cache_kwargs = {\n+                \"sin\": sin,\n+                \"cos\": cos,\n+                \"partial_rotation_size\": self.rotary_ndims,\n+                \"cache_position\": cache_position,\n+            }\n+            key, value = layer_past.update(key, value, self.layer_idx, cache_kwargs)\n \n         # Compute attention\n         attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n@@ -140,7 +212,7 @@ def forward(\n         attn_output = self._merge_heads(attn_output, self.num_attention_heads, self.head_size)\n         attn_output = self.dense(attn_output)\n \n-        outputs = (attn_output, present)\n+        outputs = (attn_output, layer_past)\n         if output_attentions:\n             outputs += (attn_weights,)\n \n@@ -171,52 +243,37 @@ def _merge_heads(cls, tensor, num_attention_heads, attn_head_size):\n         # -> [bs, seq_len, hidden_size]\n         return tensor\n \n-    def _create_causal_mask(self, key_length, query_length):\n-        causal_mask = torch.tril(\n-            torch.ones((self.max_positions, self.max_positions), dtype=torch.bool).view(\n-                1, 1, self.max_positions, self.max_positions\n-            )\n-        )\n-        return causal_mask[:, :, key_length - query_length : key_length, :key_length]\n-\n     def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n         # q, k, v: [bs, num_attention_heads, seq_len, attn_head_size]\n         # compute causal mask from causal mask buffer\n         batch_size, num_attention_heads, query_length, attn_head_size = query.size()\n         key_length = key.size(-2)\n \n-        causal_mask = self._create_causal_mask(key_length, query_length)\n-\n         query = query.view(batch_size * num_attention_heads, query_length, attn_head_size)\n         key = key.view(batch_size * num_attention_heads, key_length, attn_head_size)\n+\n+        # [batch_size * num_heads, q_length, kv_length]\n         attn_scores = torch.zeros(\n             batch_size * num_attention_heads,\n             query_length,\n             key_length,\n             dtype=query.dtype,\n             device=key.device,\n         )\n-        attn_scores = torch.baddbmm(\n+        attention_scores = torch.baddbmm(\n             attn_scores,\n             query,\n             key.transpose(1, 2),\n             beta=1.0,\n-            alpha=(torch.tensor(1.0, dtype=self.norm_factor.dtype, device=self.norm_factor.device) / self.norm_factor),\n+            alpha=1.0 / self.norm_factor,\n         )\n-        attn_scores = attn_scores.view(batch_size, num_attention_heads, query_length, key_length)\n-\n-        mask_value = torch.finfo(attn_scores.dtype).min\n-        # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n-        # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\n-        mask_value = torch.tensor(mask_value, dtype=attn_scores.dtype).to(attn_scores.device)\n-        causal_mask = causal_mask.to(attn_scores.device)\n-        attn_scores = torch.where(causal_mask, attn_scores, mask_value)\n \n-        if attention_mask is not None:\n-            # Apply the attention mask\n-            attn_scores = attn_scores + attention_mask\n+        attention_scores = attention_scores.view(batch_size, num_attention_heads, query_length, -1)\n+        if attention_mask is not None:  # no matter the length, we just slice it\n+            causal_mask = attention_mask[:, :, :, : key.shape[-2]]\n+            attention_scores = attention_scores + causal_mask\n \n-        attn_weights = nn.functional.softmax(attn_scores, dim=-1)\n+        attn_weights = nn.functional.softmax(attention_scores, dim=-1)\n         attn_weights = self.attention_dropout(attn_weights)\n         attn_weights = attn_weights.to(value.dtype)\n \n@@ -228,42 +285,92 @@ def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n         return attn_output, attn_weights\n \n \n-# Copied from transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXRotaryEmbedding with GPTNeoXRotaryEmbedding->RotaryEmbedding\n-class RotaryEmbedding(nn.Module):\n-    # Copied from transformers.models.mixtral.modeling_mixtral.MixtralRotaryEmbedding.__init__\n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n+# Copied from transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXRotaryEmbedding with GPTNeoX->GPTNeoXJapanese\n+class GPTNeoXJapaneseRotaryEmbedding(nn.Module):\n+    def __init__(\n+        self,\n+        dim=None,\n+        max_position_embeddings=2048,\n+        base=10000,\n+        device=None,\n+        scaling_factor=1.0,\n+        rope_type=\"default\",\n+        config: Optional[GPTNeoXJapaneseConfig] = None,\n+    ):\n         super().__init__()\n+        # TODO (joao): remove the `if` below, only used for BC\n+        self.rope_kwargs = {}\n+        if config is None:\n+            logger.warning_once(\n+                \"`GPTNeoXJapaneseRotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n+                \"`config` argument. All other arguments will be removed in v4.46\"\n+            )\n+            self.rope_kwargs = {\n+                \"rope_type\": rope_type,\n+                \"factor\": scaling_factor,\n+                \"dim\": dim,\n+                \"base\": base,\n+                \"max_position_embeddings\": max_position_embeddings,\n+            }\n+            self.rope_type = rope_type\n+            self.max_seq_len_cached = max_position_embeddings\n+            self.original_max_seq_len = max_position_embeddings\n+        else:\n+            # BC: \"rope_type\" was originally \"type\"\n+            if config.rope_scaling is not None:\n+                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+            else:\n+                self.rope_type = \"default\"\n+            self.max_seq_len_cached = config.max_position_embeddings\n+            self.original_max_seq_len = config.max_position_embeddings\n \n-        self.dim = dim\n-        self.max_position_embeddings = max_position_embeddings\n-        self.base = base\n-        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n \n-        # Build here to make `torch.jit.trace` work.\n-        self._set_cos_sin_cache(\n-            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n-        )\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(\n+                self.config, device, seq_len=seq_len, **self.rope_kwargs\n+            )\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n \n-    def _set_cos_sin_cache(self, seq_len, device, dtype):\n-        self.max_seq_len_cached = seq_len\n-        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n \n-        freqs = torch.outer(t, self.inv_freq)\n-        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n-        emb = torch.cat((freqs, freqs), dim=-1)\n-        self.register_buffer(\"cos_cached\", emb.cos(), persistent=False)\n-        self.register_buffer(\"sin_cached\", emb.sin(), persistent=False)\n+    @torch.no_grad()\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n \n-    def forward(self, x, seq_len=None):\n-        # x: [bs, num_attention_heads, seq_len, head_size]\n-        if seq_len > self.max_seq_len_cached:\n-            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n+        # Core RoPE block\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n+        device_type = x.device.type\n+        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos()\n+            sin = emb.sin()\n \n-        return (\n-            self.cos_cached[:seq_len],\n-            self.sin_cached[:seq_len],\n-        )\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n def rotate_half(x):\n@@ -273,9 +380,29 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n-def apply_rotary_pos_emb(q, k, cos, sin, offset: int = 0):\n-    cos = cos[..., offset : q.shape[-2] + offset, :]\n-    sin = sin[..., offset : q.shape[-2] + offset, :]\n+# Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n     q_embed = (q * cos) + (rotate_half(q) * sin)\n     k_embed = (k * cos) + (rotate_half(k) * sin)\n     return q_embed, k_embed\n@@ -325,18 +452,23 @@ def __init__(self, config, layer_number):\n         self.input_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.post_attention_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         # activate bias only last layer\n-        self.attention = GPTNeoXJapaneseAttention(config=config, use_bias=layer_number == config.num_hidden_layers - 1)\n+        self.attention = GPTNeoXJapaneseAttention(\n+            config=config, use_bias=layer_number == config.num_hidden_layers - 1, layer_idx=layer_number\n+        )\n         self.mlp = GPTNeoXJapaneseMLP(config)\n         self.hidden_dropout = config.hidden_dropout\n \n     def forward(\n         self,\n-        hidden_states,\n-        attention_mask=None,\n-        head_mask=None,\n-        use_cache=False,\n-        layer_past=None,\n-        output_attentions=False,\n+        hidden_states: Optional[torch.FloatTensor],\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = False,\n+        layer_past: Optional[Cache] = None,\n+        output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n     ):\n         residual = hidden_states\n         ln_out = self.input_layernorm(hidden_states)\n@@ -347,6 +479,9 @@ def forward(\n             head_mask=head_mask,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n+            position_ids=position_ids,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n         )\n         attn_output = attention_layer_outputs[0]  # output_attn: a, present, (attentions)\n         outputs = attention_layer_outputs[1:]\n@@ -419,6 +554,26 @@ def forward(\n             Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n             is useful if you want more control over how to convert *input_ids* indices into associated vectors than the\n             model's internal embedding lookup matrix.\n+        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n+            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n+            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n+\n+            Two formats are allowed:\n+            - a [`~cache_utils.Cache`] instance;\n+            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n+            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n+            cache format.\n+\n+            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n+            legacy cache format will be returned.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n+            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n+            of shape `(batch_size, sequence_length)`.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n         output_attentions (`bool`, *optional*):\n             Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n             tensors for more detail.\n@@ -427,6 +582,10 @@ def forward(\n             more detail.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~file_utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n \"\"\"\n \n \n@@ -444,6 +603,7 @@ def __init__(self, config):\n             [GPTNeoXJapaneseLayer(config=config, layer_number=i) for i in range(config.num_hidden_layers)]\n         )\n         self.final_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.rotary_emb = GPTNeoXJapaneseRotaryEmbedding(config=config)\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -460,24 +620,17 @@ def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Union[Cache, Tuple[Tuple[torch.FloatTensor]]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         r\"\"\"\n-        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n-            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n-            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-\n         Returns:\n \n         Example:\n@@ -502,70 +655,68 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n \n-        if input_ids is not None and inputs_embeds is not None:\n-            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n-        elif input_ids is not None:\n-            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n-            input_shape = input_ids.size()\n-        elif inputs_embeds is not None:\n-            input_shape = inputs_embeds.size()[:-1]\n-        else:\n-            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n-\n-        batch_size, seq_length = input_shape\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\n+                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n+            )\n \n-        if past_key_values is None:\n-            past_key_values = tuple([None] * self.config.num_hidden_layers)\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_in(input_ids)\n \n-        # Attention mask.\n-        if attention_mask is not None:\n-            if not batch_size > 0:\n-                raise ValueError(\"batch_size has to be defined and > 0\")\n-            attention_mask = attention_mask.view(batch_size, -1)\n-            # We create a 3D attention mask from a 2D tensor mask.\n-            # Sizes are [batch_size, 1, 1, to_seq_length]\n-            # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n-            # this attention mask is more simple than the triangular masking of causal attention\n-            # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n-            attention_mask = attention_mask[:, None, None, :]\n-\n-            # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n-            # masked positions, this operation will create a tensor which is 0.0 for\n-            # positions we want to attend and -10000.0 for masked positions.\n-            # Since we are adding it to the raw scores before the softmax, this is\n-            # effectively the same as removing these entirely.\n-            attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n-            attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min\n+        use_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            use_legacy_cache = True\n+            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+            if not self.training:\n+                logger.warning_once(\n+                    \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.45. \"\n+                    \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n+                )\n+\n+        seq_length = inputs_embeds.shape[1]\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(past_seen_tokens, past_seen_tokens + seq_length, device=inputs_embeds.device)\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = self._update_causal_mask(\n+            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        )\n \n         # Prepare head mask if needed\n         # 1.0 in head_mask indicate we keep the head\n         # attention_probs has shape bsz x n_heads x N x N\n         # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n         # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n         head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n-        if inputs_embeds is None:\n-            inputs_embeds = self.embed_in(input_ids)\n-\n         hidden_states = inputs_embeds\n \n-        presents = () if use_cache else None\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        next_decoder_cache = None\n         all_attentions = () if output_attentions else None\n         all_hidden_states = () if output_hidden_states else None\n-        for i, (layer, layer_past) in enumerate(zip(self.layers, past_key_values)):\n+        for i, layer in enumerate(self.layers):\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n+\n             outputs = layer(\n                 hidden_states,\n-                attention_mask=attention_mask,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n                 head_mask=head_mask[i],\n-                layer_past=layer_past,\n+                layer_past=past_key_values,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n             )\n             hidden_states = outputs[0]\n             if use_cache is True:\n-                presents = presents + (outputs[1],)\n+                next_decoder_cache = outputs[1]\n             if output_attentions:\n                 all_attentions = all_attentions + (outputs[2 if use_cache else 1],)\n \n@@ -574,16 +725,87 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n+        next_cache = None\n+        if use_cache:\n+            next_cache = next_decoder_cache.to_legacy_cache() if use_legacy_cache else next_decoder_cache\n+\n         if not return_dict:\n-            return tuple(v for v in [hidden_states, presents, all_hidden_states, all_attentions] if v is not None)\n+            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_attentions] if v is not None)\n \n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=presents,\n+            past_key_values=next_cache,\n             hidden_states=all_hidden_states,\n             attentions=all_attentions,\n         )\n \n+    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: torch.Tensor,\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+        output_attentions: bool,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and 0.0 in attention_mask:\n+                return attention_mask\n+            return None\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_static_cache = isinstance(past_key_values, StaticCache)\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype, device = input_tensor.dtype, input_tensor.device\n+        min_dtype = torch.finfo(dtype).min\n+        sequence_length = input_tensor.shape[1]\n+        if using_static_cache:\n+            target_length = past_key_values.get_max_length()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = _prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            device=device,\n+            min_dtype=min_dtype,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type == \"cuda\"\n+            and not output_attentions\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask\n+\n \n @add_start_docstrings(\n     \"\"\"GPTNeoXJapanese Model with a `language modeling` head on top for Classifier Model fine-tuning.\"\"\",\n@@ -614,35 +836,22 @@ def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Union[Cache, Tuple[Tuple[torch.FloatTensor]]]] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n-            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`. The two additional tensors are\n-            only required when the model is used as a decoder in a Sequence to Sequence model.\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n             `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are\n             ignored (masked), the loss is only computed for the tokens with labels n `[0, ..., config.vocab_size]`.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n \n         Returns:\n \n@@ -668,13 +877,15 @@ def forward(\n         outputs = self.gpt_neox_japanese(\n             input_ids,\n             attention_mask=attention_mask,\n+            position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         hidden_states = outputs[0]\n@@ -703,18 +914,76 @@ def forward(\n             attentions=outputs.attentions,\n         )\n \n-    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, attention_mask=None, **model_kwargs):\n-        input_shape = input_ids.shape\n-\n-        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n-        if attention_mask is None:\n-            attention_mask = input_ids.new_ones(input_shape)\n-\n-        # cut decoder_input_ids if past is used\n-        if past_key_values and past_key_values[0] is not None:\n-            input_ids = input_ids[:, -1:]\n+    # Copied from transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM.prepare_inputs_for_generation\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        attention_mask=None,\n+        inputs_embeds=None,\n+        cache_position=None,\n+        position_ids=None,\n+        use_cache=True,\n+        **kwargs,\n+    ):\n+        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n+        # Exception 1: when passing input_embeds, input_ids may be missing entries\n+        # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n+        if past_key_values is not None:\n+            if inputs_embeds is not None:  # Exception 1\n+                input_ids = input_ids[:, -cache_position.shape[0] :]\n+            elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n+                input_ids = input_ids[:, cache_position]\n+\n+        if attention_mask is not None and position_ids is None:\n+            # create position_ids on the fly for batch generation\n+            position_ids = attention_mask.long().cumsum(-1) - 1\n+            position_ids.masked_fill_(attention_mask == 0, 1)\n+            if past_key_values:\n+                position_ids = position_ids[:, -input_ids.shape[1] :]\n+\n+                # This `clone` call is needed to avoid recapturing cuda graphs with `torch.compile`'s  `mode=\"reduce-overhead`, as otherwise the input `position_ids` would have various stride during the decoding. Here, simply using `.contiguous()` is not sufficient as in the batch size = 1 case, `position_ids` is already contiguous but with varying stride which retriggers a capture.\n+                position_ids = position_ids.clone(memory_format=torch.contiguous_format)\n+\n+        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n+        if inputs_embeds is not None and cache_position[0] == 0:\n+            model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": None}\n+        else:\n+            # The clone here is for the same reason as for `position_ids`.\n+            model_inputs = {\"input_ids\": input_ids.clone(memory_format=torch.contiguous_format), \"inputs_embeds\": None}\n+\n+        if isinstance(past_key_values, StaticCache) and attention_mask.ndim == 2:\n+            if model_inputs[\"inputs_embeds\"] is not None:\n+                batch_size, sequence_length, _ = model_inputs[\"inputs_embeds\"].shape\n+                device = model_inputs[\"inputs_embeds\"].device\n+            else:\n+                batch_size, sequence_length = model_inputs[\"input_ids\"].shape\n+                device = model_inputs[\"input_ids\"].device\n+\n+            dtype = self.embed_out.weight.dtype\n+            min_dtype = torch.finfo(dtype).min\n+\n+            attention_mask = _prepare_4d_causal_attention_mask_with_cache_position(\n+                attention_mask,\n+                sequence_length=sequence_length,\n+                target_length=past_key_values.get_max_length(),\n+                dtype=dtype,\n+                device=device,\n+                min_dtype=min_dtype,\n+                cache_position=cache_position,\n+                batch_size=batch_size,\n+            )\n \n-        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"past_key_values\": past_key_values}\n+        model_inputs.update(\n+            {\n+                \"position_ids\": position_ids,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values,\n+                \"use_cache\": use_cache,\n+                \"attention_mask\": attention_mask,\n+            }\n+        )\n+        return model_inputs\n \n     def _reorder_cache(self, past_key_values, beam_idx):\n         reordered_past = ()"
        },
        {
            "sha": "0d26c4fc65de788e8bf8039d6bc6ef828ae99af0",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -1018,6 +1018,7 @@ class IdeficsPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"IdeficsDecoderLayer\", \"IdeficsGatedCrossAttentionLayer\"]\n     _supports_sdpa = True\n     _supports_cache_class = True\n+    _supports_static_cache = True\n \n     def _init_weights(self, module):\n         # important: this ported version of Idefics isn't meant for training from scratch - only"
        },
        {
            "sha": "9a1d6c0749f93211d4c84083e774a7a835d63189",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -149,7 +149,7 @@ def __init__(\n         if config is None:\n             logger.warning_once(\n                 \"`LlamaRotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n-                \"`config` argument. All other arguments will be removed in v4.45\"\n+                \"`config` argument. All other arguments will be removed in v4.46\"\n             )\n             self.rope_kwargs = {\n                 \"rope_type\": rope_type,\n@@ -224,7 +224,7 @@ class LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):\n \n     def __init__(self, *args, **kwargs):\n         logger.warning_once(\n-            \"`LlamaLinearScalingRotaryEmbedding` is deprecated an will be removed in v4.45. Please use \"\n+            \"`LlamaLinearScalingRotaryEmbedding` is deprecated an will be removed in v4.46. Please use \"\n             \"`LlamaRotaryEmbedding`, which now also does linear scaling (simply pass the model config to __init__).\"\n         )\n         kwargs[\"rope_type\"] = \"linear\"\n@@ -236,7 +236,7 @@ class LlamaDynamicNTKScalingRotaryEmbedding(LlamaRotaryEmbedding):\n \n     def __init__(self, *args, **kwargs):\n         logger.warning_once(\n-            \"`LlamaDynamicNTKScalingRotaryEmbedding` is deprecated an will be removed in v4.45. Please use \"\n+            \"`LlamaDynamicNTKScalingRotaryEmbedding` is deprecated an will be removed in v4.46. Please use \"\n             \"`LlamaRotaryEmbedding`, which now also does dynamic ntk scaling (simply pass the model config to \"\n             \"__init__).\"\n         )\n@@ -353,7 +353,7 @@ def __init__(self, config: LlamaConfig, layer_idx: Optional[int] = None):\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n         self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n \n-        # TODO (joao): remove in v4.45 (RoPE is computed in the model, not in the decoder layers)\n+        # TODO (joao): remove in v4.46 (RoPE is computed in the model, not in the decoder layers)\n         self.rotary_emb = LlamaRotaryEmbedding(config=self.config)\n \n     def forward(\n@@ -365,7 +365,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n         **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n@@ -400,7 +400,7 @@ def forward(\n             logger.warning_once(\n                 \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n                 \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.45 `position_ids` will be \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n                 \"removed and `position_embeddings` will be mandatory.\"\n             )\n             cos, sin = self.rotary_emb(value_states, position_ids)\n@@ -473,7 +473,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if isinstance(past_key_value, StaticCache):\n             raise ValueError(\n@@ -500,7 +500,7 @@ def forward(\n             logger.warning_once(\n                 \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n                 \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.45 `position_ids` will be \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n                 \"removed and `position_embeddings` will be mandatory.\"\n             )\n             cos, sin = self.rotary_emb(value_states, position_ids)\n@@ -586,7 +586,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n         **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if output_attentions:\n@@ -620,7 +620,7 @@ def forward(\n             logger.warning_once(\n                 \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n                 \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.45 `position_ids` will be \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n                 \"removed and `position_embeddings` will be mandatory.\"\n             )\n             cos, sin = self.rotary_emb(value_states, position_ids)\n@@ -695,7 +695,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n         **kwargs,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\""
        },
        {
            "sha": "c43418182c388133484a31595304193b69d350ff",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -871,7 +871,7 @@ def _update_causal_mask(\n         # to infer the attention mask.\n \n         # cache_position must be valid here no matter which cache we use\n-        past_seen_tokens = cache_position[0] if past_key_values is not None else 0\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n         using_static_cache = isinstance(past_key_values, StaticCache)\n         using_sliding_window_cache = isinstance(past_key_values, SlidingWindowCache)\n "
        },
        {
            "sha": "2e23d06699087eab695bea8db43b7a6b6de2f71b",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -848,7 +848,8 @@ def forward(\n     \"The bare Mixtral Model outputting raw hidden-states without any specific head on top.\",\n     MIXTRAL_START_DOCSTRING,\n )\n-# Copied from transformers.models.qwen2.modeling_qwen2.Qwen2PreTrainedModel with Qwen2->Mixtral\n+# copied from transformers.models.qwen2.modeling_qwen2.Qwen2PreTrainedModel with Qwen2->Mixtral\n+# TODO (Raushan): bring back copied after compile compatibility\n class MixtralPreTrainedModel(PreTrainedModel):\n     config_class = MixtralConfig\n     base_model_prefix = \"model\""
        },
        {
            "sha": "4d079b4dde104d4166fab6d2903d236f3bbfd13a",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -589,7 +589,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n         **kwargs,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\""
        },
        {
            "sha": "a53f1eeda611966fcc5e68964f209019bfc086a9",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -222,7 +222,7 @@ def __init__(\n         if config is None:\n             logger.warning_once(\n                 \"`OlmoeRotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n-                \"`config` argument. All other arguments will be removed in v4.45\"\n+                \"`config` argument. All other arguments will be removed in v4.46\"\n             )\n             self.rope_kwargs = {\n                 \"rope_type\": rope_type,"
        },
        {
            "sha": "7619d70c08fb7c96c35cd0d00f560aa8b022efdf",
            "filename": "src/transformers/models/persimmon/configuration_persimmon.py",
            "status": "modified",
            "additions": 42,
            "deletions": 28,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fconfiguration_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fconfiguration_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fconfiguration_persimmon.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -15,6 +15,7 @@\n \"\"\"Persimmon model configuration\"\"\"\n \n from ...configuration_utils import PretrainedConfig\n+from ...modeling_rope_utils import rope_config_validation\n from ...utils import logging\n \n \n@@ -60,13 +61,42 @@ class PersimmonConfig(PretrainedConfig):\n         rope_theta (`float`, *optional*, defaults to 25000.0):\n             The base period of the RoPE embeddings.\n         rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n-            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n-            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n-            `max_position_embeddings` to the expected new maximum. See the following thread for more information on how\n-            these scaling strategies behave:\n-            https://www.reddit.com/r/LocalPersimmon/comments/14mrgpr/dynamically_scaled_rope_further_increases/. This\n-            is an experimental feature, subject to breaking API changes in future versions.\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n         qk_layernorm (`bool`, *optional*, default to `True`):\n             Whether or not to normalize the Queries and Keys after projecting the hidden states\n         hidden_dropout (`float`, *optional*, default to 0.0):\n@@ -128,7 +158,11 @@ def __init__(\n         self.hidden_dropout = hidden_dropout\n         self.attention_dropout = attention_dropout\n         self.partial_rotary_factor = partial_rotary_factor\n-        self._rope_scaling_validation()\n+        # Validate the correctness of rotary position embeddings parameters\n+        # BC: if there is a 'type' field, move it to 'rope_type'.\n+        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n+            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_config_validation(self)\n \n         super().__init__(\n             pad_token_id=pad_token_id,\n@@ -137,23 +171,3 @@ def __init__(\n             tie_word_embeddings=tie_word_embeddings,\n             **kwargs,\n         )\n-\n-    def _rope_scaling_validation(self):\n-        \"\"\"\n-        Validate the `rope_scaling` configuration.\n-        \"\"\"\n-        if self.rope_scaling is None:\n-            return\n-\n-        if not isinstance(self.rope_scaling, dict) or len(self.rope_scaling) != 2:\n-            raise ValueError(\n-                \"`rope_scaling` must be a dictionary with two fields, `type` and `factor`, \" f\"got {self.rope_scaling}\"\n-            )\n-        rope_scaling_type = self.rope_scaling.get(\"type\", None)\n-        rope_scaling_factor = self.rope_scaling.get(\"factor\", None)\n-        if rope_scaling_type is None or rope_scaling_type not in [\"linear\", \"dynamic\"]:\n-            raise ValueError(\n-                f\"`rope_scaling`'s type field must be one of ['linear', 'dynamic'], got {rope_scaling_type}\"\n-            )\n-        if rope_scaling_factor is None or not isinstance(rope_scaling_factor, float) or rope_scaling_factor <= 1.0:\n-            raise ValueError(f\"`rope_scaling`'s factor field must be a float > 1, got {rope_scaling_factor}\")"
        },
        {
            "sha": "9fab09bdcc7877fc820ff081fc9db4d7631f731c",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 142,
            "deletions": 129,
            "changes": 271,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -36,6 +36,7 @@\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n from .configuration_persimmon import PersimmonConfig\n@@ -100,88 +101,119 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n     return causal_mask\n \n \n-#  Copied from transformers.models.mixtral.modeling_mixtral.MixtralRotaryEmbedding with Mixtral->Persimmon\n+# Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Persimmon\n class PersimmonRotaryEmbedding(nn.Module):\n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n+    def __init__(\n+        self,\n+        dim=None,\n+        max_position_embeddings=2048,\n+        base=10000,\n+        device=None,\n+        scaling_factor=1.0,\n+        rope_type=\"default\",\n+        config: Optional[PersimmonConfig] = None,\n+    ):\n         super().__init__()\n+        # TODO (joao): remove the `if` below, only used for BC\n+        self.rope_kwargs = {}\n+        if config is None:\n+            logger.warning_once(\n+                \"`PersimmonRotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n+                \"`config` argument. All other arguments will be removed in v4.46\"\n+            )\n+            self.rope_kwargs = {\n+                \"rope_type\": rope_type,\n+                \"factor\": scaling_factor,\n+                \"dim\": dim,\n+                \"base\": base,\n+                \"max_position_embeddings\": max_position_embeddings,\n+            }\n+            self.rope_type = rope_type\n+            self.max_seq_len_cached = max_position_embeddings\n+            self.original_max_seq_len = max_position_embeddings\n+        else:\n+            # BC: \"rope_type\" was originally \"type\"\n+            if config.rope_scaling is not None:\n+                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+            else:\n+                self.rope_type = \"default\"\n+            self.max_seq_len_cached = config.max_position_embeddings\n+            self.original_max_seq_len = config.max_position_embeddings\n \n-        self.dim = dim\n-        self.max_position_embeddings = max_position_embeddings\n-        self.base = base\n-        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-\n-        # Build here to make `torch.jit.trace` work.\n-        self._set_cos_sin_cache(\n-            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n-        )\n-\n-    def _set_cos_sin_cache(self, seq_len, device, dtype):\n-        self.max_seq_len_cached = seq_len\n-        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n-\n-        freqs = torch.outer(t, self.inv_freq)\n-        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n-        emb = torch.cat((freqs, freqs), dim=-1)\n-        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n-        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n-\n-    def forward(self, x, seq_len=None):\n-        # x: [bs, num_attention_heads, seq_len, head_size]\n-        if seq_len > self.max_seq_len_cached:\n-            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n-\n-        return (\n-            self.cos_cached[:seq_len].to(dtype=x.dtype),\n-            self.sin_cached[:seq_len].to(dtype=x.dtype),\n-        )\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n \n-# Copied from transformers.models.falcon.modeling_falcon.FalconLinearScalingRotaryEmbedding with Falcon->Persimmon\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(\n+                self.config, device, seq_len=seq_len, **self.rope_kwargs\n+            )\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n+\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n+\n+    @torch.no_grad()\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n+\n+        # Core RoPE block\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n+        device_type = x.device.type\n+        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos()\n+            sin = emb.sin()\n+\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+# Copied from transformers.models.llama.modeling_llama.LlamaLinearScalingRotaryEmbedding with Llama->Persimmon\n class PersimmonLinearScalingRotaryEmbedding(PersimmonRotaryEmbedding):\n     \"\"\"PersimmonRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n \n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n-        self.scaling_factor = scaling_factor\n-        super().__init__(dim, max_position_embeddings, base, device)\n-\n-    def _set_cos_sin_cache(self, seq_len, device, dtype):\n-        self.max_seq_len_cached = seq_len\n-        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n-        t = t / self.scaling_factor\n-\n-        freqs = torch.outer(t, self.inv_freq)\n-        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n-        emb = torch.cat((freqs, freqs), dim=-1)\n-        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n-        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n+    def __init__(self, *args, **kwargs):\n+        logger.warning_once(\n+            \"`PersimmonLinearScalingRotaryEmbedding` is deprecated an will be removed in v4.46. Please use \"\n+            \"`PersimmonRotaryEmbedding`, which now also does linear scaling (simply pass the model config to __init__).\"\n+        )\n+        kwargs[\"rope_type\"] = \"linear\"\n+        super().__init__(*args, **kwargs)\n \n \n-# Copied from transformers.models.falcon.modeling_falcon.FalconDynamicNTKScalingRotaryEmbedding with Falcon->Persimmon\n+# Copied from transformers.models.llama.modeling_llama.LlamaDynamicNTKScalingRotaryEmbedding with Llama->Persimmon\n class PersimmonDynamicNTKScalingRotaryEmbedding(PersimmonRotaryEmbedding):\n     \"\"\"PersimmonRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n \n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n-        self.scaling_factor = scaling_factor\n-        super().__init__(dim, max_position_embeddings, base, device)\n-\n-    def _set_cos_sin_cache(self, seq_len, device, dtype):\n-        self.max_seq_len_cached = seq_len\n-\n-        if seq_len > self.max_position_embeddings:\n-            base = self.base * (\n-                (self.scaling_factor * seq_len / self.max_position_embeddings) - (self.scaling_factor - 1)\n-            ) ** (self.dim / (self.dim - 2))\n-            inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-\n-        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n-\n-        freqs = torch.outer(t, self.inv_freq)\n-        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n-        emb = torch.cat((freqs, freqs), dim=-1)\n-        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n-        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n+    def __init__(self, *args, **kwargs):\n+        logger.warning_once(\n+            \"`PersimmonDynamicNTKScalingRotaryEmbedding` is deprecated an will be removed in v4.46. Please use \"\n+            \"`PersimmonRotaryEmbedding`, which now also does dynamic ntk scaling (simply pass the model config to \"\n+            \"__init__).\"\n+        )\n+        kwargs[\"rope_type\"] = \"dynamic\"\n+        super().__init__(*args, **kwargs)\n \n \n # Copied from transformers.models.llama.modeling_llama.rotate_half\n@@ -192,18 +224,17 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n-# Copied from transformers.models.mixtral.modeling_mixtral.apply_rotary_pos_emb\n-def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n+# Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n     Args:\n         q (`torch.Tensor`): The query tensor.\n         k (`torch.Tensor`): The key tensor.\n         cos (`torch.Tensor`): The cosine part of the rotary embedding.\n         sin (`torch.Tensor`): The sine part of the rotary embedding.\n-        position_ids (`torch.Tensor`):\n-            The position indices of the tokens corresponding to the query and key tensors. For example, this can be\n-            used to pass offsetted position ids when working with a KV-cache.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n         unsqueeze_dim (`int`, *optional*, defaults to 1):\n             The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n             sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n@@ -214,8 +245,8 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n     Returns:\n         `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n     \"\"\"\n-    cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n-    sin = sin[position_ids].unsqueeze(unsqueeze_dim)\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n     q_embed = (q * cos) + (rotate_half(q) * sin)\n     k_embed = (k * cos) + (rotate_half(k) * sin)\n     return q_embed, k_embed\n@@ -253,9 +284,8 @@ def __init__(self, config: PersimmonConfig, layer_idx: Optional[int] = None):\n         self.hidden_size = config.hidden_size\n         self.num_heads = config.num_attention_heads\n         self.head_dim = self.hidden_size // self.num_heads\n-        self.max_position_embeddings = config.max_position_embeddings\n         self.rope_theta = config.rope_theta\n-        self.partial_rotary_factor = config.partial_rotary_factor\n+        self.rotary_ndims = int(self.head_dim * config.partial_rotary_factor)\n         self.is_causal = True\n \n         if (self.head_dim * self.num_heads) != self.hidden_size:\n@@ -275,34 +305,7 @@ def __init__(self, config: PersimmonConfig, layer_idx: Optional[int] = None):\n                 config.hidden_size // self.num_heads, eps=config.layer_norm_eps, elementwise_affine=True\n             )\n         self.attention_dropout = nn.Dropout(config.attention_dropout)\n-        self._init_rope()\n-\n-    def _init_rope(self):\n-        if self.config.rope_scaling is None:\n-            self.rotary_emb = PersimmonRotaryEmbedding(\n-                int(self.partial_rotary_factor * self.head_dim),\n-                max_position_embeddings=self.max_position_embeddings,\n-                base=self.rope_theta,\n-            )\n-        else:\n-            scaling_type = self.config.rope_scaling[\"type\"]\n-            scaling_factor = self.config.rope_scaling[\"factor\"]\n-            if scaling_type == \"linear\":\n-                self.rotary_emb = PersimmonLinearScalingRotaryEmbedding(\n-                    int(self.partial_rotary_factor * self.head_dim),\n-                    max_position_embeddings=self.max_position_embeddings,\n-                    scaling_factor=scaling_factor,\n-                    base=self.rope_theta,\n-                )\n-            elif scaling_type == \"dynamic\":\n-                self.rotary_emb = PersimmonDynamicNTKScalingRotaryEmbedding(\n-                    int(self.partial_rotary_factor * self.head_dim),\n-                    max_position_embeddings=self.max_position_embeddings,\n-                    scaling_factor=scaling_factor,\n-                    base=self.rope_theta,\n-                )\n-            else:\n-                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n+        self.rotary_emb = PersimmonRotaryEmbedding(config=self.config)\n \n     def _split_heads(self, fused_qkv: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n         \"\"\"\n@@ -329,6 +332,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n \n@@ -347,28 +351,28 @@ def forward(\n         value_states = value_states.transpose(1, 2)\n         key_states = key_states.transpose(1, 2)\n \n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            if self.layer_idx is None:\n-                raise ValueError(\n-                    f\"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} \"\n-                    \"for auto-regressive decoding with k/v caching, please make sure to initialize the attention class \"\n-                    \"with a layer index.\"\n-                )\n-            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n-        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n \n         # Partial rotary embedding\n         query_rot, query_pass = (\n-            query_states[..., : self.rotary_emb.dim],\n-            query_states[..., self.rotary_emb.dim :],\n+            query_states[..., : self.rotary_ndims],\n+            query_states[..., self.rotary_ndims :],\n         )\n         key_rot, key_pass = (\n-            key_states[..., : self.rotary_emb.dim],\n-            key_states[..., self.rotary_emb.dim :],\n+            key_states[..., : self.rotary_ndims],\n+            key_states[..., self.rotary_ndims :],\n         )\n         # [batch_size, seq_length, num_heads, head_dim // config.partial_rotary_factor]\n-        query_rot, key_rot = apply_rotary_pos_emb(query_rot, key_rot, cos, sin, position_ids)\n+        query_rot, key_rot = apply_rotary_pos_emb(query_rot, key_rot, cos, sin)\n \n         # [batch_size, seq_length, num_heads, head_dim]\n         query_states = torch.cat((query_rot, query_pass), dim=-1)\n@@ -379,19 +383,13 @@ def forward(\n             cache_kwargs = {\n                 \"sin\": sin,\n                 \"cos\": cos,\n-                \"partial_rotation_size\": self.rotary_emb.dim,\n+                \"partial_rotation_size\": self.rotary_ndims,\n                 \"cache_position\": cache_position,\n             }\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n \n-        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n         if attention_mask is not None:  # no matter the length, we just slice it\n             causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n             attn_weights = attn_weights + causal_mask\n@@ -438,6 +436,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n@@ -447,7 +446,6 @@ def forward(\n             position_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n                 Indices of positions of each input sequence tokens in the position embeddings. Selected in the range\n                 `[0, config.n_positions - 1]`.\n-\n                 [What are position IDs?](../glossary#position-ids)\n             past_key_value (`Tuple(torch.FloatTensor)`, *optional*):\n                 cached past key and value projection states\n@@ -457,6 +455,11 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence\n+            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n+                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n+                with `head_dim` being the embedding dimension of each attention head.\n         \"\"\"\n \n         residual = hidden_states\n@@ -472,6 +475,7 @@ def forward(\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n         )\n         hidden_states = residual + hidden_states\n \n@@ -522,6 +526,8 @@ class PersimmonPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"PersimmonDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_cache_class = True\n+    _supports_quantized_cache = True\n+    _supports_static_cache = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -633,6 +639,8 @@ def __init__(self, config: PersimmonConfig):\n         )\n         self.final_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n \n+        self.rotary_emb = PersimmonRotaryEmbedding(config=config)\n+\n         self.gradient_checkpointing = False\n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -703,6 +711,9 @@ def forward(\n \n         hidden_states = inputs_embeds\n \n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n@@ -722,6 +733,7 @@ def forward(\n                     output_attentions,\n                     use_cache,\n                     cache_position,\n+                    position_embeddings,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n@@ -732,6 +744,7 @@ def forward(\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n+                    position_embeddings=position_embeddings,\n                 )\n \n             hidden_states = layer_outputs[0]"
        },
        {
            "sha": "6c871b7ea54fc78bec4dd607a2e457bba6584d27",
            "filename": "src/transformers/models/phi/configuration_phi.py",
            "status": "modified",
            "additions": 42,
            "deletions": 28,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fphi%2Fconfiguration_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fphi%2Fconfiguration_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fconfiguration_phi.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -16,6 +16,7 @@\n \"\"\"Phi model configuration\"\"\"\n \n from ...configuration_utils import PretrainedConfig\n+from ...modeling_rope_utils import rope_config_validation\n from ...utils import logging\n \n \n@@ -75,13 +76,42 @@ class PhiConfig(PretrainedConfig):\n         rope_theta (`float`, *optional*, defaults to 10000.0):\n             The base period of the RoPE embeddings.\n         rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n-            strategies: linear and dynamic. Their scaling factor must be an float greater than 1. The expected format\n-            is `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n-            `max_position_embeddings` to the expected new maximum. See the following thread for more information on how\n-            these scaling strategies behave:\n-            https://www.reddit.com/r/LocalPersimmon/comments/14mrgpr/dynamically_scaled_rope_further_increases/. This\n-            is an experimental feature, subject to breaking API changes in future versions.\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n         partial_rotary_factor (`float`, *optional*, defaults to 0.5):\n             Percentage of the query and keys which will have rotary embedding.\n         qk_layernorm (`bool`, *optional*, defaults to `False`):\n@@ -156,31 +186,15 @@ def __init__(\n         self.rope_scaling = rope_scaling\n         self.partial_rotary_factor = partial_rotary_factor\n         self.qk_layernorm = qk_layernorm\n-        self._rope_scaling_validation()\n+        # Validate the correctness of rotary position embeddings parameters\n+        # BC: if there is a 'type' field, move it to 'rope_type'.\n+        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n+            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_config_validation(self)\n \n         super().__init__(\n             bos_token_id=bos_token_id,\n             eos_token_id=eos_token_id,\n             tie_word_embeddings=tie_word_embeddings,\n             **kwargs,\n         )\n-\n-    def _rope_scaling_validation(self):\n-        \"\"\"\n-        Validate the `rope_scaling` configuration.\n-        \"\"\"\n-        if self.rope_scaling is None:\n-            return\n-\n-        if not isinstance(self.rope_scaling, dict) or len(self.rope_scaling) != 2:\n-            raise ValueError(\n-                \"`rope_scaling` must be a dictionary with two fields, `type` and `factor`, \" f\"got {self.rope_scaling}\"\n-            )\n-        rope_scaling_type = self.rope_scaling.get(\"type\", None)\n-        rope_scaling_factor = self.rope_scaling.get(\"factor\", None)\n-        if rope_scaling_type is None or rope_scaling_type not in [\"linear\", \"dynamic\"]:\n-            raise ValueError(\n-                f\"`rope_scaling`'s type field must be one of ['linear', 'dynamic'], got {rope_scaling_type}\"\n-            )\n-        if rope_scaling_factor is None or not isinstance(rope_scaling_factor, float) or rope_scaling_factor <= 1.0:\n-            raise ValueError(f\"`rope_scaling`'s factor field must be a float > 1, got {rope_scaling_factor}\")"
        },
        {
            "sha": "0d8be04af20d5ce108085bb9fe441937e791f9c4",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 173,
            "deletions": 154,
            "changes": 327,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -33,6 +33,7 @@\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_code_sample_docstrings,\n@@ -112,88 +113,119 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n     return causal_mask\n \n \n-# Copied from transformers.models.mixtral.modeling_mixtral.MixtralRotaryEmbedding with Mixtral->Phi\n+# Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Phi\n class PhiRotaryEmbedding(nn.Module):\n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n+    def __init__(\n+        self,\n+        dim=None,\n+        max_position_embeddings=2048,\n+        base=10000,\n+        device=None,\n+        scaling_factor=1.0,\n+        rope_type=\"default\",\n+        config: Optional[PhiConfig] = None,\n+    ):\n         super().__init__()\n+        # TODO (joao): remove the `if` below, only used for BC\n+        self.rope_kwargs = {}\n+        if config is None:\n+            logger.warning_once(\n+                \"`PhiRotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n+                \"`config` argument. All other arguments will be removed in v4.46\"\n+            )\n+            self.rope_kwargs = {\n+                \"rope_type\": rope_type,\n+                \"factor\": scaling_factor,\n+                \"dim\": dim,\n+                \"base\": base,\n+                \"max_position_embeddings\": max_position_embeddings,\n+            }\n+            self.rope_type = rope_type\n+            self.max_seq_len_cached = max_position_embeddings\n+            self.original_max_seq_len = max_position_embeddings\n+        else:\n+            # BC: \"rope_type\" was originally \"type\"\n+            if config.rope_scaling is not None:\n+                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+            else:\n+                self.rope_type = \"default\"\n+            self.max_seq_len_cached = config.max_position_embeddings\n+            self.original_max_seq_len = config.max_position_embeddings\n \n-        self.dim = dim\n-        self.max_position_embeddings = max_position_embeddings\n-        self.base = base\n-        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-\n-        # Build here to make `torch.jit.trace` work.\n-        self._set_cos_sin_cache(\n-            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n-        )\n-\n-    def _set_cos_sin_cache(self, seq_len, device, dtype):\n-        self.max_seq_len_cached = seq_len\n-        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n-\n-        freqs = torch.outer(t, self.inv_freq)\n-        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n-        emb = torch.cat((freqs, freqs), dim=-1)\n-        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n-        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n-\n-    def forward(self, x, seq_len=None):\n-        # x: [bs, num_attention_heads, seq_len, head_size]\n-        if seq_len > self.max_seq_len_cached:\n-            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n-\n-        return (\n-            self.cos_cached[:seq_len].to(dtype=x.dtype),\n-            self.sin_cached[:seq_len].to(dtype=x.dtype),\n-        )\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n \n-# Copied from transformers.models.falcon.modeling_falcon.FalconLinearScalingRotaryEmbedding with Falcon->Phi\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(\n+                self.config, device, seq_len=seq_len, **self.rope_kwargs\n+            )\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n+\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n+\n+    @torch.no_grad()\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n+\n+        # Core RoPE block\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n+        device_type = x.device.type\n+        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos()\n+            sin = emb.sin()\n+\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+# Copied from transformers.models.llama.modeling_llama.LlamaLinearScalingRotaryEmbedding with Llama->Phi\n class PhiLinearScalingRotaryEmbedding(PhiRotaryEmbedding):\n     \"\"\"PhiRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n \n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n-        self.scaling_factor = scaling_factor\n-        super().__init__(dim, max_position_embeddings, base, device)\n-\n-    def _set_cos_sin_cache(self, seq_len, device, dtype):\n-        self.max_seq_len_cached = seq_len\n-        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n-        t = t / self.scaling_factor\n-\n-        freqs = torch.outer(t, self.inv_freq)\n-        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n-        emb = torch.cat((freqs, freqs), dim=-1)\n-        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n-        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n+    def __init__(self, *args, **kwargs):\n+        logger.warning_once(\n+            \"`PhiLinearScalingRotaryEmbedding` is deprecated an will be removed in v4.46. Please use \"\n+            \"`PhiRotaryEmbedding`, which now also does linear scaling (simply pass the model config to __init__).\"\n+        )\n+        kwargs[\"rope_type\"] = \"linear\"\n+        super().__init__(*args, **kwargs)\n \n \n-# Copied from transformers.models.falcon.modeling_falcon.FalconDynamicNTKScalingRotaryEmbedding with Falcon->Phi\n+# Copied from transformers.models.llama.modeling_llama.LlamaDynamicNTKScalingRotaryEmbedding with Llama->Phi\n class PhiDynamicNTKScalingRotaryEmbedding(PhiRotaryEmbedding):\n     \"\"\"PhiRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n \n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n-        self.scaling_factor = scaling_factor\n-        super().__init__(dim, max_position_embeddings, base, device)\n-\n-    def _set_cos_sin_cache(self, seq_len, device, dtype):\n-        self.max_seq_len_cached = seq_len\n-\n-        if seq_len > self.max_position_embeddings:\n-            base = self.base * (\n-                (self.scaling_factor * seq_len / self.max_position_embeddings) - (self.scaling_factor - 1)\n-            ) ** (self.dim / (self.dim - 2))\n-            inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-\n-        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n-\n-        freqs = torch.outer(t, self.inv_freq)\n-        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n-        emb = torch.cat((freqs, freqs), dim=-1)\n-        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n-        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n+    def __init__(self, *args, **kwargs):\n+        logger.warning_once(\n+            \"`PhiDynamicNTKScalingRotaryEmbedding` is deprecated an will be removed in v4.46. Please use \"\n+            \"`PhiRotaryEmbedding`, which now also does dynamic ntk scaling (simply pass the model config to \"\n+            \"__init__).\"\n+        )\n+        kwargs[\"rope_type\"] = \"dynamic\"\n+        super().__init__(*args, **kwargs)\n \n \n # Copied from transformers.models.llama.modeling_llama.rotate_half\n@@ -204,18 +236,17 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n-# Copied from transformers.models.mixtral.modeling_mixtral.apply_rotary_pos_emb\n-def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n+# Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n     Args:\n         q (`torch.Tensor`): The query tensor.\n         k (`torch.Tensor`): The key tensor.\n         cos (`torch.Tensor`): The cosine part of the rotary embedding.\n         sin (`torch.Tensor`): The sine part of the rotary embedding.\n-        position_ids (`torch.Tensor`):\n-            The position indices of the tokens corresponding to the query and key tensors. For example, this can be\n-            used to pass offsetted position ids when working with a KV-cache.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n         unsqueeze_dim (`int`, *optional*, defaults to 1):\n             The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n             sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n@@ -226,8 +257,8 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n     Returns:\n         `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n     \"\"\"\n-    cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n-    sin = sin[position_ids].unsqueeze(unsqueeze_dim)\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n     q_embed = (q * cos) + (rotate_half(q) * sin)\n     k_embed = (k * cos) + (rotate_half(k) * sin)\n     return q_embed, k_embed\n@@ -282,9 +313,8 @@ def __init__(self, config: PhiConfig, layer_idx: Optional[int] = None):\n         self.head_dim = self.hidden_size // self.num_heads\n         self.num_key_value_heads = config.num_key_value_heads\n         self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n-        self.max_position_embeddings = config.max_position_embeddings\n         self.rope_theta = config.rope_theta\n-        self.partial_rotary_factor = config.partial_rotary_factor\n+        self.rotary_ndims = int(self.head_dim * config.partial_rotary_factor)\n         self.is_causal = True\n \n         if (self.head_dim * self.num_heads) != self.hidden_size:\n@@ -307,34 +337,7 @@ def __init__(self, config: PhiConfig, layer_idx: Optional[int] = None):\n                 config.hidden_size // self.num_heads, eps=config.layer_norm_eps, elementwise_affine=True\n             )\n \n-        self._init_rope()\n-\n-    def _init_rope(self):\n-        if self.config.rope_scaling is None:\n-            self.rotary_emb = PhiRotaryEmbedding(\n-                int(self.partial_rotary_factor * self.head_dim),\n-                max_position_embeddings=self.max_position_embeddings,\n-                base=self.rope_theta,\n-            )\n-        else:\n-            scaling_type = self.config.rope_scaling[\"type\"]\n-            scaling_factor = self.config.rope_scaling[\"factor\"]\n-            if scaling_type == \"linear\":\n-                self.rotary_emb = PhiLinearScalingRotaryEmbedding(\n-                    int(self.partial_rotary_factor * self.head_dim),\n-                    max_position_embeddings=self.max_position_embeddings,\n-                    scaling_factor=scaling_factor,\n-                    base=self.rope_theta,\n-                )\n-            elif scaling_type == \"dynamic\":\n-                self.rotary_emb = PhiDynamicNTKScalingRotaryEmbedding(\n-                    int(self.partial_rotary_factor * self.head_dim),\n-                    max_position_embeddings=self.max_position_embeddings,\n-                    scaling_factor=scaling_factor,\n-                    base=self.rope_theta,\n-                )\n-            else:\n-                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n+        self.rotary_emb = PhiRotaryEmbedding(config=self.config)\n \n     def forward(\n         self,\n@@ -345,6 +348,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n \n@@ -360,28 +364,28 @@ def forward(\n         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            if self.layer_idx is None:\n-                raise ValueError(\n-                    f\"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} \"\n-                    \"for auto-regressive decoding with k/v caching, please make sure to initialize the attention class \"\n-                    \"with a layer index.\"\n-                )\n-            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n-        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n \n         # Partial rotary embedding\n         query_rot, query_pass = (\n-            query_states[..., : self.rotary_emb.dim],\n-            query_states[..., self.rotary_emb.dim :],\n+            query_states[..., : self.rotary_ndims],\n+            query_states[..., self.rotary_ndims :],\n         )\n         key_rot, key_pass = (\n-            key_states[..., : self.rotary_emb.dim],\n-            key_states[..., self.rotary_emb.dim :],\n+            key_states[..., : self.rotary_ndims],\n+            key_states[..., self.rotary_ndims :],\n         )\n         # [batch_size, seq_length, num_heads, head_dim // config.partial_rotary_factor]\n-        query_rot, key_rot = apply_rotary_pos_emb(query_rot, key_rot, cos, sin, position_ids)\n+        query_rot, key_rot = apply_rotary_pos_emb(query_rot, key_rot, cos, sin)\n \n         # [batch_size, seq_length, num_heads, head_dim]\n         query_states = torch.cat((query_rot, query_pass), dim=-1)\n@@ -391,7 +395,7 @@ def forward(\n             cache_kwargs = {\n                 \"sin\": sin,\n                 \"cos\": cos,\n-                \"partial_rotation_size\": self.rotary_emb.dim,\n+                \"partial_rotation_size\": self.rotary_ndims,\n                 \"cache_position\": cache_position,\n             }\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n@@ -404,12 +408,6 @@ def forward(\n             query_states.to(torch.float32), key_states.to(torch.float32).transpose(2, 3)\n         ) / math.sqrt(self.head_dim)\n \n-        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n         if attention_mask is not None:\n             causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n             attn_weights += causal_mask\n@@ -462,6 +460,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n         **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         # PhiFlashAttention2 attention does not support output_attentions\n@@ -485,22 +484,28 @@ def forward(\n         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n-        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n \n         # Partial rotary embedding\n         query_rot, query_pass = (\n-            query_states[..., : self.rotary_emb.dim],\n-            query_states[..., self.rotary_emb.dim :],\n+            query_states[..., : self.rotary_ndims],\n+            query_states[..., self.rotary_ndims :],\n         )\n         key_rot, key_pass = (\n-            key_states[..., : self.rotary_emb.dim],\n-            key_states[..., self.rotary_emb.dim :],\n+            key_states[..., : self.rotary_ndims],\n+            key_states[..., self.rotary_ndims :],\n         )\n         # [batch_size, seq_length, num_heads, head_dim // config.partial_rotary_factor]\n-        query_rot, key_rot = apply_rotary_pos_emb(query_rot, key_rot, cos, sin, position_ids)\n+        query_rot, key_rot = apply_rotary_pos_emb(query_rot, key_rot, cos, sin)\n \n         # [batch_size, seq_length, num_heads, head_dim]\n         query_states = torch.cat((query_rot, query_pass), dim=-1)\n@@ -510,7 +515,7 @@ def forward(\n             cache_kwargs = {\n                 \"sin\": sin,\n                 \"cos\": cos,\n-                \"partial_rotation_size\": self.rotary_emb.dim,\n+                \"partial_rotation_size\": self.rotary_ndims,\n                 \"cache_position\": cache_position,\n             }\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n@@ -591,6 +596,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if output_attentions:\n             # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n@@ -623,28 +629,28 @@ def forward(\n         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            if self.layer_idx is None:\n-                raise ValueError(\n-                    f\"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} \"\n-                    \"for auto-regressive decoding with k/v caching, please make sure to initialize the attention class \"\n-                    \"with a layer index.\"\n-                )\n-            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n-        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n \n         # Partial rotary embedding\n         query_rot, query_pass = (\n-            query_states[..., : self.rotary_emb.dim],\n-            query_states[..., self.rotary_emb.dim :],\n+            query_states[..., : self.rotary_ndims],\n+            query_states[..., self.rotary_ndims :],\n         )\n         key_rot, key_pass = (\n-            key_states[..., : self.rotary_emb.dim],\n-            key_states[..., self.rotary_emb.dim :],\n+            key_states[..., : self.rotary_ndims],\n+            key_states[..., self.rotary_ndims :],\n         )\n         # [batch_size, seq_length, num_heads, head_dim // config.partial_rotary_factor]\n-        query_rot, key_rot = apply_rotary_pos_emb(query_rot, key_rot, cos, sin, position_ids)\n+        query_rot, key_rot = apply_rotary_pos_emb(query_rot, key_rot, cos, sin)\n \n         # [batch_size, seq_length, num_heads, head_dim]\n         query_states = torch.cat((query_rot, query_pass), dim=-1)\n@@ -654,7 +660,7 @@ def forward(\n             cache_kwargs = {\n                 \"sin\": sin,\n                 \"cos\": cos,\n-                \"partial_rotation_size\": self.rotary_emb.dim,\n+                \"partial_rotation_size\": self.rotary_ndims,\n                 \"cache_position\": cache_position,\n             }\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n@@ -719,6 +725,7 @@ def forward(\n         use_cache: Optional[bool] = False,\n         past_key_value: Optional[Tuple[torch.Tensor]] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n         **kwargs,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n@@ -739,6 +746,9 @@ def forward(\n             past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n+            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n+                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n+                with `head_dim` being the embedding dimension of each attention head.\n             kwargs (`dict`, *optional*):\n                 Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n                 into the model\n@@ -757,6 +767,7 @@ def forward(\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n         )\n         attn_outputs = self.resid_dropout(attn_outputs)\n \n@@ -803,6 +814,8 @@ class PhiPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n+    _supports_static_cache = True\n+    _supports_quantized_cache = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -914,6 +927,7 @@ def __init__(self, config: PhiConfig):\n             [PhiDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n         self.final_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.rotary_emb = PhiRotaryEmbedding(config=config)\n \n         self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n         self._use_sdpa = config._attn_implementation == \"sdpa\"\n@@ -989,6 +1003,9 @@ def forward(\n         inputs_embeds = self.embed_dropout(inputs_embeds)\n         hidden_states = inputs_embeds\n \n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n@@ -1008,6 +1025,7 @@ def forward(\n                     use_cache,\n                     past_key_values,\n                     cache_position,\n+                    position_embeddings,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n@@ -1018,6 +1036,7 @@ def forward(\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n+                    position_embeddings=position_embeddings,\n                 )\n \n             hidden_states = layer_outputs[0]"
        },
        {
            "sha": "20ebfb0e28572606c2a191c6c00ee7f0ba51c416",
            "filename": "src/transformers/models/qwen2/configuration_qwen2.py",
            "status": "modified",
            "additions": 45,
            "deletions": 0,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fqwen2%2Fconfiguration_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fqwen2%2Fconfiguration_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fconfiguration_qwen2.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -15,6 +15,7 @@\n \"\"\"Qwen2 model configuration\"\"\"\n \n from ...configuration_utils import PretrainedConfig\n+from ...modeling_rope_utils import rope_config_validation\n from ...utils import logging\n \n \n@@ -66,6 +67,43 @@ class Qwen2Config(PretrainedConfig):\n             Whether the model's input and output word embeddings should be tied.\n         rope_theta (`float`, *optional*, defaults to 10000.0):\n             The base period of the RoPE embeddings.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n         use_sliding_window (`bool`, *optional*, defaults to `False`):\n             Whether to use sliding window attention.\n         sliding_window (`int`, *optional*, defaults to 4096):\n@@ -106,6 +144,7 @@ def __init__(\n         use_cache=True,\n         tie_word_embeddings=False,\n         rope_theta=10000.0,\n+        rope_scaling=None,\n         use_sliding_window=False,\n         sliding_window=4096,\n         max_window_layers=28,\n@@ -132,7 +171,13 @@ def __init__(\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n         self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling\n         self.attention_dropout = attention_dropout\n+        # Validate the correctness of rotary position embeddings parameters\n+        # BC: if there is a 'type' field, move it to 'rope_type'.\n+        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n+            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_config_validation(self)\n \n         super().__init__(\n             tie_word_embeddings=tie_word_embeddings,"
        },
        {
            "sha": "030c74b034b7942b1a39120a1e80b77c5ad220ab",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 135,
            "deletions": 82,
            "changes": 217,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -36,6 +36,7 @@\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n@@ -135,41 +136,92 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n-# Copied from transformers.models.mixtral.modeling_mixtral.MixtralRotaryEmbedding with Mixtral->Qwen2\n+# Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Qwen2\n class Qwen2RotaryEmbedding(nn.Module):\n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n+    def __init__(\n+        self,\n+        dim=None,\n+        max_position_embeddings=2048,\n+        base=10000,\n+        device=None,\n+        scaling_factor=1.0,\n+        rope_type=\"default\",\n+        config: Optional[Qwen2Config] = None,\n+    ):\n         super().__init__()\n+        # TODO (joao): remove the `if` below, only used for BC\n+        self.rope_kwargs = {}\n+        if config is None:\n+            logger.warning_once(\n+                \"`Qwen2RotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n+                \"`config` argument. All other arguments will be removed in v4.46\"\n+            )\n+            self.rope_kwargs = {\n+                \"rope_type\": rope_type,\n+                \"factor\": scaling_factor,\n+                \"dim\": dim,\n+                \"base\": base,\n+                \"max_position_embeddings\": max_position_embeddings,\n+            }\n+            self.rope_type = rope_type\n+            self.max_seq_len_cached = max_position_embeddings\n+            self.original_max_seq_len = max_position_embeddings\n+        else:\n+            # BC: \"rope_type\" was originally \"type\"\n+            if config.rope_scaling is not None:\n+                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+            else:\n+                self.rope_type = \"default\"\n+            self.max_seq_len_cached = config.max_position_embeddings\n+            self.original_max_seq_len = config.max_position_embeddings\n \n-        self.dim = dim\n-        self.max_position_embeddings = max_position_embeddings\n-        self.base = base\n-        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n \n-        # Build here to make `torch.jit.trace` work.\n-        self._set_cos_sin_cache(\n-            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n-        )\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(\n+                self.config, device, seq_len=seq_len, **self.rope_kwargs\n+            )\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n \n-    def _set_cos_sin_cache(self, seq_len, device, dtype):\n-        self.max_seq_len_cached = seq_len\n-        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n \n-        freqs = torch.outer(t, self.inv_freq)\n-        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n-        emb = torch.cat((freqs, freqs), dim=-1)\n-        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n-        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n+    @torch.no_grad()\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n \n-    def forward(self, x, seq_len=None):\n-        # x: [bs, num_attention_heads, seq_len, head_size]\n-        if seq_len > self.max_seq_len_cached:\n-            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n+        # Core RoPE block\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n+        device_type = x.device.type\n+        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos()\n+            sin = emb.sin()\n \n-        return (\n-            self.cos_cached[:seq_len].to(dtype=x.dtype),\n-            self.sin_cached[:seq_len].to(dtype=x.dtype),\n-        )\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n # Copied from transformers.models.llama.modeling_llama.rotate_half\n@@ -180,18 +232,17 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n-# Copied from transformers.models.mixtral.modeling_mixtral.apply_rotary_pos_emb\n-def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n+# Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n     Args:\n         q (`torch.Tensor`): The query tensor.\n         k (`torch.Tensor`): The key tensor.\n         cos (`torch.Tensor`): The cosine part of the rotary embedding.\n         sin (`torch.Tensor`): The sine part of the rotary embedding.\n-        position_ids (`torch.Tensor`):\n-            The position indices of the tokens corresponding to the query and key tensors. For example, this can be\n-            used to pass offsetted position ids when working with a KV-cache.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n         unsqueeze_dim (`int`, *optional*, defaults to 1):\n             The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n             sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n@@ -202,8 +253,8 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n     Returns:\n         `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n     \"\"\"\n-    cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n-    sin = sin[position_ids].unsqueeze(unsqueeze_dim)\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n     q_embed = (q * cos) + (rotate_half(q) * sin)\n     k_embed = (k * cos) + (rotate_half(k) * sin)\n     return q_embed, k_embed\n@@ -259,7 +310,6 @@ def __init__(self, config: Qwen2Config, layer_idx: Optional[int] = None):\n         self.head_dim = self.hidden_size // self.num_heads\n         self.num_key_value_heads = config.num_key_value_heads\n         self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n-        self.max_position_embeddings = config.max_position_embeddings\n         self.rope_theta = config.rope_theta\n         self.is_causal = True\n         self.attention_dropout = config.attention_dropout\n@@ -274,11 +324,7 @@ def __init__(self, config: Qwen2Config, layer_idx: Optional[int] = None):\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n         self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n \n-        self.rotary_emb = Qwen2RotaryEmbedding(\n-            self.head_dim,\n-            max_position_embeddings=self.max_position_embeddings,\n-            base=self.rope_theta,\n-        )\n+        self.rotary_emb = Qwen2RotaryEmbedding(config=self.config)\n \n     def forward(\n         self,\n@@ -289,6 +335,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n \n@@ -300,17 +347,17 @@ def forward(\n         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            if self.layer_idx is None:\n-                raise ValueError(\n-                    f\"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} \"\n-                    \"for auto-regressive decoding with k/v caching, please make sure to initialize the attention class \"\n-                    \"with a layer index.\"\n-                )\n-            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n-        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n@@ -321,13 +368,6 @@ def forward(\n         value_states = repeat_kv(value_states, self.num_key_value_groups)\n \n         attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n-\n-        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n         if attention_mask is not None:  # no matter the length, we just slice it\n             causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n             attn_weights = attn_weights + causal_mask\n@@ -381,6 +421,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n     ):\n         bsz, q_len, _ = hidden_states.size()\n \n@@ -392,28 +433,22 @@ def forward(\n         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            if self.layer_idx is None:\n-                raise ValueError(\n-                    f\"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} \"\n-                    \"for auto-regressive decoding with k/v caching, please make sure to initialize the attention class \"\n-                    \"with a layer index.\"\n-                )\n-            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n-\n-        # Because the input can be padded, the absolute sequence length depends on the max position id.\n-        rotary_seq_len = (\n-            max(kv_seq_len, position_ids[:, -1].max().item() + 1) if position_ids is not None else kv_seq_len\n-        )\n-\n-        cos, sin = self.rotary_emb(value_states, seq_len=rotary_seq_len)\n-\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n             # Activate slicing cache only if the config has a value `sliding_windows` attribute\n             cache_has_contents = past_key_value.get_seq_length(self.layer_idx) > 0\n+            kv_seq_len = key_states.shape[-2] + cache_position[0]\n             if (\n                 getattr(self.config, \"sliding_window\", None) is not None\n                 and kv_seq_len > self.config.sliding_window\n@@ -504,7 +539,6 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-# Copied from transformers.models.mixtral.modeling_mixtral.MixtralSdpaAttention with Mixtral->Qwen2\n class Qwen2SdpaAttention(Qwen2Attention):\n     \"\"\"\n     Qwen2 attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n@@ -522,6 +556,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if output_attentions:\n             # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n@@ -548,12 +583,17 @@ def forward(\n         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n-        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n-\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n@@ -627,6 +667,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n         **kwargs,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n@@ -643,6 +684,9 @@ def forward(\n             past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n+            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n+                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n+                with `head_dim` being the embedding dimension of each attention head.\n             kwargs (`dict`, *optional*):\n                 Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n                 into the model\n@@ -661,6 +705,7 @@ def forward(\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n         )\n         hidden_states = residual + hidden_states\n \n@@ -711,6 +756,8 @@ class Qwen2PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n+    _supports_quantized_cache = True\n+    _supports_static_cache = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -822,6 +869,7 @@ def __init__(self, config: Qwen2Config):\n         )\n         self._attn_implementation = config._attn_implementation\n         self.norm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = Qwen2RotaryEmbedding(config=config)\n \n         self.gradient_checkpointing = False\n         # Initialize weights and apply final processing\n@@ -893,6 +941,9 @@ def forward(\n \n         hidden_states = inputs_embeds\n \n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n@@ -912,6 +963,7 @@ def forward(\n                     output_attentions,\n                     use_cache,\n                     cache_position,\n+                    position_embeddings,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n@@ -922,6 +974,7 @@ def forward(\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n+                    position_embeddings=position_embeddings,\n                 )\n \n             hidden_states = layer_outputs[0]"
        },
        {
            "sha": "a3179e4d33ea18f4049a41609f14ed0b9207d000",
            "filename": "src/transformers/models/qwen2_moe/configuration_qwen2_moe.py",
            "status": "modified",
            "additions": 45,
            "deletions": 0,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fconfiguration_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fconfiguration_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fconfiguration_qwen2_moe.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -15,6 +15,7 @@\n \"\"\"Qwen2MoE model configuration\"\"\"\n \n from ...configuration_utils import PretrainedConfig\n+from ...modeling_rope_utils import rope_config_validation\n from ...utils import logging\n \n \n@@ -66,6 +67,43 @@ class Qwen2MoeConfig(PretrainedConfig):\n             Whether the model's input and output word embeddings should be tied.\n         rope_theta (`float`, *optional*, defaults to 10000.0):\n             The base period of the RoPE embeddings.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n         use_sliding_window (`bool`, *optional*, defaults to `False`):\n             Whether to use sliding window attention.\n         sliding_window (`int`, *optional*, defaults to 4096):\n@@ -127,6 +165,7 @@ def __init__(\n         use_cache=True,\n         tie_word_embeddings=False,\n         rope_theta=10000.0,\n+        rope_scaling=None,\n         use_sliding_window=False,\n         sliding_window=4096,\n         max_window_layers=28,\n@@ -158,7 +197,13 @@ def __init__(\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n         self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling\n         self.attention_dropout = attention_dropout\n+        # Validate the correctness of rotary position embeddings parameters\n+        # BC: if there is a 'type' field, move it to 'rope_type'.\n+        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n+            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_config_validation(self)\n \n         # MoE arguments\n         self.decoder_sparse_step = decoder_sparse_step"
        },
        {
            "sha": "b196ed72a49b2329af87d001b8311a1eac75e3b6",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 135,
            "deletions": 80,
            "changes": 215,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -37,6 +37,7 @@\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n@@ -211,41 +212,92 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n-# Copied from transformers.models.mixtral.modeling_mixtral.MixtralRotaryEmbedding with Mixtral->Qwen2Moe\n+# Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Qwen2Moe\n class Qwen2MoeRotaryEmbedding(nn.Module):\n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n+    def __init__(\n+        self,\n+        dim=None,\n+        max_position_embeddings=2048,\n+        base=10000,\n+        device=None,\n+        scaling_factor=1.0,\n+        rope_type=\"default\",\n+        config: Optional[Qwen2MoeConfig] = None,\n+    ):\n         super().__init__()\n+        # TODO (joao): remove the `if` below, only used for BC\n+        self.rope_kwargs = {}\n+        if config is None:\n+            logger.warning_once(\n+                \"`Qwen2MoeRotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n+                \"`config` argument. All other arguments will be removed in v4.46\"\n+            )\n+            self.rope_kwargs = {\n+                \"rope_type\": rope_type,\n+                \"factor\": scaling_factor,\n+                \"dim\": dim,\n+                \"base\": base,\n+                \"max_position_embeddings\": max_position_embeddings,\n+            }\n+            self.rope_type = rope_type\n+            self.max_seq_len_cached = max_position_embeddings\n+            self.original_max_seq_len = max_position_embeddings\n+        else:\n+            # BC: \"rope_type\" was originally \"type\"\n+            if config.rope_scaling is not None:\n+                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+            else:\n+                self.rope_type = \"default\"\n+            self.max_seq_len_cached = config.max_position_embeddings\n+            self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        self.dim = dim\n-        self.max_position_embeddings = max_position_embeddings\n-        self.base = base\n-        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n \n-        # Build here to make `torch.jit.trace` work.\n-        self._set_cos_sin_cache(\n-            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n-        )\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(\n+                self.config, device, seq_len=seq_len, **self.rope_kwargs\n+            )\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n \n-    def _set_cos_sin_cache(self, seq_len, device, dtype):\n-        self.max_seq_len_cached = seq_len\n-        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n \n-        freqs = torch.outer(t, self.inv_freq)\n-        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n-        emb = torch.cat((freqs, freqs), dim=-1)\n-        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n-        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n+    @torch.no_grad()\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n \n-    def forward(self, x, seq_len=None):\n-        # x: [bs, num_attention_heads, seq_len, head_size]\n-        if seq_len > self.max_seq_len_cached:\n-            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n+        # Core RoPE block\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n+        device_type = x.device.type\n+        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos()\n+            sin = emb.sin()\n \n-        return (\n-            self.cos_cached[:seq_len].to(dtype=x.dtype),\n-            self.sin_cached[:seq_len].to(dtype=x.dtype),\n-        )\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n # Copied from transformers.models.llama.modeling_llama.rotate_half\n@@ -256,18 +308,17 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n-# Copied from transformers.models.mixtral.modeling_mixtral.apply_rotary_pos_emb\n-def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n+# Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n     Args:\n         q (`torch.Tensor`): The query tensor.\n         k (`torch.Tensor`): The key tensor.\n         cos (`torch.Tensor`): The cosine part of the rotary embedding.\n         sin (`torch.Tensor`): The sine part of the rotary embedding.\n-        position_ids (`torch.Tensor`):\n-            The position indices of the tokens corresponding to the query and key tensors. For example, this can be\n-            used to pass offsetted position ids when working with a KV-cache.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n         unsqueeze_dim (`int`, *optional*, defaults to 1):\n             The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n             sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n@@ -278,8 +329,8 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n     Returns:\n         `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n     \"\"\"\n-    cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n-    sin = sin[position_ids].unsqueeze(unsqueeze_dim)\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n     q_embed = (q * cos) + (rotate_half(q) * sin)\n     k_embed = (k * cos) + (rotate_half(k) * sin)\n     return q_embed, k_embed\n@@ -337,7 +388,6 @@ def __init__(self, config: Qwen2MoeConfig, layer_idx: Optional[int] = None):\n         self.head_dim = self.hidden_size // self.num_heads\n         self.num_key_value_heads = config.num_key_value_heads\n         self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n-        self.max_position_embeddings = config.max_position_embeddings\n         self.rope_theta = config.rope_theta\n         self.is_causal = True\n         self.attention_dropout = config.attention_dropout\n@@ -352,12 +402,9 @@ def __init__(self, config: Qwen2MoeConfig, layer_idx: Optional[int] = None):\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n         self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n \n-        self.rotary_emb = Qwen2MoeRotaryEmbedding(\n-            self.head_dim,\n-            max_position_embeddings=self.max_position_embeddings,\n-            base=self.rope_theta,\n-        )\n+        self.rotary_emb = Qwen2MoeRotaryEmbedding(config=self.config)\n \n+    # Ignore copy\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -367,6 +414,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n \n@@ -378,16 +426,17 @@ def forward(\n         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            if self.layer_idx is None:\n-                raise ValueError(\n-                    f\"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} \"\n-                    \"for auto-regressive decoding with k/v caching, please make sure to initialize the attention class \"\n-                    \"with a layer index.\"\n-                )\n-            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n-        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n+\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n \n         if past_key_value is not None:\n@@ -400,12 +449,6 @@ def forward(\n \n         attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n \n-        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n         if attention_mask is not None:  # no matter the length, we just slice it\n             causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n             attn_weights = attn_weights + causal_mask\n@@ -460,6 +503,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n     ):\n         bsz, q_len, _ = hidden_states.size()\n \n@@ -471,28 +515,22 @@ def forward(\n         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            if self.layer_idx is None:\n-                raise ValueError(\n-                    f\"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} \"\n-                    \"for auto-regressive decoding with k/v caching, please make sure to initialize the attention class \"\n-                    \"with a layer index.\"\n-                )\n-            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n-\n-        # Because the input can be padded, the absolute sequence length depends on the max position id.\n-        rotary_seq_len = (\n-            max(kv_seq_len, position_ids[:, -1].max().item() + 1) if position_ids is not None else kv_seq_len\n-        )\n-\n-        cos, sin = self.rotary_emb(value_states, seq_len=rotary_seq_len)\n-\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n             # Activate slicing cache only if the config has a value `sliding_windows` attribute\n             cache_has_contents = past_key_value.get_seq_length(self.layer_idx) > 0\n+            kv_seq_len = key_states.shape[-2] + cache_position[0]\n             if (\n                 getattr(self.config, \"sliding_window\", None) is not None\n                 and kv_seq_len > self.config.sliding_window\n@@ -583,7 +621,7 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-# Copied from transformers.models.mixtral.modeling_mixtral.MixtralSdpaAttention with Mixtral->Qwen2Moe\n+# Copied from transformers.models.qwen2.modeling_qwen2.Qwen2SdpaAttention with Qwen2->Qwen2Moe\n class Qwen2MoeSdpaAttention(Qwen2MoeAttention):\n     \"\"\"\n     Qwen2Moe attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n@@ -601,6 +639,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if output_attentions:\n             # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n@@ -627,12 +666,17 @@ def forward(\n         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n-        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n-\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n@@ -770,6 +814,7 @@ def forward(\n         output_router_logits: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n         **kwargs,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n@@ -789,6 +834,9 @@ def forward(\n             past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n+            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n+                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n+                with `head_dim` being the embedding dimension of each attention head.\n             kwargs (`dict`, *optional*):\n                 Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n                 into the model\n@@ -807,6 +855,7 @@ def forward(\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n         )\n         hidden_states = residual + hidden_states\n \n@@ -980,6 +1029,7 @@ def __init__(self, config: Qwen2MoeConfig):\n         )\n         self._attn_implementation = config._attn_implementation\n         self.norm = Qwen2MoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = Qwen2MoeRotaryEmbedding(config=config)\n \n         self.gradient_checkpointing = False\n         # Initialize weights and apply final processing\n@@ -1055,6 +1105,9 @@ def forward(\n \n         hidden_states = inputs_embeds\n \n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n@@ -1076,6 +1129,7 @@ def forward(\n                     output_router_logits,\n                     use_cache,\n                     cache_position,\n+                    position_embeddings,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n@@ -1087,6 +1141,7 @@ def forward(\n                     output_router_logits=output_router_logits,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n+                    position_embeddings=position_embeddings,\n                 )\n \n             hidden_states = layer_outputs[0]"
        },
        {
            "sha": "27615eb789f0b0d9d59381dca4262e022e173407",
            "filename": "src/transformers/models/qwen2_vl/configuration_qwen2_vl.py",
            "status": "modified",
            "additions": 46,
            "deletions": 7,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -18,6 +18,7 @@\n from typing import Union\n \n from ...configuration_utils import PretrainedConfig\n+from ...modeling_rope_utils import rope_config_validation\n from ...utils import logging\n \n \n@@ -128,13 +129,42 @@ class Qwen2VLConfig(PretrainedConfig):\n         vision_config (`Dict`, *optional*):\n             The config for the visual encoder initialization.\n         rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n-            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n-            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n-            `max_position_embeddings` to the expected new maximum. See the following thread for more information on how\n-            these scaling strategies behave:\n-            https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/. This is an\n-            experimental feature, subject to breaking API changes in future versions.\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n \n     ```python\n     >>> from transformers import Qwen2VLForConditionalGeneration, Qwen2VLConfig\n@@ -203,4 +233,13 @@ def __init__(\n         self.attention_dropout = attention_dropout\n         self.rope_scaling = rope_scaling\n \n+        # Validate the correctness of rotary position embeddings parameters\n+        # BC: if there is a 'type' field, move it to 'rope_type'.\n+        # and change type from 'mrope' to 'default'\n+        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n+            if self.rope_scaling[\"type\"] == \"mrope\":\n+                self.rope_scaling[\"type\"] = \"default\"\n+            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_config_validation(self)\n+\n         super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)"
        },
        {
            "sha": "e225537a3673040cbdaabec14233f0afc24d2fb9",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 134,
            "deletions": 52,
            "changes": 186,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -38,6 +38,7 @@\n     BaseModelOutputWithPast,\n     ModelOutput,\n )\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n@@ -102,41 +103,92 @@ class Qwen2VLCausalLMOutputWithPast(ModelOutput):\n     rope_deltas: Optional[torch.LongTensor] = None\n \n \n-# Copied from transformers.models.qwen2.modeling_qwen2.Qwen2RotaryEmbedding\n-class Qwen2RotaryEmbedding(nn.Module):\n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n+class Qwen2VLRotaryEmbedding(nn.Module):\n+    def __init__(\n+        self,\n+        dim=None,\n+        max_position_embeddings=2048,\n+        base=10000,\n+        device=None,\n+        scaling_factor=1.0,\n+        rope_type=\"default\",\n+        config: Optional[Qwen2VLConfig] = None,\n+    ):\n         super().__init__()\n+        # TODO (joao): remove the `if` below, only used for BC\n+        self.rope_kwargs = {}\n+        if config is None:\n+            logger.warning_once(\n+                \"`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n+                \"`config` argument. All other arguments will be removed in v4.46\"\n+            )\n+            self.rope_kwargs = {\n+                \"rope_type\": rope_type,\n+                \"factor\": scaling_factor,\n+                \"dim\": dim,\n+                \"base\": base,\n+                \"max_position_embeddings\": max_position_embeddings,\n+            }\n+            self.rope_type = rope_type\n+            self.max_seq_len_cached = max_position_embeddings\n+            self.original_max_seq_len = max_position_embeddings\n+        else:\n+            # BC: \"rope_type\" was originally \"type\"\n+            if config.rope_scaling is not None:\n+                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+            else:\n+                self.rope_type = \"default\"\n+            self.max_seq_len_cached = config.max_position_embeddings\n+            self.original_max_seq_len = config.max_position_embeddings\n \n-        self.dim = dim\n-        self.max_position_embeddings = max_position_embeddings\n-        self.base = base\n-        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-\n-        # Build here to make `torch.jit.trace` work.\n-        self._set_cos_sin_cache(\n-            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n-        )\n-\n-    def _set_cos_sin_cache(self, seq_len, device, dtype):\n-        self.max_seq_len_cached = seq_len\n-        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n-\n-        freqs = torch.outer(t, self.inv_freq)\n-        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n-        emb = torch.cat((freqs, freqs), dim=-1)\n-        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n-        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-    def forward(self, x, seq_len=None):\n-        # x: [bs, num_attention_heads, seq_len, head_size]\n-        if seq_len > self.max_seq_len_cached:\n-            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n \n-        return (\n-            self.cos_cached[:seq_len].to(dtype=x.dtype),\n-            self.sin_cached[:seq_len].to(dtype=x.dtype),\n-        )\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(\n+                self.config, device, seq_len=seq_len, **self.rope_kwargs\n+            )\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n+\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n+\n+    @torch.no_grad()\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n+\n+        # Core RoPE block. In contrast to other models, Qwen2_VL has different position ids for thw grids\n+        # So we expand the inv_freq to shape (3, ...)\n+        inv_freq_expanded = self.inv_freq[None, None, :, None].float().expand(3, position_ids.shape[1], -1, 1)\n+        position_ids_expanded = position_ids[:, :, None, :].float()  # shape (3, bs, 1, positions)\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n+        device_type = x.device.type\n+        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(2, 3)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos()\n+            sin = emb.sin()\n+\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n # Copied from transformers.models.llama.modeling_llama.rotate_half\n@@ -147,7 +199,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n-def apply_multimodal_rotary_pos_emb(q, k, cos, sin, position_ids, mrope_section, unsqueeze_dim=1):\n+def apply_multimodal_rotary_pos_emb(q, k, cos, sin, mrope_section, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding with Multimodal Sections to the query and key tensors (https://qwenlm.github.io/blog/qwen2-vl/).\n \n     Explanation:\n@@ -179,8 +231,6 @@ def apply_multimodal_rotary_pos_emb(q, k, cos, sin, position_ids, mrope_section,\n     Returns:\n         `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n     \"\"\"\n-    cos = cos[position_ids]\n-    sin = sin[position_ids]\n     mrope_section = mrope_section * 2\n     cos = torch.cat([m[i % 3] for i, m in enumerate(cos.split(mrope_section, dim=-1))], dim=-1).unsqueeze(\n         unsqueeze_dim\n@@ -525,7 +575,7 @@ def __init__(self, config: Qwen2VLConfig, layer_idx: Optional[int] = None):\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n         self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n \n-        self.rotary_emb = Qwen2RotaryEmbedding(\n+        self.rotary_emb = Qwen2VLRotaryEmbedding(\n             self.head_dim,\n             max_position_embeddings=self.max_position_embeddings,\n             base=self.rope_theta,\n@@ -540,6 +590,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n \n@@ -553,16 +604,20 @@ def forward(\n \n         kv_seq_len = key_states.shape[-2]\n         if past_key_value is not None:\n-            if self.layer_idx is None:\n-                raise ValueError(\n-                    f\"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} \"\n-                    \"for auto-regressive decoding with k/v caching, please make sure to initialize the attention class \"\n-                    \"with a layer index.\"\n-                )\n-            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n-        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n+            kv_seq_len += cache_position[0] + 1\n+\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n         query_states, key_states = apply_multimodal_rotary_pos_emb(\n-            query_states, key_states, cos, sin, position_ids, self.rope_scaling[\"mrope_section\"]\n+            query_states, key_states, cos, sin, self.rope_scaling[\"mrope_section\"]\n         )\n \n         if past_key_value is not None:\n@@ -627,6 +682,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n     ):\n         bsz, q_len, _ = hidden_states.size()\n \n@@ -649,14 +705,19 @@ def forward(\n             kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n \n         # Because the input can be padded, the absolute sequence length depends on the max position id.\n-        rotary_seq_len = (\n-            max(kv_seq_len, position_ids[:, -1].max().item() + 1) if position_ids is not None else kv_seq_len\n-        )\n-\n-        cos, sin = self.rotary_emb(value_states, seq_len=rotary_seq_len)\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n \n         query_states, key_states = apply_multimodal_rotary_pos_emb(\n-            query_states, key_states, cos, sin, position_ids, self.rope_scaling[\"mrope_section\"]\n+            query_states, key_states, cos, sin, self.rope_scaling[\"mrope_section\"]\n         )\n \n         if past_key_value is not None:\n@@ -768,6 +829,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if output_attentions:\n             # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n@@ -797,9 +859,18 @@ def forward(\n         kv_seq_len = key_states.shape[-2]\n         if past_key_value is not None:\n             kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n-        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n         query_states, key_states = apply_multimodal_rotary_pos_emb(\n-            query_states, key_states, cos, sin, position_ids, self.rope_scaling[\"mrope_section\"]\n+            query_states, key_states, cos, sin, self.rope_scaling[\"mrope_section\"]\n         )\n \n         if past_key_value is not None:\n@@ -874,6 +945,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n         **kwargs,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n@@ -890,6 +962,9 @@ def forward(\n             past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n+            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n+                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n+                with `head_dim` being the embedding dimension of each attention head.\n             kwargs (`dict`, *optional*):\n                 Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n                 into the model\n@@ -908,6 +983,7 @@ def forward(\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n         )\n         hidden_states = residual + hidden_states\n \n@@ -1061,6 +1137,7 @@ def __init__(self, config: Qwen2VLConfig):\n         )\n         self._attn_implementation = config._attn_implementation\n         self.norm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = Qwen2VLRotaryEmbedding(config=config)\n \n         self.gradient_checkpointing = False\n         # Initialize weights and apply final processing\n@@ -1123,6 +1200,9 @@ def forward(\n \n         hidden_states = inputs_embeds\n \n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n@@ -1142,6 +1222,7 @@ def forward(\n                     output_attentions,\n                     use_cache,\n                     cache_position,\n+                    position_embeddings,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n@@ -1152,6 +1233,7 @@ def forward(\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n+                    position_embeddings=position_embeddings,\n                 )\n \n             hidden_states = layer_outputs[0]"
        },
        {
            "sha": "a64c7e701d0373816eaf4490d7cd420eeaa820ec",
            "filename": "src/transformers/models/stablelm/configuration_stablelm.py",
            "status": "modified",
            "additions": 42,
            "deletions": 28,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fstablelm%2Fconfiguration_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fstablelm%2Fconfiguration_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fconfiguration_stablelm.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -15,6 +15,7 @@\n \"\"\"StableLM model configuration\"\"\"\n \n from ...configuration_utils import PretrainedConfig\n+from ...modeling_rope_utils import rope_config_validation\n from ...utils import logging\n \n \n@@ -71,13 +72,42 @@ class StableLmConfig(PretrainedConfig):\n         rope_theta (`float`, *optional*, defaults to `10000.0`):\n             The base period of the RoPE embeddings.\n         rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n-            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n-            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n-            `max_position_embeddings` to the expected new maximum. See the following thread for more information on how\n-            these scaling strategies behave:\n-            https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/. This\n-            is an experimental feature, subject to breaking API changes in future versions.\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n         use_qkv_bias (`bool`, *optional*, defaults to `False`):\n             Whether or not the model should use bias for qkv layers.\n         qk_layernorm (`bool`, *optional*, defaults to `False`):\n@@ -155,31 +185,15 @@ def __init__(\n         self.hidden_dropout = hidden_dropout\n         self.attention_dropout = attention_dropout\n         self.partial_rotary_factor = partial_rotary_factor\n-        self._rope_scaling_validation()\n+        # Validate the correctness of rotary position embeddings parameters\n+        # BC: if there is a 'type' field, move it to 'rope_type'.\n+        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n+            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_config_validation(self)\n \n         super().__init__(\n             bos_token_id=bos_token_id,\n             eos_token_id=eos_token_id,\n             tie_word_embeddings=tie_word_embeddings,\n             **kwargs,\n         )\n-\n-    def _rope_scaling_validation(self):\n-        \"\"\"\n-        Validate the `rope_scaling` configuration.\n-        \"\"\"\n-        if self.rope_scaling is None:\n-            return\n-\n-        if not isinstance(self.rope_scaling, dict) or len(self.rope_scaling) != 2:\n-            raise ValueError(\n-                \"`rope_scaling` must be a dictionary with two fields, `type` and `factor`, \" f\"got {self.rope_scaling}\"\n-            )\n-        rope_scaling_type = self.rope_scaling.get(\"type\", None)\n-        rope_scaling_factor = self.rope_scaling.get(\"factor\", None)\n-        if rope_scaling_type is None or rope_scaling_type not in [\"linear\", \"dynamic\"]:\n-            raise ValueError(\n-                f\"`rope_scaling`'s type field must be one of ['linear', 'dynamic'], got {rope_scaling_type}\"\n-            )\n-        if rope_scaling_factor is None or not isinstance(rope_scaling_factor, float) or rope_scaling_factor <= 1.0:\n-            raise ValueError(f\"`rope_scaling`'s factor field must be a float > 1, got {rope_scaling_factor}\")"
        },
        {
            "sha": "27d0c856a61bd65c3e621997568b47ccfe9d79d4",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 173,
            "deletions": 162,
            "changes": 335,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -36,6 +36,7 @@\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n@@ -111,88 +112,119 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n     return causal_mask\n \n \n-# Copied from transformers.models.mixtral.modeling_mixtral.MixtralRotaryEmbedding with Mixtral->StableLm\n+# Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->StableLm\n class StableLmRotaryEmbedding(nn.Module):\n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n+    def __init__(\n+        self,\n+        dim=None,\n+        max_position_embeddings=2048,\n+        base=10000,\n+        device=None,\n+        scaling_factor=1.0,\n+        rope_type=\"default\",\n+        config: Optional[StableLmConfig] = None,\n+    ):\n         super().__init__()\n+        # TODO (joao): remove the `if` below, only used for BC\n+        self.rope_kwargs = {}\n+        if config is None:\n+            logger.warning_once(\n+                \"`StableLmRotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n+                \"`config` argument. All other arguments will be removed in v4.46\"\n+            )\n+            self.rope_kwargs = {\n+                \"rope_type\": rope_type,\n+                \"factor\": scaling_factor,\n+                \"dim\": dim,\n+                \"base\": base,\n+                \"max_position_embeddings\": max_position_embeddings,\n+            }\n+            self.rope_type = rope_type\n+            self.max_seq_len_cached = max_position_embeddings\n+            self.original_max_seq_len = max_position_embeddings\n+        else:\n+            # BC: \"rope_type\" was originally \"type\"\n+            if config.rope_scaling is not None:\n+                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+            else:\n+                self.rope_type = \"default\"\n+            self.max_seq_len_cached = config.max_position_embeddings\n+            self.original_max_seq_len = config.max_position_embeddings\n \n-        self.dim = dim\n-        self.max_position_embeddings = max_position_embeddings\n-        self.base = base\n-        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-\n-        # Build here to make `torch.jit.trace` work.\n-        self._set_cos_sin_cache(\n-            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n-        )\n-\n-    def _set_cos_sin_cache(self, seq_len, device, dtype):\n-        self.max_seq_len_cached = seq_len\n-        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n-\n-        freqs = torch.outer(t, self.inv_freq)\n-        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n-        emb = torch.cat((freqs, freqs), dim=-1)\n-        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n-        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n-\n-    def forward(self, x, seq_len=None):\n-        # x: [bs, num_attention_heads, seq_len, head_size]\n-        if seq_len > self.max_seq_len_cached:\n-            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n-\n-        return (\n-            self.cos_cached[:seq_len].to(dtype=x.dtype),\n-            self.sin_cached[:seq_len].to(dtype=x.dtype),\n-        )\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n \n-# Copied from transformers.models.falcon.modeling_falcon.FalconLinearScalingRotaryEmbedding with Falcon->StableLm\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(\n+                self.config, device, seq_len=seq_len, **self.rope_kwargs\n+            )\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n+\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n+\n+    @torch.no_grad()\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n+\n+        # Core RoPE block\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n+        device_type = x.device.type\n+        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos()\n+            sin = emb.sin()\n+\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+# Copied from transformers.models.llama.modeling_llama.LlamaLinearScalingRotaryEmbedding with Llama->StableLm\n class StableLmLinearScalingRotaryEmbedding(StableLmRotaryEmbedding):\n     \"\"\"StableLmRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n \n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n-        self.scaling_factor = scaling_factor\n-        super().__init__(dim, max_position_embeddings, base, device)\n-\n-    def _set_cos_sin_cache(self, seq_len, device, dtype):\n-        self.max_seq_len_cached = seq_len\n-        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n-        t = t / self.scaling_factor\n-\n-        freqs = torch.outer(t, self.inv_freq)\n-        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n-        emb = torch.cat((freqs, freqs), dim=-1)\n-        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n-        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n+    def __init__(self, *args, **kwargs):\n+        logger.warning_once(\n+            \"`StableLmLinearScalingRotaryEmbedding` is deprecated an will be removed in v4.46. Please use \"\n+            \"`StableLmRotaryEmbedding`, which now also does linear scaling (simply pass the model config to __init__).\"\n+        )\n+        kwargs[\"rope_type\"] = \"linear\"\n+        super().__init__(*args, **kwargs)\n \n \n-# Copied from transformers.models.falcon.modeling_falcon.FalconDynamicNTKScalingRotaryEmbedding with Falcon->StableLm\n+# Copied from transformers.models.llama.modeling_llama.LlamaDynamicNTKScalingRotaryEmbedding with Llama->StableLm\n class StableLmDynamicNTKScalingRotaryEmbedding(StableLmRotaryEmbedding):\n     \"\"\"StableLmRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n \n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n-        self.scaling_factor = scaling_factor\n-        super().__init__(dim, max_position_embeddings, base, device)\n-\n-    def _set_cos_sin_cache(self, seq_len, device, dtype):\n-        self.max_seq_len_cached = seq_len\n-\n-        if seq_len > self.max_position_embeddings:\n-            base = self.base * (\n-                (self.scaling_factor * seq_len / self.max_position_embeddings) - (self.scaling_factor - 1)\n-            ) ** (self.dim / (self.dim - 2))\n-            inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-\n-        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n-\n-        freqs = torch.outer(t, self.inv_freq)\n-        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n-        emb = torch.cat((freqs, freqs), dim=-1)\n-        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n-        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n+    def __init__(self, *args, **kwargs):\n+        logger.warning_once(\n+            \"`StableLmDynamicNTKScalingRotaryEmbedding` is deprecated an will be removed in v4.46. Please use \"\n+            \"`StableLmRotaryEmbedding`, which now also does dynamic ntk scaling (simply pass the model config to \"\n+            \"__init__).\"\n+        )\n+        kwargs[\"rope_type\"] = \"dynamic\"\n+        super().__init__(*args, **kwargs)\n \n \n # Copied from transformers.models.llama.modeling_llama.rotate_half\n@@ -203,18 +235,17 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n-# Copied from transformers.models.mixtral.modeling_mixtral.apply_rotary_pos_emb\n-def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n+# Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n     Args:\n         q (`torch.Tensor`): The query tensor.\n         k (`torch.Tensor`): The key tensor.\n         cos (`torch.Tensor`): The cosine part of the rotary embedding.\n         sin (`torch.Tensor`): The sine part of the rotary embedding.\n-        position_ids (`torch.Tensor`):\n-            The position indices of the tokens corresponding to the query and key tensors. For example, this can be\n-            used to pass offsetted position ids when working with a KV-cache.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n         unsqueeze_dim (`int`, *optional*, defaults to 1):\n             The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n             sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n@@ -225,8 +256,8 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n     Returns:\n         `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n     \"\"\"\n-    cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n-    sin = sin[position_ids].unsqueeze(unsqueeze_dim)\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n     q_embed = (q * cos) + (rotate_half(q) * sin)\n     k_embed = (k * cos) + (rotate_half(k) * sin)\n     return q_embed, k_embed\n@@ -294,9 +325,8 @@ def __init__(self, config: StableLmConfig, layer_idx: Optional[int] = None):\n         self.head_dim = self.hidden_size // self.num_heads\n         self.num_key_value_heads = config.num_key_value_heads\n         self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n-        self.max_position_embeddings = config.max_position_embeddings\n         self.rope_theta = config.rope_theta\n-        self.partial_rotary_factor = config.partial_rotary_factor\n+        self.rotary_ndims = int(self.head_dim * config.partial_rotary_factor)\n         self.is_causal = True\n \n         if (self.head_dim * self.num_heads) != self.hidden_size:\n@@ -317,35 +347,7 @@ def __init__(self, config: StableLmConfig, layer_idx: Optional[int] = None):\n             )\n \n         self.attention_dropout = nn.Dropout(config.attention_dropout)\n-        self._init_rope()\n-\n-    # Copied from transformers.models.persimmon.modeling_persimmon.PersimmonAttention._init_rope with Persimmon->StableLm\n-    def _init_rope(self):\n-        if self.config.rope_scaling is None:\n-            self.rotary_emb = StableLmRotaryEmbedding(\n-                int(self.partial_rotary_factor * self.head_dim),\n-                max_position_embeddings=self.max_position_embeddings,\n-                base=self.rope_theta,\n-            )\n-        else:\n-            scaling_type = self.config.rope_scaling[\"type\"]\n-            scaling_factor = self.config.rope_scaling[\"factor\"]\n-            if scaling_type == \"linear\":\n-                self.rotary_emb = StableLmLinearScalingRotaryEmbedding(\n-                    int(self.partial_rotary_factor * self.head_dim),\n-                    max_position_embeddings=self.max_position_embeddings,\n-                    scaling_factor=scaling_factor,\n-                    base=self.rope_theta,\n-                )\n-            elif scaling_type == \"dynamic\":\n-                self.rotary_emb = StableLmDynamicNTKScalingRotaryEmbedding(\n-                    int(self.partial_rotary_factor * self.head_dim),\n-                    max_position_embeddings=self.max_position_embeddings,\n-                    scaling_factor=scaling_factor,\n-                    base=self.rope_theta,\n-                )\n-            else:\n-                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n+        self.rotary_emb = StableLmRotaryEmbedding(config=self.config)\n \n     def forward(\n         self,\n@@ -356,6 +358,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n \n@@ -371,28 +374,28 @@ def forward(\n             query_states = self.q_layernorm(query_states)\n             key_states = self.k_layernorm(key_states)\n \n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            if self.layer_idx is None:\n-                raise ValueError(\n-                    f\"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} \"\n-                    \"for auto-regressive decoding with k/v caching, please make sure to initialize the attention class \"\n-                    \"with a layer index.\"\n-                )\n-            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n-        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n \n         # Partial rotary embedding\n         query_rot, query_pass = (\n-            query_states[..., : self.rotary_emb.dim],\n-            query_states[..., self.rotary_emb.dim :],\n+            query_states[..., : self.rotary_ndims],\n+            query_states[..., self.rotary_ndims :],\n         )\n         key_rot, key_pass = (\n-            key_states[..., : self.rotary_emb.dim],\n-            key_states[..., self.rotary_emb.dim :],\n+            key_states[..., : self.rotary_ndims],\n+            key_states[..., self.rotary_ndims :],\n         )\n         # [batch_size, seq_length, num_heads, head_dim // config.partial_rotary_factor]\n-        query_rot, key_rot = apply_rotary_pos_emb(query_rot, key_rot, cos, sin, position_ids)\n+        query_rot, key_rot = apply_rotary_pos_emb(query_rot, key_rot, cos, sin)\n \n         # [batch_size, seq_length, num_heads, head_dim]\n         query_states = torch.cat((query_rot, query_pass), dim=-1)\n@@ -403,7 +406,7 @@ def forward(\n             cache_kwargs = {\n                 \"sin\": sin,\n                 \"cos\": cos,\n-                \"partial_rotation_size\": self.rotary_emb.dim,\n+                \"partial_rotation_size\": self.rotary_ndims,\n                 \"cache_position\": cache_position,\n             }\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n@@ -414,12 +417,6 @@ def forward(\n \n         attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n \n-        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n         if attention_mask is not None:  # no matter the length, we just slice it\n             causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n             attn_weights += causal_mask\n@@ -457,6 +454,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if output_attentions:\n             # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n@@ -487,28 +485,28 @@ def forward(\n             query_states = self.q_layernorm(query_states)\n             key_states = self.k_layernorm(key_states)\n \n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            if self.layer_idx is None:\n-                raise ValueError(\n-                    f\"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} \"\n-                    \"for auto-regressive decoding with k/v caching, please make sure to initialize the attention class \"\n-                    \"with a layer index.\"\n-                )\n-            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n-        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n \n         # Partial rotary embedding\n         query_rot, query_pass = (\n-            query_states[..., : self.rotary_emb.dim],\n-            query_states[..., self.rotary_emb.dim :],\n+            query_states[..., : self.rotary_ndims],\n+            query_states[..., self.rotary_ndims :],\n         )\n         key_rot, key_pass = (\n-            key_states[..., : self.rotary_emb.dim],\n-            key_states[..., self.rotary_emb.dim :],\n+            key_states[..., : self.rotary_ndims],\n+            key_states[..., self.rotary_ndims :],\n         )\n         # [batch_size, seq_length, num_heads, head_dim // config.partial_rotary_factor]\n-        query_rot, key_rot = apply_rotary_pos_emb(query_rot, key_rot, cos, sin, position_ids)\n+        query_rot, key_rot = apply_rotary_pos_emb(query_rot, key_rot, cos, sin)\n \n         # [batch_size, seq_length, num_heads, head_dim]\n         query_states = torch.cat((query_rot, query_pass), dim=-1)\n@@ -519,7 +517,7 @@ def forward(\n             cache_kwargs = {\n                 \"sin\": sin,\n                 \"cos\": cos,\n-                \"partial_rotation_size\": self.rotary_emb.dim,\n+                \"partial_rotation_size\": self.rotary_ndims,\n                 \"cache_position\": cache_position,\n             }\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n@@ -586,6 +584,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n         **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         # StableLmFlashAttention2 attention does not support output_attentions\n@@ -609,27 +608,27 @@ def forward(\n             query_states = self.q_layernorm(query_states)\n             key_states = self.k_layernorm(key_states)\n \n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            if self.layer_idx is None:\n-                raise ValueError(\n-                    f\"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} \"\n-                    \"for auto-regressive decoding with k/v caching, please make sure to initialize the attention class \"\n-                    \"with a layer index.\"\n-                )\n-            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n-        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n \n         # Partial rotary embedding\n         query_rot, query_pass = (\n-            query_states[..., : self.rotary_emb.dim],\n-            query_states[..., self.rotary_emb.dim :],\n+            query_states[..., : self.rotary_ndims],\n+            query_states[..., self.rotary_ndims :],\n         )\n         key_rot, key_pass = (\n-            key_states[..., : self.rotary_emb.dim],\n-            key_states[..., self.rotary_emb.dim :],\n+            key_states[..., : self.rotary_ndims],\n+            key_states[..., self.rotary_ndims :],\n         )\n-        query_rot, key_rot = apply_rotary_pos_emb(query_rot, key_rot, cos, sin, position_ids)\n+        query_rot, key_rot = apply_rotary_pos_emb(query_rot, key_rot, cos, sin)\n \n         # [batch_size, seq_length, num_heads, head_dim]\n         query_states = torch.cat((query_rot, query_pass), dim=-1)\n@@ -639,7 +638,7 @@ def forward(\n             cache_kwargs = {\n                 \"sin\": sin,\n                 \"cos\": cos,\n-                \"partial_rotation_size\": self.rotary_emb.dim,\n+                \"partial_rotation_size\": self.rotary_ndims,\n                 \"cache_position\": cache_position,\n             }\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n@@ -702,6 +701,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n@@ -722,7 +722,10 @@ def forward(\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n+                Indices depicting the position of the input sequence tokens in the sequence\n+            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n+                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n+                with `head_dim` being the embedding dimension of each attention head.\n         \"\"\"\n \n         residual = hidden_states\n@@ -738,6 +741,7 @@ def forward(\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n         )\n \n         # copied from transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXLayer.forward\n@@ -798,6 +802,7 @@ class StableLmPreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n+    _supports_static_cache = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -908,6 +913,7 @@ def __init__(self, config: StableLmConfig):\n             [StableLmDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n         self.norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.rotary_emb = StableLmRotaryEmbedding(config=config)\n \n         self._attn_implementation = config._attn_implementation\n         self.gradient_checkpointing = False\n@@ -980,6 +986,9 @@ def forward(\n \n         hidden_states = inputs_embeds\n \n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n@@ -999,6 +1008,7 @@ def forward(\n                     output_attentions,\n                     use_cache,\n                     cache_position,\n+                    position_embeddings,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n@@ -1009,6 +1019,7 @@ def forward(\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n+                    position_embeddings=position_embeddings,\n                 )\n \n             hidden_states = layer_outputs[0]"
        },
        {
            "sha": "b5b1350b36d9e5b4a10f5cb873c090071093c2e6",
            "filename": "src/transformers/models/starcoder2/configuration_starcoder2.py",
            "status": "modified",
            "additions": 45,
            "deletions": 0,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fconfiguration_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fconfiguration_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fconfiguration_starcoder2.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -15,6 +15,7 @@\n \"\"\"Starcoder2 model configuration\"\"\"\n \n from ...configuration_utils import PretrainedConfig\n+from ...modeling_rope_utils import rope_config_validation\n from ...utils import logging\n \n \n@@ -69,6 +70,43 @@ class Starcoder2Config(PretrainedConfig):\n             The id of the \"end-of-sequence\" token.\n         rope_theta (`float`, *optional*, defaults to 10000.0):\n             The base period of the RoPE embeddings.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n         sliding_window (`int`, *optional*):\n             Sliding window attention window size. If not specified, will default to `None` (no sliding window).\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -113,6 +151,7 @@ def __init__(\n         bos_token_id=50256,\n         eos_token_id=50256,\n         rope_theta=10000.0,\n+        rope_scaling=None,\n         sliding_window=None,\n         attention_dropout=0.0,\n         residual_dropout=0.0,\n@@ -134,9 +173,15 @@ def __init__(\n         self.norm_epsilon = norm_epsilon\n         self.use_cache = use_cache\n         self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling\n         self.attention_dropout = attention_dropout\n         self.residual_dropout = residual_dropout\n         self.embedding_dropout = embedding_dropout\n+        # Validate the correctness of rotary position embeddings parameters\n+        # BC: if there is a 'type' field, move it to 'rope_type'.\n+        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n+            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_config_validation(self)\n \n         super().__init__(\n             bos_token_id=bos_token_id,"
        },
        {
            "sha": "c359c07c69c0b82748ede848bc726906832790d4",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 135,
            "deletions": 81,
            "changes": 216,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -36,6 +36,7 @@\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n@@ -112,41 +113,92 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n     return causal_mask\n \n \n-# Copied from transformers.models.mixtral.modeling_mixtral.MixtralRotaryEmbedding with Mixtral->Starcoder2\n+# Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Starcoder2\n class Starcoder2RotaryEmbedding(nn.Module):\n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n+    def __init__(\n+        self,\n+        dim=None,\n+        max_position_embeddings=2048,\n+        base=10000,\n+        device=None,\n+        scaling_factor=1.0,\n+        rope_type=\"default\",\n+        config: Optional[Starcoder2Config] = None,\n+    ):\n         super().__init__()\n+        # TODO (joao): remove the `if` below, only used for BC\n+        self.rope_kwargs = {}\n+        if config is None:\n+            logger.warning_once(\n+                \"`Starcoder2RotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n+                \"`config` argument. All other arguments will be removed in v4.46\"\n+            )\n+            self.rope_kwargs = {\n+                \"rope_type\": rope_type,\n+                \"factor\": scaling_factor,\n+                \"dim\": dim,\n+                \"base\": base,\n+                \"max_position_embeddings\": max_position_embeddings,\n+            }\n+            self.rope_type = rope_type\n+            self.max_seq_len_cached = max_position_embeddings\n+            self.original_max_seq_len = max_position_embeddings\n+        else:\n+            # BC: \"rope_type\" was originally \"type\"\n+            if config.rope_scaling is not None:\n+                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+            else:\n+                self.rope_type = \"default\"\n+            self.max_seq_len_cached = config.max_position_embeddings\n+            self.original_max_seq_len = config.max_position_embeddings\n \n-        self.dim = dim\n-        self.max_position_embeddings = max_position_embeddings\n-        self.base = base\n-        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n \n-        # Build here to make `torch.jit.trace` work.\n-        self._set_cos_sin_cache(\n-            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n-        )\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(\n+                self.config, device, seq_len=seq_len, **self.rope_kwargs\n+            )\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n \n-    def _set_cos_sin_cache(self, seq_len, device, dtype):\n-        self.max_seq_len_cached = seq_len\n-        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n \n-        freqs = torch.outer(t, self.inv_freq)\n-        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n-        emb = torch.cat((freqs, freqs), dim=-1)\n-        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n-        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n+    @torch.no_grad()\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n \n-    def forward(self, x, seq_len=None):\n-        # x: [bs, num_attention_heads, seq_len, head_size]\n-        if seq_len > self.max_seq_len_cached:\n-            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n+        # Core RoPE block\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n+        device_type = x.device.type\n+        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos()\n+            sin = emb.sin()\n \n-        return (\n-            self.cos_cached[:seq_len].to(dtype=x.dtype),\n-            self.sin_cached[:seq_len].to(dtype=x.dtype),\n-        )\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n # Copied from transformers.models.llama.modeling_llama.rotate_half\n@@ -157,18 +209,17 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n-# Copied from transformers.models.mixtral.modeling_mixtral.apply_rotary_pos_emb\n-def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n+# Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n     Args:\n         q (`torch.Tensor`): The query tensor.\n         k (`torch.Tensor`): The key tensor.\n         cos (`torch.Tensor`): The cosine part of the rotary embedding.\n         sin (`torch.Tensor`): The sine part of the rotary embedding.\n-        position_ids (`torch.Tensor`):\n-            The position indices of the tokens corresponding to the query and key tensors. For example, this can be\n-            used to pass offsetted position ids when working with a KV-cache.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n         unsqueeze_dim (`int`, *optional*, defaults to 1):\n             The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n             sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n@@ -179,8 +230,8 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n     Returns:\n         `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n     \"\"\"\n-    cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n-    sin = sin[position_ids].unsqueeze(unsqueeze_dim)\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n     q_embed = (q * cos) + (rotate_half(q) * sin)\n     k_embed = (k * cos) + (rotate_half(k) * sin)\n     return q_embed, k_embed\n@@ -238,7 +289,6 @@ def __init__(self, config: Starcoder2Config, layer_idx: Optional[int] = None):\n         self.head_dim = self.hidden_size // self.num_heads\n         self.num_key_value_heads = config.num_key_value_heads\n         self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n-        self.max_position_embeddings = config.max_position_embeddings\n         self.rope_theta = config.rope_theta\n         self.use_bias = config.use_bias\n         self.is_causal = True\n@@ -255,11 +305,7 @@ def __init__(self, config: Starcoder2Config, layer_idx: Optional[int] = None):\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=self.use_bias)\n         self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=self.use_bias)\n \n-        self.rotary_emb = Starcoder2RotaryEmbedding(\n-            self.head_dim,\n-            max_position_embeddings=self.max_position_embeddings,\n-            base=self.rope_theta,\n-        )\n+        self.rotary_emb = Starcoder2RotaryEmbedding(config=self.config)\n \n     def forward(\n         self,\n@@ -270,6 +316,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n \n@@ -281,17 +328,17 @@ def forward(\n         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            if self.layer_idx is None:\n-                raise ValueError(\n-                    f\"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} \"\n-                    \"for auto-regressive decoding with k/v caching, please make sure to initialize the attention class \"\n-                    \"with a layer index.\"\n-                )\n-            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n-        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n@@ -302,13 +349,6 @@ def forward(\n         value_states = repeat_kv(value_states, self.num_key_value_groups)\n \n         attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n-\n-        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n         if attention_mask is not None:  # no matter the length, we just slice it\n             causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n             attn_weights += causal_mask\n@@ -362,6 +402,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n     ):\n         bsz, q_len, _ = hidden_states.size()\n \n@@ -373,28 +414,22 @@ def forward(\n         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            if self.layer_idx is None:\n-                raise ValueError(\n-                    f\"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} \"\n-                    \"for auto-regressive decoding with k/v caching, please make sure to initialize the attention class \"\n-                    \"with a layer index.\"\n-                )\n-            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n-\n-        # Because the input can be padded, the absolute sequence length depends on the max position id.\n-        rotary_seq_len = (\n-            max(kv_seq_len, position_ids[:, -1].max().item() + 1) if position_ids is not None else kv_seq_len\n-        )\n-\n-        cos, sin = self.rotary_emb(value_states, seq_len=rotary_seq_len)\n-\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n             # Activate slicing cache only if the config has a value `sliding_windows` attribute\n             cache_has_contents = past_key_value.get_seq_length(self.layer_idx) > 0\n+            kv_seq_len = key_states.shape[-2] + cache_position[0]\n             if (\n                 getattr(self.config, \"sliding_window\", None) is not None\n                 and kv_seq_len > self.config.sliding_window\n@@ -495,6 +530,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if output_attentions:\n             # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n@@ -521,12 +557,17 @@ def forward(\n         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n-        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n-\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n@@ -599,6 +640,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n         **kwargs,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n@@ -615,6 +657,9 @@ def forward(\n             past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n+            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n+                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n+                with `head_dim` being the embedding dimension of each attention head.\n             kwargs (`dict`, *optional*):\n                 Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n                 into the model\n@@ -633,6 +678,7 @@ def forward(\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n         )\n         hidden_states = residual + hidden_states\n \n@@ -684,6 +730,8 @@ class Starcoder2PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n+    _supports_quantized_cache = True\n+    _supports_static_cache = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -796,6 +844,7 @@ def __init__(self, config: Starcoder2Config):\n         )\n         self._attn_implementation = config._attn_implementation\n         self.norm = nn.LayerNorm(config.hidden_size, eps=config.norm_epsilon)\n+        self.rotary_emb = Starcoder2RotaryEmbedding(config=config)\n         self.gradient_checkpointing = False\n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -867,6 +916,9 @@ def forward(\n         hidden_states = inputs_embeds\n         hidden_states = nn.functional.dropout(hidden_states, p=self.embedding_dropout, training=self.training)\n \n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n@@ -886,6 +938,7 @@ def forward(\n                     output_attentions,\n                     use_cache,\n                     cache_position,\n+                    position_embeddings,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n@@ -896,6 +949,7 @@ def forward(\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n+                    position_embeddings=position_embeddings,\n                 )\n \n             hidden_states = layer_outputs[0]"
        },
        {
            "sha": "f17ee1170ab28aee67dde32d1476d65e9ce4eba2",
            "filename": "tests/models/bloom/test_modeling_bloom.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/tests%2Fmodels%2Fbloom%2Ftest_modeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/tests%2Fmodels%2Fbloom%2Ftest_modeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbloom%2Ftest_modeling_bloom.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -514,6 +514,10 @@ def test_batch_generated_text(self):\n \n         self.assertListEqual(generated_text, EXPECTED_GENERATIONS)\n \n+    @unittest.skip(\"Bloom needs a 2D attention for alibi\")\n+    def test_custom_4d_attention_mask(self):\n+        pass\n+\n \n @require_torch\n class BloomEmbeddingTest(unittest.TestCase):"
        },
        {
            "sha": "f6c28344754ee9c1de9e1a3523c2fe6110fcba46",
            "filename": "tests/models/falcon/test_modeling_falcon.py",
            "status": "modified",
            "additions": 16,
            "deletions": 12,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -461,17 +461,21 @@ def test_model_rope_scaling(self):\n \n         # Inputs\n         x = torch.randn(1, dtype=torch.float32, device=torch_device)  # used exlusively to get the dtype and the device\n+        position_ids_short = torch.arange(short_input_length, dtype=torch.long, device=torch_device)\n+        position_ids_short = position_ids_short.unsqueeze(0)\n+        position_ids_long = torch.arange(long_input_length, dtype=torch.long, device=torch_device)\n+        position_ids_long = position_ids_long.unsqueeze(0)\n \n         # Sanity check original RoPE\n         original_rope = FalconRotaryEmbedding(\n             head_dim,\n             max_position_embeddings=config.max_position_embeddings,\n             base=config.rope_theta,\n         ).to(torch_device)\n-        original_cos_short, original_sin_short = original_rope(x, short_input_length)\n-        original_cos_long, original_sin_long = original_rope(x, long_input_length)\n-        torch.testing.assert_close(original_cos_short, original_cos_long[:short_input_length, :])\n-        torch.testing.assert_close(original_sin_short, original_sin_long[:short_input_length, :])\n+        original_cos_short, original_sin_short = original_rope(x, position_ids_short)\n+        original_cos_long, original_sin_long = original_rope(x, position_ids_long)\n+        torch.testing.assert_close(original_cos_short, original_cos_long[:, :short_input_length, :])\n+        torch.testing.assert_close(original_sin_short, original_sin_long[:, :short_input_length, :])\n \n         # Sanity check linear RoPE scaling\n         # New position \"x\" should match original position with index \"x/scaling_factor\"\n@@ -481,14 +485,14 @@ def test_model_rope_scaling(self):\n             base=config.rope_theta,\n             scaling_factor=scaling_factor,\n         ).to(torch_device)\n-        linear_cos_short, linear_sin_short = linear_scaling_rope(x, short_input_length)\n-        linear_cos_long, linear_sin_long = linear_scaling_rope(x, long_input_length)\n-        torch.testing.assert_close(linear_cos_short, linear_cos_long[:short_input_length, :])\n-        torch.testing.assert_close(linear_sin_short, linear_sin_long[:short_input_length, :])\n+        linear_cos_short, linear_sin_short = linear_scaling_rope(x, position_ids_short)\n+        linear_cos_long, linear_sin_long = linear_scaling_rope(x, position_ids_long)\n+        torch.testing.assert_close(linear_cos_short, linear_cos_long[:, :short_input_length, :])\n+        torch.testing.assert_close(linear_sin_short, linear_sin_long[:, :short_input_length, :])\n         for new_position in range(0, long_input_length, scaling_factor):\n             original_position = int(new_position // scaling_factor)\n-            torch.testing.assert_close(linear_cos_long[new_position, :], original_cos_long[original_position, :])\n-            torch.testing.assert_close(linear_sin_long[new_position, :], original_sin_long[original_position, :])\n+            torch.testing.assert_close(linear_cos_long[:, new_position, :], original_cos_long[:, original_position, :])\n+            torch.testing.assert_close(linear_sin_long[:, new_position, :], original_sin_long[:, original_position, :])\n \n         # Sanity check Dynamic NTK RoPE scaling\n         # Scaling should only be observed after a long input is fed. We can observe that the frequencies increase\n@@ -499,8 +503,8 @@ def test_model_rope_scaling(self):\n             base=config.rope_theta,\n             scaling_factor=scaling_factor,\n         ).to(torch_device)\n-        ntk_cos_short, ntk_sin_short = ntk_scaling_rope(x, short_input_length)\n-        ntk_cos_long, ntk_sin_long = ntk_scaling_rope(x, long_input_length)\n+        ntk_cos_short, ntk_sin_short = ntk_scaling_rope(x, position_ids_short)\n+        ntk_cos_long, ntk_sin_long = ntk_scaling_rope(x, position_ids_long)\n         torch.testing.assert_close(ntk_cos_short, original_cos_short)\n         torch.testing.assert_close(ntk_sin_short, original_sin_short)\n         with self.assertRaises(AssertionError):"
        },
        {
            "sha": "196f873696eb70298707e817234b8be19e7eaa71",
            "filename": "tests/models/gpt_neox/test_modeling_gpt_neox.py",
            "status": "modified",
            "additions": 16,
            "deletions": 12,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -382,17 +382,21 @@ def test_model_rope_scaling(self):\n \n         # Inputs\n         x = torch.randn(1, dtype=torch.float32, device=torch_device)  # used exlusively to get the dtype and the device\n+        position_ids_short = torch.arange(short_input_length, dtype=torch.long, device=torch_device)\n+        position_ids_short = position_ids_short.unsqueeze(0)\n+        position_ids_long = torch.arange(long_input_length, dtype=torch.long, device=torch_device)\n+        position_ids_long = position_ids_long.unsqueeze(0)\n \n         # Sanity check original RoPE\n         original_rope = GPTNeoXRotaryEmbedding(\n             head_dim,\n             max_position_embeddings=config.max_position_embeddings,\n             base=config.rotary_emb_base,\n         ).to(torch_device)\n-        original_cos_short, original_sin_short = original_rope(x, short_input_length)\n-        original_cos_long, original_sin_long = original_rope(x, long_input_length)\n-        torch.testing.assert_close(original_cos_short, original_cos_long[:short_input_length, :])\n-        torch.testing.assert_close(original_sin_short, original_sin_long[:short_input_length, :])\n+        original_cos_short, original_sin_short = original_rope(x, position_ids_short)\n+        original_cos_long, original_sin_long = original_rope(x, position_ids_long)\n+        torch.testing.assert_close(original_cos_short, original_cos_long[:, :short_input_length, :])\n+        torch.testing.assert_close(original_sin_short, original_sin_long[:, :short_input_length, :])\n \n         # Sanity check linear RoPE scaling\n         # New position \"x\" should match original position with index \"x/scaling_factor\"\n@@ -402,14 +406,14 @@ def test_model_rope_scaling(self):\n             base=config.rotary_emb_base,\n             scaling_factor=scaling_factor,\n         ).to(torch_device)\n-        linear_cos_short, linear_sin_short = linear_scaling_rope(x, short_input_length)\n-        linear_cos_long, linear_sin_long = linear_scaling_rope(x, long_input_length)\n-        torch.testing.assert_close(linear_cos_short, linear_cos_long[:short_input_length, :])\n-        torch.testing.assert_close(linear_sin_short, linear_sin_long[:short_input_length, :])\n+        linear_cos_short, linear_sin_short = linear_scaling_rope(x, position_ids_short)\n+        linear_cos_long, linear_sin_long = linear_scaling_rope(x, position_ids_long)\n+        torch.testing.assert_close(linear_cos_short, linear_cos_long[:, :short_input_length, :])\n+        torch.testing.assert_close(linear_sin_short, linear_sin_long[:, :short_input_length, :])\n         for new_position in range(0, long_input_length, scaling_factor):\n             original_position = int(new_position // scaling_factor)\n-            torch.testing.assert_close(linear_cos_long[new_position, :], original_cos_long[original_position, :])\n-            torch.testing.assert_close(linear_sin_long[new_position, :], original_sin_long[original_position, :])\n+            torch.testing.assert_close(linear_cos_long[:, new_position, :], original_cos_long[:, original_position, :])\n+            torch.testing.assert_close(linear_sin_long[:, new_position, :], original_sin_long[:, original_position, :])\n \n         # Sanity check Dynamic NTK RoPE scaling\n         # Scaling should only be observed after a long input is fed. We can observe that the frequencies increase\n@@ -420,8 +424,8 @@ def test_model_rope_scaling(self):\n             base=config.rotary_emb_base,\n             scaling_factor=scaling_factor,\n         ).to(torch_device)\n-        ntk_cos_short, ntk_sin_short = ntk_scaling_rope(x, short_input_length)\n-        ntk_cos_long, ntk_sin_long = ntk_scaling_rope(x, long_input_length)\n+        ntk_cos_short, ntk_sin_short = ntk_scaling_rope(x, position_ids_short)\n+        ntk_cos_long, ntk_sin_long = ntk_scaling_rope(x, position_ids_long)\n         torch.testing.assert_close(ntk_cos_short, original_cos_short)\n         torch.testing.assert_close(ntk_sin_short, original_sin_short)\n         with self.assertRaises(AssertionError):"
        },
        {
            "sha": "784323afefdc3f76303d4e9a78efcbabe4dffee0",
            "filename": "tests/models/gpt_neox_japanese/test_modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 12,
            "deletions": 1,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/tests%2Fmodels%2Fgpt_neox_japanese%2Ftest_modeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/tests%2Fmodels%2Fgpt_neox_japanese%2Ftest_modeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_neox_japanese%2Ftest_modeling_gpt_neox_japanese.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -20,6 +20,7 @@\n from transformers.models.gpt_neox_japanese.tokenization_gpt_neox_japanese import GPTNeoXJapaneseTokenizer\n from transformers.testing_utils import require_torch, slow, torch_device\n \n+from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, ids_tensor, random_attention_mask\n from ...test_pipeline_mixin import PipelineTesterMixin\n@@ -56,6 +57,8 @@ def __init__(\n         initializer_range=0.02,\n         num_labels=3,\n         num_choices=4,\n+        bos_token_id=1,\n+        eos_token_id=0,\n         scope=None,\n     ):\n         self.parent = parent\n@@ -81,6 +84,8 @@ def __init__(\n         self.num_labels = num_labels\n         self.num_choices = num_choices\n         self.scope = scope\n+        self.eos_token_id = eos_token_id\n+        self.bos_token_id = bos_token_id\n \n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n@@ -112,6 +117,8 @@ def get_config(self):\n             type_vocab_size=self.type_vocab_size,\n             is_decoder=False,\n             initializer_range=self.initializer_range,\n+            eos_token_id=self.eos_token_id,\n+            bos_token_id=self.bos_token_id,\n         )\n \n     def prepare_config_and_inputs_for_decoder(self):\n@@ -189,7 +196,7 @@ def prepare_config_and_inputs_for_common(self):\n \n \n @require_torch\n-class GPTNeoXModelJapaneseTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class GPTNeoXModelJapaneseTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (GPTNeoXJapaneseModel, GPTNeoXJapaneseForCausalLM) if is_torch_available() else ()\n     all_generative_model_classes = (GPTNeoXJapaneseForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n@@ -257,3 +264,7 @@ def test_generation(self):\n             generated_string = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n             predicted_outputs += generated_string\n         self.assertListEqual(predicted_outputs, EXPECTED_OUTPUTS)\n+\n+    @unittest.skip(\"GPTNeoXJapanese applies bias to attention scores\")\n+    def test_custom_4d_attention_mask(self):\n+        pass"
        },
        {
            "sha": "0d267fb86910d6a8423d8b6db537c02e80740373",
            "filename": "tests/models/persimmon/test_modeling_persimmon.py",
            "status": "modified",
            "additions": 16,
            "deletions": 12,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -433,17 +433,21 @@ def test_model_rope_scaling(self):\n \n         # Inputs\n         x = torch.randn(1, dtype=torch.float32, device=torch_device)  # used exlusively to get the dtype and the device\n+        position_ids_short = torch.arange(short_input_length, dtype=torch.long, device=torch_device)\n+        position_ids_short = position_ids_short.unsqueeze(0)\n+        position_ids_long = torch.arange(long_input_length, dtype=torch.long, device=torch_device)\n+        position_ids_long = position_ids_long.unsqueeze(0)\n \n         # Sanity check original RoPE\n         original_rope = PersimmonRotaryEmbedding(\n             head_dim,\n             max_position_embeddings=config.max_position_embeddings,\n             base=config.rope_theta,\n         ).to(torch_device)\n-        original_cos_short, original_sin_short = original_rope(x, short_input_length)\n-        original_cos_long, original_sin_long = original_rope(x, long_input_length)\n-        torch.testing.assert_close(original_cos_short, original_cos_long[:short_input_length, :])\n-        torch.testing.assert_close(original_sin_short, original_sin_long[:short_input_length, :])\n+        original_cos_short, original_sin_short = original_rope(x, position_ids_short)\n+        original_cos_long, original_sin_long = original_rope(x, position_ids_long)\n+        torch.testing.assert_close(original_cos_short, original_cos_long[:, :short_input_length, :])\n+        torch.testing.assert_close(original_sin_short, original_sin_long[:, :short_input_length, :])\n \n         # Sanity check linear RoPE scaling\n         # New position \"x\" should match original position with index \"x/scaling_factor\"\n@@ -453,14 +457,14 @@ def test_model_rope_scaling(self):\n             base=config.rope_theta,\n             scaling_factor=scaling_factor,\n         ).to(torch_device)\n-        linear_cos_short, linear_sin_short = linear_scaling_rope(x, short_input_length)\n-        linear_cos_long, linear_sin_long = linear_scaling_rope(x, long_input_length)\n-        torch.testing.assert_close(linear_cos_short, linear_cos_long[:short_input_length, :])\n-        torch.testing.assert_close(linear_sin_short, linear_sin_long[:short_input_length, :])\n+        linear_cos_short, linear_sin_short = linear_scaling_rope(x, position_ids_short)\n+        linear_cos_long, linear_sin_long = linear_scaling_rope(x, position_ids_long)\n+        torch.testing.assert_close(linear_cos_short, linear_cos_long[:, :short_input_length, :])\n+        torch.testing.assert_close(linear_sin_short, linear_sin_long[:, :short_input_length, :])\n         for new_position in range(0, long_input_length, scaling_factor):\n             original_position = int(new_position // scaling_factor)\n-            torch.testing.assert_close(linear_cos_long[new_position, :], original_cos_long[original_position, :])\n-            torch.testing.assert_close(linear_sin_long[new_position, :], original_sin_long[original_position, :])\n+            torch.testing.assert_close(linear_cos_long[:, new_position, :], original_cos_long[:, original_position, :])\n+            torch.testing.assert_close(linear_sin_long[:, new_position, :], original_sin_long[:, original_position, :])\n \n         # Sanity check Dynamic NTK RoPE scaling\n         # Scaling should only be observed after a long input is fed. We can observe that the frequencies increase\n@@ -471,8 +475,8 @@ def test_model_rope_scaling(self):\n             base=config.rope_theta,\n             scaling_factor=scaling_factor,\n         ).to(torch_device)\n-        ntk_cos_short, ntk_sin_short = ntk_scaling_rope(x, short_input_length)\n-        ntk_cos_long, ntk_sin_long = ntk_scaling_rope(x, long_input_length)\n+        ntk_cos_short, ntk_sin_short = ntk_scaling_rope(x, position_ids_short)\n+        ntk_cos_long, ntk_sin_long = ntk_scaling_rope(x, position_ids_long)\n         torch.testing.assert_close(ntk_cos_short, original_cos_short)\n         torch.testing.assert_close(ntk_sin_short, original_sin_short)\n         with self.assertRaises(AssertionError):"
        },
        {
            "sha": "95b0b01c0a23d9dbd18795289b797456751397a1",
            "filename": "tests/models/phi/test_modeling_phi.py",
            "status": "modified",
            "additions": 16,
            "deletions": 12,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -409,17 +409,21 @@ def test_model_rope_scaling(self):\n \n         # Inputs\n         x = torch.randn(1, dtype=torch.float32, device=torch_device)  # used exlusively to get the dtype and the device\n+        position_ids_short = torch.arange(short_input_length, dtype=torch.long, device=torch_device)\n+        position_ids_short = position_ids_short.unsqueeze(0)\n+        position_ids_long = torch.arange(long_input_length, dtype=torch.long, device=torch_device)\n+        position_ids_long = position_ids_long.unsqueeze(0)\n \n         # Sanity check original RoPE\n         original_rope = PhiRotaryEmbedding(\n             head_dim,\n             max_position_embeddings=config.max_position_embeddings,\n             base=config.rope_theta,\n         ).to(torch_device)\n-        original_cos_short, original_sin_short = original_rope(x, short_input_length)\n-        original_cos_long, original_sin_long = original_rope(x, long_input_length)\n-        torch.testing.assert_close(original_cos_short, original_cos_long[:short_input_length, :])\n-        torch.testing.assert_close(original_sin_short, original_sin_long[:short_input_length, :])\n+        original_cos_short, original_sin_short = original_rope(x, position_ids_short)\n+        original_cos_long, original_sin_long = original_rope(x, position_ids_long)\n+        torch.testing.assert_close(original_cos_short, original_cos_long[:, :short_input_length, :])\n+        torch.testing.assert_close(original_sin_short, original_sin_long[:, :short_input_length, :])\n \n         # Sanity check linear RoPE scaling\n         # New position \"x\" should match original position with index \"x/scaling_factor\"\n@@ -429,14 +433,14 @@ def test_model_rope_scaling(self):\n             base=config.rope_theta,\n             scaling_factor=scaling_factor,\n         ).to(torch_device)\n-        linear_cos_short, linear_sin_short = linear_scaling_rope(x, short_input_length)\n-        linear_cos_long, linear_sin_long = linear_scaling_rope(x, long_input_length)\n-        torch.testing.assert_close(linear_cos_short, linear_cos_long[:short_input_length, :])\n-        torch.testing.assert_close(linear_sin_short, linear_sin_long[:short_input_length, :])\n+        linear_cos_short, linear_sin_short = linear_scaling_rope(x, position_ids_short)\n+        linear_cos_long, linear_sin_long = linear_scaling_rope(x, position_ids_long)\n+        torch.testing.assert_close(linear_cos_short, linear_cos_long[:, :short_input_length, :])\n+        torch.testing.assert_close(linear_sin_short, linear_sin_long[:, :short_input_length, :])\n         for new_position in range(0, long_input_length, scaling_factor):\n             original_position = int(new_position // scaling_factor)\n-            torch.testing.assert_close(linear_cos_long[new_position, :], original_cos_long[original_position, :])\n-            torch.testing.assert_close(linear_sin_long[new_position, :], original_sin_long[original_position, :])\n+            torch.testing.assert_close(linear_cos_long[:, new_position, :], original_cos_long[:, original_position, :])\n+            torch.testing.assert_close(linear_sin_long[:, new_position, :], original_sin_long[:, original_position, :])\n \n         # Sanity check Dynamic NTK RoPE scaling\n         # Scaling should only be observed after a long input is fed. We can observe that the frequencies increase\n@@ -447,8 +451,8 @@ def test_model_rope_scaling(self):\n             base=config.rope_theta,\n             scaling_factor=scaling_factor,\n         ).to(torch_device)\n-        ntk_cos_short, ntk_sin_short = ntk_scaling_rope(x, short_input_length)\n-        ntk_cos_long, ntk_sin_long = ntk_scaling_rope(x, long_input_length)\n+        ntk_cos_short, ntk_sin_short = ntk_scaling_rope(x, position_ids_short)\n+        ntk_cos_long, ntk_sin_long = ntk_scaling_rope(x, position_ids_long)\n         torch.testing.assert_close(ntk_cos_short, original_cos_short)\n         torch.testing.assert_close(ntk_sin_short, original_sin_short)\n         with self.assertRaises(AssertionError):"
        },
        {
            "sha": "36cad89bcfdf06aa890635b5f531f0f27aa1c28e",
            "filename": "tests/models/stablelm/test_modeling_stablelm.py",
            "status": "modified",
            "additions": 16,
            "deletions": 12,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -420,17 +420,21 @@ def test_model_rope_scaling(self):\n \n         # Inputs\n         x = torch.randn(1, dtype=torch.float32, device=torch_device)  # used exlusively to get the dtype and the device\n+        position_ids_short = torch.arange(short_input_length, dtype=torch.long, device=torch_device)\n+        position_ids_short = position_ids_short.unsqueeze(0)\n+        position_ids_long = torch.arange(long_input_length, dtype=torch.long, device=torch_device)\n+        position_ids_long = position_ids_long.unsqueeze(0)\n \n         # Sanity check original RoPE\n         original_rope = StableLmRotaryEmbedding(\n             head_dim,\n             max_position_embeddings=config.max_position_embeddings,\n             base=config.rope_theta,\n         ).to(torch_device)\n-        original_cos_short, original_sin_short = original_rope(x, short_input_length)\n-        original_cos_long, original_sin_long = original_rope(x, long_input_length)\n-        torch.testing.assert_close(original_cos_short, original_cos_long[:short_input_length, :])\n-        torch.testing.assert_close(original_sin_short, original_sin_long[:short_input_length, :])\n+        original_cos_short, original_sin_short = original_rope(x, position_ids_short)\n+        original_cos_long, original_sin_long = original_rope(x, position_ids_long)\n+        torch.testing.assert_close(original_cos_short, original_cos_long[:, :short_input_length, :])\n+        torch.testing.assert_close(original_sin_short, original_sin_long[:, :short_input_length, :])\n \n         # Sanity check linear RoPE scaling\n         # New position \"x\" should match original position with index \"x/scaling_factor\"\n@@ -440,14 +444,14 @@ def test_model_rope_scaling(self):\n             base=config.rope_theta,\n             scaling_factor=scaling_factor,\n         ).to(torch_device)\n-        linear_cos_short, linear_sin_short = linear_scaling_rope(x, short_input_length)\n-        linear_cos_long, linear_sin_long = linear_scaling_rope(x, long_input_length)\n-        torch.testing.assert_close(linear_cos_short, linear_cos_long[:short_input_length, :])\n-        torch.testing.assert_close(linear_sin_short, linear_sin_long[:short_input_length, :])\n+        linear_cos_short, linear_sin_short = linear_scaling_rope(x, position_ids_short)\n+        linear_cos_long, linear_sin_long = linear_scaling_rope(x, position_ids_long)\n+        torch.testing.assert_close(linear_cos_short, linear_cos_long[:, :short_input_length, :])\n+        torch.testing.assert_close(linear_sin_short, linear_sin_long[:, :short_input_length, :])\n         for new_position in range(0, long_input_length, scaling_factor):\n             original_position = int(new_position // scaling_factor)\n-            torch.testing.assert_close(linear_cos_long[new_position, :], original_cos_long[original_position, :])\n-            torch.testing.assert_close(linear_sin_long[new_position, :], original_sin_long[original_position, :])\n+            torch.testing.assert_close(linear_cos_long[:, new_position, :], original_cos_long[:, original_position, :])\n+            torch.testing.assert_close(linear_sin_long[:, new_position, :], original_sin_long[:, original_position, :])\n \n         # Sanity check Dynamic NTK RoPE scaling\n         # Scaling should only be observed after a long input is fed. We can observe that the frequencies increase\n@@ -458,8 +462,8 @@ def test_model_rope_scaling(self):\n             base=config.rope_theta,\n             scaling_factor=scaling_factor,\n         ).to(torch_device)\n-        ntk_cos_short, ntk_sin_short = ntk_scaling_rope(x, short_input_length)\n-        ntk_cos_long, ntk_sin_long = ntk_scaling_rope(x, long_input_length)\n+        ntk_cos_short, ntk_sin_short = ntk_scaling_rope(x, position_ids_short)\n+        ntk_cos_long, ntk_sin_long = ntk_scaling_rope(x, position_ids_long)\n         torch.testing.assert_close(ntk_cos_short, original_cos_short)\n         torch.testing.assert_close(ntk_sin_short, original_sin_short)\n         with self.assertRaises(AssertionError):"
        },
        {
            "sha": "d0091449e18cbae1b4dbfe442d618ca1fe269da5",
            "filename": "tests/pipelines/test_pipelines_text_generation.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/tests%2Fpipelines%2Ftest_pipelines_text_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/tests%2Fpipelines%2Ftest_pipelines_text_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_text_generation.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -469,6 +469,7 @@ def run_pipeline_test(self, text_generator, _):\n             \"RwkvForCausalLM\",\n             \"XGLMForCausalLM\",\n             \"GPTNeoXForCausalLM\",\n+            \"GPTNeoXJapaneseForCausalLM\",\n             \"FuyuForCausalLM\",\n         ]\n         if ("
        },
        {
            "sha": "da0570290c5761f45d43014ade635407db32aded",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/65bb28444849976f853063edb958b3ef3dd59d12/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65bb28444849976f853063edb958b3ef3dd59d12/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=65bb28444849976f853063edb958b3ef3dd59d12",
            "patch": "@@ -4640,7 +4640,7 @@ def test_custom_4d_attention_mask(self):\n             if not model_class._supports_static_cache:\n                 self.skipTest(f\"{model_class.__name__} is not guaranteed to work with custom 4D attention masks\")\n             config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-            if getattr(config, \"sliding_window\", 0) > 0:\n+            if getattr(config, \"sliding_window\", 0) is not None and getattr(config, \"sliding_window\", 0) > 0:\n                 self.skipTest(f\"{model_class.__name__} with sliding window attention is not supported by this test\")\n             model = model_class(config).to(device=torch_device, dtype=torch.float32)\n \n@@ -4689,7 +4689,7 @@ def test_static_cache_matches_dynamic(self):\n                 self.skipTest(f\"{model_class.__name__} does not support cache class\")\n \n             config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n-            if getattr(config, \"sliding_window\", 0) > 0:\n+            if getattr(config, \"sliding_window\", 0) is not None and getattr(config, \"sliding_window\", 0) > 0:\n                 self.skipTest(f\"{model_class.__name__} with sliding window attention is not supported by this test\")\n \n             model = model_class(config).to(device=torch_device, dtype=torch.float32)"
        }
    ],
    "stats": {
        "total": 3744,
        "additions": 2339,
        "deletions": 1405
    }
}