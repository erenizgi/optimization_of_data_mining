{
    "author": "MekkCyber",
    "message": "[kernels] make the module declaration implicit with decorator (#42700)\n\n* initial\n\n* fix\n\n* fix copies\n\n* fix\n\n* address review\n\n* import order\n\n* fix copies\n\n* fix copies\n\n* fix\n\n* update\n\n* fix",
    "sha": "c1ac18258d1060199a7e593b3a0fc531b809e601",
    "files": [
        {
            "sha": "a7b8bde026870b28b1418ece08282b8749de8fff",
            "filename": "src/transformers/integrations/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2F__init__.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -73,6 +73,7 @@\n         \"replace_kernel_forward_from_hub\",\n         \"use_kernel_forward_from_hub\",\n         \"use_kernel_func_from_hub\",\n+        \"use_kernelized_func\",\n     ],\n     \"integration_utils\": [\n         \"INTEGRATION_TO_CALLBACK\",\n@@ -214,6 +215,7 @@\n         replace_kernel_forward_from_hub,\n         use_kernel_forward_from_hub,\n         use_kernel_func_from_hub,\n+        use_kernelized_func,\n     )\n     from .integration_utils import (\n         INTEGRATION_TO_CALLBACK,"
        },
        {
            "sha": "f6a3b52e99afa9cf495d764b19fdffd109569960",
            "filename": "src/transformers/integrations/hub_kernels.py",
            "status": "modified",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -369,6 +369,32 @@ def lazy_load_kernel(kernel_name: str, mapping: dict[str, ModuleType | None] = _\n     return mapping[kernel_name]\n \n \n+def use_kernelized_func(module_names: list[Callable] | Callable):\n+    \"\"\"\n+    This decorator attaches the target function as an attribute of the module.\n+    The function must already be decorated with @use_kernel_func_from_hub\n+    this decorator then wraps it as an nn.Module internally.\n+    When kernelize is later applied to the full model, the function can be accessed as a regular module attribute and kernelized just like any other layer.\n+    The kernelization is performed in place, modifying the module directly.\n+    \"\"\"\n+    if isinstance(module_names, Callable):\n+        module_names = [module_names]\n+\n+    def decorator(cls):\n+        orig_init = cls.__init__\n+\n+        def new_init(self, *args, **kwargs):\n+            orig_init(self, *args, **kwargs)\n+            for fn in module_names:\n+                # we hardcode the name of the function to \"rotary_fn\" for now\n+                setattr(self, \"rotary_fn\", fn)\n+\n+        cls.__init__ = new_init\n+        return cls\n+\n+    return decorator\n+\n+\n __all__ = [\n     \"LayerRepository\",\n     \"use_kernel_forward_from_hub\",\n@@ -377,4 +403,5 @@ def lazy_load_kernel(kernel_name: str, mapping: dict[str, ModuleType | None] = _\n     \"register_kernel_mapping_transformers\",\n     \"replace_kernel_forward_from_hub\",\n     \"lazy_load_kernel\",\n+    \"use_kernelized_func\",\n ]"
        },
        {
            "sha": "4ee42f8137c2200397dbae97f5fd2af99eeff536",
            "filename": "src/transformers/models/afmoe/modeling_afmoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fafmoe%2Fmodeling_afmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fafmoe%2Fmodeling_afmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fafmoe%2Fmodeling_afmoe.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -28,7 +28,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_func_from_hub\n+from ...integrations import use_kernel_func_from_hub, use_kernelized_func\n from ...integrations.hub_kernels import use_kernel_forward_from_hub\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -338,6 +338,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class AfmoeAttention(nn.Module):\n     \"\"\"\n     Multi-headed attention module with optional sliding window and gating.\n@@ -369,7 +370,6 @@ def __init__(self, config: AfmoeConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n         # Parent LlamaAttention already sets: layer_idx, num_heads, num_key_value_heads, num_key_value_groups, head_dim\n         # We only add AFMoE-specific attributes\n         self.is_local_attention = config.layer_types[layer_idx] == \"sliding_attention\""
        },
        {
            "sha": "2425da1ea658e9af3a8dde3a704cd957551f5c04",
            "filename": "src/transformers/models/apertus/modeling_apertus.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodeling_apertus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodeling_apertus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodeling_apertus.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -28,7 +28,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GenericForTokenClassification, GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -213,6 +213,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class ApertusAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -238,7 +239,6 @@ def __init__(self, config: ApertusConfig, layer_idx: Optional[int] = None):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n         self.q_norm = ApertusRMSNorm(self.head_dim, config.rms_norm_eps)\n         self.k_norm = ApertusRMSNorm(self.head_dim, config.rms_norm_eps)\n "
        },
        {
            "sha": "f9f869c901918111a38423aa2646be24ba88b70b",
            "filename": "src/transformers/models/arcee/modeling_arcee.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -30,7 +30,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import (\n     GenericForQuestionAnswering,\n@@ -220,6 +220,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class ArceeAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -245,7 +246,6 @@ def __init__(self, config: ArceeConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "9eca2c271f4ff0e2b7986c199c6b7ff4c1213b05",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -29,7 +29,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -444,6 +444,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class AriaTextAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -469,7 +470,6 @@ def __init__(self, config: AriaTextConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "ec6e5ad8aec6109ed454688c83924bf383743ce4",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -35,7 +35,7 @@\n from ... import initialization as init\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernelized_func\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -345,6 +345,7 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     return q_embed, k_embed\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class BambaAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -370,7 +371,6 @@ def __init__(self, config: BambaConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "05017cdb56b8d6057000e6dd85efaed7de63c8f1",
            "filename": "src/transformers/models/bitnet/modeling_bitnet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -27,7 +27,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -151,6 +151,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class BitNetAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -176,7 +177,6 @@ def __init__(self, config: BitNetConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n         self.attn_sub_norm = BitNetRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n     def forward("
        },
        {
            "sha": "a3df29f848e1a0cdd994275e6d2aae90eefcf5bb",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -36,6 +36,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -222,6 +223,7 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     return q_embed.to(dtype=dtype), k_embed.to(dtype=dtype)\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class CohereAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -247,7 +249,6 @@ def __init__(self, config: CohereConfig, layer_idx: Optional[int] = None):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n         self.use_qk_norm = config.use_qk_norm\n         if self.use_qk_norm:\n             # When sharding the model using Tensor Parallelism, need to be careful to use n_local_heads"
        },
        {
            "sha": "9699844cfc6a03b996e672224c8310609da6a82a",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -28,6 +28,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernelized_func\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -197,6 +198,7 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     return q_embed.to(dtype=dtype), k_embed.to(dtype=dtype)\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class Cohere2Attention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n "
        },
        {
            "sha": "73be86a3648ba2ace650b5c5729d721677e2d6c8",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -32,7 +32,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -272,6 +272,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class CsmAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -297,7 +298,6 @@ def __init__(self, config: CsmConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "838143d22cbb7167c661eab544534a000e92a0be",
            "filename": "src/transformers/models/cwm/modeling_cwm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fcwm%2Fmodeling_cwm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fcwm%2Fmodeling_cwm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcwm%2Fmodeling_cwm.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -28,7 +28,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -179,6 +179,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class CwmAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -196,7 +197,6 @@ def __init__(self, config: CwmConfig, layer_idx: int):\n         self.k_proj = torch.nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.v_proj = torch.nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n-        self.rotary_fn = apply_rotary_pos_emb\n         self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None\n \n     def forward("
        },
        {
            "sha": "85d363e0fde6aec2735845837218091018bccfe2",
            "filename": "src/transformers/models/dia/modeling_dia.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -27,7 +27,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -266,6 +266,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class DiaSelfAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n "
        },
        {
            "sha": "1ef3a6396fb636ec5dd61f30e289d7303ba48c25",
            "filename": "src/transformers/models/dots1/modeling_dots1.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -29,7 +29,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -201,6 +201,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class Dots1Attention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -227,7 +228,6 @@ def __init__(self, config: Dots1Config, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n         self.q_norm = Dots1RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # unlike olmo, only on the head dim!\n         self.k_norm = Dots1RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # thus post q_norm does not need reshape\n         self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None"
        },
        {
            "sha": "d6654c69b614c43f44f9199cd34f1b2c740b752d",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -33,7 +33,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -118,6 +118,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class Emu3Attention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -143,7 +144,6 @@ def __init__(self, config: Emu3Config, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "deecb9e6940ff7dd61669ffdb6d5b7f85283a94b",
            "filename": "src/transformers/models/ernie4_5/modeling_ernie4_5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -27,7 +27,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -203,6 +203,7 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     return q_embed.to(original_dtype), k_embed.to(original_dtype)\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class Ernie4_5Attention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -221,7 +222,6 @@ def __init__(self, config: Ernie4_5Config, layer_idx: int):\n         self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.use_bias)\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.use_bias)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.use_bias)\n-        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "5bd3c7b896ad89b724a8507f20e5c9b2feb8398c",
            "filename": "src/transformers/models/ernie4_5_moe/modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -29,7 +29,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n@@ -226,6 +226,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class Ernie4_5_MoeAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -244,7 +245,6 @@ def __init__(self, config: Ernie4_5_MoeConfig, layer_idx: int):\n         self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.use_bias)\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.use_bias)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.use_bias)\n-        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "81cca5d3bf9a26a69ba29cd947aab8305ea9158e",
            "filename": "src/transformers/models/evolla/modeling_evolla.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -31,7 +31,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -1091,6 +1091,7 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class EvollaAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -1116,7 +1117,6 @@ def __init__(self, config: EvollaConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "ba4f8ccbfd677e2bd3bfba6ed0256e27cc3f4d01",
            "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -36,7 +36,7 @@\n from ... import initialization as init\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -361,6 +361,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class FalconH1Attention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -386,7 +387,6 @@ def __init__(self, config: FalconH1Config, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n         self.key_multiplier = config.key_multiplier\n \n     def forward("
        },
        {
            "sha": "8aeb014ff12b9437d43108efd5138283e07e59b1",
            "filename": "src/transformers/models/flex_olmo/modeling_flex_olmo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -30,7 +30,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n@@ -216,6 +216,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class FlexOlmoAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -241,7 +242,6 @@ def __init__(self, config: FlexOlmoConfig, layer_idx: Optional[int] = None):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n         self.q_norm = FlexOlmoRMSNorm(config.num_attention_heads * self.head_dim, config.rms_norm_eps)\n         self.k_norm = FlexOlmoRMSNorm(config.num_key_value_heads * self.head_dim, config.rms_norm_eps)\n "
        },
        {
            "sha": "8c14f33fb46c164f37b37d2c1520076f3d3ea08f",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -29,7 +29,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_func_from_hub\n+from ...integrations import use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import (\n     GenericForSequenceClassification,\n@@ -219,6 +219,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class GemmaAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -244,7 +245,6 @@ def __init__(self, config: GemmaConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "fb3cfcc2f5db5769c420c460b175d983e0360d53",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -29,7 +29,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_func_from_hub\n+from ...integrations import use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import (\n@@ -229,6 +229,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class Gemma2Attention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -255,7 +256,6 @@ def __init__(self, config: Gemma2Config, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n         self.attn_logit_softcapping = self.config.attn_logit_softcapping\n         self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None\n "
        },
        {
            "sha": "df3277003398a273fd9d69ea9b46a66181823552",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -31,7 +31,7 @@\n from ...cache_utils import Cache, DynamicCache\n from ...configuration_utils import PreTrainedConfig\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_func_from_hub\n+from ...integrations import use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask, create_masks_for_generate, create_sliding_window_causal_mask\n from ...modeling_layers import GenericForSequenceClassification, GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast\n@@ -305,6 +305,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class Gemma3Attention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -331,7 +332,6 @@ def __init__(self, config: Gemma3TextConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n         self.attn_logit_softcapping = self.config.attn_logit_softcapping\n         self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None\n         self.is_sliding = self.layer_type == \"sliding_attention\""
        },
        {
            "sha": "b72b20b6126179c99924f34ae9b3322ab82421a2",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -32,6 +32,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernelized_func\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -1225,6 +1226,7 @@ def apply_rotary_pos_emb(\n     return (x * cos) + (rotate_half(x) * sin)\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class Gemma3nTextAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -1251,7 +1253,6 @@ def __init__(self, config: Gemma3nTextConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n         self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None\n         self.is_sliding = self.layer_type == \"sliding_attention\"\n "
        },
        {
            "sha": "52ba32f84267b64c1c714bcf8d1400e60b0e18e4",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -28,7 +28,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import (\n     GenericForSequenceClassification,\n@@ -216,6 +216,7 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     return q_embed, k_embed\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class GlmAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -239,7 +240,6 @@ def __init__(self, config: GlmConfig, layer_idx: Optional[int] = None):\n             config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n         )\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n-        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "91cfcd7e4e65450478ede26d9f2948f587f4b7c4",
            "filename": "src/transformers/models/glm4/modeling_glm4.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -28,7 +28,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import (\n@@ -198,6 +198,7 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     return q_embed, k_embed\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class Glm4Attention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -221,7 +222,6 @@ def __init__(self, config: Glm4Config, layer_idx: Optional[int] = None):\n             config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n         )\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n-        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "f21a66f8c819ee7f9103cc5c011e63d0c04f2899",
            "filename": "src/transformers/models/glm4_moe/modeling_glm4_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -30,7 +30,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -193,6 +193,7 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     return q_embed, k_embed\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class Glm4MoeAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n "
        },
        {
            "sha": "5ef301974813f92d9de9cd41844fdd203ba11a4f",
            "filename": "src/transformers/models/glm4v_moe/modeling_glm4v_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -32,7 +32,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -299,6 +299,7 @@ def apply_multimodal_rotary_pos_emb(q, k, cos, sin, mrope_section, unsqueeze_dim\n     return q_embed, k_embed\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class Glm4vMoeTextAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -322,7 +323,6 @@ def __init__(self, config: Glm4vMoeTextConfig, layer_idx: Optional[int] = None):\n             config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n         )\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n-        self.rotary_fn = apply_rotary_pos_emb\n         self.rope_parameters = config.rope_parameters\n \n     def forward("
        },
        {
            "sha": "544ece409a22927b69769fc3b38c5e0ecee2ae36",
            "filename": "src/transformers/models/gpt_oss/modeling_gpt_oss.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -28,6 +28,7 @@\n from ... import initialization as init\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernelized_func\n from ...integrations.hub_kernels import use_kernel_forward_from_hub\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_layers import (\n@@ -307,6 +308,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class GptOssAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -332,7 +334,6 @@ def __init__(self, config: GptOssConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n         self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None\n         self.sinks = nn.Parameter(torch.empty(config.num_attention_heads))\n "
        },
        {
            "sha": "ca67268b843beb6259bb276fcecf34c14c697f49",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -28,7 +28,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -116,6 +116,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class GraniteAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -141,7 +142,6 @@ def __init__(self, config: GraniteConfig, layer_idx: Optional[int] = None):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "6cafb3a32eeedbfa55b097f3e15414124fa01516",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -30,7 +30,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n@@ -338,6 +338,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class GraniteMoeAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -363,7 +364,6 @@ def __init__(self, config: GraniteMoeConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "9e45bd420f910584288cd65d41defe549f58228e",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -31,7 +31,7 @@\n from ... import initialization as init\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n@@ -132,6 +132,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class GraniteMoeHybridAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -157,7 +158,6 @@ def __init__(self, config: GraniteMoeHybridConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "425e36a5ed05ea63da8721cc0875530bcad81cde",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -30,7 +30,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n@@ -328,6 +328,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class GraniteMoeSharedAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -353,7 +354,6 @@ def __init__(self, config: GraniteMoeSharedConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "eb786565defed12f434d73e67a56a561ba55167d",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -29,6 +29,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import (\n     GenericForSequenceClassification,\n@@ -220,6 +221,7 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     return q_embed, k_embed\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class HeliumAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -243,7 +245,6 @@ def __init__(self, config: HeliumConfig, layer_idx: Optional[int] = None):\n             config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n         )\n         self.o_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=False)\n-        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "ad47eb08d7e4a7fa20919bd89419598fb45df5cd",
            "filename": "src/transformers/models/hunyuan_v1_dense/modeling_hunyuan_v1_dense.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodeling_hunyuan_v1_dense.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodeling_hunyuan_v1_dense.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodeling_hunyuan_v1_dense.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -30,7 +30,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GenericForSequenceClassification, GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -153,6 +153,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class HunYuanDenseV1Attention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -178,7 +179,6 @@ def __init__(self, config: HunYuanDenseV1Config, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n         self.query_layernorm = HunYuanDenseV1RMSNorm(self.head_dim, eps=config.rms_norm_eps)\n         self.key_layernorm = HunYuanDenseV1RMSNorm(self.head_dim, eps=config.rms_norm_eps)\n "
        },
        {
            "sha": "bbfd8cc58c472fdfcd65221524b48b74f7078fe6",
            "filename": "src/transformers/models/hunyuan_v1_moe/modeling_hunyuan_v1_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -30,7 +30,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GenericForSequenceClassification, GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -152,6 +152,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class HunYuanMoEV1Attention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -177,7 +178,6 @@ def __init__(self, config: HunYuanMoEV1Config, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n         self.query_layernorm = HunYuanMoEV1RMSNorm(self.head_dim, eps=config.rms_norm_eps)\n         self.key_layernorm = HunYuanMoEV1RMSNorm(self.head_dim, eps=config.rms_norm_eps)\n "
        },
        {
            "sha": "9314ff7c90c6b71d178758547ecf15ee2ae76806",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -32,7 +32,7 @@\n from ... import initialization as init\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GenericForSequenceClassification, GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n@@ -248,6 +248,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class JambaAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -264,7 +265,6 @@ def __init__(self, config: JambaConfig, layer_idx: int):\n         self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n-        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "c7ef834845fe5baf69ec2031332ab9fbaf3ad476",
            "filename": "src/transformers/models/lasr/modeling_lasr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Flasr%2Fmodeling_lasr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Flasr%2Fmodeling_lasr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flasr%2Fmodeling_lasr.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -27,7 +27,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...integrations import use_kernel_func_from_hub\n+from ...integrations import use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_bidirectional_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, CausalLMOutput\n@@ -205,6 +205,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class LasrEncoderAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -230,7 +231,6 @@ def __init__(self, config: LasrEncoderConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "fbd6eb8aa39b7f47c091b4117d17cbcf74be0437",
            "filename": "src/transformers/models/lfm2/modeling_lfm2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -26,7 +26,7 @@\n \n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -358,6 +358,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class Lfm2Attention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -372,7 +373,6 @@ def __init__(self, config: Lfm2Config, layer_idx: int):\n         self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=False)\n         self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n-        self.rotary_fn = apply_rotary_pos_emb\n         self.out_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n         self.q_layernorm = Lfm2RMSNorm(self.head_dim, eps=config.norm_eps)\n         self.k_layernorm = Lfm2RMSNorm(self.head_dim, eps=config.norm_eps)"
        },
        {
            "sha": "6ad0ab9067ca119881c17c8faedf6dfa16ecb2ee",
            "filename": "src/transformers/models/lfm2_moe/modeling_lfm2_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -27,7 +27,7 @@\n from ... import initialization as init\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, MoeModelOutputWithPast\n@@ -426,6 +426,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class Lfm2MoeAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -440,7 +441,6 @@ def __init__(self, config: Lfm2MoeConfig, layer_idx: int):\n         self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=False)\n         self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n-        self.rotary_fn = apply_rotary_pos_emb\n         self.out_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n         self.q_layernorm = Lfm2MoeRMSNorm(self.head_dim, eps=config.norm_eps)\n         self.k_layernorm = Lfm2MoeRMSNorm(self.head_dim, eps=config.norm_eps)"
        },
        {
            "sha": "61ee707178766d17d28b4b48ba8afa7266dfff0f",
            "filename": "src/transformers/models/lightglue/modeling_lightglue.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -27,6 +27,7 @@\n from torch.nn.utils.rnn import pad_sequence\n \n from ...activations import ACT2FN\n+from ...integrations import use_kernelized_func\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n@@ -174,6 +175,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class LightGlueAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -199,7 +201,6 @@ def __init__(self, config: LightGlueConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "c27684870f054057eec6e6f47da332370dc0f148",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -26,7 +26,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import (\n     GenericForQuestionAnswering,\n@@ -224,6 +224,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class LlamaAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -249,7 +250,6 @@ def __init__(self, config: LlamaConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "c24bd5eef57349b4c66ad00927cec517baf7ff2c",
            "filename": "src/transformers/models/minimax/modeling_minimax.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -31,7 +31,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import (\n@@ -392,6 +392,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class MiniMaxAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -408,7 +409,6 @@ def __init__(self, config: MiniMaxConfig, layer_idx: int):\n         self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n-        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "eed97cc17d69c62feefb4426a321aabc12f1e60a",
            "filename": "src/transformers/models/ministral/modeling_ministral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fministral%2Fmodeling_ministral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fministral%2Fmodeling_ministral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fministral%2Fmodeling_ministral.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -13,7 +13,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import (\n@@ -120,6 +120,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class MinistralAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -138,7 +139,6 @@ def __init__(self, config, layer_idx: int):\n         self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n-        self.rotary_fn = apply_rotary_pos_emb\n         self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None\n \n     def forward("
        },
        {
            "sha": "dad049e3634de52855aac6441fc3764988bbaf4d",
            "filename": "src/transformers/models/ministral3/modeling_ministral3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fministral3%2Fmodeling_ministral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fministral3%2Fmodeling_ministral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fministral3%2Fmodeling_ministral3.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -15,7 +15,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import (\n@@ -110,6 +110,7 @@ def _get_llama_4_attn_scale(positions_ids: torch.Tensor, beta: float, max_positi\n     return scaling.unsqueeze(-1)\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class Ministral3Attention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -126,7 +127,6 @@ def __init__(self, config: Ministral3Config, layer_idx: int):\n         self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n-        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "0af1c53babe8a7c7696a913615693104af9dde75",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -15,7 +15,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import (\n@@ -121,6 +121,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class MistralAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -137,7 +138,6 @@ def __init__(self, config: MistralConfig, layer_idx: int):\n         self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n-        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "1b1c65215dc8f04ed78f14cd0214cb88d18d3b5a",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -37,7 +37,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import (\n@@ -290,6 +290,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class MixtralAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -306,7 +307,6 @@ def __init__(self, config: MixtralConfig, layer_idx: int):\n         self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n-        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "ca385796fd89555df58331d1f0460cadb0b2e9eb",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -30,6 +30,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n@@ -233,6 +234,7 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     return q_embed, k_embed\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class MoonshineAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -264,7 +266,6 @@ def __init__(\n             config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n         )\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n-        self.rotary_fn = apply_rotary_pos_emb\n \n         # Pad head dimension to the next specified multiple.\n         if self.config.pad_head_dim_to_multiple_of is not None:"
        },
        {
            "sha": "65630762a1c3feb1d840f61f79537042bc0bfaac",
            "filename": "src/transformers/models/nanochat/modeling_nanochat.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fnanochat%2Fmodeling_nanochat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fnanochat%2Fmodeling_nanochat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnanochat%2Fmodeling_nanochat.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -30,7 +30,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_func_from_hub\n+from ...integrations import use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -195,6 +195,7 @@ def rotate_half(x):\n     return torch.cat((x2, -x1), dim=-1)\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class NanoChatAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -220,7 +221,6 @@ def __init__(self, config: NanoChatConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n \n         self.q_norm = NanoChatRMSNorm(eps=config.rms_norm_eps)\n         self.k_norm = NanoChatRMSNorm(eps=config.rms_norm_eps)"
        },
        {
            "sha": "2172462ccc289c41f7bce5bd12345fb6110a38cd",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -34,6 +34,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -212,6 +213,7 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     return q_embed.to(q_type), k_embed.to(k_type)\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class OlmoAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -237,7 +239,6 @@ def __init__(self, config: OlmoConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "130da226011e54b9f75bf5e7dfa1b3d0f5ac3895",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -35,7 +35,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -205,6 +205,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class Olmo2Attention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -230,7 +231,6 @@ def __init__(self, config: Olmo2Config, layer_idx: Optional[int] = None):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n         self.q_norm = Olmo2RMSNorm(config.num_attention_heads * self.head_dim, config.rms_norm_eps)\n         self.k_norm = Olmo2RMSNorm(config.num_key_value_heads * self.head_dim, config.rms_norm_eps)\n "
        },
        {
            "sha": "5fca8a74d4490c57e250946455de0e18cb7554a9",
            "filename": "src/transformers/models/olmo3/modeling_olmo3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodeling_olmo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodeling_olmo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodeling_olmo3.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -30,7 +30,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -136,6 +136,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class Olmo3Attention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -161,7 +162,6 @@ def __init__(self, config: Olmo3Config, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n         self.q_norm = Olmo3RMSNorm(config.num_attention_heads * self.head_dim, config.rms_norm_eps)\n         self.k_norm = Olmo3RMSNorm(config.num_key_value_heads * self.head_dim, config.rms_norm_eps)\n         assert config.layer_types is not None"
        },
        {
            "sha": "a890f6db5ff8c77f721db7a27f8a1a4a3ea031d0",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -27,7 +27,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n@@ -214,6 +214,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class OlmoeAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -239,7 +240,6 @@ def __init__(self, config: OlmoeConfig, layer_idx: Optional[int] = None):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n         self.q_norm = OlmoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.k_norm = OlmoeRMSNorm(\n             (config.hidden_size // config.num_attention_heads) * config.num_key_value_heads, eps=config.rms_norm_eps"
        },
        {
            "sha": "672f63bb4b343c746d521bee0888853e4d7b5b8b",
            "filename": "src/transformers/models/parakeet/modeling_parakeet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodeling_parakeet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodeling_parakeet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodeling_parakeet.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -29,7 +29,7 @@\n \n from ... import initialization as init\n from ...activations import ACT2FN\n-from ...integrations import use_kernel_func_from_hub\n+from ...integrations import use_kernel_func_from_hub, use_kernelized_func\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, CausalLMOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n@@ -259,6 +259,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class ParakeetEncoderAttention(nn.Module):\n     \"\"\"Multi-head attention with relative positional encoding. See section 3.3 of https://huggingface.co/papers/1901.02860.\"\"\"\n \n@@ -284,7 +285,6 @@ def __init__(self, config: ParakeetEncoderConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n         # W_{k,R} projection\n         self.relative_k_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=False)\n         # global content bias"
        },
        {
            "sha": "23c060f0a11fcd06d6788cc595848f67f9e7d4b4",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -13,7 +13,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_func_from_hub\n+from ...integrations import use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import (\n     GenericForSequenceClassification,\n@@ -172,6 +172,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class PhiAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -187,7 +188,6 @@ def __init__(self, config: PhiConfig, layer_idx: int):\n         self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=True)\n         self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)\n-        self.rotary_fn = apply_rotary_pos_emb\n         self.dense = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=True)\n         self.rotary_ndims = int(self.head_dim * config.rope_parameters[\"partial_rotary_factor\"])\n         self.qk_layernorm = config.qk_layernorm"
        },
        {
            "sha": "752af7a192575e76e8f78bf080e69bf5e744fde6",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -30,7 +30,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_layers import GenericForSequenceClassification, GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n@@ -194,6 +194,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class PhimoeAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -219,7 +220,6 @@ def __init__(self, config: PhimoeConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "1d4f6a6baf6bcbb060b86c732fece3923d62fcc9",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -13,7 +13,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import (\n@@ -185,6 +185,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class Qwen2Attention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -202,7 +203,6 @@ def __init__(self, config: Qwen2Config, layer_idx: int):\n         self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n-        self.rotary_fn = apply_rotary_pos_emb\n         self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None\n \n     def forward("
        },
        {
            "sha": "f5d484dd3ef7805aba240597d243e0f808f5b0a5",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -35,7 +35,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_layers import (\n     GenericForQuestionAnswering,\n@@ -227,6 +227,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class Qwen2MoeAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -244,7 +245,6 @@ def __init__(self, config: Qwen2MoeConfig, layer_idx: int):\n         self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.qkv_bias)\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.qkv_bias)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n-        self.rotary_fn = apply_rotary_pos_emb\n         if self.config.layer_types[layer_idx] == \"sliding_attention\":\n             self.sliding_window = config.sliding_window\n "
        },
        {
            "sha": "b0f8084f8da6a4adab64188c518278a248c3593b",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -28,7 +28,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import (\n@@ -221,6 +221,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class Qwen3Attention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -247,7 +248,6 @@ def __init__(self, config: Qwen3Config, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n         self.q_norm = Qwen3RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # unlike olmo, only on the head dim!\n         self.k_norm = Qwen3RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # thus post q_norm does not need reshape\n         self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None"
        },
        {
            "sha": "66ceff7ae5e8a43ccb21f2c031ad2f6a702b866f",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -30,7 +30,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import (\n@@ -121,6 +121,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class Qwen3MoeAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -146,7 +147,6 @@ def __init__(self, config: Qwen3MoeConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n         self.q_norm = Qwen3MoeRMSNorm(self.head_dim, eps=config.rms_norm_eps)  # unlike olmo, only on the head dim!\n         self.k_norm = Qwen3MoeRMSNorm(self.head_dim, eps=config.rms_norm_eps)  # thus post q_norm does not need reshape\n         self.sliding_window = getattr(config, \"sliding_window\", None)"
        },
        {
            "sha": "8c066f2de40a1dba7ff643ed43f1d91ed6485a9f",
            "filename": "src/transformers/models/qwen3_next/modeling_qwen3_next.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -30,6 +30,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import (\n@@ -347,6 +348,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class Qwen3NextAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -371,7 +373,6 @@ def __init__(self, config: Qwen3NextConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n         self.q_norm = Qwen3NextRMSNorm(self.head_dim, eps=config.rms_norm_eps)  # unlike olmo, only on the head dim!\n         self.k_norm = Qwen3NextRMSNorm(\n             self.head_dim, eps=config.rms_norm_eps"
        },
        {
            "sha": "213230d660b9308aa10687d48848ca0f52168a73",
            "filename": "src/transformers/models/qwen3_omni_moe/modeling_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -35,7 +35,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -1443,6 +1443,7 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     return q_embed, k_embed\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class Qwen3OmniMoeThinkerTextAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -1468,7 +1469,6 @@ def __init__(self, config, layer_idx):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n         self.q_norm = Qwen3OmniMoeThinkerTextRMSNorm(\n             self.head_dim, eps=config.rms_norm_eps\n         )  # unlike olmo, only on the head dim!\n@@ -2321,6 +2321,7 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class Qwen3OmniMoeTalkerCodePredictorAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -2347,7 +2348,6 @@ def __init__(self, config: Qwen3OmniMoeConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n         self.q_norm = Qwen3OmniMoeRMSNorm(self.head_dim, eps=config.rms_norm_eps)  # unlike olmo, only on the head dim!\n         self.k_norm = Qwen3OmniMoeRMSNorm(\n             self.head_dim, eps=config.rms_norm_eps\n@@ -3350,6 +3350,7 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class Qwen3OmniMoeCode2WavAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -3376,7 +3377,6 @@ def __init__(self, config: Qwen3OmniMoeCode2WavConfig, layer_idx):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n         self.q_norm = nn.Identity()\n         self.k_norm = nn.Identity()\n         self.sliding_window = config.sliding_window"
        },
        {
            "sha": "0938d540742aefb3f772ee2c3009a8b751f32928",
            "filename": "src/transformers/models/qwen3_vl/modeling_qwen3_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -30,7 +30,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -413,6 +413,7 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     return q_embed, k_embed\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class Qwen3VLTextAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -439,7 +440,6 @@ def __init__(self, config: Qwen3VLTextConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n         self.q_norm = Qwen3VLTextRMSNorm(self.head_dim, eps=config.rms_norm_eps)  # unlike olmo, only on the head dim!\n         self.k_norm = Qwen3VLTextRMSNorm(\n             self.head_dim, eps=config.rms_norm_eps"
        },
        {
            "sha": "07bdadc9b18f23abad5c3d3a1cafe9c398626776",
            "filename": "src/transformers/models/qwen3_vl_moe/modeling_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -31,7 +31,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -226,6 +226,7 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     return q_embed, k_embed\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class Qwen3VLMoeTextAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -252,7 +253,6 @@ def __init__(self, config: Qwen3VLMoeTextConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n         self.q_norm = Qwen3VLMoeTextRMSNorm(\n             self.head_dim, eps=config.rms_norm_eps\n         )  # unlike olmo, only on the head dim!"
        },
        {
            "sha": "05445e8128554044f9f72a75163dd0bd5e1f6094",
            "filename": "src/transformers/models/smollm3/modeling_smollm3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -28,7 +28,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import (\n@@ -184,6 +184,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class SmolLM3Attention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -209,7 +210,6 @@ def __init__(self, config: SmolLM3Config, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n \n         self.use_rope = config.no_rope_layers[layer_idx]\n         self.sliding_window = ("
        },
        {
            "sha": "ce504c5947acb7706548ea6de0519ba29ce7be49",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -35,7 +35,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_func_from_hub\n+from ...integrations import use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import (\n@@ -141,6 +141,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class Starcoder2Attention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -157,7 +158,6 @@ def __init__(self, config: Starcoder2Config, layer_idx: Optional[int] = None):\n         self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.use_bias)\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.use_bias)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.use_bias)\n-        self.rotary_fn = apply_rotary_pos_emb\n         self.residual_dropout = config.residual_dropout\n \n     def forward("
        },
        {
            "sha": "a7be0f0e465f339c2ddf737483144e9a1b703d29",
            "filename": "src/transformers/models/t5gemma/modeling_t5gemma.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -29,7 +29,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_func_from_hub\n+from ...integrations import use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -238,6 +238,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class T5GemmaSelfAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -265,7 +266,6 @@ def __init__(self, config: T5GemmaModuleConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n         self.attn_logit_softcapping = self.config.attn_logit_softcapping\n         self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None\n \n@@ -315,6 +315,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class T5GemmaCrossAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -341,7 +342,6 @@ def __init__(self, config: T5GemmaModuleConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n         self.attn_logit_softcapping = self.config.attn_logit_softcapping\n \n         if config.cross_attention_hidden_size is None:"
        },
        {
            "sha": "09935adf509f155ab580870924db636b683c02f6",
            "filename": "src/transformers/models/t5gemma2/modeling_t5gemma2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Ft5gemma2%2Fmodeling_t5gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Ft5gemma2%2Fmodeling_t5gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma2%2Fmodeling_t5gemma2.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -30,7 +30,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache, StaticCache\n from ...generation import GenerationConfig, GenerationMixin, GenerationMode\n-from ...integrations import use_kernel_func_from_hub\n+from ...integrations import use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_bidirectional_mask, create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -253,6 +253,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class T5Gemma2SelfAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -279,7 +280,6 @@ def __init__(self, config: T5Gemma2TextConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n         self.attn_logit_softcapping = self.config.attn_logit_softcapping\n         self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None\n         self.is_sliding = self.layer_type == \"sliding_attention\"\n@@ -335,6 +335,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class T5Gemma2MergedAttention(nn.Module):\n     \"\"\"Merged self-attention and cross-attention for decoder.\"\"\"\n \n@@ -361,7 +362,6 @@ def __init__(self, config: T5Gemma2TextConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n         self.attn_logit_softcapping = self.config.attn_logit_softcapping\n         self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None\n         self.is_sliding = self.layer_type == \"sliding_attention\""
        },
        {
            "sha": "4be80f4573d3b241785dda21b498985da1704dea",
            "filename": "src/transformers/models/vaultgemma/modeling_vaultgemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fmodeling_vaultgemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1ac18258d1060199a7e593b3a0fc531b809e601/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fmodeling_vaultgemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fmodeling_vaultgemma.py?ref=c1ac18258d1060199a7e593b3a0fc531b809e601",
            "patch": "@@ -29,7 +29,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_func_from_hub\n+from ...integrations import use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -160,6 +160,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class VaultGemmaAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -186,7 +187,6 @@ def __init__(self, config: VaultGemmaConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.rotary_fn = apply_rotary_pos_emb\n         self.attn_logit_softcapping = self.config.attn_logit_softcapping\n         self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None\n "
        }
    ],
    "stats": {
        "total": 293,
        "additions": 167,
        "deletions": 126
    }
}