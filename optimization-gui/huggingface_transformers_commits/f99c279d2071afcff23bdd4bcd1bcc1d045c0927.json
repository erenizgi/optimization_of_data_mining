{
    "author": "cyyever",
    "message": "Remove deprecated code (#37059)\n\n* Remove deprecated code\n\n* fix get_loading_attributes\n\n* fix error\n\n* skip test\n\n---------\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "f99c279d2071afcff23bdd4bcd1bcc1d045c0927",
    "files": [
        {
            "sha": "a201479ae8f96c210b8200e854f118fa8a3360b2",
            "filename": "examples/pytorch/language-modeling/run_fim.py",
            "status": "modified",
            "additions": 8,
            "deletions": 4,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/f99c279d2071afcff23bdd4bcd1bcc1d045c0927/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f99c279d2071afcff23bdd4bcd1bcc1d045c0927/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim.py?ref=f99c279d2071afcff23bdd4bcd1bcc1d045c0927",
            "patch": "@@ -47,7 +47,7 @@\n     Trainer,\n     TrainingArguments,\n     default_data_collator,\n-    is_torch_tpu_available,\n+    is_torch_xla_available,\n     set_seed,\n )\n from transformers.integrations import is_deepspeed_zero3_enabled\n@@ -525,7 +525,7 @@ def main():\n     if torch.cuda.is_availble():\n         pad_factor = 8\n \n-    elif is_torch_tpu_available():\n+    elif is_torch_xla_available(check_is_tpu=True):\n         pad_factor = 128\n \n     # Add the new tokens to the tokenizer\n@@ -795,9 +795,13 @@ def compute_metrics(eval_preds):\n         processing_class=tokenizer,\n         # Data collator will default to DataCollatorWithPadding, so we change it.\n         data_collator=default_data_collator,\n-        compute_metrics=compute_metrics if training_args.do_eval and not is_torch_tpu_available() else None,\n+        compute_metrics=compute_metrics\n+        if training_args.do_eval and not is_torch_xla_available(check_is_tpu=True)\n+        else None,\n         preprocess_logits_for_metrics=(\n-            preprocess_logits_for_metrics if training_args.do_eval and not is_torch_tpu_available() else None\n+            preprocess_logits_for_metrics\n+            if training_args.do_eval and not is_torch_xla_available(check_is_tpu=True)\n+            else None\n         ),\n     )\n "
        },
        {
            "sha": "44d083bc63139939d9acd94c26b968320ce2e8fe",
            "filename": "examples/pytorch/language-modeling/run_fim_no_trainer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f99c279d2071afcff23bdd4bcd1bcc1d045c0927/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f99c279d2071afcff23bdd4bcd1bcc1d045c0927/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim_no_trainer.py?ref=f99c279d2071afcff23bdd4bcd1bcc1d045c0927",
            "patch": "@@ -52,7 +52,7 @@\n     SchedulerType,\n     default_data_collator,\n     get_scheduler,\n-    is_torch_tpu_available,\n+    is_torch_xla_available,\n )\n from transformers.integrations import is_deepspeed_zero3_enabled\n from transformers.utils import check_min_version, send_example_telemetry\n@@ -492,7 +492,7 @@ def main():\n     if torch.cuda.is_availble():\n         pad_factor = 8\n \n-    elif is_torch_tpu_available():\n+    elif is_torch_xla_available(check_is_tpu=True):\n         pad_factor = 128\n \n     # Add the new tokens to the tokenizer"
        },
        {
            "sha": "d07618a4926487c6f03a1b65121f5de9625b3664",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f99c279d2071afcff23bdd4bcd1bcc1d045c0927/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f99c279d2071afcff23bdd4bcd1bcc1d045c0927/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=f99c279d2071afcff23bdd4bcd1bcc1d045c0927",
            "patch": "@@ -1037,7 +1037,6 @@\n         \"is_torch_musa_available\",\n         \"is_torch_neuroncore_available\",\n         \"is_torch_npu_available\",\n-        \"is_torch_tpu_available\",\n         \"is_torchvision_available\",\n         \"is_torch_xla_available\",\n         \"is_torch_xpu_available\",\n@@ -6341,7 +6340,6 @@\n         is_torch_musa_available,\n         is_torch_neuroncore_available,\n         is_torch_npu_available,\n-        is_torch_tpu_available,\n         is_torch_xla_available,\n         is_torch_xpu_available,\n         is_torchvision_available,"
        },
        {
            "sha": "5b0ba3f9122f17c4355c94f471af5720bde2d814",
            "filename": "src/transformers/image_transforms.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/f99c279d2071afcff23bdd4bcd1bcc1d045c0927/src%2Ftransformers%2Fimage_transforms.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f99c279d2071afcff23bdd4bcd1bcc1d045c0927/src%2Ftransformers%2Fimage_transforms.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_transforms.py?ref=f99c279d2071afcff23bdd4bcd1bcc1d045c0927",
            "patch": "@@ -12,7 +12,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import warnings\n from collections.abc import Collection, Iterable\n from math import ceil\n from typing import Optional, Union\n@@ -453,7 +452,6 @@ def center_crop(\n     size: tuple[int, int],\n     data_format: Optional[Union[str, ChannelDimension]] = None,\n     input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    return_numpy: Optional[bool] = None,\n ) -> np.ndarray:\n     \"\"\"\n     Crops the `image` to the specified `size` using a center crop. Note that if the image is too small to be cropped to\n@@ -474,22 +472,11 @@ def center_crop(\n                 - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n                 - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n             If unset, will use the inferred format of the input image.\n-        return_numpy (`bool`, *optional*):\n-            Whether or not to return the cropped image as a numpy array. Used for backwards compatibility with the\n-            previous ImageFeatureExtractionMixin method.\n-                - Unset: will return the same type as the input image.\n-                - `True`: will return a numpy array.\n-                - `False`: will return a `PIL.Image.Image` object.\n     Returns:\n         `np.ndarray`: The cropped image.\n     \"\"\"\n     requires_backends(center_crop, [\"vision\"])\n \n-    if return_numpy is not None:\n-        warnings.warn(\"return_numpy is deprecated and will be removed in v.4.33\", FutureWarning)\n-\n-    return_numpy = True if return_numpy is None else return_numpy\n-\n     if not isinstance(image, np.ndarray):\n         raise TypeError(f\"Input image must be of type np.ndarray, got {type(image)}\")\n \n@@ -541,9 +528,6 @@ def center_crop(\n     new_image = new_image[..., max(0, top) : min(new_height, bottom), max(0, left) : min(new_width, right)]\n     new_image = to_channel_dimension_format(new_image, output_data_format, ChannelDimension.FIRST)\n \n-    if not return_numpy:\n-        new_image = to_pil_image(new_image)\n-\n     return new_image\n \n "
        },
        {
            "sha": "a427cb18f5b763a38eafe5e0f8050b25d1e35123",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f99c279d2071afcff23bdd4bcd1bcc1d045c0927/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f99c279d2071afcff23bdd4bcd1bcc1d045c0927/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=f99c279d2071afcff23bdd4bcd1bcc1d045c0927",
            "patch": "@@ -228,7 +228,6 @@\n     is_torch_sdpa_available,\n     is_torch_tensorrt_fx_available,\n     is_torch_tf32_available,\n-    is_torch_tpu_available,\n     is_torch_xla_available,\n     is_torch_xpu_available,\n     is_torchao_available,"
        },
        {
            "sha": "f98403ded287229792c3c09f994aba9a721892dc",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/f99c279d2071afcff23bdd4bcd1bcc1d045c0927/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f99c279d2071afcff23bdd4bcd1bcc1d045c0927/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=f99c279d2071afcff23bdd4bcd1bcc1d045c0927",
            "patch": "@@ -675,31 +675,6 @@ def is_g2p_en_available():\n     return _g2p_en_available\n \n \n-@lru_cache()\n-def is_torch_tpu_available(check_device=True):\n-    \"Checks if `torch_xla` is installed and potentially if a TPU is in the environment\"\n-    warnings.warn(\n-        \"`is_torch_tpu_available` is deprecated and will be removed in 4.41.0. \"\n-        \"Please use the `is_torch_xla_available` instead.\",\n-        FutureWarning,\n-    )\n-\n-    if not _torch_available:\n-        return False\n-    if importlib.util.find_spec(\"torch_xla\") is not None:\n-        if check_device:\n-            # We need to check if `xla_device` can be found, will raise a RuntimeError if not\n-            try:\n-                import torch_xla.core.xla_model as xm\n-\n-                _ = xm.xla_device()\n-                return True\n-            except RuntimeError:\n-                return False\n-        return True\n-    return False\n-\n-\n @lru_cache\n def is_torch_xla_available(check_is_tpu=False, check_is_gpu=False):\n     \"\"\""
        },
        {
            "sha": "7128677a79dbcb27e954f065a5ef0f7eaa9c1813",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 2,
            "deletions": 15,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/f99c279d2071afcff23bdd4bcd1bcc1d045c0927/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f99c279d2071afcff23bdd4bcd1bcc1d045c0927/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=f99c279d2071afcff23bdd4bcd1bcc1d045c0927",
            "patch": "@@ -682,15 +682,13 @@ def __init__(\n         self.use_exllama = use_exllama\n         self.max_input_length = max_input_length\n         self.exllama_config = exllama_config\n-        self.disable_exllama = kwargs.pop(\"disable_exllama\", None)\n         self.cache_block_outputs = cache_block_outputs\n         self.modules_in_block_to_quantize = modules_in_block_to_quantize\n         self.post_init()\n \n     def get_loading_attributes(self):\n         attibutes_dict = copy.deepcopy(self.__dict__)\n         loading_attibutes = [\n-            \"disable_exllama\",\n             \"use_exllama\",\n             \"exllama_config\",\n             \"use_cuda_fp16\",\n@@ -739,20 +737,9 @@ def post_init(self):\n                 self.use_exllama = False\n \n         # auto-gptq specific kernel control logic\n-        if self.disable_exllama is None and self.use_exllama is None:\n+        if self.use_exllama is None:\n             # New default behaviour\n             self.use_exllama = True\n-        elif self.disable_exllama is not None and self.use_exllama is None:\n-            # Follow pattern of old config\n-            logger.warning(\n-                \"Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.\"\n-                \"The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\"\n-            )\n-            self.use_exllama = not self.disable_exllama\n-            self.disable_exllama = None\n-        elif self.disable_exllama is not None and self.use_exllama is not None:\n-            # Only happens if user explicitly passes in both arguments\n-            raise ValueError(\"Cannot specify both `disable_exllama` and `use_exllama`. Please use just `use_exllama`\")\n \n         if self.exllama_config is None:\n             self.exllama_config = {\"version\": ExllamaVersion.ONE}\n@@ -809,7 +796,7 @@ def from_dict_optimum(cls, config_dict):\n         if \"disable_exllama\" in config_dict:\n             config_dict[\"use_exllama\"] = not config_dict[\"disable_exllama\"]\n             # switch to None to not trigger the warning\n-            config_dict[\"disable_exllama\"] = None\n+            config_dict.pop(\"disable_exllama\")\n \n         config = cls(**config_dict)\n         return config"
        },
        {
            "sha": "2387e5f25ff1148498e926d2d5cebb32e43b8ee6",
            "filename": "tests/models/seamless_m4t_v2/test_modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f99c279d2071afcff23bdd4bcd1bcc1d045c0927/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f99c279d2071afcff23bdd4bcd1bcc1d045c0927/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py?ref=f99c279d2071afcff23bdd4bcd1bcc1d045c0927",
            "patch": "@@ -592,7 +592,7 @@ def test_attention_outputs(self):\n     # TODO: @ydshieh: refer to #34968\n     @unittest.skip(reason=\"Failing on multi-gpu runner\")\n     def test_retain_grad_hidden_states_attentions(self):\n-        pass\n+        self.skipTest(reason=\"Failing on multi-gpu runner\")\n \n \n @require_torch"
        }
    ],
    "stats": {
        "total": 79,
        "additions": 13,
        "deletions": 66
    }
}