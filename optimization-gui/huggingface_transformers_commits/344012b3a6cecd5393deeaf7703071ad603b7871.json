{
    "author": "zucchini-nlp",
    "message": "[qwen2 vl] fix packing with all attentions (#39447)\n\n* fix qwen2 vl packing in FA2\n\n* why? delete!\n\n* qwen2-5-vl seems to work now\n\n* update\n\n* fix tests\n\n* start by adapting FA2 tests\n\n* add similar tests for sdpa/eager\n\n* address comments\n\n* why is this even in conditional model and not base model?",
    "sha": "344012b3a6cecd5393deeaf7703071ad603b7871",
    "files": [
        {
            "sha": "c9584dfe3aecc86757cff3d168fe09c3574373b5",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 42,
            "deletions": 7,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/344012b3a6cecd5393deeaf7703071ad603b7871/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/344012b3a6cecd5393deeaf7703071ad603b7871/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=344012b3a6cecd5393deeaf7703071ad603b7871",
            "patch": "@@ -40,7 +40,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, check_torch_load_is_safe, logging\n+from ...utils import TransformersKwargs, auto_docstring, check_torch_load_is_safe, logging\n from ...utils.hub import cached_file\n from .configuration_qwen2_5_omni import (\n     Qwen2_5OmniAudioEncoderConfig,\n@@ -1424,6 +1424,7 @@ def forward(\n             dropout=0.0 if not self.training else self.attention_dropout,\n             scaling=self.scaling,\n             sliding_window=self.sliding_window,\n+            position_ids=position_ids,  # pass positions for FA2\n             **kwargs,\n         )\n \n@@ -1607,9 +1608,25 @@ def forward(\n         # the hard coded `3` is for temporal, height and width.\n         if position_ids is None:\n             position_ids = cache_position.view(1, 1, -1).expand(3, inputs_embeds.shape[0], -1)\n-        elif position_ids.dim() == 2:\n+        elif position_ids.ndim == 2:\n             position_ids = position_ids[None, ...].expand(3, position_ids.shape[0], -1)\n \n+        # NOTE: we need to pass text position ids for packing. Qwen2-VL uses 3D positions\n+        # where each dim indicates visual spatial positions for temporal/height/width grids.\n+        # There are two scenarios when FA2-like packed masking might be activated.\n+        # 1. User specifically passed packed `position_ids` and no attention mask.\n+        #    In this case we expect the useer to create correct position ids for all 3 grids\n+        #    and prepend text-only position ids to it. The final tensor will be [4, bs, seq-len]\n+        # 2. User runs forward with no attention mask and no position ids. In this case, position ids\n+        #    are prepared by the model (`get_rope_index`) as `[4, bs, seq-len]` tensor. Text-only positions are\n+        #    prepended by us when creating positions so that the mask is constructed correctly. NOTE: failing to pass\n+        #    text-only positions will cause incorrect mask construction, do not change `prepare_input_for_generation`\n+        if position_ids.ndim == 3 and position_ids.shape[0] == 4:\n+            text_position_ids = position_ids[0]\n+            position_ids = position_ids[1:]\n+        else:\n+            text_position_ids = position_ids[0]\n+\n         # It may already have been prepared by e.g. `generate`\n         if not isinstance(causal_mask_mapping := attention_mask, dict):\n             # Prepare mask arguments\n@@ -1619,7 +1636,7 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n-                \"position_ids\": position_ids,\n+                \"position_ids\": text_position_ids,\n             }\n             # Create the masks\n             causal_mask_mapping = {\n@@ -1645,7 +1662,7 @@ def forward(\n             layer_outputs = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n-                position_ids=position_ids,\n+                position_ids=text_position_ids,\n                 past_key_value=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n@@ -1804,6 +1821,7 @@ def forward(\n         use_audio_in_video: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         video_second_per_grid: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Qwen2_5OmniThinkerCausalLMOutputWithPast]:\n         r\"\"\"\n         image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n@@ -1959,6 +1977,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -2146,9 +2165,25 @@ def forward(\n         # the hard coded `3` is for temporal, height and width.\n         if position_ids is None:\n             position_ids = cache_position.view(1, 1, -1).expand(3, inputs_embeds.shape[0], -1)\n-        elif position_ids.dim() == 2:\n+        elif position_ids.ndim == 2:\n             position_ids = position_ids[None, ...].expand(3, position_ids.shape[0], -1)\n \n+        # NOTE: we need to pass text position ids for packing. Qwen2-VL uses 3D positions\n+        # where each dim indicates visual spatial positions for temporal/height/width grids.\n+        # There are two scenarios when FA2-like packed masking might be activated.\n+        # 1. User specifically passed packed `position_ids` and no attention mask.\n+        #    In this case we expect the useer to create correct position ids for all 3 grids\n+        #    and prepend text-only position ids to it. The final tensor will be [4, bs, seq-len]\n+        # 2. User runs forward with no attention mask and no position ids. In this case, position ids\n+        #    are prepared by the model (`get_rope_index`) as `[4, bs, seq-len]` tensor. Text-only positions are\n+        #    prepended by us when creating positions so that the mask is constructed correctly. NOTE: failing to pass\n+        #    text-only positions will cause incorrect mask construction, do not change `prepare_input_for_generation`\n+        if position_ids.ndim == 3 and position_ids.shape[0] == 4:\n+            text_position_ids = position_ids[0]\n+            position_ids = position_ids[1:]\n+        else:\n+            text_position_ids = position_ids[0]\n+\n         # It may already have been prepared by e.g. `generate`\n         if not isinstance(causal_mask_mapping := attention_mask, dict):\n             # Prepare mask arguments\n@@ -2158,7 +2193,7 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n-                \"position_ids\": position_ids,\n+                \"position_ids\": text_position_ids,\n             }\n             # Create the masks\n             causal_mask_mapping = {\n@@ -2184,7 +2219,7 @@ def forward(\n             layer_outputs = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n-                position_ids=position_ids,\n+                position_ids=text_position_ids,\n                 past_key_value=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,"
        },
        {
            "sha": "cfd0e29c9733239d3459d07f568b0998fcdd013f",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/344012b3a6cecd5393deeaf7703071ad603b7871/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/344012b3a6cecd5393deeaf7703071ad603b7871/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=344012b3a6cecd5393deeaf7703071ad603b7871",
            "patch": "@@ -49,7 +49,9 @@\n from ...modeling_outputs import BaseModelOutput, ModelOutput\n from ...modeling_rope_utils import rope_config_validation\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...processing_utils import Unpack\n from ...utils import (\n+    TransformersKwargs,\n     auto_docstring,\n     check_torch_load_is_safe,\n     logging,\n@@ -2259,6 +2261,7 @@ def forward(\n         use_audio_in_video: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         video_second_per_grid: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Qwen2_5OmniThinkerCausalLMOutputWithPast]:\n         r\"\"\"\n         image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n@@ -2414,6 +2417,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]"
        },
        {
            "sha": "723fde6d843491a96e32c1977d14aae89136e4ac",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 57,
            "deletions": 27,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/344012b3a6cecd5393deeaf7703071ad603b7871/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/344012b3a6cecd5393deeaf7703071ad603b7871/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=344012b3a6cecd5393deeaf7703071ad603b7871",
            "patch": "@@ -710,6 +710,7 @@ def forward(\n             dropout=0.0 if not self.training else self.attention_dropout,\n             scaling=self.scaling,\n             sliding_window=self.sliding_window,\n+            position_ids=position_ids,  # pass positions for FA2\n             **kwargs,\n         )\n \n@@ -878,9 +879,25 @@ def forward(\n         # the hard coded `3` is for temporal, height and width.\n         if position_ids is None:\n             position_ids = cache_position.view(1, 1, -1).expand(3, inputs_embeds.shape[0], -1)\n-        elif position_ids.dim() == 2:\n+        elif position_ids.ndim == 2:\n             position_ids = position_ids[None, ...].expand(3, position_ids.shape[0], -1)\n \n+        # NOTE: we need to pass text position ids for packing. Qwen2-VL uses 3D positions\n+        # where each dim indicates visual spatial positions for temporal/height/width grids.\n+        # There are two scenarios when FA2-like packed masking might be activated.\n+        # 1. User specifically passed packed `position_ids` and no attention mask.\n+        #    In this case we expect the useer to create correct position ids for all 3 grids\n+        #    and prepend text-only position ids to it. The final tensor will be [4, bs, seq-len]\n+        # 2. User runs forward with no attention mask and no position ids. In this case, position ids\n+        #    are prepared by the model (`get_rope_index`) as `[4, bs, seq-len]` tensor. Text-only positions are\n+        #    prepended by us when creating positions so that the mask is constructed correctly. NOTE: failing to pass\n+        #    text-only positions will cause incorrect mask construction, do not change `prepare_input_for_generation`\n+        if position_ids.ndim == 3 and position_ids.shape[0] == 4:\n+            text_position_ids = position_ids[0]\n+            position_ids = position_ids[1:]\n+        else:\n+            text_position_ids = position_ids[0]\n+\n         # It may already have been prepared by e.g. `generate`\n         if not isinstance(causal_mask_mapping := attention_mask, dict):\n             # Prepare mask arguments\n@@ -890,7 +907,7 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n-                \"position_ids\": position_ids,\n+                \"position_ids\": text_position_ids,\n             }\n             # Create the masks\n             causal_mask_mapping = {\n@@ -916,7 +933,7 @@ def forward(\n             layer_outputs = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n-                position_ids=position_ids,\n+                position_ids=text_position_ids,\n                 past_key_value=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n@@ -1279,16 +1296,6 @@ def forward(\n             inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n \n         if position_ids is None:\n-            attention_mask_tensor = (\n-                attention_mask if not isinstance(attention_mask, dict) else attention_mask[\"full_attention\"]\n-            )\n-            if attention_mask_tensor is not None and attention_mask_tensor.ndim == 4:\n-                attention_mask_tensor = torch.diagonal(attention_mask_tensor[:, 0], dim1=1, dim2=2)\n-                # Only apply conversion for floating point tensors (inverted masks)\n-                if attention_mask_tensor.dtype.is_floating_point:\n-                    attention_mask_tensor = attention_mask_tensor / torch.finfo(attention_mask_tensor.dtype).min\n-                    attention_mask_tensor = (1.0 - attention_mask_tensor).int()\n-\n             # Calculate RoPE index once per generation in the pre-fill stage only.\n             # When compiling, we can't check tensor values thus we check only input length\n             # It is safe to assume that `length!=1` means we're in pre-fill because compiled\n@@ -1307,23 +1314,19 @@ def forward(\n                     image_grid_thw,\n                     video_grid_thw,\n                     second_per_grid_ts=second_per_grid_ts,\n-                    attention_mask=attention_mask_tensor,\n+                    attention_mask=attention_mask,\n                 )\n                 self.rope_deltas = rope_deltas\n-            # then use the prev pre-calculated rope-deltas to get the correct position ids\n             else:\n                 batch_size, seq_length, _ = inputs_embeds.shape\n-                delta = (\n-                    (cache_position[0] + self.rope_deltas).to(inputs_embeds.device)\n-                    if cache_position is not None\n-                    else 0\n-                )\n                 position_ids = torch.arange(seq_length, device=inputs_embeds.device)\n-                position_ids = position_ids.view(1, -1).expand(batch_size, -1)\n-                if cache_position is not None:  # otherwise `deltas` is an int `0`\n-                    delta = delta.repeat_interleave(batch_size // delta.shape[0], dim=0)\n-                position_ids = position_ids.add(delta)\n-                position_ids = position_ids.unsqueeze(0).expand(3, -1, -1)\n+                position_ids = position_ids.view(1, 1, -1).expand(3, batch_size, -1)\n+                if cache_position is not None:\n+                    delta = (cache_position[0] + self.rope_deltas).to(inputs_embeds.device)\n+                else:\n+                    delta = torch.zeros((batch_size, seq_length), device=inputs_embeds.device)\n+                delta = delta.repeat_interleave(batch_size // delta.shape[0], dim=1)\n+                position_ids += delta.to(position_ids.device)\n \n         outputs = self.language_model(\n             input_ids=None,\n@@ -1573,8 +1576,35 @@ def prepare_inputs_for_generation(\n             **kwargs,\n         )\n \n-        # Qwen2-5-VL position_ids are prepareed with rope_deltas in forward\n-        model_inputs[\"position_ids\"] = None\n+        # Qwen2-5-VL position_ids are prepared with rope_deltas\n+        if position_ids is None:\n+            # Calculate RoPE index once per generation in the pre-fill stage only.\n+            # When compiling, we can't check tensor values thus we check only input length\n+            # It is safe to assume that `length!=1` means we're in pre-fill because compiled\n+            # models currently cannot do asssisted decoding\n+            if cache_position[0] == 0 or self.model.rope_deltas is None:\n+                vision_positions, rope_deltas = self.model.get_rope_index(\n+                    model_inputs.get(\"input_ids\", None),\n+                    image_grid_thw=image_grid_thw,\n+                    video_grid_thw=video_grid_thw,\n+                    second_per_grid_ts=second_per_grid_ts,\n+                    attention_mask=attention_mask,\n+                )\n+                self.model.rope_deltas = rope_deltas\n+            # then use the prev pre-calculated rope-deltas to get the correct position ids\n+            elif \"position_ids\" in model_inputs:\n+                position_ids = model_inputs[\"position_ids\"][None, ...]\n+                delta = self.model.rope_deltas\n+                delta = delta.repeat_interleave(position_ids.shape[1] // delta.shape[0], dim=0)\n+                vision_positions = position_ids + delta.expand_as(position_ids)\n+                vision_positions = vision_positions.expand(3, vision_positions.shape[1], -1)\n+\n+            # Concatenate \"text + vision\" positions into [4, bs, seq-len]\n+            if \"position_ids\" not in model_inputs:\n+                text_positions = torch.arange(input_ids, device=input_ids.device)[None, None, :]\n+            else:\n+                text_positions = model_inputs[\"position_ids\"][None, ...]\n+            model_inputs[\"position_ids\"] = torch.cat([text_positions, vision_positions], dim=0)\n \n         if cache_position[0] != 0:\n             model_inputs[\"pixel_values\"] = None"
        },
        {
            "sha": "550df3750a1aac2cc88202998d59b3f0b424ba1d",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 37,
            "deletions": 24,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/344012b3a6cecd5393deeaf7703071ad603b7871/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/344012b3a6cecd5393deeaf7703071ad603b7871/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=344012b3a6cecd5393deeaf7703071ad603b7871",
            "patch": "@@ -630,16 +630,6 @@ def forward(\n             inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n \n         if position_ids is None:\n-            attention_mask_tensor = (\n-                attention_mask if not isinstance(attention_mask, dict) else attention_mask[\"full_attention\"]\n-            )\n-            if attention_mask_tensor is not None and attention_mask_tensor.ndim == 4:\n-                attention_mask_tensor = torch.diagonal(attention_mask_tensor[:, 0], dim1=1, dim2=2)\n-                # Only apply conversion for floating point tensors (inverted masks)\n-                if attention_mask_tensor.dtype.is_floating_point:\n-                    attention_mask_tensor = attention_mask_tensor / torch.finfo(attention_mask_tensor.dtype).min\n-                    attention_mask_tensor = (1.0 - attention_mask_tensor).int()\n-\n             # Calculate RoPE index once per generation in the pre-fill stage only.\n             # When compiling, we can't check tensor values thus we check only input length\n             # It is safe to assume that `length!=1` means we're in pre-fill because compiled\n@@ -658,23 +648,19 @@ def forward(\n                     image_grid_thw,\n                     video_grid_thw,\n                     second_per_grid_ts=second_per_grid_ts,\n-                    attention_mask=attention_mask_tensor,\n+                    attention_mask=attention_mask,\n                 )\n                 self.rope_deltas = rope_deltas\n-            # then use the prev pre-calculated rope-deltas to get the correct position ids\n             else:\n                 batch_size, seq_length, _ = inputs_embeds.shape\n-                delta = (\n-                    (cache_position[0] + self.rope_deltas).to(inputs_embeds.device)\n-                    if cache_position is not None\n-                    else 0\n-                )\n                 position_ids = torch.arange(seq_length, device=inputs_embeds.device)\n-                position_ids = position_ids.view(1, -1).expand(batch_size, -1)\n-                if cache_position is not None:  # otherwise `deltas` is an int `0`\n-                    delta = delta.repeat_interleave(batch_size // delta.shape[0], dim=0)\n-                position_ids = position_ids.add(delta)\n-                position_ids = position_ids.unsqueeze(0).expand(3, -1, -1)\n+                position_ids = position_ids.view(1, 1, -1).expand(3, batch_size, -1)\n+                if cache_position is not None:\n+                    delta = (cache_position[0] + self.rope_deltas).to(inputs_embeds.device)\n+                else:\n+                    delta = torch.zeros((batch_size, seq_length), device=inputs_embeds.device)\n+                delta = delta.repeat_interleave(batch_size // delta.shape[0], dim=1)\n+                position_ids += delta.to(position_ids.device)\n \n         outputs = self.language_model(\n             input_ids=None,\n@@ -848,8 +834,35 @@ def prepare_inputs_for_generation(\n             **kwargs,\n         )\n \n-        # Qwen2-5-VL position_ids are prepareed with rope_deltas in forward\n-        model_inputs[\"position_ids\"] = None\n+        # Qwen2-5-VL position_ids are prepared with rope_deltas\n+        if position_ids is None:\n+            # Calculate RoPE index once per generation in the pre-fill stage only.\n+            # When compiling, we can't check tensor values thus we check only input length\n+            # It is safe to assume that `length!=1` means we're in pre-fill because compiled\n+            # models currently cannot do asssisted decoding\n+            if cache_position[0] == 0 or self.model.rope_deltas is None:\n+                vision_positions, rope_deltas = self.model.get_rope_index(\n+                    model_inputs.get(\"input_ids\", None),\n+                    image_grid_thw=image_grid_thw,\n+                    video_grid_thw=video_grid_thw,\n+                    second_per_grid_ts=second_per_grid_ts,\n+                    attention_mask=attention_mask,\n+                )\n+                self.model.rope_deltas = rope_deltas\n+            # then use the prev pre-calculated rope-deltas to get the correct position ids\n+            elif \"position_ids\" in model_inputs:\n+                position_ids = model_inputs[\"position_ids\"][None, ...]\n+                delta = self.model.rope_deltas\n+                delta = delta.repeat_interleave(position_ids.shape[1] // delta.shape[0], dim=0)\n+                vision_positions = position_ids + delta.expand_as(position_ids)\n+                vision_positions = vision_positions.expand(3, vision_positions.shape[1], -1)\n+\n+            # Concatenate \"text + vision\" positions into [4, bs, seq-len]\n+            if \"position_ids\" not in model_inputs:\n+                text_positions = torch.arange(input_ids, device=input_ids.device)[None, None, :]\n+            else:\n+                text_positions = model_inputs[\"position_ids\"][None, ...]\n+            model_inputs[\"position_ids\"] = torch.cat([text_positions, vision_positions], dim=0)\n \n         if cache_position[0] != 0:\n             model_inputs[\"pixel_values\"] = None"
        },
        {
            "sha": "cdda0d693872055c0561be60169d46906b12382b",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 64,
            "deletions": 35,
            "changes": 99,
            "blob_url": "https://github.com/huggingface/transformers/blob/344012b3a6cecd5393deeaf7703071ad603b7871/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/344012b3a6cecd5393deeaf7703071ad603b7871/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=344012b3a6cecd5393deeaf7703071ad603b7871",
            "patch": "@@ -558,6 +558,7 @@ def forward(\n             dropout=0.0 if not self.training else self.attention_dropout,\n             scaling=self.scaling,\n             sliding_window=self.sliding_window,\n+            position_ids=position_ids,  # pass positions for FA2\n             **kwargs,\n         )\n \n@@ -853,9 +854,25 @@ def forward(\n         # the hard coded `3` is for temporal, height and width.\n         if position_ids is None:\n             position_ids = cache_position.view(1, 1, -1).expand(3, inputs_embeds.shape[0], -1)\n-        elif position_ids.dim() == 2:\n+        elif position_ids.ndim == 2:\n             position_ids = position_ids[None, ...].expand(3, position_ids.shape[0], -1)\n \n+        # NOTE: we need to pass text position ids for packing. Qwen2-VL uses 3D positions\n+        # where each dim indicates visual spatial positions for temporal/height/width grids.\n+        # There are two scenarios when FA2-like packed masking might be activated.\n+        # 1. User specifically passed packed `position_ids` and no attention mask.\n+        #    In this case we expect the useer to create correct position ids for all 3 grids\n+        #    and prepend text-only position ids to it. The final tensor will be [4, bs, seq-len]\n+        # 2. User runs forward with no attention mask and no position ids. In this case, position ids\n+        #    are prepared by the model (`get_rope_index`) as `[4, bs, seq-len]` tensor. Text-only positions are\n+        #    prepended by us when creating positions so that the mask is constructed correctly. NOTE: failing to pass\n+        #    text-only positions will cause incorrect mask construction, do not change `prepare_input_for_generation`\n+        if position_ids.ndim == 3 and position_ids.shape[0] == 4:\n+            text_position_ids = position_ids[0]\n+            position_ids = position_ids[1:]\n+        else:\n+            text_position_ids = position_ids[0]\n+\n         # It may already have been prepared by e.g. `generate`\n         if not isinstance(causal_mask_mapping := attention_mask, dict):\n             # Prepare mask arguments\n@@ -865,7 +882,7 @@ def forward(\n                 \"attention_mask\": attention_mask,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n-                \"position_ids\": position_ids,\n+                \"position_ids\": text_position_ids,\n             }\n             # Create the masks\n             causal_mask_mapping = {\n@@ -891,7 +908,7 @@ def forward(\n             layer_outputs = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n-                position_ids=position_ids,\n+                position_ids=text_position_ids,\n                 past_key_value=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n@@ -1217,44 +1234,22 @@ def forward(\n             inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n \n         if position_ids is None:\n-            attention_mask_tensor = (\n-                attention_mask if not isinstance(attention_mask, dict) else attention_mask[\"full_attention\"]\n-            )\n-            if attention_mask_tensor is not None and attention_mask_tensor.ndim == 4:\n-                attention_mask_tensor = torch.diagonal(attention_mask_tensor[:, 0], dim1=1, dim2=2)\n-                # Only apply conversion for floating point tensors (inverted masks)\n-                if attention_mask_tensor.dtype.is_floating_point:\n-                    attention_mask_tensor = attention_mask_tensor / torch.finfo(attention_mask_tensor.dtype).min\n-                    attention_mask_tensor = (1.0 - attention_mask_tensor).int()\n-\n-            # Calculate RoPE index once per generation in the pre-fill stage only.\n-            # When compiling, we can't check tensor values thus we check only input length\n-            # It is safe to assume that `length!=1` means we're in pre-fill because compiled\n-            # models currently cannot do asssisted decoding\n-            prefill_compiled_stage = is_torchdynamo_compiling() and (\n-                (input_ids is not None and input_ids.shape[1] != 1)\n-                or (inputs_embeds is not None and inputs_embeds.shape[1] != 1)\n-            )\n-            prefill_noncompiled_stage = not is_torchdynamo_compiling() and (\n-                (cache_position is not None and cache_position[0] == 0)\n-                or (past_key_values is None or past_key_values.get_seq_length() == 0)\n-            )\n-            if (prefill_compiled_stage or prefill_noncompiled_stage) or self.rope_deltas is None:\n+            if self.rope_deltas is None or cache_position is None or cache_position[0] == 0:\n                 position_ids, rope_deltas = self.get_rope_index(\n-                    input_ids, image_grid_thw, video_grid_thw, attention_mask_tensor\n+                    input_ids, image_grid_thw, video_grid_thw, attention_mask\n                 )\n                 self.rope_deltas = rope_deltas\n             # then use the prev pre-calculated rope-deltas to get the correct position ids\n             else:\n                 batch_size, seq_length, _ = inputs_embeds.shape\n-                delta = cache_position[0] + self.rope_deltas if cache_position is not None else 0\n                 position_ids = torch.arange(seq_length, device=inputs_embeds.device)\n-                position_ids = position_ids.view(1, -1).expand(batch_size, -1)\n-                if cache_position is not None:  # otherwise `deltas` is an int `0`\n-                    delta = delta.repeat_interleave(batch_size // delta.shape[0], dim=0)\n-                    delta = delta.to(position_ids.device)\n-                position_ids = position_ids.add(delta)\n-                position_ids = position_ids.unsqueeze(0).expand(3, -1, -1)\n+                position_ids = position_ids.view(1, 1, -1).expand(3, batch_size, -1)\n+                if cache_position is not None:\n+                    delta = (cache_position[0] + self.rope_deltas).to(inputs_embeds.device)\n+                else:\n+                    delta = torch.zeros((batch_size, seq_length), device=inputs_embeds.device)\n+                delta = delta.repeat_interleave(batch_size // delta.shape[0], dim=0)\n+                position_ids += delta.to(position_ids.device)\n \n         outputs = self.language_model(\n             input_ids=None,\n@@ -1465,7 +1460,41 @@ def prepare_inputs_for_generation(\n         )\n \n         # Qwen2-VL position_ids are prepareed with rope_deltas in forward\n-        model_inputs[\"position_ids\"] = None\n+        if position_ids is None:\n+            # Calculate RoPE index once per generation in the pre-fill stage only.\n+            # When compiling, we can't check tensor values thus we check only input length\n+            # It is safe to assume that `length!=1` means we're in pre-fill because compiled\n+            # models currently cannot do asssisted decoding\n+            prefill_compiled_stage = is_torchdynamo_compiling() and (\n+                (input_ids is not None and input_ids.shape[1] != 1)\n+                or (inputs_embeds is not None and inputs_embeds.shape[1] != 1)\n+            )\n+            prefill_noncompiled_stage = not is_torchdynamo_compiling() and (\n+                (cache_position is not None and cache_position[0] == 0)\n+                or (past_key_values is None or past_key_values.get_seq_length() == 0)\n+            )\n+            if (prefill_compiled_stage or prefill_noncompiled_stage) or self.model.rope_deltas is None:\n+                vision_positions, rope_deltas = self.model.get_rope_index(\n+                    model_inputs.get(\"input_ids\", None),\n+                    image_grid_thw=image_grid_thw,\n+                    video_grid_thw=video_grid_thw,\n+                    attention_mask=attention_mask,\n+                )\n+                self.model.rope_deltas = rope_deltas\n+            # then use the prev pre-calculated rope-deltas to get the correct position ids\n+            elif \"position_ids\" in model_inputs:\n+                position_ids = model_inputs[\"position_ids\"][None, ...]\n+                delta = self.model.rope_deltas\n+                delta = delta.repeat_interleave(position_ids.shape[1] // delta.shape[0], dim=0)\n+                vision_positions = position_ids + delta.expand_as(position_ids)\n+                vision_positions = vision_positions.expand(3, vision_positions.shape[1], -1)\n+\n+            # Concatenate \"text + vision\" positions into [4, bs, seq-len]\n+            if \"position_ids\" not in model_inputs:\n+                text_positions = torch.arange(input_ids, device=input_ids.device)[None, None, :]\n+            else:\n+                text_positions = model_inputs[\"position_ids\"][None, ...]\n+            model_inputs[\"position_ids\"] = torch.cat([text_positions, vision_positions], dim=0)\n \n         if model_inputs[\"cache_position\"][0] != 0:\n             model_inputs[\"pixel_values\"] = None"
        },
        {
            "sha": "2244b7011d8636f7b8eca064cdf80e63146137c0",
            "filename": "tests/models/qwen2_5_omni/test_modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 86,
            "deletions": 0,
            "changes": 86,
            "blob_url": "https://github.com/huggingface/transformers/blob/344012b3a6cecd5393deeaf7703071ad603b7871/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/344012b3a6cecd5393deeaf7703071ad603b7871/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py?ref=344012b3a6cecd5393deeaf7703071ad603b7871",
            "patch": "@@ -332,6 +332,92 @@ def test_sdpa_can_dispatch_composite_models(self):\n                     if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n                         raise ValueError(\"The eager model should not have SDPA attention layers\")\n \n+    def flash_attention_padding_matches_padding_free_with_position_ids(\n+        self, attn_implementation: str, fa_kwargs: bool = False\n+    ):\n+        max_new_tokens = 30\n+        for model_class in self.all_generative_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+            dummy_input = inputs_dict[model_class.main_input_name]\n+            if dummy_input.dtype in [torch.float32, torch.float16]:\n+                dummy_input = dummy_input.to(torch.bfloat16)\n+\n+            # make sure that all models have enough positions for generation\n+            if hasattr(config, \"max_position_embeddings\"):\n+                config.max_position_embeddings = max_new_tokens + dummy_input.shape[1] + 1\n+\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+\n+                if 0 in inputs_dict[\"attention_mask\"][:, -1]:\n+                    inputs_dict[\"attention_mask\"] = inputs_dict[\"attention_mask\"].flip(1)\n+                dummy_attention_mask = inputs_dict[\"attention_mask\"]\n+                inputs_dict[\"input_ids\"][~dummy_attention_mask.bool()] = config.get_text_config().pad_token_id\n+\n+                model = (\n+                    model_class.from_pretrained(\n+                        tmpdirname,\n+                        torch_dtype=torch.bfloat16,\n+                        attn_implementation=attn_implementation,\n+                    )\n+                    .to(torch_device)\n+                    .eval()\n+                )\n+\n+                # flatten\n+                padfree_inputs_dict = {\n+                    \"input_features\": inputs_dict[\"input_features\"],\n+                    \"feature_attention_mask\": inputs_dict[\"feature_attention_mask\"],\n+                    \"pixel_values\": inputs_dict[\"pixel_values\"],\n+                    \"image_grid_thw\": inputs_dict[\"image_grid_thw\"],\n+                    \"input_ids\": inputs_dict[\"input_ids\"][dummy_attention_mask.bool()].unsqueeze(0),\n+                }\n+\n+                # add position_ids\n+                vision_position_ids, deltas = model.get_rope_index(\n+                    input_ids=inputs_dict[\"input_ids\"],\n+                    image_grid_thw=inputs_dict[\"image_grid_thw\"],\n+                    attention_mask=inputs_dict[\"attention_mask\"],\n+                    audio_seqlens=torch.sum(inputs_dict[\"feature_attention_mask\"], dim=1),\n+                )  # [3, bs, padded-seq-len]\n+                vision_padfree_positions = vision_position_ids[:, dummy_attention_mask.bool()].view(\n+                    3, -1\n+                )  # [3, bs*padfree-len]\n+                text_padfree_positions = torch.cat(\n+                    [torch.arange(length) for length in dummy_attention_mask.sum(1).tolist()]\n+                )  # [1, bs*padfree-len]\n+                text_padfree_positions = text_padfree_positions.long().unsqueeze(0).to(torch_device)\n+                padfree_inputs_dict[\"position_ids\"] = torch.cat([text_padfree_positions, vision_padfree_positions])[\n+                    :, None, :\n+                ]\n+\n+                if fa_kwargs:\n+                    cu_seq_lens = [0] + dummy_attention_mask.sum(1).tolist()\n+                    cu_seq_lens = torch.tensor(cu_seq_lens, device=torch_device)\n+                    max_length = cu_seq_lens.diff().max().item()\n+                    padfree_inputs_dict.update(\n+                        {\n+                            \"cu_seq_lens_q\": cu_seq_lens.cumsum(-1).to(dtype=torch.int32),\n+                            \"cu_seq_lens_k\": cu_seq_lens.cumsum(-1).to(dtype=torch.int32),\n+                            \"max_length_q\": max_length,\n+                            \"max_length_k\": max_length,\n+                        }\n+                    )\n+\n+                res_padded = model(**inputs_dict, use_cache=False)\n+                res_padfree = model(**padfree_inputs_dict, use_cache=False)\n+\n+                logits_padded = res_padded.logits[inputs_dict[\"attention_mask\"].bool()]\n+                logits_padfree = res_padfree.logits[0]\n+\n+                torch.testing.assert_close(logits_padded.argmax(-1), logits_padfree.argmax(-1), rtol=0, atol=0)\n+                # acceptable numerical instability\n+                tol = torch.finfo(torch.bfloat16).eps\n+                torch.testing.assert_close(logits_padded, logits_padfree, rtol=tol, atol=tol)\n+\n     @unittest.skip(\"Cannot do contrastive generation, has custom `generate()`\")\n     def test_contrastive_generate(self):\n         pass"
        },
        {
            "sha": "f2b229851f6efeb710df71274a97d2fa9470efe0",
            "filename": "tests/models/qwen2_5_vl/test_modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 83,
            "deletions": 0,
            "changes": 83,
            "blob_url": "https://github.com/huggingface/transformers/blob/344012b3a6cecd5393deeaf7703071ad603b7871/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/344012b3a6cecd5393deeaf7703071ad603b7871/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py?ref=344012b3a6cecd5393deeaf7703071ad603b7871",
            "patch": "@@ -325,6 +325,89 @@ def test_video_forward(self):\n             )\n             self.assertIsNotNone(outputs)\n \n+    def flash_attention_padding_matches_padding_free_with_position_ids(\n+        self, attn_implementation: str, fa_kwargs: bool = False\n+    ):\n+        max_new_tokens = 30\n+        for model_class in self.all_generative_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+            dummy_input = inputs_dict[model_class.main_input_name]\n+            if dummy_input.dtype in [torch.float32, torch.float16]:\n+                dummy_input = dummy_input.to(torch.bfloat16)\n+\n+            # make sure that all models have enough positions for generation\n+            if hasattr(config, \"max_position_embeddings\"):\n+                config.max_position_embeddings = max_new_tokens + dummy_input.shape[1] + 1\n+\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+\n+                if 0 in inputs_dict[\"attention_mask\"][:, -1]:\n+                    inputs_dict[\"attention_mask\"] = inputs_dict[\"attention_mask\"].flip(1)\n+                dummy_attention_mask = inputs_dict[\"attention_mask\"]\n+                inputs_dict[\"input_ids\"][~dummy_attention_mask.bool()] = config.get_text_config().pad_token_id\n+\n+                model = (\n+                    model_class.from_pretrained(\n+                        tmpdirname,\n+                        torch_dtype=torch.bfloat16,\n+                        attn_implementation=attn_implementation,\n+                    )\n+                    .to(torch_device)\n+                    .eval()\n+                )\n+\n+                # flatten\n+                padfree_inputs_dict = {\n+                    \"pixel_values\": inputs_dict[\"pixel_values\"],\n+                    \"image_grid_thw\": inputs_dict[\"image_grid_thw\"],\n+                    \"input_ids\": inputs_dict[\"input_ids\"][dummy_attention_mask.bool()].unsqueeze(0),\n+                }\n+\n+                # add position_ids\n+                vision_position_ids, deltas = model.model.get_rope_index(\n+                    input_ids=inputs_dict[\"input_ids\"],\n+                    image_grid_thw=inputs_dict[\"image_grid_thw\"],\n+                    attention_mask=inputs_dict[\"attention_mask\"],\n+                )  # [3, bs, padded-seq-len]\n+                vision_padfree_positions = vision_position_ids[:, dummy_attention_mask.bool()].view(\n+                    3, -1\n+                )  # [3, bs*padfree-len]\n+                text_padfree_positions = torch.cat(\n+                    [torch.arange(length) for length in dummy_attention_mask.sum(1).tolist()]\n+                )  # [1, bs*padfree-len]\n+                text_padfree_positions = text_padfree_positions.long().unsqueeze(0).to(torch_device)\n+                padfree_inputs_dict[\"position_ids\"] = torch.cat([text_padfree_positions, vision_padfree_positions])[\n+                    :, None, :\n+                ]\n+\n+                if fa_kwargs:\n+                    cu_seq_lens = [0] + dummy_attention_mask.sum(1).tolist()\n+                    cu_seq_lens = torch.tensor(cu_seq_lens, device=torch_device)\n+                    max_length = cu_seq_lens.diff().max().item()\n+                    padfree_inputs_dict.update(\n+                        {\n+                            \"cu_seq_lens_q\": cu_seq_lens.cumsum(-1).to(dtype=torch.int32),\n+                            \"cu_seq_lens_k\": cu_seq_lens.cumsum(-1).to(dtype=torch.int32),\n+                            \"max_length_q\": max_length,\n+                            \"max_length_k\": max_length,\n+                        }\n+                    )\n+\n+                res_padded = model(**inputs_dict, use_cache=False)\n+                res_padfree = model(**padfree_inputs_dict, use_cache=False)\n+\n+                logits_padded = res_padded.logits[inputs_dict[\"attention_mask\"].bool()]\n+                logits_padfree = res_padfree.logits[0]\n+\n+                torch.testing.assert_close(logits_padded.argmax(-1), logits_padfree.argmax(-1), rtol=0, atol=0)\n+                # acceptable numerical instability\n+                tol = torch.finfo(torch.bfloat16).eps\n+                torch.testing.assert_close(logits_padded, logits_padfree, rtol=tol, atol=tol)\n+\n     @unittest.skip(reason=\"Feedforward chunking is not yet supported\")\n     def test_feed_forward_chunking(self):\n         pass"
        },
        {
            "sha": "9f37f611081d9a9571e47013f8e77ab94f665c67",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 86,
            "deletions": 0,
            "changes": 86,
            "blob_url": "https://github.com/huggingface/transformers/blob/344012b3a6cecd5393deeaf7703071ad603b7871/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/344012b3a6cecd5393deeaf7703071ad603b7871/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=344012b3a6cecd5393deeaf7703071ad603b7871",
            "patch": "@@ -15,6 +15,7 @@\n \n import copy\n import gc\n+import tempfile\n import unittest\n \n import requests\n@@ -168,6 +169,7 @@ def prepare_config_and_inputs_for_common(self):\n         attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=torch_device)\n \n         input_ids[:, -1] = self.pad_token_id\n+        attention_mask[:, -1] = 0\n         input_ids[input_ids == self.video_token_id] = self.pad_token_id\n         input_ids[input_ids == self.image_token_id] = self.pad_token_id\n         input_ids[input_ids == self.vision_start_token_id] = self.pad_token_id\n@@ -281,6 +283,90 @@ def test_forward_with_rope_deltas_cached(self):\n                 generation_output.logits[0], forward_output.logits[:, -1, :], rtol=1e-4, atol=1e-4\n             )\n \n+    def flash_attention_padding_matches_padding_free_with_position_ids(\n+        self, attn_implementation: str, fa_kwargs: bool = False\n+    ):\n+        max_new_tokens = 30\n+        for model_class in self.all_generative_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+            dummy_input = inputs_dict[model_class.main_input_name]\n+            if dummy_input.dtype in [torch.float32, torch.float16]:\n+                dummy_input = dummy_input.to(torch.bfloat16)\n+\n+            # make sure that all models have enough positions for generation\n+            if hasattr(config, \"max_position_embeddings\"):\n+                config.max_position_embeddings = max_new_tokens + dummy_input.shape[1] + 1\n+\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+\n+                if 0 in inputs_dict[\"attention_mask\"][:, -1]:\n+                    inputs_dict[\"attention_mask\"] = inputs_dict[\"attention_mask\"].flip(1)\n+                dummy_attention_mask = inputs_dict[\"attention_mask\"]\n+                inputs_dict[\"input_ids\"][~dummy_attention_mask.bool()] = config.get_text_config().pad_token_id\n+\n+                model = (\n+                    model_class.from_pretrained(\n+                        tmpdirname,\n+                        torch_dtype=torch.bfloat16,\n+                        attn_implementation=attn_implementation,\n+                    )\n+                    .to(torch_device)\n+                    .eval()\n+                )\n+\n+                # flatten\n+                padfree_inputs_dict = {\n+                    \"pixel_values\": inputs_dict[\"pixel_values\"],\n+                    \"image_grid_thw\": inputs_dict[\"image_grid_thw\"],\n+                    \"input_ids\": inputs_dict[\"input_ids\"][dummy_attention_mask.bool()].unsqueeze(0),\n+                }\n+\n+                # add position_ids\n+                vision_position_ids, deltas = model.model.get_rope_index(\n+                    input_ids=inputs_dict[\"input_ids\"],\n+                    image_grid_thw=inputs_dict[\"image_grid_thw\"],\n+                    attention_mask=inputs_dict[\"attention_mask\"],\n+                )  # [3, bs, padded-seq-len]\n+                vision_padfree_positions = vision_position_ids[:, dummy_attention_mask.bool()].view(\n+                    3, -1\n+                )  # [3, bs*padfree-len]\n+                text_padfree_positions = torch.cat(\n+                    [torch.arange(length) for length in dummy_attention_mask.sum(1).tolist()]\n+                )  # [1, bs*padfree-len]\n+                text_padfree_positions = text_padfree_positions.long().unsqueeze(0).to(torch_device)\n+                padfree_inputs_dict[\"position_ids\"] = torch.cat([text_padfree_positions, vision_padfree_positions])[\n+                    :, None, :\n+                ]\n+\n+                if fa_kwargs:\n+                    cu_seq_lens = [0] + dummy_attention_mask.sum(1).tolist()\n+                    cu_seq_lens = torch.tensor(cu_seq_lens, device=torch_device)\n+                    max_length = cu_seq_lens.diff().max().item()\n+                    padfree_inputs_dict.update(\n+                        {\n+                            \"cu_seq_lens_q\": cu_seq_lens.cumsum(-1).to(dtype=torch.int32),\n+                            \"cu_seq_lens_k\": cu_seq_lens.cumsum(-1).to(dtype=torch.int32),\n+                            \"max_length_q\": max_length,\n+                            \"max_length_k\": max_length,\n+                        }\n+                    )\n+\n+                # We need to do simple forward without cache in roder to trigger packed SDPA/FLEX/EAGER path\n+                res_padded = model(**inputs_dict, use_cache=False)\n+                res_padfree = model(**padfree_inputs_dict, use_cache=False)\n+\n+                logits_padded = res_padded.logits[inputs_dict[\"attention_mask\"].bool()]\n+                logits_padfree = res_padfree.logits[0]\n+\n+                torch.testing.assert_close(logits_padded.argmax(-1), logits_padfree.argmax(-1), rtol=0, atol=0)\n+                # acceptable numerical instability\n+                tol = torch.finfo(torch.bfloat16).eps\n+                torch.testing.assert_close(logits_padded, logits_padfree, rtol=tol, atol=tol)\n+\n     @unittest.skip(reason=\"Feedforward chunking is not yet supported\")\n     def test_feed_forward_chunking(self):\n         pass"
        },
        {
            "sha": "30138b08506e67dc52a07dac8dc6c4ae6ca17d19",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 19,
            "deletions": 7,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/344012b3a6cecd5393deeaf7703071ad603b7871/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/344012b3a6cecd5393deeaf7703071ad603b7871/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=344012b3a6cecd5393deeaf7703071ad603b7871",
            "patch": "@@ -4129,13 +4129,14 @@ def flash_attention_padding_matches_padding_free_with_position_ids(\n             self.skipTest(reason=\"Model architecture does not support attentions\")\n \n         max_new_tokens = 30\n+        support_flag = {\n+            \"sdpa\": \"_supports_sdpa\",\n+            \"flash_attention_2\": \"_supports_flash_attn\",\n+            \"flash_attention_3\": \"_supports_flash_attn\",\n+        }\n \n         for model_class in self.all_generative_model_classes:\n-            if not (\n-                model_class._supports_flash_attn_2\n-                if attn_implementation == \"flash_attention_2\"\n-                else model_class._supports_flash_attn_3\n-            ):\n+            if not getattr(model_class, support_flag[attn_implementation]):\n                 self.skipTest(f\"{model_class.__name__} does not support {attn_implementation}\")\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n@@ -4204,8 +4205,9 @@ def flash_attention_padding_matches_padding_free_with_position_ids(\n                         .to(torch_device)\n                     )\n \n-                res_padded = model(**inputs_dict)\n-                res_padfree = model(**padfree_inputs_dict)\n+                # We need to do simple forward without cache in roder to trigger packed SDPA/FLEX/EAGER path\n+                res_padded = model(**inputs_dict, use_cache=False)\n+                res_padfree = model(**padfree_inputs_dict, use_cache=False)\n \n                 logits_padded = res_padded.logits[inputs_dict[\"attention_mask\"].bool()]\n                 logits_padfree = res_padfree.logits[0]\n@@ -4215,6 +4217,16 @@ def flash_attention_padding_matches_padding_free_with_position_ids(\n                 tol = torch.finfo(torch.bfloat16).eps\n                 torch.testing.assert_close(logits_padded, logits_padfree, rtol=tol, atol=tol)\n \n+    # Mark slow for now as it is failing for all multimodals/non-transformer arch models and a few LLMs\n+    # FIXME @raushan\n+    @slow\n+    def test_eager_padding_matches_padding_free_with_position_ids(self):\n+        self.flash_attention_padding_matches_padding_free_with_position_ids(attn_implementation=\"eager\")\n+\n+    @slow\n+    def test_sdpa_padding_matches_padding_free_with_position_ids(self):\n+        self.flash_attention_padding_matches_padding_free_with_position_ids(attn_implementation=\"sdpa\")\n+\n     @require_flash_attn\n     @require_torch_gpu\n     @mark.flash_attn_test"
        }
    ],
    "stats": {
        "total": 578,
        "additions": 478,
        "deletions": 100
    }
}