{
    "author": "matthewdouglas",
    "message": "[CI] Fix bnb quantization tests with accelerate>=1.2.0 (#35172)",
    "sha": "34f4080ff59b1668d919a1ba9f8bc4a3a2a3f478",
    "files": [
        {
            "sha": "9512d0aa70af97437653088fde9f5662f795c3f3",
            "filename": "tests/quantization/bnb/test_4bit.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/34f4080ff59b1668d919a1ba9f8bc4a3a2a3f478/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34f4080ff59b1668d919a1ba9f8bc4a3a2a3f478/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_4bit.py?ref=34f4080ff59b1668d919a1ba9f8bc4a3a2a3f478",
            "patch": "@@ -385,14 +385,14 @@ def test_inference_without_keep_in_fp32(self):\n \n         # test with `google-t5/t5-small`\n         model = T5ForConditionalGeneration.from_pretrained(self.model_name, load_in_4bit=True, device_map=\"auto\")\n-        encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+        encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\").to(model.device)\n         _ = model.generate(**encoded_input)\n \n         # test with `flan-t5-small`\n         model = T5ForConditionalGeneration.from_pretrained(\n             self.dense_act_model_name, load_in_4bit=True, device_map=\"auto\"\n         )\n-        encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+        encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\").to(model.device)\n         _ = model.generate(**encoded_input)\n         T5ForConditionalGeneration._keep_in_fp32_modules = modules\n \n@@ -410,14 +410,14 @@ def test_inference_with_keep_in_fp32(self):\n         # there was a bug with decoders - this test checks that it is fixed\n         self.assertTrue(isinstance(model.decoder.block[0].layer[0].SelfAttention.q, bnb.nn.Linear4bit))\n \n-        encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+        encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\").to(model.device)\n         _ = model.generate(**encoded_input)\n \n         # test with `flan-t5-small`\n         model = T5ForConditionalGeneration.from_pretrained(\n             self.dense_act_model_name, load_in_4bit=True, device_map=\"auto\"\n         )\n-        encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+        encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\").to(model.device)\n         _ = model.generate(**encoded_input)\n \n "
        },
        {
            "sha": "158fdfaf71dc5c3869864f12b221ff31cbbcf439",
            "filename": "tests/quantization/bnb/test_mixed_int8.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/34f4080ff59b1668d919a1ba9f8bc4a3a2a3f478/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34f4080ff59b1668d919a1ba9f8bc4a3a2a3f478/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py?ref=34f4080ff59b1668d919a1ba9f8bc4a3a2a3f478",
            "patch": "@@ -514,14 +514,14 @@ def test_inference_without_keep_in_fp32(self):\n \n         # test with `google-t5/t5-small`\n         model = T5ForConditionalGeneration.from_pretrained(self.model_name, load_in_8bit=True, device_map=\"auto\")\n-        encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+        encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\").to(model.device)\n         _ = model.generate(**encoded_input)\n \n         # test with `flan-t5-small`\n         model = T5ForConditionalGeneration.from_pretrained(\n             self.dense_act_model_name, load_in_8bit=True, device_map=\"auto\"\n         )\n-        encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+        encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\").to(model.device)\n         _ = model.generate(**encoded_input)\n         T5ForConditionalGeneration._keep_in_fp32_modules = modules\n \n@@ -540,14 +540,14 @@ def test_inference_with_keep_in_fp32(self):\n         # there was a bug with decoders - this test checks that it is fixed\n         self.assertTrue(isinstance(model.decoder.block[0].layer[0].SelfAttention.q, bnb.nn.Linear8bitLt))\n \n-        encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+        encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\").to(model.device)\n         _ = model.generate(**encoded_input)\n \n         # test with `flan-t5-small`\n         model = T5ForConditionalGeneration.from_pretrained(\n             self.dense_act_model_name, load_in_8bit=True, device_map=\"auto\"\n         )\n-        encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+        encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\").to(model.device)\n         _ = model.generate(**encoded_input)\n \n     def test_inference_with_keep_in_fp32_serialized(self):\n@@ -571,14 +571,14 @@ def test_inference_with_keep_in_fp32_serialized(self):\n             # there was a bug with decoders - this test checks that it is fixed\n             self.assertTrue(isinstance(model.decoder.block[0].layer[0].SelfAttention.q, bnb.nn.Linear8bitLt))\n \n-            encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+            encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\").to(model.device)\n             _ = model.generate(**encoded_input)\n \n             # test with `flan-t5-small`\n             model = T5ForConditionalGeneration.from_pretrained(\n                 self.dense_act_model_name, load_in_8bit=True, device_map=\"auto\"\n             )\n-            encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+            encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\").to(model.device)\n             _ = model.generate(**encoded_input)\n \n "
        }
    ],
    "stats": {
        "total": 20,
        "additions": 10,
        "deletions": 10
    }
}