{
    "author": "MekkCyber",
    "message": "Changing __repr__ in torchao to show quantized Linear (#34202)\n\n* Changing __repr__ in torchao\r\n\r\n* small update\r\n\r\n* make style\r\n\r\n* small update\r\n\r\n* add LinearActivationQuantizedTensor\r\n\r\n* remove some cases\r\n\r\n* update imports & handle return None\r\n\r\n* update",
    "sha": "d2bae7ee9d2bc8791e05763e56f46754ed3e71ae",
    "files": [
        {
            "sha": "9a03eb25f4de0d25be691b967fa9fa81d647177f",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 31,
            "deletions": 5,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/d2bae7ee9d2bc8791e05763e56f46754ed3e71ae/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d2bae7ee9d2bc8791e05763e56f46754ed3e71ae/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=d2bae7ee9d2bc8791e05763e56f46754ed3e71ae",
            "patch": "@@ -12,6 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import importlib\n+import types\n from typing import TYPE_CHECKING, Union\n \n from packaging import version\n@@ -30,9 +31,7 @@\n \n if is_torch_available():\n     import torch\n-\n-if is_torchao_available():\n-    from torchao.quantization import quantize_\n+    import torch.nn as nn\n \n logger = logging.get_logger(__name__)\n \n@@ -46,6 +45,25 @@ def find_parent(model, name):\n     return parent\n \n \n+def _quantization_type(weight):\n+    from torchao.dtypes import AffineQuantizedTensor\n+    from torchao.quantization.linear_activation_quantized_tensor import LinearActivationQuantizedTensor\n+\n+    if isinstance(weight, AffineQuantizedTensor):\n+        return f\"{weight.__class__.__name__}({weight._quantization_type()})\"\n+\n+    if isinstance(weight, LinearActivationQuantizedTensor):\n+        return f\"{weight.__class__.__name__}(activation={weight.input_quant_func}, weight={_quantization_type(weight.original_weight_tensor)})\"\n+\n+\n+def _linear_extra_repr(self):\n+    weight = _quantization_type(self.weight)\n+    if weight is None:\n+        return f\"in_features={self.weight.shape[1]}, out_features={self.weight.shape[0]}, weight=None\"\n+    else:\n+        return f\"in_features={self.weight.shape[1]}, out_features={self.weight.shape[0]}, weight={weight}\"\n+\n+\n class TorchAoHfQuantizer(HfQuantizer):\n     \"\"\"\n     Quantizer for torchao: https://github.com/pytorch/ao/\n@@ -152,9 +170,17 @@ def create_quantized_param(\n         Each nn.Linear layer that needs to be quantized is processsed here.\n         First, we set the value the weight tensor, then we move it to the target device. Finally, we quantize the module.\n         \"\"\"\n+        from torchao.quantization import quantize_\n+\n         module, tensor_name = get_module_from_name(model, param_name)\n-        module._parameters[tensor_name] = torch.nn.Parameter(param_value).to(device=target_device)\n-        quantize_(module, self.quantization_config.get_apply_tensor_subclass())\n+\n+        if self.pre_quantized:\n+            module._parameters[tensor_name] = torch.nn.Parameter(param_value.to(device=target_device))\n+            if isinstance(module, nn.Linear):\n+                module.extra_repr = types.MethodType(_linear_extra_repr, module)\n+        else:\n+            module._parameters[tensor_name] = torch.nn.Parameter(param_value).to(device=target_device)\n+            quantize_(module, self.quantization_config.get_apply_tensor_subclass())\n \n     def _process_model_after_weight_loading(self, model):\n         \"\"\"No process required for torchao quantized model\"\"\""
        }
    ],
    "stats": {
        "total": 36,
        "additions": 31,
        "deletions": 5
    }
}