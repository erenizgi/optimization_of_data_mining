{
    "author": "zucchini-nlp",
    "message": "ðŸš¨ Move `rotary_partial_emb` to RopeParams and delete unnecessary code ðŸ”ª  (#42255)\n\n* tmp\n\n* batch push\n\n* maybe better pop and break, and we'll have one theta per config in the rope dict\n\n* update a few models?\n\n* fix tests that are easu first\n\n* dont overwrite if already present!!!\n\n* partial rotary factor\n\n* more fixes to the god of fixes\n\n* setdefault\n\n* fix copies\n\n* Update src/transformers/modeling_rope_utils.py\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\n\n* Update src/transformers/models/efficientloftr/configuration_efficientloftr.py\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\n\n* attempt one\n\n* update all models\n\n* fix tests\n\n* fix tests\n\n* oops\n\n* fix slow tests with nested rope models\n\n* fix copies\n\n* deal with  circular import and move the mixin to base config class\n\n* fix copies\n\n* fix a few tests\n\n* update the migration guide\n\n---------\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>",
    "sha": "078ff685d37a8efb4966bd3da322eef92bf875c0",
    "files": [
        {
            "sha": "3eb310021e80fc80e353717ee3e4e49c5a5c621f",
            "filename": "MIGRATION_GUIDE_V5.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/MIGRATION_GUIDE_V5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/MIGRATION_GUIDE_V5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/MIGRATION_GUIDE_V5.md?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -328,7 +328,7 @@ model_4bit = AutoModelForCausalLM.from_pretrained(\n \n - Methods to init a nested config such as `from_xxx_config` are deleted. Configs can be init from the `__init__` method in the same way. See [#41314](https://github.com/huggingface/transformers/pull/41314).\n - It is no longer possible to load a config class from a URL file. Configs must be loaded from either a local path or a repo on the Hub. See [#42383](https://github.com/huggingface/transformers/pull/42383).\n-- All parameters for configuring model's rotary embedding are now stored under `mode.rope_parameters`, including the `rope_theta` and `rope_type`. Model's `config.rope_parameters` is a simple dictionaty in most cases, and can also be a nested dict in special cases (i.e. Gemma3 and ModernBert) with different rope parameterization for each layer type. See [#39847](https://github.com/huggingface/transformers/pull/39847)\n+- All parameters for configuring model's rotary embedding are now stored under `mode.rope_parameters`, including the `rope_theta` and `rope_type`. Model's `config.rope_parameters` is a simple dictionaty in most cases, and can also be a nested dict in special cases (i.e. Gemma3 and ModernBert) with different rope parameterization for each layer type. Trying to get `config.rope_theta` will throw an attribute error from now on. See [#39847](https://github.com/huggingface/transformers/pull/39847) and [#42255](https://github.com/huggingface/transformers/pull/42255)\n - Qwen-VL family configuration is in a nested format and trying to access keys directly will throw an error (e.g. `config.vocab_size`). Users are expected to access keys from their respective sub-configs (`config.text_config.vocab_size`).\n - Configurations of non-generative models (any model that doesn't call `model.generate()`) will no longer have a `generation_config` and `model.config.generation_config` will throw an attribute error.\n "
        },
        {
            "sha": "d93f5a6e5626eb43af4de89bb70831714a724112",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -26,6 +26,7 @@\n from . import __version__\n from .dynamic_module_utils import custom_object_save\n from .modeling_gguf_pytorch_utils import load_gguf_checkpoint\n+from .modeling_rope_utils import RotaryEmbeddingConfigMixin\n from .utils import (\n     CONFIG_NAME,\n     PushToHubMixin,\n@@ -49,7 +50,7 @@\n SpecificPreTrainedConfigType = TypeVar(\"SpecificPreTrainedConfigType\", bound=\"PreTrainedConfig\")\n \n \n-class PreTrainedConfig(PushToHubMixin):\n+class PreTrainedConfig(PushToHubMixin, RotaryEmbeddingConfigMixin):\n     # no-format\n     r\"\"\"\n     Base class for all configuration classes. Handles a few parameters common to all models' configurations as well as\n@@ -261,6 +262,13 @@ def __init__(\n \n             dtype = getattr(torch, dtype)\n \n+        # BC for rotary embeddings. We will pop out legacy keys from kwargs and rename to new format\n+        if hasattr(self, \"rope_parameters\"):\n+            ignore_keys_at_rope_validation = kwargs.pop(\"ignore_keys_at_rope_validation\", None)\n+            kwargs = self.convert_rope_params_to_dict(\n+                ignore_keys_at_rope_validation=ignore_keys_at_rope_validation, **kwargs\n+            )\n+\n         # Attributes common for all models\n         self.return_dict = return_dict\n         self.output_hidden_states = output_hidden_states"
        },
        {
            "sha": "de27e5f8bd20c077d3e79d8496c3252f41cf60df",
            "filename": "src/transformers/modeling_rope_utils.py",
            "status": "modified",
            "additions": 316,
            "deletions": 338,
            "changes": 654,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_rope_utils.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -14,9 +14,8 @@\n \n import math\n from functools import wraps\n-from typing import Optional, TypedDict\n+from typing import TYPE_CHECKING, Optional, TypedDict\n \n-from .configuration_utils import PreTrainedConfig\n from .utils import is_torch_available, logging\n \n \n@@ -26,56 +25,8 @@\n if is_torch_available():\n     import torch\n \n-\n-def standardize_rope_params(config, rope_theta: float | dict[str, float] | None = None):\n-    \"\"\"\n-    Helper to standardize the config's rope params field by ensuring the params are defined for each\n-    later type. For old model the fn will duplicate a single rope param in each layer type (backward compatibility)\n-    \"\"\"\n-    rope_parameters = getattr(config, \"rope_parameters\", None)\n-    layer_types = getattr(config, \"layer_types\", None)\n-    if rope_theta is None:\n-        rope_theta = getattr(config, \"rope_theta\", None)\n-\n-    # Case 1: one RoPE theat = one RoPE param per model without nesting\n-    if not isinstance(rope_theta, dict):\n-        if rope_parameters is None:\n-            rope_parameters = {\"rope_type\": \"default\", \"rope_theta\": rope_theta}\n-        else:\n-            # BC: if there is a 'type' field, copy it it to 'rope_type'.\n-            rope_type = rope_parameters.get(\"rope_type\", rope_parameters.get(\"type\", \"default\"))\n-            rope_theta = rope_parameters.get(\"rope_theta\") or rope_theta\n-            rope_parameters.update({\"rope_theta\": rope_theta, \"rope_type\": rope_type})\n-        config.rope_parameters = rope_parameters\n-\n-    # Case 2: different RoPE for each layer as nested dict\n-    else:\n-        rope_parameters_per_layer_type = {}\n-        for layer_type in layer_types:\n-            if rope_parameters is None:\n-                rope_parameters_per_layer_type[layer_type] = {\n-                    \"rope_type\": \"default\",\n-                    \"rope_theta\": rope_theta[layer_type],\n-                }\n-            else:\n-                is_field_in_new_format = any(layer_type in rope_parameters for layer_type in layer_types)\n-                if not is_field_in_new_format:\n-                    curr_rope_type = rope_parameters.get(\"rope_type\", rope_parameters.get(\"type\"))\n-                    rope_parameters_per_layer_type[layer_type] = {\n-                        **rope_parameters,\n-                        \"rope_type\": curr_rope_type,\n-                        \"rope_theta\": rope_theta[layer_type],\n-                    }\n-                else:\n-                    curr_rope_type = rope_parameters[layer_type].get(\n-                        \"rope_type\", rope_parameters[layer_type].get(\"type\")\n-                    )\n-                    rope_parameters_per_layer_type[layer_type] = {\n-                        **rope_parameters[layer_type],\n-                        \"rope_type\": curr_rope_type,\n-                        \"rope_theta\": rope_theta[layer_type],\n-                    }\n-            config.rope_parameters = rope_parameters_per_layer_type\n+if TYPE_CHECKING:\n+    from .configuration_utils import PreTrainedConfig\n \n \n def dynamic_rope_update(rope_forward):\n@@ -176,15 +127,15 @@ def wrapper(self, x, position_ids, layer_type=None):\n \n \n def _compute_linear_scaling_rope_parameters(\n-    config: Optional[PreTrainedConfig] = None,\n+    config: Optional[\"PreTrainedConfig\"] = None,\n     device: Optional[\"torch.device\"] = None,\n     seq_len: Optional[int] = None,\n     layer_type: Optional[str] = None,\n ) -> tuple[\"torch.Tensor\", float]:\n     \"\"\"\n     Computes the inverse frequencies with linear scaling. Credits to the Reddit user /u/kaiokendev\n     Args:\n-        config ([`~transformers.PreTrainedConfig`]):\n+        config ([`~transformers.\"PreTrainedConfig\"`]):\n             The model configuration. This function assumes that the config will provide at least the following\n             properties:\n \n@@ -208,13 +159,13 @@ def _compute_linear_scaling_rope_parameters(\n         post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n     \"\"\"\n     # For backward compatibility standardize the `rope_parameters_dict` if it uses old format\n-    standardize_rope_params(config)\n+    config.standardize_rope_params()\n     rope_parameters_dict = config.rope_parameters[layer_type] if layer_type is not None else config.rope_parameters\n     factor = rope_parameters_dict[\"factor\"]\n \n     # Gets the default RoPE parameters\n     base = rope_parameters_dict[\"rope_theta\"]\n-    partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+    partial_rotary_factor = rope_parameters_dict.get(\"partial_rotary_factor\", 1.0)\n     head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n     dim = int(head_dim * partial_rotary_factor)\n     attention_factor = 1.0  # Unused in this type of RoPE\n@@ -230,7 +181,7 @@ def _compute_linear_scaling_rope_parameters(\n \n \n def _compute_dynamic_ntk_parameters(\n-    config: Optional[PreTrainedConfig] = None,\n+    config: Optional[\"PreTrainedConfig\"] = None,\n     device: Optional[\"torch.device\"] = None,\n     seq_len: Optional[int] = None,\n     layer_type: Optional[str] = None,\n@@ -239,7 +190,7 @@ def _compute_dynamic_ntk_parameters(\n     Computes the inverse frequencies with NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\n \n     Args:\n-        config ([`~transformers.PreTrainedConfig`]):\n+        config ([`~transformers.\"PreTrainedConfig\"`]):\n             The model configuration. This function assumes that the config will provide at least the following\n             properties:\n \n@@ -273,11 +224,11 @@ def _compute_dynamic_ntk_parameters(\n     \"\"\"\n     # TODO (joao): use the new `original_max_position_embeddings` from rope_parameters\n     # For backward compatibility standardize the `rope_parameters_dict` if it uses old format\n-    standardize_rope_params(config)\n+    config.standardize_rope_params()\n     rope_parameters_dict = config.rope_parameters[layer_type] if layer_type is not None else config.rope_parameters\n \n     base = rope_parameters_dict[\"rope_theta\"]\n-    partial_rotary_factor = config.partial_rotary_factor if hasattr(config, \"partial_rotary_factor\") else 1.0\n+    partial_rotary_factor = rope_parameters_dict.get(\"partial_rotary_factor\", 1.0)\n     head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n     dim = int(head_dim * partial_rotary_factor)\n     max_position_embeddings = config.max_position_embeddings\n@@ -302,7 +253,7 @@ def _compute_dynamic_ntk_parameters(\n \n \n def _compute_yarn_parameters(\n-    config: PreTrainedConfig,\n+    config: \"PreTrainedConfig\",\n     device: \"torch.device\",\n     seq_len: Optional[int] = None,\n     layer_type: Optional[str] = None,\n@@ -312,7 +263,7 @@ def _compute_yarn_parameters(\n     [original paper](https://huggingface.co/papers/2309.00071)\n \n     Args:\n-        config ([`~transformers.PreTrainedConfig`]):\n+        config ([`~transformers.\"PreTrainedConfig\"`]):\n             The model configuration. This function assumes that the config will provide at least the following\n             properties:\n \n@@ -360,11 +311,11 @@ def _compute_yarn_parameters(\n         post-processing scaling factor applied to the computed cos/sin.\n     \"\"\"\n     # For backward compatibility standardize the `rope_parameters_dict` if it uses old format\n-    standardize_rope_params(config)\n+    config.standardize_rope_params()\n     rope_parameters_dict = config.rope_parameters[layer_type] if layer_type is not None else config.rope_parameters\n \n     base = rope_parameters_dict[\"rope_theta\"]\n-    partial_rotary_factor = config.partial_rotary_factor if hasattr(config, \"partial_rotary_factor\") else 1.0\n+    partial_rotary_factor = rope_parameters_dict.get(\"partial_rotary_factor\", 1.0)\n     head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n     dim = int(head_dim * partial_rotary_factor)\n \n@@ -440,7 +391,7 @@ def linear_ramp_factor(min, max, dim):\n \n \n def _compute_longrope_parameters(\n-    config: PreTrainedConfig,\n+    config: \"PreTrainedConfig\",\n     device: \"torch.device\",\n     seq_len: Optional[int] = None,\n     layer_type: Optional[str] = None,\n@@ -450,7 +401,7 @@ def _compute_longrope_parameters(\n     [original implementation](https://github.com/microsoft/LongRoPE)\n \n     Args:\n-        config ([`~transformers.PreTrainedConfig`]):\n+        config ([`~transformers.\"PreTrainedConfig\"`]):\n             The model configuration. This function assumes that the config will provide at least the following\n             properties:\n \n@@ -490,11 +441,11 @@ def _compute_longrope_parameters(\n     \"\"\"\n     # TODO (joao): use the new `original_max_position_embeddings` from rope_parameters\n     # For backward compatibility standardize the `rope_parameters_dict` if it uses old format\n-    standardize_rope_params(config)\n+    config.standardize_rope_params()\n     rope_parameters_dict = config.rope_parameters[layer_type] if layer_type is not None else config.rope_parameters\n \n     base = rope_parameters_dict[\"rope_theta\"]\n-    partial_rotary_factor = config.partial_rotary_factor if hasattr(config, \"partial_rotary_factor\") else 1.0\n+    partial_rotary_factor = rope_parameters_dict.get(\"partial_rotary_factor\", 1.0)\n     head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n     dim = int(head_dim * partial_rotary_factor)\n \n@@ -530,7 +481,7 @@ def _compute_longrope_parameters(\n \n \n def _compute_llama3_parameters(\n-    config: PreTrainedConfig,\n+    config: \"PreTrainedConfig\",\n     device: \"torch.device\",\n     seq_len: Optional[int] = None,\n     layer_type: Optional[str] = None,\n@@ -539,7 +490,7 @@ def _compute_llama3_parameters(\n     Computes the inverse frequencies for llama 3.1.\n \n     Args:\n-        config ([`~transformers.PreTrainedConfig`]):\n+        config ([`~transformers.\"PreTrainedConfig\"`]):\n             The model configuration. This function assumes that the config will provide at least the following\n             properties:\n \n@@ -574,12 +525,12 @@ def _compute_llama3_parameters(\n         post-processing scaling factor applied to the computed cos/sin.\n     \"\"\"\n     # For backward compatibility standardize the `rope_parameters_dict` if it uses old format\n-    standardize_rope_params(config)\n+    config.standardize_rope_params()\n     rope_parameters_dict = config.rope_parameters[layer_type] if layer_type is not None else config.rope_parameters\n \n     # Gets the default RoPE parameters\n     base = rope_parameters_dict[\"rope_theta\"]\n-    partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+    partial_rotary_factor = rope_parameters_dict.get(\"partial_rotary_factor\", 1.0)\n     head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n     dim = int(head_dim * partial_rotary_factor)\n     attention_factor = 1.0  # Unused in this type of RoPE\n@@ -620,279 +571,16 @@ def _compute_llama3_parameters(\n }\n \n \n-def _check_received_keys(\n-    rope_type: str,\n-    received_keys: set,\n-    required_keys: set,\n-    optional_keys: Optional[set] = None,\n-    ignore_keys: Optional[set] = None,\n-):\n-    \"\"\"Compare the received keys in `config.rope_parameters` against the expected and optional keys\"\"\"\n-    # BC: \"rope_type\" was originally \"type\" -- let's check for \"rope_type\" when \"type\" is present\n-    if \"type\" in received_keys:\n-        received_keys -= {\"type\"}\n-        required_keys.add(\"rope_type\")\n-\n-    # Some models need to store model-specific keys, and we don't want to throw warning at them\n-    if ignore_keys is not None:\n-        received_keys -= ignore_keys\n-\n-    missing_keys = required_keys - received_keys\n-    if missing_keys:\n-        raise KeyError(f\"Missing required keys in `rope_parameters` for 'rope_type'='{rope_type}': {missing_keys}\")\n-\n-    if optional_keys is not None:\n-        unused_keys = received_keys - required_keys - optional_keys\n-    else:\n-        unused_keys = received_keys - required_keys\n-    if unused_keys:\n-        logger.warning(f\"Unrecognized keys in `rope_parameters` for 'rope_type'='{rope_type}': {unused_keys}\")\n-\n-\n-def _validate_default_rope_parameters(\n-    rope_parameters: dict, config: Optional[PreTrainedConfig] = None, ignore_keys: Optional[set] = None\n-):\n-    required_keys = {\"rope_type\", \"rope_theta\"}\n-    received_keys = set(rope_parameters.keys())\n-    rope_type = rope_parameters[\"rope_type\"]\n-    _check_received_keys(rope_type, received_keys, required_keys, ignore_keys=ignore_keys)\n-\n-\n-def _validate_linear_scaling_rope_parameters(\n-    rope_parameters: dict, config: Optional[PreTrainedConfig] = None, ignore_keys: Optional[set] = None\n-):\n-    required_keys = {\"rope_type\", \"factor\", \"rope_theta\"}\n-    received_keys = set(rope_parameters.keys())\n-    rope_type = rope_parameters[\"rope_type\"]\n-    _check_received_keys(rope_type, received_keys, required_keys, ignore_keys=ignore_keys)\n-\n-    factor = rope_parameters[\"factor\"]\n-    if factor is None or not isinstance(factor, float) or factor < 1.0:\n-        logger.warning(f\"`rope_parameters`'s factor field must be a float >= 1, got {factor}\")\n-\n-\n-def _validate_dynamic_scaling_rope_parameters(\n-    rope_parameters: dict, config: Optional[PreTrainedConfig] = None, ignore_keys: Optional[set] = None\n-):\n-    # TODO (joao): update logic for the inclusion of `original_max_position_embeddings`\n-    optional_keys = {\"original_max_position_embeddings\"}\n-    required_keys = {\"rope_type\", \"factor\"}\n-    received_keys = set(rope_parameters.keys())\n-    rope_type = rope_parameters[\"rope_type\"]\n-    _check_received_keys(rope_type, received_keys, required_keys, optional_keys, ignore_keys=ignore_keys)\n-\n-    factor = rope_parameters[\"factor\"]\n-    if factor is None or not isinstance(factor, float) or factor < 1.0:\n-        logger.warning(f\"`rope_parameters`'s factor field must be a float >= 1, got {factor}\")\n-\n-\n-def _validate_yarn_parameters(\n-    rope_parameters: dict, config: Optional[PreTrainedConfig] = None, ignore_keys: Optional[set] = None\n-):\n-    required_keys = {\"rope_type\", \"factor\", \"rope_theta\"}\n-    optional_keys = {\n-        \"attention_factor\",\n-        \"beta_fast\",\n-        \"beta_slow\",\n-        \"original_max_position_embeddings\",\n-        \"mscale\",\n-        \"mscale_all_dim\",\n-    }\n-    received_keys = set(rope_parameters.keys())\n-    rope_type = rope_parameters[\"rope_type\"]\n-    _check_received_keys(rope_type, received_keys, required_keys, optional_keys, ignore_keys=ignore_keys)\n-\n-    factor = rope_parameters[\"factor\"]\n-    if factor is None or not isinstance(factor, float) or factor < 1.0:\n-        logger.warning(f\"`rope_parameters`'s factor field must be a float >= 1, got {factor}\")\n-\n-    attention_factor = rope_parameters.get(\"attention_factor\")\n-    if attention_factor is not None and (not isinstance(attention_factor, float) or attention_factor < 0):\n-        logger.warning(\n-            f\"`rope_parameters`'s attention_factor field must be a float greater than 0, got {attention_factor}\"\n-        )\n-    beta_fast = rope_parameters.get(\"beta_fast\")\n-    if beta_fast is not None and not isinstance(beta_fast, float):\n-        logger.warning(f\"`rope_parameters`'s beta_fast field must be a float, got {beta_fast}\")\n-    beta_slow = rope_parameters.get(\"beta_slow\")\n-    if beta_slow is not None and not isinstance(beta_slow, float):\n-        logger.warning(f\"`rope_parameters`'s beta_slow field must be a float, got {beta_slow}\")\n-\n-    if (beta_fast or 32) < (beta_slow or 1):\n-        logger.warning(\n-            f\"`rope_parameters`'s beta_fast field must be greater than beta_slow, got beta_fast={beta_fast} \"\n-            f\"(defaults to 32 if None) and beta_slow={beta_slow} (defaults to 1 if None)\"\n-        )\n-\n-    # Models should set `config.rope_parameters[\"original_max_position_embeddings\"]` to their original (pre-yarn) context\n-    # length, with `config.max_position_embeddings` corresponding to their post-yarn context length.\n-    # However, for BC purposes, we allow the former to be unset.\n-    original_max_position_embeddings = config.rope_parameters.get(\"original_max_position_embeddings\")\n-    if original_max_position_embeddings is not None:\n-        # Double-check: `factor` should be the ratio between the pre-yarn and post-yarn context lengths.\n-        implicit_factor = config.max_position_embeddings / original_max_position_embeddings\n-        if implicit_factor != factor:\n-            logger.warning_once(\n-                f\"The explicitly set RoPE scaling factor (config.rope_parameters['factor'] = {factor}) does not match \"\n-                \"the ratio implicitly set by other parameters (implicit factor = \"\n-                \"post-yarn context length / pre-yarn context length = \"\n-                \"config.max_position_embeddings / config.rope_parameters['original_max_position_embeddings'] = \"\n-                f\"{implicit_factor}). Using the explicit factor ({factor}) in YaRN. This may cause unexpected \"\n-                \"behaviour in model usage, please correct the 'max_position_embeddings' fields in the model config.\"\n-            )\n-    # No `config.rope_parameters[\"original_max_position_embeddings\"]`. Is `config.max_position_embeddings` the\n-    # pre-yarn or the post-yarn context length?\n-    # BC: we assume it is the pre-yarn context length.\n-    else:\n-        logger.warning_once(\n-            \"config.rope_parameters['original_max_position_embeddings'], the pre-yarn context length, is unset. We will \"\n-            \"**assume** config.max_position_embeddings holds the pre-yarn context length. Some use cases may expect \"\n-            \"config.max_position_embeddings to hold the post-yarn context length (pre-yarn context length * \"\n-            \"factor) -- we recommend updating both fields for optimal downstream model usage.\"\n-        )\n-\n-\n-def _validate_longrope_parameters(rope_parameters: dict, config: PreTrainedConfig, ignore_keys: Optional[set] = None):\n-    required_keys = {\"rope_type\", \"short_factor\", \"long_factor\", \"rope_theta\"}\n-    # TODO (joao): update logic for the inclusion of `original_max_position_embeddings`\n-    optional_keys = {\"attention_factor\", \"factor\", \"original_max_position_embeddings\"}\n-    received_keys = set(rope_parameters.keys())\n-    rope_type = rope_parameters[\"rope_type\"]\n-    _check_received_keys(rope_type, received_keys, required_keys, optional_keys, ignore_keys=ignore_keys)\n-\n-    partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n-    head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n-    dim = int(head_dim * partial_rotary_factor)\n-\n-    short_factor = rope_parameters.get(\"short_factor\")\n-    if not isinstance(short_factor, list) and all(isinstance(x, (int, float)) for x in short_factor):\n-        logger.warning(f\"`rope_parameters`'s short_factor field must be a list of numbers, got {short_factor}\")\n-    if len(short_factor) != dim // 2:\n-        logger.warning(f\"`rope_parameters`'s short_factor field must have length {dim // 2}, got {len(short_factor)}\")\n-\n-    long_factor = rope_parameters.get(\"long_factor\")\n-    if not isinstance(long_factor, list) and all(isinstance(x, (int, float)) for x in long_factor):\n-        logger.warning(f\"`rope_parameters`'s long_factor field must be a list of numbers, got {long_factor}\")\n-    if len(long_factor) != dim // 2:\n-        logger.warning(f\"`rope_parameters`'s long_factor field must have length {dim // 2}, got {len(long_factor)}\")\n-\n-    # Handle Phi3 divergence: prefer the use of `attention_factor` and/or `factor` over\n-    # `original_max_position_embeddings` to compute internal variables. The latter lives outside `rope_parameters` and is\n-    # unique to longrope (= undesirable)\n-    if hasattr(config, \"original_max_position_embeddings\"):\n-        logger.warning_once(\n-            \"This model has set a `original_max_position_embeddings` field, to be used together with \"\n-            \"`max_position_embeddings` to determine a scaling factor. Please set the `factor` field of `rope_parameters`\"\n-            \"with this ratio instead -- we recommend the use of this field over `original_max_position_embeddings`, \"\n-            \"as it is compatible with most model architectures.\"\n-        )\n-    else:\n-        factor = rope_parameters.get(\"factor\")\n-        if factor is None:\n-            logger.warning(\"Missing required keys in `rope_parameters`: 'factor'\")\n-        elif not isinstance(factor, float) or factor < 1.0:\n-            logger.warning(f\"`rope_parameters`'s factor field must be a float >= 1, got {factor}\")\n-\n-        attention_factor = rope_parameters.get(\"attention_factor\")\n-        if attention_factor is not None:\n-            if not isinstance(attention_factor, float) or attention_factor < 0.0:\n-                logger.warning(\n-                    f\"`rope_parameters`'s attention_factor field must be a float greater than 0, got {attention_factor}\"\n-                )\n-\n-\n-def _validate_llama3_parameters(rope_parameters: dict, config: PreTrainedConfig, ignore_keys: Optional[set] = None):\n-    required_keys = {\n-        \"rope_type\",\n-        \"factor\",\n-        \"original_max_position_embeddings\",\n-        \"low_freq_factor\",\n-        \"high_freq_factor\",\n-        \"rope_theta\",\n-    }\n-    rope_type = rope_parameters[\"rope_type\"]\n-    received_keys = set(rope_parameters.keys())\n-    _check_received_keys(rope_type, received_keys, required_keys, ignore_keys=ignore_keys)\n-\n-    factor = rope_parameters[\"factor\"]\n-    if factor is None or not isinstance(factor, float) or factor < 1.0:\n-        logger.warning(f\"`rope_parameters`'s factor field must be a float >= 1, got {factor}\")\n-\n-    low_freq_factor = rope_parameters[\"low_freq_factor\"]\n-    high_freq_factor = rope_parameters[\"high_freq_factor\"]\n-    if low_freq_factor is None or not isinstance(low_freq_factor, float):\n-        logger.warning(f\"`rope_parameters`'s low_freq_factor field must be a float, got {low_freq_factor}\")\n-    if high_freq_factor is None or not isinstance(high_freq_factor, float):\n-        logger.warning(f\"`rope_parameters`'s high_freq_factor field must be a float, got {high_freq_factor}\")\n-    if high_freq_factor <= low_freq_factor:\n-        logger.warning(\n-            \"`rope_parameters`'s high_freq_factor field must be greater than low_freq_factor, got high_freq_factor=\"\n-            f\"{high_freq_factor} and low_freq_factor={low_freq_factor}\"\n-        )\n-\n-    original_max_position_embeddings = rope_parameters[\"original_max_position_embeddings\"]\n-    if original_max_position_embeddings is None or not isinstance(original_max_position_embeddings, int):\n-        logger.warning(\n-            \"`rope_parameters`'s original_max_position_embeddings field must be an integer, got \"\n-            f\"{original_max_position_embeddings}\"\n-        )\n-    if original_max_position_embeddings >= config.max_position_embeddings:\n-        logger.warning(\n-            \"`rope_parameters`'s original_max_position_embeddings field must be less than max_position_embeddings, got \"\n-            f\"{original_max_position_embeddings} and max_position_embeddings={config.max_position_embeddings}\"\n-        )\n-\n-\n-# Like `ROPE_INIT_FUNCTIONS`, this validation function mapping can be dynamically updated for custom RoPE types.\n-ROPE_VALIDATION_FUNCTIONS = {\n-    \"default\": _validate_default_rope_parameters,\n-    \"linear\": _validate_linear_scaling_rope_parameters,\n-    \"dynamic\": _validate_dynamic_scaling_rope_parameters,\n-    \"yarn\": _validate_yarn_parameters,\n-    \"longrope\": _validate_longrope_parameters,\n-    \"llama3\": _validate_llama3_parameters,\n-}\n-\n-\n-def rope_config_validation(config: PreTrainedConfig, ignore_keys: Optional[set] = None):\n-    \"\"\"\n-    Validate the RoPE config arguments, given a `PreTrainedConfig` object\n-    \"\"\"\n-    rope_parameters_dict = getattr(config, \"rope_parameters\", None)  # not a default parameter in `PreTrainedConfig`\n-    if rope_parameters_dict is None:\n-        return\n-\n-    if getattr(config, \"layer_types\", None) is not None and all(\n-        key in config.layer_types for key in rope_parameters_dict.keys()\n-    ):\n-        pass\n-    else:\n-        rope_parameters_dict = {\"full_attention\": rope_parameters_dict}\n-\n-    for rope_parameters in rope_parameters_dict.values():\n-        rope_type = rope_parameters.get(\"rope_type\", rope_parameters.get(\"type\", \"default\"))\n-        validation_fn = ROPE_VALIDATION_FUNCTIONS.get(rope_type)\n-\n-        rope_parameters[\"rope_type\"] = rope_type\n-        # BC: \"rope_theta\" was originally saved in config\n-        rope_parameters[\"rope_theta\"] = rope_parameters.get(\"rope_theta\", getattr(config, \"rope_theta\", None))\n-\n-        if validation_fn is not None:\n-            validation_fn(rope_parameters, config=config, ignore_keys=ignore_keys)\n-        else:\n-            logger.warning(\n-                f\"Missing validation function mapping in `ROPE_VALIDATION_FUNCTIONS` for 'rope_type'='{rope_type}'\"\n-            )\n-\n-\n-class RopeParameters(TypedDict):\n+class RopeParameters(TypedDict, total=False):\n     \"\"\"\n     Args:\n         rope_theta (`float`):\n             The base period of the RoPE embeddings.\n         rope_type (`str`, *optional*, defaults to \"default\"):\n             The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n             'llama3'], with 'default' being the original RoPE implementation.\n+        partial_rotary_factor (`float`, *optional*):\n+            The percentage of the query and key head embedding on which RoPE will be applied.\n         factor (`float`, *optional*):\n             Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n             most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n@@ -926,6 +614,7 @@ class RopeParameters(TypedDict):\n \n     rope_theta: float\n     rope_type: Optional[str]\n+    partial_rotary_factor: Optional[float]\n     factor: Optional[float]\n     original_max_position_embeddings: Optional[int]\n     attention_factor: Optional[float]\n@@ -935,3 +624,292 @@ class RopeParameters(TypedDict):\n     long_factor: Optional[list[float]]\n     low_freq_factor: Optional[float]\n     high_freq_factor: Optional[float]\n+\n+\n+class RotaryEmbeddingConfigMixin:\n+    \"\"\"\n+    A Mixin containing the functionality to standardize and validate RoPE parameters.\n+    \"\"\"\n+\n+    default_theta = 10_000.0\n+\n+    def convert_rope_params_to_dict(self, ignore_keys_at_rope_validation: Optional[set] = None, **kwargs):\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or self.rope_parameters\n+        self.rope_parameters = self.rope_parameters if self.rope_parameters is not None else {}\n+\n+        # Standardize and validate the correctness of rotary position embeddings parameters\n+        self.rope_parameters.setdefault(\"rope_theta\", kwargs.pop(\"rope_theta\", self.default_theta))\n+        if \"partial_rotary_factor\" in kwargs:\n+            self.rope_parameters.setdefault(\"partial_rotary_factor\", kwargs[\"partial_rotary_factor\"])\n+            ignore_keys_at_rope_validation = {\"partial_rotary_factor\"}\n+\n+        self.standardize_rope_params()\n+        self.validate_rope(ignore_keys=ignore_keys_at_rope_validation)\n+        return kwargs\n+\n+    def standardize_rope_params(self):\n+        \"\"\"\n+        Helper to standardize the config's rope params field by ensuring the params are defined for each\n+        later type. For old model the fn will duplicate a single rope param in each layer type (backward compatibility)\n+        \"\"\"\n+        # Move `rope_theta` and `partial_rotary_factor` to the params dict, if not there yet\n+        rope_theta = getattr(self, \"rope_theta\", None)\n+        partial_rotary_factor = getattr(self, \"partial_rotary_factor\", None)\n+        rope_parameters = self.rope_parameters\n+\n+        # Case 1: RoPE param keys do not intersect with possible `layer_types` -> one global dict\n+        if getattr(self, \"layer_types\", None) is None or not set(rope_parameters.keys()).issubset(self.layer_types):\n+            rope_parameters.setdefault(\"rope_type\", rope_parameters.get(\"type\", \"default\"))\n+            rope_parameters.setdefault(\"rope_theta\", rope_theta)\n+            if partial_rotary_factor is not None:\n+                rope_parameters[\"partial_rotary_factor\"] = partial_rotary_factor\n+        # Case 2: different RoPE for each layer -> several params as nested dict\n+        else:\n+            for layer_type in self.layer_types:\n+                rope_parameters[layer_type].setdefault(\"rope_type\", rope_parameters[layer_type].get(\"type\", \"default\"))\n+                rope_parameters[layer_type].setdefault(\"rope_theta\", rope_theta)\n+                if partial_rotary_factor is not None:\n+                    rope_parameters[layer_type][\"partial_rotary_factor\"] = partial_rotary_factor\n+\n+        self.rope_parameters = rope_parameters\n+\n+    def validate_rope(self: \"PreTrainedConfig\", ignore_keys: Optional[set] = None):\n+        \"\"\"\n+        Validate the RoPE config arguments, given a `\"PreTrainedConfig\"` object\n+        \"\"\"\n+        rope_parameters_dict = self.rope_parameters\n+        if rope_parameters_dict is None:\n+            return\n+\n+        if getattr(self, \"layer_types\", None) is not None and set(rope_parameters_dict.keys()).issubset(\n+            self.layer_types\n+        ):\n+            pass\n+        else:\n+            rope_parameters_dict = {\"full_attention\": rope_parameters_dict}\n+\n+        for rope_parameters in rope_parameters_dict.values():\n+            rope_type = rope_parameters.get(\"rope_type\", rope_parameters.get(\"type\", \"default\"))\n+            validation_fn = getattr(self, f\"_validate_{rope_type}_rope_parameters\")\n+            rope_parameters[\"rope_type\"] = rope_type\n+\n+            if validation_fn is not None:\n+                validation_fn(rope_parameters, ignore_keys=ignore_keys)\n+            else:\n+                logger.warning(\n+                    f\"Missing validation function mapping in `ROPE_VALIDATION_FUNCTIONS` for 'rope_type'='{rope_type}'\"\n+                )\n+\n+    def _validate_default_rope_parameters(self, rope_parameters: dict, ignore_keys: Optional[set] = None):\n+        required_keys = {\"rope_type\", \"rope_theta\"}\n+        received_keys = set(rope_parameters.keys())\n+        rope_type = rope_parameters[\"rope_type\"]\n+        self._check_received_keys(rope_type, received_keys, required_keys, ignore_keys=ignore_keys)\n+\n+    def _validate_linear_rope_parameters(self, rope_parameters: dict, ignore_keys: Optional[set] = None):\n+        required_keys = {\"rope_type\", \"factor\", \"rope_theta\"}\n+        received_keys = set(rope_parameters.keys())\n+        rope_type = rope_parameters[\"rope_type\"]\n+        self._check_received_keys(rope_type, received_keys, required_keys, ignore_keys=ignore_keys)\n+\n+        factor = rope_parameters[\"factor\"]\n+        if factor is None or not isinstance(factor, float) or factor < 1.0:\n+            logger.warning(f\"`rope_parameters`'s factor field must be a float >= 1, got {factor}\")\n+\n+    def _validate_dynamic_rope_parameters(self, rope_parameters: dict, ignore_keys: Optional[set] = None):\n+        # TODO (joao): update logic for the inclusion of `original_max_position_embeddings`\n+        optional_keys = {\"original_max_position_embeddings\"}\n+        required_keys = {\"rope_type\", \"factor\"}\n+        received_keys = set(rope_parameters.keys())\n+        rope_type = rope_parameters[\"rope_type\"]\n+        self._check_received_keys(rope_type, received_keys, required_keys, optional_keys, ignore_keys=ignore_keys)\n+\n+        factor = rope_parameters[\"factor\"]\n+        if factor is None or not isinstance(factor, float) or factor < 1.0:\n+            logger.warning(f\"`rope_parameters`'s factor field must be a float >= 1, got {factor}\")\n+\n+    def _validate_yarn_rope_parameters(self, rope_parameters: dict, ignore_keys: Optional[set] = None):\n+        required_keys = {\"rope_type\", \"factor\", \"rope_theta\"}\n+        optional_keys = {\n+            \"attention_factor\",\n+            \"beta_fast\",\n+            \"beta_slow\",\n+            \"original_max_position_embeddings\",\n+            \"mscale\",\n+            \"mscale_all_dim\",\n+        }\n+        received_keys = set(rope_parameters.keys())\n+        rope_type = rope_parameters[\"rope_type\"]\n+        self._check_received_keys(rope_type, received_keys, required_keys, optional_keys, ignore_keys=ignore_keys)\n+\n+        factor = rope_parameters[\"factor\"]\n+        if factor is None or not isinstance(factor, float) or factor < 1.0:\n+            logger.warning(f\"`rope_parameters`'s factor field must be a float >= 1, got {factor}\")\n+\n+        attention_factor = rope_parameters.get(\"attention_factor\")\n+        if attention_factor is not None and (not isinstance(attention_factor, float) or attention_factor < 0):\n+            logger.warning(\n+                f\"`rope_parameters`'s attention_factor field must be a float greater than 0, got {attention_factor}\"\n+            )\n+        beta_fast = rope_parameters.get(\"beta_fast\")\n+        if beta_fast is not None and not isinstance(beta_fast, float):\n+            logger.warning(f\"`rope_parameters`'s beta_fast field must be a float, got {beta_fast}\")\n+        beta_slow = rope_parameters.get(\"beta_slow\")\n+        if beta_slow is not None and not isinstance(beta_slow, float):\n+            logger.warning(f\"`rope_parameters`'s beta_slow field must be a float, got {beta_slow}\")\n+\n+        if (beta_fast or 32) < (beta_slow or 1):\n+            logger.warning(\n+                f\"`rope_parameters`'s beta_fast field must be greater than beta_slow, got beta_fast={beta_fast} \"\n+                f\"(defaults to 32 if None) and beta_slow={beta_slow} (defaults to 1 if None)\"\n+            )\n+\n+        # Models should set `config.rope_parameters[\"original_max_position_embeddings\"]` to their original (pre-yarn) context\n+        # length, with `config.max_position_embeddings` corresponding to their post-yarn context length.\n+        # However, for BC purposes, we allow the former to be unset.\n+        original_max_position_embeddings = self.rope_parameters.get(\"original_max_position_embeddings\")\n+        if original_max_position_embeddings is not None:\n+            # Double-check: `factor` should be the ratio between the pre-yarn and post-yarn context lengths.\n+            implicit_factor = self.max_position_embeddings / original_max_position_embeddings\n+            if implicit_factor != factor:\n+                logger.warning_once(\n+                    f\"The explicitly set RoPE scaling factor (config.rope_parameters['factor'] = {factor}) does not match \"\n+                    \"the ratio implicitly set by other parameters (implicit factor = \"\n+                    \"post-yarn context length / pre-yarn context length = \"\n+                    \"config.max_position_embeddings / config.rope_parameters['original_max_position_embeddings'] = \"\n+                    f\"{implicit_factor}). Using the explicit factor ({factor}) in YaRN. This may cause unexpected \"\n+                    \"behaviour in model usage, please correct the 'max_position_embeddings' fields in the model config.\"\n+                )\n+        # No `config.rope_parameters[\"original_max_position_embeddings\"]`. Is `config.max_position_embeddings` the\n+        # pre-yarn or the post-yarn context length?\n+        # BC: we assume it is the pre-yarn context length.\n+        else:\n+            logger.warning_once(\n+                \"config.rope_parameters['original_max_position_embeddings'], the pre-yarn context length, is unset. We will \"\n+                \"**assume** config.max_position_embeddings holds the pre-yarn context length. Some use cases may expect \"\n+                \"config.max_position_embeddings to hold the post-yarn context length (pre-yarn context length * \"\n+                \"factor) -- we recommend updating both fields for optimal downstream model usage.\"\n+            )\n+\n+    def _validate_longrope_rope_parameters(self, rope_parameters: dict, ignore_keys: Optional[set] = None):\n+        required_keys = {\"rope_type\", \"short_factor\", \"long_factor\", \"rope_theta\"}\n+        # TODO (joao): update logic for the inclusion of `original_max_position_embeddings`\n+        optional_keys = {\"attention_factor\", \"factor\", \"original_max_position_embeddings\"}\n+        received_keys = set(rope_parameters.keys())\n+        rope_type = rope_parameters[\"rope_type\"]\n+        self._check_received_keys(rope_type, received_keys, required_keys, optional_keys, ignore_keys=ignore_keys)\n+\n+        partial_rotary_factor = rope_parameters.get(\"partial_rotary_factor\", 1.0)\n+        head_dim = getattr(self, \"head_dim\", self.hidden_size // self.num_attention_heads)\n+        dim = int(head_dim * partial_rotary_factor)\n+\n+        short_factor = rope_parameters.get(\"short_factor\")\n+        if not isinstance(short_factor, list) and all(isinstance(x, (int, float)) for x in short_factor):\n+            logger.warning(f\"`rope_parameters`'s short_factor field must be a list of numbers, got {short_factor}\")\n+        if len(short_factor) != dim // 2:\n+            logger.warning(\n+                f\"`rope_parameters`'s short_factor field must have length {dim // 2}, got {len(short_factor)}\"\n+            )\n+\n+        long_factor = rope_parameters.get(\"long_factor\")\n+        if not isinstance(long_factor, list) and all(isinstance(x, (int, float)) for x in long_factor):\n+            logger.warning(f\"`rope_parameters`'s long_factor field must be a list of numbers, got {long_factor}\")\n+        if len(long_factor) != dim // 2:\n+            logger.warning(\n+                f\"`rope_parameters`'s long_factor field must have length {dim // 2}, got {len(long_factor)}\"\n+            )\n+\n+        # Handle Phi3 divergence: prefer the use of `attention_factor` and/or `factor` over\n+        # `original_max_position_embeddings` to compute internal variables. The latter lives outside `rope_parameters` and is\n+        # unique to longrope (= undesirable)\n+        if hasattr(self, \"original_max_position_embeddings\"):\n+            logger.warning_once(\n+                \"This model has set a `original_max_position_embeddings` field, to be used together with \"\n+                \"`max_position_embeddings` to determine a scaling factor. Please set the `factor` field of `rope_parameters`\"\n+                \"with this ratio instead -- we recommend the use of this field over `original_max_position_embeddings`, \"\n+                \"as it is compatible with most model architectures.\"\n+            )\n+        else:\n+            factor = rope_parameters.get(\"factor\")\n+            if factor is None:\n+                logger.warning(\"Missing required keys in `rope_parameters`: 'factor'\")\n+            elif not isinstance(factor, float) or factor < 1.0:\n+                logger.warning(f\"`rope_parameters`'s factor field must be a float >= 1, got {factor}\")\n+\n+            attention_factor = rope_parameters.get(\"attention_factor\")\n+            if attention_factor is not None:\n+                if not isinstance(attention_factor, float) or attention_factor < 0.0:\n+                    logger.warning(\n+                        f\"`rope_parameters`'s attention_factor field must be a float greater than 0, got {attention_factor}\"\n+                    )\n+\n+    def _validate_llama3_rope_parameters(self, rope_parameters: dict, ignore_keys: Optional[set] = None):\n+        required_keys = {\n+            \"rope_type\",\n+            \"factor\",\n+            \"original_max_position_embeddings\",\n+            \"low_freq_factor\",\n+            \"high_freq_factor\",\n+            \"rope_theta\",\n+        }\n+        rope_type = rope_parameters[\"rope_type\"]\n+        received_keys = set(rope_parameters.keys())\n+        self._check_received_keys(rope_type, received_keys, required_keys, ignore_keys=ignore_keys)\n+\n+        factor = rope_parameters[\"factor\"]\n+        if factor is None or not isinstance(factor, float) or factor < 1.0:\n+            logger.warning(f\"`rope_parameters`'s factor field must be a float >= 1, got {factor}\")\n+\n+        low_freq_factor = rope_parameters[\"low_freq_factor\"]\n+        high_freq_factor = rope_parameters[\"high_freq_factor\"]\n+        if low_freq_factor is None or not isinstance(low_freq_factor, float):\n+            logger.warning(f\"`rope_parameters`'s low_freq_factor field must be a float, got {low_freq_factor}\")\n+        if high_freq_factor is None or not isinstance(high_freq_factor, float):\n+            logger.warning(f\"`rope_parameters`'s high_freq_factor field must be a float, got {high_freq_factor}\")\n+        if high_freq_factor <= low_freq_factor:\n+            logger.warning(\n+                \"`rope_parameters`'s high_freq_factor field must be greater than low_freq_factor, got high_freq_factor=\"\n+                f\"{high_freq_factor} and low_freq_factor={low_freq_factor}\"\n+            )\n+\n+        original_max_position_embeddings = rope_parameters[\"original_max_position_embeddings\"]\n+        if original_max_position_embeddings is None or not isinstance(original_max_position_embeddings, int):\n+            logger.warning(\n+                \"`rope_parameters`'s original_max_position_embeddings field must be an integer, got \"\n+                f\"{original_max_position_embeddings}\"\n+            )\n+        if original_max_position_embeddings >= self.max_position_embeddings:\n+            logger.warning(\n+                \"`rope_parameters`'s original_max_position_embeddings field must be less than max_position_embeddings, got \"\n+                f\"{original_max_position_embeddings} and max_position_embeddings={self.max_position_embeddings}\"\n+            )\n+\n+    @staticmethod\n+    def _check_received_keys(\n+        rope_type: str,\n+        received_keys: set,\n+        required_keys: set,\n+        optional_keys: Optional[set] = None,\n+        ignore_keys: Optional[set] = None,\n+    ):\n+        \"\"\"Compare the received keys in `config.rope_parameters` against the expected and optional keys\"\"\"\n+        # BC: \"rope_type\" was originally \"type\" -- let's check for \"rope_type\" when \"type\" is present\n+        if \"type\" in received_keys:\n+            received_keys -= {\"type\"}\n+            required_keys.add(\"rope_type\")\n+\n+        # Some models need to store model-specific keys, and we don't want to throw warning at them\n+        if ignore_keys is not None:\n+            received_keys -= ignore_keys\n+\n+        missing_keys = required_keys - received_keys\n+        if missing_keys:\n+            raise KeyError(f\"Missing required keys in `rope_parameters` for 'rope_type'='{rope_type}': {missing_keys}\")\n+\n+        if optional_keys is not None:\n+            unused_keys = received_keys - required_keys - optional_keys\n+        else:\n+            unused_keys = received_keys - required_keys\n+        if unused_keys:\n+            logger.warning(f\"Unrecognized keys in `rope_parameters` for 'rope_type'='{rope_type}': {unused_keys}\")"
        },
        {
            "sha": "5f25d3a5d094af89ffdc82f7f9da9021fbc1827a",
            "filename": "src/transformers/models/apertus/configuration_apertus.py",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fapertus%2Fconfiguration_apertus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fapertus%2Fconfiguration_apertus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fapertus%2Fconfiguration_apertus.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -22,7 +22,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class ApertusConfig(PreTrainedConfig):\n@@ -99,6 +99,7 @@ class ApertusConfig(PreTrainedConfig):\n \n     model_type = \"apertus\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    default_theta = 12000000.0\n     base_model_tp_plan = {\n         \"layers.*.self_attn.q_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n         \"layers.*.self_attn.k_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n@@ -160,14 +161,7 @@ def __init__(\n         self.use_cache = use_cache\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 12000000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "13408fa23919e18252a2520a135347a36eedd435",
            "filename": "src/transformers/models/apertus/modular_apertus.py",
            "status": "modified",
            "additions": 30,
            "deletions": 25,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodular_apertus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodular_apertus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodular_apertus.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -20,11 +20,11 @@\n from torch import nn\n \n from ...cache_utils import Cache\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...configuration_utils import PreTrainedConfig\n+from ...modeling_rope_utils import RopeParameters\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n-from ..llama.configuration_llama import LlamaConfig\n from ..llama.modeling_llama import (\n     LlamaAttention,\n     LlamaDecoderLayer,\n@@ -43,7 +43,7 @@\n logger = logging.get_logger(__name__)\n \n \n-class ApertusConfig(LlamaConfig):\n+class ApertusConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`ApertusModel`]. It is used to instantiate a Apertus\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n@@ -116,6 +116,8 @@ class ApertusConfig(LlamaConfig):\n     ```\"\"\"\n \n     model_type = \"apertus\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+    default_theta = 12000000.0\n     base_model_tp_plan = {\n         \"layers.*.self_attn.q_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n         \"layers.*.self_attn.k_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n@@ -124,6 +126,11 @@ class ApertusConfig(LlamaConfig):\n         \"layers.*.mlp.up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\",\n     }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n \n     def __init__(\n         self,\n@@ -154,35 +161,33 @@ def __init__(\n         attention_dropout: Optional[float] = 0.0,\n         **kwargs,\n     ):\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+\n+        # for backward compatibility\n+        if num_key_value_heads is None:\n+            num_key_value_heads = num_attention_heads\n+\n+        self.num_key_value_heads = num_key_value_heads\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+        self.rope_parameters = rope_parameters\n+\n         super().__init__(\n-            vocab_size=vocab_size,\n-            hidden_size=hidden_size,\n-            intermediate_size=intermediate_size,\n-            num_hidden_layers=num_hidden_layers,\n-            num_attention_heads=num_attention_heads,\n-            num_key_value_heads=num_key_value_heads,\n-            hidden_act=hidden_act,\n-            max_position_embeddings=max_position_embeddings,\n-            initializer_range=initializer_range,\n-            rms_norm_eps=rms_norm_eps,\n-            use_cache=use_cache,\n             pad_token_id=pad_token_id,\n             bos_token_id=bos_token_id,\n             eos_token_id=eos_token_id,\n             tie_word_embeddings=tie_word_embeddings,\n-            rope_parameters=rope_parameters,\n-            attention_bias=attention_bias,\n-            attention_dropout=attention_dropout,\n             **kwargs,\n         )\n-        del self.pretraining_tp\n-        del self.mlp_bias\n-        del self.head_dim\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 12000000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n \n \n class ApertusMLP(NemotronMLP):"
        },
        {
            "sha": "50d120f3d7cbf272650949bfbc5d78cb30e88b4b",
            "filename": "src/transformers/models/arcee/configuration_arcee.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Farcee%2Fconfiguration_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Farcee%2Fconfiguration_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fconfiguration_arcee.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -22,7 +22,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class ArceeConfig(PreTrainedConfig):\n@@ -163,14 +163,7 @@ def __init__(\n         self.attention_dropout = attention_dropout\n         self.mlp_bias = mlp_bias\n         self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "268a9307c741a549d30d9ce1a192e382c73e9078",
            "filename": "src/transformers/models/aria/configuration_aria.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -21,7 +21,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n@@ -168,14 +168,7 @@ def __init__(\n         self.attention_dropout = attention_dropout\n         self.mlp_bias = mlp_bias\n         self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "fe03c6ad0f666596d1f48c4ea6bf322adbb47495",
            "filename": "src/transformers/models/bamba/configuration_bamba.py",
            "status": "modified",
            "additions": 3,
            "deletions": 11,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fbamba%2Fconfiguration_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fbamba%2Fconfiguration_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fconfiguration_bamba.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -17,7 +17,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -171,16 +171,6 @@ def __init__(\n         self.num_logits_to_keep = num_logits_to_keep\n \n         self.attn_layer_indices = attn_layer_indices\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        self.partial_rotary_factor = 0.5\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n-\n         mamba_intermediate = mamba_expand * hidden_size\n \n         if mamba_intermediate % mamba_n_heads != 0:\n@@ -203,6 +193,8 @@ def __init__(\n         self.mamba_conv_bias = mamba_conv_bias\n         self.mamba_proj_bias = mamba_proj_bias\n         self.z_loss_coefficient = z_loss_coefficient\n+        self.rope_parameters = rope_parameters\n+        kwargs[\"partial_rotary_factor\"] = 0.5  # hardcode for BC\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "27d77785722c69ed19afe345c085994bc45a6393",
            "filename": "src/transformers/models/bitnet/configuration_bitnet.py",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fbitnet%2Fconfiguration_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fbitnet%2Fconfiguration_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fconfiguration_bitnet.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -16,7 +16,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -97,6 +97,7 @@ class BitNetConfig(PreTrainedConfig):\n \n     model_type = \"bitnet\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    default_theta = 500000.0\n \n     def __init__(\n         self,\n@@ -138,14 +139,7 @@ def __init__(\n         self.use_cache = use_cache\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 500000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "326176af5e9a2b2d317ce534a96815273ad570bd",
            "filename": "src/transformers/models/blt/configuration_blt.py",
            "status": "modified",
            "additions": 11,
            "deletions": 38,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fblt%2Fconfiguration_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fblt%2Fconfiguration_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblt%2Fconfiguration_blt.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -17,7 +17,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -30,6 +30,7 @@ class BltLocalEncoderConfig(PreTrainedConfig):\n     \"\"\"\n \n     model_type = \"blt_local_encoder\"\n+    default_theta = 500000.0\n \n     def __init__(\n         self,\n@@ -65,14 +66,7 @@ def __init__(\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_act = hidden_act\n         self.initializer_range = initializer_range\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 500000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         # Remove tie_word_embeddings from kwargs to avoid duplicate parameter error\n         kwargs.pop(\"tie_word_embeddings\", None)\n@@ -85,6 +79,7 @@ class BltLocalDecoderConfig(PreTrainedConfig):\n     \"\"\"\n \n     model_type = \"blt_local_decoder\"\n+    default_theta = 500000.0\n \n     def __init__(\n         self,\n@@ -120,14 +115,7 @@ def __init__(\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_act = hidden_act\n         self.initializer_range = initializer_range\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 500000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         # Remove tie_word_embeddings from kwargs to avoid duplicate parameter error\n         kwargs.pop(\"tie_word_embeddings\", None)\n@@ -140,6 +128,7 @@ class BltGlobalTransformerConfig(PreTrainedConfig):\n     \"\"\"\n \n     model_type = \"blt_global_transformer\"\n+    default_theta = 500000.0\n \n     def __init__(\n         self,\n@@ -167,13 +156,7 @@ def __init__(\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_act = hidden_act\n         self.initializer_range = initializer_range\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 500000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n+        self.rope_parameters = rope_parameters\n \n         # Remove tie_word_embeddings from kwargs to avoid duplicate parameter error\n         kwargs.pop(\"tie_word_embeddings\", None)\n@@ -247,13 +230,7 @@ def __init__(\n         self.hidden_act = \"silu\"  # Blt uses silu activation\n         self.intermediate_size = intermediate_size or int(8 * self.hidden_size / 3)\n         self.initializer_range = initializer_range\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n+        self.rope_parameters = rope_parameters\n \n         # Remove tie_word_embeddings from kwargs to avoid duplicate parameter error\n         kwargs.pop(\"tie_word_embeddings\", None)\n@@ -329,6 +306,7 @@ class BltConfig(PreTrainedConfig):\n \n     model_type = \"blt\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    default_theta = 500000.0\n     sub_configs = {\n         \"patcher_config\": BltPatcherConfig,\n         \"encoder_config\": BltLocalEncoderConfig,\n@@ -375,13 +353,6 @@ def __init__(\n         self.realtime_patching = kwargs.get(\"realtime_patching\", True)\n         self.patching_threshold_add = kwargs.get(\"patching_threshold_add\")\n         self.monotonicity = kwargs.get(\"monotonicity\", False)\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 500000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n \n         # Cross attention configurations\n         self.cross_attn_k = cross_attn_k\n@@ -434,6 +405,8 @@ def __init__(\n             encoder_cross_output_size if encoder_cross_output_size != self.global_config.hidden_size else None\n         )\n \n+        self.rope_parameters = rope_parameters\n+\n         # Remove tie_word_embeddings from kwargs to avoid duplicate parameter error\n         kwargs.pop(\"tie_word_embeddings\", None)\n         super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)"
        },
        {
            "sha": "d22cecb87ef1a408e69bd6424d49251affd32fbc",
            "filename": "src/transformers/models/chameleon/configuration_chameleon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconfiguration_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconfiguration_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconfiguration_chameleon.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -17,7 +17,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -230,13 +230,7 @@ def __init__(\n         self.attention_dropout = attention_dropout\n         self.model_parallel_size = model_parallel_size\n         self.swin_norm = swin_norm\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n+        self.rope_parameters = rope_parameters\n \n         if vq_config is None:\n             vq_config = {}"
        },
        {
            "sha": "6b9e40e5e14358eed9d4fcd962115a8bd2208032",
            "filename": "src/transformers/models/cohere/configuration_cohere.py",
            "status": "modified",
            "additions": 3,
            "deletions": 8,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fcohere%2Fconfiguration_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fcohere%2Fconfiguration_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fconfiguration_cohere.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -22,7 +22,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -106,6 +106,7 @@ class CohereConfig(PreTrainedConfig):\n \n     model_type = \"cohere\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    default_theta = 500000.0\n     base_model_tp_plan = {\n         \"layers.*.self_attn.q_proj\": \"colwise\",\n         \"layers.*.self_attn.k_proj\": \"colwise\",\n@@ -165,13 +166,7 @@ def __init__(\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.use_qk_norm = use_qk_norm\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 500000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "7aadaaff94ce94280448fa0123f295da1ead6145",
            "filename": "src/transformers/models/cohere2/configuration_cohere2.py",
            "status": "modified",
            "additions": 11,
            "deletions": 16,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -22,7 +22,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class Cohere2Config(PreTrainedConfig):\n@@ -166,20 +166,10 @@ def __init__(\n         self.attention_dropout = attention_dropout\n         self.sliding_window = sliding_window\n         self.layer_types = layer_types\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n+\n         # Need to specify head_dim in the config so it can be used in the attention forward functions\n         self.head_dim = hidden_size // num_attention_heads\n \n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )\n-\n         # BC -> the pattern used to be a simple int, and it's still present in configs on the Hub\n         self._sliding_window_pattern = kwargs.get(\"sliding_window_pattern\", 4)\n \n@@ -192,10 +182,15 @@ def __init__(\n             ]\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n+\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n \n \n __all__ = [\"Cohere2Config\"]"
        },
        {
            "sha": "e4b83c535dd18b57052192feed2f22e0f41ec201",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 17,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -27,8 +27,6 @@\n from ...modeling_rope_utils import (\n     RopeParameters,\n     dynamic_rope_update,\n-    rope_config_validation,\n-    standardize_rope_params,\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n@@ -190,20 +188,10 @@ def __init__(\n         self.attention_dropout = attention_dropout\n         self.sliding_window = sliding_window\n         self.layer_types = layer_types\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n+\n         # Need to specify head_dim in the config so it can be used in the attention forward functions\n         self.head_dim = hidden_size // num_attention_heads\n \n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )\n-\n         # BC -> the pattern used to be a simple int, and it's still present in configs on the Hub\n         self._sliding_window_pattern = kwargs.get(\"sliding_window_pattern\", 4)\n \n@@ -216,10 +204,15 @@ def __init__(\n             ]\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n+\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n \n \n class Cohere2RotaryEmbedding(CohereRotaryEmbedding):"
        },
        {
            "sha": "d673444bb1c20b6608da01a875ddbd957d60db4b",
            "filename": "src/transformers/models/csm/configuration_csm.py",
            "status": "modified",
            "additions": 12,
            "deletions": 23,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fcsm%2Fconfiguration_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fcsm%2Fconfiguration_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fconfiguration_csm.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -16,7 +16,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n from ..auto.configuration_auto import AutoConfig\n \n@@ -103,6 +103,7 @@ class CsmDepthDecoderConfig(PreTrainedConfig):\n     model_type = \"csm_depth_decoder_model\"\n     base_config_key = \"depth_decoder_config\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    default_theta = 500000.0\n \n     def __init__(\n         self,\n@@ -132,13 +133,6 @@ def __init__(\n         if kwargs.pop(\"tie_word_embeddings\", False):\n             raise ValueError(\"`tie_word_embeddings=True` is not supported for CsmDepthDecoderConfig\")\n \n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            tie_word_embeddings=False,\n-            **kwargs,\n-        )\n         self.num_codebooks = num_codebooks\n         self.vocab_size = vocab_size\n         self.backbone_hidden_size = backbone_hidden_size\n@@ -161,14 +155,15 @@ def __init__(\n         self.attention_dropout = attention_dropout\n         self.mlp_bias = mlp_bias\n         self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n+        self.rope_parameters = rope_parameters\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 500000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=False,\n+            **kwargs,\n+        )\n \n \n class CsmConfig(PreTrainedConfig):\n@@ -264,6 +259,7 @@ class CsmConfig(PreTrainedConfig):\n     model_type = \"csm\"\n     base_config_key = \"csm_config\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    default_theta = 500000.0\n     sub_configs = {\n         \"codec_config\": AutoConfig,\n         \"depth_decoder_config\": CsmDepthDecoderConfig,\n@@ -348,14 +344,7 @@ def __init__(\n         self.attention_dropout = attention_dropout\n         self.mlp_bias = mlp_bias\n         self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 500000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "6c482fd4fc65b09a67367fb42eff47d12b6046ae",
            "filename": "src/transformers/models/cwm/configuration_cwm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fcwm%2Fconfiguration_cwm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fcwm%2Fconfiguration_cwm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcwm%2Fconfiguration_cwm.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -22,7 +22,6 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n-from ...modeling_rope_utils import rope_config_validation, standardize_rope_params\n \n \n class CwmConfig(PreTrainedConfig):\n@@ -107,6 +106,7 @@ class CwmConfig(PreTrainedConfig):\n         \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n         \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n     }\n+    default_theta = 1_000_000.0\n \n     def __init__(\n         self,\n@@ -177,14 +177,7 @@ def __init__(\n         self.attention_dropout = attention_dropout\n         self.mlp_bias = mlp_bias\n         self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 1_000_000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "a2830174ddb04d504a2b7fb5b1a54ce0d652a945",
            "filename": "src/transformers/models/cwm/modular_cwm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fcwm%2Fmodular_cwm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fcwm%2Fmodular_cwm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcwm%2Fmodular_cwm.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -21,7 +21,6 @@\n from ...configuration_utils import layer_type_validation\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_outputs import BaseModelOutputWithPast\n-from ...modeling_rope_utils import standardize_rope_params\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n from ..llama.configuration_llama import LlamaConfig\n@@ -103,6 +102,7 @@ class CwmConfig(LlamaConfig):\n     \"\"\"\n \n     model_type = \"cwm\"\n+    default_theta = 1_000_000.0\n \n     def __init__(\n         self,\n@@ -182,10 +182,6 @@ def __init__(\n         # CWM models don't use attention bias, remove it from config\n         del self.attention_bias\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 1_000_000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-\n \n class CwmRotaryEmbedding(Qwen2RotaryEmbedding):\n     pass"
        },
        {
            "sha": "3399c8618f5ca0e34a2373a68157c06e69713f79",
            "filename": "src/transformers/models/dbrx/configuration_dbrx.py",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fdbrx%2Fconfiguration_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fdbrx%2Fconfiguration_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fconfiguration_dbrx.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -17,7 +17,7 @@\n from typing import Any, Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -221,13 +221,7 @@ def __init__(\n         if tie_word_embeddings:\n             raise ValueError(\"tie_word_embeddings is not supported for DBRX models.\")\n \n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        standardize_rope_params(self, rope_theta=10000.0)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n "
        },
        {
            "sha": "2b0ee668ae69c8f76505bd02697c5d275f90c0e5",
            "filename": "src/transformers/models/deepseek_v2/configuration_deepseek_v2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fconfiguration_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fconfiguration_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fconfiguration_deepseek_v2.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -22,7 +22,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class DeepseekV2Config(PreTrainedConfig):\n@@ -214,14 +214,7 @@ def __init__(\n         self.mlp_bias = mlp_bias\n \n         self.head_dim = qk_rope_head_dim\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "abf68f399b8bb625b673389a89a2c026050ae583",
            "filename": "src/transformers/models/deepseek_v3/configuration_deepseek_v3.py",
            "status": "modified",
            "additions": 18,
            "deletions": 14,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fconfiguration_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fconfiguration_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fconfiguration_deepseek_v3.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -19,7 +19,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n DEEPSEEK_PRETRAINED_CONFIG_ARCHIVE_MAP = {}\n@@ -225,19 +225,7 @@ def __init__(\n         self.use_cache = use_cache\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-\n-        for key in [\"beta_fast\", \"beta_slow\", \"factor\"]:\n-            if key in self.rope_parameters:\n-                self.rope_parameters[key] = float(self.rope_parameters[key])\n-\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             pad_token_id=pad_token_id,\n@@ -247,5 +235,21 @@ def __init__(\n             **kwargs,\n         )\n \n+    def convert_rope_params_to_dict(self, ignore_keys_at_rope_validation: Optional[set] = None, **kwargs):\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or self.rope_parameters\n+        self.rope_parameters = self.rope_parameters if self.rope_parameters is not None else {}\n+\n+        # Standardize and validate the correctness of rotary position embeddings parameters\n+        self.rope_parameters.setdefault(\"rope_theta\", kwargs.pop(\"rope_theta\", self.default_theta))\n+        self.standardize_rope_params()\n+        self.validate_rope(ignore_keys=ignore_keys_at_rope_validation)\n+\n+        # Convert to float because RoPE fn expect a float. Models on the hub were saved as int\n+        for key in [\"beta_fast\", \"beta_slow\", \"factor\"]:\n+            if key in self.rope_parameters:\n+                self.rope_parameters[key] = float(self.rope_parameters[key])\n+        return kwargs\n+\n \n __all__ = [\"DeepseekV3Config\"]"
        },
        {
            "sha": "7927d299ca8beeb59dc6e7dcdb000fa4cc77cf01",
            "filename": "src/transformers/models/dia/configuration_dia.py",
            "status": "modified",
            "additions": 5,
            "deletions": 17,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fdia%2Fconfiguration_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fdia%2Fconfiguration_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fconfiguration_dia.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -17,7 +17,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -92,14 +92,8 @@ def __init__(\n         self.num_key_value_heads = num_key_value_heads\n         self.hidden_act = hidden_act\n         self.initializer_range = initializer_range\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n+\n         super().__init__(**kwargs)\n \n \n@@ -198,14 +192,8 @@ def __init__(\n         self.num_channels = num_channels\n         self.initializer_range = initializer_range\n         self.use_cache = use_cache\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n+\n         super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n \n "
        },
        {
            "sha": "5f9a731f1bd1e1c7770a51eec36e784221b1b98b",
            "filename": "src/transformers/models/diffllama/configuration_diffllama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fconfiguration_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fconfiguration_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fconfiguration_diffllama.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -20,7 +20,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class DiffLlamaConfig(PreTrainedConfig):\n@@ -145,14 +145,7 @@ def __init__(\n         self.attention_dropout = attention_dropout\n         self.lambda_std_dev = lambda_std_dev\n         self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "aa139d084ddf2847a6997035b870c9738768226c",
            "filename": "src/transformers/models/doge/configuration_doge.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fdoge%2Fconfiguration_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fdoge%2Fconfiguration_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fconfiguration_doge.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -23,7 +23,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class DogeConfig(PreTrainedConfig):\n@@ -189,14 +189,7 @@ def __init__(\n         self.norm_topk_prob = norm_topk_prob\n         self.output_router_logits = output_router_logits\n         self.router_aux_loss_coef = router_aux_loss_coef\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         # for backward compatibility\n         if num_key_value_heads is None:"
        },
        {
            "sha": "8d53ef5ec6819b401eb7d2314bdc02d0710b931b",
            "filename": "src/transformers/models/doge/modular_doge.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -31,7 +31,7 @@\n from ...integrations.flex_attention import compile_friendly_flex_attention\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...modeling_utils import AttentionInterface, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, is_torch_flex_attn_available, logging\n@@ -218,14 +218,7 @@ def __init__(\n         self.norm_topk_prob = norm_topk_prob\n         self.output_router_logits = output_router_logits\n         self.router_aux_loss_coef = router_aux_loss_coef\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         # for backward compatibility\n         if num_key_value_heads is None:"
        },
        {
            "sha": "57ded54fcc75981d631aee0e6dd61367d1345974",
            "filename": "src/transformers/models/dots1/configuration_dots1.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fdots1%2Fconfiguration_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fdots1%2Fconfiguration_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fconfiguration_dots1.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -15,7 +15,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -192,10 +192,6 @@ def __init__(\n         self.sliding_window = sliding_window\n         self.max_window_layers = max_window_layers\n \n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n         self.layer_types = layer_types\n         if self.layer_types is None:\n             self.layer_types = [\n@@ -206,10 +202,7 @@ def __init__(\n             ]\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             tie_word_embeddings=tie_word_embeddings,"
        },
        {
            "sha": "19c20a9a5fbf206956bc57fc67289d08a286f209",
            "filename": "src/transformers/models/efficientloftr/configuration_efficientloftr.py",
            "status": "modified",
            "additions": 3,
            "deletions": 15,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fconfiguration_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fconfiguration_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fconfiguration_efficientloftr.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -14,7 +14,6 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import rope_config_validation, standardize_rope_params\n \n \n class EfficientLoFTRConfig(PreTrainedConfig):\n@@ -67,10 +66,7 @@ class EfficientLoFTRConfig(PreTrainedConfig):\n         fine_kernel_size (`int`, *optional*, defaults to 8):\n             Kernel size used for the fine feature matching\n         batch_norm_eps (`float`, *optional*, defaults to 1e-05):\n-            The epsilon used by the batch normalization layers.\n-        partial_rotary_factor (`float`, *optional*, defaults to 4.0):\n-            Dim factor for the RoPE embeddings, in EfficientLoFTR, frequencies should be generated for\n-            the whole hidden_size, so this factor is used to compensate.\n+            The epsilon used by the batch normalization layers\n         rope_parameters (`RopeParameters`, *optional*):\n             Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n@@ -121,7 +117,6 @@ def __init__(\n         coarse_matching_border_removal: int = 2,\n         fine_kernel_size: int = 8,\n         batch_norm_eps: float = 1e-5,\n-        partial_rotary_factor: float = 4.0,\n         rope_parameters: Optional[dict] = None,\n         fine_matching_slice_dim: int = 8,\n         fine_matching_regress_temperature: float = 10.0,\n@@ -176,16 +171,9 @@ def __init__(\n         self.fine_matching_regress_temperature = fine_matching_regress_temperature\n \n         self.num_key_value_heads = num_attention_heads\n-        self.partial_rotary_factor = partial_rotary_factor\n         self.initializer_range = initializer_range\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n+        kwargs.setdefault(\"partial_rotary_factor\", 4.0)  # assign default for BC\n \n         super().__init__(**kwargs)\n "
        },
        {
            "sha": "cbe1d5e14f2681b043b726395202a2f29d402f32",
            "filename": "src/transformers/models/efficientloftr/modeling_efficientloftr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -126,7 +126,7 @@ def compute_default_rope_parameters(\n             post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n         \"\"\"\n         base = config.rope_parameters[\"rope_theta\"]\n-        partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+        partial_rotary_factor = config.rope_parameters.get(\"partial_rotary_factor\", 1.0)\n         head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n         dim = int(head_dim * partial_rotary_factor)\n "
        },
        {
            "sha": "546803916db49a714fa65a1961ca86659998a2db",
            "filename": "src/transformers/models/emu3/configuration_emu3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Femu3%2Fconfiguration_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Femu3%2Fconfiguration_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fconfiguration_emu3.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -17,7 +17,7 @@\n from typing import Optional, Union\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class Emu3VQVAEConfig(PreTrainedConfig):\n@@ -188,6 +188,7 @@ class Emu3TextConfig(PreTrainedConfig):\n     model_type = \"emu3_text_model\"\n     base_config_key = \"text_config\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    default_theta = 1000000.0\n \n     def __init__(\n         self,\n@@ -226,14 +227,7 @@ def __init__(\n         self.attention_bias = attention_bias\n         self.initializer_range = initializer_range\n         self.attention_dropout = attention_dropout\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 1000000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "13a8d4d94fe769c13ac115d78b2a87ec2f87751b",
            "filename": "src/transformers/models/ernie4_5/configuration_ernie4_5.py",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fconfiguration_ernie4_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fconfiguration_ernie4_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fconfiguration_ernie4_5.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -16,7 +16,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class Ernie4_5Config(PreTrainedConfig):\n@@ -92,6 +92,7 @@ class Ernie4_5Config(PreTrainedConfig):\n \n     model_type = \"ernie4_5\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    default_theta = 500000.0\n     # Default tensor parallel plan for base model `Ernie4_5Model`\n     base_model_tp_plan = {\n         \"layers.*.self_attn.q_proj\": \"colwise\",\n@@ -148,14 +149,7 @@ def __init__(\n         self.use_cache = use_cache\n         self.use_bias = use_bias\n         self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 500000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "bf3b8403d782eda982d10ee7d19e2c731ffb06d4",
            "filename": "src/transformers/models/ernie4_5_moe/configuration_ernie4_5_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fconfiguration_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fconfiguration_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fconfiguration_ernie4_5_moe.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -16,7 +16,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -115,6 +115,7 @@ class Ernie4_5_MoeConfig(PreTrainedConfig):\n     model_type = \"ernie4_5_moe\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n     attribute_map = {\"num_experts\": \"moe_num_experts\", \"num_experts_per_tok\": \"moe_k\"}\n+    default_theta = 500000.0\n \n     # Default tensor parallel plan for base model `Ernie4_5_MoE`\n     base_model_tp_plan = {\n@@ -181,14 +182,6 @@ def __init__(\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n         self.use_bias = use_bias\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 500000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n \n         # MoE arguments\n         self.moe_intermediate_size = moe_intermediate_size\n@@ -201,6 +194,7 @@ def __init__(\n         self.moe_norm_min = moe_norm_min\n         self.output_router_logits = output_router_logits\n         self.router_aux_loss_coef = router_aux_loss_coef\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "91981b3aaeb063b89e364c6fe810590121226e30",
            "filename": "src/transformers/models/evolla/configuration_evolla.py",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fevolla%2Fconfiguration_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fevolla%2Fconfiguration_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fconfiguration_evolla.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -17,7 +17,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -190,6 +190,7 @@ class EvollaConfig(PreTrainedConfig):\n \n     model_type = \"EvollaModel\"\n     sub_configs = {\"protein_encoder_config\": SaProtConfig}\n+    default_theta = 500000.0\n \n     def __init__(\n         self,\n@@ -249,14 +250,7 @@ def __init__(\n         self.resampler_heads = resampler_heads\n         self.resampler_num_latents = resampler_num_latents\n         self.resampler_ff_mult = resampler_ff_mult\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 500000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         # Subconfig\n         if protein_encoder_config is None:"
        },
        {
            "sha": "fbe7de4542822330ceb5f7bb57b99d6578d86d76",
            "filename": "src/transformers/models/exaone4/configuration_exaone4.py",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fexaone4%2Fconfiguration_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fexaone4%2Fconfiguration_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fconfiguration_exaone4.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -22,7 +22,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class Exaone4Config(PreTrainedConfig):\n@@ -164,9 +164,6 @@ def __init__(\n         self.attention_dropout = attention_dropout\n         self.sliding_window = sliding_window\n         self.sliding_window_pattern = sliding_window_pattern\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n \n         self.layer_types = layer_types\n         if self.sliding_window is None:\n@@ -182,10 +179,7 @@ def __init__(\n             self.cache_implementation = \"hybrid\"\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             bos_token_id=bos_token_id, eos_token_id=eos_token_id, tie_word_embeddings=tie_word_embeddings, **kwargs"
        },
        {
            "sha": "8c054f693f7a33a69b6103dc483e79d8f73d768c",
            "filename": "src/transformers/models/exaone4/modular_exaone4.py",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -30,7 +30,7 @@\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n )\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -197,9 +197,6 @@ def __init__(\n         self.attention_dropout = attention_dropout\n         self.sliding_window = sliding_window\n         self.sliding_window_pattern = sliding_window_pattern\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n \n         self.layer_types = layer_types\n         if self.sliding_window is None:\n@@ -215,10 +212,7 @@ def __init__(\n             self.cache_implementation = \"hybrid\"\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             bos_token_id=bos_token_id, eos_token_id=eos_token_id, tie_word_embeddings=tie_word_embeddings, **kwargs"
        },
        {
            "sha": "d8d0266c3bee150e9d470e1cbaaa261ab6680ee3",
            "filename": "src/transformers/models/falcon/configuration_falcon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Ffalcon%2Fconfiguration_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Ffalcon%2Fconfiguration_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fconfiguration_falcon.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -17,7 +17,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -162,14 +162,7 @@ def __init__(\n         else:\n             self.ffn_hidden_size = ffn_hidden_size\n \n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(bos_token_id=bos_token_id, eos_token_id=eos_token_id, **kwargs)\n "
        },
        {
            "sha": "23d5ca53d4ccd03b9bb6498c84e2d7775a725084",
            "filename": "src/transformers/models/falcon_h1/configuration_falcon_h1.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fconfiguration_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fconfiguration_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fconfiguration_falcon_h1.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -17,7 +17,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -197,15 +197,6 @@ def __init__(\n \n         self.use_cache = use_cache\n         self.num_logits_to_keep = num_logits_to_keep\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n-\n         self.projectors_bias = projectors_bias\n         mamba_intermediate = mamba_expand * hidden_size if mamba_d_ssm is None else mamba_d_ssm\n \n@@ -271,6 +262,8 @@ def __init__(\n         else:\n             self.ssm_out_multiplier = 1.0\n \n+        self.rope_parameters = rope_parameters\n+\n         super().__init__(\n             pad_token_id=pad_token_id,\n             bos_token_id=bos_token_id,"
        },
        {
            "sha": "a61148f95ee711b71211e5495ddd5c10b74005fd",
            "filename": "src/transformers/models/flex_olmo/configuration_flex_olmo.py",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fconfiguration_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fconfiguration_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fconfiguration_flex_olmo.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -22,7 +22,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class FlexOlmoConfig(PreTrainedConfig):\n@@ -110,6 +110,7 @@ class FlexOlmoConfig(PreTrainedConfig):\n     model_type = \"flex_olmo\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n     attribute_map = {\"num_local_experts\": \"num_experts\"}\n+    default_theta = 500000.0\n     base_model_tp_plan = {\n         \"layers.*.self_attn.q_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n         \"layers.*.self_attn.k_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n@@ -175,14 +176,7 @@ def __init__(\n         self.output_router_logits = output_router_logits\n         self.router_aux_loss_coef = router_aux_loss_coef\n         self.norm_topk_prob = norm_topk_prob\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 500000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "e3b24c5d02fe7053909772354c0cfbf3824ea48f",
            "filename": "src/transformers/models/flex_olmo/modular_flex_olmo.py",
            "status": "modified",
            "additions": 30,
            "deletions": 29,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodular_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodular_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodular_flex_olmo.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -19,15 +19,15 @@\n from torch import nn\n \n from ...cache_utils import Cache, DynamicCache\n+from ...configuration_utils import PreTrainedConfig\n from ...masking_utils import create_causal_mask\n from ...modeling_outputs import MoeModelOutputWithPast\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring\n from ...utils.generic import OutputRecorder, check_model_inputs\n from ..mixtral.modeling_mixtral import MixtralModel, MixtralPreTrainedModel\n from ..olmo2.modeling_olmo2 import Olmo2Attention, Olmo2RMSNorm, Olmo2RotaryEmbedding\n-from ..olmoe.configuration_olmoe import OlmoeConfig\n from ..olmoe.modeling_olmoe import (\n     OlmoeDecoderLayer,\n     OlmoeForCausalLM,\n@@ -36,7 +36,7 @@\n )\n \n \n-class FlexOlmoConfig(OlmoeConfig):\n+class FlexOlmoConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`FlexOlmoModel`]. It is used to instantiate an FlexOlmo\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n@@ -120,6 +120,8 @@ class FlexOlmoConfig(OlmoeConfig):\n \n     model_type = \"flex_olmo\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    attribute_map = {\"num_local_experts\": \"num_experts\"}\n+    default_theta = 500000.0\n     base_model_tp_plan = {\n         \"layers.*.self_attn.q_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n         \"layers.*.self_attn.k_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n@@ -162,40 +164,39 @@ def __init__(\n         norm_topk_prob: Optional[bool] = False,\n         **kwargs,\n     ):\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+\n+        # for backward compatibility\n+        if num_key_value_heads is None:\n+            num_key_value_heads = num_attention_heads\n+\n+        self.num_key_value_heads = num_key_value_heads\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+        self.num_experts_per_tok = num_experts_per_tok\n+        self.num_experts = num_experts\n+        self.output_router_logits = output_router_logits\n+        self.router_aux_loss_coef = router_aux_loss_coef\n+        self.norm_topk_prob = norm_topk_prob\n+        self.rope_parameters = rope_parameters\n+\n         super().__init__(\n-            vocab_size=vocab_size,\n-            max_position_embeddings=max_position_embeddings,\n-            hidden_size=hidden_size,\n-            intermediate_size=intermediate_size,\n-            num_hidden_layers=num_hidden_layers,\n-            num_attention_heads=num_attention_heads,\n-            num_key_value_heads=num_key_value_heads,\n-            hidden_act=hidden_act,\n-            initializer_range=initializer_range,\n-            rms_norm_eps=rms_norm_eps,\n-            use_cache=use_cache,\n-            rope_parameters=rope_parameters,\n-            attention_bias=attention_bias,\n-            attention_dropout=attention_dropout,\n-            num_experts_per_tok=num_experts_per_tok,\n-            num_experts=num_experts,\n-            output_router_logits=output_router_logits,\n-            router_aux_loss_coef=router_aux_loss_coef,\n-            norm_topk_prob=norm_topk_prob,\n             pad_token_id=pad_token_id,\n             bos_token_id=bos_token_id,\n             eos_token_id=eos_token_id,\n             tie_word_embeddings=tie_word_embeddings,\n             **kwargs,\n         )\n \n-        del self.clip_qkv\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 500000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n-\n \n # FlexOlmo RMS norm reuses Olmo2 RMS norm, which handles low precision slightly differently than the original Olmoe.\n class FlexOlmoRMSNorm(Olmo2RMSNorm):"
        },
        {
            "sha": "dbe828e01fbea01942c18645b29da0047f3bbb32",
            "filename": "src/transformers/models/fuyu/configuration_fuyu.py",
            "status": "modified",
            "additions": 4,
            "deletions": 15,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Ffuyu%2Fconfiguration_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Ffuyu%2Fconfiguration_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fconfiguration_fuyu.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -17,7 +17,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n from ..auto import CONFIG_MAPPING, AutoConfig\n \n@@ -77,9 +77,6 @@ class FuyuConfig(PreTrainedConfig):\n             The dropout ratio after applying the MLP to the hidden states.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio after computing the attention scores.\n-        partial_rotary_factor (`float`, *optional*, defaults to 0.5):\n-            Percentage of the query and keys which will have rotary embedding.\n-\n         pad_token_id (`int`, *optional*):\n             The id of the *padding* token.\n         bos_token_id (`int`, *optional*, defaults to 1):\n@@ -101,6 +98,7 @@ class FuyuConfig(PreTrainedConfig):\n     model_type = \"fuyu\"\n     sub_configs = {\"text_config\": AutoConfig}\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    default_theta = 25000.0\n \n     def __init__(\n         self,\n@@ -122,7 +120,6 @@ def __init__(\n         qk_layernorm: Optional[bool] = True,\n         hidden_dropout: Optional[float] = 0.0,\n         attention_dropout: Optional[float] = 0.0,\n-        partial_rotary_factor: Optional[float] = 0.5,\n         pad_token_id: Optional[int] = None,\n         bos_token_id: Optional[int] = 1,\n         eos_token_id: Optional[int] = 2,\n@@ -146,7 +143,6 @@ def __init__(\n                 \"qk_layernorm\": qk_layernorm,\n                 \"hidden_dropout\": hidden_dropout,\n                 \"attention_dropout\": attention_dropout,\n-                \"partial_rotary_factor\": partial_rotary_factor,\n                 \"pad_token_id\": pad_token_id,\n                 \"bos_token_id\": bos_token_id,\n                 \"eos_token_id\": eos_token_id,\n@@ -172,16 +168,9 @@ def __init__(\n         self.qk_layernorm = qk_layernorm\n         self.hidden_dropout = hidden_dropout\n         self.attention_dropout = attention_dropout\n-        self.partial_rotary_factor = partial_rotary_factor\n         self.image_token_id = image_token_id\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 25000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n+        kwargs.setdefault(\"partial_rotary_factor\", 0.5)  # assign default for BC\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "df0a94c014f1f209ecbab3bc7e6c36a1d4f7f4b9",
            "filename": "src/transformers/models/gemma/configuration_gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -22,7 +22,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class GemmaConfig(PreTrainedConfig):\n@@ -152,14 +152,7 @@ def __init__(\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.use_bidirectional_attention = use_bidirectional_attention\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "e305ba56a56196abfe8a9763f21abc73db6a86e3",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -23,7 +23,7 @@\n from ...configuration_utils import PreTrainedConfig\n from ...masking_utils import create_causal_mask\n from ...modeling_outputs import BaseModelOutputWithPast\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n@@ -177,14 +177,7 @@ def __init__(\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.use_bidirectional_attention = use_bidirectional_attention\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "a26cffab7bcaced45ce8e570bf1878474da7d18c",
            "filename": "src/transformers/models/gemma2/configuration_gemma2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 15,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -22,7 +22,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class Gemma2Config(PreTrainedConfig):\n@@ -153,13 +153,6 @@ def __init__(\n         use_bidirectional_attention: Optional[bool] = None,\n         **kwargs,\n     ):\n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -180,20 +173,22 @@ def __init__(\n         self.attn_logit_softcapping = attn_logit_softcapping\n         self.layer_types = layer_types\n         self.use_bidirectional_attention = use_bidirectional_attention\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n \n         if self.layer_types is None:\n             self.layer_types = [\n                 \"sliding_attention\" if bool((i + 1) % 2) else \"full_attention\" for i in range(self.num_hidden_layers)\n             ]\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n+\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n \n \n __all__ = [\"Gemma2Config\"]"
        },
        {
            "sha": "b991f2d2fc65749bebc081f5ffd2df0e25da3f46",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 16,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -30,8 +30,6 @@\n     ROPE_INIT_FUNCTIONS,\n     RopeParameters,\n     dynamic_rope_update,\n-    rope_config_validation,\n-    standardize_rope_params,\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n@@ -182,13 +180,6 @@ def __init__(\n         use_bidirectional_attention: Optional[bool] = None,\n         **kwargs,\n     ):\n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -209,20 +200,22 @@ def __init__(\n         self.attn_logit_softcapping = attn_logit_softcapping\n         self.layer_types = layer_types\n         self.use_bidirectional_attention = use_bidirectional_attention\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n \n         if self.layer_types is None:\n             self.layer_types = [\n                 \"sliding_attention\" if bool((i + 1) % 2) else \"full_attention\" for i in range(self.num_hidden_layers)\n             ]\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n+\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n \n \n class Gemma2RMSNorm(GemmaRMSNorm):"
        },
        {
            "sha": "884020640c9b66309aae81a6027228c2a61ae315",
            "filename": "src/transformers/models/gemma3/configuration_gemma3.py",
            "status": "modified",
            "additions": 33,
            "deletions": 24,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -22,7 +22,7 @@\n from typing import Any, Optional, Union\n \n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n from ..siglip import SiglipVisionConfig\n \n@@ -130,6 +130,7 @@ class Gemma3TextConfig(PreTrainedConfig):\n         \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n         \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n     }\n+    default_theta = {\"global\": 1_000_000.0, \"local\": 10_000.0}\n \n     def __init__(\n         self,\n@@ -160,13 +161,6 @@ def __init__(\n         use_bidirectional_attention: Optional[bool] = False,\n         **kwargs,\n     ):\n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -187,16 +181,6 @@ def __init__(\n         self.attn_logit_softcapping = attn_logit_softcapping\n         self.layer_types = layer_types\n \n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        if (rope_scaling := kwargs.pop(\"rope_scaling\", None)) is not None:\n-            if rope_parameters is None:\n-                rope_parameters = {\"sliding_attention\": {\"rope_type\": \"default\"}, \"full_attention\": rope_scaling}\n-            elif \"full_attention\" in rope_parameters:\n-                rope_parameters[\"full_attention\"].update(rope_scaling)\n-            else:\n-                rope_parameters.update(rope_scaling)\n-\n-        self.rope_parameters = rope_parameters\n         self.use_bidirectional_attention = use_bidirectional_attention\n         if use_bidirectional_attention:\n             self.sliding_window = (self.sliding_window // 2) + 1  # due to fa we set exclusive bounds\n@@ -211,13 +195,38 @@ def __init__(\n             ]\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = getattr(self, \"rope_theta\", 1_000_000.0)\n-        rope_local_base_freq = getattr(self, \"rope_local_base_freq\", 10000.0)\n-        standardize_rope_params(\n-            self, rope_theta={\"full_attention\": rope_theta, \"sliding_attention\": rope_local_base_freq}\n+        self.rope_parameters = rope_parameters\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+\n+    def convert_rope_params_to_dict(self, ignore_keys_at_rope_validation=None, **kwargs):\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`. If we find `rope_parameters`\n+        # as arg in the inputs, we can safely assume that it is in the new format. New naming used -> new format\n+        default_rope_params = {\n+            \"sliding_attention\": {\"rope_type\": \"default\"},\n+            \"full_attention\": {\"rope_type\": \"default\"},\n+        }\n+        self.rope_parameters = self.rope_parameters if self.rope_parameters is not None else default_rope_params\n+        if rope_scaling is not None:\n+            self.rope_parameters[\"full_attention\"].update(rope_scaling)\n+        self.rope_parameters[\"full_attention\"].setdefault(\n+            \"rope_theta\", kwargs.pop(\"rope_theta\", self.default_theta[\"global\"])\n         )\n-        rope_config_validation(self)\n+        self.rope_parameters[\"sliding_attention\"].setdefault(\n+            \"rope_theta\", kwargs.pop(\"rope_local_base_freq\", self.default_theta[\"local\"])\n+        )\n+\n+        # Standardize and validate the correctness of rotary position embeddings parameters\n+        self.standardize_rope_params()\n+        self.validate_rope(ignore_keys=ignore_keys_at_rope_validation)\n+        return kwargs\n \n \n class Gemma3Config(PreTrainedConfig):"
        },
        {
            "sha": "2d489d77f5c7797cadf1926150a97edb85751fca",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 32,
            "deletions": 25,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -30,8 +30,6 @@\n     ROPE_INIT_FUNCTIONS,\n     RopeParameters,\n     dynamic_rope_update,\n-    rope_config_validation,\n-    standardize_rope_params,\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n@@ -146,6 +144,7 @@ class Gemma3TextConfig(Gemma2Config, PreTrainedConfig):\n     \"\"\"\n \n     model_type = \"gemma3_text\"\n+    default_theta = {\"global\": 1_000_000.0, \"local\": 10_000.0}\n \n     def __init__(\n         self,\n@@ -176,13 +175,6 @@ def __init__(\n         use_bidirectional_attention: Optional[bool] = False,\n         **kwargs,\n     ):\n-        PreTrainedConfig.__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -203,16 +195,6 @@ def __init__(\n         self.attn_logit_softcapping = attn_logit_softcapping\n         self.layer_types = layer_types\n \n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        if (rope_scaling := kwargs.pop(\"rope_scaling\", None)) is not None:\n-            if rope_parameters is None:\n-                rope_parameters = {\"sliding_attention\": {\"rope_type\": \"default\"}, \"full_attention\": rope_scaling}\n-            elif \"full_attention\" in rope_parameters:\n-                rope_parameters[\"full_attention\"].update(rope_scaling)\n-            else:\n-                rope_parameters.update(rope_scaling)\n-\n-        self.rope_parameters = rope_parameters\n         self.use_bidirectional_attention = use_bidirectional_attention\n         if use_bidirectional_attention:\n             self.sliding_window = (self.sliding_window // 2) + 1  # due to fa we set exclusive bounds\n@@ -227,13 +209,38 @@ def __init__(\n             ]\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = getattr(self, \"rope_theta\", 1_000_000.0)\n-        rope_local_base_freq = getattr(self, \"rope_local_base_freq\", 10000.0)\n-        standardize_rope_params(\n-            self, rope_theta={\"full_attention\": rope_theta, \"sliding_attention\": rope_local_base_freq}\n+        self.rope_parameters = rope_parameters\n+        PreTrainedConfig.__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+\n+    def convert_rope_params_to_dict(self, ignore_keys_at_rope_validation=None, **kwargs):\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`. If we find `rope_parameters`\n+        # as arg in the inputs, we can safely assume that it is in the new format. New naming used -> new format\n+        default_rope_params = {\n+            \"sliding_attention\": {\"rope_type\": \"default\"},\n+            \"full_attention\": {\"rope_type\": \"default\"},\n+        }\n+        self.rope_parameters = self.rope_parameters if self.rope_parameters is not None else default_rope_params\n+        if rope_scaling is not None:\n+            self.rope_parameters[\"full_attention\"].update(rope_scaling)\n+        self.rope_parameters[\"full_attention\"].setdefault(\n+            \"rope_theta\", kwargs.pop(\"rope_theta\", self.default_theta[\"global\"])\n         )\n-        rope_config_validation(self)\n+        self.rope_parameters[\"sliding_attention\"].setdefault(\n+            \"rope_theta\", kwargs.pop(\"rope_local_base_freq\", self.default_theta[\"local\"])\n+        )\n+\n+        # Standardize and validate the correctness of rotary position embeddings parameters\n+        self.standardize_rope_params()\n+        self.validate_rope(ignore_keys=ignore_keys_at_rope_validation)\n+        return kwargs\n \n \n class Gemma3Config(PreTrainedConfig):"
        },
        {
            "sha": "b7415b21877d5fa9f12e806a4ffc9db4125715b8",
            "filename": "src/transformers/models/gemma3n/configuration_gemma3n.py",
            "status": "modified",
            "additions": 33,
            "deletions": 19,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -23,7 +23,7 @@\n from typing import Any, Optional, Union\n \n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import is_timm_available, logging, requires_backends\n \n \n@@ -157,6 +157,7 @@ class Gemma3nTextConfig(PreTrainedConfig):\n         \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n         \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n     }\n+    default_theta = {\"global\": 1_000_000.0, \"local\": 10_000.0}\n \n     def __init__(\n         self,\n@@ -192,13 +193,6 @@ def __init__(\n         activation_sparsity_pattern: Optional[Union[float, Sequence[float]]] = None,\n         **kwargs,\n     ):\n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            **kwargs,\n-        )\n-\n         if isinstance(intermediate_size, Sequence) and (intsize_len := len(intermediate_size)) != num_hidden_layers:\n             raise ValueError(\n                 \"intermediate_size must have an explicit intermediate size for every layer or one for all layers. \"\n@@ -225,9 +219,6 @@ def __init__(\n         self.sliding_window = sliding_window\n         self.final_logit_softcapping = final_logit_softcapping\n         self.layer_types = layer_types\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n \n         if layer_types is None:\n             self.layer_types = [\n@@ -238,14 +229,6 @@ def __init__(\n \n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 1000000.0)\n-        rope_local_base_freq = kwargs.get(\"rope_local_base_freq\", 100000.0)\n-        standardize_rope_params(\n-            self, rope_theta={\"full_attention\": rope_theta, \"sliding_attention\": rope_local_base_freq}\n-        )\n-        rope_config_validation(self)\n-\n         self.hidden_size_per_layer_input = hidden_size_per_layer_input\n         self.num_kv_shared_layers = num_kv_shared_layers\n \n@@ -266,6 +249,37 @@ def __init__(\n                 f\"Expected {num_hidden_layers} values but got {len_asp}.\"\n             )\n         self.activation_sparsity_pattern = activation_sparsity_pattern\n+        self.rope_parameters = rope_parameters\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            **kwargs,\n+        )\n+\n+    def convert_rope_params_to_dict(self, ignore_keys_at_rope_validation=None, **kwargs):\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`. If we find `rope_parameters`\n+        # as arg in the inputs, we can safely assume that it is in the new format. New naming used -> new format\n+        default_rope_params = {\n+            \"sliding_attention\": {\"rope_type\": \"default\"},\n+            \"full_attention\": {\"rope_type\": \"default\"},\n+        }\n+        self.rope_parameters = self.rope_parameters if self.rope_parameters is not None else default_rope_params\n+        if rope_scaling is not None:\n+            self.rope_parameters[\"full_attention\"].update(rope_scaling)\n+        self.rope_parameters[\"full_attention\"].setdefault(\n+            \"rope_theta\", kwargs.pop(\"rope_theta\", self.default_theta[\"global\"])\n+        )\n+        self.rope_parameters[\"sliding_attention\"].setdefault(\n+            \"rope_theta\", kwargs.pop(\"rope_local_base_freq\", self.default_theta[\"local\"])\n+        )\n+\n+        # Standardize and validate the correctness of rotary position embeddings parameters\n+        self.standardize_rope_params()\n+        self.validate_rope(ignore_keys=ignore_keys_at_rope_validation)\n+        return kwargs\n \n \n class Gemma3nAudioConfig(PreTrainedConfig):"
        },
        {
            "sha": "a97ef8bceeeb1b2fc4bcc421c9cb8a5a057901d1",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 33,
            "deletions": 19,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -28,7 +28,7 @@\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n@@ -168,6 +168,7 @@ class Gemma3nTextConfig(Gemma2Config, PreTrainedConfig):\n     \"\"\"\n \n     model_type = \"gemma3n_text\"\n+    default_theta = {\"global\": 1_000_000.0, \"local\": 10_000.0}\n \n     def __init__(\n         self,\n@@ -203,13 +204,6 @@ def __init__(\n         activation_sparsity_pattern: Optional[Union[float, Sequence[float]]] = None,\n         **kwargs,\n     ):\n-        PreTrainedConfig.__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            **kwargs,\n-        )\n-\n         if isinstance(intermediate_size, Sequence) and (intsize_len := len(intermediate_size)) != num_hidden_layers:\n             raise ValueError(\n                 \"intermediate_size must have an explicit intermediate size for every layer or one for all layers. \"\n@@ -236,9 +230,6 @@ def __init__(\n         self.sliding_window = sliding_window\n         self.final_logit_softcapping = final_logit_softcapping\n         self.layer_types = layer_types\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n \n         if layer_types is None:\n             self.layer_types = [\n@@ -249,14 +240,6 @@ def __init__(\n \n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 1000000.0)\n-        rope_local_base_freq = kwargs.get(\"rope_local_base_freq\", 100000.0)\n-        standardize_rope_params(\n-            self, rope_theta={\"full_attention\": rope_theta, \"sliding_attention\": rope_local_base_freq}\n-        )\n-        rope_config_validation(self)\n-\n         self.hidden_size_per_layer_input = hidden_size_per_layer_input\n         self.num_kv_shared_layers = num_kv_shared_layers\n \n@@ -277,6 +260,37 @@ def __init__(\n                 f\"Expected {num_hidden_layers} values but got {len_asp}.\"\n             )\n         self.activation_sparsity_pattern = activation_sparsity_pattern\n+        self.rope_parameters = rope_parameters\n+        PreTrainedConfig.__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            **kwargs,\n+        )\n+\n+    def convert_rope_params_to_dict(self, ignore_keys_at_rope_validation=None, **kwargs):\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`. If we find `rope_parameters`\n+        # as arg in the inputs, we can safely assume that it is in the new format. New naming used -> new format\n+        default_rope_params = {\n+            \"sliding_attention\": {\"rope_type\": \"default\"},\n+            \"full_attention\": {\"rope_type\": \"default\"},\n+        }\n+        self.rope_parameters = self.rope_parameters if self.rope_parameters is not None else default_rope_params\n+        if rope_scaling is not None:\n+            self.rope_parameters[\"full_attention\"].update(rope_scaling)\n+        self.rope_parameters[\"full_attention\"].setdefault(\n+            \"rope_theta\", kwargs.pop(\"rope_theta\", self.default_theta[\"global\"])\n+        )\n+        self.rope_parameters[\"sliding_attention\"].setdefault(\n+            \"rope_theta\", kwargs.pop(\"rope_local_base_freq\", self.default_theta[\"local\"])\n+        )\n+\n+        # Standardize and validate the correctness of rotary position embeddings parameters\n+        self.standardize_rope_params()\n+        self.validate_rope(ignore_keys=ignore_keys_at_rope_validation)\n+        return kwargs\n \n \n class Gemma3nAudioConfig(PreTrainedConfig):"
        },
        {
            "sha": "59328a6a4f43fe4c9f1b364cbdafd3589540ca7a",
            "filename": "src/transformers/models/glm/configuration_glm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 12,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fglm%2Fconfiguration_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fglm%2Fconfiguration_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fconfiguration_glm.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -17,7 +17,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class GlmConfig(PreTrainedConfig):\n@@ -48,7 +48,6 @@ class GlmConfig(PreTrainedConfig):\n             by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to\n             `num_attention_heads`.\n-        partial_rotary_factor (`float`, *optional*, defaults to 0.5): The factor of the partial rotary position.\n         head_dim (`int`, *optional*, defaults to 128):\n             The attention head dimension.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n@@ -112,7 +111,6 @@ def __init__(\n         num_hidden_layers: Optional[int] = 40,\n         num_attention_heads: Optional[int] = 32,\n         num_key_value_heads: Optional[int] = 2,\n-        partial_rotary_factor: Optional[float] = 0.5,\n         head_dim: Optional[int] = 128,\n         hidden_act: Optional[str] = \"silu\",\n         attention_dropout: Optional[float] = 0.0,\n@@ -134,7 +132,6 @@ def __init__(\n         self.intermediate_size = intermediate_size\n         self.num_hidden_layers = num_hidden_layers\n         self.num_attention_heads = num_attention_heads\n-        self.partial_rotary_factor = partial_rotary_factor\n         self.head_dim = head_dim\n         self.num_key_value_heads = num_key_value_heads\n         self.hidden_act = hidden_act\n@@ -143,14 +140,8 @@ def __init__(\n         self.use_cache = use_cache\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n+        kwargs.setdefault(\"partial_rotary_factor\", 0.5)  # assign default for BC\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "8a3ce70a8a5e8f8c04bf4ecdf6f335a89f7972df",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -101,7 +101,7 @@ def compute_default_rope_parameters(\n             post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n         \"\"\"\n         base = config.rope_parameters[\"rope_theta\"]\n-        partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+        partial_rotary_factor = config.rope_parameters.get(\"partial_rotary_factor\", 1.0)\n         head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n         dim = int(head_dim * partial_rotary_factor)\n "
        },
        {
            "sha": "97b47a9b8f7f39ec79d736b964f84b4aa60782cd",
            "filename": "src/transformers/models/glm/modular_glm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fglm%2Fmodular_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fglm%2Fmodular_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodular_glm.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -60,7 +60,7 @@ def compute_default_rope_parameters(\n             post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n         \"\"\"\n         base = config.rope_parameters[\"rope_theta\"]\n-        partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+        partial_rotary_factor = config.rope_parameters.get(\"partial_rotary_factor\", 1.0)\n         head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n         dim = int(head_dim * partial_rotary_factor)\n "
        },
        {
            "sha": "a6d0d2bc3d9ca1c509050ab782c0dc7a0d33fe14",
            "filename": "src/transformers/models/glm4/configuration_glm4.py",
            "status": "modified",
            "additions": 3,
            "deletions": 13,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fglm4%2Fconfiguration_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fglm4%2Fconfiguration_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fconfiguration_glm4.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -17,7 +17,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class Glm4Config(PreTrainedConfig):\n@@ -48,8 +48,6 @@ class Glm4Config(PreTrainedConfig):\n             by meanpooling all the original heads within that group. For more details, check out [this\n             paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to\n             `num_attention_heads`.\n-        partial_rotary_factor (`float`, *optional*, defaults to 0.5):\n-            The factor of the partial rotary position.\n         head_dim (`int`, *optional*, defaults to 128):\n             The attention head dimension.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n@@ -113,7 +111,6 @@ def __init__(\n         num_hidden_layers: Optional[int] = 40,\n         num_attention_heads: Optional[int] = 32,\n         num_key_value_heads: Optional[int] = 2,\n-        partial_rotary_factor: Optional[float] = 0.5,\n         head_dim: Optional[int] = 128,\n         hidden_act: Optional[str] = \"silu\",\n         attention_dropout: Optional[float] = 0.0,\n@@ -135,7 +132,6 @@ def __init__(\n         self.intermediate_size = intermediate_size\n         self.num_hidden_layers = num_hidden_layers\n         self.num_attention_heads = num_attention_heads\n-        self.partial_rotary_factor = partial_rotary_factor\n         self.head_dim = head_dim\n         self.num_key_value_heads = num_key_value_heads\n         self.hidden_act = hidden_act\n@@ -144,14 +140,8 @@ def __init__(\n         self.use_cache = use_cache\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n+        kwargs.setdefault(\"partial_rotary_factor\", 0.5)  # assign default for BC\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "625e5d4f2bf934679aa8f1eb2c7b734f7e2027fa",
            "filename": "src/transformers/models/glm4/modeling_glm4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -305,7 +305,7 @@ def compute_default_rope_parameters(\n             post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n         \"\"\"\n         base = config.rope_parameters[\"rope_theta\"]\n-        partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+        partial_rotary_factor = config.rope_parameters.get(\"partial_rotary_factor\", 1.0)\n         head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n         dim = int(head_dim * partial_rotary_factor)\n "
        },
        {
            "sha": "573b7fc10f536680cad6fd90ec41ccfea710da9b",
            "filename": "src/transformers/models/glm4_moe/configuration_glm4_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 13,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fconfiguration_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fconfiguration_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fconfiguration_glm4_moe.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -22,7 +22,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class Glm4MoeConfig(PreTrainedConfig):\n@@ -47,8 +47,6 @@ class Glm4MoeConfig(PreTrainedConfig):\n             Number of hidden layers in the Transformer encoder.\n         num_attention_heads (`int`, *optional*, defaults to 96):\n             Number of attention heads for each attention layer in the Transformer encoder.\n-        partial_rotary_factor (`float`, *optional*, defaults to 0.5):\n-            The factor of the partial rotary position.\n         num_key_value_heads (`int`, *optional*, defaults to 8):\n             This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n@@ -144,7 +142,6 @@ def __init__(\n         intermediate_size: Optional[int] = 10944,\n         num_hidden_layers: Optional[int] = 46,\n         num_attention_heads: Optional[int] = 96,\n-        partial_rotary_factor: Optional[float] = 0.5,\n         num_key_value_heads: Optional[int] = 8,\n         hidden_act: Optional[str] = \"silu\",\n         max_position_embeddings: Optional[int] = 131072,\n@@ -173,7 +170,6 @@ def __init__(\n         self.intermediate_size = intermediate_size\n         self.num_hidden_layers = num_hidden_layers\n         self.num_attention_heads = num_attention_heads\n-        self.partial_rotary_factor = partial_rotary_factor\n \n         self.num_key_value_heads = num_key_value_heads\n         self.hidden_act = hidden_act\n@@ -182,14 +178,8 @@ def __init__(\n         self.use_cache = use_cache\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n+        kwargs.setdefault(\"partial_rotary_factor\", 0.5)  # assign default for BC\n \n         # MoE arguments\n         self.moe_intermediate_size = moe_intermediate_size"
        },
        {
            "sha": "e987e3e9e424961e3cbbac0d5b7c24a905a1b054",
            "filename": "src/transformers/models/glm4_moe/modeling_glm4_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -82,7 +82,7 @@ def compute_default_rope_parameters(\n             post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n         \"\"\"\n         base = config.rope_parameters[\"rope_theta\"]\n-        partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+        partial_rotary_factor = config.rope_parameters.get(\"partial_rotary_factor\", 1.0)\n         head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n         dim = int(head_dim * partial_rotary_factor)\n "
        },
        {
            "sha": "b4a8dc4dc82da74cfed2ed364d436ccd94d2a111",
            "filename": "src/transformers/models/glm4_moe/modular_glm4_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 13,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodular_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodular_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodular_glm4_moe.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -20,7 +20,7 @@\n from torch import nn\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n from ..cohere.modeling_cohere import CohereAttention\n from ..deepseek_v3.modeling_deepseek_v3 import (\n@@ -61,8 +61,6 @@ class Glm4MoeConfig(PreTrainedConfig):\n             Number of hidden layers in the Transformer encoder.\n         num_attention_heads (`int`, *optional*, defaults to 96):\n             Number of attention heads for each attention layer in the Transformer encoder.\n-        partial_rotary_factor (`float`, *optional*, defaults to 0.5):\n-            The factor of the partial rotary position.\n         num_key_value_heads (`int`, *optional*, defaults to 8):\n             This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n@@ -158,7 +156,6 @@ def __init__(\n         intermediate_size: Optional[int] = 10944,\n         num_hidden_layers: Optional[int] = 46,\n         num_attention_heads: Optional[int] = 96,\n-        partial_rotary_factor: Optional[float] = 0.5,\n         num_key_value_heads: Optional[int] = 8,\n         hidden_act: Optional[str] = \"silu\",\n         max_position_embeddings: Optional[int] = 131072,\n@@ -187,7 +184,6 @@ def __init__(\n         self.intermediate_size = intermediate_size\n         self.num_hidden_layers = num_hidden_layers\n         self.num_attention_heads = num_attention_heads\n-        self.partial_rotary_factor = partial_rotary_factor\n \n         self.num_key_value_heads = num_key_value_heads\n         self.hidden_act = hidden_act\n@@ -196,14 +192,8 @@ def __init__(\n         self.use_cache = use_cache\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n+        kwargs.setdefault(\"partial_rotary_factor\", 0.5)  # assign default for BC\n \n         # MoE arguments\n         self.moe_intermediate_size = moe_intermediate_size"
        },
        {
            "sha": "35c29f07246d9ae6e44a7d0ae407d76e1b8aec11",
            "filename": "src/transformers/models/glm4v/configuration_glm4v.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconfiguration_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconfiguration_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconfiguration_glm4v.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -21,7 +21,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class Glm4vVisionConfig(PreTrainedConfig):\n@@ -232,16 +232,9 @@ def __init__(\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n         self.attention_dropout = attention_dropout\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n+        self.rope_parameters = rope_parameters\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self, ignore_keys={\"mrope_section\"})\n-\n-        super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n+        super().__init__(tie_word_embeddings=tie_word_embeddings, ignore_keys_at_rope_validation={\"mrope\"}, **kwargs)\n \n \n class Glm4vConfig(PreTrainedConfig):"
        },
        {
            "sha": "dd9f2fba17d3acf8664b0c66d8d0eef7a5748d71",
            "filename": "src/transformers/models/glm4v/convert_glm4v_mgt_weights_to_hf.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconvert_glm4v_mgt_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconvert_glm4v_mgt_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconvert_glm4v_mgt_weights_to_hf.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -702,9 +702,13 @@ def offset_layer(x, offset=llm_layer_offset):\n         \"dtype\": text_config.get(\"torch_dtype\", \"bfloat16\"),\n         \"use_cache\": text_config.get(\"use_cache\", True),\n         \"vocab_size\": text_config.get(\"vocab_size\", 151552),\n-        \"partial_rotary_factor\": 0.5,\n         \"tie_word_embeddings\": False,\n-        \"rope_parameters\": {\"rope_type\": \"default\", \"rope_theta\": 10000.0, \"mrope_section\": [8, 12, 12]},\n+        \"rope_parameters\": {\n+            \"rope_type\": \"default\",\n+            \"rope_theta\": 10000.0,\n+            \"mrope_section\": [8, 12, 12],\n+            \"partial_rotary_factor\": 0.5,\n+        },\n     }\n     hf_config[\"text_config\"] = txt_config\n "
        },
        {
            "sha": "9f9a58b4c530d20b7e5d7cae2eaac66c076a1066",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -425,7 +425,7 @@ def compute_default_rope_parameters(\n             post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n         \"\"\"\n         base = config.rope_parameters[\"rope_theta\"]\n-        partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+        partial_rotary_factor = config.rope_parameters.get(\"partial_rotary_factor\", 1.0)\n         head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n         dim = int(head_dim * partial_rotary_factor)\n "
        },
        {
            "sha": "aa1ae597aa24c35622f1842fb770831a8f9950c4",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -31,7 +31,7 @@\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n@@ -269,16 +269,9 @@ def __init__(\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n         self.attention_dropout = attention_dropout\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n+        self.rope_parameters = rope_parameters\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self, ignore_keys={\"mrope_section\"})\n-\n-        super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n+        super().__init__(tie_word_embeddings=tie_word_embeddings, ignore_keys_at_rope_validation={\"mrope\"}, **kwargs)\n \n \n class Glm4vConfig(PreTrainedConfig):"
        },
        {
            "sha": "20e4f3ad492c500b0b2240ae7e64219b96a2a121",
            "filename": "src/transformers/models/glm4v_moe/configuration_glm4v_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 13,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -21,7 +21,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class Glm4vMoeVisionConfig(PreTrainedConfig):\n@@ -139,7 +139,6 @@ class Glm4vMoeTextConfig(PreTrainedConfig):\n             Number of hidden layers in the Transformer encoder.\n         num_attention_heads (`int`, *optional*, defaults to 96):\n             Number of attention heads for each attention layer in the Transformer encoder.\n-        partial_rotary_factor (`float`, *optional*, defaults to 0.5): The factor of the partial rotary position.\n         num_key_value_heads (`int`, *optional*, defaults to 8):\n             This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n@@ -231,7 +230,6 @@ def __init__(\n         intermediate_size: Optional[int] = 10944,\n         num_hidden_layers: Optional[int] = 46,\n         num_attention_heads: Optional[int] = 96,\n-        partial_rotary_factor: Optional[float] = 0.5,\n         num_key_value_heads: Optional[int] = 8,\n         hidden_act: Optional[str] = \"silu\",\n         max_position_embeddings: Optional[int] = 65536,\n@@ -254,14 +252,12 @@ def __init__(\n         router_aux_loss_coef: Optional[float] = 0.0001,\n         **kwargs,\n     ):\n-        super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n         self.intermediate_size = intermediate_size\n         self.num_hidden_layers = num_hidden_layers\n         self.num_attention_heads = num_attention_heads\n-        self.partial_rotary_factor = partial_rotary_factor\n \n         self.num_key_value_heads = num_key_value_heads\n         self.hidden_act = hidden_act\n@@ -270,14 +266,8 @@ def __init__(\n         self.use_cache = use_cache\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self, ignore_keys={\"mrope_section\"})\n+        self.rope_parameters = rope_parameters\n+        kwargs.setdefault(\"partial_rotary_factor\", 0.5)  # assign default for BC\n \n         # MoE arguments\n         self.moe_intermediate_size = moe_intermediate_size\n@@ -290,6 +280,7 @@ def __init__(\n         self.first_k_dense_replace = first_k_dense_replace\n         self.norm_topk_prob = norm_topk_prob\n         self.router_aux_loss_coef = router_aux_loss_coef\n+        super().__init__(tie_word_embeddings=tie_word_embeddings, ignore_keys_at_rope_validation={\"mrope\"}, **kwargs)\n \n \n class Glm4vMoeConfig(PreTrainedConfig):"
        },
        {
            "sha": "54a9564b69c516117bd349e57c3f715811f41cd9",
            "filename": "src/transformers/models/glm4v_moe/convert_glm4v_moe_mgt_weights_to_hf.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconvert_glm4v_moe_mgt_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconvert_glm4v_moe_mgt_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconvert_glm4v_moe_mgt_weights_to_hf.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -707,7 +707,12 @@ def offset_layer(x, offset=llm_layer_offset):\n         \"n_shared_experts\": text_config.get(\"n_shared_experts\", 1),\n         \"norm_topk_prob\": text_config.get(\"norm_topk_prob\", True),\n         \"num_experts_per_tok\": text_config.get(\"num_experts_per_tok\", 8),\n-        \"rope_parameters\": {\"rope_type\": \"default\", \"rope_theta\": 10000.0, \"mrope_section\": [8, 12, 12]},\n+        \"rope_parameters\": {\n+            \"rope_type\": \"default\",\n+            \"rope_theta\": 10000.0,\n+            \"mrope_section\": [8, 12, 12],\n+            \"partial_rotary_factor\": 0.5,\n+        },\n     }\n     hf_config[\"text_config\"] = txt_config\n "
        },
        {
            "sha": "4a8f77d38f03bd46fbd7a841dcfede0d2ee3dded",
            "filename": "src/transformers/models/glm4v_moe/modeling_glm4v_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -129,7 +129,7 @@ def compute_default_rope_parameters(\n             post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n         \"\"\"\n         base = config.rope_parameters[\"rope_theta\"]\n-        partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+        partial_rotary_factor = config.rope_parameters.get(\"partial_rotary_factor\", 1.0)\n         head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n         dim = int(head_dim * partial_rotary_factor)\n "
        },
        {
            "sha": "9a10a4e4d9b5f014e9d2c911f60425253b2e1ecc",
            "filename": "src/transformers/models/glm4v_moe/modular_glm4v_moe.py",
            "status": "modified",
            "additions": 8,
            "deletions": 15,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -23,7 +23,7 @@\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import MoeModelOutputWithPast\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters, RotaryEmbeddingConfigMixin\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, logging\n@@ -66,7 +66,7 @@ class Glm4vMoeRMSNorm(Glm4MoeRMSNorm):\n     pass\n \n \n-class Glm4vMoeTextConfig(Glm4MoeConfig):\n+class Glm4vMoeTextConfig(Glm4MoeConfig, RotaryEmbeddingConfigMixin):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Glm4vMoeModel`]. It is used to instantiate a\n     GLM-4.5V model according to the specified arguments, defining the model architecture. Instantiating a\n@@ -88,7 +88,6 @@ class Glm4vMoeTextConfig(Glm4MoeConfig):\n             Number of hidden layers in the Transformer encoder.\n         num_attention_heads (`int`, *optional*, defaults to 96):\n             Number of attention heads for each attention layer in the Transformer encoder.\n-        partial_rotary_factor (`float`, *optional*, defaults to 0.5): The factor of the partial rotary position.\n         num_key_value_heads (`int`, *optional*, defaults to 8):\n             This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n@@ -177,7 +176,6 @@ def __init__(\n         intermediate_size: Optional[int] = 10944,\n         num_hidden_layers: Optional[int] = 46,\n         num_attention_heads: Optional[int] = 96,\n-        partial_rotary_factor: Optional[float] = 0.5,\n         num_key_value_heads: Optional[int] = 8,\n         hidden_act: Optional[str] = \"silu\",\n         max_position_embeddings: Optional[int] = 65536,\n@@ -200,14 +198,12 @@ def __init__(\n         router_aux_loss_coef: Optional[float] = 0.0001,\n         **kwargs,\n     ):\n-        PreTrainedConfig.__init__(self, tie_word_embeddings=tie_word_embeddings, **kwargs)\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n         self.intermediate_size = intermediate_size\n         self.num_hidden_layers = num_hidden_layers\n         self.num_attention_heads = num_attention_heads\n-        self.partial_rotary_factor = partial_rotary_factor\n \n         self.num_key_value_heads = num_key_value_heads\n         self.hidden_act = hidden_act\n@@ -216,14 +212,8 @@ def __init__(\n         self.use_cache = use_cache\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self, ignore_keys={\"mrope_section\"})\n+        self.rope_parameters = rope_parameters\n+        kwargs.setdefault(\"partial_rotary_factor\", 0.5)  # assign default for BC\n \n         # MoE arguments\n         self.moe_intermediate_size = moe_intermediate_size\n@@ -236,6 +226,9 @@ def __init__(\n         self.first_k_dense_replace = first_k_dense_replace\n         self.norm_topk_prob = norm_topk_prob\n         self.router_aux_loss_coef = router_aux_loss_coef\n+        PreTrainedConfig.__init__(\n+            self, tie_word_embeddings=tie_word_embeddings, ignore_keys_at_rope_validation={\"mrope\"}, **kwargs\n+        )\n \n \n class Glm4vMoeConfig(Glm4vConfig):\n@@ -376,7 +369,7 @@ def compute_default_rope_parameters(\n             post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n         \"\"\"\n         base = config.rope_parameters[\"rope_theta\"]\n-        partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+        partial_rotary_factor = config.rope_parameters.get(\"partial_rotary_factor\", 1.0)\n         head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n         dim = int(head_dim * partial_rotary_factor)\n "
        },
        {
            "sha": "8de9ac83a2b350c62281ae3b27ca09d895915a3d",
            "filename": "src/transformers/models/gpt_neox/configuration_gpt_neox.py",
            "status": "modified",
            "additions": 19,
            "deletions": 20,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fconfiguration_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fconfiguration_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fconfiguration_gpt_neox.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -17,7 +17,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -50,17 +50,14 @@ class GPTNeoXConfig(PreTrainedConfig):\n         hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n             The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n             `\"relu\"`, `\"selu\"` and `\"gelu_new\"` are supported.\n-        rotary_pct (`float`, *optional*, defaults to 0.25):\n-            percentage of hidden dimensions to allocate to rotary embeddings\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio probability of the attention score.\n         hidden_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio of (1) the word embeddings, (2) the post-attention hidden states, and (3) the post-mlp\n             hidden states.\n         classifier_dropout (`float`, *optional*, defaults to 0.1):\n             Argument used when doing token classification, used in the model [`GPTNeoXForTokenClassification`].\n-\n-            The dropout ratio for the hidden layer.\n+            The dropout ratio for the c;assifier head.\n         max_position_embeddings (`int`, *optional*, defaults to 2048):\n             The maximum sequence length that this model might ever be used with. Typically set this to something large\n             just in case (e.g., 512 or 1024 or 2048).\n@@ -119,7 +116,6 @@ def __init__(\n         num_attention_heads: Optional[int] = 64,\n         intermediate_size: Optional[int] = 24576,\n         hidden_act: Optional[str] = \"gelu\",\n-        rotary_pct: Optional[float] = 0.25,\n         attention_dropout: Optional[float] = 0.0,\n         hidden_dropout: Optional[float] = 0.0,\n         classifier_dropout: Optional[float] = 0.1,\n@@ -135,40 +131,43 @@ def __init__(\n         attention_bias: Optional[bool] = True,\n         **kwargs,\n     ):\n-        super().__init__(bos_token_id=bos_token_id, eos_token_id=eos_token_id, **kwargs)\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n         self.num_hidden_layers = num_hidden_layers\n         self.num_attention_heads = num_attention_heads\n         self.intermediate_size = intermediate_size\n         self.hidden_act = hidden_act\n-        self.rotary_pct = rotary_pct\n-        self.partial_rotary_factor = rotary_pct\n         self.attention_dropout = attention_dropout\n         self.hidden_dropout = hidden_dropout\n         self.classifier_dropout = classifier_dropout\n         self.initializer_range = initializer_range\n         self.layer_norm_eps = layer_norm_eps\n         self.use_cache = use_cache\n-        self.tie_word_embeddings = tie_word_embeddings\n         self.use_parallel_residual = use_parallel_residual\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n         self.attention_bias = attention_bias\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n+        self.rope_parameters = rope_parameters\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rotary_emb_base\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n         if self.hidden_size % self.num_attention_heads != 0:\n             raise ValueError(\n                 \"The hidden size is not divisible by the number of attention heads! Make sure to update them!\"\n             )\n+        super().__init__(\n+            bos_token_id=bos_token_id, eos_token_id=eos_token_id, tie_word_embeddings=tie_word_embeddings, **kwargs\n+        )\n+\n+    def convert_rope_params_to_dict(self, ignore_keys_at_rope_validation=None, **kwargs):\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or self.rope_parameters\n+        self.rope_parameters = self.rope_parameters if self.rope_parameters is not None else {}\n+\n+        # Standardize and validate the correctness of rotary position embeddings parameters\n+        # Model uses non-standard naming for rope params, overwrite!\n+        self.rope_parameters.setdefault(\"rope_theta\", kwargs.pop(\"rotary_emb_base\", self.default_theta))\n+        self.rope_parameters[\"partial_rotary_factor\"] = kwargs.pop(\"rotary_pct\", 0.25)\n+        self.standardize_rope_params()\n+        self.validate_rope(ignore_keys=ignore_keys_at_rope_validation)\n+        return kwargs\n \n \n __all__ = [\"GPTNeoXConfig\"]"
        },
        {
            "sha": "56af644f21d968ca440dc630de8c73ae89c6fa4d",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -88,7 +88,7 @@ def compute_default_rope_parameters(\n             post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n         \"\"\"\n         base = config.rope_parameters[\"rope_theta\"]\n-        partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+        partial_rotary_factor = config.rope_parameters.get(\"partial_rotary_factor\", 1.0)\n         head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n         dim = int(head_dim * partial_rotary_factor)\n \n@@ -194,7 +194,8 @@ def __init__(self, config, layer_idx=None):\n         self.config = config\n         self.head_size = config.hidden_size // config.num_attention_heads\n         self.attention_dropout = config.attention_dropout\n-        self.rotary_ndims = int(self.head_size * config.rotary_pct)\n+        partial_rotary_factor = config.rope_parameters.get(\"partial_rotary_factor\", 1.0)\n+        self.rotary_ndims = int(self.head_size * partial_rotary_factor)\n         self.scaling = self.head_size**-0.5\n         self.is_causal = True\n         self.layer_idx = layer_idx"
        },
        {
            "sha": "f31575b052d2521b8c67408e645c397892338bd9",
            "filename": "src/transformers/models/gpt_neox/modular_gpt_neox.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -62,7 +62,7 @@ def compute_default_rope_parameters(\n             post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n         \"\"\"\n         base = config.rope_parameters[\"rope_theta\"]\n-        partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+        partial_rotary_factor = config.rope_parameters.get(\"partial_rotary_factor\", 1.0)\n         head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n         dim = int(head_dim * partial_rotary_factor)\n \n@@ -146,7 +146,8 @@ def __init__(self, config, layer_idx=None):\n         self.config = config\n         self.head_size = config.hidden_size // config.num_attention_heads\n         self.attention_dropout = config.attention_dropout\n-        self.rotary_ndims = int(self.head_size * config.rotary_pct)\n+        partial_rotary_factor = config.rope_parameters.get(\"partial_rotary_factor\", 1.0)\n+        self.rotary_ndims = int(self.head_size * partial_rotary_factor)\n         self.scaling = self.head_size**-0.5\n         self.is_causal = True\n         self.layer_idx = layer_idx"
        },
        {
            "sha": "93d0e925d1af9afa8f01c0be08cbfc3085f51f4f",
            "filename": "src/transformers/models/gpt_neox_japanese/configuration_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 16,
            "deletions": 17,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fconfiguration_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fconfiguration_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fconfiguration_gpt_neox_japanese.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -17,7 +17,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -49,8 +49,6 @@ class GPTNeoXJapaneseConfig(PreTrainedConfig):\n             intermediate_multiple_size.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n             The non-linear activation function (function or string) in the encoder and pooler.\n-        rotary_pct (`float`, *optional*, defaults to 1.00):\n-            percentage of hidden dimensions to allocate to rotary embeddings\n         max_position_embeddings (`int`, *optional*, defaults to 2048):\n             The maximum sequence length that this model might ever be used with.\n         initializer_range (`float`, *optional*, defaults to 0.02):\n@@ -93,7 +91,6 @@ def __init__(\n         num_attention_heads: Optional[int] = 32,\n         intermediate_multiple_size: Optional[int] = 4,\n         hidden_act: Optional[str] = \"gelu\",\n-        rotary_pct: Optional[float] = 1.00,\n         max_position_embeddings: Optional[int] = 2048,\n         initializer_range: Optional[float] = 0.02,\n         layer_norm_eps: Optional[int] = 1e-5,\n@@ -105,32 +102,34 @@ def __init__(\n         hidden_dropout: Optional[float] = 0.0,\n         **kwargs,\n     ):\n-        super().__init__(bos_token_id=bos_token_id, eos_token_id=eos_token_id, **kwargs)\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n         self.num_hidden_layers = num_hidden_layers\n         self.num_attention_heads = num_attention_heads\n         self.intermediate_multiple_size = intermediate_multiple_size\n         self.hidden_act = hidden_act\n-        self.rotary_pct = rotary_pct\n-        self.partial_rotary_factor = rotary_pct\n         self.initializer_range = initializer_range\n         self.layer_norm_eps = layer_norm_eps\n         self.use_cache = use_cache\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n         self.attention_dropout = attention_dropout\n         self.hidden_dropout = hidden_dropout\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n+        self.rope_parameters = rope_parameters\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rotary_emb_base\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        super().__init__(bos_token_id=bos_token_id, eos_token_id=eos_token_id, **kwargs)\n+\n+    def convert_rope_params_to_dict(self, ignore_keys_at_rope_validation=None, **kwargs):\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or self.rope_parameters\n+        self.rope_parameters = self.rope_parameters if self.rope_parameters is not None else {}\n+\n+        # Standardize and validate the correctness of rotary position embeddings parameters\n+        # Model uses non-standard naming for rope params, overwrite!\n+        self.rope_parameters.setdefault(\"rope_theta\", kwargs.pop(\"rotary_emb_base\", self.default_theta))\n+        self.rope_parameters[\"partial_rotary_factor\"] = kwargs.pop(\"rotary_pct\", 1.0)\n+        self.standardize_rope_params()\n+        self.validate_rope(ignore_keys=ignore_keys_at_rope_validation)\n+        return kwargs\n \n \n __all__ = [\"GPTNeoXJapaneseConfig\"]"
        },
        {
            "sha": "f2112083d38885bce8dcd2be425e8b8a98639b4f",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -174,7 +174,8 @@ def __init__(self, config, use_bias=False, layer_idx=None):\n             )\n \n         self.layer_idx = layer_idx\n-        self.rotary_ndims = int(self.head_size * config.rotary_pct)\n+        partial_rotary_factor = config.rope_parameters.get(\"partial_rotary_factor\", 1.0)\n+        self.rotary_ndims = int(self.head_size * partial_rotary_factor)\n         self.attention_dropout = nn.Dropout(config.attention_dropout)\n         self.norm_factor = math.sqrt(self.head_size)\n "
        },
        {
            "sha": "78beab335b047172522c1d43ae47fbb9897781e9",
            "filename": "src/transformers/models/gpt_oss/configuration_gpt_oss.py",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fconfiguration_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fconfiguration_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fconfiguration_gpt_oss.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -17,7 +17,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class GptOssConfig(PreTrainedConfig):\n@@ -28,6 +28,7 @@ class GptOssConfig(PreTrainedConfig):\n     \"\"\"\n \n     model_type = \"gpt_oss\"\n+    default_theta = 150000.0\n     base_model_pp_plan = {\n         \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n         \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n@@ -109,14 +110,7 @@ def __init__(\n         self.router_aux_loss_coef = router_aux_loss_coef\n         self.output_router_logits = output_router_logits\n         self.use_cache = use_cache\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 150000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             tie_word_embeddings=tie_word_embeddings,"
        },
        {
            "sha": "c94c40169e4ecf954dc141472b9da3c7fdef412c",
            "filename": "src/transformers/models/granite/configuration_granite.py",
            "status": "modified",
            "additions": 2,
            "deletions": 11,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fgranite%2Fconfiguration_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fgranite%2Fconfiguration_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fconfiguration_granite.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -22,7 +22,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -175,14 +175,7 @@ def __init__(\n         self.logits_scaling = logits_scaling\n         self.residual_multiplier = residual_multiplier\n         self.attention_multiplier = attention_multiplier\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             pad_token_id=pad_token_id,\n@@ -192,7 +185,5 @@ def __init__(\n             **kwargs,\n         )\n \n-        rope_config_validation(self)\n-\n \n __all__ = [\"GraniteConfig\"]"
        },
        {
            "sha": "55c619c99e7227d33faa5659677fb551aa9cb0ac",
            "filename": "src/transformers/models/granitemoe/configuration_granitemoe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 12,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fconfiguration_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fconfiguration_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fconfiguration_granitemoe.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -22,7 +22,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -159,15 +159,6 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n-\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n \n@@ -181,6 +172,8 @@ def __init__(\n         self.output_router_logits = output_router_logits\n         self.router_aux_loss_coef = router_aux_loss_coef\n \n+        self.rope_parameters = rope_parameters\n+\n         super().__init__(\n             pad_token_id=pad_token_id,\n             bos_token_id=bos_token_id,\n@@ -189,7 +182,5 @@ def __init__(\n             **kwargs,\n         )\n \n-        rope_config_validation(self)\n-\n \n __all__ = [\"GraniteMoeConfig\"]"
        },
        {
            "sha": "08320e9fb51383f1940566912156b2cdcaf0ab01",
            "filename": "src/transformers/models/granitemoehybrid/configuration_granitemoehybrid.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fconfiguration_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fconfiguration_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fconfiguration_granitemoehybrid.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -18,7 +18,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -198,14 +198,7 @@ def __init__(\n         self.output_router_logits = output_router_logits\n         self.router_aux_loss_coef = router_aux_loss_coef\n         self.shared_intermediate_size = shared_intermediate_size\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         mamba_intermediate = mamba_expand * hidden_size\n "
        },
        {
            "sha": "23f35a0f1989f0cdc45b6247cdcd23b98a798e32",
            "filename": "src/transformers/models/granitemoeshared/configuration_granitemoeshared.py",
            "status": "modified",
            "additions": 5,
            "deletions": 14,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fconfiguration_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fconfiguration_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fconfiguration_granitemoeshared.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -22,7 +22,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -162,17 +162,6 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        # this model has rope embedding type, hardcoded for BC\n-        self.position_embedding_type = \"rope\"\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n-\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n \n@@ -187,6 +176,10 @@ def __init__(\n         self.router_aux_loss_coef = router_aux_loss_coef\n         self.shared_intermediate_size = shared_intermediate_size\n \n+        # this model has rope embedding type, hardcoded for BC\n+        self.position_embedding_type = \"rope\"\n+        self.rope_parameters = rope_parameters\n+\n         super().__init__(\n             pad_token_id=pad_token_id,\n             bos_token_id=bos_token_id,\n@@ -195,7 +188,5 @@ def __init__(\n             **kwargs,\n         )\n \n-        rope_config_validation(self)\n-\n \n __all__ = [\"GraniteMoeSharedConfig\"]"
        },
        {
            "sha": "d633b23ccebd67452be23733b319cb9593d15db8",
            "filename": "src/transformers/models/helium/configuration_helium.py",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fhelium%2Fconfiguration_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fhelium%2Fconfiguration_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fconfiguration_helium.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -17,7 +17,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class HeliumConfig(PreTrainedConfig):\n@@ -93,6 +93,7 @@ class HeliumConfig(PreTrainedConfig):\n \n     model_type = \"helium\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    default_theta = 100000.0\n     base_model_tp_plan = {\n         \"layers.*.self_attn.q_proj\": \"colwise\",\n         \"layers.*.self_attn.k_proj\": \"colwise\",\n@@ -147,14 +148,7 @@ def __init__(\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.mlp_bias = mlp_bias\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 100000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "f750e2302d39a778a7614baa3270476c8e0d365a",
            "filename": "src/transformers/models/hunyuan_v1_dense/configuration_hunyuan_v1_dense.py",
            "status": "modified",
            "additions": 2,
            "deletions": 39,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fconfiguration_hunyuan_v1_dense.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fconfiguration_hunyuan_v1_dense.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fconfiguration_hunyuan_v1_dense.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -17,7 +17,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -141,14 +141,7 @@ def __init__(\n         self.use_cache = use_cache\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)  # TODO needs model-specific validation?\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             pad_token_id=pad_token_id,\n@@ -158,35 +151,5 @@ def __init__(\n             **kwargs,\n         )\n \n-    def _rope_parameters_validation(self):\n-        \"\"\"\n-        Validate the `rope_parameters` configuration.\n-        \"\"\"\n-        if self.rope_parameters is None:\n-            return\n-\n-        if not isinstance(self.rope_parameters, dict) or len(self.rope_parameters) != 2:\n-            raise ValueError(\n-                \"`rope_parameters` must be a dictionary with with two fields, `type` and `factor` or `type` and `alpha`, \"\n-                f\"got {self.rope_parameters}\"\n-            )\n-        rope_parameters_type = self.rope_parameters.get(\"type\", None)\n-        rope_parameters_factor = self.rope_parameters.get(\"factor\", None)\n-        rope_parameters_alpha = self.rope_parameters.get(\"alpha\", None)\n-        if rope_parameters_type is None or rope_parameters_type not in [\"linear\", \"dynamic\"]:\n-            raise ValueError(\n-                f\"`rope_parameters`'s type field must be one of ['linear', 'dynamic'], got {rope_parameters_type}\"\n-            )\n-        if rope_parameters_factor is None and rope_parameters_alpha is None:\n-            raise ValueError(\"`rope_parameters`'s factor or alpha field must be have one, got both of none\")\n-        if rope_parameters_factor is not None:\n-            if not isinstance(rope_parameters_factor, float) or rope_parameters_factor <= 1.0:\n-                raise ValueError(\n-                    f\"`rope_parameters`'s factor field must be a float > 1.0, got {rope_parameters_factor}\"\n-                )\n-        if rope_parameters_alpha is not None:\n-            if not isinstance(rope_parameters_alpha, float) or rope_parameters_alpha <= 1.0:\n-                raise ValueError(f\"`rope_parameters`'s alpha field must be a float > 1.0, got {rope_parameters_alpha}\")\n-\n \n __all__ = [\"HunYuanDenseV1Config\"]"
        },
        {
            "sha": "ab6844c8d903923711db5ea14fdb9887bd461e53",
            "filename": "src/transformers/models/hunyuan_v1_moe/configuration_hunyuan_v1_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fconfiguration_hunyuan_v1_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fconfiguration_hunyuan_v1_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fconfiguration_hunyuan_v1_moe.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -17,7 +17,7 @@\n from typing import Optional, Union\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -157,14 +157,7 @@ def __init__(\n         self.use_cache = use_cache\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "3f3c1f6320614062deafb61d53ccf2fb9c148b8b",
            "filename": "src/transformers/models/jetmoe/configuration_jetmoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fconfiguration_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fconfiguration_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fconfiguration_jetmoe.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -17,7 +17,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -147,14 +147,7 @@ def __init__(\n         self.bos_token_id = bos_token_id\n         self.eos_token_id = eos_token_id\n         self.rms_norm_eps = rms_norm_eps\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             bos_token_id=bos_token_id, eos_token_id=eos_token_id, tie_word_embeddings=tie_word_embeddings, **kwargs"
        },
        {
            "sha": "40205002803b8eb068985b6a94782b52fda5ad08",
            "filename": "src/transformers/models/kyutai_speech_to_text/configuration_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fconfiguration_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fconfiguration_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fconfiguration_kyutai_speech_to_text.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -16,7 +16,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n from ..auto.configuration_auto import AutoConfig\n \n@@ -183,14 +183,7 @@ def __init__(\n         self.attention_dropout = attention_dropout\n         self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n         self.sliding_window = sliding_window\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             pad_token_id=pad_token_id, bos_token_id=bos_token_id, tie_word_embeddings=tie_word_embeddings, **kwargs"
        },
        {
            "sha": "9b9129455d3adbd0ebaba5d80384b4698009d66e",
            "filename": "src/transformers/models/lfm2/configuration_lfm2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Flfm2%2Fconfiguration_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Flfm2%2Fconfiguration_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fconfiguration_lfm2.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -14,7 +14,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class Lfm2Config(PreTrainedConfig):\n@@ -100,6 +100,7 @@ class Lfm2Config(PreTrainedConfig):\n \n     model_type = \"lfm2\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    default_theta = 1000000.0\n \n     def __init__(\n         self,\n@@ -148,20 +149,13 @@ def __init__(\n         self.block_multiple_of = block_multiple_of\n         self.block_ffn_dim_multiplier = block_ffn_dim_multiplier\n         self.block_auto_adjust_ff_dim = block_auto_adjust_ff_dim\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n \n         self.layer_types = layer_types\n         if self.layer_types is None:\n             full_attn_idxs = full_attn_idxs if full_attn_idxs is not None else list(range(num_hidden_layers))\n             self.layer_types = [\"full_attention\" if i in full_attn_idxs else \"conv\" for i in range(num_hidden_layers)]\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"theta\", kwargs.get(\"rope_theta\", 1000000.0))\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n-\n+        self.rope_parameters = rope_parameters\n         tie_word_embeddings = kwargs.get(\"tie_embedding\", tie_word_embeddings)  # to fit original config keys\n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "b6a1dcb1512a15e36bf79de90ebc498ca55f7df9",
            "filename": "src/transformers/models/lfm2_moe/configuration_lfm2_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fconfiguration_lfm2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fconfiguration_lfm2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fconfiguration_lfm2_moe.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -14,7 +14,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class Lfm2MoeConfig(PreTrainedConfig):\n@@ -103,6 +103,7 @@ class Lfm2MoeConfig(PreTrainedConfig):\n \n     model_type = \"lfm2_moe\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    default_theta = 1000000.0\n \n     def __init__(\n         self,\n@@ -136,9 +137,6 @@ def __init__(\n         self.hidden_size = hidden_size\n         self.intermediate_size = intermediate_size\n         self.num_hidden_layers = num_hidden_layers\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n         self.max_position_embeddings = max_position_embeddings\n         self.use_cache = use_cache\n         self.norm_eps = norm_eps\n@@ -161,11 +159,7 @@ def __init__(\n         self.norm_topk_prob = norm_topk_prob\n         self.layer_types = layer_types\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"theta\", kwargs.get(\"rope_theta\", 1000000.0))\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n-\n+        self.rope_parameters = rope_parameters\n         tie_word_embeddings = kwargs.get(\"tie_embedding\", tie_word_embeddings)  # to fit original config keys\n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "cc3db887fbb324c552916f930f90c6550250d60e",
            "filename": "src/transformers/models/llama/configuration_llama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fllama%2Fconfiguration_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fllama%2Fconfiguration_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fconfiguration_llama.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -22,7 +22,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class LlamaConfig(PreTrainedConfig):\n@@ -171,14 +171,7 @@ def __init__(\n         self.attention_dropout = attention_dropout\n         self.mlp_bias = mlp_bias\n         self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "af6b57805a927f4ff6c209b75dc9e39fd06cd2a7",
            "filename": "src/transformers/models/llama4/configuration_llama4.py",
            "status": "modified",
            "additions": 13,
            "deletions": 24,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -17,7 +17,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -124,14 +124,9 @@ def __init__(\n         self.projector_dropout = projector_dropout\n         self.attention_dropout = attention_dropout\n         self.vision_feature_select_strategy = vision_feature_select_strategy\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+\n+        self.rope_parameters = rope_parameters\n+\n         super().__init__(**kwargs)\n \n \n@@ -218,6 +213,7 @@ class Llama4TextConfig(PreTrainedConfig):\n \n     model_type = \"llama4_text\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    default_theta = 500000.0\n     base_model_tp_plan = {\n         \"layers.*.self_attn.q_proj\": \"colwise\",\n         \"layers.*.self_attn.k_proj\": \"colwise\",\n@@ -286,13 +282,6 @@ def __init__(\n         attn_scale=0.1,\n         **kwargs,\n     ):\n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )\n         self.attn_temperature_tuning = attn_temperature_tuning\n         self.attn_scale = attn_scale\n         self.floor_scale = floor_scale\n@@ -316,10 +305,6 @@ def __init__(\n         self.attention_dropout = attention_dropout\n         self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n         self.use_qk_norm = use_qk_norm\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n         self.num_experts_per_tok = num_experts_per_tok\n         self.num_local_experts = num_local_experts\n \n@@ -352,10 +337,14 @@ def __init__(\n             ]\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 500000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n \n \n class Llama4Config(PreTrainedConfig):"
        },
        {
            "sha": "79f2e82daf30846d57c52c88f0cfc49c31d50713",
            "filename": "src/transformers/models/longcat_flash/configuration_longcat_flash.py",
            "status": "modified",
            "additions": 19,
            "deletions": 14,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fconfiguration_longcat_flash.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fconfiguration_longcat_flash.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fconfiguration_longcat_flash.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -18,7 +18,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class LongcatFlashConfig(PreTrainedConfig):\n@@ -122,6 +122,7 @@ class LongcatFlashConfig(PreTrainedConfig):\n \n     model_type = \"longcat_flash\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    default_theta = 10000000.0\n     base_model_tp_plan = {\n         \"layers.*.self_attn.*.q_b_proj\": \"colwise\",\n         \"layers.*.self_attn.*.kv_b_proj\": \"colwise\",\n@@ -210,19 +211,7 @@ def __init__(\n         self.zero_expert_num = zero_expert_num\n         self.expert_ffn_hidden_size = expert_ffn_hidden_size\n         self.routed_scaling_factor = routed_scaling_factor\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-\n-        for key in [\"beta_fast\", \"beta_slow\", \"factor\"]:\n-            if key in self.rope_parameters:\n-                self.rope_parameters[key] = float(self.rope_parameters[key])\n-\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             pad_token_id=pad_token_id,\n@@ -232,5 +221,21 @@ def __init__(\n             **kwargs,\n         )\n \n+    def convert_rope_params_to_dict(self, ignore_keys_at_rope_validation: Optional[set] = None, **kwargs):\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or self.rope_parameters\n+        self.rope_parameters = self.rope_parameters if self.rope_parameters is not None else {}\n+\n+        # Standardize and validate the correctness of rotary position embeddings parameters\n+        self.rope_parameters.setdefault(\"rope_theta\", kwargs.pop(\"rope_theta\", self.default_theta))\n+        self.standardize_rope_params()\n+        self.validate_rope(ignore_keys=ignore_keys_at_rope_validation)\n+\n+        # Convert to float because RoPE fn expect a float. Models on the hub were saved as int\n+        for key in [\"beta_fast\", \"beta_slow\", \"factor\"]:\n+            if key in self.rope_parameters:\n+                self.rope_parameters[key] = float(self.rope_parameters[key])\n+        return kwargs\n+\n \n __all__ = [\"LongcatFlashConfig\"]"
        },
        {
            "sha": "7869bcfafa37a909b9fbc92b82c01172a11027af",
            "filename": "src/transformers/models/mimi/configuration_mimi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fmimi%2Fconfiguration_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fmimi%2Fconfiguration_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fconfiguration_mimi.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -20,7 +20,7 @@\n import numpy as np\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -221,14 +221,7 @@ def __init__(\n         self.head_dim = head_dim or hidden_size // num_attention_heads\n         self.layer_scale_initial_scale = layer_scale_initial_scale\n         self.attention_bias = attention_bias\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         # Handle backward compatibility for frame_rate:\n         # If frame_rate is explicitly provided, use it (backward compatibility)"
        },
        {
            "sha": "7f5e34d71e7705a66309a0e33ecda49140740e47",
            "filename": "src/transformers/models/minimax/configuration_minimax.py",
            "status": "modified",
            "additions": 3,
            "deletions": 8,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fminimax%2Fconfiguration_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fminimax%2Fconfiguration_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fconfiguration_minimax.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -23,7 +23,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class MiniMaxConfig(PreTrainedConfig):\n@@ -132,6 +132,7 @@ class MiniMaxConfig(PreTrainedConfig):\n \n     model_type = \"minimax\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    default_theta = 1000000.0\n     base_model_tp_plan = {\n         \"layers.*.self_attn.q_proj\": \"colwise\",\n         \"layers.*.self_attn.k_proj\": \"colwise\",\n@@ -221,20 +222,14 @@ def __init__(\n         self.linear_attn_beta_factor = linear_attn_beta_factor\n         self.mlp_alpha_factor = mlp_alpha_factor\n         self.mlp_beta_factor = mlp_beta_factor\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n \n         if self.layer_types is None:\n             self.layer_types = [\n                 \"full_attention\" if bool((i + 1) % 2) else \"linear_attention\" for i in range(self.num_hidden_layers)\n             ]\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = getattr(self, \"rope_theta\", 1000000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n         super().__init__(\n             pad_token_id=pad_token_id,\n             bos_token_id=bos_token_id,"
        },
        {
            "sha": "d9b428bbce86b0fc9a81c17fa9f4bf475a242585",
            "filename": "src/transformers/models/minimax/modular_minimax.py",
            "status": "modified",
            "additions": 3,
            "deletions": 8,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -28,7 +28,7 @@\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import MoeModelOutputWithPast\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n from ...utils.generic import OutputRecorder, check_model_inputs\n@@ -157,6 +157,7 @@ class MiniMaxConfig(PreTrainedConfig):\n \n     model_type = \"minimax\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    default_theta = 1000000.0\n     base_model_tp_plan = {\n         \"layers.*.self_attn.q_proj\": \"colwise\",\n         \"layers.*.self_attn.k_proj\": \"colwise\",\n@@ -246,20 +247,14 @@ def __init__(\n         self.linear_attn_beta_factor = linear_attn_beta_factor\n         self.mlp_alpha_factor = mlp_alpha_factor\n         self.mlp_beta_factor = mlp_beta_factor\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n \n         if self.layer_types is None:\n             self.layer_types = [\n                 \"full_attention\" if bool((i + 1) % 2) else \"linear_attention\" for i in range(self.num_hidden_layers)\n             ]\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = getattr(self, \"rope_theta\", 1000000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n         super().__init__(\n             pad_token_id=pad_token_id,\n             bos_token_id=bos_token_id,"
        },
        {
            "sha": "0afccfb429d992de598ffcb93f4bd09489cd2dc9",
            "filename": "src/transformers/models/ministral/configuration_ministral.py",
            "status": "modified",
            "additions": 10,
            "deletions": 15,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fministral%2Fconfiguration_ministral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fministral%2Fconfiguration_ministral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fministral%2Fconfiguration_ministral.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -7,7 +7,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class MinistralConfig(PreTrainedConfig):\n@@ -130,13 +130,6 @@ def __init__(\n         layer_types: Optional[list[str]] = None,\n         **kwargs,\n     ):\n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -157,19 +150,21 @@ def __init__(\n         self.use_cache = use_cache\n         self.attention_dropout = attention_dropout\n         self.layer_types = layer_types\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n \n         if self.layer_types is None:\n             self.layer_types = [\n                 \"sliding_attention\" if self.sliding_window is not None else \"full_attention\"\n             ] * num_hidden_layers\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = getattr(self, \"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n+\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n \n \n __all__ = [\"MinistralConfig\"]"
        },
        {
            "sha": "9e3a185f2c2502eb172040b69fdc71e07040db26",
            "filename": "src/transformers/models/ministral/modular_ministral.py",
            "status": "modified",
            "additions": 11,
            "deletions": 16,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fministral%2Fmodular_ministral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fministral%2Fmodular_ministral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fministral%2Fmodular_ministral.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -7,7 +7,7 @@\n from ...configuration_utils import PreTrainedConfig\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_outputs import BaseModelOutputWithPast\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring\n from ...utils.generic import check_model_inputs\n@@ -131,14 +131,6 @@ def __init__(\n         layer_types: Optional[list[str]] = None,\n         **kwargs,\n     ):\n-        PreTrainedConfig.__init__(\n-            self,\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -159,19 +151,22 @@ def __init__(\n         self.use_cache = use_cache\n         self.attention_dropout = attention_dropout\n         self.layer_types = layer_types\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n \n         if self.layer_types is None:\n             self.layer_types = [\n                 \"sliding_attention\" if self.sliding_window is not None else \"full_attention\"\n             ] * num_hidden_layers\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = getattr(self, \"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n+\n+        PreTrainedConfig.__init__(\n+            self,\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n \n \n class MinistralMLP(Qwen2MLP):"
        },
        {
            "sha": "1c6cb9c4e6847b48a2b2df9e33556c00c9a635e3",
            "filename": "src/transformers/models/mistral/configuration_mistral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fmistral%2Fconfiguration_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fmistral%2Fconfiguration_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fconfiguration_mistral.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -17,7 +17,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -166,14 +166,7 @@ def __init__(\n                 \"Detected Mistral model with layer_types. Consider using AutoModel or Ministral classes instead to enable alternating attention compatibility.\"\n             )\n \n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "18e58f5f64849adee9f383a538b4c996fbb8c06a",
            "filename": "src/transformers/models/mixtral/configuration_mixtral.py",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -17,7 +17,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -114,6 +114,7 @@ class MixtralConfig(PreTrainedConfig):\n \n     model_type = \"mixtral\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    default_theta = 1000000.0\n     base_model_tp_plan = {\n         \"layers.*.self_attn.q_proj\": \"colwise\",\n         \"layers.*.self_attn.k_proj\": \"colwise\",\n@@ -186,14 +187,7 @@ def __init__(\n         self.output_router_logits = output_router_logits\n         self.router_aux_loss_coef = router_aux_loss_coef\n         self.router_jitter_noise = router_jitter_noise\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 1000000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "3760b0b228f9291278e5ca5413851db72169da6e",
            "filename": "src/transformers/models/mllama/configuration_mllama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fmllama%2Fconfiguration_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fmllama%2Fconfiguration_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fconfiguration_mllama.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -16,7 +16,6 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import rope_config_validation, standardize_rope_params\n from ...utils import logging\n \n \n@@ -208,6 +207,7 @@ class MllamaTextConfig(PreTrainedConfig):\n \n     model_type = \"mllama_text_model\"\n     base_config_key = \"text_config\"\n+    default_theta = 500000.0\n \n     def __init__(\n         self,\n@@ -247,14 +247,7 @@ def __init__(\n         self.dropout = dropout\n         self.hidden_act = hidden_act\n         self.max_position_embeddings = max_position_embeddings\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 500000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "80e6c19092c6bb65955e095aa3637dd1f0066597",
            "filename": "src/transformers/models/modernbert/configuration_modernbert.py",
            "status": "modified",
            "additions": 35,
            "deletions": 19,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fconfiguration_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fconfiguration_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fconfiguration_modernbert.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -22,7 +22,7 @@\n from typing import Literal, Optional\n \n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class ModernBertConfig(PreTrainedConfig):\n@@ -130,8 +130,8 @@ class ModernBertConfig(PreTrainedConfig):\n     ```\"\"\"\n \n     model_type = \"modernbert\"\n-    attribute_map = {\"rope_theta\": \"global_rope_theta\"}\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    default_theta = {\"global\": 160_000.0, \"local\": 10_000.0}\n \n     def __init__(\n         self,\n@@ -171,14 +171,6 @@ def __init__(\n         repad_logits_with_grad: Optional[bool] = False,\n         **kwargs,\n     ):\n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            cls_token_id=cls_token_id,\n-            sep_token_id=sep_token_id,\n-            **kwargs,\n-        )\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -206,9 +198,6 @@ def __init__(\n         self.sparse_pred_ignore_index = sparse_pred_ignore_index\n         self.reference_compile = reference_compile\n         self.repad_logits_with_grad = repad_logits_with_grad\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n \n         if self.classifier_pooling not in [\"cls\", \"mean\"]:\n             raise ValueError(\n@@ -227,13 +216,40 @@ def __init__(\n             ]\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = getattr(self, \"global_rope_theta\", 160_000.0)\n-        rope_local_base_freq = getattr(self, \"local_rope_theta\", 10000.0)\n-        standardize_rope_params(\n-            self, rope_theta={\"full_attention\": rope_theta, \"sliding_attention\": rope_local_base_freq}\n+        self.rope_parameters = rope_parameters\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            cls_token_id=cls_token_id,\n+            sep_token_id=sep_token_id,\n+            **kwargs,\n+        )\n+\n+    def convert_rope_params_to_dict(self, ignore_keys_at_rope_validation=None, **kwargs):\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`. If we find `rope_parameters`\n+        # as arg in the inputs, we can safely assume that it is in the new format. New naming used -> new format\n+        default_rope_params = {\n+            \"sliding_attention\": {\"rope_type\": \"default\"},\n+            \"full_attention\": {\"rope_type\": \"default\"},\n+        }\n+        self.rope_parameters = self.rope_parameters if self.rope_parameters is not None else default_rope_params\n+        if rope_scaling is not None:\n+            self.rope_parameters[\"full_attention\"].update(rope_scaling)\n+            self.rope_parameters[\"sliding_attention\"].update(rope_scaling)\n+        self.rope_parameters[\"full_attention\"].setdefault(\n+            \"rope_theta\", kwargs.pop(\"global_rope_theta\", self.default_theta[\"global\"])\n         )\n-        rope_config_validation(self)\n+        self.rope_parameters[\"sliding_attention\"].setdefault(\n+            \"rope_theta\", kwargs.pop(\"local_rope_theta\", self.default_theta[\"local\"])\n+        )\n+\n+        # Standardize and validate the correctness of rotary position embeddings parameters\n+        self.standardize_rope_params()\n+        self.validate_rope(ignore_keys=ignore_keys_at_rope_validation)\n+        return kwargs\n \n     def to_dict(self):\n         output = super().to_dict()"
        },
        {
            "sha": "49f6244cfb2f86012aad07514f235a3f273a9b38",
            "filename": "src/transformers/models/modernbert/modular_modernbert.py",
            "status": "modified",
            "additions": 35,
            "deletions": 19,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -35,7 +35,7 @@\n     SequenceClassifierOutput,\n     TokenClassifierOutput,\n )\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, is_flash_attn_2_available, logging\n from ...utils.import_utils import is_triton_available\n@@ -158,8 +158,8 @@ class ModernBertConfig(PreTrainedConfig):\n     ```\"\"\"\n \n     model_type = \"modernbert\"\n-    attribute_map = {\"rope_theta\": \"global_rope_theta\"}\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    default_theta = {\"global\": 160_000.0, \"local\": 10_000.0}\n \n     def __init__(\n         self,\n@@ -199,14 +199,6 @@ def __init__(\n         repad_logits_with_grad: Optional[bool] = False,\n         **kwargs,\n     ):\n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            cls_token_id=cls_token_id,\n-            sep_token_id=sep_token_id,\n-            **kwargs,\n-        )\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -234,9 +226,6 @@ def __init__(\n         self.sparse_pred_ignore_index = sparse_pred_ignore_index\n         self.reference_compile = reference_compile\n         self.repad_logits_with_grad = repad_logits_with_grad\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n \n         if self.classifier_pooling not in [\"cls\", \"mean\"]:\n             raise ValueError(\n@@ -255,13 +244,40 @@ def __init__(\n             ]\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = getattr(self, \"global_rope_theta\", 160_000.0)\n-        rope_local_base_freq = getattr(self, \"local_rope_theta\", 10000.0)\n-        standardize_rope_params(\n-            self, rope_theta={\"full_attention\": rope_theta, \"sliding_attention\": rope_local_base_freq}\n+        self.rope_parameters = rope_parameters\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            cls_token_id=cls_token_id,\n+            sep_token_id=sep_token_id,\n+            **kwargs,\n         )\n-        rope_config_validation(self)\n+\n+    def convert_rope_params_to_dict(self, ignore_keys_at_rope_validation=None, **kwargs):\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`. If we find `rope_parameters`\n+        # as arg in the inputs, we can safely assume that it is in the new format. New naming used -> new format\n+        default_rope_params = {\n+            \"sliding_attention\": {\"rope_type\": \"default\"},\n+            \"full_attention\": {\"rope_type\": \"default\"},\n+        }\n+        self.rope_parameters = self.rope_parameters if self.rope_parameters is not None else default_rope_params\n+        if rope_scaling is not None:\n+            self.rope_parameters[\"full_attention\"].update(rope_scaling)\n+            self.rope_parameters[\"sliding_attention\"].update(rope_scaling)\n+        self.rope_parameters[\"full_attention\"].setdefault(\n+            \"rope_theta\", kwargs.pop(\"global_rope_theta\", self.default_theta[\"global\"])\n+        )\n+        self.rope_parameters[\"sliding_attention\"].setdefault(\n+            \"rope_theta\", kwargs.pop(\"local_rope_theta\", self.default_theta[\"local\"])\n+        )\n+\n+        # Standardize and validate the correctness of rotary position embeddings parameters\n+        self.standardize_rope_params()\n+        self.validate_rope(ignore_keys=ignore_keys_at_rope_validation)\n+        return kwargs\n \n     def to_dict(self):\n         output = super().to_dict()"
        },
        {
            "sha": "aaca8cef86c037950ebb9b359017f2731860e957",
            "filename": "src/transformers/models/modernbert_decoder/configuration_modernbert_decoder.py",
            "status": "modified",
            "additions": 36,
            "deletions": 21,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fconfiguration_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fconfiguration_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fconfiguration_modernbert_decoder.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -22,7 +22,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class ModernBertDecoderConfig(PreTrainedConfig):\n@@ -120,8 +120,8 @@ class ModernBertDecoderConfig(PreTrainedConfig):\n     ```\"\"\"\n \n     model_type = \"modernbert-decoder\"\n-    attribute_map = {\"rope_theta\": \"global_rope_theta\"}\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    default_theta = {\"global\": 160_000.0, \"local\": 10_000.0}\n \n     def __init__(\n         self,\n@@ -157,14 +157,6 @@ def __init__(\n         rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         **kwargs,\n     ):\n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            cls_token_id=cls_token_id,\n-            sep_token_id=sep_token_id,\n-            **kwargs,\n-        )\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -187,9 +179,6 @@ def __init__(\n         self.classifier_activation = classifier_activation\n         self.use_cache = use_cache\n         self.global_attn_every_n_layers = global_attn_every_n_layers\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n         # for consistency with ModernBert\n         self.reference_compile = False\n \n@@ -204,16 +193,42 @@ def __init__(\n                 else:\n                     self.layer_types.append(\"full_attention\")\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = getattr(self, \"global_rope_theta\", 160_000.0)\n-        rope_local_base_freq = getattr(self, \"local_rope_theta\", 10000.0)\n-        standardize_rope_params(\n-            self, rope_theta={\"full_attention\": rope_theta, \"sliding_attention\": rope_local_base_freq}\n-        )\n-        rope_config_validation(self)\n-\n         # NOTE: sliding window numbers matches ModernBERT but is only half of it\n         self.sliding_window = local_attention // 2 if local_attention else -1\n+        self.rope_parameters = rope_parameters\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            cls_token_id=cls_token_id,\n+            sep_token_id=sep_token_id,\n+            **kwargs,\n+        )\n+\n+    def convert_rope_params_to_dict(self, ignore_keys_at_rope_validation=None, **kwargs):\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`. If we find `rope_parameters`\n+        # as arg in the inputs, we can safely assume that it is in the new format. New naming used -> new format\n+        default_rope_params = {\n+            \"sliding_attention\": {\"rope_type\": \"default\"},\n+            \"full_attention\": {\"rope_type\": \"default\"},\n+        }\n+        self.rope_parameters = self.rope_parameters if self.rope_parameters is not None else default_rope_params\n+        if rope_scaling is not None:\n+            self.rope_parameters[\"full_attention\"].update(rope_scaling)\n+            self.rope_parameters[\"sliding_attention\"].update(rope_scaling)\n+        self.rope_parameters[\"full_attention\"].setdefault(\n+            \"rope_theta\", kwargs.pop(\"global_rope_theta\", self.default_theta[\"global\"])\n+        )\n+        self.rope_parameters[\"sliding_attention\"].setdefault(\n+            \"rope_theta\", kwargs.pop(\"local_rope_theta\", self.default_theta[\"local\"])\n+        )\n+\n+        # Standardize and validate the correctness of rotary position embeddings parameters\n+        self.standardize_rope_params()\n+        self.validate_rope(ignore_keys=ignore_keys_at_rope_validation)\n+        return kwargs\n \n \n __all__ = [\"ModernBertDecoderConfig\"]"
        },
        {
            "sha": "cfff8ec564addb8ee8a1d06bf5808bac13b585d8",
            "filename": "src/transformers/models/modernbert_decoder/modular_modernbert_decoder.py",
            "status": "modified",
            "additions": 36,
            "deletions": 21,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -28,7 +28,7 @@\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n@@ -141,8 +141,8 @@ class ModernBertDecoderConfig(PreTrainedConfig):\n     ```\"\"\"\n \n     model_type = \"modernbert-decoder\"\n-    attribute_map = {\"rope_theta\": \"global_rope_theta\"}\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    default_theta = {\"global\": 160_000.0, \"local\": 10_000.0}\n \n     def __init__(\n         self,\n@@ -178,14 +178,6 @@ def __init__(\n         rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         **kwargs,\n     ):\n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            cls_token_id=cls_token_id,\n-            sep_token_id=sep_token_id,\n-            **kwargs,\n-        )\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -208,9 +200,6 @@ def __init__(\n         self.classifier_activation = classifier_activation\n         self.use_cache = use_cache\n         self.global_attn_every_n_layers = global_attn_every_n_layers\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n         # for consistency with ModernBert\n         self.reference_compile = False\n \n@@ -225,16 +214,42 @@ def __init__(\n                 else:\n                     self.layer_types.append(\"full_attention\")\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = getattr(self, \"global_rope_theta\", 160_000.0)\n-        rope_local_base_freq = getattr(self, \"local_rope_theta\", 10000.0)\n-        standardize_rope_params(\n-            self, rope_theta={\"full_attention\": rope_theta, \"sliding_attention\": rope_local_base_freq}\n-        )\n-        rope_config_validation(self)\n-\n         # NOTE: sliding window numbers matches ModernBERT but is only half of it\n         self.sliding_window = local_attention // 2 if local_attention else -1\n+        self.rope_parameters = rope_parameters\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            cls_token_id=cls_token_id,\n+            sep_token_id=sep_token_id,\n+            **kwargs,\n+        )\n+\n+    def convert_rope_params_to_dict(self, ignore_keys_at_rope_validation=None, **kwargs):\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+\n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`. If we find `rope_parameters`\n+        # as arg in the inputs, we can safely assume that it is in the new format. New naming used -> new format\n+        default_rope_params = {\n+            \"sliding_attention\": {\"rope_type\": \"default\"},\n+            \"full_attention\": {\"rope_type\": \"default\"},\n+        }\n+        self.rope_parameters = self.rope_parameters if self.rope_parameters is not None else default_rope_params\n+        if rope_scaling is not None:\n+            self.rope_parameters[\"full_attention\"].update(rope_scaling)\n+            self.rope_parameters[\"sliding_attention\"].update(rope_scaling)\n+        self.rope_parameters[\"full_attention\"].setdefault(\n+            \"rope_theta\", kwargs.pop(\"global_rope_theta\", self.default_theta[\"global\"])\n+        )\n+        self.rope_parameters[\"sliding_attention\"].setdefault(\n+            \"rope_theta\", kwargs.pop(\"local_rope_theta\", self.default_theta[\"local\"])\n+        )\n+\n+        # Standardize and validate the correctness of rotary position embeddings parameters\n+        self.standardize_rope_params()\n+        self.validate_rope(ignore_keys=ignore_keys_at_rope_validation)\n+        return kwargs\n \n \n class ModernBertDecoderEmbeddings(ModernBertEmbeddings):"
        },
        {
            "sha": "ddc6b3c2ba8b49ffefc625e30a3ce1f9b6e4cd53",
            "filename": "src/transformers/models/moonshine/configuration_moonshine.py",
            "status": "modified",
            "additions": 4,
            "deletions": 13,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fconfiguration_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fconfiguration_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fconfiguration_moonshine.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -21,7 +21,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class MoonshineConfig(PreTrainedConfig):\n@@ -87,8 +87,6 @@ class MoonshineConfig(PreTrainedConfig):\n             Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n-        partial_rotary_factor (`float`, *optional*, defaults to 0.9):\n-            Percentage of the query and keys which will have rotary embedding.\n         is_encoder_decoder (`bool`, *optional*, defaults to `True`):\n             Whether the model is used as an encoder/decoder or not.\n         attention_bias (`bool`, *optional*, defaults to `False`):\n@@ -142,7 +140,6 @@ def __init__(\n         decoder_start_token_id: Optional[int] = 1,\n         use_cache: Optional[bool] = True,\n         rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n-        partial_rotary_factor: Optional[float] = 0.9,\n         is_encoder_decoder: Optional[bool] = True,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n@@ -174,18 +171,12 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.decoder_start_token_id = decoder_start_token_id\n         self.use_cache = use_cache\n-        self.partial_rotary_factor = partial_rotary_factor\n         self.is_encoder_decoder = is_encoder_decoder\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n+        kwargs.setdefault(\"partial_rotary_factor\", 0.9)  # assign default for BC\n+\n         super().__init__(\n             bos_token_id=bos_token_id,\n             eos_token_id=eos_token_id,"
        },
        {
            "sha": "2bd7f032d4a4bdf6cdf744359dd94c020f6093a7",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -118,7 +118,7 @@ def compute_default_rope_parameters(\n             post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n         \"\"\"\n         base = config.rope_parameters[\"rope_theta\"]\n-        partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+        partial_rotary_factor = config.rope_parameters.get(\"partial_rotary_factor\", 1.0)\n         head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n         dim = int(head_dim * partial_rotary_factor)\n "
        },
        {
            "sha": "717012fd1fc235b51cdb8fdc032b105a2e1bad67",
            "filename": "src/transformers/models/moonshine/modular_moonshine.py",
            "status": "modified",
            "additions": 4,
            "deletions": 13,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -35,7 +35,7 @@\n     Seq2SeqLMOutput,\n     Seq2SeqModelOutput,\n )\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n@@ -110,8 +110,6 @@ class MoonshineConfig(PreTrainedConfig):\n             Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n-        partial_rotary_factor (`float`, *optional*, defaults to 0.9):\n-            Percentage of the query and keys which will have rotary embedding.\n         is_encoder_decoder (`bool`, *optional*, defaults to `True`):\n             Whether the model is used as an encoder/decoder or not.\n         attention_bias (`bool`, *optional*, defaults to `False`):\n@@ -165,7 +163,6 @@ def __init__(\n         decoder_start_token_id: Optional[int] = 1,\n         use_cache: Optional[bool] = True,\n         rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n-        partial_rotary_factor: Optional[float] = 0.9,\n         is_encoder_decoder: Optional[bool] = True,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n@@ -197,18 +194,12 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.decoder_start_token_id = decoder_start_token_id\n         self.use_cache = use_cache\n-        self.partial_rotary_factor = partial_rotary_factor\n         self.is_encoder_decoder = is_encoder_decoder\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n+        kwargs.setdefault(\"partial_rotary_factor\", 0.9)  # assign default for BC\n+\n         super().__init__(\n             bos_token_id=bos_token_id,\n             eos_token_id=eos_token_id,"
        },
        {
            "sha": "f17dd6dcc14b972b1acce98890cc5d838df37f19",
            "filename": "src/transformers/models/moshi/configuration_moshi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fmoshi%2Fconfiguration_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fmoshi%2Fconfiguration_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fconfiguration_moshi.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -17,7 +17,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n from ..auto.configuration_auto import AutoConfig\n \n@@ -282,14 +282,7 @@ def __init__(\n         self.ffn_dim = ffn_dim\n         self.rms_norm_eps = rms_norm_eps\n         self.num_codebooks = num_codebooks\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         audio_encoder_config = kwargs.pop(\"audio_encoder_config\", {})\n         audio_encoder_model_type = audio_encoder_config.pop(\"model_type\", \"mimi\")"
        },
        {
            "sha": "e690e26fe7f54d87b6e5b9ae8036e1ea7013a2e1",
            "filename": "src/transformers/models/nanochat/configuration_nanochat.py",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fnanochat%2Fconfiguration_nanochat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fnanochat%2Fconfiguration_nanochat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnanochat%2Fconfiguration_nanochat.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \n from ...configuration_utils import PretrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class NanoChatConfig(PretrainedConfig):\n@@ -144,6 +144,7 @@ def __init__(\n         self.use_cache = use_cache\n         self.final_logit_softcapping = final_logit_softcapping\n         self.attention_bias = attention_bias\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             bos_token_id=bos_token_id,\n@@ -153,12 +154,5 @@ def __init__(\n             **kwargs,\n         )\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        # Must be done after super().__init__() to avoid being overridden by kwargs\n-        self.rope_parameters = rope_parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n-\n \n __all__ = [\"NanoChatConfig\"]"
        },
        {
            "sha": "083efe87beed8613a30a0e53b15f2d997ce3eaa7",
            "filename": "src/transformers/models/nemotron/configuration_nemotron.py",
            "status": "modified",
            "additions": 3,
            "deletions": 12,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fnemotron%2Fconfiguration_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fnemotron%2Fconfiguration_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fconfiguration_nemotron.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -18,7 +18,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -80,7 +80,6 @@ class NemotronConfig(PreTrainedConfig):\n             Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n-        partial_rotary_factor (`float`, *optional*, defaults to 0.5): Percentage of the query and keys which will have rotary embedding.\n         attention_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -123,7 +122,6 @@ def __init__(\n         eos_token_id: Optional[int] = 3,\n         tie_word_embeddings: Optional[bool] = False,\n         rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n-        partial_rotary_factor: Optional[float] = 0.5,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         mlp_bias: Optional[bool] = False,\n@@ -141,18 +139,11 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.norm_eps = norm_eps\n         self.use_cache = use_cache\n-        self.partial_rotary_factor = partial_rotary_factor\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.mlp_bias = mlp_bias\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n+        kwargs.setdefault(\"partial_rotary_factor\", 0.5)  # assign default for BC\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "2b9b19a52c1b2329dc1e7409524d9b734a068bb3",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -132,7 +132,7 @@ def compute_default_rope_parameters(\n             post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n         \"\"\"\n         base = config.rope_parameters[\"rope_theta\"]\n-        partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+        partial_rotary_factor = config.rope_parameters.get(\"partial_rotary_factor\", 1.0)\n         head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n         dim = int(head_dim * partial_rotary_factor)\n \n@@ -250,7 +250,7 @@ def __init__(self, config: NemotronConfig, layer_idx: Optional[int] = None):\n         self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n         self.max_position_embeddings = config.max_position_embeddings\n \n-        self.partial_rotary_factor = config.partial_rotary_factor\n+        self.partial_rotary_factor = config.rope_parameters[\"partial_rotary_factor\"]\n         self.is_causal = True\n         self.rotary_emb = NemotronRotaryEmbedding(config=config)\n "
        },
        {
            "sha": "d72c6ee4d1638a81fb152aa29bb30297ccfd12bb",
            "filename": "src/transformers/models/olmo/configuration_olmo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Folmo%2Fconfiguration_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Folmo%2Fconfiguration_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fconfiguration_olmo.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -22,7 +22,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -158,14 +158,7 @@ def __init__(\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.clip_qkv = clip_qkv\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "d5a60ea0248475594b6c1a364d7cbd977da57985",
            "filename": "src/transformers/models/olmo2/configuration_olmo2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Folmo2%2Fconfiguration_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Folmo2%2Fconfiguration_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fconfiguration_olmo2.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -27,7 +27,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class Olmo2Config(PreTrainedConfig):\n@@ -158,14 +158,7 @@ def __init__(\n         self.use_cache = use_cache\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "5bd057c477a2b8d9c1acdf7c3c304f464735ba92",
            "filename": "src/transformers/models/olmo3/configuration_olmo3.py",
            "status": "modified",
            "additions": 10,
            "deletions": 16,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Folmo3%2Fconfiguration_olmo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Folmo3%2Fconfiguration_olmo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo3%2Fconfiguration_olmo3.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -22,7 +22,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class Olmo3Config(PreTrainedConfig):\n@@ -143,13 +143,6 @@ def __init__(\n         layer_types: Optional[list[str]] = None,\n         **kwargs,\n     ):\n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -168,10 +161,6 @@ def __init__(\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.rms_norm_eps = rms_norm_eps\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n         self.sliding_window = sliding_window\n         self.layer_types = layer_types\n         if self.layer_types is None:\n@@ -180,10 +169,15 @@ def __init__(\n             ]\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = getattr(self, \"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n+\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n \n \n __all__ = [\"Olmo3Config\"]"
        },
        {
            "sha": "488280bb7c559530e1510009a1489adcf9fc2a82",
            "filename": "src/transformers/models/olmo3/modular_olmo3.py",
            "status": "modified",
            "additions": 10,
            "deletions": 16,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodular_olmo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodular_olmo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodular_olmo3.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -25,7 +25,7 @@\n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_outputs import BaseModelOutputWithPast\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ..gemma2.modeling_gemma2 import Gemma2RotaryEmbedding\n@@ -159,13 +159,6 @@ def __init__(\n         layer_types: Optional[list[str]] = None,\n         **kwargs,\n     ):\n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -184,10 +177,6 @@ def __init__(\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.rms_norm_eps = rms_norm_eps\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n         self.sliding_window = sliding_window\n         self.layer_types = layer_types\n         if self.layer_types is None:\n@@ -196,10 +185,15 @@ def __init__(\n             ]\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = getattr(self, \"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n+\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n \n \n class Olmo3RMSNorm(Olmo2RMSNorm):"
        },
        {
            "sha": "38316e05d3647dfd9eb070c611b3bc4251340470",
            "filename": "src/transformers/models/olmoe/configuration_olmoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Folmoe%2Fconfiguration_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Folmoe%2Fconfiguration_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fconfiguration_olmoe.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -14,7 +14,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class OlmoeConfig(PreTrainedConfig):\n@@ -158,14 +158,7 @@ def __init__(\n         self.output_router_logits = output_router_logits\n         self.router_aux_loss_coef = router_aux_loss_coef\n         self.norm_topk_prob = norm_topk_prob\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "a11b1658811ef8a5108e772d18dbd8767c974ea9",
            "filename": "src/transformers/models/persimmon/configuration_persimmon.py",
            "status": "modified",
            "additions": 3,
            "deletions": 13,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fconfiguration_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fconfiguration_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fconfiguration_persimmon.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -17,7 +17,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -70,8 +70,6 @@ class PersimmonConfig(PreTrainedConfig):\n             The dropout ratio after applying the MLP to the hidden states.\n         attention_dropout (`float`, *optional*, default to 0.0):\n             The dropout ratio after computing the attention scores.\n-        partial_rotary_factor (`float`, *optional*, default to 0.5):\n-            Percentage of the query and keys which will have rotary embedding.\n \n         Example:\n \n@@ -102,7 +100,6 @@ def __init__(\n         qk_layernorm: Optional[bool] = True,\n         hidden_dropout: Optional[float] = 0.0,\n         attention_dropout: Optional[float] = 0.0,\n-        partial_rotary_factor: Optional[float] = 0.5,\n         pad_token_id: Optional[int] = None,\n         bos_token_id: Optional[int] = 1,\n         eos_token_id: Optional[int] = 2,\n@@ -121,15 +118,8 @@ def __init__(\n         self.qk_layernorm = qk_layernorm\n         self.hidden_dropout = hidden_dropout\n         self.attention_dropout = attention_dropout\n-        self.partial_rotary_factor = partial_rotary_factor\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 25000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n+        kwargs.setdefault(\"partial_rotary_factor\", 0.5)  # assign default for BC\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "a7054a2bd989ca661d9864f0dd1a7df97f91faff",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -99,7 +99,7 @@ def compute_default_rope_parameters(\n             post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n         \"\"\"\n         base = config.rope_parameters[\"rope_theta\"]\n-        partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+        partial_rotary_factor = config.rope_parameters.get(\"partial_rotary_factor\", 1.0)\n         head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n         dim = int(head_dim * partial_rotary_factor)\n \n@@ -219,7 +219,7 @@ def __init__(self, config: PersimmonConfig, layer_idx: Optional[int] = None):\n         self.num_heads = config.num_attention_heads\n         self.head_dim = self.hidden_size // self.num_heads\n \n-        self.rotary_ndims = int(self.head_dim * config.partial_rotary_factor)\n+        self.rotary_ndims = int(self.head_dim * config.rope_parameters[\"partial_rotary_factor\"])\n         self.is_causal = True\n \n         if (self.head_dim * self.num_heads) != self.hidden_size:"
        },
        {
            "sha": "5af2c3220c2f0cbe497ae26c12088f6e2a353ce4",
            "filename": "src/transformers/models/phi/configuration_phi.py",
            "status": "modified",
            "additions": 3,
            "deletions": 13,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fphi%2Fconfiguration_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fphi%2Fconfiguration_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fconfiguration_phi.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -18,7 +18,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -79,8 +79,6 @@ class PhiConfig(PreTrainedConfig):\n             Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n-        partial_rotary_factor (`float`, *optional*, defaults to 0.5):\n-            Percentage of the query and keys which will have rotary embedding.\n         qk_layernorm (`bool`, *optional*, defaults to `False`):\n             Whether or not to normalize the Queries and Keys after projecting the hidden states.\n         bos_token_id (`int`, *optional*, defaults to 1):\n@@ -138,7 +136,6 @@ def __init__(\n         use_cache: Optional[bool] = True,\n         tie_word_embeddings: Optional[bool] = False,\n         rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n-        partial_rotary_factor: Optional[float] = 0.5,\n         qk_layernorm: Optional[bool] = False,\n         bos_token_id: Optional[int] = 1,\n         eos_token_id: Optional[int] = 2,\n@@ -162,16 +159,9 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.layer_norm_eps = layer_norm_eps\n         self.use_cache = use_cache\n-        self.partial_rotary_factor = partial_rotary_factor\n         self.qk_layernorm = qk_layernorm\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n+        kwargs.setdefault(\"partial_rotary_factor\", 0.5)  # assign default for BC\n \n         super().__init__(\n             bos_token_id=bos_token_id,"
        },
        {
            "sha": "196b66df0e3ad5c911935c28cdbd06fe00ef4693",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -70,7 +70,7 @@ def compute_default_rope_parameters(\n             post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n         \"\"\"\n         base = config.rope_parameters[\"rope_theta\"]\n-        partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+        partial_rotary_factor = config.rope_parameters.get(\"partial_rotary_factor\", 1.0)\n         head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n         dim = int(head_dim * partial_rotary_factor)\n \n@@ -186,7 +186,7 @@ def __init__(self, config: PhiConfig, layer_idx: int):\n         self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)\n         self.dense = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=True)\n-        self.rotary_ndims = int(self.head_dim * config.partial_rotary_factor)\n+        self.rotary_ndims = int(self.head_dim * config.rope_parameters[\"partial_rotary_factor\"])\n         self.qk_layernorm = config.qk_layernorm\n         if self.qk_layernorm:\n             self.q_layernorm = nn.LayerNorm("
        },
        {
            "sha": "75e52d93409760c6a5a2ded6d741254e80fe5ff4",
            "filename": "src/transformers/models/phi/modular_phi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -54,7 +54,7 @@ def compute_default_rope_parameters(\n             post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n         \"\"\"\n         base = config.rope_parameters[\"rope_theta\"]\n-        partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+        partial_rotary_factor = config.rope_parameters.get(\"partial_rotary_factor\", 1.0)\n         head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n         dim = int(head_dim * partial_rotary_factor)\n \n@@ -75,7 +75,7 @@ def __init__(self, config: PhiConfig, layer_idx: int):\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)\n         self.dense = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=True)\n         del self.o_proj\n-        self.rotary_ndims = int(self.head_dim * config.partial_rotary_factor)\n+        self.rotary_ndims = int(self.head_dim * config.rope_parameters[\"partial_rotary_factor\"])\n         self.qk_layernorm = config.qk_layernorm\n         if self.qk_layernorm:\n             self.q_layernorm = nn.LayerNorm("
        },
        {
            "sha": "bcdff40584261602ab0804c4b7e91d8f32b6b7db",
            "filename": "src/transformers/models/phi3/configuration_phi3.py",
            "status": "modified",
            "additions": 24,
            "deletions": 22,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fphi3%2Fconfiguration_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fphi3%2Fconfiguration_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fconfiguration_phi3.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -18,7 +18,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -81,8 +81,6 @@ class Phi3Config(PreTrainedConfig):\n             Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n-        partial_rotary_factor (`float`, *optional*, defaults to 1.0):\n-            Percentage of the query and keys which will have rotary embedding. Must be between 0.0 and 1.0.\n         bos_token_id (`int`, *optional*, defaults to 1):\n             The id of the \"beginning-of-sequence\" token.\n         eos_token_id (`int`, *optional*, defaults to 32000):\n@@ -140,7 +138,6 @@ def __init__(\n         use_cache: Optional[bool] = True,\n         tie_word_embeddings: Optional[bool] = False,\n         rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n-        partial_rotary_factor: Optional[float] = 1.0,\n         bos_token_id: Optional[int] = 1,\n         eos_token_id: Optional[int] = 32000,\n         pad_token_id: Optional[int] = 32000,\n@@ -166,17 +163,8 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.partial_rotary_factor = partial_rotary_factor\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n-        self._rope_parameters_adjustment()\n-        self._rope_parameters_validation()\n+        self.rope_parameters = rope_parameters\n+        kwargs.setdefault(\"partial_rotary_factor\", 1.0)  # assign default for BC\n         self.sliding_window = sliding_window\n \n         super().__init__(\n@@ -187,26 +175,40 @@ def __init__(\n             **kwargs,\n         )\n \n-    def _rope_parameters_adjustment(self):\n-        \"\"\"\n-        Adjust the `type` of the `rope_parameters` configuration for backward compatibility.\n-        \"\"\"\n-        rope_parameters_type = self.rope_parameters.get(\"rope_type\", None)\n+    def convert_rope_params_to_dict(\n+        self, default_theta: int | float = 10_000.0, ignore_keys: Optional[set] = None, **kwargs\n+    ):\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or self.rope_parameters\n+        self.rope_parameters = self.rope_parameters if self.rope_parameters is not None else {}\n+\n+        # Standardize and validate the correctness of rotary position embeddings parameters\n+        self.rope_parameters.setdefault(\"rope_theta\", kwargs.pop(\"rope_theta\", default_theta))\n+        self.rope_parameters.setdefault(\"partial_rotary_factor\", kwargs[\"partial_rotary_factor\"])\n+        self.standardize_rope_params()\n \n         # For backward compatibility if previous version used \"su\" or \"yarn\"\n+        rope_parameters_type = self.rope_parameters.get(\"rope_type\", None)\n         if rope_parameters_type is not None and rope_parameters_type in [\"su\", \"yarn\"]:\n             self.rope_parameters[\"rope_type\"] = \"longrope\"\n+        self.validate_rope(ignore_keys=ignore_keys)\n+        return kwargs\n \n-    def _rope_parameters_validation(self):\n+    def validate_rope(self, ignore_keys: Optional[set] = None):\n         \"\"\"\n         Validate the `rope_parameters` configuration.\n         \"\"\"\n+        super().validate_rope(ignore_keys=ignore_keys)\n+\n+        # Run Phi3 specific validation\n         if not isinstance(self.rope_parameters, dict):\n             raise ValueError(f\"`rope_parameters` must be a dictionary but got {self.rope_parameters}\")\n         rope_parameters_type = self.rope_parameters.get(\"rope_type\", None)\n         rope_parameters_short_factor = self.rope_parameters.get(\"short_factor\", None)\n         rope_parameters_long_factor = self.rope_parameters.get(\"long_factor\", None)\n-        rotary_ndims = int(self.hidden_size // self.num_attention_heads * self.partial_rotary_factor)\n+        rotary_ndims = int(\n+            self.hidden_size // self.num_attention_heads * self.rope_parameters[\"partial_rotary_factor\"]\n+        )\n         if rope_parameters_type not in [\"default\", \"longrope\"]:\n             raise ValueError(f\"`rope_parameters`'s type field must be one of ['longrope'], got {rope_parameters_type}\")\n "
        },
        {
            "sha": "3f98bb1b0042eb7d45eda75a2bc30081ad82bef2",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -104,7 +104,7 @@ def compute_default_rope_parameters(\n             post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n         \"\"\"\n         base = config.rope_parameters[\"rope_theta\"]\n-        partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+        partial_rotary_factor = config.rope_parameters.get(\"partial_rotary_factor\", 1.0)\n         head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n         dim = int(head_dim * partial_rotary_factor)\n "
        },
        {
            "sha": "c06858bea98ab53a4cc146ac5fdfb998337be5be",
            "filename": "src/transformers/models/phi4_multimodal/configuration_phi4_multimodal.py",
            "status": "modified",
            "additions": 24,
            "deletions": 22,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fconfiguration_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fconfiguration_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fconfiguration_phi4_multimodal.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -22,7 +22,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class Phi4MultimodalVisionConfig(PreTrainedConfig):\n@@ -296,8 +296,6 @@ class Phi4MultimodalConfig(PreTrainedConfig):\n             Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n-        partial_rotary_factor (`float`, *optional*, defaults to `1.0`):\n-            Percentage of the query and keys which will have rotary embedding. Must be between 0.0 and 1.0.\n         bos_token_id (`int`, *optional*, defaults to 199999):\n             The id of the \"beginning-of-sequence\" token.\n         eos_token_id (`int` or `list[int]`, *optional*, defaults to `[199999, 200020]`):\n@@ -367,7 +365,6 @@ def __init__(\n         use_cache: Optional[bool] = True,\n         tie_word_embeddings: Optional[bool] = False,\n         rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n-        partial_rotary_factor: Optional[int] = 1,\n         bos_token_id: Optional[int] = 199999,\n         eos_token_id: Optional[list[int]] = [199999, 200020],\n         pad_token_id: Optional[int] = 199999,\n@@ -407,17 +404,8 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.partial_rotary_factor = partial_rotary_factor\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n-        self._rope_parameters_adjustment()\n-        self._rope_parameters_validation()\n+        self.rope_parameters = rope_parameters\n+        kwargs.setdefault(\"partial_rotary_factor\", 1.0)  # assign default for BC\n         self.sliding_window = sliding_window\n \n         super().__init__(\n@@ -428,26 +416,40 @@ def __init__(\n             **kwargs,\n         )\n \n-    def _rope_parameters_adjustment(self):\n-        \"\"\"\n-        Adjust the `type` of the `rope_parameters` configuration for backward compatibility.\n-        \"\"\"\n-        rope_parameters_type = self.rope_parameters.get(\"rope_type\", None)\n+    def convert_rope_params_to_dict(\n+        self, default_theta: int | float = 10_000.0, ignore_keys: Optional[set] = None, **kwargs\n+    ):\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or self.rope_parameters\n+        self.rope_parameters = self.rope_parameters if self.rope_parameters is not None else {}\n+\n+        # Standardize and validate the correctness of rotary position embeddings parameters\n+        self.rope_parameters.setdefault(\"rope_theta\", kwargs.pop(\"rope_theta\", default_theta))\n+        self.rope_parameters.setdefault(\"partial_rotary_factor\", kwargs[\"partial_rotary_factor\"])\n+        self.standardize_rope_params()\n \n         # For backward compatibility if previous version used \"su\" or \"yarn\"\n+        rope_parameters_type = self.rope_parameters.get(\"rope_type\", None)\n         if rope_parameters_type is not None and rope_parameters_type in [\"su\", \"yarn\"]:\n             self.rope_parameters[\"rope_type\"] = \"longrope\"\n+        self.validate_rope(ignore_keys=ignore_keys)\n+        return kwargs\n \n-    def _rope_parameters_validation(self):\n+    def validate_rope(self, ignore_keys: Optional[set] = None):\n         \"\"\"\n         Validate the `rope_parameters` configuration.\n         \"\"\"\n+        super().validate_rope(ignore_keys=ignore_keys)\n+\n+        # Run Phi4Multimodal specific validation\n         if not isinstance(self.rope_parameters, dict):\n             raise ValueError(f\"`rope_parameters` must be a dictionary but got {self.rope_parameters}\")\n         rope_parameters_type = self.rope_parameters.get(\"rope_type\", None)\n         rope_parameters_short_factor = self.rope_parameters.get(\"short_factor\", None)\n         rope_parameters_long_factor = self.rope_parameters.get(\"long_factor\", None)\n-        rotary_ndims = int(self.hidden_size // self.num_attention_heads * self.partial_rotary_factor)\n+        rotary_ndims = int(\n+            self.hidden_size // self.num_attention_heads * self.rope_parameters[\"partial_rotary_factor\"]\n+        )\n         if rope_parameters_type not in [\"default\", \"longrope\"]:\n             raise ValueError(f\"`rope_parameters`'s type field must be one of ['longrope'], got {rope_parameters_type}\")\n "
        },
        {
            "sha": "6aab73a09b199e4186be5485179fbdedd836d0d7",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -1481,7 +1481,7 @@ def compute_default_rope_parameters(\n             post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n         \"\"\"\n         base = config.rope_parameters[\"rope_theta\"]\n-        partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+        partial_rotary_factor = config.rope_parameters.get(\"partial_rotary_factor\", 1.0)\n         head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n         dim = int(head_dim * partial_rotary_factor)\n "
        },
        {
            "sha": "5db53409cb766496da168d47aa7827711feabd76",
            "filename": "src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -333,8 +333,6 @@ class Phi4MultimodalConfig(Phi3Config):\n             Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n-        partial_rotary_factor (`float`, *optional*, defaults to `1.0`):\n-            Percentage of the query and keys which will have rotary embedding. Must be between 0.0 and 1.0.\n         bos_token_id (`int`, *optional*, defaults to 199999):\n             The id of the \"beginning-of-sequence\" token.\n         eos_token_id (`int` or `list[int]`, *optional*, defaults to `[199999, 200020]`):\n@@ -390,7 +388,6 @@ def __init__(\n         use_cache: Optional[bool] = True,\n         tie_word_embeddings: Optional[bool] = False,\n         rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n-        partial_rotary_factor: Optional[int] = 1,\n         bos_token_id: Optional[int] = 199999,\n         eos_token_id: Optional[list[int]] = [199999, 200020],\n         pad_token_id: Optional[int] = 199999,\n@@ -429,7 +426,6 @@ def __init__(\n             use_cache=use_cache,\n             tie_word_embeddings=tie_word_embeddings,\n             rope_parameters=rope_parameters,\n-            partial_rotary_factor=partial_rotary_factor,\n             bos_token_id=bos_token_id,\n             eos_token_id=eos_token_id,\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "9c89b8036827237e24678580cf83769e1def56c6",
            "filename": "src/transformers/models/phimoe/configuration_phimoe.py",
            "status": "modified",
            "additions": 17,
            "deletions": 17,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fphimoe%2Fconfiguration_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fphimoe%2Fconfiguration_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fconfiguration_phimoe.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -18,7 +18,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -110,6 +110,7 @@ class PhimoeConfig(PreTrainedConfig):\n \n     model_type = \"phimoe\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    default_theta = 1000000.0\n \n     def __init__(\n         self,\n@@ -167,14 +168,23 @@ def __init__(\n         self.router_aux_loss_coef = router_aux_loss_coef\n         self.router_jitter_noise = router_jitter_noise\n         self.input_jitter_noise = input_jitter_noise\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n+        self.rope_parameters = rope_parameters\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 1000000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n \n+    def validate_rope(self, ignore_keys=None):\n+        \"\"\"\n+        Validate the `rope_parameters` configuration.\n+        \"\"\"\n+        super().validate_rope(ignore_keys=ignore_keys)\n+\n+        # Run model-specific rope validation\n         if self.rope_parameters[\"rope_type\"] != \"default\":\n             if \"original_max_position_embeddings\" in self.rope_parameters:\n                 self.original_max_position_embeddings = self.rope_parameters[\"original_max_position_embeddings\"]\n@@ -189,15 +199,5 @@ def __init__(\n                     f\"`rope_parameters`'s long_mscale field must be a number, got {rope_parameters_long_mscale}\"\n                 )\n \n-        rope_config_validation(self)\n-\n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )\n-\n \n __all__ = [\"PhimoeConfig\"]"
        },
        {
            "sha": "89586615c4a2ae4a8cdf7d20c1153d8ff262c53d",
            "filename": "src/transformers/models/pixtral/configuration_pixtral.py",
            "status": "modified",
            "additions": 4,
            "deletions": 11,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fpixtral%2Fconfiguration_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fpixtral%2Fconfiguration_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fconfiguration_pixtral.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -16,7 +16,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -90,8 +90,6 @@ def __init__(\n         initializer_range: Optional[float] = 0.02,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n-\n         self.hidden_size = hidden_size\n         self.intermediate_size = intermediate_size\n         self.num_hidden_layers = num_hidden_layers\n@@ -103,14 +101,9 @@ def __init__(\n         self.hidden_act = hidden_act\n         self.head_dim = hidden_size // num_attention_heads\n         self.initializer_range = initializer_range\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n+\n+        super().__init__(**kwargs)\n \n \n __all__ = [\"PixtralVisionConfig\"]"
        },
        {
            "sha": "ffb46ca932aa83e95960cf277cf88f94dd1d03d0",
            "filename": "src/transformers/models/qwen2/configuration_qwen2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fqwen2%2Fconfiguration_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fqwen2%2Fconfiguration_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fconfiguration_qwen2.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -17,7 +17,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -157,9 +157,6 @@ def __init__(\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n         self.attention_dropout = attention_dropout\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n \n         self.layer_types = layer_types\n         if self.layer_types is None:\n@@ -171,10 +168,7 @@ def __init__(\n             ]\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             tie_word_embeddings=tie_word_embeddings,"
        },
        {
            "sha": "6a23e0668083639565a67a1d302cbb73f0b42a59",
            "filename": "src/transformers/models/qwen2_5_omni/configuration_qwen2_5_omni.py",
            "status": "modified",
            "additions": 13,
            "deletions": 29,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -22,7 +22,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -291,6 +291,7 @@ class Qwen2_5OmniTextConfig(PreTrainedConfig):\n \n     model_type = \"qwen2_5_omni_text\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    default_theta = 1000000.0\n \n     # Default tensor parallel plan for base model `Qwen25OmniText`\n     base_model_tp_plan = {\n@@ -330,10 +331,6 @@ def __init__(\n         attention_dropout: Optional[float] = 0.0,\n         **kwargs,\n     ):\n-        super().__init__(\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -354,9 +351,6 @@ def __init__(\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n         self.attention_dropout = attention_dropout\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n \n         self.layer_types = layer_types\n         if self.layer_types is None:\n@@ -368,10 +362,12 @@ def __init__(\n             ]\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = getattr(self, \"rope_theta\", 1000000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self, ignore_keys={\"mrope_section\"})\n+        self.rope_parameters = rope_parameters\n+        super().__init__(\n+            tie_word_embeddings=tie_word_embeddings,\n+            ignore_keys_at_rope_validation={\"mrope\"},\n+            **kwargs,\n+        )\n \n \n class Qwen2_5OmniThinkerConfig(PreTrainedConfig):\n@@ -613,6 +609,7 @@ class Qwen2_5OmniTalkerConfig(PreTrainedConfig):\n     ```\"\"\"\n \n     model_type = \"qwen2_5_omni_talker\"\n+    default_theta = 1000000.0\n     attribute_map = {\n         \"image_token_id\": \"image_token_index\",\n         \"video_token_id\": \"video_token_index\",\n@@ -697,9 +694,6 @@ def __init__(\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n         self.attention_dropout = attention_dropout\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n         self.position_id_per_seconds = position_id_per_seconds  # zf\n         self.seconds_per_chunk = seconds_per_chunk  # zf\n         self.audio_start_token_id = audio_start_token_id  # zf\n@@ -718,12 +712,8 @@ def __init__(\n             ]\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 1000000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n-\n-        super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n+        self.rope_parameters = rope_parameters\n+        super().__init__(tie_word_embeddings=tie_word_embeddings, ignore_keys_at_rope_validation={\"mrope\"}, **kwargs)\n \n \n class Qwen2_5OmniDiTConfig(PreTrainedConfig):\n@@ -822,14 +812,8 @@ def __init__(\n         self.enc_attention_channels = enc_attention_channels\n         self.enc_res2net_scale = enc_res2net_scale\n         self.enc_se_channels = enc_se_channels\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n+\n         super().__init__(**kwargs)\n \n "
        },
        {
            "sha": "44c71c5eb0fa2349ec0ebfc3b7c59f32b24406e1",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 13,
            "deletions": 29,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -45,7 +45,7 @@\n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutput, ModelOutput\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -325,6 +325,7 @@ class Qwen2_5OmniTextConfig(PreTrainedConfig):\n \n     model_type = \"qwen2_5_omni_text\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    default_theta = 1000000.0\n \n     # Default tensor parallel plan for base model `Qwen25OmniText`\n     base_model_tp_plan = {\n@@ -364,10 +365,6 @@ def __init__(\n         attention_dropout: Optional[float] = 0.0,\n         **kwargs,\n     ):\n-        super().__init__(\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -388,9 +385,6 @@ def __init__(\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n         self.attention_dropout = attention_dropout\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n \n         self.layer_types = layer_types\n         if self.layer_types is None:\n@@ -402,10 +396,12 @@ def __init__(\n             ]\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = getattr(self, \"rope_theta\", 1000000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self, ignore_keys={\"mrope_section\"})\n+        self.rope_parameters = rope_parameters\n+        super().__init__(\n+            tie_word_embeddings=tie_word_embeddings,\n+            ignore_keys_at_rope_validation={\"mrope\"},\n+            **kwargs,\n+        )\n \n \n class Qwen2_5OmniThinkerConfig(PreTrainedConfig):\n@@ -647,6 +643,7 @@ class Qwen2_5OmniTalkerConfig(PreTrainedConfig):\n     ```\"\"\"\n \n     model_type = \"qwen2_5_omni_talker\"\n+    default_theta = 1000000.0\n     attribute_map = {\n         \"image_token_id\": \"image_token_index\",\n         \"video_token_id\": \"video_token_index\",\n@@ -731,9 +728,6 @@ def __init__(\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n         self.attention_dropout = attention_dropout\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n         self.position_id_per_seconds = position_id_per_seconds  # zf\n         self.seconds_per_chunk = seconds_per_chunk  # zf\n         self.audio_start_token_id = audio_start_token_id  # zf\n@@ -752,12 +746,8 @@ def __init__(\n             ]\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 1000000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n-\n-        super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n+        self.rope_parameters = rope_parameters\n+        super().__init__(tie_word_embeddings=tie_word_embeddings, ignore_keys_at_rope_validation={\"mrope\"}, **kwargs)\n \n \n class Qwen2_5OmniDiTConfig(PreTrainedConfig):\n@@ -856,14 +846,8 @@ def __init__(\n         self.enc_attention_channels = enc_attention_channels\n         self.enc_res2net_scale = enc_res2net_scale\n         self.enc_se_channels = enc_se_channels\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n+\n         super().__init__(**kwargs)\n \n "
        },
        {
            "sha": "084b4d8c9ce65c9b94c2e11a24bde856e6f8364d",
            "filename": "src/transformers/models/qwen2_5_vl/configuration_qwen2_5_vl.py",
            "status": "modified",
            "additions": 17,
            "deletions": 11,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -28,7 +28,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class Qwen2_5_VLVisionConfig(PreTrainedConfig):\n@@ -151,6 +151,7 @@ class Qwen2_5_VLTextConfig(PreTrainedConfig):\n     model_type = \"qwen2_5_vl_text\"\n     base_config_key = \"text_config\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    default_theta = 1000000.0\n     # Default tensor parallel plan for base model `Qwen2_5_VL`\n     base_model_tp_plan = {\n         \"layers.*.self_attn.q_proj\": \"colwise\",\n@@ -212,9 +213,6 @@ def __init__(\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n         self.attention_dropout = attention_dropout\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n \n         self.layer_types = layer_types\n         if self.layer_types is None:\n@@ -226,21 +224,29 @@ def __init__(\n             ]\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 1000000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        if self.rope_parameters[\"rope_type\"] == \"mrope\":\n-            self.rope_parameters[\"rope_type\"] = \"default\"\n-        rope_config_validation(self, ignore_keys={\"mrope_section\"})\n-\n+        self.rope_parameters = rope_parameters\n         super().__init__(\n             tie_word_embeddings=tie_word_embeddings,\n             bos_token_id=bos_token_id,\n             eos_token_id=eos_token_id,\n             pad_token_id=pad_token_id,\n+            ignore_keys_at_rope_validation={\"mrope\"},\n             **kwargs,\n         )\n \n+    def convert_rope_params_to_dict(self, ignore_keys_at_rope_validation: Optional[set] = None, **kwargs):\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or self.rope_parameters\n+        self.rope_parameters = self.rope_parameters if self.rope_parameters is not None else {}\n+\n+        # Standardize and validate the correctness of rotary position embeddings parameters\n+        self.rope_parameters.setdefault(\"rope_theta\", kwargs.pop(\"rope_theta\", self.default_theta))\n+        if self.rope_parameters.get(\"rope_type\", self.rope_parameters.get(\"type\")) == \"mrope\":\n+            self.rope_parameters[\"rope_type\"] = \"default\"\n+        self.standardize_rope_params()\n+        self.validate_rope(ignore_keys=ignore_keys_at_rope_validation)\n+        return kwargs\n+\n \n class Qwen2_5_VLConfig(PreTrainedConfig):\n     r\"\"\""
        },
        {
            "sha": "567bc0d66dd5c7e1dfba8d5faec04a5f5eeba746",
            "filename": "src/transformers/models/qwen2_moe/configuration_qwen2_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fconfiguration_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fconfiguration_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fconfiguration_qwen2_moe.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -17,7 +17,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -185,9 +185,6 @@ def __init__(\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n         self.attention_dropout = attention_dropout\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n \n         # MoE arguments\n         self.decoder_sparse_step = decoder_sparse_step\n@@ -209,10 +206,7 @@ def __init__(\n             ]\n         layer_type_validation(self.layer_types)\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             tie_word_embeddings=tie_word_embeddings,"
        },
        {
            "sha": "e4578375036f6228031ae8db07e78df2428ed609",
            "filename": "src/transformers/models/qwen2_vl/configuration_qwen2_vl.py",
            "status": "modified",
            "additions": 17,
            "deletions": 11,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -18,7 +18,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -139,6 +139,7 @@ class Qwen2VLTextConfig(PreTrainedConfig):\n     model_type = \"qwen2_vl_text\"\n     base_config_key = \"text_config\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    default_theta = 1000000.0\n     # Default tensor parallel plan for base model `Qwen2VL`\n     base_model_tp_plan = {\n         \"layers.*.self_attn.q_proj\": \"colwise\",\n@@ -200,9 +201,6 @@ def __init__(\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n         self.attention_dropout = attention_dropout\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n \n         self.layer_types = layer_types\n         if self.layer_types is None:\n@@ -214,21 +212,29 @@ def __init__(\n             ]\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 1000000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        if self.rope_parameters[\"rope_type\"] == \"mrope\":\n-            self.rope_parameters[\"rope_type\"] = \"default\"\n-        rope_config_validation(self, ignore_keys={\"mrope_section\"})\n-\n+        self.rope_parameters = rope_parameters\n         super().__init__(\n             tie_word_embeddings=tie_word_embeddings,\n             bos_token_id=bos_token_id,\n             eos_token_id=eos_token_id,\n             pad_token_id=pad_token_id,\n+            ignore_keys_at_rope_validation={\"mrope\"},\n             **kwargs,\n         )\n \n+    def convert_rope_params_to_dict(self, ignore_keys_at_rope_validation: Optional[set] = None, **kwargs):\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or self.rope_parameters\n+        self.rope_parameters = self.rope_parameters if self.rope_parameters is not None else {}\n+\n+        # Standardize and validate the correctness of rotary position embeddings parameters\n+        self.rope_parameters.setdefault(\"rope_theta\", kwargs.pop(\"rope_theta\", self.default_theta))\n+        if self.rope_parameters.get(\"rope_type\", self.rope_parameters.get(\"type\")) == \"mrope\":\n+            self.rope_parameters[\"rope_type\"] = \"default\"\n+        self.standardize_rope_params()\n+        self.validate_rope(ignore_keys=ignore_keys_at_rope_validation)\n+        return kwargs\n+\n \n class Qwen2VLConfig(PreTrainedConfig):\n     r\"\"\""
        },
        {
            "sha": "f3e3caf4061d65f613979de5373132bccb54d505",
            "filename": "src/transformers/models/qwen3/configuration_qwen3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fqwen3%2Fconfiguration_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fqwen3%2Fconfiguration_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fconfiguration_qwen3.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -17,7 +17,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -165,9 +165,6 @@ def __init__(\n         self.use_cache = use_cache\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n \n         self.layer_types = layer_types\n         if self.layer_types is None:\n@@ -179,10 +176,7 @@ def __init__(\n             ]\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             tie_word_embeddings=tie_word_embeddings,"
        },
        {
            "sha": "2d0be9fbff1268114eeb76bac048116827991f09",
            "filename": "src/transformers/models/qwen3_moe/configuration_qwen3_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fconfiguration_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fconfiguration_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fconfiguration_qwen3_moe.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -17,7 +17,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -179,14 +179,7 @@ def __init__(\n         self.use_cache = use_cache\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         # MoE arguments\n         self.decoder_sparse_step = decoder_sparse_step"
        },
        {
            "sha": "b6f8ea322905068d591d29c4f5dab9c8b209a207",
            "filename": "src/transformers/models/qwen3_next/configuration_qwen3_next.py",
            "status": "modified",
            "additions": 4,
            "deletions": 14,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fconfiguration_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fconfiguration_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fconfiguration_qwen3_next.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -17,7 +17,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -71,8 +71,6 @@ class Qwen3NextConfig(PreTrainedConfig):\n             Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n-        partial_rotary_factor (`float`, *optional*, defaults to 0.25):\n-            Percentage of the query and keys which will have rotary embedding.\n         attention_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -166,7 +164,6 @@ def __init__(\n         use_cache: Optional[bool] = True,\n         tie_word_embeddings: Optional[bool] = False,\n         rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n-        partial_rotary_factor: Optional[float] = 0.25,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         head_dim: Optional[int] = 256,\n@@ -187,7 +184,6 @@ def __init__(\n         layer_types: Optional[list[str]] = None,\n         **kwargs,\n     ):\n-        super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -199,13 +195,11 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.partial_rotary_factor = partial_rotary_factor\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.head_dim = head_dim\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n+        self.rope_parameters = rope_parameters\n+        kwargs.setdefault(\"partial_rotary_factor\", 0.25)  # assign default for BC\n \n         self.layer_types = layer_types\n         if self.layer_types is None:\n@@ -216,11 +210,6 @@ def __init__(\n             ]\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = getattr(self, \"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n-\n         # linear attention part\n         self.linear_conv_kernel_dim = linear_conv_kernel_dim\n         self.linear_key_head_dim = linear_key_head_dim\n@@ -238,6 +227,7 @@ def __init__(\n         self.output_router_logits = output_router_logits\n         self.router_aux_loss_coef = router_aux_loss_coef\n         self.mlp_only_layers = mlp_only_layers\n+        super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n \n \n __all__ = [\"Qwen3NextConfig\"]"
        },
        {
            "sha": "d0bf37e64de25238d814d0eea96aa4330a7f219e",
            "filename": "src/transformers/models/qwen3_next/modeling_qwen3_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -213,7 +213,7 @@ def compute_default_rope_parameters(\n             post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n         \"\"\"\n         base = config.rope_parameters[\"rope_theta\"]\n-        partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+        partial_rotary_factor = config.rope_parameters.get(\"partial_rotary_factor\", 1.0)\n         head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n         dim = int(head_dim * partial_rotary_factor)\n "
        },
        {
            "sha": "d4f9d017d2d307d8440405e25ad1fc1d8c4d1e2e",
            "filename": "src/transformers/models/qwen3_next/modular_qwen3_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodular_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodular_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodular_qwen3_next.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -203,7 +203,7 @@ def compute_default_rope_parameters(\n             post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n         \"\"\"\n         base = config.rope_parameters[\"rope_theta\"]\n-        partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+        partial_rotary_factor = config.rope_parameters.get(\"partial_rotary_factor\", 1.0)\n         head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n         dim = int(head_dim * partial_rotary_factor)\n "
        },
        {
            "sha": "43e6250c0fd442453968c813e0665d19f879d80d",
            "filename": "src/transformers/models/qwen3_omni_moe/configuration_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 8,
            "deletions": 34,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fconfiguration_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fconfiguration_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fconfiguration_qwen3_omni_moe.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -22,7 +22,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -261,6 +261,7 @@ class Qwen3OmniMoeTextConfig(PreTrainedConfig):\n \n     model_type = \"qwen3_omni_moe_text\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    default_theta = 1000000.0\n \n     # Default tensor parallel plan for base model `Qwen3OmniMoeText`\n     base_model_tp_plan = {\n@@ -324,14 +325,7 @@ def __init__(\n         self.use_cache = use_cache\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 1000000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         # MoE arguments\n         self.decoder_sparse_step = decoder_sparse_step\n@@ -345,9 +339,9 @@ def __init__(\n \n         super().__init__(\n             tie_word_embeddings=tie_word_embeddings,\n+            ignore_keys_at_rope_validation={\"mrope_section\", \"interleaved\", \"mrope_interleaved\"},\n             **kwargs,\n         )\n-        rope_config_validation(self, ignore_keys={\"mrope_section\", \"interleaved\", \"mrope_interleaved\"})\n \n \n class Qwen3OmniMoeThinkerConfig(PreTrainedConfig):\n@@ -589,9 +583,6 @@ def __init__(\n         self.use_cache = use_cache\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n \n         self.layer_types = layer_types\n         if self.layer_types is None:\n@@ -603,10 +594,7 @@ def __init__(\n             ]\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             tie_word_embeddings=tie_word_embeddings,\n@@ -768,14 +756,7 @@ def __init__(\n         self.use_cache = use_cache\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         # MoE arguments\n         self.decoder_sparse_step = decoder_sparse_step\n@@ -1018,7 +999,6 @@ def __init__(\n         attention_dropout=0.0,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n         self.codebook_size = codebook_size\n         self.hidden_size = hidden_size\n         self.max_position_embeddings = max_position_embeddings\n@@ -1036,15 +1016,9 @@ def __init__(\n         self.upsampling_ratios = upsampling_ratios\n         self.decoder_dim = decoder_dim\n         self.attention_dropout = attention_dropout\n+        self.rope_parameters = rope_parameters\n \n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        super().__init__(**kwargs)\n \n     @property\n     def layer_types(self):"
        },
        {
            "sha": "d061aaf5e321ea5e49df1709a88f31fab54c2bb3",
            "filename": "src/transformers/models/qwen3_omni_moe/modular_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 143,
            "deletions": 43,
            "changes": 186,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -42,7 +42,7 @@\n     MoeCausalLMOutputWithPast,\n     MoeModelOutputWithPast,\n )\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import ProcessorMixin, Unpack\n from ...tokenization_utils_base import TextInput\n@@ -156,7 +156,117 @@ class Qwen3OmniMoeVisionEncoderConfig(Qwen3VLMoeVisionConfig):\n     pass\n \n \n-class Qwen3OmniMoeTextConfig(Qwen3MoeConfig):\n+class Qwen3OmniMoeTextConfig(PreTrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Qwen3OmniMoeTextModel`]. It is used to instantiate a\n+    Qwen3OmniMoeText model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of [Qwen/Qwen3-15B-A2B](https://huggingface.co/Qwen/Qwen3-15B-A2B).\n+\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n+\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 151936):\n+            Vocabulary size of the Qwen3OmniMoeText model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`Qwen3OmniMoeTextModel`]\n+        hidden_size (`int`, *optional*, defaults to 2048):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 6144):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 24):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 32):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        num_key_value_heads (`int`, *optional*, defaults to 4):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details, check out [this\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to `32`.\n+\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 32768):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether the model's input and output word embeddings should be tied.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n+        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        use_sliding_window (`bool`, *optional*, defaults to `False`):\n+            Whether to use sliding window attention.\n+        sliding_window (`int`, *optional*, defaults to 4096):\n+            Sliding window attention (SWA) window size. If not specified, will default to `4096`.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        decoder_sparse_step (`int`, *optional*, defaults to 1):\n+            The frequency of the MoE layer.\n+        moe_intermediate_size (`int`, *optional*, defaults to 768):\n+            Intermediate size of the routed expert.\n+        num_experts_per_tok (`int`, *optional*, defaults to 8):\n+            Number of selected experts.\n+        num_experts (`int`, *optional*, defaults to 128):\n+            Number of routed experts.\n+        norm_topk_prob (`bool`, *optional*, defaults to `False`):\n+            Whether to normalize the topk probabilities.\n+        output_router_logits (`bool`, *optional*, defaults to `False`):\n+            Whether or not the router logits should be returned by the model. Enabling this will also\n+            allow the model to output the auxiliary loss, including load balancing loss and router z-loss.\n+        router_aux_loss_coef (`float`, *optional*, defaults to 0.001):\n+            The aux loss factor for the total loss.\n+        mlp_only_layers (`list[int]`, *optional*, defaults to `[]`):\n+            Indicate which layers use Qwen3OmniMoeTextMLP rather than Qwen3OmniMoeTextSparseMoeBlock\n+            The list contains layer index, from 0 to num_layers-1 if we have num_layers layers\n+            If `mlp_only_layers` is empty, `decoder_sparse_step` is used to determine the sparsity.\n+\n+    ```python\n+    >>> from transformers import Qwen3OmniMoeTextModel, Qwen3OmniMoeTextConfig\n+\n+    >>> # Initializing a Qwen3OmniMoeText style configuration\n+    >>> configuration = Qwen3OmniMoeTextConfig()\n+\n+    >>> # Initializing a model from the Qwen3-15B-A2B\" style configuration\n+    >>> model = Qwen3OmniMoeTextModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"qwen3_omni_moe_text\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+    default_theta = 1000000.0\n+\n+    # Default tensor parallel plan for base model `Qwen3OmniMoeText`\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.experts.gate_up_proj\": \"local_rowwise\",\n+        \"layers.*.mlp.experts.down_proj\": \"local_rowwise\",\n+        \"layers.*.mlp.experts\": \"gather\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n+\n     def __init__(\n         self,\n         vocab_size: Optional[int] = 3584,\n@@ -185,41 +295,38 @@ def __init__(\n         mlp_only_layers: Optional[list[int]] = None,\n         **kwargs,\n     ):\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.sliding_window = sliding_window\n+\n+        self.num_key_value_heads = num_key_value_heads\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+        self.rope_parameters = rope_parameters\n+\n+        # MoE arguments\n+        self.decoder_sparse_step = decoder_sparse_step\n+        self.moe_intermediate_size = moe_intermediate_size\n+        self.num_experts_per_tok = num_experts_per_tok\n+        self.num_experts = num_experts\n+        self.norm_topk_prob = norm_topk_prob\n+        self.output_router_logits = output_router_logits\n+        self.router_aux_loss_coef = router_aux_loss_coef\n+        self.mlp_only_layers = [] if mlp_only_layers is None else mlp_only_layers\n+\n         super().__init__(\n-            vocab_size,\n-            hidden_size,\n-            intermediate_size,\n-            num_hidden_layers,\n-            num_attention_heads,\n-            num_key_value_heads,\n-            hidden_act,\n-            max_position_embeddings,\n-            initializer_range,\n-            rms_norm_eps,\n-            use_cache,\n-            tie_word_embeddings,\n-            rope_parameters,\n-            attention_bias,\n-            False,\n-            sliding_window,\n-            attention_dropout,\n-            decoder_sparse_step,\n-            moe_intermediate_size,\n-            num_experts_per_tok,\n-            num_experts,\n-            norm_topk_prob,\n-            output_router_logits,\n-            router_aux_loss_coef,\n-            mlp_only_layers,\n+            tie_word_embeddings=tie_word_embeddings,\n+            ignore_keys_at_rope_validation={\"mrope_section\", \"interleaved\", \"mrope_interleaved\"},\n             **kwargs,\n         )\n-        del self.use_sliding_window\n-        self.sliding_window = sliding_window\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 1000000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self, ignore_keys={\"mrope_section\", \"interleaved\", \"mrope_interleaved\"})\n \n \n class Qwen3OmniMoeThinkerConfig(Qwen2_5OmniThinkerConfig):\n@@ -653,7 +760,6 @@ def __init__(\n         attention_dropout=0.0,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n         self.codebook_size = codebook_size\n         self.hidden_size = hidden_size\n         self.max_position_embeddings = max_position_embeddings\n@@ -671,15 +777,9 @@ def __init__(\n         self.upsampling_ratios = upsampling_ratios\n         self.decoder_dim = decoder_dim\n         self.attention_dropout = attention_dropout\n+        self.rope_parameters = rope_parameters\n \n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        super().__init__(**kwargs)\n \n     @property\n     def layer_types(self):"
        },
        {
            "sha": "cf6f1736467253f816f5178b6b5476a2a5df9b90",
            "filename": "src/transformers/models/qwen3_vl/configuration_qwen3_vl.py",
            "status": "modified",
            "additions": 8,
            "deletions": 10,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fconfiguration_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fconfiguration_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fconfiguration_qwen3_vl.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -21,7 +21,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class Qwen3VLVisionConfig(PreTrainedConfig):\n@@ -130,6 +130,7 @@ class Qwen3VLTextConfig(PreTrainedConfig):\n \n     model_type = \"qwen3_vl_text\"\n     base_config_key = \"text_config\"\n+    default_theta = 500000.0\n \n     def __init__(\n         self,\n@@ -170,16 +171,13 @@ def __init__(\n         self.use_cache = use_cache\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n+        self.rope_parameters = rope_parameters\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 5000000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self, ignore_keys={\"mrope_section\", \"mrope_interleaved\"})\n-\n-        super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n+        super().__init__(\n+            tie_word_embeddings=tie_word_embeddings,\n+            ignore_keys_at_rope_validation={\"mrope_section\", \"mrope_interleaved\"},\n+            **kwargs,\n+        )\n \n \n class Qwen3VLConfig(PreTrainedConfig):"
        },
        {
            "sha": "82b385c53744974bfe81cc47e5005cb8d686b43a",
            "filename": "src/transformers/models/qwen3_vl/modular_qwen3_vl.py",
            "status": "modified",
            "additions": 8,
            "deletions": 10,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -30,7 +30,7 @@\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast\n-from ...modeling_rope_utils import RopeParameters, dynamic_rope_update, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import ProcessingKwargs, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n@@ -171,6 +171,7 @@ class Qwen3VLTextConfig(PreTrainedConfig):\n \n     model_type = \"qwen3_vl_text\"\n     base_config_key = \"text_config\"\n+    default_theta = 500000.0\n \n     def __init__(\n         self,\n@@ -211,16 +212,13 @@ def __init__(\n         self.use_cache = use_cache\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n+        self.rope_parameters = rope_parameters\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 5000000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self, ignore_keys={\"mrope_section\", \"mrope_interleaved\"})\n-\n-        super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n+        super().__init__(\n+            tie_word_embeddings=tie_word_embeddings,\n+            ignore_keys_at_rope_validation={\"mrope_section\", \"mrope_interleaved\"},\n+            **kwargs,\n+        )\n \n \n class Qwen3VLConfig(PreTrainedConfig):"
        },
        {
            "sha": "bdf9d32c57ccb2531d5e7fad1a902030075f8404",
            "filename": "src/transformers/models/qwen3_vl_moe/configuration_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 8,
            "deletions": 10,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fconfiguration_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fconfiguration_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fconfiguration_qwen3_vl_moe.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -22,7 +22,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class Qwen3VLMoeTextConfig(PreTrainedConfig):\n@@ -106,6 +106,7 @@ class Qwen3VLMoeTextConfig(PreTrainedConfig):\n     model_type = \"qwen3_vl_moe_text\"\n     base_config_key = \"text_config\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    default_theta = 500000.0\n     # Default tensor parallel plan for base model `Qwen3VLMoe`\n     base_model_tp_plan = {\n         \"layers.*.self_attn.q_proj\": \"colwise\",\n@@ -166,14 +167,7 @@ def __init__(\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.head_dim = head_dim or hidden_size // num_attention_heads\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 5000000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self, ignore_keys={\"mrope_section\", \"mrope_interleaved\"})\n+        self.rope_parameters = rope_parameters\n \n         # MoE arguments\n         self.decoder_sparse_step = decoder_sparse_step\n@@ -182,7 +176,11 @@ def __init__(\n         self.num_experts = num_experts\n         self.mlp_only_layers = [] if mlp_only_layers is None else mlp_only_layers\n \n-        super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n+        super().__init__(\n+            tie_word_embeddings=tie_word_embeddings,\n+            ignore_keys_at_rope_validation={\"mrope_section\", \"mrope_interleaved\"},\n+            **kwargs,\n+        )\n \n \n class Qwen3VLMoeVisionConfig(PreTrainedConfig):"
        },
        {
            "sha": "1186b8433cf40a3b5d3c8468e52a1eda143c59ad",
            "filename": "src/transformers/models/qwen3_vl_moe/modular_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 8,
            "deletions": 10,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodular_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodular_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodular_qwen3_vl_moe.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -23,7 +23,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n@@ -128,6 +128,7 @@ class Qwen3VLMoeTextConfig(PreTrainedConfig):\n     model_type = \"qwen3_vl_moe_text\"\n     base_config_key = \"text_config\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    default_theta = 500000.0\n     # Default tensor parallel plan for base model `Qwen3VLMoe`\n     base_model_tp_plan = {\n         \"layers.*.self_attn.q_proj\": \"colwise\",\n@@ -188,14 +189,7 @@ def __init__(\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.head_dim = head_dim or hidden_size // num_attention_heads\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 5000000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self, ignore_keys={\"mrope_section\", \"mrope_interleaved\"})\n+        self.rope_parameters = rope_parameters\n \n         # MoE arguments\n         self.decoder_sparse_step = decoder_sparse_step\n@@ -204,7 +198,11 @@ def __init__(\n         self.num_experts = num_experts\n         self.mlp_only_layers = [] if mlp_only_layers is None else mlp_only_layers\n \n-        super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n+        super().__init__(\n+            tie_word_embeddings=tie_word_embeddings,\n+            ignore_keys_at_rope_validation={\"mrope_section\", \"mrope_interleaved\"},\n+            **kwargs,\n+        )\n \n \n class Qwen3VLMoeVisionConfig(Qwen3VLVisionConfig):"
        },
        {
            "sha": "ce5c632104789648aa9d0b84da02454ae839d861",
            "filename": "src/transformers/models/recurrent_gemma/configuration_recurrent_gemma.py",
            "status": "modified",
            "additions": 3,
            "deletions": 13,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fconfiguration_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fconfiguration_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fconfiguration_recurrent_gemma.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -17,7 +17,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -75,8 +75,6 @@ class RecurrentGemmaConfig(PreTrainedConfig):\n             Beginning of stream token id.\n         hidden_activation (``str` or `function``, *optional*, defaults to `\"gelu_pytorch_tanh\"`):\n             The hidden activation used in the recurrent block as well as the MLP layer of the decoder layers.\n-        partial_rotary_factor (`float`, *optional*, defaults to 0.5):\n-            The partial rotary factor used in the initialization of the rotary embeddings.\n         rope_parameters (`RopeParameters`, *optional*):\n             Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n@@ -119,7 +117,6 @@ def __init__(\n         eos_token_id: Optional[int] = 1,\n         bos_token_id: Optional[int] = 2,\n         hidden_activation: Optional[str] = \"gelu_pytorch_tanh\",\n-        partial_rotary_factor: Optional[float] = 0.5,\n         rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         block_types: Optional[list[str]] = (\"recurrent\", \"recurrent\", \"attention\"),\n         attention_dropout: Optional[float] = 0.0,\n@@ -139,7 +136,6 @@ def __init__(\n         self.logits_soft_cap = logits_soft_cap\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.partial_rotary_factor = partial_rotary_factor\n         self.block_types = list(block_types)\n         self.hidden_activation = hidden_activation\n         self.head_dim = self.hidden_size // self.num_attention_heads\n@@ -150,14 +146,8 @@ def __init__(\n         self.attention_bias = attention_bias\n         self.w_init_variance_scale = w_init_variance_scale\n         self.final_w_init_variance_scale = 2.0 / self.num_hidden_layers\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n+        kwargs.setdefault(\"partial_rotary_factor\", 0.5)  # assign default for BC\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "8f3061d495a0faad05338ba6bad313d7a86cd72d",
            "filename": "src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -102,7 +102,7 @@ def compute_default_rope_parameters(\n             post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n         \"\"\"\n         base = config.rope_parameters[\"rope_theta\"]\n-        partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+        partial_rotary_factor = config.rope_parameters.get(\"partial_rotary_factor\", 1.0)\n         head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n         dim = int(head_dim * partial_rotary_factor)\n "
        },
        {
            "sha": "63a5c20c28589483477b7ff72f7c9b8ff4bb1e0e",
            "filename": "src/transformers/models/seed_oss/configuration_seed_oss.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fconfiguration_seed_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fconfiguration_seed_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fconfiguration_seed_oss.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -16,7 +16,7 @@\n from typing import Optional\n \n from transformers.configuration_utils import PreTrainedConfig\n-from transformers.modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from transformers.modeling_rope_utils import RopeParameters\n \n \n class SeedOssConfig(PreTrainedConfig):\n@@ -170,14 +170,7 @@ def __init__(\n         self.residual_dropout = residual_dropout\n         self.mlp_bias = mlp_bias\n         self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "03701376dc26da73c6c7d52e7abb5225ade1a79b",
            "filename": "src/transformers/models/smollm3/configuration_smollm3.py",
            "status": "modified",
            "additions": 9,
            "deletions": 11,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fconfiguration_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fconfiguration_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fconfiguration_smollm3.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -22,7 +22,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class SmolLM3Config(PreTrainedConfig):\n@@ -108,6 +108,7 @@ class SmolLM3Config(PreTrainedConfig):\n \n     model_type = \"smollm3\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    default_theta = 2000000.0\n \n     base_model_tp_plan = {\n         \"layers.*.self_attn.q_proj\": \"colwise\",\n@@ -151,12 +152,6 @@ def __init__(\n         mlp_bias: Optional[bool] = False,\n         **kwargs,\n     ):\n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            **kwargs,\n-        )\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.mlp_bias = mlp_bias\n@@ -201,10 +196,13 @@ def __init__(\n         self.layer_types = layer_types\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = getattr(self, \"rope_theta\", 2000000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            **kwargs,\n+        )\n \n \n __all__ = [\"SmolLM3Config\"]"
        },
        {
            "sha": "c72abf8e1b96de6da864474483dbfd1f12721116",
            "filename": "src/transformers/models/smollm3/modular_smollm3.py",
            "status": "modified",
            "additions": 9,
            "deletions": 11,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodular_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodular_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodular_smollm3.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -21,7 +21,7 @@\n from ...cache_utils import Cache\n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import logging\n@@ -125,6 +125,7 @@ class SmolLM3Config(PreTrainedConfig):\n \n     model_type = \"smollm3\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    default_theta = 2000000.0\n \n     base_model_tp_plan = {\n         \"layers.*.self_attn.q_proj\": \"colwise\",\n@@ -168,12 +169,6 @@ def __init__(\n         mlp_bias: Optional[bool] = False,\n         **kwargs,\n     ):\n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            **kwargs,\n-        )\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.mlp_bias = mlp_bias\n@@ -218,10 +213,13 @@ def __init__(\n         self.layer_types = layer_types\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = getattr(self, \"rope_theta\", 2000000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            **kwargs,\n+        )\n \n \n class SmolLM3RotaryEmbedding(Qwen2RotaryEmbedding):"
        },
        {
            "sha": "4e06ef7c4eddf8cd08d09b41b71f91d2625f85b7",
            "filename": "src/transformers/models/stablelm/configuration_stablelm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 13,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fstablelm%2Fconfiguration_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fstablelm%2Fconfiguration_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fconfiguration_stablelm.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -17,7 +17,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -86,8 +86,6 @@ class StableLmConfig(PreTrainedConfig):\n             The dropout ratio after applying the MLP to the hidden states.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n-        partial_rotary_factor (`float`, *optional*, defaults to 0.25):\n-            Percentage of the query and keys which will have rotary embedding.\n         bos_token_id (int, *optional*, defaults to 0):\n             The id of the `BOS` token in the vocabulary.\n         eos_token_id (int, *optional*, defaults to 0):\n@@ -125,7 +123,6 @@ def __init__(\n         use_parallel_residual: Optional[bool] = False,\n         hidden_dropout: Optional[float] = 0.0,\n         attention_dropout: Optional[float] = 0.0,\n-        partial_rotary_factor: Optional[float] = 0.25,\n         bos_token_id: Optional[int] = 0,\n         eos_token_id: Optional[int] = 0,\n         **kwargs,\n@@ -148,15 +145,8 @@ def __init__(\n         self.use_parallel_residual = use_parallel_residual\n         self.hidden_dropout = hidden_dropout\n         self.attention_dropout = attention_dropout\n-        self.partial_rotary_factor = partial_rotary_factor\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n+        kwargs.setdefault(\"partial_rotary_factor\", 0.25)  # assign default for BC\n \n         super().__init__(\n             bos_token_id=bos_token_id,"
        },
        {
            "sha": "27a9f6b47ce10f82067794c621155f348ee6eb0a",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -98,7 +98,7 @@ def compute_default_rope_parameters(\n             post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n         \"\"\"\n         base = config.rope_parameters[\"rope_theta\"]\n-        partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n+        partial_rotary_factor = config.rope_parameters.get(\"partial_rotary_factor\", 1.0)\n         head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n         dim = int(head_dim * partial_rotary_factor)\n \n@@ -254,7 +254,7 @@ def __init__(self, config: StableLmConfig, layer_idx: Optional[int] = None):\n         self.num_key_value_heads = config.num_key_value_heads\n         self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n \n-        self.rotary_ndims = int(self.head_dim * config.partial_rotary_factor)\n+        self.rotary_ndims = int(self.head_dim * config.rope_parameters[\"partial_rotary_factor\"])\n         self.is_causal = True\n         self.scaling = self.head_dim**-0.5\n "
        },
        {
            "sha": "d0fcfea44b420368101a8ec6b2ea5a44e68f369f",
            "filename": "src/transformers/models/starcoder2/configuration_starcoder2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fconfiguration_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fconfiguration_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fconfiguration_starcoder2.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -17,7 +17,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n \n \n@@ -155,14 +155,7 @@ def __init__(\n         self.attention_dropout = attention_dropout\n         self.residual_dropout = residual_dropout\n         self.embedding_dropout = embedding_dropout\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             bos_token_id=bos_token_id,"
        },
        {
            "sha": "93883844a1eb4a5f059b1c86092a4f23fdd01369",
            "filename": "src/transformers/models/t5gemma/configuration_t5gemma.py",
            "status": "modified",
            "additions": 10,
            "deletions": 15,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fconfiguration_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fconfiguration_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fconfiguration_t5gemma.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -22,7 +22,7 @@\n from typing import Any, Optional, Union\n \n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class T5GemmaModuleConfig(PreTrainedConfig):\n@@ -150,13 +150,6 @@ def __init__(\n         attn_logit_softcapping: Optional[float] = 50.0,\n         **kwargs,\n     ):\n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -176,20 +169,22 @@ def __init__(\n         self.final_logit_softcapping = final_logit_softcapping\n         self.attn_logit_softcapping = attn_logit_softcapping\n         self.layer_types = layer_types\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n \n         if self.layer_types is None:\n             self.layer_types = [\n                 \"sliding_attention\" if bool((i + 1) % 2) else \"full_attention\" for i in range(self.num_hidden_layers)\n             ]\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n+\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n \n \n class T5GemmaConfig(PreTrainedConfig):"
        },
        {
            "sha": "a4dc54e3f0289547b37f277757f2baca18018dc5",
            "filename": "src/transformers/models/vaultgemma/configuration_vaultgemma.py",
            "status": "modified",
            "additions": 10,
            "deletions": 15,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fconfiguration_vaultgemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fconfiguration_vaultgemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fconfiguration_vaultgemma.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -22,7 +22,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class VaultGemmaConfig(PreTrainedConfig):\n@@ -150,13 +150,6 @@ def __init__(\n         attn_logit_softcapping: Optional[float] = 50.0,\n         **kwargs,\n     ):\n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -176,20 +169,22 @@ def __init__(\n         self.final_logit_softcapping = final_logit_softcapping\n         self.attn_logit_softcapping = attn_logit_softcapping\n         self.layer_types = layer_types\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n \n         if self.layer_types is None:\n             self.layer_types = [\n                 \"sliding_attention\" if bool((i + 1) % 2) else \"full_attention\" for i in range(self.num_hidden_layers)\n             ]\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n+\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n \n \n __all__ = [\"VaultGemmaConfig\"]"
        },
        {
            "sha": "69cfd2404f0ae6529b8fa9b1dba7bf8122865959",
            "filename": "src/transformers/models/zamba2/configuration_zamba2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 15,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fzamba2%2Fconfiguration_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/src%2Ftransformers%2Fmodels%2Fzamba2%2Fconfiguration_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fconfiguration_zamba2.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -23,7 +23,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class Zamba2Config(PreTrainedConfig):\n@@ -173,12 +173,6 @@ def __init__(\n         use_long_context: Optional[bool] = False,\n         **kwargs,\n     ):\n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            **kwargs,\n-        )\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -195,14 +189,7 @@ def __init__(\n         self.attention_dropout = attention_dropout\n         self.use_mem_rope = use_mem_rope\n         self.use_long_context = use_long_context\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         self.mamba_d_state = mamba_d_state\n         self.mamba_d_conv = mamba_d_conv\n@@ -246,6 +233,12 @@ def __init__(\n         self.num_logits_to_keep = num_logits_to_keep\n         self.hybrid_layer_ids = [index for index, type in enumerate(self.layers_block_type) if type == \"hybrid\"]\n         self.use_mem_eff_path = use_mem_eff_path\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            **kwargs,\n+        )\n \n \n __all__ = [\"Zamba2Config\"]"
        },
        {
            "sha": "cc5095e69ce0d67ae0f3caa7424d50e32d8cd9a7",
            "filename": "tests/causal_lm_tester.py",
            "status": "modified",
            "additions": 44,
            "deletions": 6,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/tests%2Fcausal_lm_tester.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/tests%2Fcausal_lm_tester.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcausal_lm_tester.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -433,19 +433,30 @@ def test_model_rope_scaling_from_config(self, scaling_type):\n         if not _config_supports_rope_scaling(config):\n             self.skipTest(\"This model does not support RoPE scaling\")\n \n+        partial_rotary_factor = config.rope_parameters.get(\"partial_rotary_factor\", 1.0)\n         short_input = ids_tensor([1, 10], config.vocab_size)\n         long_input = ids_tensor([1, int(config.max_position_embeddings * 1.5)], config.vocab_size)\n \n         set_seed(42)  # Fixed seed at init time so the two models get the same random weights\n-        _set_config_rope_params(config, {\"rope_type\": \"default\", \"rope_theta\": 10_000.0})\n+        _set_config_rope_params(\n+            config, {\"rope_type\": \"default\", \"rope_theta\": 10_000.0, \"partial_rotary_factor\": partial_rotary_factor}\n+        )\n         original_model = self.model_tester_class.base_model_class(config)\n         original_model.to(torch_device)\n         original_model.eval()\n         original_short_output = original_model(short_input).last_hidden_state\n         original_long_output = original_model(long_input).last_hidden_state\n \n         set_seed(42)  # Fixed seed at init time so the two models get the same random weights\n-        _set_config_rope_params(config, {\"rope_type\": scaling_type, \"factor\": 10.0, \"rope_theta\": 10_000.0})\n+        _set_config_rope_params(\n+            config,\n+            {\n+                \"rope_type\": scaling_type,\n+                \"factor\": 10.0,\n+                \"rope_theta\": 10_000.0,\n+                \"partial_rotary_factor\": partial_rotary_factor,\n+            },\n+        )\n         scaled_model = self.model_tester_class.base_model_class(config)\n         scaled_model.to(torch_device)\n         scaled_model.eval()\n@@ -485,6 +496,7 @@ def test_model_rope_scaling_frequencies(self):\n \n         scaling_factor = 10\n         short_input_length = 10\n+        partial_rotary_factor = config.rope_parameters.get(\"partial_rotary_factor\", 1.0)\n         long_input_length = int(config.max_position_embeddings * 1.5)\n \n         # Inputs\n@@ -497,7 +509,9 @@ def test_model_rope_scaling_frequencies(self):\n         position_ids_long = position_ids_long.unsqueeze(0)\n \n         # Sanity check original RoPE\n-        _set_config_rope_params(config, {\"rope_type\": \"default\", \"rope_theta\": 10_000.0})\n+        _set_config_rope_params(\n+            config, {\"rope_type\": \"default\", \"rope_theta\": 10_000.0, \"partial_rotary_factor\": partial_rotary_factor}\n+        )\n         original_rope = rope_class(config=config).to(torch_device)\n         original_cos_short, original_sin_short = original_rope(x, position_ids_short)\n         original_cos_long, original_sin_long = original_rope(x, position_ids_long)\n@@ -506,7 +520,15 @@ def test_model_rope_scaling_frequencies(self):\n \n         # Sanity check linear RoPE scaling\n         # New position \"x\" should match original position with index \"x/scaling_factor\"\n-        _set_config_rope_params(config, {\"rope_type\": \"linear\", \"factor\": scaling_factor, \"rope_theta\": 10_000.0})\n+        _set_config_rope_params(\n+            config,\n+            {\n+                \"rope_type\": \"linear\",\n+                \"factor\": scaling_factor,\n+                \"rope_theta\": 10_000.0,\n+                \"partial_rotary_factor\": partial_rotary_factor,\n+            },\n+        )\n         linear_scaling_rope = rope_class(config=config).to(torch_device)\n         linear_cos_short, linear_sin_short = linear_scaling_rope(x, position_ids_short)\n         linear_cos_long, linear_sin_long = linear_scaling_rope(x, position_ids_long)\n@@ -520,7 +542,15 @@ def test_model_rope_scaling_frequencies(self):\n         # Sanity check Dynamic NTK RoPE scaling\n         # Scaling should only be observed after a long input is fed. We can observe that the frequencies increase\n         # with scaling_factor (or that `inv_freq` decreases)\n-        _set_config_rope_params(config, {\"rope_type\": \"dynamic\", \"factor\": scaling_factor, \"rope_theta\": 10_000.0})\n+        _set_config_rope_params(\n+            config,\n+            {\n+                \"rope_type\": \"dynamic\",\n+                \"factor\": scaling_factor,\n+                \"rope_theta\": 10_000.0,\n+                \"partial_rotary_factor\": partial_rotary_factor,\n+            },\n+        )\n         ntk_scaling_rope = rope_class(config=config).to(torch_device)\n         ntk_cos_short, ntk_sin_short = ntk_scaling_rope(x, position_ids_short)\n         ntk_cos_long, ntk_sin_long = ntk_scaling_rope(x, position_ids_long)\n@@ -534,7 +564,15 @@ def test_model_rope_scaling_frequencies(self):\n \n         # Sanity check Yarn RoPE scaling\n         # Scaling should be over the entire input\n-        _set_config_rope_params(config, {\"rope_type\": \"yarn\", \"factor\": scaling_factor, \"rope_theta\": 10_000.0})\n+        _set_config_rope_params(\n+            config,\n+            {\n+                \"rope_type\": \"yarn\",\n+                \"factor\": scaling_factor,\n+                \"rope_theta\": 10_000.0,\n+                \"partial_rotary_factor\": partial_rotary_factor,\n+            },\n+        )\n         yarn_scaling_rope = rope_class(config=config).to(torch_device)\n         yarn_cos_short, yarn_sin_short = yarn_scaling_rope(x, position_ids_short)\n         yarn_cos_long, yarn_sin_long = yarn_scaling_rope(x, position_ids_long)"
        },
        {
            "sha": "730c4967368e78f680146234cdcc9616a11e987a",
            "filename": "tests/utils/test_modeling_rope_utils.py",
            "status": "modified",
            "additions": 15,
            "deletions": 16,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/078ff685d37a8efb4966bd3da322eef92bf875c0/tests%2Futils%2Ftest_modeling_rope_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/078ff685d37a8efb4966bd3da322eef92bf875c0/tests%2Futils%2Ftest_modeling_rope_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_rope_utils.py?ref=078ff685d37a8efb4966bd3da322eef92bf875c0",
            "patch": "@@ -24,7 +24,6 @@\n     import torch\n \n     from transformers import ROPE_INIT_FUNCTIONS\n-    from transformers.modeling_rope_utils import rope_config_validation\n     from transformers.models.llama.modeling_llama import LlamaRotaryEmbedding\n \n \n@@ -35,13 +34,13 @@ def test_rope_validation(self):\n         all_rope_types = ROPE_INIT_FUNCTIONS.keys()\n \n         # The base config is always valid (default RoPE)\n-        rope_config_validation(config)\n+        config.validate_rope()\n \n         # If we explicitly set the other RoPE types, then validation should fail\n         for rope_type in all_rope_types:\n             config.rope_parameters = {\"rope_type\": rope_type, \"rope_theta\": 10000.0}\n             with self.assertRaises(KeyError):\n-                rope_config_validation(config)\n+                config.validate_rope()\n \n         # Parameters are exclusive to their own RoPE type, and should raise an exception if incorrectly passed\n         valid_param_mapping = {\n@@ -60,31 +59,31 @@ def test_rope_validation(self):\n                     continue\n                 else:\n                     with self.assertRaises(KeyError):\n-                        rope_config_validation(config)\n+                        config.validate_rope()\n \n         # Any other parameters passed to RoPE will raise a warning that a particular key is not used\n         # But sometimes we can have model-specific RoPE kwargs and bypass warning with `ignore_keys`\n         model_specific_kwarg = \"mrope_sections\"  # e,g in Qwen2-VL\n \n         config.rope_parameters = {\"rope_type\": \"default\", \"rope_theta\": 10000.0, model_specific_kwarg: True}\n-        rope_config_validation(config, ignore_keys={model_specific_kwarg})\n+        config.validate_rope(ignore_keys={model_specific_kwarg})\n         with self.assertLogs(\"transformers.modeling_rope_utils\", level=\"WARNING\") as logs:\n-            rope_config_validation(config)\n+            config.validate_rope()\n             self.assertEqual(len(logs.output), 1)\n             self.assertIn(model_specific_kwarg, logs.output[0])\n \n         # We can indicate Different RoPE params for each attention type\n         # We can also have only one RoPE params defined for all layer, we don't raise an error\n         # because it is not required to have separate RoPE per layer type\n-        config.layer_types = [\"global_attn\", \"local_attn\"]\n+        config.layer_types = [\"full_attention\", \"sliding_attention\"]\n         config.rope_parameters = {\n-            \"global_attn\": {\"rope_type\": \"default\", \"rope_theta\": 10000},\n-            \"local_attn\": {\"rope_type\": \"linear\", \"rope_theta\": 10000, \"factor\": 2.0},\n+            \"full_attention\": {\"rope_type\": \"default\", \"rope_theta\": 10000},\n+            \"sliding_attention\": {\"rope_type\": \"linear\", \"rope_theta\": 10000, \"factor\": 2.0},\n         }\n-        rope_config_validation(config)\n+        config.validate_rope()\n \n-        config.rope_parameters = config.rope_parameters[\"local_attn\"]\n-        rope_config_validation(config)\n+        config.rope_parameters = config.rope_parameters[\"full_attention\"]\n+        config.validate_rope()\n \n     def test_yarn_original_original_max_position_embeddings_validation(self):\n         \"\"\"Tests that models with no/bad `original_max_position_embeddings` raise a warning\"\"\"\n@@ -100,7 +99,7 @@ def test_yarn_original_original_max_position_embeddings_validation(self):\n         config.rope_parameters = rope_config\n         with self.assertRaises(AssertionError):  # confirm that no warnings are thrown\n             with self.assertLogs(\"transformers.modeling_rope_utils\", level=\"WARNING\") as logs:\n-                rope_config_validation(config)\n+                config.validate_rope()\n \n         # bad rope config, no `original_max_position_embeddings` -> warning\n         rope_config = {\n@@ -110,7 +109,7 @@ def test_yarn_original_original_max_position_embeddings_validation(self):\n         }\n         config.rope_parameters = rope_config\n         with self.assertLogs(\"transformers.modeling_rope_utils\", level=\"WARNING\") as logs:\n-            rope_config_validation(config)\n+            config.validate_rope()\n             self.assertEqual(len(logs.output), 1)\n             self.assertIn(\"is unset\", logs.output[0])\n \n@@ -123,7 +122,7 @@ def test_yarn_original_original_max_position_embeddings_validation(self):\n         }\n         config.rope_parameters = rope_config\n         with self.assertLogs(\"transformers.modeling_rope_utils\", level=\"WARNING\") as logs:\n-            rope_config_validation(config)\n+            config.validate_rope()\n             self.assertEqual(len(logs.output), 1)\n             self.assertIn(\"implicit factor\", logs.output[0])\n \n@@ -373,7 +372,7 @@ def test_longrope_rope_numerically(self):\n             }\n             self.assertEqual(config.rope_parameters.get(\"attention_factor\"), None)\n             # Verify that \"TypeError: '<' not supported between instances of 'NoneType' and 'int'\" is not raised.\n-            rope_config_validation(config)\n+            config.validate_rope()\n \n         # Check 2: seq_len == 0 -> short factor is applied to the default frequencies\n         config.rope_parameters = {"
        }
    ],
    "stats": {
        "total": 3522,
        "additions": 1509,
        "deletions": 2013
    }
}