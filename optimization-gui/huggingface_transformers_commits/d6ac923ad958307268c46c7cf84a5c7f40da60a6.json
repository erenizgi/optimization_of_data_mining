{
    "author": "gmlwns2000",
    "message": "Change default value of `attn_temperature_tuning` (#37501)\n\nfix: change default value of `attn_temperature_tuning`",
    "sha": "d6ac923ad958307268c46c7cf84a5c7f40da60a6",
    "files": [
        {
            "sha": "c4cef4d4ab5528fd50a916b5edaf2f95cb1c9749",
            "filename": "src/transformers/models/llama4/configuration_llama4.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d6ac923ad958307268c46c7cf84a5c7f40da60a6/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d6ac923ad958307268c46c7cf84a5c7f40da60a6/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py?ref=d6ac923ad958307268c46c7cf84a5c7f40da60a6",
            "patch": "@@ -228,7 +228,9 @@ class Llama4TextConfig(PretrainedConfig):\n         no_rope_layer_interval (`int`, *optional*, defaults to 4): TODO\n         attention_chunk_size (`int`, *optional*, defaults to 8192):\n             <TODO>\n-        attn_temperature_tuning (`int`, *optional*, defaults to 4): TODO\n+        attn_temperature_tuning (`bool`, *optional*, defaults to `True`):\n+            Whether to dynamically scale the attention temperature for each query token based on sequence length.\n+            Recommended for long sequences (e.g., >32k tokens) to maintain stable output results.\n         floor_scale (`int`, *optional*, defaults to 8192): TODO\n         attn_scale (`int`, *optional*, defaults to 0.1): TODO\n         cache_implementation (`<fill_type>`, *optional*, defaults to `\"hybrid\"`): <fill_docstring>\n@@ -291,7 +293,7 @@ def __init__(\n         no_rope_layers=None,\n         no_rope_layer_interval=4,\n         attention_chunk_size=8192,\n-        attn_temperature_tuning=4,\n+        attn_temperature_tuning=True,\n         floor_scale=8192,\n         attn_scale=0.1,\n         cache_implementation=\"hybrid\","
        }
    ],
    "stats": {
        "total": 6,
        "additions": 4,
        "deletions": 2
    }
}