{
    "author": "faaany",
    "message": "[docs] use device-agnostic API instead of cuda  (#34913)\n\nadd device-agnostic API\r\n\r\nSigned-off-by: Lin, Fanli <fanli.lin@intel.com>",
    "sha": "6bc0c219c1fa2232bcaf00dc17983fde4c94349b",
    "files": [
        {
            "sha": "525f0d567bcb61191701cd7e0a645aaa7f403c62",
            "filename": "docs/source/en/perplexity.md",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6bc0c219c1fa2232bcaf00dc17983fde4c94349b/docs%2Fsource%2Fen%2Fperplexity.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6bc0c219c1fa2232bcaf00dc17983fde4c94349b/docs%2Fsource%2Fen%2Fperplexity.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperplexity.md?ref=6bc0c219c1fa2232bcaf00dc17983fde4c94349b",
            "patch": "@@ -73,8 +73,9 @@ Let's demonstrate this process with GPT-2.\n \n ```python\n from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n+from accelerate.test_utils.testing import get_backend\n \n-device = \"cuda\"\n+device, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n model_id = \"openai-community/gpt2-large\"\n model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n tokenizer = GPT2TokenizerFast.from_pretrained(model_id)"
        }
    ],
    "stats": {
        "total": 3,
        "additions": 2,
        "deletions": 1
    }
}