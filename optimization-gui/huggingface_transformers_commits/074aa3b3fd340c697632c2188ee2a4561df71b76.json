{
    "author": "yonigozlan",
    "message": "Uniformize kwargs for Idefics/2 processors (#32568)\n\n* Add uniformize idefics processor kwargs and tests\r\n\r\n* Uniformize idefics2 processor kwargs\r\n\r\n* add image_processor tests idefics\r\n\r\n* add BC args order change idefics2 processor and update doc\r\n\r\n* Add support for multiple images per prompt in image-text-to-text mode idefics\r\n\r\n* Fix processor input args in idefics tests\r\n\r\n* improve test processing common, remove unnecessary tests, update process uniformization\r\n\r\n* fix doctrings idefics\r\n\r\n* fix tests processors idefics/2",
    "sha": "074aa3b3fd340c697632c2188ee2a4561df71b76",
    "files": [
        {
            "sha": "3406ab2226e08baf114d46aecb011fec59b65bf4",
            "filename": "src/transformers/models/idefics/processing_idefics.py",
            "status": "modified",
            "additions": 103,
            "deletions": 59,
            "changes": 162,
            "blob_url": "https://github.com/huggingface/transformers/blob/074aa3b3fd340c697632c2188ee2a4561df71b76/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/074aa3b3fd340c697632c2188ee2a4561df71b76/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py?ref=074aa3b3fd340c697632c2188ee2a4561df71b76",
            "patch": "@@ -16,13 +16,21 @@\n Processor class for IDEFICS.\n \"\"\"\n \n-from typing import Callable, List, Optional, Union\n+from typing import Callable, Dict, List, Optional, Union\n from urllib.parse import urlparse\n \n from ...feature_extraction_utils import BatchFeature\n-from ...processing_utils import ProcessorMixin\n-from ...tokenization_utils_base import BatchEncoding, PaddingStrategy, TextInput, TruncationStrategy\n+from ...processing_utils import (\n+    ImagesKwargs,\n+    ProcessingKwargs,\n+    ProcessorMixin,\n+    TextKwargs,\n+    Unpack,\n+    _validate_images_text_input_order,\n+)\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import is_tf_available, is_torch_available\n+from ...utils.deprecation import deprecate_kwarg\n \n \n if is_torch_available():\n@@ -34,6 +42,32 @@\n IMAGE_TOKEN = \"<image>\"\n \n \n+class IdeficsImagesKwargs(ImagesKwargs, total=False):\n+    transform: Optional[Callable]\n+    image_size: Optional[Dict[str, int]]\n+    image_mean: Optional[Union[float, List[float]]]\n+    image_std: Optional[Union[float, List[float]]]\n+\n+\n+class IdeficsTextKwargs(TextKwargs, total=False):\n+    add_eos_token: Optional[bool]\n+    add_end_of_utterance_token: Optional[bool]\n+\n+\n+class IdeficsProcessorKwargs(ProcessingKwargs, total=False):\n+    text_kwargs: IdeficsTextKwargs\n+    images_kwargs: IdeficsImagesKwargs\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"add_special_tokens\": False,\n+            \"padding\": \"longest\",\n+            \"add_eos_token\": False,\n+        },\n+        \"images_kwargs\": {},\n+        \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+    }\n+\n+\n # copied from m4.training.packing\n def incremental_to_binary_attention_mask(incremental_mask, return_tensors, num_classes=-1):\n     # Set elements >= num_classes to -1\n@@ -199,52 +233,32 @@ def __init__(self, image_processor, tokenizer=None, image_size=224, add_end_of_u\n             else False\n         )\n \n+    @deprecate_kwarg(old_name=\"prompts\", version=\"5.0.0\", new_name=\"text\", raise_if_both_names=True)\n     def __call__(\n         self,\n-        prompts: Union[List[TextInput], List[List[TextInput]]],\n-        padding: Union[bool, str, PaddingStrategy] = \"longest\",\n-        truncation: Union[bool, str, TruncationStrategy] = None,\n-        max_length: Optional[int] = None,\n-        transform: Callable = None,\n-        add_eos_token=False,\n-        add_end_of_utterance_token=None,\n-        debug=False,\n-        return_tensors=\"pt\",\n-    ) -> BatchEncoding:\n+        images=None,\n+        text: Union[\n+            TextInput,\n+            PreTokenizedInput,\n+            List[TextInput],\n+            List[PreTokenizedInput],\n+            List[List[TextInput]],\n+            List[List[PreTokenizedInput]],\n+        ] = None,\n+        audio=None,\n+        videos=None,\n+        **kwargs: Unpack[IdeficsProcessorKwargs],\n+    ) -> BatchFeature:\n         \"\"\"This method takes batched or non-batched prompts made of text and images and converts them into prompts that\n         the model was trained on and prepares the image pixel values for the model to process.\n \n         Args:\n-            prompts (`Union[List[TextInput], [List[List[TextInput]]]]`):\n+            images (`Union[PIL.Image, str, List[PIL.Image], List[str]]`):\n+                either a single image or a batched list of images - can be passed in when text contains only text prompts,\n+                in order to use the image-text-to-text behavior.\n+            text (`Union[List[TextInput], [List[List[TextInput]]]]`):\n                 either a single prompt or a batched list of prompts - see the detailed description immediately after\n                 the end of the arguments doc section.\n-            padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `\"longest\"`):\n-                Select a strategy to pad the returned sequences (according to the model's padding side and padding\n-                index) among:\n-                - `True` or `'longest'` (default): Pad to the longest sequence in the batch (or no padding if only a single\n-                  sequence if provided).\n-                - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n-                  acceptable input length for the model if that argument is not provided.\n-                - `False` or `'do_not_pad'`: No padding. This will raise an error if the input sequences are of different\n-                  lengths.\n-                Note: Unlike most processors, which set padding=`False` by default, `IdeficsProcessor` sets `padding=\"longest\"`\n-                  by default. See https://github.com/huggingface/transformers/pull/29449#pullrequestreview-1925576061 for why.\n-            max_length (`int`, *optional*):\n-                Maximum length of the returned list and optionally padding length (see above).\n-            truncation (`bool`, *optional*):\n-                Activates truncation to cut input sequences longer than `max_length` to `max_length`.\n-            transform (`Callable`, *optional*):\n-                A custom transform function that accepts a single image can be passed for training. For example,\n-                `torchvision.Compose` can be used to compose multiple functions. If `None` a preset inference-specific\n-                set of transforms will be applied to the images\n-            add_eos_token (`bool`, *optional*, defaults to `False`):\n-                Adds `eos_token` at the end of the final prompt if True`\n-            add_end_of_utterance_token (`bool`, *optional*)\n-                Whether to automatically add `<end_of_utterance>` after each prompt's text input (unless followed by an\n-                image). If `None` the tokenizer will be checked instead and if this token is found in\n-                `additional_special_tokens` then the value will be `True`.\n-            debug (`bool`, *optional*, defaults to `False`):\n-                `True` value will help debug prompt generation by dumping useful information\n             return_tensors (`str` or `TensorType`, *optional*, defaults to `TensorType.PYTORCH`):\n                 The type of tensors to return. Can be one of:\n                     - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n@@ -255,7 +269,7 @@ def __call__(\n \n         Detailed explanation:\n \n-        Each entry in `prompts` is either a text to be passed as is or an image that will be processed.\n+        Each entry in `text` is either a text to be passed as is or an image that will be processed.\n \n         An image can be either an image object (`PIL.Image`) or a url from which the image can be retrieved.\n \n@@ -279,7 +293,7 @@ def __call__(\n             \"Describe this image.\\nAssistant:\",\n         ]\n \n-        inputs = processor(prompts, return_tensors=\"pt\")\n+        inputs = processor(text=prompts, return_tensors=\"pt\")\n         generated_ids = model.generate(**inputs, max_length=100)\n         generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n         ```\n@@ -311,18 +325,55 @@ def __call__(\n                 transforms.Normalize(mean=self.image_mean, std=self.image_std),\n             ]\n         )\n-        inputs = processor(prompts, transform=image_transform, return_tensors=\"pt\")\n+        inputs = processor(text=prompts, transform=image_transform, return_tensors=\"pt\")\n         ```\n \n         In order to help debug prompt generation enable `debug=True` which will show you what's happening.\n \n         \"\"\"\n+        if images is None and text is None:\n+            raise ValueError(\"You need to specify either `text` or `images` and `text`.\")\n+        # check if images and text inputs are reversed for BC\n+        images, text = _validate_images_text_input_order(images, text)\n+\n+        if images is None:\n+            # assuming the user wants to use the old behavior with prompts as the only argument\n+            prompts = text\n+        elif text is not None:\n+            # Assuming image-text-to-text behavior:\n+            # Check if batched images are provided\n+            if not isinstance(images, (list, tuple)):\n+                images = [images]\n+            if isinstance(text, str):\n+                text = [text]\n+            # Check if batched images and text are in the correct format\n+            if isinstance(text, (list, tuple)) and len(text) != len(images):\n+                raise ValueError(\n+                    \"When providing both images and text arguments, the number of text prompts should be the same as the number of images.\"\n+                    \"If you want to have several images per prompt, images should be nested as such: images=[[img1, img2], [img3, img4], ...] for text=[prompt1, prompt2, ...].\"\n+                )\n+            # Check that only text is present in the prompts\n+            if not all(isinstance(i, str) for i in text):\n+                raise ValueError(\"When using the image-text-to-text behavior, the prompts should only contain text.\")\n+            if isinstance(images[0], (list, tuple)):\n+                # if nested images, nest text as well\n+                text = [[i] for i in text]\n+            prompts = list(zip(images, text))\n+\n+        output_kwargs = self._merge_kwargs(\n+            IdeficsProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+\n+        add_eos_token = output_kwargs[\"text_kwargs\"].pop(\"add_eos_token\", False)\n+        add_end_of_utterance_token = output_kwargs[\"text_kwargs\"].pop(\"add_end_of_utterance_token\", None)\n \n         # if the value isn't overriden by the user, check if the tokenizer was trained with this token and then use it\n         if add_end_of_utterance_token is None:\n             add_end_of_utterance_token = self.tokenizer_was_trained_with_end_of_utterance_token\n         # turn non-batched prompts into batched\n-        if not any(isinstance(i, list) for i in prompts):\n+        if not any(isinstance(i, (list, tuple)) for i in prompts):\n             prompts = [prompts]\n \n         fake_token = \"<fake_token_around_image>\"\n@@ -371,21 +422,14 @@ def image_tokens(last_was_image):\n             if add_eos_token:\n                 full_text += self.tokenizer.eos_token\n \n-            if debug is True:\n-                print(f\"{full_text=}\")\n-\n-            image_objects = self.image_processor(image_objects, transform=transform, return_tensors=return_tensors)\n+            image_objects = self.image_processor(image_objects, **output_kwargs[\"images_kwargs\"])\n \n             all_prompts.append(full_text)\n             all_images.append(image_objects)\n \n-        text_encoding = self.tokenizer(\n-            text=all_prompts,\n-            add_special_tokens=False,\n-            padding=padding,\n-            truncation=truncation,\n-            max_length=max_length,\n-        )\n+        # For BC\n+        return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", \"pt\")\n+        text_encoding = self.tokenizer(all_prompts, **output_kwargs[\"text_kwargs\"])\n         all_texts = text_encoding[\"input_ids\"]\n         all_attention_masks = text_encoding[\"attention_mask\"]\n \n@@ -398,12 +442,12 @@ def image_tokens(last_was_image):\n         output_images = []\n         output_attention_masks = []\n \n-        for text, attention_mask, images in zip(all_texts, all_attention_masks, all_images):\n-            padded_input_ids = text\n+        for text_single, attention_mask, extracted_images in zip(all_texts, all_attention_masks, all_images):\n+            padded_input_ids = text_single\n             image_count = padded_input_ids.count(self.image_token_id)\n             local_max_num_images = min(image_count, max_num_images)\n \n-            current_images = images[:local_max_num_images]\n+            current_images = extracted_images[:local_max_num_images]\n \n             if len(current_images) > 0:\n                 if return_tensors == \"pt\":"
        },
        {
            "sha": "e4079a2368fdb1b2cd72dd8a2a1437f7188948ff",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/074aa3b3fd340c697632c2188ee2a4561df71b76/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/074aa3b3fd340c697632c2188ee2a4561df71b76/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=074aa3b3fd340c697632c2188ee2a4561df71b76",
            "patch": "@@ -1584,7 +1584,7 @@ def forward(\n         ...   \"In which city is that bridge located?<image>\",\n         ... ]\n         >>> images = [[image1, image2], [image3]]\n-        >>> inputs = processor(text=prompts, images=images, padding=True, return_tensors=\"pt\").to(\"cuda\")\n+        >>> inputs = processor(images=images, text=prompts, padding=True, return_tensors=\"pt\").to(\"cuda\")\n \n         >>> # Generate\n         >>> generated_ids = model.generate(**inputs, bad_words_ids=BAD_WORDS_IDS, max_new_tokens=20)"
        },
        {
            "sha": "68566d182678c2d013681c0ff7af034afd31e48b",
            "filename": "src/transformers/models/idefics2/processing_idefics2.py",
            "status": "modified",
            "additions": 49,
            "deletions": 40,
            "changes": 89,
            "blob_url": "https://github.com/huggingface/transformers/blob/074aa3b3fd340c697632c2188ee2a4561df71b76/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/074aa3b3fd340c697632c2188ee2a4561df71b76/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py?ref=074aa3b3fd340c697632c2188ee2a4561df71b76",
            "patch": "@@ -20,9 +20,15 @@\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput, is_valid_image, load_image\n-from ...processing_utils import ProcessorMixin\n-from ...tokenization_utils_base import AddedToken, BatchEncoding, PaddingStrategy, TextInput, TruncationStrategy\n-from ...utils import TensorType, logging\n+from ...processing_utils import (\n+    ImagesKwargs,\n+    ProcessingKwargs,\n+    ProcessorMixin,\n+    Unpack,\n+    _validate_images_text_input_order,\n+)\n+from ...tokenization_utils_base import AddedToken, TextInput\n+from ...utils import logging\n \n \n if TYPE_CHECKING:\n@@ -40,6 +46,23 @@ def is_image_or_image_url(elem):\n     return is_url(elem) or is_valid_image(elem)\n \n \n+class Idefics2ImagesKwargs(ImagesKwargs, total=False):\n+    image_seq_len: Optional[int]\n+\n+\n+class Idefics2ProcessorKwargs(ProcessingKwargs, total=False):\n+    images_kwargs: Idefics2ImagesKwargs\n+\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"add_special_tokens\": True,\n+            \"padding\": False,\n+            \"is_split_into_words\": False,\n+        },\n+        \"images_kwargs\": {},\n+    }\n+\n+\n class Idefics2Processor(ProcessorMixin):\n     r\"\"\"\n     Constructs a IDEFICS2 processor which wraps a LLama tokenizer and IDEFICS2 image processor into a single processor.\n@@ -97,16 +120,12 @@ def _extract_images_from_prompts(self, prompts):\n \n     def __call__(\n         self,\n-        text: Union[TextInput, \"PreTokenizedInput\", List[TextInput], List[\"PreTokenizedInput\"]] = None,\n         images: Union[ImageInput, List[ImageInput], List[List[ImageInput]]] = None,\n-        image_seq_len: Optional[int] = None,\n-        padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy] = None,\n-        max_length: Optional[int] = None,\n-        is_split_into_words: bool = False,\n-        add_special_tokens: bool = True,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-    ) -> BatchEncoding:\n+        text: Union[TextInput, \"PreTokenizedInput\", List[TextInput], List[\"PreTokenizedInput\"]] = None,\n+        audio=None,\n+        videos=None,\n+        **kwargs: Unpack[Idefics2ProcessorKwargs],\n+    ) -> BatchFeature:\n         \"\"\"\n         Processes the input prompts and returns a BatchEncoding.\n \n@@ -130,42 +149,40 @@ def __call__(\n         ...     \"<image>In this image, we see\",\n         ...     \"bla bla bla<image>\",\n         ... ]\n-        >>> outputs = processor(text=text, images=images, return_tensors=\"pt\", padding=True)\n+        >>> outputs = processor(images=images, text=text, return_tensors=\"pt\", padding=True)\n         >>> input_ids = outputs.input_ids\n         >>> input_tokens = processor.tokenizer.batch_decode(input_ids)\n         >>> print(input_tokens)\n         ['<s><fake_token_around_image><image><image><fake_token_around_image> In this image, we see', '<s> bla bla bla<fake_token_around_image><image><image><fake_token_around_image>']\n         ```\n \n         Args:\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`, *optional*):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. If is of type `List[ImageInput]`, it's assumed that this is for a single prompt i.e. of batch size 1.\n             text (`Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]`, *optional*):\n                 The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                 (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                 `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n \n                 Wherever an image token, `<image>` is encountered it is expanded to\n                 `<fake_token_around_image>` + `<image>` * `image_seq_len` * <fake_token_around_image>`.\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`, *optional*):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. If is of type `List[ImageInput]`, it's assumed that this is for a single prompt i.e. of batch size 1.\n-            image_seq_len (`int`, *optional*):\n-                The length of the image sequence. If not provided, the default value is used.\n-            padding (`Union[bool, str, PaddingStrategy]`, *optional*, defaults to `False`):\n-                Padding strategy applied to the input ids. See [`PreTrainedTokenizerFast.pad`] for more information.\n-            truncation (`Union[bool, str, TruncationStrategy]`, *optional*):\n-                Truncation strategy applied to the input ids. See [`PreTrainedTokenizerFast.truncate`] for more information.\n-            max_length (`int`, *optional*):\n-                Maximum length of the returned list and optionally padding/truncation length. See\n-                [`PreTrainedTokenizerFast.__call__`] for more information.\n-            is_split_into_words (`bool`, *optional*, defaults to `False`):\n-                Whether the input text is split into words or not. If set to `True`, the tokenizer will skip the\n-                tokenization process and assume the input is already tokenized.\n-            add_special_tokens (`bool`, *optional*, defaults to `True`):\n-                Whether to add special tokens or not. See [`PreTrainedTokenizerFast.__call__`] for more information.\n             return_tensors (`Union[str, TensorType]`, *optional*):\n                 If set, will return tensors of a particular framework. See [`PreTrainedTokenizerFast.__call__`] for more\n                 information.\n+\n         \"\"\"\n+        if text is None and images is None:\n+            raise ValueError(\"You must provide either `text` or `images`.\")\n+        # check if images and text inputs are reversed for BC\n+        images, text = _validate_images_text_input_order(images, text)\n+\n+        output_kwargs = self._merge_kwargs(\n+            Idefics2ProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+        image_seq_len = output_kwargs[\"images_kwargs\"].pop(\"image_seq_len\", None)\n         image_seq_len = image_seq_len if image_seq_len is not None else self.image_seq_len\n \n         n_images_in_text = []\n@@ -194,15 +211,7 @@ def __call__(\n                 sample = sample.replace(f\"{fake_image_token}{fake_image_token}\", f\"{fake_image_token}\")\n                 prompt_strings.append(sample)\n \n-            text_inputs = self.tokenizer(\n-                text=prompt_strings,\n-                add_special_tokens=add_special_tokens,\n-                padding=padding,\n-                truncation=truncation,\n-                max_length=max_length,\n-                is_split_into_words=is_split_into_words,\n-                return_tensors=return_tensors,\n-            )\n+            text_inputs = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"])\n             inputs.update(text_inputs)\n \n         if images is not None:\n@@ -227,7 +236,7 @@ def __call__(\n \n             # Load images if they are URLs\n             images = [[load_image(im) for im in sample] for sample in images]\n-            image_inputs = self.image_processor(images, return_tensors=return_tensors)\n+            image_inputs = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n             inputs.update(image_inputs)\n \n         return inputs"
        },
        {
            "sha": "a49bce8d878fb4563adf6e50736ced0b044c2cd0",
            "filename": "tests/models/idefics/test_modeling_idefics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/074aa3b3fd340c697632c2188ee2a4561df71b76/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/074aa3b3fd340c697632c2188ee2a4561df71b76/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py?ref=074aa3b3fd340c697632c2188ee2a4561df71b76",
            "patch": "@@ -662,7 +662,7 @@ def test_inference_natural_language_visual_reasoning(self):\n             \"HuggingFaceM4/idefics-9b\", quantization_config=quantization_config, device_map=\"auto\"\n         )\n         processor = self.default_processor\n-        inputs = processor(prompts, return_tensors=\"pt\", padding=\"longest\").to(torch_device)\n+        inputs = processor(text=prompts, return_tensors=\"pt\", padding=\"longest\").to(torch_device)\n         generated_ids = model.generate(**inputs, max_length=100)\n         generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n "
        },
        {
            "sha": "062b578a684ede3b8368520160c2240f83189d2f",
            "filename": "tests/models/idefics/test_processor_idefics.py",
            "status": "modified",
            "additions": 168,
            "deletions": 24,
            "changes": 192,
            "blob_url": "https://github.com/huggingface/transformers/blob/074aa3b3fd340c697632c2188ee2a4561df71b76/tests%2Fmodels%2Fidefics%2Ftest_processor_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/074aa3b3fd340c697632c2188ee2a4561df71b76/tests%2Fmodels%2Fidefics%2Ftest_processor_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_processor_idefics.py?ref=074aa3b3fd340c697632c2188ee2a4561df71b76",
            "patch": "@@ -12,49 +12,57 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+import shutil\n+import tempfile\n+import unittest\n+\n import numpy as np\n \n-from transformers.testing_utils import TestCasePlus, require_torch, require_vision\n+from transformers import (\n+    AutoProcessor,\n+    IdeficsImageProcessor,\n+    IdeficsProcessor,\n+    LlamaTokenizerFast,\n+    PreTrainedTokenizerFast,\n+)\n+from transformers.testing_utils import require_torch, require_vision\n from transformers.utils import is_torch_available, is_vision_available\n \n+from ...test_processing_common import ProcessorTesterMixin\n+\n \n if is_torch_available():\n     import torch\n \n if is_vision_available():\n     from PIL import Image\n \n-    from transformers import (\n-        AutoProcessor,\n-        IdeficsImageProcessor,\n-        IdeficsProcessor,\n-        LlamaTokenizerFast,\n-        PreTrainedTokenizerFast,\n-    )\n-\n \n @require_torch\n @require_vision\n-class IdeficsProcessorTest(TestCasePlus):\n-    def setUp(self):\n-        super().setUp()\n+class IdeficsProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = IdeficsProcessor\n \n-        self.checkpoint_path = self.get_auto_remove_tmp_dir()\n+    def setUp(self):\n+        self.tmpdirname = tempfile.mkdtemp()\n \n         image_processor = IdeficsImageProcessor(return_tensors=\"pt\")\n         tokenizer = LlamaTokenizerFast.from_pretrained(\"HuggingFaceM4/tiny-random-idefics\")\n \n         processor = IdeficsProcessor(image_processor, tokenizer)\n \n-        processor.save_pretrained(self.checkpoint_path)\n+        processor.save_pretrained(self.tmpdirname)\n \n         self.input_keys = [\"pixel_values\", \"input_ids\", \"attention_mask\", \"image_attention_mask\"]\n \n     def get_tokenizer(self, **kwargs):\n-        return AutoProcessor.from_pretrained(self.checkpoint_path, **kwargs).tokenizer\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n \n     def get_image_processor(self, **kwargs):\n-        return AutoProcessor.from_pretrained(self.checkpoint_path, **kwargs).image_processor\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.tmpdirname)\n \n     def prepare_prompts(self):\n         \"\"\"This function prepares a list of PIL images\"\"\"\n@@ -100,13 +108,13 @@ def prepare_prompts(self):\n \n     def test_save_load_pretrained_additional_features(self):\n         processor = IdeficsProcessor(tokenizer=self.get_tokenizer(), image_processor=self.get_image_processor())\n-        processor.save_pretrained(self.checkpoint_path)\n+        processor.save_pretrained(self.tmpdirname)\n \n         tokenizer_add_kwargs = self.get_tokenizer(bos_token=\"(BOS)\", eos_token=\"(EOS)\")\n         image_processor_add_kwargs = self.get_image_processor(do_normalize=False, padding_value=1.0)\n \n         processor = IdeficsProcessor.from_pretrained(\n-            self.checkpoint_path, bos_token=\"(BOS)\", eos_token=\"(EOS)\", do_normalize=False, padding_value=1.0\n+            self.tmpdirname, bos_token=\"(BOS)\", eos_token=\"(EOS)\", do_normalize=False, padding_value=1.0\n         )\n \n         self.assertEqual(processor.tokenizer.get_vocab(), tokenizer_add_kwargs.get_vocab())\n@@ -124,7 +132,7 @@ def test_processor(self):\n         prompts = self.prepare_prompts()\n \n         # test that all prompts succeeded\n-        input_processor = processor(prompts, return_tensors=\"pt\", padding=\"longest\")\n+        input_processor = processor(text=prompts, return_tensors=\"pt\", padding=\"longest\")\n         for key in self.input_keys:\n             assert torch.is_tensor(input_processor[key])\n \n@@ -157,8 +165,8 @@ def test_tokenizer_padding(self):\n         ]\n         prompts = [[prompt] for prompt in self.prepare_prompts()[2]]\n \n-        max_length = processor(prompts, padding=\"max_length\", truncation=True, max_length=20, return_tensors=\"pt\")\n-        longest = processor(prompts, padding=\"longest\", truncation=True, max_length=30, return_tensors=\"pt\")\n+        max_length = processor(text=prompts, padding=\"max_length\", truncation=True, max_length=20, return_tensors=\"pt\")\n+        longest = processor(text=prompts, padding=\"longest\", truncation=True, max_length=30, return_tensors=\"pt\")\n \n         decoded_max_length = processor.tokenizer.decode(max_length[\"input_ids\"][-1])\n         decoded_longest = processor.tokenizer.decode(longest[\"input_ids\"][-1])\n@@ -185,8 +193,8 @@ def test_tokenizer_left_padding(self):\n             ([0] * 10) + ([1] * 10),\n         ]\n         prompts = [[prompt] for prompt in self.prepare_prompts()[2]]\n-        max_length = processor(prompts, padding=\"max_length\", truncation=True, max_length=20)\n-        longest = processor(prompts, padding=\"longest\", truncation=True, max_length=30)\n+        max_length = processor(text=prompts, padding=\"max_length\", truncation=True, max_length=20)\n+        longest = processor(text=prompts, padding=\"longest\", truncation=True, max_length=30)\n \n         decoded_max_length = processor.tokenizer.decode(max_length[\"input_ids\"][-1])\n         decoded_longest = processor.tokenizer.decode(longest[\"input_ids\"][-1])\n@@ -204,7 +212,143 @@ def test_model_input_names(self):\n         processor = IdeficsProcessor(tokenizer=tokenizer, image_processor=image_processor)\n         prompts = self.prepare_prompts()\n \n-        inputs = processor(prompts, padding=\"longest\", return_tensors=\"pt\")\n+        inputs = processor(text=prompts, padding=\"longest\", return_tensors=\"pt\")\n \n         # For now the processor supports only ['pixel_values', 'input_ids', 'attention_mask']\n         self.assertSetEqual(set(inputs.keys()), set(self.input_keys))\n+\n+    # Override the following tests as Idefics image processor does not accept do_rescale and rescale_factor\n+    @require_torch\n+    @require_vision\n+    def test_image_processor_defaults_preserved_by_image_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\", image_size=234)\n+        tokenizer = self.get_component(\"tokenizer\", max_length=117)\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = self.prepare_text_inputs()\n+        image_input = self.prepare_image_inputs()\n+\n+        inputs = processor(text=input_str, images=image_input)\n+        self.assertEqual(len(inputs[\"pixel_values\"][0][0][0]), 234)\n+\n+    @require_torch\n+    @require_vision\n+    def test_kwargs_overrides_default_image_processor_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\", image_size=234)\n+        tokenizer = self.get_component(\"tokenizer\", max_length=117)\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = self.prepare_text_inputs()\n+        image_input = self.prepare_image_inputs()\n+\n+        inputs = processor(text=input_str, images=image_input, image_size=224)\n+        self.assertEqual(len(inputs[\"pixel_values\"][0][0][0]), 224)\n+\n+    @require_torch\n+    @require_vision\n+    def test_unstructured_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = self.prepare_text_inputs()\n+        image_input = self.prepare_image_inputs()\n+        inputs = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            image_size=214,\n+            padding=\"max_length\",\n+            max_length=76,\n+        )\n+\n+        self.assertEqual(inputs[\"pixel_values\"].shape[3], 214)\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+\n+    @require_torch\n+    @require_vision\n+    def test_unstructured_kwargs_batched(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = self.prepare_text_inputs(batch_size=2)\n+        image_input = self.prepare_image_inputs(batch_size=2)\n+        inputs = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            image_size=214,\n+            padding=\"longest\",\n+            max_length=76,\n+        )\n+\n+        self.assertEqual(inputs[\"pixel_values\"].shape[3], 214)\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 8)\n+\n+    @require_torch\n+    @require_vision\n+    def test_structured_kwargs_nested(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = self.prepare_text_inputs()\n+        image_input = self.prepare_image_inputs()\n+\n+        # Define the kwargs for each modality\n+        all_kwargs = {\n+            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+            \"images_kwargs\": {\"image_size\": 214},\n+            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n+        }\n+\n+        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n+        self.skip_processor_without_typed_kwargs(processor)\n+        self.assertEqual(inputs[\"pixel_values\"].shape[3], 214)\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+\n+    @require_torch\n+    @require_vision\n+    def test_structured_kwargs_nested_from_dict(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+        input_str = self.prepare_text_inputs()\n+        image_input = self.prepare_image_inputs()\n+\n+        # Define the kwargs for each modality\n+        all_kwargs = {\n+            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+            \"images_kwargs\": {\"image_size\": 214},\n+            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n+        }\n+\n+        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n+        self.assertEqual(inputs[\"pixel_values\"].shape[3], 214)\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)"
        },
        {
            "sha": "bf713c6fb8cfbbeea2e73542c29b3b7d2ebf9513",
            "filename": "tests/models/idefics2/test_processor_idefics2.py",
            "status": "modified",
            "additions": 87,
            "deletions": 35,
            "changes": 122,
            "blob_url": "https://github.com/huggingface/transformers/blob/074aa3b3fd340c697632c2188ee2a4561df71b76/tests%2Fmodels%2Fidefics2%2Ftest_processor_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/074aa3b3fd340c697632c2188ee2a4561df71b76/tests%2Fmodels%2Fidefics2%2Ftest_processor_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics2%2Ftest_processor_idefics2.py?ref=074aa3b3fd340c697632c2188ee2a4561df71b76",
            "patch": "@@ -13,25 +13,42 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+import shutil\n+import tempfile\n import unittest\n from io import BytesIO\n+from typing import Optional\n \n import requests\n \n from transformers import Idefics2Processor\n from transformers.testing_utils import require_torch, require_vision\n from transformers.utils import is_vision_available\n \n+from ...test_processing_common import ProcessorTesterMixin\n+\n \n if is_vision_available():\n     from PIL import Image\n \n+    from transformers import (\n+        AutoProcessor,\n+        Idefics2Processor,\n+    )\n+\n \n @require_torch\n @require_vision\n-class Idefics2ProcessorTest(unittest.TestCase):\n+class Idefics2ProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = Idefics2Processor\n+\n     def setUp(self):\n-        self.processor = Idefics2Processor.from_pretrained(\"HuggingFaceM4/idefics2-8b\", image_seq_len=2)\n+        self.tmpdirname = tempfile.mkdtemp()\n+\n+        processor = Idefics2Processor.from_pretrained(\"HuggingFaceM4/idefics2-8b\", image_seq_len=2)\n+\n+        processor.save_pretrained(self.tmpdirname)\n+\n         self.image1 = Image.open(\n             BytesIO(\n                 requests.get(\n@@ -49,22 +66,35 @@ def setUp(self):\n                 ).content\n             )\n         )\n-        self.bos_token = self.processor.tokenizer.bos_token\n-        self.image_token = self.processor.image_token.content\n-        self.fake_image_token = self.processor.fake_image_token.content\n+        self.bos_token = processor.tokenizer.bos_token\n+        self.image_token = processor.image_token.content\n+        self.fake_image_token = processor.fake_image_token.content\n+\n+        self.bos_token_id = processor.tokenizer.convert_tokens_to_ids(self.bos_token)\n+        self.image_token_id = processor.tokenizer.convert_tokens_to_ids(self.image_token)\n+        self.fake_image_token_id = processor.tokenizer.convert_tokens_to_ids(self.fake_image_token)\n+        self.image_seq_len = processor.image_seq_len\n+\n+    def get_tokenizer(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n+\n+    def get_image_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n \n-        self.bos_token_id = self.processor.tokenizer.convert_tokens_to_ids(self.bos_token)\n-        self.image_token_id = self.processor.tokenizer.convert_tokens_to_ids(self.image_token)\n-        self.fake_image_token_id = self.processor.tokenizer.convert_tokens_to_ids(self.fake_image_token)\n-        self.image_seq_len = self.processor.image_seq_len\n+    def get_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs)\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.tmpdirname)\n \n     def test_process_interleaved_images_prompts_no_image_splitting(self):\n-        old_image_splitting = self.processor.image_processor.do_image_splitting\n+        tokenizer = self.get_tokenizer()\n+        processor = self.get_processor()\n \n-        self.processor.image_processor.do_image_splitting = False\n+        processor.image_processor.do_image_splitting = False\n \n         # Test that a single image is processed correctly\n-        inputs = self.processor(images=self.image1)\n+        inputs = processor(images=self.image1)\n         self.assertEqual(inputs[\"pixel_values\"].shape, (1, 1, 3, 653, 980))\n         self.assertEqual(inputs[\"pixel_attention_mask\"].shape, (1, 1, 653, 980))\n         # fmt: on\n@@ -73,10 +103,10 @@ def test_process_interleaved_images_prompts_no_image_splitting(self):\n         image_str = \"<image>\"\n         text_str = \"In this image, we see\"\n         text = image_str + text_str\n-        inputs = self.processor(text=text, images=self.image1)\n+        inputs = processor(text=text, images=self.image1)\n \n         # fmt: off\n-        tokenized_sentence = self.processor.tokenizer(text_str, add_special_tokens=False)\n+        tokenized_sentence = tokenizer(text_str, add_special_tokens=False)\n         expected_input_ids = [[self.bos_token_id] + [self.fake_image_token_id] + [self.image_token_id] * self.image_seq_len + [self.fake_image_token_id] + tokenized_sentence[\"input_ids\"]]\n         self.assertEqual(inputs[\"input_ids\"], expected_input_ids)\n         self.assertEqual(inputs[\"attention_mask\"], [[1] * len(expected_input_ids[0])])\n@@ -95,11 +125,11 @@ def test_process_interleaved_images_prompts_no_image_splitting(self):\n         ]\n         images = [[self.image1], [self.image2, self.image3]]\n \n-        inputs = self.processor(text=text, images=images, padding=True)\n+        inputs = processor(text=text, images=images, padding=True)\n \n         # fmt: off\n-        tokenized_sentence_1 = self.processor.tokenizer(text_str_1, add_special_tokens=False)\n-        tokenized_sentence_2 = self.processor.tokenizer(text_str_2, add_special_tokens=False)\n+        tokenized_sentence_1 = tokenizer(text_str_1, add_special_tokens=False)\n+        tokenized_sentence_2 = tokenizer(text_str_2, add_special_tokens=False)\n         expected_input_ids_1 = [self.bos_token_id] + [self.fake_image_token_id] + [self.image_token_id] * self.image_seq_len + [self.fake_image_token_id] + tokenized_sentence_1[\"input_ids\"]\n         expected_input_ids_2 = [self.bos_token_id] + tokenized_sentence_2[\"input_ids\"] + [self.fake_image_token_id] + [self.image_token_id] * self.image_seq_len + [self.fake_image_token_id] + [self.image_token_id] * self.image_seq_len + [self.fake_image_token_id]\n         # Pad the first input to match the second input\n@@ -117,15 +147,13 @@ def test_process_interleaved_images_prompts_no_image_splitting(self):\n         self.assertEqual(inputs['pixel_attention_mask'].shape, (2, 2, 767, 980))\n         # fmt: on\n \n-        self.processor.image_processor.do_image_splitting = old_image_splitting\n-\n     def test_process_interleaved_images_prompts_image_splitting(self):\n-        old_image_splitting = self.processor.image_processor.do_image_splitting\n-\n-        self.processor.image_processor.do_image_splitting = True\n+        processor = self.get_processor()\n+        tokenizer = self.get_tokenizer()\n+        processor.image_processor.do_image_splitting = True\n \n         # Test that a single image is processed correctly\n-        inputs = self.processor(images=self.image1)\n+        inputs = processor(images=self.image1)\n         self.assertEqual(inputs[\"pixel_values\"].shape, (1, 5, 3, 653, 980))\n         self.assertEqual(inputs[\"pixel_attention_mask\"].shape, (1, 5, 653, 980))\n         # fmt: on\n@@ -134,10 +162,10 @@ def test_process_interleaved_images_prompts_image_splitting(self):\n         image_str = \"<image>\"\n         text_str = \"In this image, we see\"\n         text = image_str + text_str\n-        inputs = self.processor(text=text, images=self.image1)\n+        inputs = processor(text=text, images=self.image1)\n \n         # fmt: off\n-        tokenized_sentence = self.processor.tokenizer(text_str, add_special_tokens=False)\n+        tokenized_sentence = tokenizer(text_str, add_special_tokens=False)\n         expected_input_ids = [[self.bos_token_id] + ([self.fake_image_token_id] + [self.image_token_id] * self.image_seq_len) * 5 + [self.fake_image_token_id] + tokenized_sentence[\"input_ids\"]]\n         self.assertEqual(inputs[\"input_ids\"], expected_input_ids)\n         self.assertEqual(inputs[\"attention_mask\"], [[1] * len(expected_input_ids[0])])\n@@ -156,11 +184,11 @@ def test_process_interleaved_images_prompts_image_splitting(self):\n         ]\n         images = [[self.image1], [self.image2, self.image3]]\n \n-        inputs = self.processor(text=text, images=images, padding=True)\n+        inputs = processor(text=text, images=images, padding=True)\n \n         # fmt: off\n-        tokenized_sentence_1 = self.processor.tokenizer(text_str_1, add_special_tokens=False)\n-        tokenized_sentence_2 = self.processor.tokenizer(text_str_2, add_special_tokens=False)\n+        tokenized_sentence_1 = tokenizer(text_str_1, add_special_tokens=False)\n+        tokenized_sentence_2 = tokenizer(text_str_2, add_special_tokens=False)\n         expected_input_ids_1 = [self.bos_token_id] + ([self.fake_image_token_id] + [self.image_token_id] * self.image_seq_len) * 5 + [self.fake_image_token_id] + tokenized_sentence_1[\"input_ids\"]\n         expected_input_ids_2 = [self.bos_token_id] + tokenized_sentence_2[\"input_ids\"] + ([self.fake_image_token_id] + [self.image_token_id] * self.image_seq_len) * 5 + ([self.fake_image_token_id] + [self.image_token_id] * self.image_seq_len) * 5 + [self.fake_image_token_id]\n         # Pad the first input to match the second input\n@@ -178,22 +206,22 @@ def test_process_interleaved_images_prompts_image_splitting(self):\n         self.assertEqual(inputs['pixel_attention_mask'].shape, (2, 10, 767, 980))\n         # fmt: on\n \n-        self.processor.image_processor.do_image_splitting = old_image_splitting\n-\n     def test_add_special_tokens_processor(self):\n+        processor = self.get_processor()\n+        tokenizer = self.get_tokenizer()\n         image_str = \"<image>\"\n         text_str = \"In this image, we see\"\n         text = text_str + image_str\n \n-        n_image_repeat = 5 if self.processor.image_processor.do_image_splitting else 1\n+        n_image_repeat = 5 if processor.image_processor.do_image_splitting else 1\n \n         # fmt: off\n-        inputs = self.processor(text=text, images=self.image1, add_special_tokens=False)\n-        tokenized_sentence = self.processor.tokenizer(text_str, add_special_tokens=False)\n+        inputs = processor(text=text, images=self.image1, add_special_tokens=False)\n+        tokenized_sentence = tokenizer(text_str, add_special_tokens=False)\n         expected_input_ids = [tokenized_sentence[\"input_ids\"] + ([self.fake_image_token_id] + [self.image_token_id] * self.image_seq_len) * n_image_repeat + [self.fake_image_token_id]]\n         self.assertEqual(inputs[\"input_ids\"], expected_input_ids)\n \n-        inputs = self.processor(text=text, images=self.image1)\n+        inputs = processor(text=text, images=self.image1)\n         expected_input_ids = [[self.bos_token_id] + tokenized_sentence[\"input_ids\"] + ([self.fake_image_token_id] + [self.image_token_id] * self.image_seq_len) * n_image_repeat + [self.fake_image_token_id]]\n         self.assertEqual(inputs[\"input_ids\"], expected_input_ids)\n         # fmt: on\n@@ -222,7 +250,7 @@ def test_apply_chat_template(self):\n             {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"And who is that?\"}]},\n         ]\n \n-        processor = self.processor\n+        processor = self.get_processor()\n         # Make short sequence length to test that the fake tokens are added correctly\n         rendered = processor.apply_chat_template(messages, add_generation_prompt=True)\n \n@@ -233,3 +261,27 @@ def test_apply_chat_template(self):\n             \"Assistant:\"\n         )\n         self.assertEqual(rendered, expected_rendered)\n+\n+    # Override as Idefics2Processor needs image tokens in prompts\n+    def prepare_text_inputs(self, batch_size: Optional[int] = None):\n+        if batch_size is None:\n+            return \"lower newer <image>\"\n+\n+        if batch_size < 1:\n+            raise ValueError(\"batch_size must be greater than 0\")\n+\n+        if batch_size == 1:\n+            return [\"lower newer <image>\"]\n+        return [\"lower newer <image>\", \"<image> upper older longer string\"] + [\"<image> lower newer\"] * (\n+            batch_size - 2\n+        )\n+\n+    # Override as PixtralProcessor needs nested images to work properly with batched inputs\n+    @require_vision\n+    def prepare_image_inputs(self, batch_size: Optional[int] = None):\n+        \"\"\"This function prepares a list of PIL images for testing\"\"\"\n+        if batch_size is None:\n+            return super().prepare_image_inputs()\n+        if batch_size < 1:\n+            raise ValueError(\"batch_size must be greater than 0\")\n+        return [[super().prepare_image_inputs()]] * batch_size"
        }
    ],
    "stats": {
        "total": 569,
        "additions": 409,
        "deletions": 160
    }
}