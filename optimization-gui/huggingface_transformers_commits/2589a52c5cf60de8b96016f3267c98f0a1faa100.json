{
    "author": "zucchini-nlp",
    "message": "Fix aria tests (#39879)\n\n* fix aria tests\n\n* awful bug\n\n* fix copies\n\n* fix tests\n\n* fix style\n\n* revert this",
    "sha": "2589a52c5cf60de8b96016f3267c98f0a1faa100",
    "files": [
        {
            "sha": "1959a543881162af9187130ab466e6f8c2d947df",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 25,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/2589a52c5cf60de8b96016f3267c98f0a1faa100/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2589a52c5cf60de8b96016f3267c98f0a1faa100/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=2589a52c5cf60de8b96016f3267c98f0a1faa100",
            "patch": "@@ -1014,18 +1014,9 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[tuple, AriaModelOutputWithPast]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n@@ -1037,7 +1028,7 @@ def forward(\n                 vision_feature_layer=self.config.vision_feature_layer,\n             )\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-            special_image_mask = self._get_image_mask(\n+            special_image_mask = self.get_placeholder_mask(\n                 input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n             )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n@@ -1048,9 +1039,6 @@ def forward(\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=True,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -1156,9 +1144,6 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -1223,12 +1208,6 @@ def forward(\n         >>> print(generated_texts[1])\n         Assistant: The bridge is in San Francisco.\n         ```\"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.model(\n             input_ids=input_ids,\n             pixel_values=pixel_values,\n@@ -1238,9 +1217,6 @@ def forward(\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )"
        },
        {
            "sha": "75772892d1d25bbe27c63398d3bcf509a86aed22",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 25,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/2589a52c5cf60de8b96016f3267c98f0a1faa100/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2589a52c5cf60de8b96016f3267c98f0a1faa100/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=2589a52c5cf60de8b96016f3267c98f0a1faa100",
            "patch": "@@ -1414,18 +1414,9 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[tuple, AriaModelOutputWithPast]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n@@ -1437,7 +1428,7 @@ def forward(\n                 vision_feature_layer=self.config.vision_feature_layer,\n             )\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-            special_image_mask = self._get_image_mask(\n+            special_image_mask = self.get_placeholder_mask(\n                 input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n             )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n@@ -1448,9 +1439,6 @@ def forward(\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=True,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -1498,9 +1486,6 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n@@ -1565,12 +1550,6 @@ def forward(\n         >>> print(generated_texts[1])\n         Assistant: The bridge is in San Francisco.\n         ```\"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.model(\n             input_ids=input_ids,\n             pixel_values=pixel_values,\n@@ -1580,9 +1559,6 @@ def forward(\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )"
        },
        {
            "sha": "94dafdc2d49c416ba5ccfaa2b2d84edeb18dd534",
            "filename": "tests/models/aria/test_modeling_aria.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2589a52c5cf60de8b96016f3267c98f0a1faa100/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2589a52c5cf60de8b96016f3267c98f0a1faa100/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py?ref=2589a52c5cf60de8b96016f3267c98f0a1faa100",
            "patch": "@@ -137,8 +137,8 @@ def __init__(\n \n     def get_config(self):\n         return AriaConfig(\n-            text_config=self.text_config,\n-            vision_config=self.vision_config,\n+            text_config=self.text_config.to_dict(),\n+            vision_config=self.vision_config.to_dict(),\n             ignore_index=self.ignore_index,\n             image_token_index=self.image_token_index,\n             projector_hidden_act=self.projector_hidden_act,"
        }
    ],
    "stats": {
        "total": 56,
        "additions": 4,
        "deletions": 52
    }
}