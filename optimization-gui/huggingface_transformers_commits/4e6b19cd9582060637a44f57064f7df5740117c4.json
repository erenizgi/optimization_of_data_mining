{
    "author": "MekkCyber",
    "message": "Fix : BitNet tests (#34895)\n\n* fix_tests_bitnet\r\n\r\n* fix format",
    "sha": "4e6b19cd9582060637a44f57064f7df5740117c4",
    "files": [
        {
            "sha": "dc6a2546cced7e658647b21d13f53569b3b81c61",
            "filename": "tests/quantization/bitnet_integration/test_bitnet.py",
            "status": "modified",
            "additions": 12,
            "deletions": 10,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e6b19cd9582060637a44f57064f7df5740117c4/tests%2Fquantization%2Fbitnet_integration%2Ftest_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e6b19cd9582060637a44f57064f7df5740117c4/tests%2Fquantization%2Fbitnet_integration%2Ftest_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbitnet_integration%2Ftest_bitnet.py?ref=4e6b19cd9582060637a44f57064f7df5740117c4",
            "patch": "@@ -95,16 +95,16 @@ def test_replace_with_bitlinear(self):\n \n         self.assertEqual(nb_linears - 1, nb_bitnet_linear)\n \n-    def test_quantized_model(self, quantized_model, tokenizer):\n+    def test_quantized_model(self):\n         \"\"\"\n         Simple test that checks if the quantized model is working properly\n         \"\"\"\n         input_text = \"What are we having for dinner?\"\n         expected_output = \"What are we having for dinner? What are we going to do for fun this weekend?\"\n-        input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+        input_ids = self.tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n \n-        output = quantized_model.generate(**input_ids, max_new_tokens=11, do_sample=False)\n-        self.assertEqual(tokenizer.decode(output[0], skip_special_tokens=True), expected_output)\n+        output = self.quantized_model.generate(**input_ids, max_new_tokens=11, do_sample=False)\n+        self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), expected_output)\n \n     def test_packing_unpacking(self):\n         \"\"\"\n@@ -113,9 +113,12 @@ def test_packing_unpacking(self):\n \n         from transformers.integrations import pack_weights, unpack_weights\n \n-        u = torch.randint(0, 255, (1024, 1024), dtype=torch.uint8)\n+        u = torch.randint(0, 255, (256, 256), dtype=torch.uint8)\n         unpacked_u = unpack_weights(u, dtype=torch.bfloat16)\n-        self.assertEqual(pack_weights(unpacked_u), u)\n+        repacked_u = pack_weights(unpacked_u)\n+        for i in range(u.shape[0]):\n+            for j in range(u.shape[1]):\n+                self.assertEqual(repacked_u[i][j], u[i][j])\n \n     def test_activation_quant(self):\n         \"\"\"\n@@ -127,15 +130,14 @@ def test_activation_quant(self):\n         layer = BitLinear(in_features=4, out_features=2, bias=False, dtype=torch.float32)\n         layer.to(self.device)\n \n-        input_tensor = torch.tensor([[1.0, -1.0, -1.0, 1.0], [1.0, -1.0, 1.0, 1.0]], dtype=torch.float32).to(\n-            torch_device\n-        )\n+        input_tensor = torch.tensor([1.0, -1.0, -1.0, 1.0], dtype=torch.float32).to(torch_device)\n \n         # Quantize the input tensor\n         quantized_tensor, scale = layer.activation_quant(input_tensor)\n \n         # Verify the output quantized tensor\n-        self.assertEqual(quantized_tensor, input_tensor)\n+        for i in range(input_tensor.shape[0]):\n+            self.assertEqual(quantized_tensor[i] / scale, input_tensor[i])\n \n         # Verify the scale tensor\n         self.assertEqual(scale, 127)"
        }
    ],
    "stats": {
        "total": 22,
        "additions": 12,
        "deletions": 10
    }
}