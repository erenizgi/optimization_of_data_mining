{
    "author": "yonigozlan",
    "message": "Add fast image processor SAM (#39385)\n\n* add fast image processor sam\n\n* nits",
    "sha": "de0dd3139df7fe6f521b54958829a59f89acb2c8",
    "files": [
        {
            "sha": "ac73c107b8860751f9f9c7b207a961b1daeaad6c",
            "filename": "docs/source/en/model_doc/sam.md",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/de0dd3139df7fe6f521b54958829a59f89acb2c8/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de0dd3139df7fe6f521b54958829a59f89acb2c8/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam.md?ref=de0dd3139df7fe6f521b54958829a59f89acb2c8",
            "patch": "@@ -25,7 +25,7 @@ rendered properly in your Markdown viewer.\n \n SAM (Segment Anything Model) was proposed in [Segment Anything](https://huggingface.co/papers/2304.02643v1.pdf) by Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alex Berg, Wan-Yen Lo, Piotr Dollar, Ross Girshick.\n \n-The model can be used to predict segmentation masks of any object of interest given an input image. \n+The model can be used to predict segmentation masks of any object of interest given an input image.\n \n ![example image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/sam-output.png)\n \n@@ -37,9 +37,9 @@ Tips:\n \n - The model predicts binary masks that states the presence or not of the object of interest given an image.\n - The model predicts much better results if input 2D points and/or input bounding boxes are provided\n-- You can prompt multiple points for the same image, and predict a single mask. \n+- You can prompt multiple points for the same image, and predict a single mask.\n - Fine-tuning the model is not supported yet\n-- According to the paper, textual input should be also supported. However, at this time of writing this seems not to be supported according to [the official repository](https://github.com/facebookresearch/segment-anything/issues/4#issuecomment-1497626844). \n+- According to the paper, textual input should be also supported. However, at this time of writing this seems not to be supported according to [the official repository](https://github.com/facebookresearch/segment-anything/issues/4#issuecomment-1497626844).\n \n \n This model was contributed by [ybelkada](https://huggingface.co/ybelkada) and [ArthurZ](https://huggingface.co/ArthurZ).\n@@ -149,6 +149,11 @@ alt=\"drawing\" width=\"900\"/>\n [[autodoc]] SamImageProcessor\n \n \n+## SamImageProcessorFast\n+\n+[[autodoc]] SamImageProcessorFast\n+\n+\n ## SamVisionModel\n \n [[autodoc]] SamVisionModel"
        },
        {
            "sha": "84e6a75b168a63d3be79a3698e95a5837621ae43",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de0dd3139df7fe6f521b54958829a59f89acb2c8/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de0dd3139df7fe6f521b54958829a59f89acb2c8/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=de0dd3139df7fe6f521b54958829a59f89acb2c8",
            "patch": "@@ -147,8 +147,8 @@\n             (\"regnet\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),\n             (\"resnet\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),\n             (\"rt_detr\", (\"RTDetrImageProcessor\", \"RTDetrImageProcessorFast\")),\n-            (\"sam\", (\"SamImageProcessor\",)),\n-            (\"sam_hq\", (\"SamImageProcessor\",)),\n+            (\"sam\", (\"SamImageProcessor\", \"SamImageProcessorFast\")),\n+            (\"sam_hq\", (\"SamImageProcessor\", \"SamImageProcessorFast\")),\n             (\"segformer\", (\"SegformerImageProcessor\",)),\n             (\"seggpt\", (\"SegGptImageProcessor\",)),\n             (\"shieldgemma2\", (\"Gemma3ImageProcessor\", \"Gemma3ImageProcessorFast\")),"
        },
        {
            "sha": "b28e1cfe4ae009b61b5479cb9e171ef58f39c0fe",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/de0dd3139df7fe6f521b54958829a59f89acb2c8/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de0dd3139df7fe6f521b54958829a59f89acb2c8/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=de0dd3139df7fe6f521b54958829a59f89acb2c8",
            "patch": "@@ -656,6 +656,15 @@\n         (\"vipllava\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"visual_bert\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"vits\", (\"VitsTokenizer\", None)),\n+        (\n+            \"voxtral\",\n+            (\n+                \"MistralCommonTokenizer\"\n+                if is_mistral_common_available()\n+                else (\"LlamaTokenizer\" if is_sentencepiece_available() else None),\n+                \"LlamaTokenizerFast\" if is_tokenizers_available() and not is_mistral_common_available() else None,\n+            ),\n+        ),\n         (\"wav2vec2\", (\"Wav2Vec2CTCTokenizer\", None)),\n         (\"wav2vec2-bert\", (\"Wav2Vec2CTCTokenizer\", None)),\n         (\"wav2vec2-conformer\", (\"Wav2Vec2CTCTokenizer\", None)),"
        },
        {
            "sha": "bb8a2b98e636b7653747861461b7d648b91164c7",
            "filename": "src/transformers/models/sam/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/de0dd3139df7fe6f521b54958829a59f89acb2c8/src%2Ftransformers%2Fmodels%2Fsam%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de0dd3139df7fe6f521b54958829a59f89acb2c8/src%2Ftransformers%2Fmodels%2Fsam%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2F__init__.py?ref=de0dd3139df7fe6f521b54958829a59f89acb2c8",
            "patch": "@@ -20,6 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_sam import *\n     from .image_processing_sam import *\n+    from .image_processing_sam_fast import *\n     from .modeling_sam import *\n     from .modeling_tf_sam import *\n     from .processing_sam import *"
        },
        {
            "sha": "c431bb72cabb8a89d310a7897584cedc4e93db0f",
            "filename": "src/transformers/models/sam/image_processing_sam.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/de0dd3139df7fe6f521b54958829a59f89acb2c8/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de0dd3139df7fe6f521b54958829a59f89acb2c8/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py?ref=de0dd3139df7fe6f521b54958829a59f89acb2c8",
            "patch": "@@ -387,6 +387,11 @@ def _preprocess_mask(\n \n         return segmentation_map, original_size\n \n+    def __call__(self, images, segmentation_maps=None, **kwargs):\n+        # Overrides the `__call__` method of the `BaseImageProcessor` class such that the images and segmentation maps can both\n+        # be passed in as positional arguments.\n+        return super().__call__(images, segmentation_maps=segmentation_maps, **kwargs)\n+\n     @filter_out_non_signature_kwargs()\n     def preprocess(\n         self,"
        },
        {
            "sha": "b50ba955be17ee0cd07887f0aab6af4decfd4f54",
            "filename": "src/transformers/models/sam/image_processing_sam_fast.py",
            "status": "added",
            "additions": 865,
            "deletions": 0,
            "changes": 865,
            "blob_url": "https://github.com/huggingface/transformers/blob/de0dd3139df7fe6f521b54958829a59f89acb2c8/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de0dd3139df7fe6f521b54958829a59f89acb2c8/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam_fast.py?ref=de0dd3139df7fe6f521b54958829a59f89acb2c8",
            "patch": "@@ -0,0 +1,865 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for SAM.\"\"\"\n+\n+import math\n+from copy import deepcopy\n+from itertools import product\n+from typing import Any, Optional, Union\n+\n+import numpy as np\n+import torch\n+\n+from ...image_processing_utils import BatchFeature, get_size_dict\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+    DefaultFastImageProcessorKwargs,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import (\n+    IMAGENET_DEFAULT_MEAN,\n+    IMAGENET_DEFAULT_STD,\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    SizeDict,\n+    make_list_of_images,\n+    pil_torch_interpolation_mapping,\n+    validate_kwargs,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+    from torch.nn import functional as F_t\n+\n+if is_torchvision_available() and is_torchvision_v2_available():\n+    from torchvision.ops.boxes import batched_nms\n+    from torchvision.transforms.v2 import functional as F\n+elif is_torchvision_available():\n+    from torchvision.ops.boxes import batched_nms\n+    from torchvision.transforms import functional as F\n+\n+\n+class SamFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    r\"\"\"\n+    do_pad (`bool`, *optional*, defaults to `True`):\n+        Controls whether to pad the image. Can be overridden by the `do_pad` parameter in the `preprocess`\n+        method. If `True`, padding will be applied to the bottom and right of the image with zeros.\n+    pad_size (`dict[str, int]`, *optional*):\n+        The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n+        provided for preprocessing.\n+    mask_size (`dict[str, int]`, *optional*):\n+        The size `{\"longest_edge\": int}` to resize the segmentation maps to.\n+    mask_pad_size (`dict[str, int]`, *optional*):\n+        The size `{\"height\": int, \"width\": int}` to pad the segmentation maps to. Must be larger than any segmentation\n+        map size provided for preprocessing.\n+    \"\"\"\n+\n+    mask_size: Optional[dict[str, int]]\n+    do_pad: Optional[bool]\n+    pad_size: Optional[dict[str, int]]\n+    mask_pad_size: Optional[dict[str, int]]\n+\n+\n+@auto_docstring\n+class SamImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BILINEAR\n+    image_mean = IMAGENET_DEFAULT_MEAN\n+    image_std = IMAGENET_DEFAULT_STD\n+    size = {\"longest_edge\": 1024}\n+    mask_size = {\"longest_edge\": 256}\n+    do_resize = True\n+    do_rescale = True\n+    do_normalize = True\n+    do_convert_rgb = True\n+\n+    valid_kwargs = SamFastImageProcessorKwargs\n+\n+    do_pad = True\n+    pad_size = {\"height\": 1024, \"width\": 1024}\n+    mask_pad_size = {\"height\": 256, \"width\": 256}\n+\n+    def __init__(self, **kwargs: Unpack[SamFastImageProcessorKwargs]):\n+        super().__init__(**kwargs)\n+\n+    def pad_image(self, images: \"torch.Tensor\", pad_size: SizeDict):\n+        \"\"\"Pad images to the specified size.\"\"\"\n+        output_height, output_width = pad_size.height, pad_size.width\n+        input_height, input_width = images.shape[-2:]\n+        pad_width = output_width - input_width\n+        pad_height = output_height - input_height\n+        padding = (0, 0, pad_width, pad_height)\n+        return F.pad(images, padding)\n+\n+    def _get_preprocess_shape(self, old_shape: tuple[int, int], longest_edge: int):\n+        \"\"\"\n+        Compute the output size given input size and target long side length.\n+        \"\"\"\n+        oldh, oldw = old_shape\n+        scale = longest_edge * 1.0 / max(oldh, oldw)\n+        newh, neww = oldh * scale, oldw * scale\n+        newh = int(newh + 0.5)\n+        neww = int(neww + 0.5)\n+        return (newh, neww)\n+\n+    def resize(\n+        self, image: \"torch.Tensor\", size: SizeDict, interpolation: Optional[\"F.InterpolationMode\"], **kwargs\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Resize an image to `(size[\"height\"], size[\"width\"])`.\n+\n+        Args:\n+            image (`np.ndarray`):\n+                Image to resize.\n+            size (`dict[str, int]`):\n+                Dictionary in the format `{\"longest_edge\": int}` specifying the size of the output image. The longest\n+                edge of the image will be resized to the specified size, while the other edge will be resized to\n+                maintain the aspect ratio.\n+            interpolation:\n+                `F.InterpolationMode` filter to use when resizing the image e.g. `F.InterpolationMode.BICUBIC`.\n+\n+        Returns:\n+            `torch.Tensor`: The resized image.\n+        \"\"\"\n+        if not size.longest_edge:\n+            raise ValueError(f\"The `size` dictionary must contain the key `longest_edge`. Got {size.keys()}\")\n+        input_size = image.shape[-2:]\n+        output_height, output_width = self._get_preprocess_shape(input_size, size.longest_edge)\n+        return super().resize(\n+            image, size=SizeDict(height=output_height, width=output_width), interpolation=interpolation, **kwargs\n+        )\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_resize: bool,\n+        size: SizeDict,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        do_pad: bool,\n+        pad_size: SizeDict,\n+        disable_grouping: Optional[bool],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n+    ) -> BatchFeature:\n+        # Group images by size for batched resizing\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        resized_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(image=stacked_images, size=size, interpolation=interpolation)\n+            resized_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n+\n+        # Group images by size for further processing\n+        # Needed in case do_resize is False, or resize returns images with different sizes\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            if do_pad:\n+                stacked_images = self.pad_image(stacked_images, pad_size)\n+            processed_images_grouped[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n+\n+        return processed_images\n+\n+    def _preprocess_segmentation_maps(\n+        self,\n+        segmentation_maps,\n+        **kwargs,\n+    ):\n+        \"\"\"Preprocesses segmentation maps.\"\"\"\n+        processed_segmentation_maps = []\n+        for segmentation_map in segmentation_maps:\n+            segmentation_map = self._process_image(\n+                segmentation_map, do_convert_rgb=False, input_data_format=ChannelDimension.FIRST\n+            )\n+\n+            if segmentation_map.ndim == 2:\n+                segmentation_map = segmentation_map[None, ...]\n+            processed_segmentation_maps.append(segmentation_map)\n+\n+        kwargs[\"do_rescale\"] = False\n+        kwargs[\"do_normalize\"] = False\n+        kwargs[\"interpolation\"] = pil_torch_interpolation_mapping[PILImageResampling.NEAREST]\n+        kwargs[\"size\"] = kwargs.pop(\"mask_size\")\n+        kwargs[\"pad_size\"] = kwargs.pop(\"mask_pad_size\")\n+        processed_segmentation_maps = self._preprocess(images=processed_segmentation_maps, **kwargs)\n+\n+        processed_segmentation_maps = processed_segmentation_maps.squeeze(1)  # Remove channel dimension\n+\n+        processed_segmentation_maps = processed_segmentation_maps.to(torch.int64)\n+        return processed_segmentation_maps\n+\n+    def _further_process_kwargs(\n+        self,\n+        size: Optional[SizeDict] = None,\n+        pad_size: Optional[SizeDict] = None,\n+        mask_size: Optional[SizeDict] = None,\n+        mask_pad_size: Optional[SizeDict] = None,\n+        default_to_square: Optional[bool] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n+        data_format: Optional[ChannelDimension] = None,\n+        **kwargs,\n+    ) -> dict:\n+        \"\"\"\n+        Update kwargs that need further processing before being validated\n+        Can be overridden by subclasses to customize the processing of kwargs.\n+        \"\"\"\n+        if kwargs is None:\n+            kwargs = {}\n+        if size is not None:\n+            size = SizeDict(**get_size_dict(size=size, default_to_square=default_to_square))\n+        if pad_size is not None:\n+            pad_size = SizeDict(**get_size_dict(pad_size, param_name=\"pad_size\"))\n+        if mask_size is not None:\n+            mask_size = SizeDict(**get_size_dict(mask_size, param_name=\"mask_size\"))\n+        if mask_pad_size is not None:\n+            mask_pad_size = SizeDict(**get_size_dict(mask_pad_size, param_name=\"mask_pad_size\"))\n+        if isinstance(image_mean, list):\n+            image_mean = tuple(image_mean)\n+        if isinstance(image_std, list):\n+            image_std = tuple(image_std)\n+        if data_format is None:\n+            data_format = ChannelDimension.FIRST\n+\n+        kwargs[\"size\"] = size\n+        kwargs[\"pad_size\"] = pad_size\n+        kwargs[\"mask_size\"] = mask_size\n+        kwargs[\"mask_pad_size\"] = mask_pad_size\n+        kwargs[\"default_to_square\"] = default_to_square\n+        kwargs[\"image_mean\"] = image_mean\n+        kwargs[\"image_std\"] = image_std\n+        kwargs[\"data_format\"] = data_format\n+\n+        return kwargs\n+\n+    @auto_docstring\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        segmentation_maps: Optional[ImageInput] = None,\n+        **kwargs: Unpack[SamFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        r\"\"\"\n+        segmentation_maps (`ImageInput`, *optional*):\n+            The segmentation maps to preprocess.\n+        \"\"\"\n+        validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self.valid_kwargs.__annotations__.keys())\n+        # Set default kwargs from self. This ensures that if a kwarg is not provided\n+        # by the user, it gets its default value from the instance, or is set to None.\n+        for kwarg_name in self.valid_kwargs.__annotations__:\n+            kwargs.setdefault(kwarg_name, getattr(self, kwarg_name, None))\n+\n+        # Extract parameters that are only used for preparing the input images\n+        do_convert_rgb = kwargs.pop(\"do_convert_rgb\")\n+        input_data_format = kwargs.pop(\"input_data_format\")\n+        device = kwargs.pop(\"device\")\n+        # Prepare input images\n+        images = self._prepare_input_images(\n+            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+        )\n+\n+        # Prepare segmentation maps\n+        if segmentation_maps is not None:\n+            segmentation_maps = make_list_of_images(images=segmentation_maps, expected_ndims=2)\n+\n+        # Update kwargs that need further processing before being validated\n+        kwargs = self._further_process_kwargs(**kwargs)\n+\n+        # Validate kwargs\n+        self._validate_preprocess_kwargs(**kwargs)\n+\n+        # torch resize uses interpolation instead of resample\n+        resample = kwargs.pop(\"resample\")\n+        kwargs[\"interpolation\"] = (\n+            pil_torch_interpolation_mapping[resample] if isinstance(resample, (PILImageResampling, int)) else resample\n+        )\n+\n+        # Pop kwargs that are not needed in _preprocess\n+        kwargs.pop(\"default_to_square\")\n+        kwargs.pop(\"data_format\")\n+\n+        original_sizes = [image.shape[-2:] for image in images]\n+\n+        images = self._preprocess(\n+            images=images,\n+            **kwargs,\n+        )\n+        reshaped_input_sizes = [image.shape[-2:] for image in images]\n+\n+        if segmentation_maps is not None:\n+            segmentation_maps = self._preprocess_segmentation_maps(\n+                segmentation_maps=segmentation_maps,\n+                **kwargs,\n+            )\n+\n+            return BatchFeature(\n+                data={\n+                    \"pixel_values\": images,\n+                    \"labels\": segmentation_maps,\n+                    \"original_sizes\": original_sizes,\n+                    \"reshaped_input_sizes\": reshaped_input_sizes,\n+                },\n+                tensor_type=kwargs[\"return_tensors\"],\n+            )\n+\n+        return BatchFeature(\n+            data={\n+                \"pixel_values\": images,\n+                \"original_sizes\": original_sizes,\n+                \"reshaped_input_sizes\": reshaped_input_sizes,\n+            },\n+            tensor_type=kwargs[\"return_tensors\"],\n+        )\n+\n+    def generate_crop_boxes(\n+        self,\n+        image: \"torch.Tensor\",\n+        target_size,\n+        crop_n_layers: int = 0,\n+        overlap_ratio: float = 512 / 1500,\n+        points_per_crop: Optional[int] = 32,\n+        crop_n_points_downscale_factor: Optional[list[int]] = 1,\n+        device: Optional[\"torch.device\"] = None,\n+    ):\n+        \"\"\"\n+        Generates a list of crop boxes of different sizes. Each layer has (2**i)**2 boxes for the ith layer.\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                Input original image\n+            target_size (`int`):\n+                Target size of the resized image\n+            crop_n_layers (`int`, *optional*, defaults to 0):\n+                If >0, mask prediction will be run again on crops of the image. Sets the number of layers to run, where\n+                each layer has 2**i_layer number of image crops.\n+            overlap_ratio (`float`, *optional*, defaults to 512/1500):\n+                Sets the degree to which crops overlap. In the first crop layer, crops will overlap by this fraction of\n+                the image length. Later layers with more crops scale down this overlap.\n+            points_per_crop (`int`, *optional*, defaults to 32):\n+                Number of points to sample from each crop.\n+            crop_n_points_downscale_factor (`list[int]`, *optional*, defaults to 1):\n+                The number of points-per-side sampled in layer n is scaled down by crop_n_points_downscale_factor**n.\n+            device (`torch.device`, *optional*, defaults to None):\n+                Device to use for the computation. If None, cpu will be used.\n+            input_data_format (`str` or `ChannelDimension`, *optional*):\n+                The channel dimension format of the input image. If not provided, it will be inferred.\n+            return_tensors (`str`, *optional*, defaults to `pt`):\n+                If `pt`, returns `torch.Tensor`. If `tf`, returns `tf.Tensor`.\n+        \"\"\"\n+        image = self._process_image(image)\n+        crop_boxes, points_per_crop, cropped_images, input_labels = _generate_crop_boxes(\n+            image,\n+            target_size,\n+            crop_n_layers,\n+            overlap_ratio,\n+            points_per_crop,\n+            crop_n_points_downscale_factor,\n+        )\n+        if device is None:\n+            device = torch.device(\"cpu\")\n+        crop_boxes = crop_boxes.to(device)\n+        points_per_crop = points_per_crop.to(device)\n+        # cropped_images stays as torch.Tensor\n+        input_labels = input_labels.to(device)\n+\n+        return crop_boxes, points_per_crop, cropped_images, input_labels\n+\n+    def filter_masks(\n+        self,\n+        masks,\n+        iou_scores,\n+        original_size,\n+        cropped_box_image,\n+        pred_iou_thresh=0.88,\n+        stability_score_thresh=0.95,\n+        mask_threshold=0,\n+        stability_score_offset=1,\n+    ):\n+        \"\"\"\n+        Filters the predicted masks by selecting only the ones that meets several criteria. The first criterion being\n+        that the iou scores needs to be greater than `pred_iou_thresh`. The second criterion is that the stability\n+        score needs to be greater than `stability_score_thresh`. The method also converts the predicted masks to\n+        bounding boxes and pad the predicted masks if necessary.\n+\n+        Args:\n+            masks (`torch.Tensor`):\n+                Input masks.\n+            iou_scores (`torch.Tensor`):\n+                List of IoU scores.\n+            original_size (`tuple[int,int]`):\n+                Size of the original image.\n+            cropped_box_image (`torch.Tensor`):\n+                The cropped image.\n+            pred_iou_thresh (`float`, *optional*, defaults to 0.88):\n+                The threshold for the iou scores.\n+            stability_score_thresh (`float`, *optional*, defaults to 0.95):\n+                The threshold for the stability score.\n+            mask_threshold (`float`, *optional*, defaults to 0):\n+                The threshold for the predicted masks.\n+            stability_score_offset (`float`, *optional*, defaults to 1):\n+                The offset for the stability score used in the `_compute_stability_score` method.\n+\n+        \"\"\"\n+        original_height, original_width = original_size\n+        iou_scores = iou_scores.flatten(0, 1)\n+        masks = masks.flatten(0, 1)\n+\n+        if masks.shape[0] != iou_scores.shape[0]:\n+            raise ValueError(\"masks and iou_scores must have the same batch size.\")\n+\n+        if masks.device != iou_scores.device:\n+            iou_scores = iou_scores.to(masks.device)\n+\n+        batch_size = masks.shape[0]\n+\n+        keep_mask = torch.ones(batch_size, dtype=torch.bool, device=masks.device)\n+\n+        if pred_iou_thresh > 0.0:\n+            keep_mask = keep_mask & (iou_scores > pred_iou_thresh)\n+\n+        # compute stability score\n+        if stability_score_thresh > 0.0:\n+            stability_scores = _compute_stability_score(masks, mask_threshold, stability_score_offset)\n+            keep_mask = keep_mask & (stability_scores > stability_score_thresh)\n+\n+        scores = iou_scores[keep_mask]\n+        masks = masks[keep_mask]\n+\n+        # binarize masks\n+        masks = masks > mask_threshold\n+        converted_boxes = _batched_mask_to_box(masks)\n+\n+        keep_mask = ~_is_box_near_crop_edge(\n+            converted_boxes, cropped_box_image, [0, 0, original_width, original_height]\n+        )\n+\n+        scores = scores[keep_mask]\n+        masks = masks[keep_mask]\n+        converted_boxes = converted_boxes[keep_mask]\n+\n+        masks = _pad_masks(masks, cropped_box_image, original_height, original_width)\n+        # conversion to rle is necessary to run non-maximum suppression\n+        masks = _mask_to_rle(masks)\n+\n+        return masks, scores, converted_boxes\n+\n+    def post_process_masks(\n+        self,\n+        masks,\n+        original_sizes,\n+        reshaped_input_sizes,\n+        mask_threshold=0.0,\n+        binarize=True,\n+        pad_size=None,\n+    ):\n+        \"\"\"\n+        Remove padding and upscale masks to the original image size.\n+\n+        Args:\n+            masks (`Union[List[torch.Tensor], List[np.ndarray]]`):\n+                Batched masks from the mask_decoder in (batch_size, num_channels, height, width) format.\n+            original_sizes (`Union[torch.Tensor, List[Tuple[int,int]]]`):\n+                The original sizes of each image before it was resized to the model's expected input shape, in (height,\n+                width) format.\n+            reshaped_input_sizes (`Union[torch.Tensor, List[Tuple[int,int]]]`):\n+                The size of each image as it is fed to the model, in (height, width) format. Used to remove padding.\n+            mask_threshold (`float`, *optional*, defaults to 0.0):\n+                The threshold to use for binarizing the masks.\n+            binarize (`bool`, *optional*, defaults to `True`):\n+                Whether to binarize the masks.\n+            pad_size (`int`, *optional*, defaults to `self.pad_size`):\n+                The target size the images were padded to before being passed to the model. If None, the target size is\n+                assumed to be the processor's `pad_size`.\n+        Returns:\n+            (`torch.Tensor`): Batched masks in batch_size, num_channels, height, width) format, where (height, width)\n+            is given by original_size.\n+        \"\"\"\n+        pad_size = self.size if pad_size is None else pad_size\n+        target_image_size = (pad_size[\"height\"], pad_size[\"width\"])\n+        if isinstance(original_sizes, (torch.Tensor, np.ndarray)):\n+            original_sizes = original_sizes.tolist()\n+        if isinstance(reshaped_input_sizes, (torch.Tensor, np.ndarray)):\n+            reshaped_input_sizes = reshaped_input_sizes.tolist()\n+\n+        output_masks = []\n+        for i, original_size in enumerate(original_sizes):\n+            if isinstance(masks[i], np.ndarray):\n+                masks[i] = torch.from_numpy(masks[i])\n+            elif not isinstance(masks[i], torch.Tensor):\n+                raise ValueError(\"Input masks should be a list of `torch.tensors` or a list of `np.ndarray`\")\n+            interpolated_mask = F_t.interpolate(masks[i], target_image_size, mode=\"bilinear\", align_corners=False)\n+            interpolated_mask = interpolated_mask[..., : reshaped_input_sizes[i][0], : reshaped_input_sizes[i][1]]\n+            interpolated_mask = F_t.interpolate(interpolated_mask, original_size, mode=\"bilinear\", align_corners=False)\n+            if binarize:\n+                interpolated_mask = interpolated_mask > mask_threshold\n+            output_masks.append(interpolated_mask)\n+\n+        return output_masks\n+\n+    def post_process_for_mask_generation(self, all_masks, all_scores, all_boxes, crops_nms_thresh):\n+        \"\"\"\n+        Post processes mask that are generated by calling the Non Maximum Suppression algorithm on the predicted masks.\n+\n+        Args:\n+            all_masks (`torch.Tensor`):\n+                List of all predicted segmentation masks\n+            all_scores (`torch.Tensor`):\n+                List of all predicted iou scores\n+            all_boxes (`torch.Tensor`):\n+                List of all bounding boxes of the predicted masks\n+            crops_nms_thresh (`float`):\n+                Threshold for NMS (Non Maximum Suppression) algorithm.\n+        \"\"\"\n+        return _post_process_for_mask_generation(all_masks, all_scores, all_boxes, crops_nms_thresh)\n+\n+\n+def _compute_stability_score(masks: \"torch.Tensor\", mask_threshold: float, stability_score_offset: int):\n+    # One mask is always contained inside the other.\n+    # Save memory by preventing unnecessary cast to torch.int64\n+    intersections = (\n+        (masks > (mask_threshold + stability_score_offset)).sum(-1, dtype=torch.int16).sum(-1, dtype=torch.int32)\n+    )\n+    unions = (masks > (mask_threshold - stability_score_offset)).sum(-1, dtype=torch.int16).sum(-1, dtype=torch.int32)\n+    stability_scores = intersections / unions\n+    return stability_scores\n+\n+\n+def _mask_to_rle(input_mask: \"torch.Tensor\"):\n+    \"\"\"\n+    Encodes masks the run-length encoding (RLE), in the format expected by pycoco tools.\n+    \"\"\"\n+    # Put in fortran order and flatten height and width\n+    batch_size, height, width = input_mask.shape\n+    input_mask = input_mask.permute(0, 2, 1).flatten(1)\n+\n+    # Compute change indices\n+    diff = input_mask[:, 1:] ^ input_mask[:, :-1]\n+    change_indices = diff.nonzero()\n+\n+    # Encode run length\n+    out = []\n+    for i in range(batch_size):\n+        cur_idxs = change_indices[change_indices[:, 0] == i, 1] + 1\n+        if len(cur_idxs) == 0:\n+            # No changes => either all 0 or all 1\n+            # If the entire mask is 0, RLE is [height*width] or if the entire mask is 1, RLE is [0, height*width].\n+            if input_mask[i, 0] == 0:\n+                out.append({\"size\": [height, width], \"counts\": [height * width]})\n+            else:\n+                out.append({\"size\": [height, width], \"counts\": [0, height * width]})\n+            continue\n+        btw_idxs = cur_idxs[1:] - cur_idxs[:-1]\n+        counts = [] if input_mask[i, 0] == 0 else [0]\n+        counts += [cur_idxs[0].item()] + btw_idxs.tolist() + [height * width - cur_idxs[-1].item()]\n+        out.append({\"size\": [height, width], \"counts\": counts})\n+    return out\n+\n+\n+def _batched_mask_to_box(masks: \"torch.Tensor\"):\n+    \"\"\"\n+    Computes the bounding boxes around the given input masks. The bounding boxes are in the XYXY format which\n+    corresponds the following required indices:\n+        - LEFT: left hand side of the bounding box\n+        - TOP: top of the bounding box\n+        - RIGHT: right of the bounding box\n+        - BOTTOM: bottom of the bounding box\n+\n+    Return [0,0,0,0] for an empty mask. For input shape channel_1 x channel_2 x ... x height x width, the output shape\n+    is channel_1 x channel_2 x ... x 4.\n+\n+    Args:\n+        - masks (`torch.Tensor` of shape `(batch, nb_mask, height, width)`)\n+    \"\"\"\n+    # torch.max below raises an error on empty inputs, just skip in this case\n+\n+    if torch.numel(masks) == 0:\n+        return torch.zeros(*masks.shape[:-2], 4, device=masks.device)\n+\n+    # Normalize shape to Cxheightxwidth\n+    shape = masks.shape\n+    height, width = shape[-2:]\n+\n+    # Get top and bottom edges\n+    in_height, _ = torch.max(masks, dim=-1)\n+    in_height_coords = in_height * torch.arange(height, device=in_height.device)[None, :]\n+    bottom_edges, _ = torch.max(in_height_coords, dim=-1)\n+    in_height_coords = in_height_coords + height * (~in_height)\n+    top_edges, _ = torch.min(in_height_coords, dim=-1)\n+\n+    # Get left and right edges\n+    in_width, _ = torch.max(masks, dim=-2)\n+    in_width_coords = in_width * torch.arange(width, device=in_width.device)[None, :]\n+    right_edges, _ = torch.max(in_width_coords, dim=-1)\n+    in_width_coords = in_width_coords + width * (~in_width)\n+    left_edges, _ = torch.min(in_width_coords, dim=-1)\n+\n+    # If the mask is empty the right edge will be to the left of the left edge.\n+    # Replace these boxes with [0, 0, 0, 0]\n+    empty_filter = (right_edges < left_edges) | (bottom_edges < top_edges)\n+    out = torch.stack([left_edges, top_edges, right_edges, bottom_edges], dim=-1)\n+    out = out * (~empty_filter).unsqueeze(-1)\n+\n+    # Return to original shape\n+    out = out.reshape(*shape[:-2], 4)\n+    return out\n+\n+\n+def _is_box_near_crop_edge(boxes, crop_box, orig_box, atol=20.0):\n+    \"\"\"Filter masks at the edge of a crop, but not at the edge of the original image.\"\"\"\n+    crop_box_torch = torch.as_tensor(crop_box, dtype=torch.float, device=boxes.device)\n+    orig_box_torch = torch.as_tensor(orig_box, dtype=torch.float, device=boxes.device)\n+\n+    left, top, _, _ = crop_box\n+    offset = torch.tensor([[left, top, left, top]], device=boxes.device)\n+    # Check if boxes has a channel dimension\n+    if len(boxes.shape) == 3:\n+        offset = offset.unsqueeze(1)\n+    boxes = (boxes + offset).float()\n+\n+    near_crop_edge = torch.isclose(boxes, crop_box_torch[None, :], atol=atol, rtol=0)\n+    near_image_edge = torch.isclose(boxes, orig_box_torch[None, :], atol=atol, rtol=0)\n+    near_crop_edge = torch.logical_and(near_crop_edge, ~near_image_edge)\n+    return torch.any(near_crop_edge, dim=1)\n+\n+\n+def _pad_masks(masks, crop_box: list[int], orig_height: int, orig_width: int):\n+    left, top, right, bottom = crop_box\n+    if left == 0 and top == 0 and right == orig_width and bottom == orig_height:\n+        return masks\n+    # Coordinate transform masks\n+    pad_x, pad_y = orig_width - (right - left), orig_height - (bottom - top)\n+    pad = (left, pad_x - left, top, pad_y - top)\n+    return torch.nn.functional.pad(masks, pad, value=0)\n+\n+\n+def _generate_crop_boxes(\n+    image,\n+    target_size: int,  # Is it tuple here?\n+    crop_n_layers: int = 0,\n+    overlap_ratio: float = 512 / 1500,\n+    points_per_crop: Optional[int] = 32,\n+    crop_n_points_downscale_factor: Optional[list[int]] = 1,\n+) -> tuple[list[list[int]], list[int]]:\n+    \"\"\"\n+    Generates a list of crop boxes of different sizes. Each layer has (2**i)**2 boxes for the ith layer.\n+\n+    Args:\n+        image (Union[`numpy.ndarray`, `PIL.Image`, `torch.Tensor`]):\n+            Image to generate crops for.\n+        target_size (`int`):\n+            Size of the smallest crop.\n+        crop_n_layers (`int`, *optional*):\n+            If `crops_n_layers>0`, mask prediction will be run again on crops of the image. Sets the number of layers\n+            to run, where each layer has 2**i_layer number of image crops.\n+        overlap_ratio (`int`, *optional*):\n+            Sets the degree to which crops overlap. In the first crop layer, crops will overlap by this fraction of the\n+            image length. Later layers with more crops scale down this overlap.\n+        points_per_crop (`int`, *optional*):\n+            Number of points to sample per crop.\n+        crop_n_points_downscale_factor (`int`, *optional*):\n+            The number of points-per-side sampled in layer n is scaled down by crop_n_points_downscale_factor**n.\n+        input_data_format (`str` or `ChannelDimension`, *optional*):\n+            The channel dimension format of the input image. If not provided, it will be inferred.\n+    \"\"\"\n+\n+    if isinstance(image, list):\n+        raise ValueError(\"Only one image is allowed for crop generation.\")\n+    original_size = image.shape[-2:]\n+\n+    points_grid = []\n+    for i in range(crop_n_layers + 1):\n+        n_points = int(points_per_crop / (crop_n_points_downscale_factor**i))\n+        points_grid.append(_build_point_grid(n_points))\n+\n+    crop_boxes, layer_idxs = _generate_per_layer_crops(crop_n_layers, overlap_ratio, original_size)\n+\n+    cropped_images, point_grid_per_crop = _generate_crop_images(\n+        crop_boxes, image, points_grid, layer_idxs, target_size, original_size\n+    )\n+    crop_boxes = torch.tensor(crop_boxes)\n+    crop_boxes = crop_boxes.float()\n+    points_per_crop = torch.stack(point_grid_per_crop)\n+    points_per_crop = points_per_crop.unsqueeze(0).permute(0, 2, 1, 3)\n+    cropped_images = torch.stack(cropped_images)\n+\n+    input_labels = torch.ones_like(points_per_crop[:, :, :, 0], dtype=torch.int64)\n+\n+    return crop_boxes, points_per_crop, cropped_images, input_labels\n+\n+\n+def _generate_per_layer_crops(crop_n_layers, overlap_ratio, original_size):\n+    \"\"\"\n+    Generates 2 ** (layers idx + 1) crops for each crop_n_layers. Crops are in the XYWH format : The XYWH format\n+    consists of the following required indices:\n+        - X: X coordinate of the top left of the bounding box\n+        - Y: Y coordinate of the top left of the bounding box\n+        - W: width of the bounding box\n+        - H: height of the bounding box\n+    \"\"\"\n+    crop_boxes, layer_idxs = [], []\n+    im_height, im_width = original_size\n+    short_side = min(im_height, im_width)\n+\n+    # Original image\n+    crop_boxes.append([0, 0, im_width, im_height])\n+    layer_idxs.append(0)\n+    for i_layer in range(crop_n_layers):\n+        n_crops_per_side = 2 ** (i_layer + 1)\n+        overlap = int(overlap_ratio * short_side * (2 / n_crops_per_side))\n+\n+        crop_width = int(math.ceil((overlap * (n_crops_per_side - 1) + im_width) / n_crops_per_side))\n+        crop_height = int(math.ceil((overlap * (n_crops_per_side - 1) + im_height) / n_crops_per_side))\n+\n+        crop_box_x0 = [int((crop_width - overlap) * i) for i in range(n_crops_per_side)]\n+        crop_box_y0 = [int((crop_height - overlap) * i) for i in range(n_crops_per_side)]\n+\n+        for left, top in product(crop_box_x0, crop_box_y0):\n+            box = [left, top, min(left + crop_width, im_width), min(top + crop_height, im_height)]\n+            crop_boxes.append(box)\n+            layer_idxs.append(i_layer + 1)\n+\n+    return crop_boxes, layer_idxs\n+\n+\n+def _build_point_grid(n_per_side: int) -> torch.Tensor:\n+    \"\"\"Generates a 2D grid of points evenly spaced in [0,1]x[0,1].\"\"\"\n+    offset = 1 / (2 * n_per_side)\n+    points_one_side = torch.linspace(offset, 1 - offset, n_per_side)\n+    points_x = torch.tile(points_one_side[None, :], (n_per_side, 1))\n+    points_y = torch.tile(points_one_side[:, None], (1, n_per_side))\n+    points = torch.stack([points_x, points_y], dim=-1).reshape(-1, 2)\n+    return points\n+\n+\n+def _generate_crop_images(\n+    crop_boxes, image, points_grid, layer_idxs, target_size, original_size, input_data_format=None\n+):\n+    \"\"\"\n+    Takes as an input bounding boxes that are used to crop the image. Based in the crops, the corresponding points are\n+    also passed.\n+    \"\"\"\n+    cropped_images = []\n+    total_points_per_crop = []\n+    for i, crop_box in enumerate(crop_boxes):\n+        left, top, right, bottom = crop_box\n+        cropped_im = image[:, top:bottom, left:right]\n+\n+        cropped_images.append(cropped_im)\n+\n+        cropped_im_size = cropped_im.shape[-2:]\n+        points_scale = torch.tensor(cropped_im_size).flip(dims=(0,)).unsqueeze(0)\n+\n+        points = points_grid[layer_idxs[i]] * points_scale\n+        normalized_points = _normalize_coordinates(target_size, points, original_size)\n+        total_points_per_crop.append(normalized_points)\n+\n+    return cropped_images, total_points_per_crop\n+\n+\n+def _normalize_coordinates(\n+    target_size: int, coords: torch.Tensor, original_size: tuple[int, int], is_bounding_box=False\n+) -> torch.Tensor:\n+    \"\"\"\n+    Expects a numpy array of length 2 in the final dimension. Requires the original image size in (height, width)\n+    format.\n+    \"\"\"\n+    old_height, old_width = original_size\n+\n+    scale = target_size * 1.0 / max(old_height, old_width)\n+    new_height, new_width = old_height * scale, old_width * scale\n+    new_width = int(new_width + 0.5)\n+    new_height = int(new_height + 0.5)\n+\n+    coords = deepcopy(coords).float()\n+\n+    if is_bounding_box:\n+        coords = coords.reshape(-1, 2, 2)\n+\n+    coords[..., 0] = coords[..., 0] * (new_width / old_width)\n+    coords[..., 1] = coords[..., 1] * (new_height / old_height)\n+\n+    if is_bounding_box:\n+        coords = coords.reshape(-1, 4)\n+\n+    return coords\n+\n+\n+def _rle_to_mask(rle: dict[str, Any]) -> torch.Tensor:\n+    \"\"\"Compute a binary mask from an uncompressed RLE.\"\"\"\n+    height, width = rle[\"size\"]\n+    mask = torch.empty(height * width, dtype=bool)\n+    idx = 0\n+    parity = False\n+    for count in rle[\"counts\"]:\n+        mask[idx : idx + count] = parity\n+        idx += count\n+        parity = not parity\n+    mask = mask.reshape(width, height)\n+    return mask.transpose(0, 1)  # Reshape to original shape\n+\n+\n+def _post_process_for_mask_generation(rle_masks, iou_scores, mask_boxes, amg_crops_nms_thresh=0.7):\n+    \"\"\"\n+    Perform NMS (Non Maximum Suppression) on the outputs.\n+\n+    Args:\n+            rle_masks (`torch.Tensor`):\n+                binary masks in the RLE format\n+            iou_scores (`torch.Tensor` of shape (nb_masks, 1)):\n+                iou_scores predicted by the model\n+            mask_boxes (`torch.Tensor`):\n+                The bounding boxes corresponding to segmentation masks\n+            amg_crops_nms_thresh (`float`, *optional*, defaults to 0.7):\n+                NMS threshold.\n+    \"\"\"\n+    keep_by_nms = batched_nms(\n+        boxes=mask_boxes.float(),\n+        scores=iou_scores,\n+        idxs=torch.zeros(mask_boxes.shape[0]),\n+        iou_threshold=amg_crops_nms_thresh,\n+    )\n+\n+    iou_scores = iou_scores[keep_by_nms]\n+    rle_masks = [rle_masks[i] for i in keep_by_nms]\n+    mask_boxes = mask_boxes[keep_by_nms]\n+    masks = [_rle_to_mask(rle) for rle in rle_masks]\n+\n+    return masks, iou_scores, rle_masks, mask_boxes\n+\n+\n+__all__ = [\"SamImageProcessorFast\"]"
        },
        {
            "sha": "c6aef45b15d3919ed843e5981702ae675ff6fb1b",
            "filename": "tests/models/sam/test_image_processing_sam.py",
            "status": "added",
            "additions": 301,
            "deletions": 0,
            "changes": 301,
            "blob_url": "https://github.com/huggingface/transformers/blob/de0dd3139df7fe6f521b54958829a59f89acb2c8/tests%2Fmodels%2Fsam%2Ftest_image_processing_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de0dd3139df7fe6f521b54958829a59f89acb2c8/tests%2Fmodels%2Fsam%2Ftest_image_processing_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam%2Ftest_image_processing_sam.py?ref=de0dd3139df7fe6f521b54958829a59f89acb2c8",
            "patch": "@@ -0,0 +1,301 @@\n+# Copyright 2025 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+import unittest\n+\n+from datasets import load_dataset\n+\n+from transformers.file_utils import is_torch_available, is_vision_available\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_torchvision_available\n+\n+from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_vision_available():\n+    from transformers import SamImageProcessor\n+\n+    if is_torchvision_available():\n+        from transformers import SamImageProcessorFast\n+\n+\n+class SamImageProcessingTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=7,\n+        num_channels=3,\n+        image_size=18,\n+        min_resolution=30,\n+        max_resolution=400,\n+        do_pad=True,\n+        pad_size=None,\n+        mask_size=None,\n+        mask_pad_size=None,\n+        do_resize=True,\n+        size=None,\n+        do_normalize=True,\n+        image_mean=[0.5, 0.5, 0.5],\n+        image_std=[0.5, 0.5, 0.5],\n+    ):\n+        size = size if size is not None else {\"longest_edge\": 20}\n+        pad_size = pad_size if pad_size is not None else {\"height\": 20, \"width\": 20}\n+        mask_size = mask_size if mask_size is not None else {\"longest_edge\": 12}\n+        mask_pad_size = mask_pad_size if mask_pad_size is not None else {\"height\": 12, \"width\": 12}\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.min_resolution = min_resolution\n+        self.max_resolution = max_resolution\n+        self.do_pad = do_pad\n+        self.pad_size = pad_size\n+        self.mask_size = mask_size\n+        self.mask_pad_size = mask_pad_size\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+\n+    def prepare_image_processor_dict(self):\n+        return {\n+            \"image_mean\": self.image_mean,\n+            \"image_std\": self.image_std,\n+            \"do_normalize\": self.do_normalize,\n+            \"do_resize\": self.do_resize,\n+            \"size\": self.size,\n+            \"do_pad\": self.do_pad,\n+            \"pad_size\": self.pad_size,\n+            \"mask_size\": self.mask_size,\n+            \"mask_pad_size\": self.mask_pad_size,\n+        }\n+\n+    def expected_output_image_shape(self, images):\n+        return self.num_channels, self.pad_size[\"height\"], self.pad_size[\"width\"]\n+\n+    def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=False):\n+        return prepare_image_inputs(\n+            batch_size=self.batch_size,\n+            num_channels=self.num_channels,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=equal_resolution,\n+            numpify=numpify,\n+            torchify=torchify,\n+        )\n+\n+\n+# Copied from transformers.tests.models.beit.test_image_processing_beit.prepare_semantic_single_inputs\n+def prepare_semantic_single_inputs():\n+    ds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\")\n+    example = ds[0]\n+    return example[\"image\"], example[\"map\"]\n+\n+\n+# Copied from transformers.tests.models.beit.test_image_processing_beit.prepare_semantic_batch_inputs\n+def prepare_semantic_batch_inputs():\n+    ds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\")\n+    return list(ds[\"image\"][:2]), list(ds[\"map\"][:2])\n+\n+\n+@require_torch\n+@require_vision\n+class SamImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n+    image_processing_class = SamImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = SamImageProcessorFast if is_torchvision_available() else None\n+\n+    def setUp(self):\n+        super().setUp()\n+        self.image_processor_tester = SamImageProcessingTester(self)\n+\n+    @property\n+    def image_processor_dict(self):\n+        return self.image_processor_tester.prepare_image_processor_dict()\n+\n+    def test_image_processor_properties(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n+            self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n+            self.assertTrue(hasattr(image_processing, \"do_pad\"))\n+            self.assertTrue(hasattr(image_processing, \"pad_size\"))\n+            self.assertTrue(hasattr(image_processing, \"mask_size\"))\n+            self.assertTrue(hasattr(image_processing, \"mask_pad_size\"))\n+\n+    def test_image_processor_from_dict_with_kwargs(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processing_class = image_processing_class(**self.image_processor_dict)\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"longest_edge\": 20})\n+\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict, size={\"longest_edge\": 42})\n+            self.assertEqual(image_processor.size, {\"longest_edge\": 42})\n+\n+    def test_call_segmentation_maps(self):\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processor\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            # create random PyTorch tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+            maps = []\n+            for image in image_inputs:\n+                self.assertIsInstance(image, torch.Tensor)\n+                maps.append(torch.zeros(image.shape[-2:]).long())\n+\n+            # Test not batched input\n+            encoding = image_processor(image_inputs[0], maps[0], return_tensors=\"pt\")\n+            self.assertEqual(\n+                encoding[\"pixel_values\"].shape,\n+                (\n+                    1,\n+                    self.image_processor_tester.num_channels,\n+                    self.image_processor_tester.pad_size[\"height\"],\n+                    self.image_processor_tester.pad_size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(\n+                encoding[\"labels\"].shape,\n+                (\n+                    1,\n+                    self.image_processor_tester.mask_pad_size[\"height\"],\n+                    self.image_processor_tester.mask_pad_size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+\n+            # Test batched\n+            encoding = image_processor(image_inputs, maps, return_tensors=\"pt\")\n+            self.assertEqual(\n+                encoding[\"pixel_values\"].shape,\n+                (\n+                    self.image_processor_tester.batch_size,\n+                    self.image_processor_tester.num_channels,\n+                    self.image_processor_tester.pad_size[\"height\"],\n+                    self.image_processor_tester.pad_size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(\n+                encoding[\"labels\"].shape,\n+                (\n+                    self.image_processor_tester.batch_size,\n+                    self.image_processor_tester.mask_pad_size[\"height\"],\n+                    self.image_processor_tester.mask_pad_size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+\n+            # Test not batched input (PIL images)\n+            image, segmentation_map = prepare_semantic_single_inputs()\n+\n+            encoding = image_processor(image, segmentation_map, return_tensors=\"pt\")\n+            self.assertEqual(\n+                encoding[\"pixel_values\"].shape,\n+                (\n+                    1,\n+                    self.image_processor_tester.num_channels,\n+                    self.image_processor_tester.pad_size[\"height\"],\n+                    self.image_processor_tester.pad_size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(\n+                encoding[\"labels\"].shape,\n+                (\n+                    1,\n+                    self.image_processor_tester.mask_pad_size[\"height\"],\n+                    self.image_processor_tester.mask_pad_size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+\n+            # Test batched input (PIL images)\n+            images, segmentation_maps = prepare_semantic_batch_inputs()\n+\n+            encoding = image_processor(images, segmentation_maps, return_tensors=\"pt\")\n+            self.assertEqual(\n+                encoding[\"pixel_values\"].shape,\n+                (\n+                    2,\n+                    self.image_processor_tester.num_channels,\n+                    self.image_processor_tester.pad_size[\"height\"],\n+                    self.image_processor_tester.pad_size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(\n+                encoding[\"labels\"].shape,\n+                (\n+                    2,\n+                    self.image_processor_tester.mask_pad_size[\"height\"],\n+                    self.image_processor_tester.mask_pad_size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+\n+    def test_slow_fast_equivalence(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        dummy_image, dummy_map = prepare_semantic_single_inputs()\n+\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        image_encoding_slow = image_processor_slow(dummy_image, segmentation_maps=dummy_map, return_tensors=\"pt\")\n+        image_encoding_fast = image_processor_fast(dummy_image, segmentation_maps=dummy_map, return_tensors=\"pt\")\n+\n+        self.assertTrue(torch.allclose(image_encoding_slow.pixel_values, image_encoding_fast.pixel_values, atol=1e-1))\n+        self.assertLessEqual(\n+            torch.mean(torch.abs(image_encoding_slow.pixel_values - image_encoding_fast.pixel_values)).item(), 1e-3\n+        )\n+        self.assertTrue(torch.allclose(image_encoding_slow.labels, image_encoding_fast.labels, atol=1e-1))\n+\n+    def test_slow_fast_equivalence_batched(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        dummy_images, dummy_maps = prepare_semantic_batch_inputs()\n+\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_images, segmentation_maps=dummy_maps, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_images, segmentation_maps=dummy_maps, return_tensors=\"pt\")\n+\n+        self.assertTrue(torch.allclose(encoding_slow.pixel_values, encoding_fast.pixel_values, atol=1e-1))\n+        self.assertLessEqual(\n+            torch.mean(torch.abs(encoding_slow.pixel_values - encoding_fast.pixel_values)).item(), 1e-3\n+        )"
        }
    ],
    "stats": {
        "total": 1196,
        "additions": 1191,
        "deletions": 5
    }
}