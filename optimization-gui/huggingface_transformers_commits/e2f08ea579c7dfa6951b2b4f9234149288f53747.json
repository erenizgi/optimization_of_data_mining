{
    "author": "molbap",
    "message": "Attempt to fix VLM gradient enabling (#41993)\n\n* attempt to fix gradients\n\n* Improve tests, use PreTrainedModel hooks, cleanup\n\n* missing patch_embed\n\n* fix arg name\n\n* local revert\n\n* adapt BART test\n\n* lingering fails\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "e2f08ea579c7dfa6951b2b4f9234149288f53747",
    "files": [
        {
            "sha": "64c5893b51729d7b51e2d24fea718e1b00bd7269",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 33,
            "deletions": 2,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2f08ea579c7dfa6951b2b4f9234149288f53747/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2f08ea579c7dfa6951b2b4f9234149288f53747/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=e2f08ea579c7dfa6951b2b4f9234149288f53747",
            "patch": "@@ -1976,13 +1976,44 @@ def enable_input_require_grads(self):\n         def make_inputs_require_grads(module, input, output):\n             output.requires_grad_(True)\n \n-        self._require_grads_hook = self.get_input_embeddings().register_forward_hook(make_inputs_require_grads)\n+        hooks = []\n+        seen_modules = set()\n+\n+        for module in self.modules():\n+            if not (isinstance(module, PreTrainedModel) and hasattr(module, \"get_input_embeddings\")):\n+                continue\n+\n+            input_embeddings = module.get_input_embeddings()\n+\n+            if input_embeddings is None:\n+                continue\n+\n+            embedding_id = id(input_embeddings)\n+            if embedding_id in seen_modules:\n+                continue\n+\n+            seen_modules.add(embedding_id)\n+            hooks.append(input_embeddings.register_forward_hook(make_inputs_require_grads))\n+\n+        self._require_grads_hooks = hooks\n+        if hooks:\n+            # for BC\n+            self._require_grads_hook = hooks[0]\n \n     def disable_input_require_grads(self):\n         \"\"\"\n         Removes the `_require_grads_hook`.\n         \"\"\"\n-        self._require_grads_hook.remove()\n+        hooks = getattr(self, \"_require_grads_hooks\", None)\n+        if not hooks:\n+            return\n+\n+        for hook in hooks:\n+            hook.remove()\n+\n+        self._require_grads_hooks = []\n+        if hasattr(self, \"_require_grads_hook\"):\n+            del self._require_grads_hook\n \n     def get_encoder(self, modality: Optional[str] = None):\n         \"\"\""
        },
        {
            "sha": "4107f448717b8892cd19cdefa24e6e5be0f15772",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2f08ea579c7dfa6951b2b4f9234149288f53747/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2f08ea579c7dfa6951b2b4f9234149288f53747/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=e2f08ea579c7dfa6951b2b4f9234149288f53747",
            "patch": "@@ -888,10 +888,12 @@ def __init__(self, config: Blip2QFormerConfig):\n         self.post_init()\n \n     def get_input_embeddings(self):\n-        return self.embeddings.word_embeddings\n+        # The Q-Former operates on embeddings provided by upstream modules (e.g. query tokens or text embeddings).\n+        # It does not own input embeddings itself, so we return `None` to signal that there is nothing to update.\n+        return None\n \n     def set_input_embeddings(self, value):\n-        self.embeddings.word_embeddings = value\n+        raise NotImplementedError(\"Blip2QFormerModel does not own input embeddings and cannot set them.\")\n \n     def get_extended_attention_mask(\n         self,"
        },
        {
            "sha": "1a394de39153f4232f183e9ff49c9d80ba0e529e",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 48,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2f08ea579c7dfa6951b2b4f9234149288f53747/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2f08ea579c7dfa6951b2b4f9234149288f53747/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=e2f08ea579c7dfa6951b2b4f9234149288f53747",
            "patch": "@@ -787,36 +787,6 @@ def __init__(self, config: Idefics2Config):\n \n         self.post_init()\n \n-    def enable_input_require_grads(self):\n-        \"\"\"\n-        Enables the gradients for the input embeddings.\n-\n-        This is useful for lora when using gradient checkpointing.\n-        c.f. https://github.com/huggingface/peft/issues/1402#issuecomment-1913675032\n-\n-        Override to set output.requires_grad = True for both the decoder's and vision model's embeddings.\n-        \"\"\"\n-\n-        def get_lowest_module(module):\n-            if len(list(module.children())) == 0:\n-                # If the module has no children, it is a leaf module (e.g., Linear, Conv2d, etc.)\n-                return module\n-            else:\n-                # Recursively call the function on each child module\n-                return get_lowest_module(list(module.children())[0])\n-\n-        def make_inputs_require_grads(module, input, output):\n-            output.requires_grad_(True)\n-\n-        self._text_require_grads_hook = self.get_input_embeddings().register_forward_hook(make_inputs_require_grads)\n-        self._vision_require_grads_hook = get_lowest_module(self.vision_model).register_forward_hook(\n-            make_inputs_require_grads\n-        )\n-\n-    def disable_input_require_grads(self):\n-        self._text_require_grads_hook.remove()\n-        self._vision_require_grads_hook.remove()\n-\n     def get_input_embeddings(self):\n         return self.text_model.get_input_embeddings()\n \n@@ -1009,24 +979,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def enable_input_require_grads(self):\n-        \"\"\"\n-        Enables the gradients for the input embeddings. This is useful for fine-tuning adapter weights while keeping\n-        the model weights fixed.\n-        \"\"\"\n-\n-        def make_inputs_require_grads(module, input, output):\n-            output.requires_grad_(True)\n-\n-        self._text_require_grads_hook = self.get_input_embeddings().register_forward_hook(make_inputs_require_grads)\n-        self._vision_require_grads_hook = self.model.vision_model.get_input_embeddings().register_forward_hook(\n-            make_inputs_require_grads\n-        )\n-\n-    def disable_input_require_grads(self):\n-        self._text_require_grads_hook.remove()\n-        self._vision_require_grads_hook.remove()\n-\n     def get_input_embeddings(self):\n         return self.model.text_model.get_input_embeddings()\n "
        },
        {
            "sha": "9efb9287d9c6e88a03dacc6fb467af6805665598",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 52,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2f08ea579c7dfa6951b2b4f9234149288f53747/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2f08ea579c7dfa6951b2b4f9234149288f53747/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=e2f08ea579c7dfa6951b2b4f9234149288f53747",
            "patch": "@@ -530,38 +530,6 @@ def __init__(self, config: Idefics3Config):\n \n         self.post_init()\n \n-    # Copied from transformers.models.idefics2.modeling_idefics2.Idefics2Model.enable_input_require_grads\n-    def enable_input_require_grads(self):\n-        \"\"\"\n-        Enables the gradients for the input embeddings.\n-\n-        This is useful for lora when using gradient checkpointing.\n-        c.f. https://github.com/huggingface/peft/issues/1402#issuecomment-1913675032\n-\n-        Override to set output.requires_grad = True for both the decoder's and vision model's embeddings.\n-        \"\"\"\n-\n-        def get_lowest_module(module):\n-            if len(list(module.children())) == 0:\n-                # If the module has no children, it is a leaf module (e.g., Linear, Conv2d, etc.)\n-                return module\n-            else:\n-                # Recursively call the function on each child module\n-                return get_lowest_module(list(module.children())[0])\n-\n-        def make_inputs_require_grads(module, input, output):\n-            output.requires_grad_(True)\n-\n-        self._text_require_grads_hook = self.get_input_embeddings().register_forward_hook(make_inputs_require_grads)\n-        self._vision_require_grads_hook = get_lowest_module(self.vision_model).register_forward_hook(\n-            make_inputs_require_grads\n-        )\n-\n-    # Copied from transformers.models.idefics2.modeling_idefics2.Idefics2Model.disable_input_require_grads\n-    def disable_input_require_grads(self):\n-        self._text_require_grads_hook.remove()\n-        self._vision_require_grads_hook.remove()\n-\n     # Copied from transformers.models.idefics2.modeling_idefics2.Idefics2Model.get_input_embeddings\n     def get_input_embeddings(self):\n         return self.text_model.get_input_embeddings()\n@@ -765,26 +733,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    # Copied from transformers.models.idefics2.modeling_idefics2.Idefics2ForConditionalGeneration.enable_input_require_grads\n-    def enable_input_require_grads(self):\n-        \"\"\"\n-        Enables the gradients for the input embeddings. This is useful for fine-tuning adapter weights while keeping\n-        the model weights fixed.\n-        \"\"\"\n-\n-        def make_inputs_require_grads(module, input, output):\n-            output.requires_grad_(True)\n-\n-        self._text_require_grads_hook = self.get_input_embeddings().register_forward_hook(make_inputs_require_grads)\n-        self._vision_require_grads_hook = self.model.vision_model.get_input_embeddings().register_forward_hook(\n-            make_inputs_require_grads\n-        )\n-\n-    # Copied from transformers.models.idefics2.modeling_idefics2.Idefics2ForConditionalGeneration.disable_input_require_grads\n-    def disable_input_require_grads(self):\n-        self._text_require_grads_hook.remove()\n-        self._vision_require_grads_hook.remove()\n-\n     # Copied from transformers.models.idefics2.modeling_idefics2.Idefics2ForConditionalGeneration.get_input_embeddings\n     def get_input_embeddings(self):\n         return self.model.text_model.get_input_embeddings()"
        },
        {
            "sha": "ef5e846c83fc9c90bbbfae3c176274b4f7dcba0f",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2f08ea579c7dfa6951b2b4f9234149288f53747/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2f08ea579c7dfa6951b2b4f9234149288f53747/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=e2f08ea579c7dfa6951b2b4f9234149288f53747",
            "patch": "@@ -1076,6 +1076,7 @@ def forward(self, x: torch.Tensor) -> torch.Tensor:\n class Qwen2_5OmniVisionEncoder(Qwen2_5OmniPreTrainedModel):\n     config: Qwen2_5OmniVisionEncoderConfig\n     _no_split_modules = [\"Qwen2_5OmniVisionBlock\"]\n+    _input_embed_layer = \"patch_embed\"\n     input_modalities = (\"image\", \"video\")\n \n     def __init__(self, config: Qwen2_5OmniVisionEncoderConfig, *inputs, **kwargs) -> None:"
        },
        {
            "sha": "2d179285d5f9631bfd35793bce4f8e25b296c419",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2f08ea579c7dfa6951b2b4f9234149288f53747/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2f08ea579c7dfa6951b2b4f9234149288f53747/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=e2f08ea579c7dfa6951b2b4f9234149288f53747",
            "patch": "@@ -1920,6 +1920,7 @@ class Qwen2_5OmniVisionEncoder(Qwen2_5_VisionTransformerPretrainedModel):\n     config: Qwen2_5OmniVisionEncoderConfig\n     input_modalities = (\"image\", \"video\")\n     _no_split_modules = [\"Qwen2_5OmniVisionBlock\"]\n+    _input_embed_layer = \"patch_embed\"\n \n     def __init__(self, config: Qwen2_5OmniVisionEncoderConfig, *inputs, **kwargs) -> None:\n         super().__init__(config, *inputs, **kwargs)"
        },
        {
            "sha": "e5d1ca7e194395ef24152ace87503f0f4007e58d",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2f08ea579c7dfa6951b2b4f9234149288f53747/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2f08ea579c7dfa6951b2b4f9234149288f53747/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=e2f08ea579c7dfa6951b2b4f9234149288f53747",
            "patch": "@@ -307,6 +307,7 @@ class Qwen2_5_VLPreTrainedModel(PreTrainedModel):\n class Qwen2_5_VisionTransformerPretrainedModel(Qwen2_5_VLPreTrainedModel):\n     config: Qwen2_5_VLVisionConfig\n     _no_split_modules = [\"Qwen2_5_VLVisionBlock\"]\n+    _input_embed_layer = \"patch_embed\"\n \n     def __init__(self, config, *inputs, **kwargs) -> None:\n         super().__init__(config, *inputs, **kwargs)"
        },
        {
            "sha": "a8d88a73d9bccbf42e88d03acc2a992486f8bd3a",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2f08ea579c7dfa6951b2b4f9234149288f53747/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2f08ea579c7dfa6951b2b4f9234149288f53747/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=e2f08ea579c7dfa6951b2b4f9234149288f53747",
            "patch": "@@ -179,6 +179,7 @@ class Qwen2_5_VLPreTrainedModel(Qwen2VLPreTrainedModel):\n class Qwen2_5_VisionTransformerPretrainedModel(Qwen2_5_VLPreTrainedModel):\n     config: Qwen2_5_VLVisionConfig\n     _no_split_modules = [\"Qwen2_5_VLVisionBlock\"]\n+    _input_embed_layer = \"patch_embed\"\n \n     def __init__(self, config, *inputs, **kwargs) -> None:\n         super().__init__(config, *inputs, **kwargs)"
        },
        {
            "sha": "cbe2848d8f7b75054e58b00efae18271fc8e2155",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2f08ea579c7dfa6951b2b4f9234149288f53747/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2f08ea579c7dfa6951b2b4f9234149288f53747/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=e2f08ea579c7dfa6951b2b4f9234149288f53747",
            "patch": "@@ -671,6 +671,7 @@ class Qwen2VisionTransformerPretrainedModel(Qwen2VLPreTrainedModel):\n     config: Qwen2VLVisionConfig\n     input_modalities = (\"image\", \"video\")\n     _no_split_modules = [\"Qwen2VLVisionBlock\"]\n+    _input_embed_layer = \"patch_embed\"\n \n     def __init__(self, config) -> None:\n         super().__init__(config)"
        },
        {
            "sha": "098cfae41af0e3958003be0e5f5e93b64562dccd",
            "filename": "src/transformers/models/qwen3_omni_moe/modeling_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2f08ea579c7dfa6951b2b4f9234149288f53747/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2f08ea579c7dfa6951b2b4f9234149288f53747/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py?ref=e2f08ea579c7dfa6951b2b4f9234149288f53747",
            "patch": "@@ -686,10 +686,10 @@ def _freeze_parameters(self):\n         self._requires_grad = False\n \n     def get_input_embeddings(self) -> nn.Module:\n-        return self.conv1\n+        return self.conv2d1\n \n-    def set_input_embeddings(self, value: nn.Module):\n-        self.conv1 = value\n+    def set_input_embeddings(self, value):\n+        self.conv2d1 = value\n \n     def _prepare_attention_mask(self, inputs_tensor: torch.Tensor, cu_seqlens: torch.Tensor) -> torch.Tensor:\n         # Flash Attention 2 doesn't need a 4D mask and relies on `cu_seqlens/max_seqlen`"
        },
        {
            "sha": "5760043d59ed4bdb7664594e1f63ff3cfbe2085e",
            "filename": "src/transformers/models/qwen3_omni_moe/modular_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2f08ea579c7dfa6951b2b4f9234149288f53747/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2f08ea579c7dfa6951b2b4f9234149288f53747/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py?ref=e2f08ea579c7dfa6951b2b4f9234149288f53747",
            "patch": "@@ -1194,6 +1194,12 @@ def __init__(self, config: Qwen3OmniMoeAudioEncoderConfig):\n         self.n_window_infer = self.config.n_window_infer\n         self.conv_chunksize = self.config.conv_chunksize\n \n+    def get_input_embeddings(self):\n+        return self.conv2d1\n+\n+    def set_input_embeddings(self, value):\n+        self.conv2d1 = value\n+\n     def forward(\n         self,\n         input_features,"
        },
        {
            "sha": "e85048037c3372ccdf5d9cb2cc61ed6303b9529e",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 48,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2f08ea579c7dfa6951b2b4f9234149288f53747/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2f08ea579c7dfa6951b2b4f9234149288f53747/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=e2f08ea579c7dfa6951b2b4f9234149288f53747",
            "patch": "@@ -470,36 +470,6 @@ def __init__(self, config: SmolVLMConfig):\n \n         self.post_init()\n \n-    def enable_input_require_grads(self):\n-        \"\"\"\n-        Enables the gradients for the input embeddings.\n-\n-        This is useful for lora when using gradient checkpointing.\n-        c.f. https://github.com/huggingface/peft/issues/1402#issuecomment-1913675032\n-\n-        Override to set output.requires_grad = True for both the decoder's and vision model's embeddings.\n-        \"\"\"\n-\n-        def get_lowest_module(module):\n-            if len(list(module.children())) == 0:\n-                # If the module has no children, it is a leaf module (e.g., Linear, Conv2d, etc.)\n-                return module\n-            else:\n-                # Recursively call the function on each child module\n-                return get_lowest_module(list(module.children())[0])\n-\n-        def make_inputs_require_grads(module, input, output):\n-            output.requires_grad_(True)\n-\n-        self._text_require_grads_hook = self.get_input_embeddings().register_forward_hook(make_inputs_require_grads)\n-        self._vision_require_grads_hook = get_lowest_module(self.vision_model).register_forward_hook(\n-            make_inputs_require_grads\n-        )\n-\n-    def disable_input_require_grads(self):\n-        self._text_require_grads_hook.remove()\n-        self._vision_require_grads_hook.remove()\n-\n     def get_input_embeddings(self):\n         return self.text_model.get_input_embeddings()\n \n@@ -748,24 +718,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def enable_input_require_grads(self):\n-        \"\"\"\n-        Enables the gradients for the input embeddings. This is useful for fine-tuning adapter weights while keeping\n-        the model weights fixed.\n-        \"\"\"\n-\n-        def make_inputs_require_grads(module, input, output):\n-            output.requires_grad_(True)\n-\n-        self._text_require_grads_hook = self.get_input_embeddings().register_forward_hook(make_inputs_require_grads)\n-        self._vision_require_grads_hook = self.model.vision_model.get_input_embeddings().register_forward_hook(\n-            make_inputs_require_grads\n-        )\n-\n-    def disable_input_require_grads(self):\n-        self._text_require_grads_hook.remove()\n-        self._vision_require_grads_hook.remove()\n-\n     def get_input_embeddings(self):\n         return self.model.text_model.get_input_embeddings()\n "
        },
        {
            "sha": "f3689acebcc5d5438c8982899c6748a2cd89853d",
            "filename": "src/transformers/models/timm_wrapper/modeling_timm_wrapper.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2f08ea579c7dfa6951b2b4f9234149288f53747/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2f08ea579c7dfa6951b2b4f9234149288f53747/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py?ref=e2f08ea579c7dfa6951b2b4f9234149288f53747",
            "patch": "@@ -150,6 +150,13 @@ def __init__(self, config: TimmWrapperConfig):\n         self.timm_model = _create_timm_model_with_error_handling(config, num_classes=0, **extra_init_kwargs)\n         self.post_init()\n \n+    def get_input_embeddings(self):\n+        # Vision backbones from timm do not expose token embeddings, so there is nothing to return.\n+        return None\n+\n+    def set_input_embeddings(self, value):\n+        raise NotImplementedError(\"TimmWrapperModel does not own token embeddings and cannot set them.\")\n+\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "2570b34cf0ebf8c8d2eecfdbf89c4ccb91fc02d1",
            "filename": "tests/models/bart/test_modeling_bart.py",
            "status": "modified",
            "additions": 15,
            "deletions": 4,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2f08ea579c7dfa6951b2b4f9234149288f53747/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2f08ea579c7dfa6951b2b4f9234149288f53747/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py?ref=e2f08ea579c7dfa6951b2b4f9234149288f53747",
            "patch": "@@ -478,23 +478,34 @@ def test_inputs_embeds(self):\n             with torch.no_grad():\n                 model(**inputs)[0]\n \n-    @unittest.skip(\"Bart no longer always uses self.shared so not working.\")\n     def test_input_embeddings_support_forward_hook(self):\n         # Make sure that registering hooks on the input embeddings are indeed called\n         # in forward. This is necessary for gradient checkpointing in PEFT, see also #41821.\n+        # For BART with tied embeddings, encoder and decoder have separate embedding modules,\n+        # so we need to check that hooks on those modules are called during forward.\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs()\n         for model_class in self.all_model_classes:\n             model = model_class(config)\n             model.to(torch_device)\n             model.eval()\n \n-            hook = unittest.mock.MagicMock(return_value=None)\n-            model.get_input_embeddings().register_forward_hook(hook)\n+            hooks = []\n+            base_model = model.model if hasattr(model, \"model\") else model\n+\n+            if hasattr(base_model, \"encoder\") and hasattr(base_model.encoder, \"embed_tokens\"):\n+                hook = unittest.mock.MagicMock(return_value=None)\n+                base_model.encoder.embed_tokens.register_forward_hook(hook)\n+                hooks.append(hook)\n+            if hasattr(base_model, \"decoder\") and hasattr(base_model.decoder, \"embed_tokens\"):\n+                hook = unittest.mock.MagicMock(return_value=None)\n+                base_model.decoder.embed_tokens.register_forward_hook(hook)\n+                hooks.append(hook)\n \n             inputs = copy.deepcopy(self._prepare_for_class(inputs_dict, model_class))\n             model(**inputs)\n \n-            self.assertGreater(hook.call_count, 0)\n+            total_calls = sum(hook.call_count for hook in hooks)\n+            self.assertGreater(total_calls, 0, f\"Hooks on embeddings were not called for {model_class.__name__}\")\n \n     @require_torch_fp16\n     def test_generate_fp16(self):"
        },
        {
            "sha": "5f90620ca7030537607735ed0d251d258156584e",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 56,
            "deletions": 0,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2f08ea579c7dfa6951b2b4f9234149288f53747/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2f08ea579c7dfa6951b2b4f9234149288f53747/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=e2f08ea579c7dfa6951b2b4f9234149288f53747",
            "patch": "@@ -362,6 +362,62 @@ def test_sdpa_can_dispatch_on_flash(self):\n     def test_multi_gpu_data_parallel_forward(self):\n         pass\n \n+    def test_enable_input_require_grads_with_gradient_checkpointing(self):\n+        if not self.model_tester.is_training:\n+            self.skipTest(reason=\"ModelTester not in training mode\")\n+\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.use_cache = False\n+        config.return_dict = True\n+\n+        for model_class in self.all_model_classes:\n+            if not model_class.supports_gradient_checkpointing:\n+                continue\n+\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": True})\n+            model.enable_input_require_grads()\n+            model.train()\n+\n+            for parameter in model.parameters():\n+                parameter.requires_grad = False\n+\n+            vision_module = None\n+            if hasattr(model, \"visual\"):\n+                vision_module = model.visual\n+            elif hasattr(model, \"model\") and hasattr(model.model, \"visual\"):\n+                vision_module = model.model.visual\n+\n+            if vision_module is None:\n+                continue\n+\n+            target_linear = vision_module.blocks[0].attn.qkv\n+            target_linear.weight.requires_grad = True\n+            if target_linear.bias is not None:\n+                target_linear.bias.requires_grad = True\n+\n+            inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n+            outputs = model(**inputs)\n+\n+            if hasattr(outputs, \"loss\") and outputs.loss is not None:\n+                loss = outputs.loss\n+            else:\n+                logits = outputs.logits if hasattr(outputs, \"logits\") else outputs[0]\n+                loss = logits.sum()\n+\n+            loss.backward()\n+\n+            self.assertIsNotNone(\n+                target_linear.weight.grad,\n+                f\"qkv weights should receive gradients when enable_input_require_grads is used with gradient checkpointing. Model: {model_class.__name__}\",\n+            )\n+            self.assertGreater(\n+                target_linear.weight.grad.abs().sum().item(),\n+                0,\n+                f\"qkv weights should have non-zero gradients when enable_input_require_grads is used with gradient checkpointing. Model: {model_class.__name__}\",\n+            )\n+\n \n @require_torch\n class Qwen2VLIntegrationTest(unittest.TestCase):"
        }
    ],
    "stats": {
        "total": 288,
        "additions": 129,
        "deletions": 159
    }
}