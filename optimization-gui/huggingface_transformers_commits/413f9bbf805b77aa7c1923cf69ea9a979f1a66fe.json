{
    "author": "manueldeprada",
    "message": "Fixes #37219 : RecurrentGemma crashes for inputs longer than sliding window length (#37613)\n\n* fix: RecurrentGemma crashes during inference for inputs longer than sliding window width\n\n* fix recurrentgemma tests; add long test bigger than context window",
    "sha": "413f9bbf805b77aa7c1923cf69ea9a979f1a66fe",
    "files": [
        {
            "sha": "fcf2402ec9b1ea039a307ba7fce454a0330dc8d5",
            "filename": "src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/413f9bbf805b77aa7c1923cf69ea9a979f1a66fe/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/413f9bbf805b77aa7c1923cf69ea9a979f1a66fe/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py?ref=413f9bbf805b77aa7c1923cf69ea9a979f1a66fe",
            "patch": "@@ -767,6 +767,8 @@ def _update_causal_mask(self, attention_mask, input_tensor, cache_position):\n         if attention_mask is not None:\n             causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n             if attention_mask.dim() == 2:\n+                # Crop the attention mask to the target length.\n+                attention_mask = attention_mask[:, -target_length:]\n                 mask_length = attention_mask.shape[-1]\n                 padding_mask = causal_mask[..., :mask_length].eq(0.0) * attention_mask[:, None, None, :].eq(0.0)\n                 causal_mask[..., :mask_length] = causal_mask[..., :mask_length].masked_fill(padding_mask, min_dtype)"
        },
        {
            "sha": "640c8cfa8049ff2a6ca009e106e13fadcdc24016",
            "filename": "tests/models/recurrent_gemma/test_modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 24,
            "deletions": 13,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/413f9bbf805b77aa7c1923cf69ea9a979f1a66fe/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/413f9bbf805b77aa7c1923cf69ea9a979f1a66fe/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py?ref=413f9bbf805b77aa7c1923cf69ea9a979f1a66fe",
            "patch": "@@ -278,11 +278,12 @@ def test_initialization(self):\n @slow\n class RecurrentGemmaIntegrationTest(unittest.TestCase):\n     input_text = [\"Hello I am doing\", \"Hi today\"]\n+    input_long_text = ['<bos><s>Marseille, France (CNN)The French prosecutor leading an investigation into the crash of Germanwings Flight 9525 insisted Wednesday that he was not aware of any video footage from on board the plane. Marseille prosecutor Brice Robin told CNN that \"so far no videos were used in the crash investigation.\" He added, \"A person who has such a video needs to immediately give it to the investigators.\" Robin\\'s comments follow claims by two magazines, German daily Bild and French Paris Match, of a cell phone video showing the harrowing final seconds from on board Germanwings Flight 9525 as it crashed into the French Alps. All 150 on board were killed. Paris Match and Bild reported that the video was recovered from a phone at the wreckage site. The two publications described the supposed video, but did not post it on their websites. The publications said that they watched the video, which was found by a source close to the investigation. \"One can hear cries of \\'My God\\' in several languages,\" Paris Match reported. \"Metallic banging can also be heard more than three times, perhaps of the pilot trying to open the cockpit door with a heavy object.  Towards the end, after a heavy shake, stronger than the others, the screaming intensifies. Then nothing.\" \"It is a very disturbing scene,\" said Julian Reichelt, editor-in-chief of Bild online. An official with France\\'s accident investigation agency, the BEA, said the agency is not aware of any such video. Lt. Col.']  # fmt: skip\n     model_id = \"google/recurrentgemma-2b\"\n \n     @require_read_token\n     def test_2b_generate(self):\n-        EXPECTED_TEXTS = ['Hello I am doing a project on the topic of \"The impact of the internet on the society\" and I am looking for some information on the topic. I am looking for some information on the impact of the internet on the society. I am looking for some information on the impact of the internet on the society. I am looking for some', 'Hi today is a very good day for you. You will be able to do all the work you want to do. You will be able to do all the work you want to do. You will be able to do all the work you want to do. You will be able to do all the work you want to do.']  # fmt: skip\n+        EXPECTED_TEXTS = ['Hello I am doing a project on the topic of \"The impact of the internet on the society\" and I am looking for some information on the topic. I am looking for some information on the impact of the internet on the society. I am looking for some information on the impact of the internet on the society. I am looking for some', 'Hi today is a new app that allows you to make money by watching videos.\\n\\nThe app is very simple to use and you can earn money by watching videos.\\n\\nThe app is available for both Android and iOS devices and you can download it from the Google Play Store or the App Store.\\n\\nOnce you have downloaded the app']  # fmt: skip\n         model = AutoModelForCausalLM.from_pretrained(self.model_id, low_cpu_mem_usage=True).to(torch_device)\n \n         tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n@@ -296,7 +297,7 @@ def test_2b_generate(self):\n         self.assertEqual(output_text, EXPECTED_TEXTS)\n \n         tokenizer.padding_side = \"left\"\n-        EXPECTED_TEXTS = ['Hello I am doing a project on the topic of \"The impact of the internet on the society\" and I am looking for some information on the topic. I am looking for some information on the impact of the internet on the society. I am looking for some information on the impact of the internet on the society. I am looking for some', 'Hi today I am going to share with you the best <strong><em>free online video editing software</em></strong>.\\n\\n<h2><strong>Best Free Online Video Editing Software</strong></h2>\\n\\n<strong>1.</strong> <strong>Wondershare Filmora</strong>\\n\\nWondershare Filmora is a free online video editing software that is used to edit videos.']  # fmt: skip\n+        EXPECTED_TEXTS = ['Hello I am doing a project on the topic of \"The impact of the internet on the society\" and I am looking for some information on the topic. I am looking for some information on the impact of the internet on the society. I am looking for some information on the impact of the internet on the society. I am looking for some', 'Hi today Iâ€™m going to show you how to make a simple and easy to make a <strong>DIY</strong> <strong>DIY</strong> <strong>DIY</strong> <strong>DIY</strong> <strong>DIY</strong> <strong>DIY</strong> <strong>DIY</strong> <strong>DIY</strong> <strong>DIY</strong> <strong>DIY</strong> <strong>DIY</strong> <strong>DIY']  # fmt: skip\n \n         inputs = tokenizer(self.input_text, return_tensors=\"pt\", padding=True).to(torch_device)\n         output = model.generate(**inputs, max_new_tokens=64, do_sample=False)\n@@ -316,7 +317,7 @@ def test_2b_generate(self):\n     @require_read_token\n     def test_2b_sample(self):\n         set_seed(0)\n-        EXPECTED_TEXT = ['Where is Paris ?\\n\\nAnswer this question \"yes\" or \"no\": Could a person pass out in subzero temperatures?\\n\\nFor the sentence below, underline the pronoun in parentheses that agrees with its antecedent.\\n\\nExample 1. Mary and Pam will have the opportunity to prove (herself, $\\\\underline{\\\\text{themselves}}$) at the concert.\\n\\nThe waiters and the manager at the restaurant will do <em>(his, their)</em> best to assist you.\\n\\nA vocabulary word appears in italics in the short passage below. Think about how the word is used. Then write a definition for the vocabulary word.\\n\\nAfter a one-hour $']  # fmt: skip\n+        EXPECTED_TEXT = ['Where is Paris ?\\n\\nChoose the word or phrase that is closest in meaning to the word in capital letters.\\n\\nREDEEM\\n(A) sort out\\n(B) think over\\n(C) turn in\\n(D) take back\\n\\nWrite the correct word in the space next to each definition. Use each word only once.\\n\\nto badly damage\\n\\nOn the lines provided below, write <em>P</em> if the underlined word group is a phrase and <em>NP</em> if it is not a phrase. Example $\\\\underline{\\\\text{P}}$ 1. We have finally discovered the secret $\\\\underline{\\\\text{of delicious pizza. }}$']  # fmt: skip\n         model = AutoModelForCausalLM.from_pretrained(self.model_id).to(torch_device)\n \n         tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n@@ -329,13 +330,13 @@ def test_2b_sample(self):\n     @require_bitsandbytes\n     @require_read_token\n     def test_model_2b_8bit(self):\n-        EXPECTED_TEXTS = ['<bos>Hello I am doing a project on the topic of \"The impact of the internet on the society\" and I am looking', \"<bos>Hi today<pad><pad> I'm going to show you how to make a simple and easy to use <strong><em><u>\"]  # fmt: skip\n+        EXPECTED_TEXTS = ['Hello I am doing a project on the topic of \"The impact of the internet on the society\" and I am looking', \"Hi today I'm going to show you how to make a simple and easy to make a simple and easy\"]  # fmt: skip\n \n         model = AutoModelForCausalLM.from_pretrained(\n             \"gg-hf/recurrent-gemma-2b-hf\", device_map={\"\": torch_device}, load_in_8bit=True, torch_dtype=torch.bfloat16\n         )\n \n-        tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n+        tokenizer = AutoTokenizer.from_pretrained(self.model_id, padding_side=\"left\")\n         inputs = tokenizer(self.input_text, return_tensors=\"pt\", padding=True).to(torch_device)\n \n         output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n@@ -345,18 +346,28 @@ def test_model_2b_8bit(self):\n \n     @require_read_token\n     def test_long_context(self):\n-        input_text = [\n-            '<bos><s>Marseille, France (CNN)The French prosecutor leading an investigation into the crash of Germanwings Flight 9525 insisted Wednesday that he was not aware of any video footage from on board the plane. Marseille prosecutor Brice Robin told CNN that \"so far no videos were used in the crash investigation.\" He added, \"A person who has such a video needs to immediately give it to the investigators.\" Robin\\'s comments follow claims by two magazines, German daily Bild and French Paris Match, of a cell phone video showing the harrowing final seconds from on board Germanwings Flight 9525 as it crashed into the French Alps. All 150 on board were killed. Paris Match and Bild reported that the video was recovered from a phone at the wreckage site. The two publications described the supposed video, but did not post it on their websites. The publications said that they watched the video, which was found by a source close to the investigation. \"One can hear cries of \\'My God\\' in several languages,\" Paris Match reported. \"Metallic banging can also be heard more than three times, perhaps of the pilot trying to open the cockpit door with a heavy object.  Towards the end, after a heavy shake, stronger than the others, the screaming intensifies. Then nothing.\" \"It is a very disturbing scene,\" said Julian Reichelt, editor-in-chief of Bild online. An official with France\\'s accident investigation agency, the BEA, said the agency is not aware of any such video. Lt. Col.'\n-        ]\n-        EXPECTED_GENERATION = [\n-            ' Jean-Paul Delannoy told CNN that the BEA is \"not aware of any video footage that could have been taken on board the plane.\" \"We are not aware of any video footage that could have been taken on board the plane,\" Delannoy said. \"We are not aware of any video footage that could'\n-        ]\n+        EXPECTED_GENERATION = [' Jean-Paul Delannoy told CNN that the BEA is \"not aware of any video footage that could have been taken on board the plane.\" He added that the BEA is \"not aware of any video footage that could have been taken on board the plane.\" The BEA is the French equivalent of the National Transportation Safety Board']  # fmt: skip\n \n         model = AutoModelForCausalLM.from_pretrained(\n             self.model_id, low_cpu_mem_usage=True, torch_dtype=torch.float16\n         ).to(torch_device)\n-        tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n-        inputs = tokenizer(input_text, return_tensors=\"pt\").to(torch_device)\n+        tokenizer = AutoTokenizer.from_pretrained(self.model_id, padding_side=\"left\")\n+        inputs = tokenizer(self.input_long_text, return_tensors=\"pt\").to(torch_device)\n+        output = model.generate(**inputs, max_new_tokens=64, do_sample=False)\n+        output_text = tokenizer.batch_decode(output[:, inputs.input_ids.shape[1] :], skip_special_tokens=True)\n+        print(output_text)\n+        self.assertEqual(output_text, EXPECTED_GENERATION)\n+\n+    @require_read_token\n+    def test_longer_than_window(self):\n+        EXPECTED_GENERATION = [\" Robin's comments follow claims by two magazines, German daily Bild and French Paris Match, of a cell phone video showing the harrowing final seconds from on board Germanwings Flight 9525 as it crashed into the French Alps. All 150 on board were killed. Paris Match and Bild reported that the\"]  # fmt: skip\n+\n+        model = AutoModelForCausalLM.from_pretrained(\n+            self.model_id, low_cpu_mem_usage=True, torch_dtype=torch.float16\n+        ).to(torch_device)\n+        model.config.attention_window_size = 256  # Make the attention window size shorter than the current prompt\n+        tokenizer = AutoTokenizer.from_pretrained(self.model_id, padding_side=\"left\")\n+        inputs = tokenizer(self.input_long_text, return_tensors=\"pt\").to(torch_device)\n         output = model.generate(**inputs, max_new_tokens=64, do_sample=False)\n         output_text = tokenizer.batch_decode(output[:, inputs.input_ids.shape[1] :], skip_special_tokens=True)\n         self.assertEqual(output_text, EXPECTED_GENERATION)"
        }
    ],
    "stats": {
        "total": 39,
        "additions": 26,
        "deletions": 13
    }
}