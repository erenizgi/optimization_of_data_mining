{
    "author": "arkhamHack",
    "message": "Superpoint fast image processor (#37804)\n\n* feat: superpoint fast image processor\n\n* fix: reran fast cli command to generate fast config\n\n* feat: updated test cases\n\n* fix: removed old model add\n\n* fix: format fix\n\n* Update src/transformers/models/superpoint/image_processing_superpoint_fast.py\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\n\n* fix: ported to torch and made requested changes\n\n* fix: removed changes to init\n\n* fix: init fix\n\n* fix: init format fix\n\n* fixed testcases and ported to torch\n\n* fix: format fixes\n\n* failed\ntest case fix\n\n* fix superpoint fast\n\n* fix docstring\n\n---------\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\nCo-authored-by: yonigozlan <yoni.gozlan@huggingface.co>",
    "sha": "c353f2bb5e29a52a15831d3fbe565f7b7fff3a08",
    "files": [
        {
            "sha": "27ab95ac674d5391af81d94da0514bd398f393fe",
            "filename": "docs/source/en/model_doc/superpoint.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c353f2bb5e29a52a15831d3fbe565f7b7fff3a08/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperpoint.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c353f2bb5e29a52a15831d3fbe565f7b7fff3a08/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperpoint.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperpoint.md?ref=c353f2bb5e29a52a15831d3fbe565f7b7fff3a08",
            "patch": "@@ -130,6 +130,11 @@ processed_outputs = processor.post_process_keypoint_detection(outputs, [image_si\n \n [[autodoc]] SuperPointImageProcessor\n \n+- preprocess\n+\n+## SuperPointImageProcessorFast\n+\n+[[autodoc]] SuperPointImageProcessorFast\n - preprocess\n - post_process_keypoint_detection\n "
        },
        {
            "sha": "527b25c10ace11b41be5680d03a390007474a86e",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c353f2bb5e29a52a15831d3fbe565f7b7fff3a08/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c353f2bb5e29a52a15831d3fbe565f7b7fff3a08/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=c353f2bb5e29a52a15831d3fbe565f7b7fff3a08",
            "patch": "@@ -162,6 +162,13 @@\n             (\"siglip2\", (\"Siglip2ImageProcessor\", \"Siglip2ImageProcessorFast\")),\n             (\"smolvlm\", (\"SmolVLMImageProcessor\", \"SmolVLMImageProcessorFast\")),\n             (\"superglue\", (\"SuperGlueImageProcessor\",)),\n+            (\n+                \"superpoint\",\n+                (\n+                    \"SuperPointImageProcessor\",\n+                    \"SuperPointImageProcessorFast\",\n+                ),\n+            ),\n             (\"swiftformer\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"swin\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"swin2sr\", (\"Swin2SRImageProcessor\", \"Swin2SRImageProcessorFast\")),"
        },
        {
            "sha": "ccec260fa5e7d33818153d651d833d43226224e7",
            "filename": "src/transformers/models/superpoint/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c353f2bb5e29a52a15831d3fbe565f7b7fff3a08/src%2Ftransformers%2Fmodels%2Fsuperpoint%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c353f2bb5e29a52a15831d3fbe565f7b7fff3a08/src%2Ftransformers%2Fmodels%2Fsuperpoint%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperpoint%2F__init__.py?ref=c353f2bb5e29a52a15831d3fbe565f7b7fff3a08",
            "patch": "@@ -20,6 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_superpoint import *\n     from .image_processing_superpoint import *\n+    from .image_processing_superpoint_fast import *\n     from .modeling_superpoint import *\n else:\n     import sys"
        },
        {
            "sha": "e6167160c29dc800828ce8ed664b5aefae11e8ff",
            "filename": "src/transformers/models/superpoint/image_processing_superpoint.py",
            "status": "modified",
            "additions": 11,
            "deletions": 1,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/c353f2bb5e29a52a15831d3fbe565f7b7fff3a08/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c353f2bb5e29a52a15831d3fbe565f7b7fff3a08/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint.py?ref=c353f2bb5e29a52a15831d3fbe565f7b7fff3a08",
            "patch": "@@ -23,6 +23,7 @@\n from ...image_utils import (\n     ChannelDimension,\n     ImageInput,\n+    PILImageResampling,\n     infer_channel_dimension_format,\n     is_scaled_image,\n     make_list_of_images,\n@@ -107,6 +108,8 @@ class SuperPointImageProcessor(BaseImageProcessor):\n         size (`dict[str, int]` *optional*, defaults to `{\"height\": 480, \"width\": 640}`):\n             Resolution of the output image after `resize` is applied. Only has an effect if `do_resize` is set to\n             `True`. Can be overridden by `size` in the `preprocess` method.\n+        resample (`Resampling`, *optional*, defaults to `2`):\n+            Resampling filter to use if resizing the image. Can be overridden by `resample` in the `preprocess` method.\n         do_rescale (`bool`, *optional*, defaults to `True`):\n             Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by `do_rescale` in\n             the `preprocess` method.\n@@ -123,6 +126,7 @@ def __init__(\n         self,\n         do_resize: bool = True,\n         size: Optional[dict[str, int]] = None,\n+        resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n         rescale_factor: float = 1 / 255,\n         do_grayscale: bool = False,\n@@ -134,6 +138,7 @@ def __init__(\n \n         self.do_resize = do_resize\n         self.size = size\n+        self.resample = resample\n         self.do_rescale = do_rescale\n         self.rescale_factor = rescale_factor\n         self.do_grayscale = do_grayscale\n@@ -182,6 +187,7 @@ def preprocess(\n         images,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n+        resample: PILImageResampling = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_grayscale: Optional[bool] = None,\n@@ -231,6 +237,7 @@ def preprocess(\n         \"\"\"\n \n         do_resize = do_resize if do_resize is not None else self.do_resize\n+        resample = resample if resample is not None else self.resample\n         do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n         rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n         do_grayscale = do_grayscale if do_grayscale is not None else self.do_grayscale\n@@ -266,7 +273,10 @@ def preprocess(\n             input_data_format = infer_channel_dimension_format(images[0])\n \n         if do_resize:\n-            images = [self.resize(image=image, size=size, input_data_format=input_data_format) for image in images]\n+            images = [\n+                self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n+                for image in images\n+            ]\n \n         if do_rescale:\n             images = ["
        },
        {
            "sha": "e70bb397ff6a8a468f4dbb6f803f152d796f1f92",
            "filename": "src/transformers/models/superpoint/image_processing_superpoint_fast.py",
            "status": "added",
            "additions": 182,
            "deletions": 0,
            "changes": 182,
            "blob_url": "https://github.com/huggingface/transformers/blob/c353f2bb5e29a52a15831d3fbe565f7b7fff3a08/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c353f2bb5e29a52a15831d3fbe565f7b7fff3a08/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint_fast.py?ref=c353f2bb5e29a52a15831d3fbe565f7b7fff3a08",
            "patch": "@@ -0,0 +1,182 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for Superpoint.\"\"\"\n+\n+from typing import TYPE_CHECKING, Optional, Union\n+\n+from ...image_processing_utils import BatchFeature\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+    DefaultFastImageProcessorKwargs,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import (\n+    PILImageResampling,\n+    SizeDict,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if TYPE_CHECKING:\n+    from .modeling_superpoint import SuperPointKeypointDescriptionOutput\n+\n+if is_torchvision_v2_available():\n+    import torchvision.transforms.v2.functional as F\n+elif is_torchvision_available():\n+    import torchvision.transforms.functional as F\n+\n+\n+def is_grayscale(\n+    image: \"torch.Tensor\",\n+):\n+    \"\"\"Checks if an image is grayscale (all RGB channels are identical).\"\"\"\n+    if image.ndim < 3 or image.shape[0 if image.ndim == 3 else 1] == 1:\n+        return True\n+    return torch.all(image[..., 0, :, :] == image[..., 1, :, :]) and torch.all(\n+        image[..., 1, :, :] == image[..., 2, :, :]\n+    )\n+\n+\n+class SuperPointFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    r\"\"\"\n+    do_grayscale (`bool`, *optional*, defaults to `True`):\n+        Whether to convert the image to grayscale. Can be overridden by `do_grayscale` in the `preprocess` method.\n+    \"\"\"\n+\n+    do_grayscale: Optional[bool] = True\n+\n+\n+def convert_to_grayscale(\n+    image: \"torch.Tensor\",\n+) -> \"torch.Tensor\":\n+    \"\"\"\n+    Converts an image to grayscale format using the NTSC formula. Only support torch.Tensor.\n+\n+    This function is supposed to return a 1-channel image, but it returns a 3-channel image with the same value in each\n+    channel, because of an issue that is discussed in :\n+    https://github.com/huggingface/transformers/pull/25786#issuecomment-1730176446\n+\n+    Args:\n+        image (torch.Tensor):\n+            The image to convert.\n+    \"\"\"\n+    if is_grayscale(image):\n+        return image\n+    return F.rgb_to_grayscale(image, num_output_channels=3)\n+\n+\n+@auto_docstring\n+class SuperPointImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BILINEAR\n+    size = {\"height\": 480, \"width\": 640}\n+    default_to_square = False\n+    do_resize = True\n+    do_rescale = True\n+    rescale_factor = 1 / 255\n+    do_normalize = None\n+    valid_kwargs = SuperPointFastImageProcessorKwargs\n+\n+    def __init__(self, **kwargs: Unpack[SuperPointFastImageProcessorKwargs]):\n+        super().__init__(**kwargs)\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        size: Union[dict[str, int], SizeDict],\n+        rescale_factor: float,\n+        do_rescale: bool,\n+        do_resize: bool,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_grayscale: bool,\n+        disable_grouping: bool,\n+        return_tensors: Union[str, TensorType],\n+        **kwargs,\n+    ) -> BatchFeature:\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_grayscale:\n+                stacked_images = convert_to_grayscale(stacked_images)\n+            if do_resize:\n+                stacked_images = self.resize(stacked_images, size=size, interpolation=interpolation)\n+            if do_rescale:\n+                stacked_images = self.rescale(stacked_images, rescale_factor)\n+            processed_images_grouped[shape] = stacked_images\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n+        return BatchFeature(data={\"pixel_values\": processed_images})\n+\n+    def post_process_keypoint_detection(\n+        self, outputs: \"SuperPointKeypointDescriptionOutput\", target_sizes: Union[TensorType, list[tuple]]\n+    ) -> list[dict[str, \"torch.Tensor\"]]:\n+        \"\"\"\n+        Converts the raw output of [`SuperPointForKeypointDetection`] into lists of keypoints, scores and descriptors\n+        with coordinates absolute to the original image sizes.\n+\n+        Args:\n+            outputs ([`SuperPointKeypointDescriptionOutput`]):\n+                Raw outputs of the model containing keypoints in a relative (x, y) format, with scores and descriptors.\n+            target_sizes (`torch.Tensor` or `List[Tuple[int, int]]`):\n+                Tensor of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the target size\n+                `(height, width)` of each image in the batch. This must be the original\n+                image size (before any processing).\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the keypoints in absolute format according\n+            to target_sizes, scores and descriptors for an image in the batch as predicted by the model.\n+        \"\"\"\n+        if len(outputs.mask) != len(target_sizes):\n+            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the mask\")\n+\n+        if isinstance(target_sizes, list):\n+            image_sizes = torch.tensor(target_sizes, device=outputs.mask.device)\n+        else:\n+            if target_sizes.shape[1] != 2:\n+                raise ValueError(\n+                    \"Each element of target_sizes must contain the size (h, w) of each image of the batch\"\n+                )\n+            image_sizes = target_sizes\n+\n+        # Flip the image sizes to (width, height) and convert keypoints to absolute coordinates\n+        image_sizes = torch.flip(image_sizes, [1])\n+        masked_keypoints = outputs.keypoints * image_sizes[:, None]\n+\n+        # Convert masked_keypoints to int\n+        masked_keypoints = masked_keypoints.to(torch.int32)\n+\n+        results = []\n+        for image_mask, keypoints, scores, descriptors in zip(\n+            outputs.mask, masked_keypoints, outputs.scores, outputs.descriptors\n+        ):\n+            indices = torch.nonzero(image_mask).squeeze(1)\n+            keypoints = keypoints[indices]\n+            scores = scores[indices]\n+            descriptors = descriptors[indices]\n+            results.append({\"keypoints\": keypoints, \"scores\": scores, \"descriptors\": descriptors})\n+\n+        return results\n+\n+\n+__all__ = [\"SuperPointImageProcessorFast\"]"
        },
        {
            "sha": "242e3702bedefec11de7683460533274829e8c21",
            "filename": "tests/models/superpoint/test_image_processing_superpoint.py",
            "status": "modified",
            "additions": 46,
            "deletions": 33,
            "changes": 79,
            "blob_url": "https://github.com/huggingface/transformers/blob/c353f2bb5e29a52a15831d3fbe565f7b7fff3a08/tests%2Fmodels%2Fsuperpoint%2Ftest_image_processing_superpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c353f2bb5e29a52a15831d3fbe565f7b7fff3a08/tests%2Fmodels%2Fsuperpoint%2Ftest_image_processing_superpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsuperpoint%2Ftest_image_processing_superpoint.py?ref=c353f2bb5e29a52a15831d3fbe565f7b7fff3a08",
            "patch": "@@ -16,12 +16,9 @@\n import numpy as np\n \n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n-from ...test_image_processing_common import (\n-    ImageProcessingTestMixin,\n-    prepare_image_inputs,\n-)\n+from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n \n if is_torch_available():\n@@ -32,6 +29,9 @@\n if is_vision_available():\n     from transformers import SuperPointImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import SuperPointImageProcessorFast\n+\n \n class SuperPointImageProcessingTester:\n     def __init__(\n@@ -100,6 +100,7 @@ def prepare_keypoint_detection_output(self, pixel_values):\n @require_vision\n class SuperPointImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = SuperPointImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = SuperPointImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self) -> None:\n         super().setUp()\n@@ -110,40 +111,44 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processing(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n-        self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n-        self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n-        self.assertTrue(hasattr(image_processing, \"do_grayscale\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n+            self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n+            self.assertTrue(hasattr(image_processing, \"do_grayscale\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        self.assertEqual(image_processor.size, {\"height\": 480, \"width\": 640})\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"height\": 480, \"width\": 640})\n \n-        image_processor = self.image_processing_class.from_dict(\n-            self.image_processor_dict, size={\"height\": 42, \"width\": 42}\n-        )\n-        self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n+            image_processor = self.image_processing_class.from_dict(\n+                self.image_processor_dict, size={\"height\": 42, \"width\": 42}\n+            )\n+            self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n \n     @unittest.skip(reason=\"SuperPointImageProcessor is always supposed to return a grayscaled image\")\n     def test_call_numpy_4_channels(self):\n         pass\n \n     def test_input_image_properly_converted_to_grayscale(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        image_inputs = self.image_processor_tester.prepare_image_inputs()\n-        pre_processed_images = image_processor.preprocess(image_inputs)\n-        for image in pre_processed_images[\"pixel_values\"]:\n-            self.assertTrue(np.all(image[0, ...] == image[1, ...]) and np.all(image[1, ...] == image[2, ...]))\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            image_inputs = self.image_processor_tester.prepare_image_inputs()\n+            pre_processed_images = image_processor.preprocess(image_inputs)\n+            for image in pre_processed_images[\"pixel_values\"]:\n+                if isinstance(image, torch.Tensor):\n+                    self.assertTrue(\n+                        torch.all(image[0, ...] == image[1, ...]).item()\n+                        and torch.all(image[1, ...] == image[2, ...]).item()\n+                    )\n+                else:\n+                    self.assertTrue(np.all(image[0, ...] == image[1, ...]) and np.all(image[1, ...] == image[2, ...]))\n \n     @require_torch\n     def test_post_processing_keypoint_detection(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        image_inputs = self.image_processor_tester.prepare_image_inputs()\n-        pre_processed_images = image_processor.preprocess(image_inputs, return_tensors=\"pt\")\n-        outputs = self.image_processor_tester.prepare_keypoint_detection_output(**pre_processed_images)\n-\n         def check_post_processed_output(post_processed_output, image_size):\n             for post_processed_output, image_size in zip(post_processed_output, image_size):\n                 self.assertTrue(\"keypoints\" in post_processed_output)\n@@ -157,12 +162,20 @@ def check_post_processed_output(post_processed_output, image_size):\n                 self.assertTrue(all_below_image_size)\n                 self.assertTrue(all_above_zero)\n \n-        tuple_image_sizes = [(image.size[0], image.size[1]) for image in image_inputs]\n-        tuple_post_processed_outputs = image_processor.post_process_keypoint_detection(outputs, tuple_image_sizes)\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            image_inputs = self.image_processor_tester.prepare_image_inputs()\n+            pre_processed_images = image_processor.preprocess(image_inputs, return_tensors=\"pt\")\n+            outputs = self.image_processor_tester.prepare_keypoint_detection_output(**pre_processed_images)\n+\n+            tuple_image_sizes = [(image.size[0], image.size[1]) for image in image_inputs]\n+            tuple_post_processed_outputs = image_processor.post_process_keypoint_detection(outputs, tuple_image_sizes)\n \n-        check_post_processed_output(tuple_post_processed_outputs, tuple_image_sizes)\n+            check_post_processed_output(tuple_post_processed_outputs, tuple_image_sizes)\n \n-        tensor_image_sizes = torch.tensor([image.size for image in image_inputs]).flip(1)\n-        tensor_post_processed_outputs = image_processor.post_process_keypoint_detection(outputs, tensor_image_sizes)\n+            tensor_image_sizes = torch.tensor([image.size for image in image_inputs]).flip(1)\n+            tensor_post_processed_outputs = image_processor.post_process_keypoint_detection(\n+                outputs, tensor_image_sizes\n+            )\n \n-        check_post_processed_output(tensor_post_processed_outputs, tensor_image_sizes)\n+            check_post_processed_output(tensor_post_processed_outputs, tensor_image_sizes)"
        }
    ],
    "stats": {
        "total": 286,
        "additions": 252,
        "deletions": 34
    }
}