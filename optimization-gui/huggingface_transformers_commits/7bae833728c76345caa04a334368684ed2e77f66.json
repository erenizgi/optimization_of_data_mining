{
    "author": "ArthurZucker",
    "message": "properly fix and RUN_SLOW (#33965)\n\n* properly fix and RUN_SLOW\r\n\r\n* lots of models were affected\r\n\r\n* fix-copies\r\n\r\n* more fixes",
    "sha": "7bae833728c76345caa04a334368684ed2e77f66",
    "files": [
        {
            "sha": "7e39a5f0f1182ff55efd55a8c774d64713093570",
            "filename": "src/transformers/models/altclip/modeling_altclip.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/7bae833728c76345caa04a334368684ed2e77f66/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7bae833728c76345caa04a334368684ed2e77f66/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py?ref=7bae833728c76345caa04a334368684ed2e77f66",
            "patch": "@@ -1024,15 +1024,15 @@ def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width:\n         \"\"\"\n \n         num_patches = embeddings.shape[1] - 1\n-        self.position_embeddings = self.position_embedding.weight.unsqueeze(0)\n-        num_positions = self.position_embeddings.shape[1] - 1\n+        position_embedding = self.position_embedding.weight.unsqueeze(0)\n+        num_positions = position_embedding.shape[1] - 1\n \n         # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n         if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n-            return self.position_embeddings\n+            return self.position_embedding(self.position_ids)\n \n-        class_pos_embed = self.position_embeddings[:, :1]\n-        patch_pos_embed = self.position_embeddings[:, 1:]\n+        class_pos_embed = position_embedding[:, :1]\n+        patch_pos_embed = position_embedding[:, 1:]\n \n         dim = embeddings.shape[-1]\n "
        },
        {
            "sha": "9a900acf500c1ed51b53b0b052fc6cffc89bcc93",
            "filename": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/7bae833728c76345caa04a334368684ed2e77f66/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7bae833728c76345caa04a334368684ed2e77f66/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py?ref=7bae833728c76345caa04a334368684ed2e77f66",
            "patch": "@@ -295,15 +295,15 @@ def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width:\n         \"\"\"\n \n         num_patches = embeddings.shape[1] - 1\n-        self.position_embeddings = self.position_embedding.weight.unsqueeze(0)\n-        num_positions = self.position_embeddings.shape[1] - 1\n+        position_embedding = self.position_embedding.weight.unsqueeze(0)\n+        num_positions = position_embedding.shape[1] - 1\n \n         # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n         if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n-            return self.position_embeddings\n+            return self.position_embedding(self.position_ids)\n \n-        class_pos_embed = self.position_embeddings[:, :1]\n-        patch_pos_embed = self.position_embeddings[:, 1:]\n+        class_pos_embed = position_embedding[:, :1]\n+        patch_pos_embed = position_embedding[:, 1:]\n \n         dim = embeddings.shape[-1]\n "
        },
        {
            "sha": "dffa9028af4ffe51a3c550e7bd4a90409816721e",
            "filename": "src/transformers/models/chinese_clip/modeling_chinese_clip.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/7bae833728c76345caa04a334368684ed2e77f66/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7bae833728c76345caa04a334368684ed2e77f66/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py?ref=7bae833728c76345caa04a334368684ed2e77f66",
            "patch": "@@ -200,15 +200,15 @@ def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width:\n         \"\"\"\n \n         num_patches = embeddings.shape[1] - 1\n-        self.position_embeddings = self.position_embedding.weight.unsqueeze(0)\n-        num_positions = self.position_embeddings.shape[1] - 1\n+        position_embedding = self.position_embedding.weight.unsqueeze(0)\n+        num_positions = position_embedding.shape[1] - 1\n \n         # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n         if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n-            return self.position_embeddings\n+            return self.position_embedding(self.position_ids)\n \n-        class_pos_embed = self.position_embeddings[:, :1]\n-        patch_pos_embed = self.position_embeddings[:, 1:]\n+        class_pos_embed = position_embedding[:, :1]\n+        patch_pos_embed = position_embedding[:, 1:]\n \n         dim = embeddings.shape[-1]\n "
        },
        {
            "sha": "f946f828eec63910daadc2f7f490e09763d5cd0a",
            "filename": "src/transformers/models/clip/modeling_clip.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/7bae833728c76345caa04a334368684ed2e77f66/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7bae833728c76345caa04a334368684ed2e77f66/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py?ref=7bae833728c76345caa04a334368684ed2e77f66",
            "patch": "@@ -208,15 +208,15 @@ def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width:\n         \"\"\"\n \n         num_patches = embeddings.shape[1] - 1\n-        self.position_embeddings = self.position_embedding.weight.unsqueeze(0)\n-        num_positions = self.position_embeddings.shape[1] - 1\n+        position_embedding = self.position_embedding.weight.unsqueeze(0)\n+        num_positions = position_embedding.shape[1] - 1\n \n         # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n         if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n-            return self.position_embeddings\n+            return self.position_embedding(self.position_ids)\n \n-        class_pos_embed = self.position_embeddings[:, :1]\n-        patch_pos_embed = self.position_embeddings[:, 1:]\n+        class_pos_embed = position_embedding[:, :1]\n+        patch_pos_embed = position_embedding[:, 1:]\n \n         dim = embeddings.shape[-1]\n "
        },
        {
            "sha": "8ff7f1cd96a0d2ff534bb808164a771e473180ba",
            "filename": "src/transformers/models/clipseg/modeling_clipseg.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/7bae833728c76345caa04a334368684ed2e77f66/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7bae833728c76345caa04a334368684ed2e77f66/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py?ref=7bae833728c76345caa04a334368684ed2e77f66",
            "patch": "@@ -175,15 +175,15 @@ def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width:\n         \"\"\"\n \n         num_patches = embeddings.shape[1] - 1\n-        self.position_embeddings = self.position_embedding.weight.unsqueeze(0)\n-        num_positions = self.position_embeddings.shape[1] - 1\n+        position_embedding = self.position_embedding.weight.unsqueeze(0)\n+        num_positions = position_embedding.shape[1] - 1\n \n         # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n         if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n-            return self.position_embeddings\n+            return self.position_embedding(self.position_ids)\n \n-        class_pos_embed = self.position_embeddings[:, :1]\n-        patch_pos_embed = self.position_embeddings[:, 1:]\n+        class_pos_embed = position_embedding[:, :1]\n+        patch_pos_embed = position_embedding[:, 1:]\n \n         dim = embeddings.shape[-1]\n "
        },
        {
            "sha": "c7f9ceafe19452e7232cc9ad4878feea3727d6d9",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/7bae833728c76345caa04a334368684ed2e77f66/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7bae833728c76345caa04a334368684ed2e77f66/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=7bae833728c76345caa04a334368684ed2e77f66",
            "patch": "@@ -650,15 +650,15 @@ def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width:\n         \"\"\"\n \n         num_patches = embeddings.shape[1] - 1\n-        self.position_embeddings = self.position_embedding.weight.unsqueeze(0)\n-        num_positions = self.position_embeddings.shape[1] - 1\n+        position_embedding = self.position_embedding.weight.unsqueeze(0)\n+        num_positions = position_embedding.shape[1] - 1\n \n         # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n         if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n-            return self.position_embeddings\n+            return self.position_embedding(self.position_ids)\n \n-        class_pos_embed = self.position_embeddings[:, :1]\n-        patch_pos_embed = self.position_embeddings[:, 1:]\n+        class_pos_embed = position_embedding[:, :1]\n+        patch_pos_embed = position_embedding[:, 1:]\n \n         dim = embeddings.shape[-1]\n "
        },
        {
            "sha": "7674e29db6b9157fe60a19bff0ffeea7891415f4",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/7bae833728c76345caa04a334368684ed2e77f66/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7bae833728c76345caa04a334368684ed2e77f66/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=7bae833728c76345caa04a334368684ed2e77f66",
            "patch": "@@ -417,15 +417,15 @@ def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width:\n         \"\"\"\n \n         num_patches = embeddings.shape[1] - 1\n-        self.position_embeddings = self.position_embedding.weight.unsqueeze(0)\n-        num_positions = self.position_embeddings.shape[1] - 1\n+        position_embedding = self.position_embedding.weight.unsqueeze(0)\n+        num_positions = position_embedding.shape[1] - 1\n \n         # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n         if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n-            return self.position_embeddings\n+            return self.position_embedding(self.position_ids)\n \n-        class_pos_embed = self.position_embeddings[:, :1]\n-        patch_pos_embed = self.position_embeddings[:, 1:]\n+        class_pos_embed = position_embedding[:, :1]\n+        patch_pos_embed = position_embedding[:, 1:]\n \n         dim = embeddings.shape[-1]\n "
        },
        {
            "sha": "507e0768a226ef5e7d03d0af9f6c6d2e9c98e582",
            "filename": "src/transformers/models/siglip/modeling_siglip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7bae833728c76345caa04a334368684ed2e77f66/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7bae833728c76345caa04a334368684ed2e77f66/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py?ref=7bae833728c76345caa04a334368684ed2e77f66",
            "patch": "@@ -279,13 +279,13 @@ def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width:\n         \"\"\"\n \n         num_patches = embeddings.shape[1]\n-        num_positions = self.position_embedding.shape[1]\n+        num_positions = self.position_embedding.weight.shape[0]\n \n         # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n         if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n             return self.position_embedding(self.position_ids)\n \n-        patch_pos_embed = self.position_embedding\n+        patch_pos_embed = self.position_embedding.weight.unsqueeze(0)\n \n         dim = embeddings.shape[-1]\n "
        },
        {
            "sha": "25208c43a85a6ce0947de0efee3c6e64a90855b7",
            "filename": "src/transformers/models/x_clip/modeling_x_clip.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/7bae833728c76345caa04a334368684ed2e77f66/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7bae833728c76345caa04a334368684ed2e77f66/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py?ref=7bae833728c76345caa04a334368684ed2e77f66",
            "patch": "@@ -133,15 +133,15 @@ def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width:\n         \"\"\"\n \n         num_patches = embeddings.shape[1] - 1\n-        self.position_embeddings = self.position_embedding.weight.unsqueeze(0)\n-        num_positions = self.position_embeddings.shape[1] - 1\n+        position_embedding = self.position_embedding.weight.unsqueeze(0)\n+        num_positions = position_embedding.shape[1] - 1\n \n         # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n         if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n-            return self.position_embeddings\n+            return self.position_embedding(self.position_ids)\n \n-        class_pos_embed = self.position_embeddings[:, :1]\n-        patch_pos_embed = self.position_embeddings[:, 1:]\n+        class_pos_embed = position_embedding[:, :1]\n+        patch_pos_embed = position_embedding[:, 1:]\n \n         dim = embeddings.shape[-1]\n "
        }
    ],
    "stats": {
        "total": 84,
        "additions": 42,
        "deletions": 42
    }
}