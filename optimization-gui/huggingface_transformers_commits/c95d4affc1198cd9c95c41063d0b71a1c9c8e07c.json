{
    "author": "zucchini-nlp",
    "message": "remove reference to TF models from docs (#42443)\n\n* delete TF ref from model docs\n\n* and in pipelines\n\n* dots",
    "sha": "c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
    "files": [
        {
            "sha": "5c9671f8a50f532eb0778420b673a2d20c02de84",
            "filename": "src/transformers/models/albert/configuration_albert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Falbert%2Fconfiguration_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Falbert%2Fconfiguration_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fconfiguration_albert.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -20,7 +20,7 @@\n \n class AlbertConfig(PreTrainedConfig):\n     r\"\"\"\n-    This is the configuration class to store the configuration of a [`AlbertModel`] or a [`TFAlbertModel`]. It is used\n+    This is the configuration class to store the configuration of a [`AlbertModel`]. It is used\n     to instantiate an ALBERT model according to the specified arguments, defining the model architecture. Instantiating\n     a configuration with the defaults will yield a similar configuration to that of the ALBERT\n     [albert/albert-xxlarge-v2](https://huggingface.co/albert/albert-xxlarge-v2) architecture.\n@@ -31,7 +31,7 @@ class AlbertConfig(PreTrainedConfig):\n     Args:\n         vocab_size (`int`, *optional*, defaults to 30000):\n             Vocabulary size of the ALBERT model. Defines the number of different tokens that can be represented by the\n-            `inputs_ids` passed when calling [`AlbertModel`] or [`TFAlbertModel`].\n+            `inputs_ids` passed when calling [`AlbertModel`].\n         embedding_size (`int`, *optional*, defaults to 128):\n             Dimensionality of vocabulary embeddings.\n         hidden_size (`int`, *optional*, defaults to 4096):\n@@ -57,7 +57,7 @@ class AlbertConfig(PreTrainedConfig):\n             The maximum sequence length that this model might ever be used with. Typically set this to something large\n             (e.g., 512 or 1024 or 2048).\n         type_vocab_size (`int`, *optional*, defaults to 2):\n-            The vocabulary size of the `token_type_ids` passed when calling [`AlbertModel`] or [`TFAlbertModel`].\n+            The vocabulary size of the `token_type_ids` passed when calling [`AlbertModel`].\n         initializer_range (`float`, *optional*, defaults to 0.02):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-12):"
        },
        {
            "sha": "39b738697f65d7499d62f936a481863a0d8dc184",
            "filename": "src/transformers/models/bart/configuration_bart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fbart%2Fconfiguration_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fbart%2Fconfiguration_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fconfiguration_bart.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -35,7 +35,7 @@ class BartConfig(PreTrainedConfig):\n     Args:\n         vocab_size (`int`, *optional*, defaults to 50265):\n             Vocabulary size of the BART model. Defines the number of different tokens that can be represented by the\n-            `inputs_ids` passed when calling [`BartModel`] or [`TFBartModel`].\n+            `inputs_ids` passed when calling [`BartModel`].\n         d_model (`int`, *optional*, defaults to 1024):\n             Dimensionality of the layers and the pooler layer.\n         encoder_layers (`int`, *optional*, defaults to 12):"
        },
        {
            "sha": "0eef2319f1494a42c9f3f1a3912fe4aedaa8563e",
            "filename": "src/transformers/models/bert/configuration_bert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fbert%2Fconfiguration_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fbert%2Fconfiguration_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fconfiguration_bert.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -24,7 +24,7 @@\n \n class BertConfig(PreTrainedConfig):\n     r\"\"\"\n-    This is the configuration class to store the configuration of a [`BertModel`] or a [`TFBertModel`]. It is used to\n+    This is the configuration class to store the configuration of a [`BertModel`]. It is used to\n     instantiate a BERT model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the BERT\n     [google-bert/bert-base-uncased](https://huggingface.co/google-bert/bert-base-uncased) architecture.\n@@ -36,7 +36,7 @@ class BertConfig(PreTrainedConfig):\n     Args:\n         vocab_size (`int`, *optional*, defaults to 30522):\n             Vocabulary size of the BERT model. Defines the number of different tokens that can be represented by the\n-            `inputs_ids` passed when calling [`BertModel`] or [`TFBertModel`].\n+            `inputs_ids` passed when calling [`BertModel`].\n         hidden_size (`int`, *optional*, defaults to 768):\n             Dimensionality of the encoder layers and the pooler layer.\n         num_hidden_layers (`int`, *optional*, defaults to 12):\n@@ -56,7 +56,7 @@ class BertConfig(PreTrainedConfig):\n             The maximum sequence length that this model might ever be used with. Typically set this to something large\n             just in case (e.g., 512 or 1024 or 2048).\n         type_vocab_size (`int`, *optional*, defaults to 2):\n-            The vocabulary size of the `token_type_ids` passed when calling [`BertModel`] or [`TFBertModel`].\n+            The vocabulary size of the `token_type_ids` passed when calling [`BertModel`].\n         initializer_range (`float`, *optional*, defaults to 0.02):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-12):"
        },
        {
            "sha": "d75f33de386f76d161e427190d8eea5508cfaa82",
            "filename": "src/transformers/models/blenderbot/configuration_blenderbot.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fconfiguration_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fconfiguration_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fconfiguration_blenderbot.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -35,7 +35,7 @@ class BlenderbotConfig(PreTrainedConfig):\n     Args:\n         vocab_size (`int`, *optional*, defaults to 50265):\n             Vocabulary size of the Blenderbot model. Defines the number of different tokens that can be represented by\n-            the `inputs_ids` passed when calling [`BlenderbotModel`] or [`TFBlenderbotModel`].\n+            the `inputs_ids` passed when calling [`BlenderbotModel`].\n         d_model (`int`, *optional*, defaults to 1024):\n             Dimensionality of the layers and the pooler layer.\n         encoder_layers (`int`, *optional*, defaults to 12):"
        },
        {
            "sha": "41c0ccf0eb4bd8504df9667aeae893a50ab5f842",
            "filename": "src/transformers/models/blenderbot_small/configuration_blenderbot_small.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fconfiguration_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fconfiguration_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fconfiguration_blenderbot_small.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -35,7 +35,7 @@ class BlenderbotSmallConfig(PreTrainedConfig):\n     Args:\n         vocab_size (`int`, *optional*, defaults to 50265):\n             Vocabulary size of the BlenderbotSmall model. Defines the number of different tokens that can be\n-            represented by the `inputs_ids` passed when calling [`BlenderbotSmallModel`] or [`TFBlenderbotSmallModel`].\n+            represented by the `inputs_ids` passed when calling [`BlenderbotSmallModel`].\n         d_model (`int`, *optional*, defaults to 512):\n             Dimensionality of the layers and the pooler layer.\n         encoder_layers (`int`, *optional*, defaults to 8):"
        },
        {
            "sha": "5c9d6cb4ab1721d7f57b69b0043170a50860964e",
            "filename": "src/transformers/models/bros/configuration_bros.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fbros%2Fconfiguration_bros.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fbros%2Fconfiguration_bros.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbros%2Fconfiguration_bros.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -23,7 +23,7 @@\n \n class BrosConfig(PreTrainedConfig):\n     r\"\"\"\n-    This is the configuration class to store the configuration of a [`BrosModel`] or a [`TFBrosModel`]. It is used to\n+    This is the configuration class to store the configuration of a [`BrosModel`]. It is used to\n     instantiate a Bros model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the Bros\n     [jinho8345/bros-base-uncased](https://huggingface.co/jinho8345/bros-base-uncased) architecture.\n@@ -34,7 +34,7 @@ class BrosConfig(PreTrainedConfig):\n     Args:\n         vocab_size (`int`, *optional*, defaults to 30522):\n             Vocabulary size of the Bros model. Defines the number of different tokens that can be represented by the\n-            `inputs_ids` passed when calling [`BrosModel`] or [`TFBrosModel`].\n+            `inputs_ids` passed when calling [`BrosModel`].\n         hidden_size (`int`, *optional*, defaults to 768):\n             Dimensionality of the encoder layers and the pooler layer.\n         num_hidden_layers (`int`, *optional*, defaults to 12):\n@@ -54,7 +54,7 @@ class BrosConfig(PreTrainedConfig):\n             The maximum sequence length that this model might ever be used with. Typically set this to something large\n             just in case (e.g., 512 or 1024 or 2048).\n         type_vocab_size (`int`, *optional*, defaults to 2):\n-            The vocabulary size of the `token_type_ids` passed when calling [`BrosModel`] or [`TFBrosModel`].\n+            The vocabulary size of the `token_type_ids` passed when calling [`BrosModel`].\n         initializer_range (`float`, *optional*, defaults to 0.02):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-12):"
        },
        {
            "sha": "3cbdda6c4223f3d491ef2e1a347f0133c688ab48",
            "filename": "src/transformers/models/camembert/configuration_camembert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fcamembert%2Fconfiguration_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fcamembert%2Fconfiguration_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fconfiguration_camembert.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -24,7 +24,7 @@\n \n class CamembertConfig(PreTrainedConfig):\n     \"\"\"\n-    This is the configuration class to store the configuration of a [`CamembertModel`] or a [`TFCamembertModel`]. It is\n+    This is the configuration class to store the configuration of a [`CamembertModel`]. It is\n     used to instantiate a Camembert model according to the specified arguments, defining the model architecture.\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the Camembert\n     [almanach/camembert-base](https://huggingface.co/almanach/camembert-base) architecture.\n@@ -36,7 +36,7 @@ class CamembertConfig(PreTrainedConfig):\n     Args:\n         vocab_size (`int`, *optional*, defaults to 30522):\n             Vocabulary size of the BERT model. Defines the number of different tokens that can be represented by the\n-            `inputs_ids` passed when calling [`CamembertModel`] or [`TFCamembertModel`].\n+            `inputs_ids` passed when calling [`CamembertModel`].\n         hidden_size (`int`, *optional*, defaults to 768):\n             Dimensionality of the encoder layers and the pooler layer.\n         num_hidden_layers (`int`, *optional*, defaults to 12):\n@@ -56,7 +56,7 @@ class CamembertConfig(PreTrainedConfig):\n             The maximum sequence length that this model might ever be used with. Typically set this to something large\n             just in case (e.g., 512 or 1024 or 2048).\n         type_vocab_size (`int`, *optional*, defaults to 2):\n-            The vocabulary size of the `token_type_ids` passed when calling [`CamembertModel`] or [`TFCamembertModel`].\n+            The vocabulary size of the `token_type_ids` passed when calling [`CamembertModel`].\n         initializer_range (`float`, *optional*, defaults to 0.02):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-12):"
        },
        {
            "sha": "514d3d9dadea3ace031753b085c344b2f53867dc",
            "filename": "src/transformers/models/convbert/configuration_convbert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fconvbert%2Fconfiguration_convbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fconvbert%2Fconfiguration_convbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2Fconfiguration_convbert.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -35,7 +35,7 @@ class ConvBertConfig(PreTrainedConfig):\n     Args:\n         vocab_size (`int`, *optional*, defaults to 30522):\n             Vocabulary size of the ConvBERT model. Defines the number of different tokens that can be represented by\n-            the `inputs_ids` passed when calling [`ConvBertModel`] or [`TFConvBertModel`].\n+            the `inputs_ids` passed when calling [`ConvBertModel`].\n         hidden_size (`int`, *optional*, defaults to 768):\n             Dimensionality of the encoder layers and the pooler layer.\n         num_hidden_layers (`int`, *optional*, defaults to 12):\n@@ -55,7 +55,7 @@ class ConvBertConfig(PreTrainedConfig):\n             The maximum sequence length that this model might ever be used with. Typically set this to something large\n             just in case (e.g., 512 or 1024 or 2048).\n         type_vocab_size (`int`, *optional*, defaults to 2):\n-            The vocabulary size of the `token_type_ids` passed when calling [`ConvBertModel`] or [`TFConvBertModel`].\n+            The vocabulary size of the `token_type_ids` passed when calling [`ConvBertModel`].\n         initializer_range (`float`, *optional*, defaults to 0.02):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-12):"
        },
        {
            "sha": "4803f2e95f98515f795954243c634e2f9aea246c",
            "filename": "src/transformers/models/ctrl/configuration_ctrl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fctrl%2Fconfiguration_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fctrl%2Fconfiguration_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Fconfiguration_ctrl.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -23,7 +23,7 @@\n \n class CTRLConfig(PreTrainedConfig):\n     \"\"\"\n-    This is the configuration class to store the configuration of a [`CTRLModel`] or a [`TFCTRLModel`]. It is used to\n+    This is the configuration class to store the configuration of a [`CTRLModel`]. It is used to\n     instantiate a CTRL model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the\n     [Salesforce/ctrl](https://huggingface.co/Salesforce/ctrl) architecture from SalesForce.\n@@ -34,7 +34,7 @@ class CTRLConfig(PreTrainedConfig):\n     Args:\n         vocab_size (`int`, *optional*, defaults to 246534):\n             Vocabulary size of the CTRL model. Defines the number of different tokens that can be represented by the\n-            `inputs_ids` passed when calling [`CTRLModel`] or [`TFCTRLModel`].\n+            `inputs_ids` passed when calling [`CTRLModel`].\n         n_positions (`int`, *optional*, defaults to 256):\n             The maximum sequence length that this model might ever be used with. Typically set this to something large\n             just in case (e.g., 512 or 1024 or 2048)."
        },
        {
            "sha": "3d677f3fedf069e87823f000a2f5caf0ee698a7d",
            "filename": "src/transformers/models/data2vec/configuration_data2vec_audio.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconfiguration_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconfiguration_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconfiguration_data2vec_audio.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -37,7 +37,7 @@ class Data2VecAudioConfig(PreTrainedConfig):\n     Args:\n         vocab_size (`int`, *optional*, defaults to 32):\n             Vocabulary size of the Data2VecAudio model. Defines the number of different tokens that can be represented\n-            by the `inputs_ids` passed when calling [`Data2VecAudioModel`] or [`TFData2VecAudioModel`]. Vocabulary size\n+            by the `inputs_ids` passed when calling [`Data2VecAudioModel`]. Vocabulary size\n             of the model. Defines the different tokens that can be represented by the *inputs_ids* passed to the\n             forward method of [`Data2VecAudioModel`].\n         hidden_size (`int`, *optional*, defaults to 768):"
        },
        {
            "sha": "715e66e4fb932b14943ebf69827c878776dd64b1",
            "filename": "src/transformers/models/deberta/configuration_deberta.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fdeberta%2Fconfiguration_deberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fdeberta%2Fconfiguration_deberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta%2Fconfiguration_deberta.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -23,7 +23,7 @@\n \n class DebertaConfig(PreTrainedConfig):\n     r\"\"\"\n-    This is the configuration class to store the configuration of a [`DebertaModel`] or a [`TFDebertaModel`]. It is\n+    This is the configuration class to store the configuration of a [`DebertaModel`]. It is\n     used to instantiate a DeBERTa model according to the specified arguments, defining the model architecture.\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the DeBERTa\n     [microsoft/deberta-base](https://huggingface.co/microsoft/deberta-base) architecture.\n@@ -34,7 +34,7 @@ class DebertaConfig(PreTrainedConfig):\n     Arguments:\n         vocab_size (`int`, *optional*, defaults to 50265):\n             Vocabulary size of the DeBERTa model. Defines the number of different tokens that can be represented by the\n-            `inputs_ids` passed when calling [`DebertaModel`] or [`TFDebertaModel`].\n+            `inputs_ids` passed when calling [`DebertaModel`].\n         hidden_size (`int`, *optional*, defaults to 768):\n             Dimensionality of the encoder layers and the pooler layer.\n         num_hidden_layers (`int`, *optional*, defaults to 12):\n@@ -55,7 +55,7 @@ class DebertaConfig(PreTrainedConfig):\n             The maximum sequence length that this model might ever be used with. Typically set this to something large\n             just in case (e.g., 512 or 1024 or 2048).\n         type_vocab_size (`int`, *optional*, defaults to 0):\n-            The vocabulary size of the `token_type_ids` passed when calling [`DebertaModel`] or [`TFDebertaModel`].\n+            The vocabulary size of the `token_type_ids` passed when calling [`DebertaModel`].\n         initializer_range (`float`, *optional*, defaults to 0.02):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-12):"
        },
        {
            "sha": "95aa5507320eb7ce18f3e305212808a6e0780b30",
            "filename": "src/transformers/models/deberta_v2/configuration_deberta_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fconfiguration_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fconfiguration_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fconfiguration_deberta_v2.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -55,7 +55,7 @@ class DebertaV2Config(PreTrainedConfig):\n             The maximum sequence length that this model might ever be used with. Typically set this to something large\n             just in case (e.g., 512 or 1024 or 2048).\n         type_vocab_size (`int`, *optional*, defaults to 0):\n-            The vocabulary size of the `token_type_ids` passed when calling [`DebertaModel`] or [`TFDebertaModel`].\n+            The vocabulary size of the `token_type_ids` passed when calling [`DebertaModel`].\n         initializer_range (`float`, *optional*, defaults to 0.02):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-7):"
        },
        {
            "sha": "6cfeb52d9b54fd308c96a91f7cc24fc31002ed34",
            "filename": "src/transformers/models/distilbert/configuration_distilbert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fconfiguration_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fconfiguration_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fconfiguration_distilbert.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -23,7 +23,7 @@\n \n class DistilBertConfig(PreTrainedConfig):\n     r\"\"\"\n-    This is the configuration class to store the configuration of a [`DistilBertModel`] or a [`TFDistilBertModel`]. It\n+    This is the configuration class to store the configuration of a [`DistilBertModel`]. It\n     is used to instantiate a DistilBERT model according to the specified arguments, defining the model architecture.\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the DistilBERT\n     [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased) architecture.\n@@ -34,7 +34,7 @@ class DistilBertConfig(PreTrainedConfig):\n     Args:\n         vocab_size (`int`, *optional*, defaults to 30522):\n             Vocabulary size of the DistilBERT model. Defines the number of different tokens that can be represented by\n-            the `inputs_ids` passed when calling [`DistilBertModel`] or [`TFDistilBertModel`].\n+            the `inputs_ids` passed when calling [`DistilBertModel`].\n         max_position_embeddings (`int`, *optional*, defaults to 512):\n             The maximum sequence length that this model might ever be used with. Typically set this to something large\n             just in case (e.g., 512 or 1024 or 2048)."
        },
        {
            "sha": "986169a4c8318b7579ccdb373f0fc69d9da36f11",
            "filename": "src/transformers/models/electra/configuration_electra.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Felectra%2Fconfiguration_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Felectra%2Fconfiguration_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fconfiguration_electra.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -24,7 +24,7 @@\n \n class ElectraConfig(PreTrainedConfig):\n     r\"\"\"\n-    This is the configuration class to store the configuration of a [`ElectraModel`] or a [`TFElectraModel`]. It is\n+    This is the configuration class to store the configuration of a [`ElectraModel`]. It is\n     used to instantiate a ELECTRA model according to the specified arguments, defining the model architecture.\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the ELECTRA\n     [google/electra-small-discriminator](https://huggingface.co/google/electra-small-discriminator) architecture.\n@@ -36,7 +36,7 @@ class ElectraConfig(PreTrainedConfig):\n     Args:\n         vocab_size (`int`, *optional*, defaults to 30522):\n             Vocabulary size of the ELECTRA model. Defines the number of different tokens that can be represented by the\n-            `inputs_ids` passed when calling [`ElectraModel`] or [`TFElectraModel`].\n+            `inputs_ids` passed when calling [`ElectraModel`].\n         embedding_size (`int`, *optional*, defaults to 128):\n             Dimensionality of the encoder layers and the pooler layer.\n         hidden_size (`int`, *optional*, defaults to 256):\n@@ -58,7 +58,7 @@ class ElectraConfig(PreTrainedConfig):\n             The maximum sequence length that this model might ever be used with. Typically set this to something large\n             just in case (e.g., 512 or 1024 or 2048).\n         type_vocab_size (`int`, *optional*, defaults to 2):\n-            The vocabulary size of the `token_type_ids` passed when calling [`ElectraModel`] or [`TFElectraModel`].\n+            The vocabulary size of the `token_type_ids` passed when calling [`ElectraModel`].\n         initializer_range (`float`, *optional*, defaults to 0.02):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-12):"
        },
        {
            "sha": "801bc2edd361e2bef17739e95d25383696d0ae61",
            "filename": "src/transformers/models/ernie/configuration_ernie.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fernie%2Fconfiguration_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fernie%2Fconfiguration_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fconfiguration_ernie.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -24,7 +24,7 @@\n \n class ErnieConfig(PreTrainedConfig):\n     r\"\"\"\n-    This is the configuration class to store the configuration of a [`ErnieModel`] or a [`TFErnieModel`]. It is used to\n+    This is the configuration class to store the configuration of a [`ErnieModel`]. It is used to\n     instantiate a ERNIE model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the ERNIE\n     [nghuyong/ernie-3.0-base-zh](https://huggingface.co/nghuyong/ernie-3.0-base-zh) architecture.\n@@ -36,7 +36,7 @@ class ErnieConfig(PreTrainedConfig):\n     Args:\n         vocab_size (`int`, *optional*, defaults to 30522):\n             Vocabulary size of the ERNIE model. Defines the number of different tokens that can be represented by the\n-            `inputs_ids` passed when calling [`ErnieModel`] or [`TFErnieModel`].\n+            `inputs_ids` passed when calling [`ErnieModel`].\n         hidden_size (`int`, *optional*, defaults to 768):\n             Dimensionality of the encoder layers and the pooler layer.\n         num_hidden_layers (`int`, *optional*, defaults to 12):\n@@ -56,7 +56,7 @@ class ErnieConfig(PreTrainedConfig):\n             The maximum sequence length that this model might ever be used with. Typically set this to something large\n             just in case (e.g., 512 or 1024 or 2048).\n         type_vocab_size (`int`, *optional*, defaults to 2):\n-            The vocabulary size of the `token_type_ids` passed when calling [`ErnieModel`] or [`TFErnieModel`].\n+            The vocabulary size of the `token_type_ids` passed when calling [`ErnieModel`].\n         task_type_vocab_size (`int`, *optional*, defaults to 3):\n             The vocabulary size of the `task_type_ids` for ERNIE2.0/ERNIE3.0 model\n         use_task_id (`bool`, *optional*, defaults to `False`):"
        },
        {
            "sha": "b5ff15d8bc463ccec3a369c7c0749ba78aa51c09",
            "filename": "src/transformers/models/flaubert/configuration_flaubert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fflaubert%2Fconfiguration_flaubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fflaubert%2Fconfiguration_flaubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflaubert%2Fconfiguration_flaubert.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -23,7 +23,7 @@\n \n class FlaubertConfig(PreTrainedConfig):\n     \"\"\"\n-    This is the configuration class to store the configuration of a [`FlaubertModel`] or a [`TFFlaubertModel`]. It is\n+    This is the configuration class to store the configuration of a [`FlaubertModel`]. It is\n     used to instantiate a FlauBERT model according to the specified arguments, defining the model architecture.\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the FlauBERT\n     [flaubert/flaubert_base_uncased](https://huggingface.co/flaubert/flaubert_base_uncased) architecture.\n@@ -40,7 +40,7 @@ class FlaubertConfig(PreTrainedConfig):\n             Structured Dropout. ICLR 2020)\n         vocab_size (`int`, *optional*, defaults to 30145):\n             Vocabulary size of the FlauBERT model. Defines the number of different tokens that can be represented by\n-            the `inputs_ids` passed when calling [`FlaubertModel`] or [`TFFlaubertModel`].\n+            the `inputs_ids` passed when calling [`FlaubertModel`].\n         emb_dim (`int`, *optional*, defaults to 2048):\n             Dimensionality of the encoder layers and the pooler layer.\n         n_layer (`int`, *optional*, defaults to 12):"
        },
        {
            "sha": "77b07fcd15bcad632193a72a63b93dd9fffbf4d5",
            "filename": "src/transformers/models/fnet/configuration_fnet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Ffnet%2Fconfiguration_fnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Ffnet%2Fconfiguration_fnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffnet%2Fconfiguration_fnet.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -35,7 +35,7 @@ class FNetConfig(PreTrainedConfig):\n     Args:\n         vocab_size (`int`, *optional*, defaults to 32000):\n             Vocabulary size of the FNet model. Defines the number of different tokens that can be represented by the\n-            `inputs_ids` passed when calling [`FNetModel`] or [`TFFNetModel`].\n+            `inputs_ids` passed when calling [`FNetModel`].\n         hidden_size (`int`, *optional*, defaults to 768):\n             Dimension of the encoder layers and the pooler layer.\n         num_hidden_layers (`int`, *optional*, defaults to 12):\n@@ -51,7 +51,7 @@ class FNetConfig(PreTrainedConfig):\n             The maximum sequence length that this model might ever be used with. Typically set this to something large\n             just in case (e.g., 512 or 1024 or 2048).\n         type_vocab_size (`int`, *optional*, defaults to 4):\n-            The vocabulary size of the `token_type_ids` passed when calling [`FNetModel`] or [`TFFNetModel`].\n+            The vocabulary size of the `token_type_ids` passed when calling [`FNetModel`].\n         initializer_range (`float`, *optional*, defaults to 0.02):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-12):"
        },
        {
            "sha": "85fe33b7ec2e2da7a85792975b034bb5a9d67afa",
            "filename": "src/transformers/models/funnel/configuration_funnel.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Ffunnel%2Fconfiguration_funnel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Ffunnel%2Fconfiguration_funnel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffunnel%2Fconfiguration_funnel.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -23,7 +23,7 @@\n \n class FunnelConfig(PreTrainedConfig):\n     r\"\"\"\n-    This is the configuration class to store the configuration of a [`FunnelModel`] or a [`TFBertModel`]. It is used to\n+    This is the configuration class to store the configuration of a [`FunnelModel`]. It is used to\n     instantiate a Funnel Transformer model according to the specified arguments, defining the model architecture.\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the Funnel\n     Transformer [funnel-transformer/small](https://huggingface.co/funnel-transformer/small) architecture.\n@@ -34,7 +34,7 @@ class FunnelConfig(PreTrainedConfig):\n     Args:\n         vocab_size (`int`, *optional*, defaults to 30522):\n             Vocabulary size of the Funnel transformer. Defines the number of different tokens that can be represented\n-            by the `inputs_ids` passed when calling [`FunnelModel`] or [`TFFunnelModel`].\n+            by the `inputs_ids` passed when calling [`FunnelModel`].\n         block_sizes (`list[int]`, *optional*, defaults to `[4, 4, 4]`):\n             The sizes of the blocks used in the model.\n         block_repeats (`list[int]`, *optional*):"
        },
        {
            "sha": "d9a06a3b3e0221e5d4a715ac3aa5629b48a18093",
            "filename": "src/transformers/models/gpt2/configuration_gpt2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 10,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fgpt2%2Fconfiguration_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fgpt2%2Fconfiguration_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fconfiguration_gpt2.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -24,7 +24,7 @@\n \n class GPT2Config(PreTrainedConfig):\n     \"\"\"\n-    This is the configuration class to store the configuration of a [`GPT2Model`] or a [`TFGPT2Model`]. It is used to\n+    This is the configuration class to store the configuration of a [`GPT2Model`]. It is used to\n     instantiate a GPT-2 model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the GPT-2\n     [openai-community/gpt2](https://huggingface.co/openai-community/gpt2) architecture.\n@@ -36,7 +36,7 @@ class GPT2Config(PreTrainedConfig):\n     Args:\n         vocab_size (`int`, *optional*, defaults to 50257):\n             Vocabulary size of the GPT-2 model. Defines the number of different tokens that can be represented by the\n-            `inputs_ids` passed when calling [`GPT2Model`] or [`TFGPT2Model`].\n+            `inputs_ids` passed when calling [`GPT2Model`].\n         n_positions (`int`, *optional*, defaults to 1024):\n             The maximum sequence length that this model might ever be used with. Typically set this to something large\n             just in case (e.g., 512 or 1024 or 2048).\n@@ -61,8 +61,7 @@ class GPT2Config(PreTrainedConfig):\n         initializer_range (`float`, *optional*, defaults to 0.02):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         summary_type (`string`, *optional*, defaults to `\"cls_index\"`):\n-            Argument used when doing sequence summary, used in the models [`GPT2DoubleHeadsModel`] and\n-            [`TFGPT2DoubleHeadsModel`].\n+            Argument used when doing sequence summary, used in the models [`GPT2DoubleHeadsModel`].\n \n             Has to be one of the following options:\n \n@@ -72,8 +71,7 @@ class GPT2Config(PreTrainedConfig):\n                 - `\"cls_index\"`: Supply a Tensor of classification token position (like GPT/GPT-2).\n                 - `\"attn\"`: Not implemented now, use multi-head attention.\n         summary_use_proj (`bool`, *optional*, defaults to `True`):\n-            Argument used when doing sequence summary, used in the models [`GPT2DoubleHeadsModel`] and\n-            [`TFGPT2DoubleHeadsModel`].\n+            Argument used when doing sequence summary, used in the models [`GPT2DoubleHeadsModel`].\n \n             Whether or not to add a projection after the vector extraction.\n         summary_activation (`str`, *optional*):\n@@ -82,13 +80,11 @@ class GPT2Config(PreTrainedConfig):\n \n             Pass `\"tanh\"` for a tanh activation to the output, any other value will result in no activation.\n         summary_proj_to_labels (`bool`, *optional*, defaults to `True`):\n-            Argument used when doing sequence summary, used in the models [`GPT2DoubleHeadsModel`] and\n-            [`TFGPT2DoubleHeadsModel`].\n+            Argument used when doing sequence summary, used in the models [`GPT2DoubleHeadsModel`].\n \n             Whether the projection outputs should have `config.num_labels` or `config.hidden_size` classes.\n         summary_first_dropout (`float`, *optional*, defaults to 0.1):\n-            Argument used when doing sequence summary, used in the models [`GPT2DoubleHeadsModel`] and\n-            [`TFGPT2DoubleHeadsModel`].\n+            Argument used when doing sequence summary, used in the models [`GPT2DoubleHeadsModel`].\n \n             The dropout ratio to be used after the projection and activation.\n         scale_attn_weights (`bool`, *optional*, defaults to `True`):"
        },
        {
            "sha": "10d795385d6fe394f914847b922abb947c40e939",
            "filename": "src/transformers/models/imagegpt/configuration_imagegpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fconfiguration_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fconfiguration_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fconfiguration_imagegpt.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -23,7 +23,7 @@\n \n class ImageGPTConfig(PreTrainedConfig):\n     \"\"\"\n-    This is the configuration class to store the configuration of a [`ImageGPTModel`] or a [`TFImageGPTModel`]. It is\n+    This is the configuration class to store the configuration of a [`ImageGPTModel`]. It is\n     used to instantiate a GPT-2 model according to the specified arguments, defining the model architecture.\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the ImageGPT\n     [openai/imagegpt-small](https://huggingface.co/openai/imagegpt-small) architecture.\n@@ -35,7 +35,7 @@ class ImageGPTConfig(PreTrainedConfig):\n     Args:\n         vocab_size (`int`, *optional*, defaults to 512):\n             Vocabulary size of the GPT-2 model. Defines the number of different tokens that can be represented by the\n-            `inputs_ids` passed when calling [`ImageGPTModel`] or [`TFImageGPTModel`].\n+            `inputs_ids` passed when calling [`ImageGPTModel`].\n         n_positions (`int`, *optional*, defaults to 32*32):\n             The maximum sequence length that this model might ever be used with. Typically set this to something large\n             just in case (e.g., 512 or 1024 or 2048)."
        },
        {
            "sha": "9a47c694f897a1660ddd7c0e9859239769ab3bc4",
            "filename": "src/transformers/models/led/configuration_led.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fled%2Fconfiguration_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fled%2Fconfiguration_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Fconfiguration_led.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -37,7 +37,7 @@ class LEDConfig(PreTrainedConfig):\n     Args:\n         vocab_size (`int`, *optional*, defaults to 50265):\n             Vocabulary size of the LED model. Defines the number of different tokens that can be represented by the\n-            `inputs_ids` passed when calling [`LEDModel`] or [`TFLEDModel`].\n+            `inputs_ids` passed when calling [`LEDModel`].\n         d_model (`int`, *optional*, defaults to 1024):\n             Dimensionality of the layers and the pooler layer.\n         encoder_layers (`int`, *optional*, defaults to 12):"
        },
        {
            "sha": "2fef0be721c2415f643eacaa3eb3b8a47800b014",
            "filename": "src/transformers/models/longformer/configuration_longformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Flongformer%2Fconfiguration_longformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Flongformer%2Fconfiguration_longformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongformer%2Fconfiguration_longformer.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -25,7 +25,7 @@\n \n class LongformerConfig(PreTrainedConfig):\n     r\"\"\"\n-    This is the configuration class to store the configuration of a [`LongformerModel`] or a [`TFLongformerModel`]. It\n+    This is the configuration class to store the configuration of a [`LongformerModel`]. It\n     is used to instantiate a Longformer model according to the specified arguments, defining the model architecture.\n \n     This is the configuration class to store the configuration of a [`LongformerModel`]. It is used to instantiate an\n@@ -40,7 +40,7 @@ class LongformerConfig(PreTrainedConfig):\n     Args:\n         vocab_size (`int`, *optional*, defaults to 30522):\n             Vocabulary size of the Longformer model. Defines the number of different tokens that can be represented by\n-            the `inputs_ids` passed when calling [`LongformerModel`] or [`TFLongformerModel`].\n+            the `inputs_ids` passed when calling [`LongformerModel`].\n         hidden_size (`int`, *optional*, defaults to 768):\n             Dimensionality of the encoder layers and the pooler layer.\n         num_hidden_layers (`int`, *optional*, defaults to 12):\n@@ -60,8 +60,7 @@ class LongformerConfig(PreTrainedConfig):\n             The maximum sequence length that this model might ever be used with. Typically set this to something large\n             just in case (e.g., 512 or 1024 or 2048).\n         type_vocab_size (`int`, *optional*, defaults to 2):\n-            The vocabulary size of the `token_type_ids` passed when calling [`LongformerModel`] or\n-            [`TFLongformerModel`].\n+            The vocabulary size of the `token_type_ids` passed when calling [`LongformerModel`].\n         initializer_range (`float`, *optional*, defaults to 0.02):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-12):"
        },
        {
            "sha": "e5f982016aa4f36480c01ab350c75f34cfba3eb2",
            "filename": "src/transformers/models/lxmert/configuration_lxmert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Flxmert%2Fconfiguration_lxmert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Flxmert%2Fconfiguration_lxmert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flxmert%2Fconfiguration_lxmert.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -23,7 +23,7 @@\n \n class LxmertConfig(PreTrainedConfig):\n     r\"\"\"\n-    This is the configuration class to store the configuration of a [`LxmertModel`] or a [`TFLxmertModel`]. It is used\n+    This is the configuration class to store the configuration of a [`LxmertModel`]. It is used\n     to instantiate a LXMERT model according to the specified arguments, defining the model architecture. Instantiating\n     a configuration with the defaults will yield a similar configuration to that of the Lxmert\n     [unc-nlp/lxmert-base-uncased](https://huggingface.co/unc-nlp/lxmert-base-uncased) architecture.\n@@ -35,7 +35,7 @@ class LxmertConfig(PreTrainedConfig):\n     Args:\n         vocab_size (`int`, *optional*, defaults to 30522):\n             Vocabulary size of the LXMERT model. Defines the number of different tokens that can be represented by the\n-            `inputs_ids` passed when calling [`LxmertModel`] or [`TFLxmertModel`].\n+            `inputs_ids` passed when calling [`LxmertModel`].\n         hidden_size (`int`, *optional*, defaults to 768):\n             Dimensionality of the encoder layers and the pooler layer.\n         num_attention_heads (`int`, *optional*, defaults to 12):"
        },
        {
            "sha": "26403a98b0c6f321c3317cde6908f9e1438283c3",
            "filename": "src/transformers/models/marian/configuration_marian.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fmarian%2Fconfiguration_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fmarian%2Fconfiguration_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fconfiguration_marian.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -35,7 +35,7 @@ class MarianConfig(PreTrainedConfig):\n     Args:\n         vocab_size (`int`, *optional*, defaults to 58101):\n             Vocabulary size of the Marian model. Defines the number of different tokens that can be represented by the\n-            `inputs_ids` passed when calling [`MarianModel`] or [`TFMarianModel`].\n+            `inputs_ids` passed when calling [`MarianModel`].\n         d_model (`int`, *optional*, defaults to 1024):\n             Dimensionality of the layers and the pooler layer.\n         encoder_layers (`int`, *optional*, defaults to 12):"
        },
        {
            "sha": "6188a72012f258421d80fee68092e8a1077ca47a",
            "filename": "src/transformers/models/mbart/configuration_mbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fmbart%2Fconfiguration_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fmbart%2Fconfiguration_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fconfiguration_mbart.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -35,7 +35,7 @@ class MBartConfig(PreTrainedConfig):\n     Args:\n         vocab_size (`int`, *optional*, defaults to 50265):\n             Vocabulary size of the MBART model. Defines the number of different tokens that can be represented by the\n-            `inputs_ids` passed when calling [`MBartModel`] or [`TFMBartModel`].\n+            `inputs_ids` passed when calling [`MBartModel`].\n         d_model (`int`, *optional*, defaults to 1024):\n             Dimensionality of the layers and the pooler layer.\n         encoder_layers (`int`, *optional*, defaults to 12):"
        },
        {
            "sha": "9e6a95515144e91d8533619044370427d2144365",
            "filename": "src/transformers/models/mobilebert/configuration_mobilebert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fconfiguration_mobilebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fconfiguration_mobilebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fconfiguration_mobilebert.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -23,7 +23,7 @@\n \n class MobileBertConfig(PreTrainedConfig):\n     r\"\"\"\n-    This is the configuration class to store the configuration of a [`MobileBertModel`] or a [`TFMobileBertModel`]. It\n+    This is the configuration class to store the configuration of a [`MobileBertModel`]. It\n     is used to instantiate a MobileBERT model according to the specified arguments, defining the model architecture.\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the MobileBERT\n     [google/mobilebert-uncased](https://huggingface.co/google/mobilebert-uncased) architecture.\n@@ -35,7 +35,7 @@ class MobileBertConfig(PreTrainedConfig):\n     Args:\n         vocab_size (`int`, *optional*, defaults to 30522):\n             Vocabulary size of the MobileBERT model. Defines the number of different tokens that can be represented by\n-            the `inputs_ids` passed when calling [`MobileBertModel`] or [`TFMobileBertModel`].\n+            the `inputs_ids` passed when calling [`MobileBertModel`].\n         hidden_size (`int`, *optional*, defaults to 512):\n             Dimensionality of the encoder layers and the pooler layer.\n         num_hidden_layers (`int`, *optional*, defaults to 24):\n@@ -55,8 +55,7 @@ class MobileBertConfig(PreTrainedConfig):\n             The maximum sequence length that this model might ever be used with. Typically set this to something large\n             just in case (e.g., 512 or 1024 or 2048).\n         type_vocab_size (`int`, *optional*, defaults to 2):\n-            The vocabulary size of the `token_type_ids` passed when calling [`MobileBertModel`] or\n-            [`TFMobileBertModel`].\n+            The vocabulary size of the `token_type_ids` passed when calling [`MobileBertModel`].\n         initializer_range (`float`, *optional*, defaults to 0.02):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-12):"
        },
        {
            "sha": "deb5d35283b0969f0a0ba20507e3c1d426f50546",
            "filename": "src/transformers/models/mpnet/configuration_mpnet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fmpnet%2Fconfiguration_mpnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fmpnet%2Fconfiguration_mpnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpnet%2Fconfiguration_mpnet.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -24,7 +24,7 @@\n \n class MPNetConfig(PreTrainedConfig):\n     r\"\"\"\n-    This is the configuration class to store the configuration of a [`MPNetModel`] or a [`TFMPNetModel`]. It is used to\n+    This is the configuration class to store the configuration of a [`MPNetModel`]. It is used to\n     instantiate a MPNet model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the MPNet\n     [microsoft/mpnet-base](https://huggingface.co/microsoft/mpnet-base) architecture.\n@@ -35,7 +35,7 @@ class MPNetConfig(PreTrainedConfig):\n     Args:\n         vocab_size (`int`, *optional*, defaults to 30527):\n             Vocabulary size of the MPNet model. Defines the number of different tokens that can be represented by the\n-            `inputs_ids` passed when calling [`MPNetModel`] or [`TFMPNetModel`].\n+            `inputs_ids` passed when calling [`MPNetModel`].\n         hidden_size (`int`, *optional*, defaults to 768):\n             Dimensionality of the encoder layers and the pooler layer.\n         num_hidden_layers (`int`, *optional*, defaults to 12):"
        },
        {
            "sha": "e46dba265c8869883abd5b0bf5dc8fa729db89c8",
            "filename": "src/transformers/models/mt5/configuration_mt5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fmt5%2Fconfiguration_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fmt5%2Fconfiguration_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fconfiguration_mt5.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -23,7 +23,7 @@\n \n class MT5Config(PreTrainedConfig):\n     r\"\"\"\n-    This is the configuration class to store the configuration of a [`MT5Model`] or a [`TFMT5Model`]. It is used to\n+    This is the configuration class to store the configuration of a [`MT5Model`]. It is used to\n     instantiate a mT5 model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the mT5\n     [google/mt5-small](https://huggingface.co/google/mt5-small) architecture.\n@@ -34,7 +34,7 @@ class MT5Config(PreTrainedConfig):\n     Arguments:\n         vocab_size (`int`, *optional*, defaults to 250112):\n             Vocabulary size of the T5 model. Defines the number of different tokens that can be represented by the\n-            `inputs_ids` passed when calling [`T5Model`] or [`TFT5Model`].\n+            `inputs_ids` passed when calling [`T5Model`].\n         d_model (`int`, *optional*, defaults to 512):\n             Size of the encoder layers and the pooler layer.\n         d_kv (`int`, *optional*, defaults to 64):"
        },
        {
            "sha": "0441e6d25d7c9437be9f743564a3bd54e285ba26",
            "filename": "src/transformers/models/openai/configuration_openai.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fopenai%2Fconfiguration_openai.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fopenai%2Fconfiguration_openai.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopenai%2Fconfiguration_openai.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -24,7 +24,7 @@\n \n class OpenAIGPTConfig(PreTrainedConfig):\n     \"\"\"\n-    This is the configuration class to store the configuration of a [`OpenAIGPTModel`] or a [`TFOpenAIGPTModel`]. It is\n+    This is the configuration class to store the configuration of a [`OpenAIGPTModel`]. It is\n     used to instantiate a GPT model according to the specified arguments, defining the model architecture.\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the GPT\n     [openai-community/openai-gpt](https://huggingface.co/openai-community/openai-gpt) architecture from OpenAI.\n@@ -35,7 +35,7 @@ class OpenAIGPTConfig(PreTrainedConfig):\n     Args:\n         vocab_size (`int`, *optional*, defaults to 40478):\n             Vocabulary size of the GPT-2 model. Defines the number of different tokens that can be represented by the\n-            `inputs_ids` passed when calling [`OpenAIGPTModel`] or [`TFOpenAIGPTModel`].\n+            `inputs_ids` passed when calling [`OpenAIGPTModel`].\n         n_positions (`int`, *optional*, defaults to 512):\n             The maximum sequence length that this model might ever be used with. Typically set this to something large\n             just in case (e.g., 512 or 1024 or 2048)."
        },
        {
            "sha": "84dc3ffb25cd89dbcc4c33d60197e97ecaf33504",
            "filename": "src/transformers/models/pegasus/configuration_pegasus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fpegasus%2Fconfiguration_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fpegasus%2Fconfiguration_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fconfiguration_pegasus.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -35,7 +35,7 @@ class PegasusConfig(PreTrainedConfig):\n     Args:\n         vocab_size (`int`, *optional*, defaults to 50265):\n             Vocabulary size of the PEGASUS model. Defines the number of different tokens that can be represented by the\n-            `inputs_ids` passed when calling [`PegasusModel`] or [`TFPegasusModel`].\n+            `inputs_ids` passed when calling [`PegasusModel`].\n         d_model (`int`, *optional*, defaults to 1024):\n             Dimensionality of the layers and the pooler layer.\n         encoder_layers (`int`, *optional*, defaults to 12):"
        },
        {
            "sha": "c464269209cc3e1db9b0836ed2eeb50659c9e316",
            "filename": "src/transformers/models/rembert/configuration_rembert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Frembert%2Fconfiguration_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Frembert%2Fconfiguration_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Fconfiguration_rembert.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -35,7 +35,7 @@ class RemBertConfig(PreTrainedConfig):\n     Args:\n         vocab_size (`int`, *optional*, defaults to 250300):\n             Vocabulary size of the RemBERT model. Defines the number of different tokens that can be represented by the\n-            `inputs_ids` passed when calling [`RemBertModel`] or [`TFRemBertModel`]. Vocabulary size of the model.\n+            `inputs_ids` passed when calling [`RemBertModel`]. Vocabulary size of the model.\n             Defines the different tokens that can be represented by the *inputs_ids* passed to the forward method of\n             [`RemBertModel`].\n         hidden_size (`int`, *optional*, defaults to 1152):\n@@ -63,7 +63,7 @@ class RemBertConfig(PreTrainedConfig):\n             The maximum sequence length that this model might ever be used with. Typically set this to something large\n             just in case (e.g., 512 or 1024 or 2048).\n         type_vocab_size (`int`, *optional*, defaults to 2):\n-            The vocabulary size of the `token_type_ids` passed when calling [`RemBertModel`] or [`TFRemBertModel`].\n+            The vocabulary size of the `token_type_ids` passed when calling [`RemBertModel`].\n         initializer_range (`float`, *optional*, defaults to 0.02):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-12):"
        },
        {
            "sha": "66e57d289a8a42826fab8b30cdf53f966cc63edb",
            "filename": "src/transformers/models/roberta/configuration_roberta.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Froberta%2Fconfiguration_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Froberta%2Fconfiguration_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fconfiguration_roberta.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -24,7 +24,7 @@\n \n class RobertaConfig(PreTrainedConfig):\n     r\"\"\"\n-    This is the configuration class to store the configuration of a [`RobertaModel`] or a [`TFRobertaModel`]. It is\n+    This is the configuration class to store the configuration of a [`RobertaModel`]. It is\n     used to instantiate a RoBERTa model according to the specified arguments, defining the model architecture.\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the RoBERTa\n     [FacebookAI/roberta-base](https://huggingface.co/FacebookAI/roberta-base) architecture.\n@@ -36,7 +36,7 @@ class RobertaConfig(PreTrainedConfig):\n     Args:\n         vocab_size (`int`, *optional*, defaults to 50265):\n             Vocabulary size of the RoBERTa model. Defines the number of different tokens that can be represented by the\n-            `inputs_ids` passed when calling [`RobertaModel`] or [`TFRobertaModel`].\n+            `inputs_ids` passed when calling [`RobertaModel`].\n         hidden_size (`int`, *optional*, defaults to 768):\n             Dimensionality of the encoder layers and the pooler layer.\n         num_hidden_layers (`int`, *optional*, defaults to 12):\n@@ -56,7 +56,7 @@ class RobertaConfig(PreTrainedConfig):\n             The maximum sequence length that this model might ever be used with. Typically set this to something large\n             just in case (e.g., 512 or 1024 or 2048).\n         type_vocab_size (`int`, *optional*, defaults to 2):\n-            The vocabulary size of the `token_type_ids` passed when calling [`RobertaModel`] or [`TFRobertaModel`].\n+            The vocabulary size of the `token_type_ids` passed when calling [`RobertaModel`].\n         initializer_range (`float`, *optional*, defaults to 0.02):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-12):"
        },
        {
            "sha": "884904774d3f7dbfa9e840b242995fd9f238c652",
            "filename": "src/transformers/models/roberta_prelayernorm/configuration_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fconfiguration_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fconfiguration_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fconfiguration_roberta_prelayernorm.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -25,7 +25,7 @@\n # Copied from transformers.models.roberta.configuration_roberta.RobertaConfig with FacebookAI/roberta-base->andreasmadsen/efficient_mlm_m0.40,RoBERTa->RoBERTa-PreLayerNorm,Roberta->RobertaPreLayerNorm,roberta->roberta-prelayernorm\n class RobertaPreLayerNormConfig(PreTrainedConfig):\n     r\"\"\"\n-    This is the configuration class to store the configuration of a [`RobertaPreLayerNormModel`] or a [`TFRobertaPreLayerNormModel`]. It is\n+    This is the configuration class to store the configuration of a [`RobertaPreLayerNormModel`]. It is\n     used to instantiate a RoBERTa-PreLayerNorm model according to the specified arguments, defining the model architecture.\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the RoBERTa-PreLayerNorm\n     [andreasmadsen/efficient_mlm_m0.40](https://huggingface.co/andreasmadsen/efficient_mlm_m0.40) architecture.\n@@ -37,7 +37,7 @@ class RobertaPreLayerNormConfig(PreTrainedConfig):\n     Args:\n         vocab_size (`int`, *optional*, defaults to 50265):\n             Vocabulary size of the RoBERTa-PreLayerNorm model. Defines the number of different tokens that can be represented by the\n-            `inputs_ids` passed when calling [`RobertaPreLayerNormModel`] or [`TFRobertaPreLayerNormModel`].\n+            `inputs_ids` passed when calling [`RobertaPreLayerNormModel`].\n         hidden_size (`int`, *optional*, defaults to 768):\n             Dimensionality of the encoder layers and the pooler layer.\n         num_hidden_layers (`int`, *optional*, defaults to 12):\n@@ -57,7 +57,7 @@ class RobertaPreLayerNormConfig(PreTrainedConfig):\n             The maximum sequence length that this model might ever be used with. Typically set this to something large\n             just in case (e.g., 512 or 1024 or 2048).\n         type_vocab_size (`int`, *optional*, defaults to 2):\n-            The vocabulary size of the `token_type_ids` passed when calling [`RobertaPreLayerNormModel`] or [`TFRobertaPreLayerNormModel`].\n+            The vocabulary size of the `token_type_ids` passed when calling [`RobertaPreLayerNormModel`].\n         initializer_range (`float`, *optional*, defaults to 0.02):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-12):"
        },
        {
            "sha": "979d6cf9608f3119fd1b51d45c4f35a9a61f74f6",
            "filename": "src/transformers/models/roformer/configuration_roformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Froformer%2Fconfiguration_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Froformer%2Fconfiguration_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Fconfiguration_roformer.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -35,7 +35,7 @@ class RoFormerConfig(PreTrainedConfig):\n     Args:\n         vocab_size (`int`, *optional*, defaults to 50000):\n             Vocabulary size of the RoFormer model. Defines the number of different tokens that can be represented by\n-            the `inputs_ids` passed when calling [`RoFormerModel`] or [`TFRoFormerModel`].\n+            the `inputs_ids` passed when calling [`RoFormerModel`].\n         embedding_size (`int`, *optional*, defaults to None):\n             Dimensionality of the encoder layers and the pooler layer. Defaults to the `hidden_size` if not provided.\n         hidden_size (`int`, *optional*, defaults to 768):\n@@ -57,7 +57,7 @@ class RoFormerConfig(PreTrainedConfig):\n             The maximum sequence length that this model might ever be used with. Typically set this to something large\n             just in case (e.g., 512 or 1024 or 1536).\n         type_vocab_size (`int`, *optional*, defaults to 2):\n-            The vocabulary size of the `token_type_ids` passed when calling [`RoFormerModel`] or [`TFRoFormerModel`].\n+            The vocabulary size of the `token_type_ids` passed when calling [`RoFormerModel`].\n         initializer_range (`float`, *optional*, defaults to 0.02):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-12):"
        },
        {
            "sha": "3c629eb27e42bf078977d1df5c8ef121d5538129",
            "filename": "src/transformers/models/squeezebert/configuration_squeezebert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Fconfiguration_squeezebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Fconfiguration_squeezebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Fconfiguration_squeezebert.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -55,7 +55,7 @@ class SqueezeBertConfig(PreTrainedConfig):\n             The maximum sequence length that this model might ever be used with. Typically set this to something large\n             just in case (e.g., 512 or 1024 or 2048).\n         type_vocab_size (`int`, *optional*, defaults to 2):\n-            The vocabulary size of the `token_type_ids` passed when calling [`BertModel`] or [`TFBertModel`].\n+            The vocabulary size of the `token_type_ids` passed when calling [`BertModel`].\n         initializer_range (`float`, *optional*, defaults to 0.02):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-12):"
        },
        {
            "sha": "66c1b73145bbdae0bfc8529bc08656bede4abe4a",
            "filename": "src/transformers/models/t5/configuration_t5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Ft5%2Fconfiguration_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Ft5%2Fconfiguration_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fconfiguration_t5.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -23,7 +23,7 @@\n \n class T5Config(PreTrainedConfig):\n     r\"\"\"\n-    This is the configuration class to store the configuration of a [`T5Model`] or a [`TFT5Model`]. It is used to\n+    This is the configuration class to store the configuration of a [`T5Model`]. It is used to\n     instantiate a T5 model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the T5\n     [google-t5/t5-small](https://huggingface.co/google-t5/t5-small) architecture.\n@@ -34,7 +34,7 @@ class T5Config(PreTrainedConfig):\n     Arguments:\n         vocab_size (`int`, *optional*, defaults to 32128):\n             Vocabulary size of the T5 model. Defines the number of different tokens that can be represented by the\n-            `inputs_ids` passed when calling [`T5Model`] or [`TFT5Model`].\n+            `inputs_ids` passed when calling [`T5Model`].\n         d_model (`int`, *optional*, defaults to 512):\n             Size of the encoder layers and the pooler layer.\n         d_kv (`int`, *optional*, defaults to 64):"
        },
        {
            "sha": "41a03afd7523f07a684d07cbe9f55192796b16ea",
            "filename": "src/transformers/models/timesfm/configuration_timesfm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fconfiguration_timesfm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fconfiguration_timesfm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fconfiguration_timesfm.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -23,7 +23,7 @@\n \n class TimesFmConfig(PreTrainedConfig):\n     r\"\"\"\n-    This is the configuration class to store the configuration of a [`TimesFmModelForPrediction`] or a [`TFTimesFmModel`]. It is used to\n+    This is the configuration class to store the configuration of a [`TimesFmModelForPrediction`]. It is used to\n     instantiate a TimesFM model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the TimesFM\n     [google/timesfm-2.0-500m-pytorch](https://huggingface.co/google/timesfm-2.0-500m-pytorch) architecture."
        },
        {
            "sha": "c9dd26e7c5059b1040742beeedc2cb036f00a237",
            "filename": "src/transformers/models/umt5/configuration_umt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fumt5%2Fconfiguration_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fumt5%2Fconfiguration_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fconfiguration_umt5.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -34,7 +34,7 @@ class UMT5Config(PreTrainedConfig):\n     Arguments:\n         vocab_size (`int`, *optional*, defaults to 250112):\n             Vocabulary size of the UMT5 model. Defines the number of different tokens that can be represented by the\n-            `inputs_ids` passed when calling [`UMT5Model`] or [`TFUMT5Model`].\n+            `inputs_ids` passed when calling [`UMT5Model`].\n         d_model (`int`, *optional*, defaults to 512):\n             Size of the encoder layers and the pooler layer.\n         d_kv (`int`, *optional*, defaults to 64):"
        },
        {
            "sha": "4bfa4fca3e7e66d234359e36c5ea327a22c6f87a",
            "filename": "src/transformers/models/wav2vec2/configuration_wav2vec2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fconfiguration_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fconfiguration_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fconfiguration_wav2vec2.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -38,7 +38,7 @@ class Wav2Vec2Config(PreTrainedConfig):\n     Args:\n         vocab_size (`int`, *optional*, defaults to 32):\n             Vocabulary size of the Wav2Vec2 model. Defines the number of different tokens that can be represented by\n-            the `inputs_ids` passed when calling [`Wav2Vec2Model`] or [`TFWav2Vec2Model`]. Vocabulary size of the\n+            the `inputs_ids` passed when calling [`Wav2Vec2Model`]. Vocabulary size of the\n             model. Defines the different tokens that can be represented by the *inputs_ids* passed to the forward\n             method of [`Wav2Vec2Model`].\n         hidden_size (`int`, *optional*, defaults to 768):"
        },
        {
            "sha": "b89315cfb09da8735226819a7072ff008ca141c4",
            "filename": "src/transformers/models/xlm/configuration_xlm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fxlm%2Fconfiguration_xlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fxlm%2Fconfiguration_xlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm%2Fconfiguration_xlm.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -23,7 +23,7 @@\n \n class XLMConfig(PreTrainedConfig):\n     \"\"\"\n-    This is the configuration class to store the configuration of a [`XLMModel`] or a [`TFXLMModel`]. It is used to\n+    This is the configuration class to store the configuration of a [`XLMModel`]. It is used to\n     instantiate a XLM model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the\n     [FacebookAI/xlm-mlm-en-2048](https://huggingface.co/FacebookAI/xlm-mlm-en-2048) architecture.\n@@ -34,7 +34,7 @@ class XLMConfig(PreTrainedConfig):\n     Args:\n         vocab_size (`int`, *optional*, defaults to 30145):\n             Vocabulary size of the BERT model. Defines the number of different tokens that can be represented by the\n-            `inputs_ids` passed when calling [`XLMModel`] or [`TFXLMModel`].\n+            `inputs_ids` passed when calling [`XLMModel`].\n         emb_dim (`int`, *optional*, defaults to 2048):\n             Dimensionality of the encoder layers and the pooler layer.\n         n_layer (`int`, *optional*, defaults to 12):"
        },
        {
            "sha": "7b9d81760f54f5fa1e3c094c1e2c28b7a06d7660",
            "filename": "src/transformers/models/xlm_roberta/configuration_xlm_roberta.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fconfiguration_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fconfiguration_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fconfiguration_xlm_roberta.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -24,7 +24,7 @@\n \n class XLMRobertaConfig(PreTrainedConfig):\n     r\"\"\"\n-    This is the configuration class to store the configuration of a [`XLMRobertaModel`] or a [`TFXLMRobertaModel`]. It\n+    This is the configuration class to store the configuration of a [`XLMRobertaModel`]. It\n     is used to instantiate a XLM-RoBERTa model according to the specified arguments, defining the model architecture.\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the XLMRoBERTa\n     [FacebookAI/xlm-roberta-base](https://huggingface.co/FacebookAI/xlm-roberta-base) architecture.\n@@ -36,7 +36,7 @@ class XLMRobertaConfig(PreTrainedConfig):\n     Args:\n         vocab_size (`int`, *optional*, defaults to 30522):\n             Vocabulary size of the XLM-RoBERTa model. Defines the number of different tokens that can be represented by\n-            the `inputs_ids` passed when calling [`XLMRobertaModel`] or [`TFXLMRobertaModel`].\n+            the `inputs_ids` passed when calling [`XLMRobertaModel`].\n         hidden_size (`int`, *optional*, defaults to 768):\n             Dimensionality of the encoder layers and the pooler layer.\n         num_hidden_layers (`int`, *optional*, defaults to 12):\n@@ -56,8 +56,7 @@ class XLMRobertaConfig(PreTrainedConfig):\n             The maximum sequence length that this model might ever be used with. Typically set this to something large\n             just in case (e.g., 512 or 1024 or 2048).\n         type_vocab_size (`int`, *optional*, defaults to 2):\n-            The vocabulary size of the `token_type_ids` passed when calling [`XLMRobertaModel`] or\n-            [`TFXLMRobertaModel`].\n+            The vocabulary size of the `token_type_ids` passed when calling [`XLMRobertaModel`].\n         initializer_range (`float`, *optional*, defaults to 0.02):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-12):"
        },
        {
            "sha": "006dcfbb9bb68041974ab1b20a799089422f7c78",
            "filename": "src/transformers/models/xlm_roberta_xl/configuration_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fconfiguration_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fconfiguration_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fconfiguration_xlm_roberta_xl.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -23,7 +23,7 @@\n \n class XLMRobertaXLConfig(PreTrainedConfig):\n     r\"\"\"\n-    This is the configuration class to store the configuration of a [`XLMRobertaXLModel`] or a [`TFXLMRobertaXLModel`].\n+    This is the configuration class to store the configuration of a [`XLMRobertaXLModel`].\n     It is used to instantiate a XLM_ROBERTA_XL model according to the specified arguments, defining the model\n     architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the\n     XLM_ROBERTA_XL [facebook/xlm-roberta-xl](https://huggingface.co/facebook/xlm-roberta-xl) architecture.\n@@ -55,8 +55,7 @@ class XLMRobertaXLConfig(PreTrainedConfig):\n             The maximum sequence length that this model might ever be used with. Typically set this to something large\n             just in case (e.g., 512 or 1024 or 2048).\n         type_vocab_size (`int`, *optional*, defaults to 1):\n-            The vocabulary size of the `token_type_ids` passed when calling [`XLMRobertaXLModel`] or\n-            [`TFXLMRobertaXLModel`].\n+            The vocabulary size of the `token_type_ids` passed when calling [`XLMRobertaXLModel`].\n         initializer_range (`float`, *optional*, defaults to 0.02):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-5):"
        },
        {
            "sha": "8d983ca7362a1b274ae948e85c99aa30c8e7dc59",
            "filename": "src/transformers/models/xlnet/configuration_xlnet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fxlnet%2Fconfiguration_xlnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fmodels%2Fxlnet%2Fconfiguration_xlnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlnet%2Fconfiguration_xlnet.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -26,7 +26,7 @@\n \n class XLNetConfig(PreTrainedConfig):\n     \"\"\"\n-    This is the configuration class to store the configuration of a [`XLNetModel`] or a [`TFXLNetModel`]. It is used to\n+    This is the configuration class to store the configuration of a [`XLNetModel`]. It is used to\n     instantiate a XLNet model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the\n     [xlnet/xlnet-large-cased](https://huggingface.co/xlnet/xlnet-large-cased) architecture.\n@@ -37,7 +37,7 @@ class XLNetConfig(PreTrainedConfig):\n     Args:\n         vocab_size (`int`, *optional*, defaults to 32000):\n             Vocabulary size of the XLNet model. Defines the number of different tokens that can be represented by the\n-            `inputs_ids` passed when calling [`XLNetModel`] or [`TFXLNetModel`].\n+            `inputs_ids` passed when calling [`XLNetModel`].\n         d_model (`int`, *optional*, defaults to 1024):\n             Dimensionality of the encoder layers and the pooler layer.\n         n_layer (`int`, *optional*, defaults to 24):"
        },
        {
            "sha": "b69e833f37c474f5934b7d5654af35c951eb923a",
            "filename": "src/transformers/pipelines/text_generation.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fpipelines%2Ftext_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c95d4affc1198cd9c95c41063d0b71a1c9c8e07c/src%2Ftransformers%2Fpipelines%2Ftext_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftext_generation.py?ref=c95d4affc1198cd9c95c41063d0b71a1c9c8e07c",
            "patch": "@@ -110,8 +110,6 @@ def __init__(self, *args, **kwargs):\n             if prefix is None and self.model.__class__.__name__ in [\n                 \"XLNetLMHeadModel\",\n                 \"TransfoXLLMHeadModel\",\n-                \"TFXLNetLMHeadModel\",\n-                \"TFTransfoXLLMHeadModel\",\n             ]:\n                 # For XLNet and TransformerXL we add an article to the prompt to give more state to the model.\n                 prefix = self.XL_PREFIX"
        }
    ],
    "stats": {
        "total": 188,
        "additions": 89,
        "deletions": 99
    }
}