{
    "author": "zucchini-nlp",
    "message": "[VLMs]  support attention backends (#37576)\n\n* update models\n\n* why rename\n\n* return attn weights when sdpa\n\n* fixes\n\n* fix attn implementation composite\n\n* fix moshi\n\n* add message\n\n* add typings\n\n* use explicitly all flags for each attn type\n\n* fix some tests\n\n* import what is needed\n\n* kosmos on main has ew attention already, yay\n\n* new models in main, run fixup\n\n* won't fix kosmos yet\n\n* fix-copies\n\n* clean up after rebasing\n\n* fix tests\n\n* style\n\n* dont cast attns to fp32\n\n* did we update ruff? oke, let's just do what it asks\n\n* fix pixtral after rebase",
    "sha": "d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
    "files": [
        {
            "sha": "1c0bff6cf39e2fe3729b85852f3fc82c0b4b01ed",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -833,6 +833,7 @@ def to_diff_dict(self) -> dict[str, Any]:\n                 if \"model_type\" in value:\n                     # Needs to be set even if it's not in the diff\n                     diff[\"model_type\"] = value[\"model_type\"]\n+\n                 serializable_config_dict[key] = diff\n             elif (\n                 key not in default_config_dict\n@@ -1003,6 +1004,8 @@ def _remove_keys_not_serialized(self, d: dict[str, Any]) -> None:\n             del d[\"_commit_hash\"]\n         if \"_attn_implementation_internal\" in d:\n             del d[\"_attn_implementation_internal\"]\n+        if \"_attn_implementation_autoset\" in d:\n+            del d[\"_attn_implementation_autoset\"]\n         # Do not serialize `base_model_tp_plan` for now\n         if \"base_model_tp_plan\" in d:\n             del d[\"base_model_tp_plan\"]"
        },
        {
            "sha": "cdaf68d7616779bcabeb77251efc63440cac810b",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -3430,9 +3430,6 @@ def save_pretrained(\n         # Attach architecture to the config\n         model_to_save.config.architectures = [model_to_save.__class__.__name__]\n \n-        # Unset attn implementation so it can be set to another one when loading back\n-        model_to_save.config._attn_implementation_autoset = False\n-\n         # If we have a custom model, we copy the file defining it in the folder and set the attributes so it can be\n         # loaded from the Hub.\n         if self._auto_class is not None:"
        },
        {
            "sha": "bef0e2cec153add48cfdaf0f30f14606cf028e89",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 8,
            "deletions": 4,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -669,6 +669,7 @@ class AriaTextPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn_2 = False\n     _supports_sdpa = True\n     _supports_cache_class = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -1409,6 +1410,7 @@ def get_image_features(\n         image_features = self.multi_modal_projector(selected_image_feature, attn_mask=image_attn_mask)\n         return image_features\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(ARIA_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -1424,6 +1426,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, AriaModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1470,16 +1473,16 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=True,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n-        output = AriaModelOutputWithPast(\n+        return AriaModelOutputWithPast(\n             last_hidden_state=outputs.last_hidden_state,\n             past_key_values=outputs.past_key_values if use_cache else None,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n             image_hidden_states=image_features if pixel_values is not None else None,\n         )\n-        return output if return_dict else output.to_tuple()\n \n     def _create_patch_attention_mask(self, pixel_mask):\n         if pixel_mask is None:\n@@ -1563,7 +1566,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **loss_kwargs,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, AriaCausalLMOutputWithPast]:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1645,6 +1648,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -1655,7 +1659,7 @@ def forward(\n         loss = None\n         if labels is not None:\n             loss = self.loss_function(\n-                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **loss_kwargs\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n             )\n \n         return AriaCausalLMOutputWithPast("
        },
        {
            "sha": "e6beaf89197d9225c8cfdae38ede1c62761b3e12",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 13,
            "deletions": 4,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -32,6 +32,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import CausalLMOutputWithPast\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n@@ -40,6 +41,7 @@\n     TextInput,\n )\n from ...utils import (\n+    LossKwargs,\n     TensorType,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n@@ -1240,6 +1242,7 @@ class AriaTextPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn_2 = False\n     _supports_sdpa = True\n     _supports_cache_class = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -1290,6 +1293,9 @@ def __init__(self, config: AriaTextConfig):\n         self.post_init()\n \n \n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n class AriaTextForCausalLM(AriaTextPreTrainedModel, LlamaForCausalLM):\n     \"\"\"\n     Aria model for causal language modeling tasks.\n@@ -1434,6 +1440,7 @@ def get_image_features(\n         image_features = self.multi_modal_projector(selected_image_feature, attn_mask=image_attn_mask)\n         return image_features\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(ARIA_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -1449,6 +1456,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, AriaModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1495,16 +1503,16 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=True,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n-        output = AriaModelOutputWithPast(\n+        return AriaModelOutputWithPast(\n             last_hidden_state=outputs.last_hidden_state,\n             past_key_values=outputs.past_key_values if use_cache else None,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n             image_hidden_states=image_features if pixel_values is not None else None,\n         )\n-        return output if return_dict else output.to_tuple()\n \n \n @add_start_docstrings(\n@@ -1533,7 +1541,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **loss_kwargs,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, AriaCausalLMOutputWithPast]:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1615,6 +1623,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -1625,7 +1634,7 @@ def forward(\n         loss = None\n         if labels is not None:\n             loss = self.loss_function(\n-                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **loss_kwargs\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n             )\n \n         return AriaCausalLMOutputWithPast("
        },
        {
            "sha": "c700a97129487beba5ced28577c8f53c44bcdaa2",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 16,
            "deletions": 7,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -27,9 +27,12 @@\n \n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_utils import PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n+    LossKwargs,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     can_return_tuple,\n@@ -124,6 +127,7 @@ class AyaVisionPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_quantized_cache = False\n     _supports_static_cache = False\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = (\n@@ -358,6 +362,7 @@ def get_image_features(\n         image_features = self.multi_modal_projector(selected_image_feature)\n         return image_features\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(AYA_VISION_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -375,7 +380,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         image_sizes: torch.Tensor = None,\n-        **lm_kwargs,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, AyaVisionModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -434,17 +439,19 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=True,\n             cache_position=cache_position,\n-            **lm_kwargs,\n+            **kwargs,\n         )\n \n-        output = AyaVisionModelOutputWithPast(\n+        return AyaVisionModelOutputWithPast(\n             last_hidden_state=outputs.last_hidden_state,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n             image_hidden_states=image_features if pixel_values is not None else None,\n         )\n-        return output if return_dict else output.to_tuple()\n+\n+\n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n \n \n @add_start_docstrings(\n@@ -512,7 +519,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         image_sizes: Optional[torch.Tensor] = None,\n-        **lm_kwargs,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, AyaVisionCausalLMOutputWithPast]:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -589,7 +596,7 @@ def forward(\n             return_dict=True,\n             cache_position=cache_position,\n             image_sizes=image_sizes,\n-            **lm_kwargs,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -599,7 +606,9 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+            )\n \n         return AyaVisionCausalLMOutputWithPast(\n             loss=loss,"
        },
        {
            "sha": "977170708f0d2fb7729b367176b6e3865c94e16f",
            "filename": "src/transformers/models/aya_vision/modular_aya_vision.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -20,13 +20,15 @@\n from torch import nn\n \n from transformers.models.llava.modeling_llava import (\n+    KwargsForCausalLM,\n     LlavaCausalLMOutputWithPast,\n     LlavaForConditionalGeneration,\n     LlavaModel,\n     LlavaPreTrainedModel,\n )\n \n from ...activations import ACT2FN\n+from ...processing_utils import Unpack\n from ...utils import (\n     add_start_docstrings,\n     logging,\n@@ -148,7 +150,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         image_sizes: Optional[torch.Tensor] = None,\n-        **lm_kwargs,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, AyaVisionCausalLMOutputWithPast]:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -213,7 +215,7 @@ def forward(\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n             image_sizes=image_sizes,\n-            **lm_kwargs,\n+            **kwargs,\n         )\n \n "
        },
        {
            "sha": "fcc7b2b497952e904c5465ee414a65eab1dafc90",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 84,
            "deletions": 33,
            "changes": 117,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -16,7 +16,7 @@\n \n import math\n from dataclasses import dataclass\n-from typing import Any, Optional, Tuple, Union\n+from typing import Any, Callable, Optional, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -25,15 +25,18 @@\n \n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPooling,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import (\n+    LossKwargs,\n     ModelOutput,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n@@ -255,6 +258,30 @@ def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding: boo\n         return embeddings\n \n \n+# Adapted from transformers.models.siglip.modeling_siglip.eager_attention_forward -> BLIP doesn't cast attn weights to fp32\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class Blip2Attention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -270,7 +297,8 @@ def __init__(self, config):\n                 f\" {self.num_heads}).\"\n             )\n         self.scale = self.head_dim**-0.5\n-        self.dropout = nn.Dropout(config.attention_dropout)\n+        self.is_causal = False\n+        self.attention_dropout = config.attention_dropout\n \n         # small tweak here compared to CLIP, no bias here\n         self.qkv = nn.Linear(self.embed_dim, 3 * self.embed_dim, bias=False)\n@@ -296,6 +324,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         head_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n+        **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n@@ -308,31 +337,32 @@ def forward(\n         )\n         query_states, key_states, value_states = mixed_qkv[0], mixed_qkv[1], mixed_qkv[2]\n \n-        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-        attention_scores = torch.matmul(query_states, key_states.transpose(-1, -2))\n-\n-        attention_scores = attention_scores * self.scale\n+        attention_interface: Callable = eager_attention_forward\n \n-        # Normalize the attention scores to probabilities.\n-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n-\n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.dropout(attention_probs)\n-\n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n-        context_layer = torch.matmul(attention_probs, value_states).permute(0, 2, 1, 3)\n-\n-        new_context_layer_shape = context_layer.size()[:-2] + (self.embed_dim,)\n-        context_layer = context_layer.reshape(new_context_layer_shape)\n-\n-        output = self.projection(context_layer)\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask=None,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scale,\n+            **kwargs,\n+        )\n \n-        outputs = (output, attention_probs) if output_attentions else (output, None)\n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n+        attn_output = self.projection(attn_output)\n \n+        outputs = (attn_output, attn_weights) if output_attentions else (attn_output, None)\n         return outputs\n \n \n@@ -410,6 +440,10 @@ class Blip2PreTrainedModel(PreTrainedModel):\n     config_class = Blip2Config\n     base_model_prefix = \"blip\"\n     supports_gradient_checkpointing = True\n+    _supports_attention_backend = True\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n \n     _no_split_modules = [\n         \"Blip2Attention\",\n@@ -1332,6 +1366,11 @@ def forward(\n     BLIP_2_QFORMER_START_DOCSTRING,\n )\n class Blip2QFormerModel(Blip2PreTrainedModel):\n+    _supports_attention_backend = False  # adds position on attn weights before last matmul\n+    _supports_flash_attn_2 = False\n+    _supports_sdpa = False\n+    _supports_flex_attn = False\n+\n     def __init__(self, config: Blip2QFormerConfig):\n         super().__init__(config)\n         self.config = config\n@@ -1511,6 +1550,9 @@ def forward(\n         )\n \n \n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n @add_start_docstrings(\n     \"\"\"\n     BLIP-2 Model for generating text and image features. The model consists of a vision encoder, Querying Transformer\n@@ -1526,10 +1568,10 @@ class Blip2Model(Blip2PreTrainedModel):\n     def __init__(self, config: Blip2Config):\n         super().__init__(config)\n \n-        self.vision_model = Blip2VisionModel(config.vision_config)\n+        self.vision_model = Blip2VisionModel._from_config(config.vision_config)\n \n         self.query_tokens = nn.Parameter(torch.zeros(1, config.num_query_tokens, config.qformer_config.hidden_size))\n-        self.qformer = Blip2QFormerModel(config.qformer_config)\n+        self.qformer = Blip2QFormerModel._from_config(config.qformer_config)\n \n         self.language_projection = nn.Linear(config.qformer_config.hidden_size, config.text_config.hidden_size)\n         if config.use_decoder_only_language_model:\n@@ -1580,6 +1622,7 @@ def get_text_features(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ):\n         r\"\"\"\n         Returns:\n@@ -1611,6 +1654,7 @@ def get_text_features(\n                 output_attentions=output_attentions,\n                 output_hidden_states=output_hidden_states,\n                 return_dict=return_dict,\n+                **kwargs,\n             )\n         else:\n             inputs_embeds = self.language_model.get_input_embeddings()(input_ids)\n@@ -1624,6 +1668,7 @@ def get_text_features(\n                 output_hidden_states=output_hidden_states,\n                 return_dict=return_dict,\n                 labels=labels,\n+                **kwargs,\n             )\n \n         return text_outputs\n@@ -1749,6 +1794,7 @@ def forward(\n         labels: Optional[torch.LongTensor] = None,\n         return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, Blip2ForConditionalGenerationModelOutput]:\n         r\"\"\"\n         Returns:\n@@ -1826,6 +1872,7 @@ def forward(\n                 output_attentions=output_attentions,\n                 output_hidden_states=output_hidden_states,\n                 return_dict=return_dict,\n+                **kwargs,\n             )\n             logits = outputs.logits if return_dict else outputs[0]\n             loss = None\n@@ -1851,6 +1898,7 @@ def forward(\n                 output_hidden_states=output_hidden_states,\n                 return_dict=True,  # toggle for easier access to loss/logits below\n                 labels=labels,\n+                **kwargs,\n             )\n             loss = outputs.loss\n             logits = outputs.logits\n@@ -1981,10 +2029,10 @@ class Blip2VisionModelWithProjection(Blip2PreTrainedModel):\n     def __init__(self, config: Blip2Config):\n         super().__init__(config)\n \n-        self.vision_model = Blip2VisionModel(config.vision_config)\n+        self.vision_model = Blip2VisionModel._from_config(config.vision_config)\n \n         self.query_tokens = nn.Parameter(torch.zeros(1, config.num_query_tokens, config.qformer_config.hidden_size))\n-        self.qformer = Blip2QFormerModel(config.qformer_config)\n+        self.qformer = Blip2QFormerModel._from_config(config.qformer_config)\n \n         # vision projection layer\n         self.vision_projection = nn.Linear(config.qformer_config.hidden_size, config.image_text_hidden_size)\n@@ -2102,10 +2150,10 @@ class Blip2ForConditionalGeneration(Blip2PreTrainedModel, GenerationMixin):\n     def __init__(self, config: Blip2Config):\n         super().__init__(config)\n \n-        self.vision_model = Blip2VisionModel(config.vision_config)\n+        self.vision_model = Blip2VisionModel._from_config(config.vision_config)\n \n         self.query_tokens = nn.Parameter(torch.zeros(1, config.num_query_tokens, config.qformer_config.hidden_size))\n-        self.qformer = Blip2QFormerModel(config.qformer_config)\n+        self.qformer = Blip2QFormerModel._from_config(config.qformer_config)\n \n         self.language_projection = nn.Linear(config.qformer_config.hidden_size, config.text_config.hidden_size)\n         if config.use_decoder_only_language_model:\n@@ -2180,6 +2228,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         use_cache: Optional[bool] = None,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, Blip2ForConditionalGenerationModelOutput]:\n         r\"\"\"\n         Returns:\n@@ -2308,6 +2357,7 @@ def forward(\n                 output_hidden_states=output_hidden_states,\n                 return_dict=return_dict,\n                 use_cache=use_cache,\n+                **kwargs,\n             )\n             logits = outputs.logits if return_dict else outputs[0]\n             loss = None\n@@ -2334,6 +2384,7 @@ def forward(\n                 return_dict=True,  # toggle for easier access to loss/logits below\n                 labels=labels,\n                 use_cache=use_cache,\n+                **kwargs,\n             )\n             loss = outputs.loss\n             logits = outputs.logits\n@@ -2463,12 +2514,12 @@ class Blip2ForImageTextRetrieval(Blip2PreTrainedModel):\n     def __init__(self, config: Blip2Config):\n         super().__init__(config)\n \n-        self.vision_model = Blip2VisionModel(config.vision_config)\n+        self.vision_model = Blip2VisionModel._from_config(config.vision_config)\n \n         self.query_tokens = nn.Parameter(torch.zeros(1, config.num_query_tokens, config.qformer_config.hidden_size))\n \n         self.embeddings = Blip2TextEmbeddings(config.qformer_config)\n-        self.qformer = Blip2QFormerModel(config.qformer_config)\n+        self.qformer = Blip2QFormerModel._from_config(config.qformer_config)\n \n         # vision projection layer\n         self.vision_projection = nn.Linear(config.qformer_config.hidden_size, config.image_text_hidden_size)"
        },
        {
            "sha": "3928e88ae017de0894aba945db8c431e09ec1ca0",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 60,
            "deletions": 256,
            "changes": 316,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -14,31 +14,32 @@\n # limitations under the License.\n \"\"\"PyTorch Chameleon model.\"\"\"\n \n-import math\n from functools import cached_property\n-from typing import Optional, Tuple, Union\n+from typing import Callable, Optional, Tuple, Union\n \n import torch\n import torch.nn.functional as F\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n-from ...modeling_flash_attention_utils import _flash_attention_forward, flash_attn_supports_top_left_mask\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n from ...utils import (\n+    LossKwargs,\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torch_flex_attn_available,\n     is_torchdynamo_compiling,\n     logging,\n@@ -235,6 +236,33 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n+# Copied from transformers.models.llama.modeling_llama.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class ChameleonAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -259,6 +287,7 @@ def __init__(self, config: ChameleonConfig, layer_idx: Optional[int] = None):\n         self.rope_theta = config.rope_theta\n         self.is_causal = True\n         self.model_parallel_size = config.model_parallel_size\n+        self.scaling = self.head_dim**-0.5\n \n         if (self.head_dim * self.num_heads) != self.hidden_size:\n             raise ValueError(\n@@ -338,144 +367,26 @@ def forward(\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n-\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-# NO LONGER EXIST copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2 with Llama->Chameleon\n-# TODO(joao): add me back asap :)\n-class ChameleonFlashAttention2(ChameleonAttention):\n-    \"\"\"\n-    Chameleon flash attention module. This module inherits from `ChameleonAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n-\n-    # Ignore copy\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if isinstance(past_key_value, StaticCache):\n-            raise ValueError(\n-                \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n-                \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n-            )\n-\n-        output_attentions = False\n-\n-        bsz, q_len, _ = hidden_states.size()\n+        attention_interface: Callable = eager_attention_forward\n \n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.reshape(-1, self.num_heads, self.head_dim)\n-        query_states = self.q_norm(query_states)\n-\n-        key_states = key_states.reshape(-1, self.num_key_value_heads, self.head_dim)\n-        key_states = self.k_norm(key_states)\n-\n-        # Flash attention requires the input to have the shape\n-        # batch_size x seq_length x head_dim x hidden_dim\n-        # therefore we just need to keep the original shape\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = self.rotary_emb(value_states, position_ids)\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; position_ids needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim].\n-        # We would need to refactor the KV cache to be able to avoid many of these transpose/reshape/view.\n-        query_states = query_states.transpose(1, 2)\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        dropout_rate = self.attention_dropout if self.training else 0.0\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (ChameleonRMSNorm handles it correctly)\n-\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n             else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        attn_output = _flash_attention_forward(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n             attention_mask,\n-            q_len,\n-            dropout=dropout_rate,\n-            sliding_window=getattr(self, \"sliding_window\", None),\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-            is_causal=self.is_causal,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n         )\n \n         attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n@@ -487,114 +398,13 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-class ChameleonSdpaAttention(ChameleonAttention):\n-    \"\"\"\n-    Chameleon attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `ChameleonAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n-    \"\"\"\n-\n-    # Adapted from ChameleonAttention.forward\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"ChameleonModel is using ChameleonSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_value=past_key_value,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.reshape(-1, self.num_heads, self.head_dim)\n-        query_states = self.q_norm(query_states)\n-\n-        key_states = key_states.reshape(-1, self.num_key_value_heads, self.head_dim)\n-        key_states = self.k_norm(key_states)\n-\n-        query_states = query_states.reshape(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.reshape(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = self.rotary_emb(value_states, position_ids)\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, None)\n-\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; position_ids needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        causal_mask = attention_mask\n-        if attention_mask is not None and cache_position is not None:\n-            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and causal_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n-\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.attention_dropout if self.training else 0.0,\n-            is_causal=is_causal,\n-        )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, self.hidden_size)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        return attn_output, None, past_key_value\n-\n-\n-CHAMELEON_ATTENTION_CLASSES = {\n-    \"eager\": ChameleonAttention,\n-    \"flash_attention_2\": ChameleonFlashAttention2,\n-    \"sdpa\": ChameleonSdpaAttention,\n-}\n-\n-\n # copied from transformers.models.llama.modeling_llama.LlamaDecoderLayer with Llama->Chameleon, LLAMA->CHAMELEON\n-# TODO(joao): add me back asap :)\n class ChameleonDecoderLayer(nn.Module):\n     def __init__(self, config: ChameleonConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n \n-        self.self_attn = CHAMELEON_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n+        self.self_attn = ChameleonAttention(config=config, layer_idx=layer_idx)\n \n         self.mlp = ChameleonMLP(config)\n         self.input_layernorm = ChameleonRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n@@ -669,7 +479,7 @@ def __init__(self, config: ChameleonConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n \n-        self.self_attn = CHAMELEON_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n+        self.self_attn = ChameleonAttention(config=config, layer_idx=layer_idx)\n \n         self.mlp = ChameleonMLP(config)\n         self.input_layernorm = ChameleonRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n@@ -1052,6 +862,7 @@ class ChameleonPreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_static_cache = True\n     _supports_param_buffer_assignment = False\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -1256,6 +1067,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1342,6 +1154,7 @@ def forward(\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n+                    **kwargs,\n                 )\n \n             hidden_states = layer_outputs[0]\n@@ -1498,6 +1311,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         return causal_mask\n \n \n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n @add_start_docstrings(\n     \"Chameleon Model with a head on top used for outputting logits for next token prediction.\",\n     CHAMELEON_START_DOCSTRING,\n@@ -1532,6 +1348,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(CHAMELEON_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -1548,6 +1365,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1596,6 +1414,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -1607,22 +1426,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Upcast to float if we need to compute the loss to avoid potential precision issues\n-            logits = logits.float()\n-            # Shift so that tokens < n predict n\n-            shift_logits = logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n-            shift_labels = shift_labels.view(-1)\n-            # Enable model parallelism\n-            shift_labels = shift_labels.to(shift_logits.device)\n-            loss = loss_fct(shift_logits, shift_labels)\n-\n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         return CausalLMOutputWithPast(\n             loss=loss,"
        },
        {
            "sha": "761e40a2fe13cdd03348c9f80dcc176874197816",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 12,
            "deletions": 3,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -1010,6 +1010,7 @@ class Emu3VQVAE(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n     _supports_flex_attn = True\n+    _supports_attention_backend = True\n     _no_split_modules = [\n         \"Emu3VQVAETemporalResnetBlock\",\n         \"Emu3VQVAEAttentionBlock\",\n@@ -1202,6 +1203,7 @@ class Emu3PreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_static_cache = True\n     _supports_param_buffer_assignment = False\n+    _supports_attention_backend = True\n     _supports_flex_attn = True\n \n     def _init_weights(self, module):\n@@ -1836,6 +1838,7 @@ def decode_image_tokens(self, image_tokens: torch.LongTensor, height: int, width\n         image = self.vqmodel.decode(image_tokens)\n         return image\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(EMU3_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -1851,6 +1854,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1884,8 +1888,9 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         return outputs\n@@ -1941,6 +1946,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -2007,8 +2013,9 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -2018,7 +2025,9 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+            )\n \n         return CausalLMOutputWithPast(\n             loss=loss,"
        },
        {
            "sha": "971d94e29d7063f36bfb837a134b905ed8ff7123",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 15,
            "deletions": 3,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -25,10 +25,12 @@\n \n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     CausalLMOutputWithPast,\n )\n from ...modeling_utils import PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n@@ -41,6 +43,7 @@\n     ChameleonVQVAEEncoderConvDownsample,\n )\n from ..llama.modeling_llama import (\n+    KwargsForCausalLM,\n     LlamaDecoderLayer,\n     LlamaForCausalLM,\n     LlamaModel,\n@@ -736,6 +739,7 @@ class Emu3VQVAE(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n     _supports_flex_attn = True\n+    _supports_attention_backend = True\n     _no_split_modules = [\n         \"Emu3VQVAETemporalResnetBlock\",\n         \"Emu3VQVAEAttentionBlock\",\n@@ -898,6 +902,7 @@ class Emu3PreTrainedModel(ChameleonPreTrainedModel, Emu3VQVAE):\n         \"Emu3DecoderLayer\",\n     ]\n     _supports_flex_attn = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = self.config.get_text_config().initializer_range\n@@ -1179,6 +1184,7 @@ def decode_image_tokens(self, image_tokens: torch.LongTensor, height: int, width\n         image = self.vqmodel.decode(image_tokens)\n         return image\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(EMU3_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -1194,6 +1200,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1227,8 +1234,9 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         return outputs\n@@ -1284,6 +1292,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1350,8 +1359,9 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -1361,7 +1371,9 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+            )\n \n         return CausalLMOutputWithPast(\n             loss=loss,"
        },
        {
            "sha": "a580d47ef1cc8f11f2940131dafc91ed6b695001",
            "filename": "src/transformers/models/fuyu/modeling_fuyu.py",
            "status": "modified",
            "additions": 22,
            "deletions": 9,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -21,10 +21,19 @@\n from torch import nn\n \n from ...generation import GenerationMixin\n-from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_outputs import CausalLMOutputWithPast\n from ...modeling_utils import PreTrainedModel\n from ...models.auto.modeling_auto import AutoModel\n-from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    LossKwargs,\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n+    logging,\n+    replace_return_docstrings,\n+)\n from .configuration_fuyu import FuyuConfig\n \n \n@@ -58,6 +67,10 @@ class FuyuPreTrainedModel(PreTrainedModel):\n     config_class = FuyuConfig\n     base_model_prefix = \"fuyu\"\n     supports_gradient_checkpointing = True\n+    _supports_attention_backend = True\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n     _no_split_modules = []\n     _skip_keys_device_placement = \"past_key_values\"\n \n@@ -142,6 +155,9 @@ def _init_weights(self, module):\n \"\"\"\n \n \n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n @add_start_docstrings(\n     \"\"\"The Fuyu model which consists of a vision backbone and a language model, without a language modeling head.\"\"\",\n     FUYU_START_DOCSTRING,\n@@ -224,8 +240,8 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-        **kwargs,\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+        **kwargs: Unpack[KwargsForCausalLM],\n+    ) -> Union[Tuple, CausalLMOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -323,6 +339,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model.get_decoder()\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(FUYU_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -392,7 +409,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             use_cache=use_cache,\n-            return_dict=return_dict,\n+            return_dict=True,\n             # don't pass kwargs because Persimmon-backbone doesn't accept FA2 kwargs yet, TODO: raushan\n         )\n \n@@ -407,10 +424,6 @@ def forward(\n                 logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n             )\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "145a9ce752fef5054f4699de033073c65b48bfcf",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 17,
            "deletions": 3,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -30,9 +30,12 @@\n \n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_utils import PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n+    LossKwargs,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     can_return_tuple,\n@@ -619,6 +622,7 @@ class GotOcr2PreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n@@ -745,6 +749,7 @@ def get_image_features(\n         image_outputs = self.vision_tower(pixel_values).last_hidden_state\n         return self.multi_modal_projector(image_outputs)\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(GOT_OCR2_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -759,6 +764,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, GotOcr2ModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -800,16 +806,19 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=True,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n-        output = GotOcr2ModelOutputWithPast(\n+        return GotOcr2ModelOutputWithPast(\n             last_hidden_state=outputs.last_hidden_state,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n             image_hidden_states=image_features if pixel_values is not None else None,\n         )\n-        return output if return_dict else output.to_tuple()\n+\n+\n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n \n \n @add_start_docstrings(\n@@ -874,6 +883,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, GotOcr2CausalLMOutputWithPast]:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -937,6 +947,8 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=True,\n             cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -946,7 +958,9 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+            )\n \n         return GotOcr2CausalLMOutputWithPast(\n             loss=loss,"
        },
        {
            "sha": "fe02b6147778bfa479280636742f3c1a861824cc",
            "filename": "src/transformers/models/got_ocr2/modular_got_ocr2.py",
            "status": "modified",
            "additions": 13,
            "deletions": 3,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -21,6 +21,7 @@\n import torch.utils.checkpoint\n \n from transformers.models.llava.modeling_llava import (\n+    KwargsForCausalLM,\n     LlavaCausalLMOutputWithPast,\n     LlavaForConditionalGeneration,\n     LlavaModel,\n@@ -30,6 +31,8 @@\n from transformers.models.sam.modeling_sam import SamMLPBlock, SamVisionAttention, SamVisionEncoder, SamVisionLayer\n \n from ...configuration_utils import PretrainedConfig\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...processing_utils import Unpack\n from ...utils import (\n     add_start_docstrings_to_model_forward,\n     can_return_tuple,\n@@ -393,6 +396,7 @@ def get_image_features(\n         image_outputs = self.vision_tower(pixel_values).last_hidden_state\n         return self.multi_modal_projector(image_outputs)\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(GOT_OCR2_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -407,6 +411,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, GotOcr2ModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -448,16 +453,16 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=True,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n-        output = GotOcr2ModelOutputWithPast(\n+        return GotOcr2ModelOutputWithPast(\n             last_hidden_state=outputs.last_hidden_state,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n             image_hidden_states=image_features if pixel_values is not None else None,\n         )\n-        return output if return_dict else output.to_tuple()\n \n \n class GotOcr2ForConditionalGeneration(LlavaForConditionalGeneration):\n@@ -479,6 +484,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, GotOcr2CausalLMOutputWithPast]:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -542,6 +548,8 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=True,\n             cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -551,7 +559,9 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+            )\n \n         return GotOcr2CausalLMOutputWithPast(\n             loss=loss,"
        },
        {
            "sha": "ebe43db4598a22cc92914bad4f392228fd7b78c5",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 70,
            "deletions": 59,
            "changes": 129,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -20,24 +20,27 @@\n \"\"\"PyTorch Idefics model.\"\"\"\n \n from dataclasses import dataclass\n-from typing import Any, Dict, List, Optional, Tuple, Union\n+from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n \n import torch\n import torch.nn.functional as F\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import ModelOutput\n-from ...modeling_utils import PretrainedConfig, PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PretrainedConfig, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n from ...utils import (\n+    LossKwargs,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n@@ -500,6 +503,30 @@ def forward(self, x):\n         return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n \n \n+# Copied from transformers.models.siglip.modeling_siglip.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n # this was adapted from LlamaAttention\n class IdeficsAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n@@ -515,11 +542,13 @@ def __init__(\n         layer_idx: Optional[int] = None,\n     ):\n         super().__init__()\n+        self.config = config\n         self.hidden_size = hidden_size\n         self.num_heads = num_heads\n         self.head_dim = hidden_size // num_heads\n         self.dropout = dropout\n         self.is_causal = True\n+        self.scaling = self.head_dim**-0.5\n \n         self.layer_idx = layer_idx\n         if layer_idx is None:\n@@ -596,6 +625,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         # if key_value_states are provided this layer is used as a cross-attention layer\n         is_cross_attention = self.is_cross_attention or key_value_states is not None\n@@ -631,47 +661,33 @@ def forward(\n             query_states = self.q_layer_norm(query_states)\n             key_states = self.k_layer_norm(key_states)\n \n-        causal_mask = attention_mask\n-        if attention_mask is not None:\n-            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and attention_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n+        attention_interface: Callable = eager_attention_forward\n \n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n-        is_causal = True if self.is_causal and causal_mask is None and q_len > 1 else False\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.dropout if self.training else 0.0,\n-            is_causal=is_causal,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n         )\n \n-        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2)\n-        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n-\n+        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n \n-        attn_weights = None\n         if output_attentions:\n-            logger.warning_once(\n-                \"attn_weights are not extracted in scaled_dot_product_attention. The model returns None instead\"\n-            )\n+            attn_weights = None\n \n         return attn_output, attn_weights, past_key_value\n \n@@ -706,6 +722,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n@@ -734,6 +751,7 @@ def forward(\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n         hidden_states = residual + hidden_states\n@@ -833,6 +851,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        **kwargs,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n@@ -875,6 +894,7 @@ def forward(\n             key_value_states=image_hidden_states,\n             attention_mask=image_attention_mask,\n             output_attentions=output_attentions,\n+            **kwargs,\n         )\n         hidden_states = nn.functional.dropout(hidden_states, p=self.config, training=self.training)\n         # Fill in zeros for cross_attention hidden_states of tokens attending to no images\n@@ -927,7 +947,9 @@ class IdeficsPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"IdeficsDecoderLayer\", \"IdeficsGatedCrossAttentionLayer\"]\n     _supports_sdpa = True\n     _supports_cache_class = True\n+    _supports_flash_attn_2 = True\n     _supports_static_cache = False  # IDEFICS cannot compile due to dynamic control flow when checking inputs\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         # important: this ported version of Idefics isn't meant for training from scratch - only\n@@ -1029,6 +1051,9 @@ def _init_weights(self, module):\n \"\"\"\n \n \n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n @add_start_docstrings(\n     \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n     LLAMA_START_DOCSTRING,\n@@ -1112,6 +1137,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -1130,6 +1156,7 @@ def forward(\n         interpolate_pos_encoding: Optional[bool] = False,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, IdeficsBaseModelOutputWithPast]:\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n@@ -1292,6 +1319,7 @@ def vblock(\n                         output_attentions=output_attentions,\n                         use_cache=use_cache,\n                         past_key_value=None,  # not implemented\n+                        **kwargs,\n                     )\n                     hidden_states = outputs[0]\n \n@@ -1303,6 +1331,7 @@ def vblock(\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n+                    **kwargs,\n                 )\n \n                 return layer_outputs\n@@ -1348,6 +1377,7 @@ def vblock(\n                     cross_layer_interval=self.cross_layer_interval,\n                     gated_cross_attn_layers=self.gated_cross_attn_layers,\n                     cache_position=cache_position,\n+                    **kwargs,\n                 )\n \n             hidden_states = layer_outputs[0]\n@@ -1368,12 +1398,7 @@ def vblock(\n         if return_legacy_cache:\n             next_cache = next_cache.to_legacy_cache()\n         image_hidden_states = image_hidden_states.view(batch_size, num_images, image_seq_len, image_hidden_size)\n-        if not return_dict:\n-            return tuple(\n-                v\n-                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, image_hidden_states]\n-                if v is not None\n-            )\n+\n         return IdeficsBaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=next_cache,\n@@ -1565,6 +1590,7 @@ def tie_weights(self):\n             ):\n                 output_embeddings.out_additional_features = input_embeddings.num_additional_embeddings\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=IdeficsCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -1585,6 +1611,7 @@ def forward(\n         interpolate_pos_encoding: Optional[bool] = False,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, IdeficsCausalLMOutputWithPast]:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1641,33 +1668,17 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n-            return_dict=return_dict,\n+            return_dict=True,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n         logits = self.lm_head(hidden_states)\n \n         loss = None\n         if labels is not None:\n-            labels = labels.to(logits.device)\n-            # Shift so that tokens < n predict n\n-            if attention_mask is not None:\n-                # we use the input attention mask to shift the logits and labels, because it is 2D.\n-                # we also crop attn mask in case it is longer, which happens in PrefixTuning with peft\n-                shift_attention_mask = attention_mask[:, -(logits.shape[1] - 1) :].to(logits.device)\n-                shift_logits = logits[..., :-1, :][shift_attention_mask != 0].contiguous()\n-                shift_labels = labels[..., 1:][shift_attention_mask != 0].contiguous()\n-            else:\n-                shift_logits = logits[..., :-1, :].contiguous()\n-                shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n-\n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         return IdeficsCausalLMOutputWithPast(\n             loss=loss,"
        },
        {
            "sha": "e5e3b9f3a53c45d89d99d2366d852e68836852ba",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 19,
            "deletions": 26,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -20,17 +20,20 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutput, ModelOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n+    LossKwargs,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     logging,\n     replace_return_docstrings,\n )\n@@ -514,6 +517,7 @@ class Idefics2PreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n@@ -1089,6 +1093,7 @@ def inputs_merger(\n         new_inputs_embeds[special_image_token_mask] = reshaped_image_hidden_states.to(new_inputs_embeds.device)\n         return new_inputs_embeds\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(\n         \"\"\"\n         Inputs fed to the model can have an arbitrary number of images. To account for this, pixel_values fed to\n@@ -1117,6 +1122,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, Idefics2BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1226,15 +1232,13 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             cache_position=cache_position,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         if return_legacy_cache and use_cache:\n             outputs.past_key_values = outputs.past_key_values.to_legacy_cache()\n \n-        if not return_dict:\n-            return tuple(v for v in [*outputs, image_hidden_states] if v is not None)\n-\n         return Idefics2BaseModelOutputWithPast(\n             last_hidden_state=outputs.last_hidden_state,\n             past_key_values=outputs.past_key_values,\n@@ -1244,6 +1248,9 @@ def forward(\n         )\n \n \n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n @add_start_docstrings(\n     \"\"\"The Idefics2 Model with a language modeling head. It is made up a SigLIP vision encoder, with a language modeling head on top. \"\"\",\n     IDEFICS2_START_DOCSTRING,\n@@ -1292,6 +1299,7 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(IDEFICS2_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=Idefics2CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -1311,6 +1319,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, Idefics2CausalLMOutputWithPast]:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1386,7 +1395,8 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             cache_position=cache_position,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -1396,26 +1406,9 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Upcast to float if we need to compute the loss to avoid potential precision issues\n-            logits = logits.float()\n-            labels = labels.to(logits.device)\n-            # Shift so that tokens < n predict n\n-            if attention_mask is not None:\n-                # we use the input attention mask to shift the logits and labels, because it is 2D.\n-                # we also crop attn mask in case it is longer, which happens in PrefixTuning with peft\n-                shift_attention_mask = attention_mask[:, -(logits.shape[1] - 1) :].to(logits.device)\n-                shift_logits = logits[..., :-1, :][shift_attention_mask != 0].contiguous()\n-                shift_labels = labels[..., 1:][shift_attention_mask != 0].contiguous()\n-            else:\n-                shift_logits = logits[..., :-1, :].contiguous()\n-                shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n-\n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+            )\n \n         return Idefics2CausalLMOutputWithPast(\n             loss=loss,"
        },
        {
            "sha": "4e1a00fa98c4602ff5f8de94a73386e86ceec069",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 19,
            "deletions": 26,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -20,17 +20,20 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...cache_utils import DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutput, ModelOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n+    LossKwargs,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     logging,\n     replace_return_docstrings,\n )\n@@ -532,6 +535,7 @@ class Idefics3PreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n@@ -816,6 +820,7 @@ def inputs_merger(\n         new_inputs_embeds[special_image_token_mask] = reshaped_image_hidden_states\n         return new_inputs_embeds\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(\n         \"\"\"\n         Inputs fed to the model can have an arbitrary number of images. To account for this, pixel_values fed to\n@@ -843,6 +848,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, Idefics3BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -939,12 +945,10 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             cache_position=cache_position,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n-        if not return_dict:\n-            return tuple(v for v in [*outputs, image_hidden_states] if v is not None)\n-\n         return Idefics3BaseModelOutputWithPast(\n             last_hidden_state=outputs.last_hidden_state,\n             past_key_values=outputs.past_key_values,\n@@ -954,6 +958,9 @@ def forward(\n         )\n \n \n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n @add_start_docstrings(\n     \"\"\"The Idefics3 Model with a language modeling head. It is made up a SigLIP vision encoder, with a language modeling head on top. \"\"\",\n     IDEFICS3_START_DOCSTRING,\n@@ -1009,6 +1016,7 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(IDEFICS3_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=Idefics3CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -1028,6 +1036,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         return_dict: Optional[bool] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, Idefics3CausalLMOutputWithPast]:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1117,7 +1126,8 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             cache_position=cache_position,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -1127,26 +1137,9 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Upcast to float if we need to compute the loss to avoid potential precision issues\n-            logits = logits.float()\n-            labels = labels.to(logits.device)\n-            # Shift so that tokens < n predict n\n-            if attention_mask is not None:\n-                # we use the input attention mask to shift the logits and labels, because it is 2D.\n-                # we also crop attn mask in case it is longer, which happens in PrefixTuning with peft\n-                shift_attention_mask = attention_mask[:, -(logits.shape[1] - 1) :].to(logits.device)\n-                shift_logits = logits[..., :-1, :][shift_attention_mask != 0].contiguous()\n-                shift_labels = labels[..., 1:][shift_attention_mask != 0].contiguous()\n-            else:\n-                shift_logits = logits[..., :-1, :].contiguous()\n-                shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n-\n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+            )\n \n         return Idefics3CausalLMOutputWithPast(\n             loss=loss,"
        },
        {
            "sha": "ec051387d659f8eeffc77e090f615c29b8bdd95b",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 81,
            "deletions": 45,
            "changes": 126,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -16,27 +16,30 @@\n \n import math\n from dataclasses import dataclass\n-from typing import Any, Optional, Tuple, Union\n+from typing import Any, Callable, Optional, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPooling,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import (\n+    LossKwargs,\n     ModelOutput,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     logging,\n     replace_return_docstrings,\n     torch_int,\n@@ -159,6 +162,30 @@ def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding: boo\n         return embeddings\n \n \n+# Adapted from transformers.models.siglip.modeling_siglip.eager_attention_forward -> InstructBLIP doesn't cast attn weights to fp32\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n # Copied from transformers.models.blip_2.modeling_blip_2.Blip2Attention with Blip2->InstructBlip\n class InstructBlipAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n@@ -175,7 +202,8 @@ def __init__(self, config):\n                 f\" {self.num_heads}).\"\n             )\n         self.scale = self.head_dim**-0.5\n-        self.dropout = nn.Dropout(config.attention_dropout)\n+        self.is_causal = False\n+        self.attention_dropout = config.attention_dropout\n \n         # small tweak here compared to CLIP, no bias here\n         self.qkv = nn.Linear(self.embed_dim, 3 * self.embed_dim, bias=False)\n@@ -201,6 +229,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         head_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n+        **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n@@ -213,31 +242,32 @@ def forward(\n         )\n         query_states, key_states, value_states = mixed_qkv[0], mixed_qkv[1], mixed_qkv[2]\n \n-        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-        attention_scores = torch.matmul(query_states, key_states.transpose(-1, -2))\n-\n-        attention_scores = attention_scores * self.scale\n-\n-        # Normalize the attention scores to probabilities.\n-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n-\n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.dropout(attention_probs)\n-\n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n-        context_layer = torch.matmul(attention_probs, value_states).permute(0, 2, 1, 3)\n+        attention_interface: Callable = eager_attention_forward\n \n-        new_context_layer_shape = context_layer.size()[:-2] + (self.embed_dim,)\n-        context_layer = context_layer.reshape(new_context_layer_shape)\n-\n-        output = self.projection(context_layer)\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask=None,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scale,\n+            **kwargs,\n+        )\n \n-        outputs = (output, attention_probs) if output_attentions else (output, None)\n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n+        attn_output = self.projection(attn_output)\n \n+        outputs = (attn_output, attn_weights) if output_attentions else (attn_output, None)\n         return outputs\n \n \n@@ -315,6 +345,10 @@ class InstructBlipPreTrainedModel(PreTrainedModel):\n     config_class = InstructBlipConfig\n     base_model_prefix = \"blip\"\n     supports_gradient_checkpointing = True\n+    _supports_attention_backend = True\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_static_cache = True\n     _supports_quantized_cache = False  # not all LM bacbones support (e.g. T5)\n@@ -1087,6 +1121,11 @@ class InstructBlipQFormerModel(InstructBlipPreTrainedModel):\n     instruction as input.\n     \"\"\"\n \n+    _supports_attention_backend = False  # adds position on attn weights before last matmul\n+    _supports_flash_attn_2 = False\n+    _supports_sdpa = False\n+    _supports_flex_attn = False\n+\n     def __init__(self, config: InstructBlipQFormerConfig):\n         super().__init__(config)\n         self.config = config\n@@ -1277,6 +1316,9 @@ def forward(\n         )\n \n \n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n @add_start_docstrings(\n     \"\"\"\n     InstructBLIP base Model consisting of language model, qformer and vision encoder.\n@@ -1337,6 +1379,7 @@ def _preprocess_accelerate(self):\n         if hasattr(self.language_model, \"_hf_hook\"):\n             self.language_model._hf_hook.io_same_device = True  # For `generate` compatibility\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(INSTRUCTBLIP_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -1352,6 +1395,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         use_cache: Optional[bool] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, InstructBlipForConditionalGenerationModelOutput]:\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n@@ -1404,6 +1448,7 @@ def forward(\n                 output_hidden_states=output_hidden_states,\n                 return_dict=return_dict,\n                 use_cache=use_cache,\n+                **kwargs,\n             )\n         else:\n             outputs = self.language_model(\n@@ -1415,11 +1460,9 @@ def forward(\n                 output_hidden_states=output_hidden_states,\n                 return_dict=return_dict,\n                 use_cache=use_cache,\n+                **kwargs,\n             )\n \n-        if not return_dict:\n-            return (vision_outputs, query_outputs, outputs)\n-\n         return InstructBlipForConditionalGenerationModelOutput(\n             vision_outputs=vision_outputs,\n             qformer_outputs=query_outputs,\n@@ -1448,10 +1491,10 @@ class InstructBlipForConditionalGeneration(InstructBlipPreTrainedModel, Generati\n     def __init__(self, config: InstructBlipConfig):\n         super().__init__(config)\n \n-        self.vision_model = InstructBlipVisionModel(config.vision_config)\n+        self.vision_model = InstructBlipVisionModel._from_config(config.vision_config)\n \n         self.query_tokens = nn.Parameter(torch.zeros(1, config.num_query_tokens, config.qformer_config.hidden_size))\n-        self.qformer = InstructBlipQFormerModel(config.qformer_config)\n+        self.qformer = InstructBlipQFormerModel._from_config(config.qformer_config)\n \n         self.language_projection = nn.Linear(config.qformer_config.hidden_size, config.text_config.hidden_size)\n \n@@ -1516,6 +1559,7 @@ def _preprocess_accelerate(self):\n         if hasattr(self.language_model, \"_hf_hook\"):\n             self.language_model._hf_hook.io_same_device = True  # For `generate` compatibility\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(INSTRUCTBLIP_INPUTS_DOCSTRING)\n     @replace_return_docstrings(\n         output_type=InstructBlipForConditionalGenerationModelOutput, config_class=InstructBlipVisionConfig\n@@ -1535,6 +1579,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         use_cache: Optional[bool] = None,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, InstructBlipForConditionalGenerationModelOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -1646,21 +1691,15 @@ def forward(\n                 output_hidden_states=output_hidden_states,\n                 return_dict=return_dict,\n                 use_cache=use_cache,\n+                **kwargs,\n             )\n             logits = outputs.logits if return_dict else outputs[0]\n             loss = None\n-            # we compute the loss here since we need to take into account the sequence length of the query embeds\n             if labels is not None:\n-                labels = labels.to(logits.device)\n-                logits = logits[:, -labels.size(1) :, :]\n-                # Shift so that tokens < n predict n\n-                shift_logits = logits[..., :-1, :].contiguous()\n-                shift_labels = labels[..., 1:].contiguous().to(logits.device)\n-\n-                # Flatten the tokens\n-                loss_fct = CrossEntropyLoss(reduction=\"mean\")\n+                loss = self.loss_function(\n+                    logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+                )\n \n-                loss = loss_fct(shift_logits.view(-1, self.config.text_config.vocab_size), shift_labels.view(-1))\n         else:\n             outputs = self.language_model(\n                 inputs_embeds=inputs_embeds,\n@@ -1672,14 +1711,11 @@ def forward(\n                 return_dict=return_dict,\n                 labels=labels,\n                 use_cache=use_cache,\n+                **kwargs,\n             )\n             loss = outputs.loss if return_dict else outputs[0]\n             logits = outputs.logits if return_dict else outputs[1]\n \n-        if not return_dict:\n-            output = (logits, vision_outputs, query_outputs, outputs)\n-            return ((loss,) + output) if loss is not None else output\n-\n         return InstructBlipForConditionalGenerationModelOutput(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "4e78afaa8821b6cf603e4f576955ec4d1c82018d",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 82,
            "deletions": 46,
            "changes": 128,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -21,26 +21,29 @@\n \n import math\n from dataclasses import dataclass\n-from typing import Any, Optional, Tuple, Union\n+from typing import Any, Callable, Optional, Tuple, Union\n \n import torch\n from torch import nn\n-from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPooling,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import (\n+    LossKwargs,\n     ModelOutput,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     logging,\n     replace_return_docstrings,\n     torch_int,\n@@ -130,6 +133,30 @@ def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding: boo\n         return embeddings\n \n \n+# Adapted from transformers.models.siglip.modeling_siglip.eager_attention_forward -> InstructBlipVideo doesn't cast attn weights to fp32\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class InstructBlipVideoAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -145,7 +172,8 @@ def __init__(self, config):\n                 f\" {self.num_heads}).\"\n             )\n         self.scale = self.head_dim**-0.5\n-        self.dropout = nn.Dropout(config.attention_dropout)\n+        self.is_causal = False\n+        self.attention_dropout = config.attention_dropout\n \n         # small tweak here compared to CLIP, no bias here\n         self.qkv = nn.Linear(self.embed_dim, 3 * self.embed_dim, bias=False)\n@@ -171,6 +199,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         head_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n+        **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n@@ -183,31 +212,32 @@ def forward(\n         )\n         query_states, key_states, value_states = mixed_qkv[0], mixed_qkv[1], mixed_qkv[2]\n \n-        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-        attention_scores = torch.matmul(query_states, key_states.transpose(-1, -2))\n-\n-        attention_scores = attention_scores * self.scale\n-\n-        # Normalize the attention scores to probabilities.\n-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n-\n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.dropout(attention_probs)\n-\n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n-        context_layer = torch.matmul(attention_probs, value_states).permute(0, 2, 1, 3)\n+        attention_interface: Callable = eager_attention_forward\n \n-        new_context_layer_shape = context_layer.size()[:-2] + (self.embed_dim,)\n-        context_layer = context_layer.reshape(new_context_layer_shape)\n-\n-        output = self.projection(context_layer)\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask=None,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scale,\n+            **kwargs,\n+        )\n \n-        outputs = (output, attention_probs) if output_attentions else (output, None)\n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n+        attn_output = self.projection(attn_output)\n \n+        outputs = (attn_output, attn_weights) if output_attentions else (attn_output, None)\n         return outputs\n \n \n@@ -852,6 +882,9 @@ def forward(\n         return embeddings\n \n \n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n INSTRUCTBLIPVIDEO_START_DOCSTRING = r\"\"\"\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n@@ -945,6 +978,10 @@ class InstructBlipVideoPreTrainedModel(PreTrainedModel):\n     config_class = InstructBlipVideoConfig\n     base_model_prefix = \"blip\"\n     supports_gradient_checkpointing = True\n+    _supports_attention_backend = True\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_static_cache = True\n     _supports_quantized_cache = False  # not all LM bacbones support (e.g. T5)\n@@ -1049,6 +1086,11 @@ class InstructBlipVideoQFormerModel(InstructBlipVideoPreTrainedModel):\n     instruction as input.\n     \"\"\"\n \n+    _supports_attention_backend = False  # adds position on attn weights before last matmul\n+    _supports_flash_attn_2 = False\n+    _supports_sdpa = False\n+    _supports_flex_attn = False\n+\n     def __init__(self, config: InstructBlipVideoQFormerConfig):\n         super().__init__(config)\n         self.config = config\n@@ -1332,6 +1374,7 @@ def _preprocess_accelerate(self):\n         if hasattr(self.language_model, \"_hf_hook\"):\n             self.language_model._hf_hook.io_same_device = True  # For `generate` compatibility\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(INSTRUCTBLIPVIDEO_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -1347,6 +1390,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         use_cache: Optional[bool] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, InstructBlipVideoForConditionalGenerationModelOutput]:\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n@@ -1409,6 +1453,7 @@ def forward(\n                 output_hidden_states=output_hidden_states,\n                 return_dict=return_dict,\n                 use_cache=use_cache,\n+                **kwargs,\n             )\n         else:\n             outputs = self.language_model(\n@@ -1420,11 +1465,9 @@ def forward(\n                 output_hidden_states=output_hidden_states,\n                 return_dict=return_dict,\n                 use_cache=use_cache,\n+                **kwargs,\n             )\n \n-        if not return_dict:\n-            return (vision_outputs, query_outputs, outputs)\n-\n         return InstructBlipVideoForConditionalGenerationModelOutput(\n             vision_outputs=vision_outputs,\n             qformer_outputs=query_outputs,\n@@ -1453,10 +1496,10 @@ class InstructBlipVideoForConditionalGeneration(InstructBlipVideoPreTrainedModel\n     def __init__(self, config: InstructBlipVideoConfig):\n         super().__init__(config)\n \n-        self.vision_model = InstructBlipVideoVisionModel(config.vision_config)\n+        self.vision_model = InstructBlipVideoVisionModel._from_config(config.vision_config)\n \n         self.query_tokens = nn.Parameter(torch.zeros(1, config.num_query_tokens, config.qformer_config.hidden_size))\n-        self.qformer = InstructBlipVideoQFormerModel(config.qformer_config)\n+        self.qformer = InstructBlipVideoQFormerModel._from_config(config.qformer_config)\n \n         self.language_projection = nn.Linear(config.qformer_config.hidden_size, config.text_config.hidden_size)\n \n@@ -1519,9 +1562,10 @@ def _preprocess_accelerate(self):\n         if hasattr(self.language_model, \"_hf_hook\"):\n             self.language_model._hf_hook.io_same_device = True  # For `generate` compatibility\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(INSTRUCTBLIPVIDEO_INPUTS_DOCSTRING)\n     @replace_return_docstrings(\n-        output_type=InstructBlipVideoForConditionalGenerationModelOutput, config_class=InstructBlipVideoVisionConfig\n+        output_type=InstructBlipVideoForConditionalGenerationModelOutput, config_class=InstructBlipVideoConfig\n     )\n     def forward(\n         self,\n@@ -1538,6 +1582,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         use_cache: Optional[bool] = None,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, InstructBlipVideoForConditionalGenerationModelOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -1682,21 +1727,15 @@ def forward(\n                 output_hidden_states=output_hidden_states,\n                 return_dict=return_dict,\n                 use_cache=use_cache,\n+                **kwargs,\n             )\n             logits = outputs.logits if return_dict else outputs[0]\n             loss = None\n-            # we compute the loss here since we need to take into account the sequence length of the query embeds\n             if labels is not None:\n-                labels = labels.to(logits.device)\n-                logits = logits[:, -labels.size(1) :, :]\n-                # Shift so that tokens < n predict n\n-                shift_logits = logits[..., :-1, :].contiguous()\n-                shift_labels = labels[..., 1:].contiguous().to(logits.device)\n-\n-                # Flatten the tokens\n-                loss_fct = CrossEntropyLoss(reduction=\"mean\")\n+                loss = self.loss_function(\n+                    logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+                )\n \n-                loss = loss_fct(shift_logits.view(-1, self.config.text_config.vocab_size), shift_labels.view(-1))\n         else:\n             outputs = self.language_model(\n                 inputs_embeds=inputs_embeds,\n@@ -1708,14 +1747,11 @@ def forward(\n                 return_dict=return_dict,\n                 labels=labels,\n                 use_cache=use_cache,\n+                **kwargs,\n             )\n             loss = outputs.loss if return_dict else outputs[0]\n             logits = outputs.logits if return_dict else outputs[1]\n \n-        if not return_dict:\n-            output = (logits, vision_outputs, query_outputs, outputs)\n-            return ((loss,) + output) if loss is not None else output\n-\n         return InstructBlipVideoForConditionalGenerationModelOutput(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "566b00d983cec3779168f2033d98ba7b132a633c",
            "filename": "src/transformers/models/instructblipvideo/modular_instructblipvideo.py",
            "status": "modified",
            "additions": 19,
            "deletions": 19,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -18,7 +18,6 @@\n \n import torch\n import torch.utils.checkpoint\n-from torch.nn import CrossEntropyLoss\n \n from transformers.models.instructblip.configuration_instructblip import (\n     InstructBlipQFormerConfig,\n@@ -31,11 +30,14 @@\n     InstructBlipPreTrainedModel,\n     InstructBlipQFormerModel,\n     InstructBlipVisionModel,\n+    KwargsForCausalLM,\n )\n \n from ...configuration_utils import PretrainedConfig\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n-from ...utils import add_start_docstrings_to_model_forward, logging\n+from ...processing_utils import Unpack\n+from ...utils import add_start_docstrings_to_model_forward, can_return_tuple, logging, replace_return_docstrings\n from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n@@ -196,6 +198,7 @@ class InstructBlipVideoForConditionalGenerationModelOutput(InstructBlipForCondit\n \n \n class InstructBlipVideoModel(InstructBlipModel):\n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(INSTRUCTBLIPVIDEO_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -211,6 +214,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         use_cache: Optional[bool] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, InstructBlipVideoForConditionalGenerationModelOutput]:\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n@@ -273,6 +277,7 @@ def forward(\n                 output_hidden_states=output_hidden_states,\n                 return_dict=return_dict,\n                 use_cache=use_cache,\n+                **kwargs,\n             )\n         else:\n             outputs = self.language_model(\n@@ -284,11 +289,9 @@ def forward(\n                 output_hidden_states=output_hidden_states,\n                 return_dict=return_dict,\n                 use_cache=use_cache,\n+                **kwargs,\n             )\n \n-        if not return_dict:\n-            return (vision_outputs, query_outputs, outputs)\n-\n         return InstructBlipVideoForConditionalGenerationModelOutput(\n             vision_outputs=vision_outputs,\n             qformer_outputs=query_outputs,\n@@ -297,6 +300,11 @@ def forward(\n \n \n class InstructBlipVideoForConditionalGeneration(InstructBlipForConditionalGeneration):\n+    @can_return_tuple\n+    @add_start_docstrings_to_model_forward(INSTRUCTBLIPVIDEO_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(\n+        output_type=InstructBlipVideoForConditionalGenerationModelOutput, config_class=InstructBlipVideoConfig\n+    )\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -312,6 +320,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         use_cache: Optional[bool] = None,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, InstructBlipVideoForConditionalGenerationModelOutput]:\n         r\"\"\"\n         ```python\n@@ -447,21 +456,15 @@ def forward(\n                 output_hidden_states=output_hidden_states,\n                 return_dict=return_dict,\n                 use_cache=use_cache,\n+                **kwargs,\n             )\n             logits = outputs.logits if return_dict else outputs[0]\n             loss = None\n-            # we compute the loss here since we need to take into account the sequence length of the query embeds\n             if labels is not None:\n-                labels = labels.to(logits.device)\n-                logits = logits[:, -labels.size(1) :, :]\n-                # Shift so that tokens < n predict n\n-                shift_logits = logits[..., :-1, :].contiguous()\n-                shift_labels = labels[..., 1:].contiguous().to(logits.device)\n-\n-                # Flatten the tokens\n-                loss_fct = CrossEntropyLoss(reduction=\"mean\")\n+                loss = self.loss_function(\n+                    logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+                )\n \n-                loss = loss_fct(shift_logits.view(-1, self.config.text_config.vocab_size), shift_labels.view(-1))\n         else:\n             outputs = self.language_model(\n                 inputs_embeds=inputs_embeds,\n@@ -473,14 +476,11 @@ def forward(\n                 return_dict=return_dict,\n                 labels=labels,\n                 use_cache=use_cache,\n+                **kwargs,\n             )\n             loss = outputs.loss if return_dict else outputs[0]\n             logits = outputs.logits if return_dict else outputs[1]\n \n-        if not return_dict:\n-            output = (logits, vision_outputs, query_outputs, outputs)\n-            return ((loss,) + output) if loss is not None else output\n-\n         return InstructBlipVideoForConditionalGenerationModelOutput(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "2b1683e497c625d8582c2e852a1a08231ea0ed16",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 15,
            "deletions": 8,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -35,6 +35,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n+    LossKwargs,\n     ModelOutput,\n     add_code_sample_docstrings,\n     add_start_docstrings,\n@@ -621,6 +622,7 @@ class InternVLPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n@@ -828,6 +830,7 @@ def get_image_features(\n \n         return vision_features\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(INTERNVL_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -845,7 +848,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         image_sizes: torch.Tensor = None,\n-        **lm_kwargs,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, InternVLModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -904,17 +907,16 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=True,\n             cache_position=cache_position,\n-            **lm_kwargs,\n+            **kwargs,\n         )\n \n-        output = InternVLModelOutputWithPast(\n+        return InternVLModelOutputWithPast(\n             last_hidden_state=outputs.last_hidden_state,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n             image_hidden_states=image_features if pixel_values is not None else None,\n         )\n-        return output if return_dict else output.to_tuple()\n \n     def pixel_shuffle(self, vision_features: torch.Tensor, scale_factor: float = 0.5):\n         \"\"\"Perform pixel shuffle downsampling on vision features.\n@@ -992,6 +994,9 @@ class InternVLCausalLMOutputWithPast(ModelOutput):\n     image_hidden_states: Optional[torch.FloatTensor] = None\n \n \n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n @add_start_docstrings(\n     \"\"\"The INTERNVL model which consists of a vision backbone and a language model.\"\"\",\n     INTERNVL_START_DOCSTRING,\n@@ -1056,8 +1061,8 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        image_sizes: torch.Tensor = None,\n-        **lm_kwargs,\n+        image_sizes: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, InternVLCausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1138,7 +1143,7 @@ def forward(\n             return_dict=True,\n             cache_position=cache_position,\n             image_sizes=image_sizes,\n-            **lm_kwargs,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -1148,7 +1153,9 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+            )\n \n         return InternVLCausalLMOutputWithPast(\n             loss=loss,"
        },
        {
            "sha": "b5eed0954994e4ad6fb4188a39a4d076ffba5973",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 58,
            "deletions": 63,
            "changes": 121,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -21,21 +21,24 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPooling,\n     CausalLMOutputWithCrossAttentions,\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n+    LossKwargs,\n     ModelOutput,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     logging,\n     replace_return_docstrings,\n     torch_int,\n@@ -466,7 +469,7 @@ def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding=Fals\n         return embeddings\n \n \n-# Copied from transformers.models.siglip.modeling_siglip.eager_attention_forward\n+# Adapted from transformers.models.siglip.modeling_siglip.eager_attention_forward -> Kosmos2 doesn't cast attn weights to fp32\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n@@ -481,7 +484,7 @@ def eager_attention_forward(\n     if attention_mask is not None:\n         attn_weights = attn_weights + attention_mask\n \n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)\n@@ -892,6 +895,7 @@ def __init__(\n         bias: bool = True,\n     ):\n         super().__init__()\n+        self.config = config\n         self.embed_dim = embed_dim\n         self.num_heads = num_heads\n         self.dropout = dropout\n@@ -929,6 +933,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n@@ -953,8 +958,7 @@ def forward(\n                 key_states = torch.cat([past_key_value[0], key_states], dim=2)\n                 value_states = torch.cat([past_key_value[1], value_states], dim=2)\n \n-        query_states = self._shape(self.q_proj(hidden_states) * self.scaling)\n-        attn_weights = torch.matmul(query_states, key_states.transpose(-1, -2))\n+        query_states = self._shape(self.q_proj(hidden_states))\n \n         if self.is_decoder:\n             # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n@@ -966,32 +970,33 @@ def forward(\n             # if encoder bi-directional self-attention `past_key_value` is always `None`\n             past_key_value = (key_states, value_states)\n \n-        src_len = key_states.size(2)\n+        attention_interface: Callable = eager_attention_forward\n \n-        if attention_mask is not None:\n-            if attention_mask.size() != (batch_size, 1, seq_length, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(batch_size, 1, seq_length, src_len)}, but is {attention_mask.size()}\"\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n                 )\n-            attn_weights = attn_weights + attention_mask\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        # Mask heads if we want to\n-        if layer_head_mask is not None:\n-            attn_weights = attn_weights * layer_head_mask\n-\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        #  attn_output = torch.bmm(attn_probs, value_states) ?\n-        context_states = torch.matmul(attn_weights, value_states)\n-        # attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim) ?\n-        context_states = context_states.permute(0, 2, 1, 3).contiguous().view(batch_size, seq_length, -1)\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n \n+        attn_output = attn_output.reshape(batch_size, seq_length, -1).contiguous()\n         if self.inner_attn_ln is not None:\n-            context_states = self.inner_attn_ln(context_states)\n+            attn_output = self.inner_attn_ln(attn_output)\n \n-        attn_output = self.out_proj(context_states)\n+        attn_output = self.out_proj(attn_output)\n \n         return attn_output, attn_weights, past_key_value\n \n@@ -1060,6 +1065,7 @@ def forward(\n         past_key_value: Optional[Tuple[torch.Tensor]] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n+        **kwargs,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n \n@@ -1076,6 +1082,7 @@ def forward(\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n+            **kwargs,\n         )\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n         hidden_states = residual + hidden_states\n@@ -1103,6 +1110,7 @@ def forward(\n                 layer_head_mask=cross_attn_layer_head_mask,\n                 past_key_value=cross_attn_past_key_value,\n                 output_attentions=output_attentions,\n+                **kwargs,\n             )\n             hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n             hidden_states = residual + hidden_states\n@@ -1216,6 +1224,7 @@ def forward_embedding(\n \n         return hidden_states\n \n+    @can_return_tuple\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -1233,6 +1242,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1338,6 +1348,7 @@ def forward(\n                     past_key_value=past_key_value,\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n+                    **kwargs,\n                 )\n             hidden_states = layer_outputs[0]\n \n@@ -1357,18 +1368,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        if not return_dict:\n-            return tuple(\n-                v\n-                for v in [\n-                    hidden_states,\n-                    present_key_value_states,\n-                    all_hidden_states,\n-                    all_self_attns,\n-                    all_cross_attentions,\n-                ]\n-                if v is not None\n-            )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n             past_key_values=present_key_value_states,\n@@ -1387,6 +1386,9 @@ class Kosmos2PreTrainedModel(PreTrainedModel):\n     config_class = Kosmos2Config\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Kosmos2VisionEncoderLayer\", \"Kosmos2TextBlock\"]\n+    _supports_attention_backend = True\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n@@ -1525,6 +1527,7 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(KOSMOS2_TEXT_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=BaseModelOutputWithPastAndCrossAttentions, config_class=Kosmos2TextConfig)\n     def forward(\n@@ -1544,6 +1547,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         r\"\"\"\n         Returns:\n@@ -1565,9 +1569,13 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            **kwargs,\n         )\n \n \n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n @add_start_docstrings(\n     \"\"\"\n     The text model from KOSMOS-2 with a language modeling head on top (linear layer with weights tied to the input\n@@ -1600,6 +1608,7 @@ def get_output_embeddings(self) -> nn.Module:\n     def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(KOSMOS2_TEXT_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=Kosmos2TextConfig)\n     def forward(\n@@ -1620,6 +1629,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1652,27 +1662,14 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n         lm_logits = self.lm_head(outputs[0])\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n-            labels = labels.to(lm_logits.device)\n-            # Shift so that tokens < n predict n\n-            shift_logits = lm_logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            batch_size, seq_length, vocab_size = shift_logits.shape\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(\n-                shift_logits.view(batch_size * seq_length, vocab_size), shift_labels.view(batch_size * seq_length)\n-            )\n-\n-        if not return_dict:\n-            output = (lm_logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n+            loss = self.loss_function(logits=lm_logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         return CausalLMOutputWithCrossAttentions(\n             loss=loss,\n@@ -1804,6 +1801,7 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value):\n         self.text_model.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(KOSMOS2_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=Kosmos2ModelOutput, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -1822,6 +1820,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, Kosmos2ModelOutput]:\n         r\"\"\"\n         Returns:\n@@ -1893,13 +1892,10 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n-        if not return_dict:\n-            outputs = outputs + (image_embeds, projection_attentions, vision_model_output)\n-            return tuple(output for output in outputs if output is not None)\n-\n         return Kosmos2ModelOutput(\n             last_hidden_state=outputs.last_hidden_state,\n             past_key_values=outputs.past_key_values,\n@@ -1946,6 +1942,7 @@ def get_output_embeddings(self) -> nn.Module:\n     def set_output_embeddings(self, new_embeddings):\n         self.text_model.set_output_embeddings(new_embeddings)\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(KOSMOS2_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=Kosmos2ForConditionalGenerationModelOutput, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -1964,6 +1961,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, Kosmos2ForConditionalGenerationModelOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -2048,13 +2046,10 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n-        if not return_dict:\n-            outputs = lm_outputs + (image_embeds, projection_attentions, vision_model_output)\n-            return tuple(output for output in outputs if output is not None)\n-\n         return Kosmos2ForConditionalGenerationModelOutput(\n             loss=lm_outputs.loss,\n             logits=lm_outputs.logits,"
        },
        {
            "sha": "00f924b4d74b15159075044e94e8faa9ea5402b4",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 17,
            "deletions": 13,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -39,8 +39,10 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n+    LossKwargs,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n@@ -244,6 +246,7 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n+# Adapted from transformers.models.llama.modeling_llama.eager_attention_forward -> llama4 doesn't cast attn weights to fp32\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n@@ -256,12 +259,13 @@ def eager_attention_forward(\n ):\n     key_states = repeat_kv(key, module.num_key_value_groups)\n     value_states = repeat_kv(value, module.num_key_value_groups)\n-    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) / math.sqrt(module.head_dim)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n     if attention_mask is not None:\n         causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n         attn_weights = attn_weights + causal_mask\n \n-    attn_weights = nn.functional.softmax(attn_weights.float(), dim=-1).to(query.dtype)\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n     attn_output = torch.matmul(attn_weights, value_states)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n@@ -607,6 +611,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(LLAMA4_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -712,13 +717,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-        return output if return_dict else output.to_tuple()\n \n     @torch.compiler.disable(recursive=False)  # the operations in this method are not compilable\n     def _update_causal_mask(\n@@ -931,6 +935,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         return causal_mask\n \n \n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n class Llama4ForCausalLM(Llama4PreTrainedModel, GenerationMixin):\n     _no_split_modules = [\"Llama4TextDecoderLayer\"]\n     base_model_prefix = \"language_model\"\n@@ -965,6 +972,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(LLAMA4_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -981,7 +989,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        **kwargs,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1031,7 +1039,7 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -1044,10 +1052,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n@@ -1208,6 +1212,7 @@ def __init__(self, config: Llama4VisionConfig):\n         self.head_dim = config.hidden_size // config.num_attention_heads\n         self.num_key_value_groups = 1\n         self.attention_dropout = config.attention_dropout\n+        self.scaling = self.head_dim**-0.5\n \n         self.q_proj = nn.Linear(self.embed_dim, self.num_heads * self.head_dim, bias=True)\n         self.k_proj = nn.Linear(self.embed_dim, self.num_heads * self.head_dim, bias=True)\n@@ -1593,7 +1598,6 @@ class Llama4ForConditionalGeneration(Llama4PreTrainedModel, GenerationMixin):\n     _tp_plan = {}\n     base_model_prefix = \"\"\n     config_class = Llama4Config\n-    _supports_flex_attn = True\n \n     def __init__(self, config: Llama4Config):\n         super().__init__(config)\n@@ -1673,7 +1677,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         image_sizes: torch.Tensor = None,\n-        **lm_kwargs,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, Llama4CausalLMOutputWithPast]:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1780,7 +1784,7 @@ def forward(\n             return_dict=return_dict,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n-            **lm_kwargs,\n+            **kwargs,\n         )\n \n         logits = outputs[0]"
        },
        {
            "sha": "42fc86c77ccd44f66931d2459e0d2e29eccaf79a",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 17,
            "deletions": 8,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -23,9 +23,12 @@\n \n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_utils import PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n+    LossKwargs,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     can_return_tuple,\n@@ -171,6 +174,7 @@ class LlavaPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         # important: this ported version of Llava isn't meant for training from scratch - only\n@@ -331,6 +335,7 @@ def get_image_features(\n         image_features = self.multi_modal_projector(selected_image_feature)\n         return image_features\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(LLAVA_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -348,7 +353,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         image_sizes: torch.Tensor = None,\n-        **lm_kwargs,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, LlavaModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -407,17 +412,19 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=True,\n             cache_position=cache_position,\n-            **lm_kwargs,\n+            **kwargs,\n         )\n \n-        output = LlavaModelOutputWithPast(\n+        return LlavaModelOutputWithPast(\n             last_hidden_state=outputs.last_hidden_state,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n             image_hidden_states=image_features if pixel_values is not None else None,\n         )\n-        return output if return_dict else output.to_tuple()\n+\n+\n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n \n \n @add_start_docstrings(\n@@ -484,8 +491,8 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        image_sizes: torch.Tensor = None,\n-        **lm_kwargs,\n+        image_sizes: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, LlavaCausalLMOutputWithPast]:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -553,7 +560,7 @@ def forward(\n             return_dict=True,\n             cache_position=cache_position,\n             image_sizes=image_sizes,\n-            **lm_kwargs,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -563,7 +570,9 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+            )\n \n         return LlavaCausalLMOutputWithPast(\n             loss=loss,"
        },
        {
            "sha": "ff1cd57d9dc1f0bf1f031836d821674f66109839",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 16,
            "deletions": 7,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -26,9 +26,12 @@\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n from ...image_processing_utils import select_best_resolution\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_utils import PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n+    LossKwargs,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     can_return_tuple,\n@@ -280,6 +283,7 @@ class LlavaNextPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n@@ -528,6 +532,7 @@ def get_image_features(\n         image_features = torch.split(image_features, image_num_patches, dim=0)\n         return image_features\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(LLAVA_NEXT_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -545,7 +550,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **lm_kwargs,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, LlavaNextModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -609,17 +614,19 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=True,\n             cache_position=cache_position,\n-            **lm_kwargs,\n+            **kwargs,\n         )\n \n-        output = LlavaNextModelOutputWithPast(\n+        return LlavaNextModelOutputWithPast(\n             last_hidden_state=outputs.last_hidden_state,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n             image_hidden_states=image_features if pixel_values is not None else None,\n         )\n-        return output if return_dict else output.to_tuple()\n+\n+\n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n \n \n @add_start_docstrings(\n@@ -688,7 +695,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        **lm_kwargs,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, LlavaNextCausalLMOutputWithPast]:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -756,7 +763,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n-            **lm_kwargs,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -766,7 +773,9 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+            )\n \n         return LlavaNextCausalLMOutputWithPast(\n             loss=loss,"
        },
        {
            "sha": "f124e3b4c1f70a010e5c4bd9d2834f7c802308ae",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 17,
            "deletions": 8,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -30,9 +30,12 @@\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n from ...image_processing_utils import select_best_resolution\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_utils import PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n+    LossKwargs,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     can_return_tuple,\n@@ -223,6 +226,7 @@ class LlavaNextVideoPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n@@ -581,6 +585,7 @@ def get_image_features(\n         image_features = torch.split(image_features, image_num_patches, dim=0)\n         return image_features\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(LLAVA_NEXT_VIDEO_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -599,7 +604,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **lm_kwargs,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, LlavaNextVideoModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -684,18 +689,17 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=True,\n             cache_position=cache_position,\n-            **lm_kwargs,\n+            **kwargs,\n         )\n \n-        output = LlavaNextVideoModelOutputWithPast(\n+        return LlavaNextVideoModelOutputWithPast(\n             last_hidden_state=outputs.last_hidden_state,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n             image_hidden_states=image_features if pixel_values is not None else None,\n             video_hidden_states=video_features if pixel_values_videos is not None else None,\n         )\n-        return output if return_dict else output.to_tuple()\n \n     def get_video_features(\n         self,\n@@ -744,6 +748,9 @@ def get_video_features(\n         return video_features\n \n \n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n @add_start_docstrings(\n     \"\"\"The LLAVA-NeXT model which consists of a vision backbone and a language model.\"\"\",\n     LLAVA_NEXT_VIDEO_START_DOCSTRING,\n@@ -811,7 +818,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        **lm_kwargs,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, LlavaNextVideoCausalLMOutputWithPast]:\n         r\"\"\"\n             pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, num_frames, num_channels, image_size, image_size)):\n@@ -915,10 +922,10 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n             cache_position=cache_position,\n             image_sizes=image_sizes,\n-            **lm_kwargs,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -928,7 +935,9 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+            )\n \n         return LlavaNextVideoCausalLMOutputWithPast(\n             loss=loss,"
        },
        {
            "sha": "cacf167a86eb3abfacd6a4c06eee7ea039bfc508",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 12,
            "deletions": 8,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -22,6 +22,7 @@\n from torch import nn\n \n from transformers.models.llava_next.modeling_llava_next import (\n+    KwargsForCausalLM,\n     LlavaNextCausalLMOutputWithPast,\n     LlavaNextForConditionalGeneration,\n     LlavaNextModel,\n@@ -31,6 +32,8 @@\n )\n \n from ...configuration_utils import PretrainedConfig\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...processing_utils import Unpack\n from ...utils import (\n     add_start_docstrings_to_model_forward,\n     can_return_tuple,\n@@ -378,7 +381,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **lm_kwargs,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, LlavaNextVideoModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -463,18 +466,17 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=True,\n             cache_position=cache_position,\n-            **lm_kwargs,\n+            **kwargs,\n         )\n \n-        output = LlavaNextVideoModelOutputWithPast(\n+        return LlavaNextVideoModelOutputWithPast(\n             last_hidden_state=outputs.last_hidden_state,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n             image_hidden_states=image_features if pixel_values is not None else None,\n             video_hidden_states=video_features if pixel_values_videos is not None else None,\n         )\n-        return output if return_dict else output.to_tuple()\n \n \n LLAVA_NEXT_VIDEO_INPUTS_DOCSTRING = r\"\"\"\n@@ -580,7 +582,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        **lm_kwargs,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, LlavaNextVideoCausalLMOutputWithPast]:\n         r\"\"\"\n             pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, num_frames, num_channels, image_size, image_size)):\n@@ -684,10 +686,10 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n             cache_position=cache_position,\n             image_sizes=image_sizes,\n-            **lm_kwargs,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -697,7 +699,9 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+            )\n \n         return LlavaNextVideoCausalLMOutputWithPast(\n             loss=loss,"
        },
        {
            "sha": "73700e2448f0d6363d621308d1460ad40e6b5191",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 16,
            "deletions": 8,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -30,9 +30,12 @@\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n from ...image_processing_utils import select_best_resolution\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_utils import PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n+    LossKwargs,\n     add_start_docstrings,\n     can_return_tuple,\n     is_torchdynamo_compiling,\n@@ -405,6 +408,7 @@ class LlavaOnevisionPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n@@ -570,6 +574,7 @@ def get_image_features(\n         image_features = torch.split(image_features, image_num_patches, dim=0)\n         return image_features\n \n+    @can_return_tuple\n     @add_start_docstrings(LLAVA_ONEVISION_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -590,7 +595,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **lm_kwargs,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, LlavaOnevisionModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -681,10 +686,10 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=True,\n             cache_position=cache_position,\n-            **lm_kwargs,\n+            **kwargs,\n         )\n \n-        output = LlavaOnevisionModelOutputWithPast(\n+        return LlavaOnevisionModelOutputWithPast(\n             last_hidden_state=outputs.last_hidden_state,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n@@ -693,8 +698,6 @@ def forward(\n             video_hidden_states=video_features if pixel_values_videos is not None else None,\n         )\n \n-        return output if return_dict else output.to_tuple()\n-\n     def get_video_features(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -756,6 +759,9 @@ def apply_pooling(self, image_features):\n         return image_features\n \n \n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n @add_start_docstrings(\n     \"\"\"The LLAVA-NeXT model which consists of a vision backbone and a language model.\"\"\",\n     LLAVA_ONEVISION_START_DOCSTRING,\n@@ -824,7 +830,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        **lm_kwargs,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, LlavaOnevisionCausalLMOutputWithPast]:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -909,7 +915,7 @@ def forward(\n             return_dict=True,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n-            **lm_kwargs,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -919,7 +925,9 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+            )\n \n         return LlavaOnevisionCausalLMOutputWithPast(\n             loss=loss,"
        },
        {
            "sha": "c96d2e36195ec35e2731280b0a78a8db61feb840",
            "filename": "src/transformers/models/llava_onevision/modular_llava_onevision.py",
            "status": "modified",
            "additions": 12,
            "deletions": 8,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -21,6 +21,7 @@\n \n from transformers.models.llava_next.image_processing_llava_next_fast import LlavaNextImageProcessorFast\n from transformers.models.llava_next_video.modeling_llava_next_video import (\n+    KwargsForCausalLM,\n     LlavaNextVideoCausalLMOutputWithPast,\n     LlavaNextVideoForConditionalGeneration,\n     LlavaNextVideoModel,\n@@ -36,6 +37,8 @@\n     OPENAI_CLIP_STD,\n     PILImageResampling,\n )\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...processing_utils import Unpack\n from ...utils import add_start_docstrings, can_return_tuple, is_torchdynamo_compiling, logging\n \n \n@@ -217,6 +220,7 @@ def get_video_features(\n \n         return video_features\n \n+    @can_return_tuple\n     @add_start_docstrings(LLAVA_ONEVISION_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -237,7 +241,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **lm_kwargs,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, LlavaOnevisionModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -328,10 +332,10 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=True,\n             cache_position=cache_position,\n-            **lm_kwargs,\n+            **kwargs,\n         )\n \n-        output = LlavaOnevisionModelOutputWithPast(\n+        return LlavaOnevisionModelOutputWithPast(\n             last_hidden_state=outputs.last_hidden_state,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n@@ -340,8 +344,6 @@ def forward(\n             video_hidden_states=video_features if pixel_values_videos is not None else None,\n         )\n \n-        return output if return_dict else output.to_tuple()\n-\n \n class LlavaOnevisionForConditionalGeneration(LlavaNextVideoForConditionalGeneration):\n     @can_return_tuple\n@@ -367,7 +369,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        **lm_kwargs,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, LlavaOnevisionCausalLMOutputWithPast]:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -452,7 +454,7 @@ def forward(\n             return_dict=True,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n-            **lm_kwargs,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -462,7 +464,9 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+            )\n \n         return LlavaOnevisionCausalLMOutputWithPast(\n             loss=loss,"
        },
        {
            "sha": "24cc21af6096a2708c053d541499b3d977570e64",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 94,
            "deletions": 86,
            "changes": 180,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -28,9 +28,12 @@\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_utils import PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n+    LossKwargs,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     can_return_tuple,\n@@ -233,6 +236,7 @@ class Mistral3PreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         # important: this ported version of Mistral3 isn't meant for training from scratch - only\n@@ -251,83 +255,6 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n \n \n-MISTRAL3_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):\n-            The tensors corresponding to the input images. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details ([]`Mistral3Processor`] uses\n-            [`CLIPImageProcessor`] for processing images).\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n-            `past_key_values`).\n-\n-            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n-            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n-            information on the default strategy.\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n-            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        vision_feature_layer (`Union[int, List[int]], *optional*, defaults to -2`):\n-            The index of the layer to select the vision feature. If multiple indices are provided,\n-            the vision feature of the corresponding indices will be concatenated to form the\n-            vision features.\n-        vision_feature_select_strategy (`str`, *optional*, defaults to `\"default\"`):\n-            The feature selection strategy used to select the vision feature from the vision backbone.\n-            Can be one of `\"default\"` or `\"full\"`.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n-            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n-            the complete sequence length.\n-\"\"\"\n-\n-\n @add_start_docstrings(\n     \"\"\"The Mistral3 model which consists of a vision backbone and a language model, without a language modeling head.\"\"\",\n     MISTRAL3_START_DOCSTRING,\n@@ -385,7 +312,7 @@ def get_image_features(\n         image_features = self.multi_modal_projector(selected_image_feature.squeeze(0), image_sizes)\n         return image_features\n \n-    @add_start_docstrings_to_model_forward(MISTRAL3_INPUTS_DOCSTRING)\n+    @can_return_tuple\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n@@ -401,7 +328,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         image_sizes: torch.Tensor = None,\n-        **lm_kwargs,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, Mistral3ModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -451,17 +378,96 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=True,\n             cache_position=cache_position,\n-            **lm_kwargs,\n+            **kwargs,\n         )\n \n-        output = Mistral3ModelOutputWithPast(\n+        return Mistral3ModelOutputWithPast(\n             last_hidden_state=outputs.last_hidden_state,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n             image_hidden_states=image_features if pixel_values is not None else None,\n         )\n-        return output if return_dict else output.to_tuple()\n+\n+\n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n+MISTRAL3_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):\n+            The tensors corresponding to the input images. Pixel values can be obtained using\n+            [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details ([]`Mistral3Processor`] uses\n+            [`CLIPImageProcessor`] for processing images).\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n+            `past_key_values`).\n+\n+            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n+            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n+            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n+            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n+            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        vision_feature_layer (`Union[int, List[int]], *optional*, defaults to -2`):\n+            The index of the layer to select the vision feature. If multiple indices are provided,\n+            the vision feature of the corresponding indices will be concatenated to form the\n+            vision features.\n+        vision_feature_select_strategy (`str`, *optional*, defaults to `\"default\"`):\n+            The feature selection strategy used to select the vision feature from the vision backbone.\n+            Can be one of `\"default\"` or `\"full\"`.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n+\"\"\"\n \n \n @add_start_docstrings(\n@@ -526,8 +532,8 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        image_sizes: torch.Tensor = None,\n-        **lm_kwargs,\n+        image_sizes: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, Mistral3CausalLMOutputWithPast]:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -585,7 +591,7 @@ def forward(\n             return_dict=True,\n             cache_position=cache_position,\n             image_sizes=image_sizes,\n-            **lm_kwargs,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -595,7 +601,9 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+            )\n \n         return Mistral3CausalLMOutputWithPast(\n             loss=loss,"
        },
        {
            "sha": "11dcfa2c3a4a00025991d553cf053f3b8568b691",
            "filename": "src/transformers/models/mistral3/modular_mistral3.py",
            "status": "modified",
            "additions": 13,
            "deletions": 8,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -19,6 +19,8 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...processing_utils import Unpack\n from ...utils import (\n     add_start_docstrings_to_model_forward,\n     can_return_tuple,\n@@ -27,6 +29,7 @@\n     replace_return_docstrings,\n )\n from ..llava.modeling_llava import (\n+    KwargsForCausalLM,\n     LlavaCausalLMOutputWithPast,\n     LlavaForConditionalGeneration,\n     LlavaModel,\n@@ -174,6 +177,7 @@ def get_image_features(\n         image_features = self.multi_modal_projector(selected_image_feature.squeeze(0), image_sizes)\n         return image_features\n \n+    @can_return_tuple\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n@@ -189,7 +193,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         image_sizes: torch.Tensor = None,\n-        **lm_kwargs,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, Mistral3ModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -239,17 +243,16 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=True,\n             cache_position=cache_position,\n-            **lm_kwargs,\n+            **kwargs,\n         )\n \n-        output = Mistral3ModelOutputWithPast(\n+        return Mistral3ModelOutputWithPast(\n             last_hidden_state=outputs.last_hidden_state,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n             image_hidden_states=image_features if pixel_values is not None else None,\n         )\n-        return output if return_dict else output.to_tuple()\n \n \n class Mistral3ForConditionalGeneration(LlavaForConditionalGeneration):\n@@ -271,8 +274,8 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        image_sizes: torch.Tensor = None,\n-        **lm_kwargs,\n+        image_sizes: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, Mistral3CausalLMOutputWithPast]:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -330,7 +333,7 @@ def forward(\n             return_dict=True,\n             cache_position=cache_position,\n             image_sizes=image_sizes,\n-            **lm_kwargs,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -340,7 +343,9 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+            )\n \n         return Mistral3CausalLMOutputWithPast(\n             loss=loss,"
        },
        {
            "sha": "300b172c2d8af1fa6581168c12300b2caf538ea9",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 136,
            "deletions": 284,
            "changes": 420,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -15,21 +15,24 @@\n \"\"\"PyTorch Mllama model.\"\"\"\n \n import math\n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n import torch.nn.functional as F\n import torch.utils.checkpoint\n from torch import nn\n \n-from ... import PreTrainedModel\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n+    LossKwargs,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     can_return_tuple,\n@@ -180,13 +183,56 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n+# Copied from transformers.models.llama.modeling_llama.repeat_kv\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+# Copied from transformers.models.llama.modeling_llama.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class MllamaVisionAttention(nn.Module):\n     def __init__(self, config: MllamaVisionConfig):\n         super().__init__()\n \n+        self.config = config\n         self.embed_dim = config.hidden_size\n         self.num_heads = config.attention_heads\n         self.head_dim = config.hidden_size // config.attention_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.num_key_value_groups = 1\n \n         self.q_proj = nn.Linear(self.embed_dim, self.num_heads * self.head_dim, bias=False)\n         self.k_proj = nn.Linear(self.embed_dim, self.num_heads * self.head_dim, bias=False)\n@@ -198,6 +244,7 @@ def forward(\n         hidden_state: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n+        **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n         query = self.q_proj(hidden_state)\n         key = self.k_proj(hidden_state)\n@@ -210,73 +257,35 @@ def forward(\n         key = key.view(batch_size, kv_seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n         value = value.view(batch_size, kv_seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n \n-        attn_weights = torch.matmul(query, key.transpose(2, 3)) / math.sqrt(self.head_dim)\n-\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n+        attention_interface: Callable = eager_attention_forward\n \n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n-        attn_output = torch.matmul(attn_weights, value)\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.reshape(batch_size, q_seq_len, -1)\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query,\n+            key,\n+            value,\n+            attention_mask,\n+            dropout=0.0,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n \n-        output = self.o_proj(attn_output)\n+        attn_output = attn_output.reshape(batch_size, q_seq_len, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n \n         if not output_attentions:\n             attn_weights = None\n \n-        return output, attn_weights\n-\n-\n-class MllamaVisionSdpaAttention(MllamaVisionAttention):\n-    # Adapted from MllamaVisionAttention\n-    def forward(\n-        self,\n-        hidden_state: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-    ) -> torch.Tensor:\n-        # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-        if output_attentions:\n-            logger.warning_once(\n-                \"MllamaModel is using MllamaVisionSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_state=hidden_state,\n-                attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n-            )\n-\n-        query = self.q_proj(hidden_state)\n-        key = self.k_proj(hidden_state)\n-        value = self.v_proj(hidden_state)\n-\n-        batch_size, q_seq_len, _ = query.shape\n-        _, kv_seq_len, _ = key.shape\n-\n-        query = query.view(batch_size, q_seq_len, self.num_heads, self.head_dim)\n-        key = key.view(batch_size, kv_seq_len, self.num_heads, self.head_dim)\n-        value = value.view(batch_size, kv_seq_len, self.num_heads, self.head_dim)\n-\n-        query = query.transpose(1, 2)\n-        key = key.transpose(1, 2)\n-        value = value.transpose(1, 2)\n-\n-        attn_output = F.scaled_dot_product_attention(query, key, value, attn_mask=attention_mask)\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.reshape(batch_size, q_seq_len, -1)\n-\n-        output = self.o_proj(attn_output)\n-\n-        return output, None\n-\n-\n-MLLAMA_VISION_ATTENTION_CLASSES = {\"eager\": MllamaVisionAttention, \"sdpa\": MllamaVisionSdpaAttention}\n+        return attn_output, attn_weights\n \n \n class MllamaVisionEncoderLayer(nn.Module):\n@@ -288,7 +297,7 @@ def __init__(self, config: MllamaVisionConfig, is_gated: bool = False):\n         self.is_gated = is_gated\n         self.intermediate_size = config.intermediate_size\n \n-        self.self_attn = MLLAMA_VISION_ATTENTION_CLASSES[config._attn_implementation](config)\n+        self.self_attn = MllamaVisionAttention(config)\n         self.mlp = MllamaVisionMLP(config)\n \n         self.input_layernorm = nn.LayerNorm(self.hidden_size, eps=config.norm_eps)\n@@ -453,6 +462,7 @@ def __init__(\n         self.head_dim = config.hidden_size // self.num_heads\n         self.layer_idx = layer_idx\n         self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n \n         self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n         self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n@@ -471,6 +481,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n         bsz, q_len, _ = hidden_states.size()\n@@ -503,117 +514,35 @@ def forward(\n                 \"Cross attention layer can't find neither `cross_attn_states` nor cached values for key/values!\"\n             )\n \n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n-\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.reshape(bsz, q_len, -1)\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class MllamaTextCrossSdpaAttention(MllamaTextCrossAttention):\n-    \"\"\"\n-    Mllama attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `MllamaTextCrossAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n-    \"\"\"\n-\n-    # Adapted from MllamaTextCrossAttention.forward\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        cross_attention_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-        use_cache: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"MllamaModel is using MllamaTextCrossSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                cross_attention_states=cross_attention_states,\n-                attention_mask=attention_mask,\n-                past_key_value=past_key_value,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-        query_states = self.q_proj(hidden_states)\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        query_states = self.q_norm(query_states)\n-\n-        if cross_attention_states is not None:\n-            key_states = self.k_proj(cross_attention_states)\n-            value_states = self.v_proj(cross_attention_states)\n-            key_states = key_states.view(bsz, -1, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-            value_states = value_states.view(bsz, -1, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        attention_interface: Callable = eager_attention_forward\n \n-            if past_key_value is not None:\n-                # if we have a new image + new tokens, we only computed key_states on that new image\n-                # we still update the cross key states, past_image, new_image. And use it!\n-                key_states, value_states = past_key_value.update(\n-                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n                 )\n-        elif cache_position[0] != 0:\n-            key_states, value_states = (\n-                past_key_value.key_cache[self.layer_idx],\n-                past_key_value.value_cache[self.layer_idx],\n-            )\n-        else:\n-            raise ValueError(\n-                \"Cross attention layer can't find neither `cross_attn_states` nor cached values for key/values!\"\n-            )\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        key_states = self.k_norm(key_states)\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and attention_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if attention_mask is None and q_len > 1 else False\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n-            attn_mask=attention_mask,\n-            dropout_p=self.dropout if self.training else 0.0,\n-            is_causal=is_causal,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n         )\n \n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.reshape(bsz, q_len, -1)\n+        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n \n-        return attn_output, None, past_key_value\n+        if not output_attentions:\n+            attn_weights = None\n+\n+        return attn_output, attn_weights, past_key_value\n \n \n # Copied from transformers.models.llama.modeling_llama.rotate_half\n@@ -652,19 +581,6 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     return q_embed, k_embed\n \n \n-# Copied from transformers.models.llama.modeling_llama.repeat_kv\n-def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n-    \"\"\"\n-    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n-    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n-    \"\"\"\n-    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n-    if n_rep == 1:\n-        return hidden_states\n-    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n-    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n-\n-\n class MllamaTextSelfAttention(nn.Module):\n     def __init__(self, config: MllamaTextConfig, layer_idx: int):\n         super().__init__()\n@@ -675,6 +591,7 @@ def __init__(self, config: MllamaTextConfig, layer_idx: int):\n         self.num_key_value_heads = config.num_key_value_heads\n         self.head_dim = config.hidden_size // self.num_heads\n         self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n         self.rope_theta = config.rope_theta\n         self.layer_idx = layer_idx\n \n@@ -712,115 +629,35 @@ def forward(\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n-\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, -1)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class MllamaTextSelfSdpaAttention(MllamaTextSelfAttention):\n-    # Adapted from MllamaTextSelfAttention\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: torch.Tensor,\n-        position_embeddings: torch.Tensor,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        past_key_value=None,\n-        cache_position=None,\n-        **kwargs,\n-    ):\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"MllamaModel is using MllamaTextSelfSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_embeddings=position_embeddings,\n-                past_key_value=past_key_value,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-                **kwargs,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = position_embeddings\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        causal_mask = attention_mask\n-        if attention_mask is not None:\n-            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n+        attention_interface: Callable = eager_attention_forward\n \n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and causal_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.dropout if self.training else 0.0,\n-            is_causal=is_causal,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n         )\n \n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, -1)\n-\n+        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n-        return attn_output, None, past_key_value\n \n+        if not output_attentions:\n+            attn_weights = None\n \n-MLLAMA_TEXT_CROSS_ATTENTION_CLASSES = {\"eager\": MllamaTextCrossAttention, \"sdpa\": MllamaTextCrossSdpaAttention}\n-MLLAMA_TEXT_ATTENTION_CLASSES = {\"eager\": MllamaTextSelfAttention, \"sdpa\": MllamaTextSelfSdpaAttention}\n+        return attn_output, attn_weights, past_key_value\n \n \n # Copied from transformers.models.gemma2.modeling_gemma2.Gemma2MLP with Gemma2->MllamaText\n@@ -847,7 +684,7 @@ def __init__(self, config: MllamaTextConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n \n-        self.self_attn = MLLAMA_TEXT_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n+        self.self_attn = MllamaTextSelfAttention(config=config, layer_idx=layer_idx)\n \n         self.mlp = MllamaTextMLP(config)\n         self.input_layernorm = MllamaTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n@@ -868,6 +705,7 @@ def forward(\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n@@ -905,6 +743,7 @@ def forward(\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n+            **kwargs,\n         )\n         hidden_states = residual + hidden_states\n \n@@ -931,7 +770,7 @@ class MllamaCrossAttentionDecoderLayer(torch.nn.Module):\n     def __init__(self, config: MllamaTextConfig, layer_idx: int) -> None:\n         super().__init__()\n         self.layer_idx = layer_idx\n-        self.cross_attn = MLLAMA_TEXT_CROSS_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx=layer_idx)\n+        self.cross_attn = MllamaTextCrossAttention(config, layer_idx=layer_idx)\n \n         self.input_layernorm = MllamaTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.cross_attn_attn_gate = torch.nn.Parameter(torch.zeros(1))\n@@ -953,6 +792,7 @@ def forward(\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor]:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n@@ -964,6 +804,7 @@ def forward(\n             past_key_value=past_key_value,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n         hidden_states = residual + self.cross_attn_attn_gate.tanh() * hidden_states\n \n@@ -1026,7 +867,9 @@ class MllamaPreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_static_cache = False  # static cache cannot have different shapes for each layer\n     _supports_sdpa = True\n+    _supports_flash_attn_2 = True\n     _supports_quantized_cache = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n@@ -1694,6 +1537,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         \"\"\"\n \n@@ -1804,6 +1648,7 @@ def forward(\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n                     position_embeddings=position_embeddings,\n+                    **kwargs,\n                 )\n \n             hidden_states = layer_outputs[0]\n@@ -1832,6 +1677,9 @@ def forward(\n         )\n \n \n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n @add_start_docstrings(\n     \"\"\"The Mllama Text Model with a language modeling head on top.\"\"\",\n     MLLAMA_START_DOCSTRING,\n@@ -1888,7 +1736,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        **loss_kwargs,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1945,6 +1793,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -1953,7 +1802,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits, labels, self.vocab_size, **loss_kwargs)\n+            loss = self.loss_function(logits, labels, self.vocab_size, **kwargs)\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]\n@@ -1999,6 +1848,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(MLLAMA_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -2017,6 +1867,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -2079,15 +1930,15 @@ def forward(\n             output_attentions=output_attentions,\n             return_dict=True,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=outputs.last_hidden_state,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n-        return output if return_dict else output.to_tuple()\n \n \n @add_start_docstrings(\n@@ -2153,7 +2004,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        **loss_kwargs,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -2220,6 +2071,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=True,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -2229,7 +2081,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits, labels, self.config.text_config.vocab_size, **loss_kwargs)\n+            loss = self.loss_function(logits, labels, self.config.text_config.vocab_size, **kwargs)\n \n         return CausalLMOutputWithPast(\n             loss=loss,"
        },
        {
            "sha": "1da69740a7ef500cdb2777f0b90453f43850b605",
            "filename": "src/transformers/models/moshi/configuration_moshi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fmoshi%2Fconfiguration_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fmoshi%2Fconfiguration_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fconfiguration_moshi.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -236,7 +236,7 @@ class MoshiConfig(PretrainedConfig):\n \n     model_type = \"moshi\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n-    sub_configs = {\"audio_encoder_config\": AutoConfig}\n+    sub_configs = {\"audio_encoder_config\": AutoConfig, \"depth_decoder_config\": MoshiDepthConfig}\n \n     def __init__(\n         self,"
        },
        {
            "sha": "4bc31452a3a18ae9e38e81a95e34b291ce061a3c",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -1907,7 +1907,7 @@ def __init__(self, config: MoshiConfig):\n         self.audio_encoder = AutoModel.from_config(config.audio_encoder_config)\n         self.decoder = MoshiForCausalLM(config)\n \n-        self.depth_decoder = MoshiDepthDecoder(config.depth_decoder_config)\n+        self.depth_decoder = MoshiDepthDecoder._from_config(config.depth_decoder_config)\n \n         self.num_codebooks = config.num_codebooks\n         self.post_init()"
        },
        {
            "sha": "c26e46ba5a143d6e883649e22c97120aee42514a",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 67,
            "deletions": 206,
            "changes": 273,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"PyTorch OPT model.\"\"\"\n \n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -27,18 +27,21 @@\n from ...modeling_attn_mask_utils import (\n     AttentionMaskConverter,\n )\n-from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs, is_flash_attn_available\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n     QuestionAnsweringModelOutput,\n     SequenceClassifierOutputWithPast,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n+    LossKwargs,\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n@@ -53,7 +56,7 @@\n \n \n if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n+    pass\n \n \n logger = logging.get_logger(__name__)\n@@ -98,6 +101,30 @@ def forward(\n         return super().forward(position_ids + self.offset)\n \n \n+# Copied from transformers.models.siglip.modeling_siglip.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class OPTAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -143,9 +170,8 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n-        # isn't needed in normal attention, but needed in flash attention so to keep the signature same\n-        position_ids: Optional[torch.Tensor] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Cache]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n         bsz, tgt_len, _ = hidden_states.size()\n@@ -165,214 +191,43 @@ def forward(\n                 key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n             )\n \n-        attn_weights = torch.matmul(query_states, key_states.transpose(3, 2))\n-        if attention_mask is not None:\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n-\n-        # upcast to fp32 if the weights are in fp16. Please see https://github.com/huggingface/transformers/pull/17437\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n+        attention_interface: Callable = eager_attention_forward\n \n-        if layer_head_mask is not None:\n-            if layer_head_mask.size() != (self.num_heads,):\n-                raise ValueError(\n-                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n-                    f\" {layer_head_mask.size()}\"\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n                 )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights\n-\n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-        attn_output = torch.matmul(attn_probs, value_states)\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned aross GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, attn_probs, past_key_value\n-\n-\n-class OptFlashAttention2(OPTAttention):\n-    \"\"\"\n-    OPT flash attention module. This module inherits from `OPTAttention` as the weights of the module stays untouched.\n-    The only required change would be on the forward pass where it needs to correctly call the public API of flash\n-    attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-        position_ids: Optional[torch.Tensor] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-\n-        bsz, query_length, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        query_states = query_states.view(bsz, -1, self.num_heads, self.head_dim)\n-\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-        key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-\n-        if past_key_value is not None:\n-            # save all key/value_states to cache to be re-used for fast auto-regressive generation\n-            key_states, value_states = past_key_value.update(\n-                key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n-            )\n-\n-        attn_dropout = self.dropout if self.training else 0.0\n-\n-        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n-        # to be able to avoid many of these transpose/reshape/view.\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in float16 just to be sure everything works as expected.\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n             else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output = _flash_attention_forward(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n             attention_mask,\n-            query_length,\n-            position_ids=position_ids,\n-            dropout=attn_dropout,\n-            is_causal=self.is_causal,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-        )\n-\n-        attn_weights_reshaped = attn_output.reshape(bsz, query_length, self.num_heads * self.head_dim)\n-        attn_output = self.out_proj(attn_weights_reshaped)\n-\n-        if not output_attentions:\n-            attn_weights_reshaped = None\n-\n-        return attn_output, attn_weights_reshaped, past_key_value\n-\n-\n-class OPTSdpaAttention(OPTAttention):\n-    \"\"\"\n-    OPT sdpa attention module. This module inherits from `OPTAttention` as the weights of the module stays untouched.\n-    The only required change would be on the forward pass where it needs to correctly call the public API of sdpa\n-    attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-        position_ids: Optional[torch.Tensor] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if output_attentions or layer_head_mask is not None:\n-            logger.warning_once(\n-                \"OPTModel is using SDPA attention, which currently does not support output_attentions=True.\"\n-                'failing back to eager attention. remove warning using attn_implementation=\"eager\".'\n-            )\n-\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                layer_head_mask=layer_head_mask,\n-                past_key_value=past_key_value,\n-                output_attentions=output_attentions,\n-                cache_position=cache_position,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        query_states = query_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-        key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n-\n-        if past_key_value is not None:\n-            # save all key/value_states to cache to be re-used for fast auto-regressive generation\n-            key_states, value_states = past_key_value.update(\n-                key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n-            )\n-\n-        causal_mask = attention_mask\n-        if attention_mask is not None:\n-            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n-\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.dropout if self.training else 0.0,\n-            is_causal=is_causal,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n         )\n \n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, -1)\n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, None, past_key_value\n-\n+        if not output_attentions:\n+            attn_weights = None\n \n-OPT_ATTENTION_CLASSES = {\n-    \"eager\": OPTAttention,\n-    \"flash_attention_2\": OptFlashAttention2,\n-    \"sdpa\": OPTSdpaAttention,\n-}\n+        return attn_output, attn_weights, past_key_value\n \n \n class OPTDecoderLayer(nn.Module):\n     def __init__(self, config: OPTConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n \n-        self.self_attn = OPT_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n+        self.self_attn = OPTAttention(config=config, layer_idx=layer_idx)\n \n         self.do_layer_norm_before = config.do_layer_norm_before\n         self.dropout = config.dropout\n@@ -395,6 +250,7 @@ def forward(\n         use_cache: Optional[bool] = False,\n         position_ids: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n@@ -429,6 +285,7 @@ def forward(\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n         hidden_states = residual + hidden_states\n@@ -495,8 +352,10 @@ class OPTPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"OPTDecoderLayer\"]\n+    _supports_attention_backend = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n@@ -763,6 +622,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n \n         return causal_mask\n \n+    @can_return_tuple\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -776,6 +636,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -942,6 +803,7 @@ def forward(\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n+                    **kwargs,\n                 )\n \n             hidden_states = layer_outputs[0]\n@@ -966,8 +828,6 @@ def forward(\n         if return_legacy_cache:\n             next_cache = next_cache.to_legacy_cache()\n \n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=next_cache,\n@@ -996,6 +856,7 @@ def set_input_embeddings(self, value):\n     def get_decoder(self):\n         return self.decoder\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(OPT_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_CHECKPOINT_FOR_DOC,\n@@ -1016,6 +877,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1035,13 +897,11 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n-        if not return_dict:\n-            return decoder_outputs\n-\n         return BaseModelOutputWithPast(\n             last_hidden_state=decoder_outputs.last_hidden_state,\n             past_key_values=decoder_outputs.past_key_values,\n@@ -1050,6 +910,9 @@ def forward(\n         )\n \n \n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n class OPTForCausalLM(OPTPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n@@ -1081,6 +944,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model.decoder\n \n+    @can_return_tuple\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n@@ -1096,7 +960,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.Tensor] = None,\n-        **kwargs,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1198,8 +1062,9 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         logits = self.lm_head(outputs[0]).contiguous()\n@@ -1215,10 +1080,6 @@ def forward(\n                 **kwargs,\n             )\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "d8096d8113faafb58a8b532e1396b18f00c58d01",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 16,
            "deletions": 7,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -23,9 +23,12 @@\n \n from ...cache_utils import Cache, HybridCache, StaticCache\n from ...generation import GenerationMixin\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast\n from ...modeling_utils import PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n+    LossKwargs,\n     ModelOutput,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n@@ -159,6 +162,7 @@ class PaliGemmaPreTrainedModel(PreTrainedModel):\n     _supports_static_cache = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         # important: this ported version of PaliGemmaisn't meant for training from scratch - only\n@@ -352,6 +356,7 @@ def get_image_features(self, pixel_values: torch.FloatTensor):\n         image_features = image_features / (self.config.text_config.hidden_size**0.5)\n         return image_features\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(PALIGEMMA_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -368,7 +373,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-        **lm_kwargs,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, PaligemmaModelOutputWithPast]:\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -436,17 +441,19 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=True,\n             cache_position=cache_position,\n-            **lm_kwargs,\n+            **kwargs,\n         )\n \n-        output = PaligemmaModelOutputWithPast(\n+        return PaligemmaModelOutputWithPast(\n             last_hidden_state=outputs.last_hidden_state,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n             image_hidden_states=image_features if pixel_values is not None else None,\n         )\n-        return output if return_dict else output.to_tuple()\n+\n+\n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n \n \n @add_start_docstrings(\n@@ -512,7 +519,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        **lm_kwargs,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, PaliGemmaCausalLMOutputWithPast]:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -570,7 +577,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=True,\n             cache_position=cache_position,\n-            **lm_kwargs,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -580,7 +587,9 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+            )\n \n         return PaliGemmaCausalLMOutputWithPast(\n             loss=loss,"
        },
        {
            "sha": "171bf63986ed9923a8d480f303a02e93f3dbd49a",
            "filename": "src/transformers/models/pixtral/modeling_pixtral.py",
            "status": "modified",
            "additions": 24,
            "deletions": 23,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -21,14 +21,18 @@\n import torch.utils.checkpoint\n from torch import nn\n \n-from ... import PreTrainedModel\n from ...activations import ACT2FN\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutput\n from ...modeling_rope_utils import dynamic_rope_update\n-from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging\n+from ...utils import (\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n+    logging,\n+)\n from .configuration_pixtral import PixtralVisionConfig\n \n \n@@ -132,7 +136,7 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     return q_embed, k_embed\n \n \n-# Copied from transformers.models.smolvlm.modeling_smolvlm.eager_attention_forward\n+# Copied from transformers.models.siglip.modeling_siglip.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n@@ -167,10 +171,11 @@ def __init__(self, config):\n         self.embed_dim = config.hidden_size\n         self.num_heads = config.num_attention_heads\n         self.head_dim = self.embed_dim // self.num_heads\n+        self.is_causal = False\n \n+        self.scaling = self.head_dim**-0.5\n         self.is_causal = False\n \n-        self.scale = self.head_dim**-0.5\n         self.dropout = config.attention_dropout\n \n         self.k_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n@@ -211,28 +216,22 @@ def forward(\n             else:\n                 attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        # Since we use packing, if Flash-Attn 2 is selected we rely on position_ids\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            kwargs[\"position_ids\"] = kwargs[\"position_ids\"].to(hidden_states.device, non_blocking=True)\n-            attention_mask = None\n-\n         attn_output, attn_weights = attention_interface(\n             self,\n             query_states,\n             key_states,\n             value_states,\n             attention_mask,\n-            scaling=self.scale,\n             dropout=0.0 if not self.training else self.dropout,\n-            is_causal=self.is_causal,\n-            output_attentions=output_attentions,\n+            scaling=self.scaling,\n             **kwargs,\n         )\n \n-        attn_output = attn_output.reshape(batch_size, patches, -1)\n-\n+        attn_output = attn_output.reshape(batch_size, patches, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n \n+        if not output_attentions:\n+            attn_weights = None\n         return attn_output, attn_weights\n \n \n@@ -288,7 +287,7 @@ def forward(\n         attention_mask: torch.Tensor,\n         position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n         output_attentions: Optional[bool] = None,\n-        **kwargs,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor]:\n         \"\"\"\n         Args:\n@@ -341,7 +340,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-        **kwargs,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutput]:\n         r\"\"\"\n         Args:\n@@ -383,7 +382,6 @@ def forward(\n                     attention_mask,\n                     position_embeddings,\n                     output_attentions,\n-                    **kwargs,\n                 )\n             else:\n                 layer_outputs = encoder_layer(\n@@ -431,6 +429,10 @@ class PixtralPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"model\"\n     main_input_name = \"pixel_values\"\n     supports_gradient_checkpointing = True\n+    _supports_attention_backend = True\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n     _no_split_modules = [\"PixtralAttentionLayer\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n@@ -508,6 +510,7 @@ def __init__(self, config):\n     def get_input_embeddings(self):\n         return self.patch_conv\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(PIXTRAL_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -517,7 +520,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         *args,\n-        **kwargs,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutput]:\n         \"\"\"\n         Returns:\n@@ -551,17 +554,15 @@ def forward(\n             [p.shape[-2] * p.shape[-1] for p in patch_embeds_list], patch_embeds\n         )\n \n-        out = self.transformer(\n+        return self.transformer(\n             patch_embeds,\n             attention_mask=attention_mask,\n             position_embeddings=position_embeddings,\n             output_hidden_states=output_hidden_states,\n             output_attentions=output_attentions,\n-            return_dict=return_dict,\n+            return_dict=True,\n             **kwargs,\n         )\n \n-        return out\n-\n \n __all__ = [\"PixtralVisionModel\", \"PixtralPreTrainedModel\"]"
        },
        {
            "sha": "e5abf36a3e498e5d86a047652bc36cb60a048e7a",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 93,
            "deletions": 112,
            "changes": 205,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -24,17 +24,20 @@\n \n import torch\n from torch import nn\n-from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...cache_utils import DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutput, ModelOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n+    LossKwargs,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     logging,\n     replace_return_docstrings,\n )\n@@ -78,6 +81,7 @@ class SmolVLMPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n@@ -574,80 +578,6 @@ def forward(self, image_hidden_states):\n         return image_hidden_states\n \n \n-SMOLVLM_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n-            `past_key_values`).\n-\n-            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n-            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n-            information on the default strategy.\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n-            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):\n-            The tensors corresponding to the input images. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details ([]`LlavaProcessor`] uses\n-            [`CLIPImageProcessor`] for processing images).\n-        pixel_attention_mask (`torch.Tensor` of shape `(batch_size, image_size, image_size)`, *optional*):\n-            Mask to avoid performing attention on padding pixel indices.\n-        image_hidden_states (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n-            The hidden states of the image encoder after modality projection.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n-            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n-            the complete sequence length.\n-\"\"\"\n-\n-\n @add_start_docstrings(\n     \"\"\"SmolVLM model consisting of a SIGLIP vision encoder and Llama3 language decoder\"\"\",\n     SMOLVLM_START_DOCSTRING,\n@@ -746,18 +676,7 @@ def inputs_merger(\n         merged_embeds = torch.where(image_mask.unsqueeze(-1), image_embeds, inputs_embeds)\n         return merged_embeds\n \n-    @add_start_docstrings_to_model_forward(\n-        \"\"\"\n-        Inputs fed to the model can have an arbitrary number of images. To account for this, pixel_values fed to\n-        the model have image padding -> (batch_size, max_num_images, 3, max_heights, max_widths) where\n-        max_num_images is the maximum number of images among the batch_size samples in the batch.\n-        Padding images are not needed beyond padding the pixel_values at the entrance of the model.\n-        For efficiency, we only pass through the vision_model's forward the real images by\n-        discarding the padding images i.e. pixel_values of size (image_batch_size, 3, height, width) where\n-        image_batch_size would be 7 when num_images_per_sample=[1, 3, 1, 2] and max_num_images would be 3.\n-        \"\"\",\n-        SMOLVLM_INPUTS_DOCSTRING,\n-    )\n+    @can_return_tuple\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -773,6 +692,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, SmolVLMBaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -873,13 +793,11 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n-        if not return_dict:\n-            return tuple(v for v in [*outputs, image_hidden_states] if v is not None)\n-\n         return SmolVLMBaseModelOutputWithPast(\n             last_hidden_state=outputs.last_hidden_state,\n             past_key_values=outputs.past_key_values,\n@@ -927,6 +845,83 @@ class SmolVLMCausalLMOutputWithPast(ModelOutput):\n     image_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n \n \n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n+SMOLVLM_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n+            `past_key_values`).\n+\n+            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n+            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n+            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n+            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n+            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):\n+            The tensors corresponding to the input images. Pixel values can be obtained using\n+            [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details ([]`LlavaProcessor`] uses\n+            [`CLIPImageProcessor`] for processing images).\n+        pixel_attention_mask (`torch.Tensor` of shape `(batch_size, image_size, image_size)`, *optional*):\n+            Mask to avoid performing attention on padding pixel indices.\n+        image_hidden_states (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+            The hidden states of the image encoder after modality projection.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n+\"\"\"\n+\n+\n @add_start_docstrings(\n     \"\"\"The SmolVLM Model with a language modeling head. It is made up a SigLIP vision encoder, with a language modeling head on top. \"\"\",\n     SMOLVLM_START_DOCSTRING,\n@@ -979,6 +974,7 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(SMOLVLM_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=SmolVLMCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -998,6 +994,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         return_dict: Optional[bool] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, SmolVLMCausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1066,7 +1063,8 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             cache_position=cache_position,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -1076,26 +1074,9 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Upcast to float if we need to compute the loss to avoid potential precision issues\n-            logits = logits.float()\n-            labels = labels.to(logits.device)\n-            # Shift so that tokens < n predict n\n-            if attention_mask is not None:\n-                # we use the input attention mask to shift the logits and labels, because it is 2D.\n-                # we also crop attn mask in case it is longer, which happens in PrefixTuning with peft\n-                shift_attention_mask = attention_mask[:, -(logits.shape[1] - 1) :].to(logits.device)\n-                shift_logits = logits[..., :-1, :][shift_attention_mask != 0].contiguous()\n-                shift_labels = labels[..., 1:][shift_attention_mask != 0].contiguous()\n-            else:\n-                shift_logits = logits[..., :-1, :].contiguous()\n-                shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n-\n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+            )\n \n         return SmolVLMCausalLMOutputWithPast(\n             loss=loss,"
        },
        {
            "sha": "66f77752610c099f859e7b410d3da72f8caa7848",
            "filename": "src/transformers/models/smolvlm/modular_smolvlm.py",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -20,7 +20,10 @@\n from torch import nn\n \n from ...cache_utils import DynamicCache\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...processing_utils import Unpack\n from ...utils import (\n+    can_return_tuple,\n     logging,\n )\n from ..idefics3.configuration_idefics3 import Idefics3Config, Idefics3VisionConfig\n@@ -195,6 +198,7 @@ def inputs_merger(\n         merged_embeds = torch.where(image_mask.unsqueeze(-1), image_embeds, inputs_embeds)\n         return merged_embeds\n \n+    @can_return_tuple\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -210,6 +214,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, SmolVLMBaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -310,13 +315,11 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n-        if not return_dict:\n-            return tuple(v for v in [*outputs, image_hidden_states] if v is not None)\n-\n         return SmolVLMBaseModelOutputWithPast(\n             last_hidden_state=outputs.last_hidden_state,\n             past_key_values=outputs.past_key_values,"
        },
        {
            "sha": "fc970dadc0e9f23815e7b2e3b33bdacd65c4898e",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 16,
            "deletions": 7,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -23,9 +23,12 @@\n \n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import ModelOutput\n from ...modeling_utils import PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n+    LossKwargs,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     can_return_tuple,\n@@ -181,6 +184,7 @@ class VideoLlavaPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = (\n@@ -387,6 +391,7 @@ def get_video_features(\n \n         return video_features, num_frames\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(VIDEO_LLAVA_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -404,7 +409,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **lm_kwargs,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, VideoLlavaModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -475,18 +480,20 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=True,\n             cache_position=cache_position,\n-            **lm_kwargs,\n+            **kwargs,\n         )\n \n-        output = VideoLlavaModelOutputWithPast(\n+        return VideoLlavaModelOutputWithPast(\n             last_hidden_state=outputs.last_hidden_state,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n             image_hidden_states=image_features if pixel_values_images is not None else None,\n             video_hidden_states=video_features if pixel_values_videos is not None else None,\n         )\n-        return output if return_dict else output.to_tuple()\n+\n+\n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n \n \n @add_start_docstrings(\n@@ -559,7 +566,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        **lm_kwargs,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, VideoLlavaCausalLMOutputWithPast]:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -671,7 +678,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=True,\n             cache_position=cache_position,\n-            **lm_kwargs,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -681,7 +688,9 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+            )\n \n         return VideoLlavaCausalLMOutputWithPast(\n             loss=loss,"
        },
        {
            "sha": "c1dd0f9941c7821185b9b6c34eca6afb9a626c5c",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -171,6 +171,7 @@ class VipLlavaPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         # important: this ported version of VipLlava isn't meant for training from scratch - only"
        },
        {
            "sha": "89579ae833a698a53ef059a88757d3323befc0a1",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 19,
            "deletions": 38,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -461,6 +461,7 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class Blip2ForConditionalGenerationDecoderOnlyTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (Blip2ForConditionalGeneration,) if is_torch_available() else ()\n+    additional_model_inputs = [\"input_ids\"]\n     fx_compatible = False\n     test_head_masking = False\n     test_pruning = False\n@@ -526,15 +527,11 @@ def test_sdpa_can_dispatch_composite_models(self):\n                 model_sdpa = model_class.from_pretrained(tmpdirname)\n                 model_sdpa = model_sdpa.eval().to(torch_device)\n \n-                text_attn = \"sdpa\" if model.language_model._supports_sdpa else \"eager\"\n-                vision_attn = \"sdpa\" if model.vision_model._supports_sdpa else \"eager\"\n-                qformer_attn = \"sdpa\" if model.qformer._supports_sdpa else \"eager\"\n-\n                 # `None` as it is the requested one which will be assigned to each sub-config\n                 # Sub-model will dispatch to SDPA if it can (checked below that `SDPA` layers are present)\n-                self.assertTrue(model.language_model.config._attn_implementation == text_attn)\n-                self.assertTrue(model.vision_model.config._attn_implementation == vision_attn)\n-                self.assertTrue(model.qformer.config._attn_implementation == qformer_attn)\n+                self.assertTrue(model.language_model.config._attn_implementation == \"sdpa\")\n+                self.assertTrue(model.vision_model.config._attn_implementation == \"sdpa\")\n+                self.assertTrue(model.qformer.config._attn_implementation == \"eager\")\n \n                 model_eager = model_class.from_pretrained(tmpdirname, attn_implementation=\"eager\")\n                 model_eager = model_eager.eval().to(torch_device)\n@@ -545,20 +542,13 @@ def test_sdpa_can_dispatch_composite_models(self):\n \n                 for name, submodule in model_eager.named_modules():\n                     class_name = submodule.__class__.__name__\n-                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                    if (\n+                        class_name.endswith(\"Attention\")\n+                        and getattr(submodule, \"config\", None)\n+                        and submodule.config._attn_implementation == \"sdpa\"\n+                    ):\n                         raise ValueError(\"The eager model should not have SDPA attention layers\")\n \n-                has_sdpa = False\n-                for name, submodule in model_sdpa.named_modules():\n-                    class_name = submodule.__class__.__name__\n-                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n-                        has_sdpa = True\n-                        break\n-                if not has_sdpa and any(\n-                    module_attn == \"sdpa\" for module_attn in [text_attn, vision_attn, qformer_attn]\n-                ):\n-                    raise ValueError(\"The SDPA model should have SDPA attention layers\")\n-\n     def test_forward_signature(self):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -869,6 +859,7 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class Blip2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (Blip2ForConditionalGeneration, Blip2Model) if is_torch_available() else ()\n+    additional_model_inputs = [\"input_ids\", \"decoder_input_ids\"]\n     # Doesn't run generation tests. TODO: fix generation tests for Blip2ForConditionalGeneration\n     all_generative_model_classes = ()\n     pipeline_model_mapping = (\n@@ -967,15 +958,11 @@ def test_sdpa_can_dispatch_composite_models(self):\n                 model_sdpa = model_class.from_pretrained(tmpdirname)\n                 model_sdpa = model_sdpa.eval().to(torch_device)\n \n-                text_attn = \"sdpa\" if model.language_model._supports_sdpa else \"eager\"\n-                vision_attn = \"sdpa\" if model.vision_model._supports_sdpa else \"eager\"\n-                qformer_attn = \"sdpa\" if model.qformer._supports_sdpa else \"eager\"\n-\n                 # `None` as it is the requested one which will be assigned to each sub-config\n                 # Sub-model will dispatch to SDPA if it can (checked below that `SDPA` layers are present)\n-                self.assertTrue(model.language_model.config._attn_implementation == text_attn)\n-                self.assertTrue(model.vision_model.config._attn_implementation == vision_attn)\n-                self.assertTrue(model.qformer.config._attn_implementation == qformer_attn)\n+                self.assertTrue(model.language_model.config._attn_implementation == \"eager\")\n+                self.assertTrue(model.vision_model.config._attn_implementation == \"sdpa\")\n+                self.assertTrue(model.qformer.config._attn_implementation == \"eager\")\n \n                 model_eager = model_class.from_pretrained(tmpdirname, attn_implementation=\"eager\")\n                 model_eager = model_eager.eval().to(torch_device)\n@@ -986,20 +973,13 @@ def test_sdpa_can_dispatch_composite_models(self):\n \n                 for name, submodule in model_eager.named_modules():\n                     class_name = submodule.__class__.__name__\n-                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                    if (\n+                        class_name.endswith(\"Attention\")\n+                        and getattr(submodule, \"config\", None)\n+                        and submodule.config._attn_implementation == \"sdpa\"\n+                    ):\n                         raise ValueError(\"The eager model should not have SDPA attention layers\")\n \n-                has_sdpa = False\n-                for name, submodule in model_sdpa.named_modules():\n-                    class_name = submodule.__class__.__name__\n-                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n-                        has_sdpa = True\n-                        break\n-                if not has_sdpa and any(\n-                    module_attn == \"sdpa\" for module_attn in [text_attn, vision_attn, qformer_attn]\n-                ):\n-                    raise ValueError(\"The SDPA model should have SDPA attention layers\")\n-\n     def test_forward_signature(self):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -1485,6 +1465,7 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class Blip2TextRetrievalModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (Blip2ForImageTextRetrieval,) if is_torch_available() else ()\n+    additional_model_inputs = [\"input_ids\"]\n     fx_compatible = False\n     test_head_masking = False\n     test_pruning = False"
        },
        {
            "sha": "923a8749c61083594e62bd9e0ffe4c0e12e06e2b",
            "filename": "tests/models/instructblip/test_modeling_instructblip.py",
            "status": "modified",
            "additions": 9,
            "deletions": 19,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -475,6 +475,7 @@ class InstructBlipForConditionalGenerationDecoderOnlyTest(ModelTesterMixin, Gene\n         else ()\n     )\n     pipeline_model_mapping = {\"image-text-to-text\": InstructBlipForConditionalGeneration}\n+    additional_model_inputs = [\"qformer_input_ids\", \"input_ids\"]\n     fx_compatible = False\n     test_head_masking = False\n     test_pruning = False\n@@ -687,15 +688,11 @@ def test_sdpa_can_dispatch_composite_models(self):\n                 model_sdpa = model_class.from_pretrained(tmpdirname)\n                 model_sdpa = model_sdpa.eval().to(torch_device)\n \n-                text_attn = \"sdpa\" if model.language_model._supports_sdpa else \"eager\"\n-                vision_attn = \"sdpa\" if model.vision_model._supports_sdpa else \"eager\"\n-                qformer_attn = \"sdpa\" if model.qformer._supports_sdpa else \"eager\"\n-\n                 # `None` as it is the requested one which will be assigned to each sub-config\n                 # Sub-model will dispatch to SDPA if it can (checked below that `SDPA` layers are present)\n-                self.assertTrue(model.language_model.config._attn_implementation == text_attn)\n-                self.assertTrue(model.vision_model.config._attn_implementation == vision_attn)\n-                self.assertTrue(model.qformer.config._attn_implementation == qformer_attn)\n+                self.assertTrue(model.language_model.config._attn_implementation == \"sdpa\")\n+                self.assertTrue(model.vision_model.config._attn_implementation == \"sdpa\")\n+                self.assertTrue(model.qformer.config._attn_implementation == \"eager\")\n \n                 model_eager = model_class.from_pretrained(tmpdirname, attn_implementation=\"eager\")\n                 model_eager = model_eager.eval().to(torch_device)\n@@ -706,20 +703,13 @@ def test_sdpa_can_dispatch_composite_models(self):\n \n                 for name, submodule in model_eager.named_modules():\n                     class_name = submodule.__class__.__name__\n-                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                    if (\n+                        class_name.endswith(\"Attention\")\n+                        and getattr(submodule, \"config\", None)\n+                        and submodule.config._attn_implementation == \"sdpa\"\n+                    ):\n                         raise ValueError(\"The eager model should not have SDPA attention layers\")\n \n-                has_sdpa = False\n-                for name, submodule in model_sdpa.named_modules():\n-                    class_name = submodule.__class__.__name__\n-                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n-                        has_sdpa = True\n-                        break\n-                if not has_sdpa and any(\n-                    module_attn == \"sdpa\" for module_attn in [text_attn, vision_attn, qformer_attn]\n-                ):\n-                    raise ValueError(\"The SDPA model should have SDPA attention layers\")\n-\n \n # We will verify our results on an image of cute cats\n def prepare_img():"
        },
        {
            "sha": "a7870a4b29cd33cdaf90c53c44627e8847da86c3",
            "filename": "tests/models/instructblipvideo/test_modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 9,
            "deletions": 19,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -492,6 +492,7 @@ class InstructBlipVideoForConditionalGenerationDecoderOnlyTest(\n     all_model_classes = (\n         (InstructBlipVideoForConditionalGeneration, InstructBlipVideoModel) if is_torch_available() else ()\n     )\n+    additional_model_inputs = [\"qformer_input_ids\", \"input_ids\"]\n     fx_compatible = False\n     test_head_masking = False\n     test_pruning = False\n@@ -702,15 +703,11 @@ def test_sdpa_can_dispatch_composite_models(self):\n                 model_sdpa = model_class.from_pretrained(tmpdirname)\n                 model_sdpa = model_sdpa.eval().to(torch_device)\n \n-                text_attn = \"sdpa\" if model.language_model._supports_sdpa else \"eager\"\n-                vision_attn = \"sdpa\" if model.vision_model._supports_sdpa else \"eager\"\n-                qformer_attn = \"sdpa\" if model.qformer._supports_sdpa else \"eager\"\n-\n                 # `None` as it is the requested one which will be assigned to each sub-config\n                 # Sub-model will dispatch to SDPA if it can (checked below that `SDPA` layers are present)\n-                self.assertTrue(model.language_model.config._attn_implementation == text_attn)\n-                self.assertTrue(model.vision_model.config._attn_implementation == vision_attn)\n-                self.assertTrue(model.qformer.config._attn_implementation == qformer_attn)\n+                self.assertTrue(model.language_model.config._attn_implementation == \"sdpa\")\n+                self.assertTrue(model.vision_model.config._attn_implementation == \"sdpa\")\n+                self.assertTrue(model.qformer.config._attn_implementation == \"eager\")\n \n                 model_eager = model_class.from_pretrained(tmpdirname, attn_implementation=\"eager\")\n                 model_eager = model_eager.eval().to(torch_device)\n@@ -721,20 +718,13 @@ def test_sdpa_can_dispatch_composite_models(self):\n \n                 for name, submodule in model_eager.named_modules():\n                     class_name = submodule.__class__.__name__\n-                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                    if (\n+                        class_name.endswith(\"Attention\")\n+                        and getattr(submodule, \"config\", None)\n+                        and submodule.config._attn_implementation == \"sdpa\"\n+                    ):\n                         raise ValueError(\"The eager model should not have SDPA attention layers\")\n \n-                has_sdpa = False\n-                for name, submodule in model_sdpa.named_modules():\n-                    class_name = submodule.__class__.__name__\n-                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n-                        has_sdpa = True\n-                        break\n-                if not has_sdpa and any(\n-                    module_attn == \"sdpa\" for module_attn in [text_attn, vision_attn, qformer_attn]\n-                ):\n-                    raise ValueError(\"The SDPA model should have SDPA attention layers\")\n-\n \n # We will verify our results on an image of cute cats\n def prepare_video():"
        },
        {
            "sha": "1e61d536d75c58eb2f02d63bb25dadaa14d8f965",
            "filename": "tests/models/kosmos2/test_modeling_kosmos2.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -30,6 +30,7 @@\n     IS_ROCM_SYSTEM,\n     IS_XPU_SYSTEM,\n     require_torch,\n+    require_torch_sdpa,\n     require_vision,\n     slow,\n     torch_device,\n@@ -42,6 +43,7 @@\n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import (\n+    TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION,\n     ModelTesterMixin,\n     _config_zero_init,\n     floats_tensor,\n@@ -259,6 +261,7 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class Kosmos2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (Kosmos2Model, Kosmos2ForConditionalGeneration) if is_torch_available() else ()\n+    additional_model_inputs = [\"input_ids\", \"image_embeds_position_mask\"]\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": Kosmos2Model,\n@@ -462,6 +465,14 @@ def check_same_values(layer_1, layer_2):\n     def test_generate_from_inputs_embeds(self):\n         pass\n \n+    @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n+    @require_torch_sdpa\n+    @unittest.skip(\"KOSMOS-2 doesn't support padding\")\n+    def test_eager_matches_sdpa_inference(\n+        self, name, torch_dtype, padding_side, use_attention_mask, output_attentions, enable_kernels\n+    ):\n+        pass\n+\n     @pytest.mark.generate\n     def test_left_padding_compatibility(self):\n         # Overwrite because Kosmos-2 need to padd pixel values and pad image-attn-mask"
        },
        {
            "sha": "5916f42f5f6ac91674448f4c9b2318f57431c0b8",
            "filename": "tests/models/opt/test_modeling_opt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -219,9 +219,10 @@ class OPTModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n         else {}\n     )\n     is_encoder_decoder = False\n-    fx_compatible = True\n+    fx_compatible = False  # Broken by attention refactor cc @Cyrilvallez\n     test_pruning = False\n     test_missing_keys = False\n+    test_head_masking = False  # new attn API doesn't support head mask\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip("
        },
        {
            "sha": "1a7b2ad01d3288d2b0c009bfc2ffc1ea1bd0bc70",
            "filename": "tests/models/pixtral/test_modeling_pixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/tests%2Fmodels%2Fpixtral%2Ftest_modeling_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/tests%2Fmodels%2Fpixtral%2Ftest_modeling_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpixtral%2Ftest_modeling_pixtral.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -109,6 +109,7 @@ class PixtralVisionModelModelTest(ModelTesterMixin, unittest.TestCase):\n     \"\"\"\n \n     all_model_classes = (PixtralVisionModel,) if is_torch_available() else ()\n+    additional_model_inputs = [\"image_sizes\"]\n     test_pruning = False\n     test_head_masking = False\n     test_torchscript = False"
        },
        {
            "sha": "ff86e157fdb7eee32e9c17eb031d462860ddd318",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d23aae2b8c8738a12ab1b6710e60ae5866beaf9d/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=d23aae2b8c8738a12ab1b6710e60ae5866beaf9d",
            "patch": "@@ -3765,6 +3765,10 @@ def test_eager_matches_sdpa_inference(\n                     key = \"decoder_hidden_states\"\n                 elif \"logits\" in outputs_eager and \"Classification\" in model_class.__name__:\n                     key = \"logits\"\n+                elif \"language_model_outputs\" in outputs_eager and \"blip\" in model_class.__name__.lower():\n+                    outputs_eager = outputs_eager[\"language_model_outputs\"]\n+                    outputs_sdpa = outputs_sdpa[\"language_model_outputs\"]\n+                    key = \"hidden_states\" if \"hidden_states\" in outputs_eager else \"decoder_hidden_states\"\n                 else:\n                     key = \"hidden_states\"\n \n@@ -4002,14 +4006,14 @@ def test_flash_attn_2_can_dispatch_composite_models(self):\n                 model = model_class.from_pretrained(tmpdirname, torch_dtype=torch_dtype)\n \n                 sub_models_supporting_fa2 = [\n-                    module._supports_flash_attn_2\n+                    (module._supports_flash_attn_2 or module._supports_attention_backend)\n                     for name, module in model.named_modules()\n                     if isinstance(module, PreTrainedModel) and name != \"\"\n                 ]\n                 supports_fa2_all_modules = (\n                     all(sub_models_supporting_fa2)\n                     if len(sub_models_supporting_fa2) > 0\n-                    else model._supports_flash_attn_2\n+                    else (model._supports_flash_attn_2 or model._supports_attention_backend)\n                 )\n                 if not supports_fa2_all_modules:\n                     with self.assertRaises(ValueError):"
        }
    ],
    "stats": {
        "total": 2763,
        "additions": 1263,
        "deletions": 1500
    }
}