{
    "author": "manueldeprada",
    "message": "Create test for #38916 (custom generate from local dir with imports) (#39015)\n\n* create test for #38916 (custom generate from local dir with imports)",
    "sha": "3abeaba7e53512ef9c1314163dd7e462ab405ce6",
    "files": [
        {
            "sha": "2525b020c49276f4c3fbc8e73ece97aa3c400e97",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 22,
            "deletions": 0,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/3abeaba7e53512ef9c1314163dd7e462ab405ce6/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3abeaba7e53512ef9c1314163dd7e462ab405ce6/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=3abeaba7e53512ef9c1314163dd7e462ab405ce6",
            "patch": "@@ -22,6 +22,7 @@\n import tempfile\n import unittest\n import warnings\n+from pathlib import Path\n \n import numpy as np\n import pytest\n@@ -4995,6 +4996,27 @@ def test_custom_generate_requires_trust_remote_code(self):\n         with self.assertRaises(ValueError):\n             model.generate(**model_inputs, custom_generate=\"transformers-community/custom_generate_example\")\n \n+    def test_custom_generate_local_directory(self):\n+        \"\"\"Tests that custom_generate works with local directories containing importable relative modules\"\"\"\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            custom_generate_dir = Path(tmp_dir) / \"custom_generate\"\n+            custom_generate_dir.mkdir()\n+            with open(custom_generate_dir / \"generate.py\", \"w\") as f:\n+                f.write(\"from .helper import ret_success\\ndef generate(*args, **kwargs):\\n    return ret_success()\\n\")\n+            with open(custom_generate_dir / \"helper.py\", \"w\") as f:\n+                f.write('def ret_success():\\n    return \"success\"\\n')\n+            model = AutoModelForCausalLM.from_pretrained(\n+                \"hf-internal-testing/tiny-random-MistralForCausalLM\", device_map=\"auto\"\n+            )\n+            tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-MistralForCausalLM\")\n+            model_inputs = tokenizer(\"Hello, world!\", return_tensors=\"pt\").to(model.device)\n+            value = model.generate(\n+                **model_inputs,\n+                custom_generate=str(tmp_dir),\n+                trust_remote_code=True,\n+            )\n+            assert value == \"success\"\n+\n \n @require_torch\n class TokenHealingTestCase(unittest.TestCase):"
        }
    ],
    "stats": {
        "total": 22,
        "additions": 22,
        "deletions": 0
    }
}