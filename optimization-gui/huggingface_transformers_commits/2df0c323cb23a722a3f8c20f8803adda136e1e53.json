{
    "author": "Rocketknight1",
    "message": "byebye torch 2.1 (#40317)\n\n* Bump minimum torch version to 2.2\n\n* Remove is_torch_greater_or_equal_than_2_2\n\n* update versions table\n\n* Deprecate is_torch_sdpa_available (except for backward compat), remove require_torch_sdpa",
    "sha": "2df0c323cb23a722a3f8c20f8803adda136e1e53",
    "files": [
        {
            "sha": "3b67610db313dc3c557d8c90b3840d3bfa0da545",
            "filename": "setup.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -190,7 +190,7 @@\n     \"tiktoken\",\n     \"timm<=1.0.19,!=1.0.18\",\n     \"tokenizers>=0.21,<0.22\",\n-    \"torch>=2.1\",\n+    \"torch>=2.2\",\n     \"torchaudio\",\n     \"torchvision\",\n     \"pyctcdecode>=0.4.0\","
        },
        {
            "sha": "809a646e55cf988a7f14f7385bbcaf0072517779",
            "filename": "src/transformers/dependency_versions_table.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/src%2Ftransformers%2Fdependency_versions_table.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/src%2Ftransformers%2Fdependency_versions_table.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdependency_versions_table.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -92,7 +92,7 @@\n     \"tiktoken\": \"tiktoken\",\n     \"timm\": \"timm<=1.0.19,!=1.0.18\",\n     \"tokenizers\": \"tokenizers>=0.21,<0.22\",\n-    \"torch\": \"torch>=2.1\",\n+    \"torch\": \"torch>=2.2\",\n     \"torchaudio\": \"torchaudio\",\n     \"torchvision\": \"torchvision\",\n     \"pyctcdecode\": \"pyctcdecode>=0.4.0\","
        },
        {
            "sha": "4cc129366baea19b78ab5e7335fa21c5a371326b",
            "filename": "src/transformers/models/albert/modeling_albert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -38,7 +38,6 @@\n from ...pytorch_utils import (\n     apply_chunking_to_forward,\n     find_pruneable_heads_and_indices,\n-    is_torch_greater_or_equal_than_2_2,\n     prune_linear_layer,\n )\n from ...utils import ModelOutput, auto_docstring, logging\n@@ -356,7 +355,6 @@ class AlbertSdpaAttention(AlbertAttention):\n     def __init__(self, config):\n         super().__init__(config)\n         self.dropout_prob = config.attention_probs_dropout_prob\n-        self.require_contiguous_qkv = not is_torch_greater_or_equal_than_2_2\n \n     def forward(\n         self,\n@@ -392,14 +390,6 @@ def forward(\n             .transpose(1, 2)\n         )\n \n-        # SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom\n-        # attn_mask, so we need to call `.contiguous()` here. This was fixed in torch==2.2.0.\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577\n-        if self.require_contiguous_qkv and query_layer.device.type == \"cuda\" and attention_mask is not None:\n-            query_layer = query_layer.contiguous()\n-            key_layer = key_layer.contiguous()\n-            value_layer = value_layer.contiguous()\n-\n         attention_output = torch.nn.functional.scaled_dot_product_attention(\n             query=query_layer,\n             key=key_layer,"
        },
        {
            "sha": "58f5aed348f0782fdeef29b0a5ed8447cfc73ad6",
            "filename": "src/transformers/models/distilbert/modeling_distilbert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -44,7 +44,6 @@\n from ...pytorch_utils import (\n     apply_chunking_to_forward,\n     find_pruneable_heads_and_indices,\n-    is_torch_greater_or_equal_than_2_2,\n     prune_linear_layer,\n )\n from ...utils import (\n@@ -338,7 +337,6 @@ class DistilBertSdpaAttention(MultiHeadSelfAttention):\n     def __init__(self, config: PretrainedConfig):\n         super().__init__(config=config)\n         self.dropout_prob = config.attention_dropout\n-        self.require_contiguous_qkv = not is_torch_greater_or_equal_than_2_2\n \n     def forward(\n         self,\n@@ -391,14 +389,6 @@ def unshape(x: torch.Tensor) -> torch.Tensor:\n         k = shape(self.k_lin(key))  # (bs, n_heads, k_length, dim_per_head)\n         v = shape(self.v_lin(value))  # (bs, n_heads, k_length, dim_per_head)\n \n-        # SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom\n-        # attn_mask, so we need to call `.contiguous()` here. This was fixed in torch==2.2.0.\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577\n-        if self.require_contiguous_qkv and q.device.type == \"cuda\" and mask is not None:\n-            q = q.contiguous()\n-            k = k.contiguous()\n-            v = v.contiguous()\n-\n         attn_output = torch.nn.functional.scaled_dot_product_attention(\n             q,\n             k,"
        },
        {
            "sha": "87136d079f104f28480a6fa8183dcff3640e63a3",
            "filename": "src/transformers/pytorch_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/src%2Ftransformers%2Fpytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/src%2Ftransformers%2Fpytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpytorch_utils.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -38,9 +38,9 @@\n is_torch_greater_or_equal_than_2_6 = is_torch_greater_or_equal(\"2.6\", accept_dev=True)\n is_torch_greater_or_equal_than_2_4 = is_torch_greater_or_equal(\"2.4\", accept_dev=True)\n is_torch_greater_or_equal_than_2_3 = is_torch_greater_or_equal(\"2.3\", accept_dev=True)\n-is_torch_greater_or_equal_than_2_2 = is_torch_greater_or_equal(\"2.2\", accept_dev=True)\n \n # For backwards compatibility (e.g. some remote codes on Hub using those variables).\n+is_torch_greater_or_equal_than_2_2 = is_torch_greater_or_equal(\"2.2\", accept_dev=True)\n is_torch_greater_or_equal_than_2_1 = is_torch_greater_or_equal(\"2.1\", accept_dev=True)\n is_torch_greater_or_equal_than_2_0 = is_torch_greater_or_equal(\"2.0\", accept_dev=True)\n is_torch_greater_or_equal_than_1_13 = is_torch_greater_or_equal(\"1.13\", accept_dev=True)"
        },
        {
            "sha": "fff760f458bef491eaa8c45077325090bd0512d3",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -159,7 +159,6 @@\n     is_torch_neuroncore_available,\n     is_torch_npu_available,\n     is_torch_optimi_available,\n-    is_torch_sdpa_available,\n     is_torch_tensorrt_fx_available,\n     is_torch_tf32_available,\n     is_torch_xla_available,\n@@ -624,15 +623,6 @@ def require_flash_attn_3(test_case):\n     return unittest.skipUnless(is_flash_attn_3_available(), \"test requires Flash Attention 3\")(test_case)\n \n \n-def require_torch_sdpa(test_case):\n-    \"\"\"\n-    Decorator marking a test that requires PyTorch's SDPA.\n-\n-    These tests are skipped when requirements are not met (torch version).\n-    \"\"\"\n-    return unittest.skipUnless(is_torch_sdpa_available(), \"test requires PyTorch SDPA\")(test_case)\n-\n-\n def require_read_token(test_case):\n     \"\"\"\n     A decorator that loads the HF token for tests that require to load gated models."
        },
        {
            "sha": "52dd78c3e517247882740fc245a64f1dd541a688",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -451,17 +451,10 @@ def get_torch_major_and_minor_version() -> str:\n \n \n def is_torch_sdpa_available():\n+    # Mostly retained for backward compatibility in remote code, since sdpa works correctly on all torch versions >= 2.2\n     if not is_torch_available() or _torch_version == \"N/A\":\n         return False\n-\n-    # NOTE: MLU is OK with non-contiguous inputs.\n-    if is_torch_mlu_available():\n-        return True\n-    # NOTE: NPU can use SDPA in Transformers with torch>=2.1.0.\n-    if is_torch_npu_available():\n-        return True\n-    # NOTE: We require torch>=2.1.1 to avoid a numerical issue in SDPA with non-contiguous inputs: https://github.com/pytorch/pytorch/issues/112577\n-    return version.parse(_torch_version) >= version.parse(\"2.1.1\")\n+    return True\n \n \n def is_torch_flex_attn_available():"
        },
        {
            "sha": "28216e660add13c43d684bca8fa9f5ac044e0ebe",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -51,7 +51,6 @@\n     require_torch_gpu,\n     require_torch_greater_or_equal,\n     require_torch_multi_accelerator,\n-    require_torch_sdpa,\n     set_config_for_less_flaky_test,\n     set_model_for_less_flaky_test,\n     set_model_tester_for_less_flaky_test,\n@@ -2366,7 +2365,6 @@ def _test_attention_implementation(self, attn_implementation):\n                 self.assertTrue(has_similar_generate_outputs(res_eager, res_attn, atol=1e-3, rtol=1e-3))\n \n     @pytest.mark.generate\n-    @require_torch_sdpa\n     @slow\n     def test_eager_matches_sdpa_generate(self):\n         \"\"\"Tests that generate has equivalent outputs with SDPA and eager attention implementations.\"\"\""
        },
        {
            "sha": "1e2932b1cc49cc41fbeb807fce2737ceeeb1025a",
            "filename": "tests/models/aimv2/test_modeling_aimv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Faimv2%2Ftest_modeling_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Faimv2%2Ftest_modeling_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faimv2%2Ftest_modeling_aimv2.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -28,7 +28,6 @@\n     require_flash_attn,\n     require_torch,\n     require_torch_gpu,\n-    require_torch_sdpa,\n     require_vision,\n     slow,\n     torch_device,\n@@ -563,7 +562,6 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n                 )\n \n     @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n-    @require_torch_sdpa\n     def test_eager_matches_sdpa_inference(\n         self,\n         name,"
        },
        {
            "sha": "4d76c487dfb452fb9fcc53b7e2b9e169e32c3ed2",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -31,7 +31,6 @@\n     require_torch_fp16,\n     require_torch_gpu,\n     require_torch_multi_accelerator,\n-    require_torch_sdpa,\n     require_vision,\n     slow,\n     torch_device,\n@@ -508,7 +507,6 @@ def test_retain_grad_hidden_states_attentions(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n-    @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         \"\"\"\n         Tests if composite models dispatch correctly on SDPA/eager when requested so when loading the model.\n@@ -945,7 +943,6 @@ def test_model_get_set_embeddings(self):\n     def test_cpu_offload(self):\n         pass\n \n-    @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         \"\"\"\n         Tests if composite models dispatch correctly on SDPA/eager when requested so when loading the model."
        },
        {
            "sha": "bdc0535dbfed1dd4e17654c761f3b3d8a292a232",
            "filename": "tests/models/camembert/test_modeling_camembert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fcamembert%2Ftest_modeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fcamembert%2Ftest_modeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcamembert%2Ftest_modeling_camembert.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -19,7 +19,6 @@\n     require_sentencepiece,\n     require_tokenizers,\n     require_torch,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -62,7 +61,6 @@ def test_output_embeds_base_model(self):\n         torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n-    @require_torch_sdpa\n     def test_output_embeds_base_model_sdpa(self):\n         input_ids = torch.tensor(\n             [[5, 121, 11, 660, 16, 730, 25543, 110, 83, 6]],"
        },
        {
            "sha": "b22d4a4b5ad4a44ea010edbfb817817daba297ef",
            "filename": "tests/models/clip/test_modeling_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -29,7 +29,6 @@\n     require_flash_attn,\n     require_torch,\n     require_torch_gpu,\n-    require_torch_sdpa,\n     require_vision,\n     slow,\n     torch_device,\n@@ -164,7 +163,6 @@ def prepare_config_and_inputs_for_common(self):\n         return config, inputs_dict\n \n     @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n-    @require_torch_sdpa\n     def test_eager_matches_sdpa_inference(self, *args):\n         return getattr(ModelTesterMixin, self._testMethodName)(self)\n \n@@ -176,7 +174,6 @@ class CLIPModelTesterMixin(ModelTesterMixin):\n     different output logits, and are not supposed to be used or tested with padding_side=\"left\".\n     \"\"\"\n \n-    @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         for model_class in self.all_model_classes:\n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n@@ -295,13 +292,11 @@ def test_model_with_projection_from_pretrained(self):\n         self.assertTrue(hasattr(model, \"visual_projection\"))\n \n     @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n-    @require_torch_sdpa\n     @is_flaky()\n     def test_eager_matches_sdpa_inference(self, *args):\n         # adding only flaky decorator here and call the parent test method\n         return getattr(ModelTesterMixin, self._testMethodName)(self)\n \n-    @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         super().test_sdpa_can_dispatch_composite_models()\n \n@@ -465,18 +460,15 @@ def test_model_with_projection_from_pretrained(self):\n         self.assertTrue(hasattr(model, \"text_projection\"))\n \n     @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n-    @require_torch_sdpa\n     @slow\n     @is_flaky()\n     def test_eager_matches_sdpa_inference(self, *args):\n         # adding only flaky decorator here and call the parent test method\n         return getattr(ModelTesterMixin, self._testMethodName)(self)\n \n-    @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         super().test_sdpa_can_dispatch_composite_models()\n \n-    @require_torch_sdpa\n     def test_sdpa_can_dispatch_on_flash(self):\n         self.skipTest(reason=\"CLIPTextModel has two attention masks: `causal_attention_mask` and `attention_mask`\")\n \n@@ -693,22 +685,18 @@ def test_model_from_pretrained(self):\n         self.assertIsNotNone(model)\n \n     @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n-    @require_torch_sdpa\n     @slow\n     @is_flaky()\n     def test_eager_matches_sdpa_inference(self, *args):\n         # adding only flaky decorator here and call the parent test method\n         return getattr(ModelTesterMixin, self._testMethodName)(self)\n \n-    @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         super().test_sdpa_can_dispatch_composite_models()\n \n-    @require_torch_sdpa\n     def test_sdpa_can_dispatch_on_flash(self):\n         self.skipTest(reason=\"CLIP text tower has two attention masks: `causal_attention_mask` and `attention_mask`\")\n \n-    @require_torch_sdpa\n     @pytest.mark.torch_compile_test\n     def test_sdpa_can_compile_dynamic(self):\n         self.skipTest(reason=\"CLIP model can't be compiled dynamic, error in clip_loss`\")\n@@ -864,14 +852,12 @@ def test_initialization(self):\n         pass\n \n     @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n-    @require_torch_sdpa\n     @slow\n     @is_flaky()\n     def test_eager_matches_sdpa_inference(self, *args):\n         # adding only flaky decorator here and call the parent test method\n         return getattr(ModelTesterMixin, self._testMethodName)(self)\n \n-    @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         super().test_sdpa_can_dispatch_composite_models()\n "
        },
        {
            "sha": "9f31d17bd26181e1c3534449b22fc113481aa73d",
            "filename": "tests/models/cohere/test_modeling_cohere.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -20,7 +20,6 @@\n     require_bitsandbytes,\n     require_torch,\n     require_torch_multi_accelerator,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -224,7 +223,6 @@ def test_batched_4bit(self):\n         output = model.generate(**inputs, max_new_tokens=40, do_sample=False)\n         self.assertEqual(tokenizer.batch_decode(output, skip_special_tokens=True), EXPECTED_TEXT)\n \n-    @require_torch_sdpa\n     def test_batched_small_model_logits(self):\n         # Since the model is very large, we created a random cohere model so that we can do a simple\n         # logits check on it."
        },
        {
            "sha": "04c2530a1acf3de859d7ca6ab778f2585982614f",
            "filename": "tests/models/deepseek_v3/test_modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -27,7 +27,6 @@\n     require_torch_accelerator,\n     require_torch_gpu,\n     require_torch_large_accelerator,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -429,7 +428,6 @@ def test_past_key_values_format(self):\n         super().test_past_key_values_format(custom_all_cache_shapes=all_cache_shapes)\n \n     @require_torch_large_accelerator\n-    @require_torch_sdpa\n     @slow\n     def test_eager_matches_sdpa_generate(self):\n         \"\"\""
        },
        {
            "sha": "3abdd8be35baf31db96ed2a34ffa31ee03db4ffb",
            "filename": "tests/models/deepseek_vl/test_modeling_deepseek_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fdeepseek_vl%2Ftest_modeling_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fdeepseek_vl%2Ftest_modeling_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_vl%2Ftest_modeling_deepseek_vl.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -28,7 +28,6 @@\n from transformers.testing_utils import (\n     require_torch,\n     require_torch_accelerator,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -193,7 +192,6 @@ def test_inputs_embeds_matches_input_ids(self):\n     def test_initialization(self):\n         pass\n \n-    @require_torch_sdpa\n     # Copied from tests.models.janus.test_modeling_janus.JanusVisionText2TextModelTest.test_sdpa_can_dispatch_composite_models\n     def test_sdpa_can_dispatch_composite_models(self):\n         for model_class in self.all_model_classes:"
        },
        {
            "sha": "09f242c525006e9d8ad856ab0871dcf43faf1533",
            "filename": "tests/models/deepseek_vl_hybrid/test_modeling_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_modeling_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_modeling_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_modeling_deepseek_vl_hybrid.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -28,7 +28,6 @@\n from transformers.testing_utils import (\n     require_torch,\n     require_torch_accelerator,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -224,7 +223,6 @@ def test_inputs_embeds_matches_input_ids(self):\n     def test_initialization(self):\n         pass\n \n-    @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         for model_class in self.all_model_classes:\n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "a8f81647f6b6ae257fab96493f04a1931020b7ed",
            "filename": "tests/models/dia/test_modeling_dia.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -26,7 +26,6 @@\n     is_flaky,\n     require_torch,\n     require_torch_accelerator,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -415,7 +414,6 @@ def _check_scores(self, batch_size, scores, generated_length, config):\n         self.assertEqual(len(scores), generated_length)\n         self.assertListEqual([iter_scores.shape for iter_scores in scores], [expected_shape] * len(scores))\n \n-    @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         \"\"\"\n         Overwritten as it relies on hardcoded namings atm - checking for our case here specifically"
        },
        {
            "sha": "fac03c592dcdff26b996530d77f50d087aec18cd",
            "filename": "tests/models/diffllama/test_modeling_diffllama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -31,7 +31,6 @@\n     require_torch,\n     require_torch_accelerator,\n     require_torch_gpu,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -501,7 +500,6 @@ def test_use_flash_attention_2_true(self):\n                 if not has_flash:\n                     raise ValueError(\"The flash model should have flash attention layers\")\n \n-    @require_torch_sdpa\n     @slow\n     def test_eager_matches_sdpa_generate(self):\n         \"\"\""
        },
        {
            "sha": "3d39e973637dfdb8f5ead468a005695dca8355b6",
            "filename": "tests/models/encoder_decoder/test_modeling_encoder_decoder.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -22,7 +22,6 @@\n     Expectations,\n     require_deterministic_for_xpu,\n     require_torch,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -696,7 +695,6 @@ def test_real_model_save_load_from_pretrained(self):\n                 max_diff = np.amax(np.abs(out_1 - out_2))\n                 self.assertLessEqual(max_diff, 1e-5)\n \n-    @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         if not self.supports_sdpa:\n             self.skipTest(\"SDPA is not supported\")"
        },
        {
            "sha": "8077e627413f969e60c7ea3b42b6d67235b5a6fb",
            "filename": "tests/models/evolla/test_modeling_evolla.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fevolla%2Ftest_modeling_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fevolla%2Ftest_modeling_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fevolla%2Ftest_modeling_evolla.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -23,7 +23,6 @@\n     TestCasePlus,\n     require_bitsandbytes,\n     require_torch,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -324,7 +323,6 @@ def test_initialization(self):\n                     )\n \n     @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n-    @require_torch_sdpa\n     @unittest.skip(\"Evolla requires both text and protein inputs which is currently not done in this test.\")\n     def test_eager_matches_sdpa_inference(self):\n         pass"
        },
        {
            "sha": "d5e6578a3294ed8684d8a96c34b7d35cbaafe02c",
            "filename": "tests/models/exaone4/test_modeling_exaone4.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fexaone4%2Ftest_modeling_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fexaone4%2Ftest_modeling_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fexaone4%2Ftest_modeling_exaone4.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -31,7 +31,6 @@\n     require_flash_attn,\n     require_torch,\n     require_torch_accelerator,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -301,7 +300,6 @@ def test_model_generation(self):\n         cleanup(torch_device, gc_collect=True)\n \n     @slow\n-    @require_torch_sdpa\n     def test_model_generation_bf16_sdpa(self):\n         EXPECTED_TEXT = \"Tell me about the Miracle on the Han river.\\n\\nThe Miracle on the Han River is a story about the miracle of the Korean War Armistice.\\n\\nThe Korean War broke out in 35 years ago in 1950. The war was the result of the ideological conflict between the communist north and the capitalist south. The war was brought to a halt in 1953. There was to be peace talks but no peace treaty. As a result of the stalemate the Korean people have neither a peace treaty nor a reunification nor a democratization of Korea. The stalemate of 35 years has produced a people of 70 million\"\n         prompt = \"Tell me about the Miracle on the Han river.\"\n@@ -336,7 +334,6 @@ def test_model_generation_long_flash(self):\n \n     @slow\n     @require_torch_accelerator\n-    @require_torch_sdpa\n     def test_model_generation_beyond_sliding_window(self):\n         EXPECTED_TEXT_COMPLETION = (\n             \" but I'm not sure if I'm going to be able to see it. I really enjoy the scenery, but I'm not sure if I\""
        },
        {
            "sha": "f15b86d425f1e805a0722a0a3ad837086f35f749",
            "filename": "tests/models/falcon/test_modeling_falcon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -24,7 +24,6 @@\n from transformers.testing_utils import (\n     require_bitsandbytes,\n     require_torch,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -203,7 +202,6 @@ def test_batched_generation(self):\n         self.assertEqual(padded_gen_text[0], expected_output)\n \n     @slow\n-    @require_torch_sdpa\n     def test_falcon_alibi_sdpa_matches_eager(self):\n         input_ids = torch.randint(0, 1000, (5, 20))\n "
        },
        {
            "sha": "05ba4aa5521f0bde6c4b45e141042d99d058cbee",
            "filename": "tests/models/gemma3n/test_modeling_gemma3n.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -39,7 +39,6 @@\n     require_read_token,\n     require_torch,\n     require_torch_gpu,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -374,7 +373,6 @@ def _check_hidden_states_for_generate(\n             )\n \n     @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n-    @require_torch_sdpa\n     def test_eager_matches_sdpa_inference(\n         self,\n         name,"
        },
        {
            "sha": "d12a9a6f9064db817dbbd5b5370003d665a4f409",
            "filename": "tests/models/glm/test_modeling_glm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -23,7 +23,6 @@\n     require_flash_attn,\n     require_torch,\n     require_torch_large_accelerator,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -151,7 +150,6 @@ def test_model_9b_eager(self):\n \n         self.assertEqual(output_text, EXPECTED_TEXTS)\n \n-    @require_torch_sdpa\n     def test_model_9b_sdpa(self):\n         EXPECTED_TEXTS = [\n             \"Hello I am doing a project on the history of the internetSolution:\\n\\nStep 1: Introduction\\nThe history of the\","
        },
        {
            "sha": "8974365d473eae8ee92c3f12b260372e34aa77c8",
            "filename": "tests/models/glm4/test_modeling_glm4.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fglm4%2Ftest_modeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fglm4%2Ftest_modeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4%2Ftest_modeling_glm4.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -26,7 +26,6 @@\n     require_torch,\n     require_torch_large_accelerator,\n     require_torch_large_gpu,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -171,7 +170,6 @@ def test_model_9b_eager(self):\n \n         self.assertEqual(output_text, EXPECTED_TEXT)\n \n-    @require_torch_sdpa\n     def test_model_9b_sdpa(self):\n         EXPECTED_TEXTS = Expectations(\n             {"
        },
        {
            "sha": "93088d86ef4f9320745ba2f6905d1f79748fae85",
            "filename": "tests/models/granite_speech/test_modeling_granite_speech.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fgranite_speech%2Ftest_modeling_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fgranite_speech%2Ftest_modeling_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranite_speech%2Ftest_modeling_granite_speech.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -27,7 +27,6 @@\n from transformers.testing_utils import (\n     cleanup,\n     require_torch,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -269,7 +268,6 @@ def test_initialization(self):\n                         msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n                     )\n \n-    @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         # overwrite because Granite Speech is audio+text model (not vision+text)\n         if not self.has_attentions:\n@@ -308,7 +306,6 @@ def test_sdpa_can_dispatch_composite_models(self):\n                         raise ValueError(\"The eager model should not have SDPA attention layers\")\n \n     @pytest.mark.generate\n-    @require_torch_sdpa\n     @slow\n     @unittest.skip(reason=\"Granite Speech doesn't support SDPA for all backbones\")\n     def test_eager_matches_sdpa_generate(self):"
        },
        {
            "sha": "c15c16022282eafd44c3a43a6f9818083390f5cc",
            "filename": "tests/models/idefics2/test_modeling_idefics2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -37,7 +37,6 @@\n     require_torch,\n     require_torch_gpu,\n     require_torch_multi_accelerator,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -335,7 +334,6 @@ def test_resize_embeddings_untied(self):\n             # Check that the model can still do a forward pass successfully (every parameter should be resized)\n             model(**self._prepare_for_class(inputs_dict, model_class))\n \n-    @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         for model_class in self.all_model_classes:\n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n@@ -411,7 +409,6 @@ def test_prompt_lookup_decoding_matches_greedy_search(self):\n         pass\n \n     @pytest.mark.generate\n-    @require_torch_sdpa\n     @slow\n     @unittest.skip(\n         reason=\"Idefics2 doesn't support SDPA for all backbones, vision backbones has only eager/FA2 attention\""
        },
        {
            "sha": "73517814b0d2dfeac7030586567bc0422ee3ec4f",
            "filename": "tests/models/idefics3/test_modeling_idefics3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -29,7 +29,6 @@\n     cleanup,\n     require_bitsandbytes,\n     require_torch,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -371,7 +370,6 @@ def test_prompt_lookup_decoding_matches_greedy_search(self):\n         pass\n \n     @pytest.mark.generate\n-    @require_torch_sdpa\n     @slow\n     @unittest.skip(\n         reason=\"Idefics3 doesn't support SDPA for all backbones, vision backbones has only eager/FA2 attention\""
        },
        {
            "sha": "afb67fdf988797db9b3158e1ce4ecd8c2181d6ec",
            "filename": "tests/models/instructblip/test_modeling_instructblip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -34,7 +34,6 @@\n     require_accelerate,\n     require_bitsandbytes,\n     require_torch,\n-    require_torch_sdpa,\n     require_vision,\n     slow,\n     torch_device,\n@@ -655,7 +654,6 @@ def _prepare_model_kwargs(input_ids, attention_mask, signature):\n             # They should result in very similar logits\n             torch.testing.assert_close(next_logits_wo_padding, next_logits_with_padding, rtol=1e-5, atol=1e-5)\n \n-    @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         \"\"\"\n         Tests if composite models dispatch correctly on SDPA/eager when requested so when loading the model."
        },
        {
            "sha": "270ba8bcc63f50cf89b689a7b3db25498654f854",
            "filename": "tests/models/instructblipvideo/test_modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -32,7 +32,6 @@\n     require_accelerate,\n     require_bitsandbytes,\n     require_torch,\n-    require_torch_sdpa,\n     require_vision,\n     slow,\n     torch_device,\n@@ -667,7 +666,6 @@ def _prepare_model_kwargs(input_ids, attention_mask, signature):\n             # They should result in very similar logits\n             torch.testing.assert_close(next_logits_wo_padding, next_logits_with_padding, rtol=1e-5, atol=1e-5)\n \n-    @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         \"\"\"\n         Tests if composite models dispatch correctly on SDPA/eager when requested so when loading the model."
        },
        {
            "sha": "c14911d67a6e2ec0d64d3e4a85333602b7ad84d8",
            "filename": "tests/models/kosmos2/test_modeling_kosmos2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -30,7 +30,6 @@\n     IS_ROCM_SYSTEM,\n     IS_XPU_SYSTEM,\n     require_torch,\n-    require_torch_sdpa,\n     require_vision,\n     slow,\n     torch_device,\n@@ -458,7 +457,6 @@ def check_same_values(layer_1, layer_2):\n             # self.assertTrue(check_same_values(model.transformer.wte, model.lm_head))\n \n     @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n-    @require_torch_sdpa\n     @unittest.skip(\"KOSMOS-2 doesn't support padding\")\n     def test_eager_matches_sdpa_inference(\n         self, name, torch_dtype, padding_side, use_attention_mask, output_attentions, enable_kernels"
        },
        {
            "sha": "ef5db1b1793b7655eb0205d21da46a662edc06a3",
            "filename": "tests/models/kosmos2_5/test_modeling_kosmos2_5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fkosmos2_5%2Ftest_modeling_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fkosmos2_5%2Ftest_modeling_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2_5%2Ftest_modeling_kosmos2_5.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -33,7 +33,6 @@\n     require_flash_attn,\n     require_torch,\n     require_torch_gpu,\n-    require_torch_sdpa,\n     require_vision,\n     slow,\n     torch_device,\n@@ -549,7 +548,7 @@ def test_flash_attn_2_inference_equivalence(self):\n         pass\n \n     # TODO: ydshieh\n-    @require_torch_sdpa\n+\n     @require_torch_gpu\n     @slow\n     @unittest.skip(reason=\"_update_causal_mask is not implemented yet which fails this test\")"
        },
        {
            "sha": "791a699b44e6a7f2489c8ca344b912b583d9b476",
            "filename": "tests/models/kyutai_speech_to_text/test_modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -33,7 +33,6 @@\n     require_accelerate,\n     require_torch,\n     require_torch_accelerator,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -340,7 +339,6 @@ def test_initialization(self):\n                         )\n \n     @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n-    @require_torch_sdpa\n     def test_eager_matches_sdpa_inference(\n         self, name, torch_dtype, padding_side, use_attention_mask, output_attentions, enable_kernels\n     ):"
        },
        {
            "sha": "dc23977d221ed1cd846d540576c66ea5bd4bcf93",
            "filename": "tests/models/metaclip_2/test_modeling_metaclip_2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fmetaclip_2%2Ftest_modeling_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fmetaclip_2%2Ftest_modeling_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmetaclip_2%2Ftest_modeling_metaclip_2.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -28,7 +28,6 @@\n     require_flash_attn,\n     require_torch,\n     require_torch_gpu,\n-    require_torch_sdpa,\n     require_vision,\n     slow,\n     torch_device,\n@@ -163,7 +162,6 @@ def prepare_config_and_inputs_for_common(self):\n         return config, inputs_dict\n \n     @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n-    @require_torch_sdpa\n     def test_eager_matches_sdpa_inference(self, *args):\n         return getattr(ModelTesterMixin, self._testMethodName)(self)\n \n@@ -175,7 +173,6 @@ class MetaClip2ModelTesterMixin(ModelTesterMixin):\n     different output logits, and are not supposed to be used or tested with padding_side=\"left\".\n     \"\"\"\n \n-    @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         for model_class in self.all_model_classes:\n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n@@ -296,13 +293,11 @@ def test_model_with_projection_from_pretrained(self):\n         self.assertTrue(hasattr(model, \"visual_projection\"))\n \n     @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n-    @require_torch_sdpa\n     @is_flaky()\n     def test_eager_matches_sdpa_inference(self, *args):\n         # adding only flaky decorator here and call the parent test method\n         return getattr(ModelTesterMixin, self._testMethodName)(self)\n \n-    @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         super().test_sdpa_can_dispatch_composite_models()\n \n@@ -471,18 +466,15 @@ def test_model_with_projection_from_pretrained(self):\n         self.assertTrue(hasattr(model, \"text_projection\"))\n \n     @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n-    @require_torch_sdpa\n     @slow\n     @is_flaky()\n     def test_eager_matches_sdpa_inference(self, *args):\n         # adding only flaky decorator here and call the parent test method\n         return getattr(ModelTesterMixin, self._testMethodName)(self)\n \n-    @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         super().test_sdpa_can_dispatch_composite_models()\n \n-    @require_torch_sdpa\n     def test_sdpa_can_dispatch_on_flash(self):\n         self.skipTest(\n             reason=\"MetaClip2TextModel has two attention masks: `causal_attention_mask` and `attention_mask`\"\n@@ -703,24 +695,20 @@ def test_model_from_pretrained(self):\n         self.assertIsNotNone(model)\n \n     @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n-    @require_torch_sdpa\n     @slow\n     @is_flaky()\n     def test_eager_matches_sdpa_inference(self, *args):\n         # adding only flaky decorator here and call the parent test method\n         return getattr(ModelTesterMixin, self._testMethodName)(self)\n \n-    @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         super().test_sdpa_can_dispatch_composite_models()\n \n-    @require_torch_sdpa\n     def test_sdpa_can_dispatch_on_flash(self):\n         self.skipTest(\n             reason=\"MetaClip2 text tower has two attention masks: `causal_attention_mask` and `attention_mask`\"\n         )\n \n-    @require_torch_sdpa\n     def test_sdpa_can_compile_dynamic(self):\n         self.skipTest(reason=\"MetaClip2 model can't be compiled dynamic, error in metaclip_2_loss`\")\n \n@@ -875,14 +863,12 @@ def test_initialization(self):\n         pass\n \n     @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n-    @require_torch_sdpa\n     @slow\n     @is_flaky()\n     def test_eager_matches_sdpa_inference(self, *args):\n         # adding only flaky decorator here and call the parent test method\n         return getattr(ModelTesterMixin, self._testMethodName)(self)\n \n-    @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         super().test_sdpa_can_dispatch_composite_models()\n "
        },
        {
            "sha": "3c83a7de90f142160137dbe4058ef59991f74a4c",
            "filename": "tests/models/mistral/test_modeling_mistral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -34,7 +34,6 @@\n     require_torch,\n     require_torch_accelerator,\n     require_torch_gpu,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -225,7 +224,6 @@ def test_model_7b_long_prompt(self):\n         self.assertEqual(EXPECTED_OUTPUT_TOKEN_IDS, generated_ids[0][-2:].tolist())\n \n     @slow\n-    @require_torch_sdpa\n     def test_model_7b_long_prompt_sdpa(self):\n         EXPECTED_OUTPUT_TOKEN_IDS = [306, 338]\n         # An input with 4097 tokens that is above the size of the sliding window"
        },
        {
            "sha": "21ef4b457aaccea24f987c10ec1029e74bc4630c",
            "filename": "tests/models/moshi/test_modeling_moshi.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -35,7 +35,6 @@\n     is_torch_available,\n     require_torch,\n     require_torch_fp16,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -194,7 +193,6 @@ def _get_logits_processor_kwargs(self, do_sample=False, config=None):\n         return logits_processor_kwargs\n \n     @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n-    @require_torch_sdpa\n     def test_eager_matches_sdpa_inference(\n         self, name, torch_dtype, padding_side, use_attention_mask, output_attentions, enable_kernels\n     ):\n@@ -692,7 +690,6 @@ def test_left_padding_compatibility(self):\n             # They should result in very similar logits\n             torch.testing.assert_close(next_logits_wo_padding, next_logits_with_padding, rtol=1e-5, atol=1e-5)\n \n-    @require_torch_sdpa\n     @slow\n     @is_flaky(max_attempts=5, description=\"flaky on some models.\")\n     def test_eager_matches_sdpa_generate(self):"
        },
        {
            "sha": "1464adc31c77c30cc4bc8510d6e4280bcbad9082",
            "filename": "tests/models/musicgen/test_modeling_musicgen.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -40,7 +40,6 @@\n     require_torch_accelerator,\n     require_torch_fp16,\n     require_torch_gpu,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -1032,7 +1031,6 @@ def test_flash_attn_2_inference_equivalence(self):\n     def test_flash_attn_2_conversion(self):\n         self.skipTest(reason=\"Musicgen doesn't use the MusicgenFlashAttention2 class method.\")\n \n-    @require_torch_sdpa\n     @require_torch_accelerator\n     @slow\n     def test_sdpa_can_dispatch_on_flash(self):\n@@ -1170,7 +1168,6 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n \n                 assert torch.allclose(logits_fa[:-1], logits[:-1], atol=4e-2, rtol=4e-2)\n \n-    @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         if not self.has_attentions:\n             self.skipTest(reason=\"Model architecture does not support attentions\")"
        },
        {
            "sha": "5bff83f4a0285fe02b5e3cf2189f7f342127c54e",
            "filename": "tests/models/musicgen_melody/test_modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -41,7 +41,6 @@\n     require_torch_accelerator,\n     require_torch_fp16,\n     require_torch_gpu,\n-    require_torch_sdpa,\n     require_torchaudio,\n     slow,\n     torch_device,\n@@ -1033,7 +1032,6 @@ def test_flash_attn_2_inference_equivalence(self):\n     def test_flash_attn_2_conversion(self):\n         self.skipTest(reason=\"MusicgenMelody doesn't use the MusicgenMelodyFlashAttention2 class method.\")\n \n-    @require_torch_sdpa\n     @require_torch_accelerator\n     @slow\n     def test_sdpa_can_dispatch_on_flash(self):\n@@ -1171,7 +1169,6 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n \n                 assert torch.allclose(logits_fa[:-1], logits[:-1], atol=4e-2, rtol=4e-2)\n \n-    @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         if not self.has_attentions:\n             self.skipTest(reason=\"Model architecture does not support attentions\")"
        },
        {
            "sha": "8f87f55b577f2e3debc54c94ee62f7c349829abd",
            "filename": "tests/models/qwen2/test_modeling_qwen2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -28,7 +28,6 @@\n     require_flash_attn,\n     require_torch,\n     require_torch_gpu,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -179,7 +178,6 @@ def test_model_450m_long_prompt(self):\n         gc.collect()\n \n     @slow\n-    @require_torch_sdpa\n     def test_model_450m_long_prompt_sdpa(self):\n         EXPECTED_OUTPUT_TOKEN_IDS = [306, 338]\n         # An input with 4097 tokens that is above the size of the sliding window"
        },
        {
            "sha": "155da7e66f246ae133a341d4b33c0b38a66b9cc9",
            "filename": "tests/models/qwen2_5_omni/test_modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -38,7 +38,6 @@\n     require_flash_attn,\n     require_torch,\n     require_torch_gpu,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -295,7 +294,6 @@ def test_sdpa_can_dispatch_on_flash(self):\n     def test_model_outputs_equivalence(self):\n         pass\n \n-    @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         # overwrite because Qwen2 is audio+text model (not vision+text)\n         if not self.has_attentions:"
        },
        {
            "sha": "525ddad76e97d865108813992103a23d03e614dc",
            "filename": "tests/models/qwen2_audio/test_modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -30,7 +30,6 @@\n from transformers.testing_utils import (\n     cleanup,\n     require_torch,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -161,7 +160,6 @@ def test_sdpa_can_dispatch_on_flash(self):\n     def test_flash_attn_2_inference_equivalence_right_padding(self):\n         pass\n \n-    @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         # overwrite because Qwen2 is audio+text model (not vision+text)\n         if not self.has_attentions:"
        },
        {
            "sha": "01bab62c7e017d24eb0bc5c9b1239953ce3d12fb",
            "filename": "tests/models/qwen2_moe/test_modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -25,7 +25,6 @@\n     require_flash_attn,\n     require_torch,\n     require_torch_gpu,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -213,7 +212,6 @@ def test_model_a2_7b_long_prompt(self):\n         gc.collect()\n \n     @slow\n-    @require_torch_sdpa\n     def test_model_a2_7b_long_prompt_sdpa(self):\n         EXPECTED_OUTPUT_TOKEN_IDS = [306, 338]\n         # An input with 4097 tokens that is above the size of the sliding window"
        },
        {
            "sha": "1645802323cbd5f38b2016f85155277704917223",
            "filename": "tests/models/qwen3/test_modeling_qwen3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -27,7 +27,6 @@\n     require_flash_attn,\n     require_torch,\n     require_torch_gpu,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -168,7 +167,6 @@ def test_model_600m_long_prompt(self):\n         self.assertEqual(EXPECTED_OUTPUT_TOKEN_IDS, generated_ids[0][-2:].tolist())\n \n     @slow\n-    @require_torch_sdpa\n     def test_model_600m_long_prompt_sdpa(self):\n         EXPECTED_OUTPUT_TOKEN_IDS = [198, 198]\n         # An input with 4097 tokens that is above the size of the sliding window"
        },
        {
            "sha": "1f58483bc72f7007adf0a70e78455b180413cd1a",
            "filename": "tests/models/qwen3_moe/test_modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fqwen3_moe%2Ftest_modeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fqwen3_moe%2Ftest_modeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_moe%2Ftest_modeling_qwen3_moe.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -26,7 +26,6 @@\n     require_torch_gpu,\n     require_torch_large_accelerator,\n     require_torch_multi_accelerator,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -218,7 +217,6 @@ def test_model_15b_a2b_long_prompt(self):\n         self.assertEqual(EXPECTED_OUTPUT_TOKEN_IDS, generated_ids[0][-2:].tolist())\n \n     @slow\n-    @require_torch_sdpa\n     def test_model_15b_a2b_long_prompt_sdpa(self):\n         EXPECTED_OUTPUT_TOKEN_IDS = [306, 338]\n         # An input with 4097 tokens that is above the size of the sliding window"
        },
        {
            "sha": "4ee29697dba49f1880aa4eb53b59d69ee089c884",
            "filename": "tests/models/sam/test_modeling_sam.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -20,7 +20,7 @@\n import requests\n \n from transformers import SamConfig, SamMaskDecoderConfig, SamPromptEncoderConfig, SamVisionConfig, pipeline\n-from transformers.testing_utils import Expectations, cleanup, require_torch, require_torch_sdpa, slow, torch_device\n+from transformers.testing_utils import Expectations, cleanup, require_torch, slow, torch_device\n from transformers.utils import is_torch_available, is_vision_available\n \n from ...test_configuration_common import ConfigTester\n@@ -257,7 +257,6 @@ def test_retain_grad_hidden_states_attentions(self):\n     def test_hidden_states_output(self):\n         pass\n \n-    @require_torch_sdpa\n     @pytest.mark.torch_compile_test\n     def test_sdpa_can_compile_dynamic(self):\n         self.skipTest(reason=\"SAM model can't be compiled dynamic yet\")\n@@ -659,12 +658,10 @@ def test_model_from_pretrained(self):\n         model = SamModel.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n-    @require_torch_sdpa\n     @pytest.mark.torch_compile_test\n     def test_sdpa_can_compile_dynamic(self):\n         self.skipTest(reason=\"SAM model can't be compiled dynamic yet\")\n \n-    @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         \"\"\"\n         Tests if composite models dispatch correctly on SDPA/eager when requested so when loading the model."
        },
        {
            "sha": "73b4922f665feca49f437ac386bfe5182da1cd09",
            "filename": "tests/models/sam2/test_modeling_sam2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fsam2%2Ftest_modeling_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fsam2%2Ftest_modeling_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam2%2Ftest_modeling_sam2.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -32,7 +32,6 @@\n from transformers.testing_utils import (\n     backend_empty_cache,\n     require_torch,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -276,7 +275,6 @@ def check_hidden_states_output(inputs_dict, config, model_class, image_size):\n     def test_batching_equivalence(self, atol=5e-4, rtol=5e-4):\n         super().test_batching_equivalence(atol=atol, rtol=rtol)\n \n-    @require_torch_sdpa\n     def test_sdpa_can_compile_dynamic(self):\n         self.skipTest(reason=\"SAM model can't be compiled dynamic yet\")\n \n@@ -560,7 +558,7 @@ def test_attention_outputs(self):\n             )\n \n     # Override as Sam2Model has different sub-modules\n-    @require_torch_sdpa\n+\n     def test_sdpa_can_dispatch_composite_models(self):\n         \"\"\"\n         Tests if composite models dispatch correctly on SDPA/eager when requested so when loading the model.\n@@ -716,7 +714,6 @@ def test_model_from_pretrained(self):\n         model = Sam2Model.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n-    @require_torch_sdpa\n     def test_sdpa_can_compile_dynamic(self):\n         self.skipTest(reason=\"SAM2 model can't be compiled dynamic yet\")\n "
        },
        {
            "sha": "c2587ac67523fcdd1729505ce5baf58f18e3d9d8",
            "filename": "tests/models/sam_hq/test_modeling_sam_hq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fsam_hq%2Ftest_modeling_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fsam_hq%2Ftest_modeling_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam_hq%2Ftest_modeling_sam_hq.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -28,7 +28,7 @@\n     SamHQVisionModel,\n     pipeline,\n )\n-from transformers.testing_utils import Expectations, cleanup, require_torch, require_torch_sdpa, slow, torch_device\n+from transformers.testing_utils import Expectations, cleanup, require_torch, slow, torch_device\n from transformers.utils import is_torch_available, is_vision_available\n \n from ...test_configuration_common import ConfigTester\n@@ -265,7 +265,6 @@ def test_retain_grad_hidden_states_attentions(self):\n     def test_hidden_states_output(self):\n         pass\n \n-    @require_torch_sdpa\n     @pytest.mark.torch_compile_test\n     def test_sdpa_can_compile_dynamic(self):\n         self.skipTest(reason=\"SAM model can't be compiled dynamic yet\")\n@@ -707,12 +706,10 @@ def test_model_from_pretrained(self):\n         model = SamHQModel.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n-    @require_torch_sdpa\n     @pytest.mark.torch_compile_test\n     def test_sdpa_can_compile_dynamic(self):\n         self.skipTest(reason=\"SamHQModel can't be compiled dynamic yet\")\n \n-    @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         \"\"\"\n         Tests if composite models dispatch correctly on SDPA/eager when requested so when loading the model."
        },
        {
            "sha": "194da0234e3a6b041fc6b7945fd0fbf994de82ec",
            "filename": "tests/models/siglip/test_modeling_siglip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -46,7 +46,6 @@\n     floats_tensor,\n     ids_tensor,\n     random_attention_mask,\n-    require_torch_sdpa,\n )\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n@@ -64,7 +63,6 @@\n \n \n class SiglipModelTesterMixin(ModelTesterMixin):\n-    @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         for model_class in self.all_model_classes:\n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n@@ -257,7 +255,6 @@ def test_model_from_pretrained(self):\n         self.assertIsNotNone(model)\n \n     @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n-    @require_torch_sdpa\n     @is_flaky()\n     def test_eager_matches_sdpa_inference(self, *args):\n         # adding only flaky decorator here and call the parent test method"
        },
        {
            "sha": "15f160beecabff995f1d483220d5c18ced72763e",
            "filename": "tests/models/siglip2/test_modeling_siglip2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fsiglip2%2Ftest_modeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fsiglip2%2Ftest_modeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsiglip2%2Ftest_modeling_siglip2.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -44,7 +44,6 @@\n     floats_tensor,\n     ids_tensor,\n     random_attention_mask,\n-    require_torch_sdpa,\n )\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n@@ -62,7 +61,6 @@\n \n \n class Siglip2ModelTesterMixin(ModelTesterMixin):\n-    @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         for model_class in self.all_model_classes:\n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n@@ -347,7 +345,6 @@ def test_model_from_pretrained(self):\n         self.assertIsNotNone(model)\n \n     @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n-    @require_torch_sdpa\n     @is_flaky()\n     def test_eager_matches_sdpa_inference(self, *args):\n         # adding only flaky decorator here and call the parent test method"
        },
        {
            "sha": "5f83a477683703140fd4ced9d32d769cb4849037",
            "filename": "tests/models/smollm3/test_modeling_smollm3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fsmollm3%2Ftest_modeling_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fsmollm3%2Ftest_modeling_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmollm3%2Ftest_modeling_smollm3.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -28,7 +28,6 @@\n     require_bitsandbytes,\n     require_flash_attn,\n     require_torch,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -93,7 +92,6 @@ class SmolLM3ModelTest(CausalLMModelTest, unittest.TestCase):\n     )\n \n     @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n-    @require_torch_sdpa\n     @is_flaky()\n     def test_eager_matches_sdpa_inference(self, *args):\n         # flaky test_eager_matches_sdpa_inference_24_fp32_pad_left_output_attentions"
        },
        {
            "sha": "60c1943c74574c33883b148909967e4457a19c0e",
            "filename": "tests/models/smolvlm/test_modeling_smolvlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -31,7 +31,6 @@\n     cleanup,\n     is_flaky,\n     require_torch,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -399,7 +398,6 @@ def test_sdpa_can_dispatch_on_flash(self):\n         pass\n \n     @pytest.mark.generate\n-    @require_torch_sdpa\n     @slow\n     @unittest.skip(\n         reason=\"SmolVLM doesn't support SDPA for all backbones, vision backbones has only eager/FA2 attention\""
        },
        {
            "sha": "fb0384c3bf8b76eb2fbb63239afd1c5d49adfd7b",
            "filename": "tests/models/speech_encoder_decoder/test_modeling_speech_encoder_decoder.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fspeech_encoder_decoder%2Ftest_modeling_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fspeech_encoder_decoder%2Ftest_modeling_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeech_encoder_decoder%2Ftest_modeling_speech_encoder_decoder.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -20,7 +20,6 @@\n from transformers.testing_utils import (\n     require_deterministic_for_xpu,\n     require_torch,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -455,7 +454,6 @@ def test_real_model_save_load_from_pretrained(self):\n                 max_diff = np.amax(np.abs(out_1 - out_2))\n                 self.assertLessEqual(max_diff, 1e-5)\n \n-    @require_torch_sdpa\n     @unittest.skip(\"TODO Arthur I have to skip for now because I don't understand it\")\n     def test_sdpa_can_dispatch_composite_models(self):\n         inputs_dict = self.prepare_config_and_inputs()"
        },
        {
            "sha": "b603d2b266da24917e7317d15d2579a810a73439",
            "filename": "tests/models/t5gemma/test_modeling_t5gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -25,7 +25,6 @@\n     require_torch,\n     require_torch_accelerator,\n     require_torch_gpu,\n-    require_torch_sdpa,\n     torch_device,\n )\n \n@@ -829,7 +828,7 @@ def test_T5Gemma_token_classification_model(self):\n \n     # Based on tests.models.gemma.test_modeling_gemma.GemmaModelTest.test_sdpa_equivalence\n     # Add decoder_input_ids and adjust hidden states.\n-    @require_torch_sdpa\n+\n     @require_torch_accelerator\n     def test_sdpa_equivalence(self):\n         for model_class in self.all_model_classes:"
        },
        {
            "sha": "dbaa2d3d8db1271e1a2d73bc4e1701afe298e02c",
            "filename": "tests/models/vision_encoder_decoder/test_modeling_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_vision_encoder_decoder.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -26,7 +26,6 @@\n     require_nltk,\n     require_sentencepiece,\n     require_torch,\n-    require_torch_sdpa,\n     require_vision,\n     slow,\n     to_2tuple,\n@@ -393,7 +392,6 @@ def test_real_model_save_load_from_pretrained(self):\n                 max_diff = np.amax(np.abs(out_1 - out_2))\n                 self.assertLessEqual(max_diff, 1e-5)\n \n-    @require_torch_sdpa\n     @unittest.skip(\"TODO Arthur I have to skip for now because I don't understand it\")\n     def test_sdpa_can_dispatch_composite_models(self):\n         if not self.supports_sdpa:"
        },
        {
            "sha": "9118f3ade6b3106d78b183237b3c4a8a107263ca",
            "filename": "tests/models/voxtral/test_modeling_voxtral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fvoxtral%2Ftest_modeling_voxtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fvoxtral%2Ftest_modeling_voxtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvoxtral%2Ftest_modeling_voxtral.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -25,7 +25,6 @@\n from transformers.testing_utils import (\n     cleanup,\n     require_torch,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -190,7 +189,6 @@ def test_flash_attention_3_padding_matches_padding_free_with_position_ids(self):\n     def test_flash_attention_3_padding_matches_padding_free_with_position_ids_and_fa_kwargs(self):\n         pass\n \n-    @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         # overwrite because Voxtral is audio+text model (not vision+text)\n         if not self.has_attentions:"
        },
        {
            "sha": "269b08860d79fdd1d8bcb85be7a098d50a7ed2a1",
            "filename": "tests/models/xlm_roberta/test_modeling_xlm_roberta.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fxlm_roberta%2Ftest_modeling_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fxlm_roberta%2Ftest_modeling_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlm_roberta%2Ftest_modeling_xlm_roberta.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -20,7 +20,6 @@\n     require_sentencepiece,\n     require_tokenizers,\n     require_torch,\n-    require_torch_sdpa,\n     slow,\n )\n \n@@ -54,7 +53,6 @@ def test_xlm_roberta_base(self):\n         # compare the actual values for a slice of last dim\n         torch.testing.assert_close(output[:, :, -1], expected_output_values_last_dim, rtol=1e-3, atol=1e-3)\n \n-    @require_torch_sdpa\n     def test_xlm_roberta_base_sdpa(self):\n         input_ids = torch.tensor([[0, 581, 10269, 83, 99942, 136, 60742, 23, 70, 80583, 18276, 2]])\n         # The dog is cute and lives in the garden house"
        },
        {
            "sha": "f294e7baec360603899a5f7ae2457c8c3900635f",
            "filename": "tests/models/xlstm/test_modeling_xlstm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fxlstm%2Ftest_modeling_xlstm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Fmodels%2Fxlstm%2Ftest_modeling_xlstm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlstm%2Ftest_modeling_xlstm.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -34,9 +34,6 @@\n         xLSTMModel,\n     )\n     from transformers.models.xlstm.modeling_xlstm import xLSTMBlock, xLSTMCache\n-    from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_2\n-else:\n-    is_torch_greater_or_equal_than_2_2 = False\n \n \n class xLSTMModelTester:"
        },
        {
            "sha": "e663e495e5f819d4d35c930cfa0cefbf19ec7d0d",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 13,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -98,7 +98,6 @@\n     require_torch_mps,\n     require_torch_multi_accelerator,\n     require_torch_multi_gpu,\n-    require_torch_sdpa,\n     run_first,\n     run_test_using_subprocess,\n     set_config_for_less_flaky_test,\n@@ -114,7 +113,6 @@\n     is_accelerate_available,\n     is_torch_bf16_available_on_device,\n     is_torch_fp16_available_on_device,\n-    is_torch_sdpa_available,\n )\n from transformers.utils.generic import ContextManagers\n \n@@ -1384,11 +1382,7 @@ def _create_and_check_torchscript(self, config, inputs_dict):\n         configs_no_init.torchscript = True\n         for model_class in self.all_model_classes:\n             for attn_implementation in [\"eager\", \"sdpa\"]:\n-                if (\n-                    attn_implementation == \"sdpa\"\n-                    and (not model_class._supports_sdpa or not is_torch_sdpa_available())\n-                    or config.output_attentions\n-                ):\n+                if attn_implementation == \"sdpa\" and not model_class._supports_sdpa or config.output_attentions:\n                     continue\n \n                 configs_no_init._attn_implementation = attn_implementation\n@@ -3699,7 +3693,6 @@ def test_attn_implementation_composite_models(self):\n                     ):\n                         self.assertTrue(submodule.config._attn_implementation == \"eager\")\n \n-    @require_torch_sdpa\n     def test_sdpa_can_dispatch_non_composite_models(self):\n         \"\"\"\n         Tests if non-composite models dispatch correctly on SDPA/eager when requested so when loading the model.\n@@ -3737,7 +3730,6 @@ def test_sdpa_can_dispatch_non_composite_models(self):\n                             f\"The eager model should not have SDPA attention layers but got `{class_name}.config._attn_implementation={submodule.config._attn_implementation}`\"\n                         )\n \n-    @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         \"\"\"\n         Tests if composite models dispatch correctly on SDPA/eager when requested so when loading the model.\n@@ -3794,15 +3786,13 @@ def test_sdpa_can_dispatch_composite_models(self):\n                         raise ValueError(\"The eager model should not have SDPA attention layers\")\n \n     @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n-    @require_torch_sdpa\n     def test_eager_matches_sdpa_inference(\n         self, name, torch_dtype, padding_side, use_attention_mask, output_attentions, enable_kernels\n     ):\n         _test_eager_matches_sdpa_inference(\n             self, name, torch_dtype, padding_side, use_attention_mask, output_attentions, enable_kernels\n         )\n \n-    @require_torch_sdpa\n     @require_torch_accelerator\n     @slow\n     def test_sdpa_can_dispatch_on_flash(self):\n@@ -3884,7 +3874,6 @@ def test_sdpa_can_dispatch_on_flash(self):\n                 with sdpa_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n                     _ = model(**inputs_dict)\n \n-    @require_torch_sdpa\n     @require_torch_accelerator\n     @pytest.mark.torch_compile_test\n     @slow\n@@ -3950,7 +3939,6 @@ def test_sdpa_can_compile_dynamic(self):\n                 with torch.no_grad():\n                     _ = model(**inputs_dict)\n \n-    @require_torch_sdpa\n     def test_sdpa_matches_eager_sliding_window(self):\n         if not self.has_attentions:\n             self.skipTest(reason=\"Model architecture does not support attentions\")"
        },
        {
            "sha": "7e36ebdfd9aaaa53bce9e5ad2b151f045892d6be",
            "filename": "tests/utils/test_model_output.py",
            "status": "modified",
            "additions": 4,
            "deletions": 10,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Futils%2Ftest_model_output.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Futils%2Ftest_model_output.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_model_output.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -27,8 +27,6 @@\n if is_torch_available():\n     import torch\n \n-    from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_2\n-\n \n @dataclass\n class ModelOutputTest(ModelOutput):\n@@ -153,20 +151,16 @@ def test_torch_pytree(self):\n         unflattened_x = pytree.tree_unflatten(actual_flat_outs, actual_tree_spec)\n         self.assertEqual(x, unflattened_x)\n \n-        if is_torch_greater_or_equal_than_2_2:\n-            self.assertEqual(\n-                pytree.treespec_dumps(actual_tree_spec),\n-                '[1, {\"type\": \"tests.utils.test_model_output.ModelOutputTest\", \"context\": \"[\\\\\"a\\\\\", \\\\\"c\\\\\"]\", \"children_spec\": [{\"type\": null, \"context\": null, \"children_spec\": []}, {\"type\": null, \"context\": null, \"children_spec\": []}]}]',\n-            )\n+        self.assertEqual(\n+            pytree.treespec_dumps(actual_tree_spec),\n+            '[1, {\"type\": \"tests.utils.test_model_output.ModelOutputTest\", \"context\": \"[\\\\\"a\\\\\", \\\\\"c\\\\\"]\", \"children_spec\": [{\"type\": null, \"context\": null, \"children_spec\": []}, {\"type\": null, \"context\": null, \"children_spec\": []}]}]',\n+        )\n \n     # TODO: @ydshieh\n     @unittest.skip(reason=\"CPU OOM\")\n     @require_torch\n     @pytest.mark.torch_export_test\n     def test_export_serialization(self):\n-        if not is_torch_greater_or_equal_than_2_2:\n-            self.skipTest(reason=\"Export serialization requires torch >= 2.2.0\")\n-\n         model_cls = AlbertForMaskedLM\n         model_config = model_cls.config_class()\n         model = model_cls(model_config)"
        },
        {
            "sha": "0747b469f24ff9ffc330b2230787115b51ed0f91",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 18,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2df0c323cb23a722a3f8c20f8803adda136e1e53/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=2df0c323cb23a722a3f8c20f8803adda136e1e53",
            "patch": "@@ -78,7 +78,6 @@\n     is_flash_attn_2_available,\n     is_flash_attn_3_available,\n     is_torch_npu_available,\n-    is_torch_sdpa_available,\n )\n \n \n@@ -693,9 +692,7 @@ def test_model_from_pretrained_attn_implementation(self):\n         # test that the model can be instantiated with attn_implementation of either\n         # 1. explicit from_pretrained's attn_implementation argument\n         # 2. explicit from_pretrained's attn_implementation argument with a config argument\n-        attn_implementation_available = [\"eager\"]\n-        if is_torch_sdpa_available():\n-            attn_implementation_available.append(\"sdpa\")\n+        attn_implementation_available = [\"eager\", \"sdpa\"]\n \n         if is_flash_attn_available():\n             attn_implementation_available.append(\"flash_attention_2\")\n@@ -720,9 +717,7 @@ def test_model_from_config_attn_implementation(self):\n         # 1. config created with explicit attn_implementatation and from_config\n         # 2. explicit from_config's attn_implementation argument with a config argument\n         # 3. config created with explicit attn_implementatation and from_config overriding with explicit attn_implementation argument\n-        attn_implementation_available = [\"eager\"]\n-        if is_torch_sdpa_available():\n-            attn_implementation_available.append(\"sdpa\")\n+        attn_implementation_available = [\"eager\", \"sdpa\"]\n \n         if is_flash_attn_available():\n             attn_implementation_available.append(\"flash_attention_2\")\n@@ -2757,17 +2752,6 @@ def test_not_available_flash_with_config(self):\n \n         self.assertTrue(\"the package flash_attn seems to be not installed\" in str(cm.exception))\n \n-    def test_not_available_sdpa(self):\n-        if is_torch_sdpa_available():\n-            self.skipTest(reason=\"This test requires torch<=2.0\")\n-\n-        with self.assertRaises(ImportError) as cm:\n-            _ = AutoModel.from_pretrained(\n-                \"hf-internal-testing/tiny-random-GPTBigCodeModel\", attn_implementation=\"sdpa\"\n-            )\n-\n-        self.assertTrue(\"PyTorch SDPA requirements in Transformers are not met\" in str(cm.exception))\n-\n \n @require_torch\n class TestTensorSharing(TestCasePlus):"
        }
    ],
    "stats": {
        "total": 240,
        "additions": 17,
        "deletions": 223
    }
}