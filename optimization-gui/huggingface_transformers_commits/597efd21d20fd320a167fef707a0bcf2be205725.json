{
    "author": "ArthurZucker",
    "message": "Auto compile when static cache (#34247)\n\n* generate with compile\r\n\r\n* nits\r\n\r\n* simple\r\n\r\n* generate with compile\r\n\r\n* nits\r\n\r\n* simple\r\n\r\n* safe\r\n\r\n* style\r\n\r\n* Update src/transformers/generation/utils.py\r\n\r\nCo-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>\r\n\r\n* remove TOKENIZER forked warning\r\n\r\n---------\r\n\r\nCo-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>",
    "sha": "597efd21d20fd320a167fef707a0bcf2be205725",
    "files": [
        {
            "sha": "c839a6538dcf5e7e45d9ef185d58b77b1b7b8e83",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 16,
            "deletions": 2,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/597efd21d20fd320a167fef707a0bcf2be205725/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/597efd21d20fd320a167fef707a0bcf2be205725/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=597efd21d20fd320a167fef707a0bcf2be205725",
            "patch": "@@ -15,6 +15,7 @@\n # limitations under the License.\n import copy\n import inspect\n+import os\n import warnings\n from dataclasses import dataclass\n from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n@@ -3224,6 +3225,16 @@ def _sample(\n         unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=input_ids.device)\n         model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)\n \n+        def model_forward(model, *args, **kwargs):\n+            return model.forward(*args, **kwargs)\n+\n+        if isinstance(model_kwargs.get(\"past_key_values\"), StaticCache):\n+            if self.device.type == \"cuda\":\n+                logger.warning_once(\"Using `torch.compile`.\")\n+                os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n+                model_forward = torch.compile(model_forward, mode=\"reduce-overhead\", fullgraph=True)\n+\n+        i = 0\n         while self._has_unfinished_sequences(\n             this_peer_finished, synced_gpus, device=input_ids.device, cur_len=cur_len, max_length=max_length\n         ):\n@@ -3234,8 +3245,11 @@ def _sample(\n             model_inputs.update({\"output_attentions\": output_attentions} if output_attentions else {})\n             model_inputs.update({\"output_hidden_states\": output_hidden_states} if output_hidden_states else {})\n \n-            # forward pass to get next token\n-            outputs = self(**model_inputs, return_dict=True)\n+            if i == 0:\n+                outputs = self(**model_inputs, return_dict=True)\n+                i += 1\n+            else:\n+                outputs = model_forward(self, return_dict=True, **model_inputs)\n \n             # synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\n             model_kwargs = self._update_model_kwargs_for_generation("
        }
    ],
    "stats": {
        "total": 18,
        "additions": 16,
        "deletions": 2
    }
}