{
    "author": "gante",
    "message": "[tests] `CausalLMTester` automatically infers other test classes from `base_model_class` üêõ üî´  (#41066)\n\n* halfway through the models\n\n* update test checks\n\n* refactor all\n\n* another one\n\n* use tuples\n\n* more deletions\n\n* solve bad inheritance patterns\n\n* type\n\n* PR ready?\n\n* automatic model class inference from the base class\n\n* vaultgemma\n\n* make fixup\n\n* make fixup\n\n* rebase with gpt2\n\n* make fixup :'(\n\n* gpt2 is special",
    "sha": "4fade1148f86418dadd5a4a21b57af906e0bd5be",
    "files": [
        {
            "sha": "8e420bf27904f9228cac81c4233096450f85219a",
            "filename": "src/transformers/models/gemma3n/configuration_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -291,9 +291,7 @@ def __init__(\n \n         if activation_sparsity_pattern is None:\n             num_sparse_layers = 10 if num_hidden_layers > 10 else 0\n-            activation_sparsity_pattern = (0.95,) * num_sparse_layers + (0.0,) * (\n-                num_hidden_layers - num_sparse_layers\n-            )\n+            activation_sparsity_pattern = [0.95] * num_sparse_layers + [0.0] * (num_hidden_layers - num_sparse_layers)\n \n         if (len_asp := len(activation_sparsity_pattern)) != num_hidden_layers:\n             raise ValueError("
        },
        {
            "sha": "0d83f0f292fd872838c6220cded2ce36c2031cef",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -304,9 +304,7 @@ def __init__(\n \n         if activation_sparsity_pattern is None:\n             num_sparse_layers = 10 if num_hidden_layers > 10 else 0\n-            activation_sparsity_pattern = (0.95,) * num_sparse_layers + (0.0,) * (\n-                num_hidden_layers - num_sparse_layers\n-            )\n+            activation_sparsity_pattern = [0.95] * num_sparse_layers + [0.0] * (num_hidden_layers - num_sparse_layers)\n \n         if (len_asp := len(activation_sparsity_pattern)) != num_hidden_layers:\n             raise ValueError("
        },
        {
            "sha": "28db55ac1f34a4488cf2b9583f00ad2af8c15b05",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -204,6 +204,17 @@\n # Not critical, only usable on the sandboxed CI instance.\n TOKEN = \"hf_94wBhPGp6KrrTH3KDchhKpRxZwd6dmHWLL\"\n \n+\n+# Used in CausalLMModelTester (and related classes/methods) to infer the common model classes from the base model class\n+_COMMON_MODEL_NAMES_MAP = {\n+    \"config_class\": \"Config\",\n+    \"causal_lm_class\": \"ForCausalLM\",\n+    \"question_answering_class\": \"ForQuestionAnswering\",\n+    \"sequence_classification_class\": \"ForSequenceClassification\",\n+    \"token_classification_class\": \"ForTokenClassification\",\n+}\n+\n+\n if is_torch_available():\n     import torch\n "
        },
        {
            "sha": "dc57c708829cb50ceeb8dec3e75f58c4aaab397f",
            "filename": "tests/causal_lm_tester.py",
            "status": "modified",
            "additions": 82,
            "deletions": 21,
            "changes": 103,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fcausal_lm_tester.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fcausal_lm_tester.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcausal_lm_tester.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -19,7 +19,9 @@\n from parameterized import parameterized\n \n from transformers import AutoModelForCausalLM, PretrainedConfig, set_seed\n+from transformers.models.auto.auto_factory import getattribute_from_module\n from transformers.testing_utils import (\n+    _COMMON_MODEL_NAMES_MAP,\n     is_flaky,\n     require_flash_attn,\n     require_torch_gpu,\n@@ -43,31 +45,99 @@\n \n \n class CausalLMModelTester:\n-    _required_attributes = (\"base_model_class\", \"config_class\", \"causal_lm_class\")\n-    forced_config_args = [\n-        \"pad_token_id\"\n-    ]  # Arguments that should be passed to the config class even if not in its signature\n-    config_class = None\n+    # If the model follows the standard naming conventions, only `base_model_class` needs to be set (the others are\n+    # inferred from available public classes).\n     base_model_class = None\n+    # ‚ö†Ô∏è Don't set these unless the model does NOT follow the standard naming conventions ‚ö†Ô∏è\n+    config_class = None\n     causal_lm_class = None\n+    question_answering_class = None\n     sequence_classification_class = None\n     token_classification_class = None\n-    question_answering_class = None\n+    # These attributes are required after the initialization phase of the tester.\n+    _required_attributes = (\"base_model_class\", \"config_class\", \"causal_lm_class\")\n+\n+    # Arguments that should be passed to the config class even if not in its signature\n+    forced_config_args = [\"pad_token_id\"]\n+\n+    @classmethod\n+    def _verify_and_infer_model_attributes(cls):\n+        \"\"\"\n+        Verifies that the required tester attributes are set correctly, and infers unset tester attributes.\n+        Intentionally nitpicks the tester class attributes, to prevent human errors.\n+        \"\"\"\n+        # `base_model_class` is mandatory, and it must be a valid model class.\n+        base_model_class = getattr(cls, \"base_model_class\")\n+        if base_model_class is None or \"PreTrainedModel\" not in str(base_model_class.__mro__):\n+            raise ValueError(\n+                f\"You have inherited from `CausalLMModelTester` but did not set the `base_model_class` \"\n+                f\"attribute to a valid model class. (It's set to `{base_model_class}`)\"\n+            )\n \n-    def _verify_model_attributes(self):\n-        for required_attribute in self._required_attributes:\n-            if getattr(self, required_attribute) is None:\n+        # Infers other model classes from the base class name and available public classes, if the corresponding\n+        # attributes are not set explicitly. If they are set, they must be set to a valid class (config or model).\n+        model_name = base_model_class.__name__.replace(\"Model\", \"\")\n+        base_class_module = \".\".join(base_model_class.__module__.split(\".\")[:-1])\n+        for tester_attribute_name, model_class_termination in _COMMON_MODEL_NAMES_MAP.items():\n+            if getattr(cls, tester_attribute_name) is None:\n+                try:\n+                    model_class = getattribute_from_module(base_class_module, model_name + model_class_termination)\n+                    setattr(cls, tester_attribute_name, model_class)\n+                except ValueError:\n+                    pass\n+            else:\n+                if tester_attribute_name == \"config_class\":\n+                    if \"PretrainedConfig\" not in str(getattr(cls, tester_attribute_name).__mro__):\n+                        raise ValueError(\n+                            f\"You have inherited from `CausalLMModelTester` but did not set the \"\n+                            f\"`{tester_attribute_name}` attribute to a valid config class. (It's set to \"\n+                            f\"`{getattr(cls, tester_attribute_name)}`). If the config class follows a standard \"\n+                            f\"naming convention, you should unset `{tester_attribute_name}`.\"\n+                        )\n+                else:\n+                    if \"PreTrainedModel\" not in str(getattr(cls, tester_attribute_name).__mro__):\n+                        raise ValueError(\n+                            f\"You have inherited from `CausalLMModelTester` but did not set the \"\n+                            f\"`{tester_attribute_name}` attribute to a valid model class. (It's set to \"\n+                            f\"`{getattr(cls, tester_attribute_name)}`). If the model class follows a standard \"\n+                            f\"naming convention, you should unset `{tester_attribute_name}`.\"\n+                        )\n+\n+        # After inferring, if we don't have the basic classes set, we raise an error.\n+        for required_attribute in cls._required_attributes:\n+            if getattr(cls, required_attribute) is None:\n+                raise ValueError(\n+                    f\"You have inherited from `CausalLMModelTester` but did not set the `{required_attribute}` \"\n+                    \"attribute. It can't be automatically inferred either -- this means it is not following a \"\n+                    \"standard naming convention. If this is intentional, please set the attribute explicitly.\"\n+                )\n+\n+        # To prevent issues with typos, no other attributes can be set to a model class\n+        for instance_attribute_name, instance_attribute in cls.__dict__.items():\n+            if (\n+                (\n+                    instance_attribute_name not in _COMMON_MODEL_NAMES_MAP\n+                    and instance_attribute_name != \"base_model_class\"\n+                )\n+                and isinstance(instance_attribute, type)\n+                and \"PreTrainedModel\" in str(instance_attribute.__mro__)\n+            ):\n                 raise ValueError(\n-                    f\"You have inherited from CausalLMModelTester but did not set the {required_attribute} attribute.\"\n+                    f\"You have inherited from `CausalLMModelTester` but set an unexpected attribute to a model class \"\n+                    f\"(`{instance_attribute_name}` is set to `{instance_attribute}`). \"\n+                    f\"Only the following attributes can be set to model classes: {_COMMON_MODEL_NAMES_MAP.keys()}.\"\n                 )\n \n     @property\n     def all_model_classes(self):\n+        # Models that set `all_model_classes` in their `XXXModelTest` class must have a new class that doesn't fit\n+        # any of the common classes.\n         return [\n             model_class\n             for model_class in (\n                 self.base_model_class,\n                 self.causal_lm_class,\n+                self.question_answering_class,\n                 self.sequence_classification_class,\n                 self.token_classification_class,\n             )\n@@ -118,7 +188,7 @@ def __init__(\n         mamba_expand=2,\n         mamba_chunk_size=16,\n     ):\n-        self._verify_model_attributes()\n+        self._verify_and_infer_model_attributes()\n         self.parent = parent\n         self.batch_size = batch_size\n         self.seq_length = seq_length\n@@ -210,16 +280,7 @@ def create_and_check_model(\n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n \n     def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = config_and_inputs\n+        config, input_ids, _, input_mask, _, _, _ = self.prepare_config_and_inputs()\n         inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n         return config, inputs_dict\n "
        },
        {
            "sha": "30e7fdbf21f513d77184101fd4c52c3e4243d3cf",
            "filename": "tests/models/apertus/test_modeling_apertus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fapertus%2Ftest_modeling_apertus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fapertus%2Ftest_modeling_apertus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fapertus%2Ftest_modeling_apertus.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -33,7 +33,6 @@\n \n if is_torch_available():\n     from transformers import (\n-        ApertusConfig,\n         ApertusForCausalLM,\n         ApertusForTokenClassification,\n         ApertusModel,\n@@ -42,23 +41,11 @@\n \n class ApertusModelTester(CausalLMModelTester):\n     if is_torch_available():\n-        config_class = ApertusConfig\n         base_model_class = ApertusModel\n-        causal_lm_class = ApertusForCausalLM\n-        token_class = ApertusForTokenClassification\n \n \n @require_torch\n class ApertusModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            ApertusModel,\n-            ApertusForCausalLM,\n-            ApertusForTokenClassification,\n-        )\n-        if is_torch_available()\n-        else ()\n-    )\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": ApertusModel,\n@@ -68,8 +55,6 @@ class ApertusModelTest(CausalLMModelTest, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    test_headmasking = False\n-    test_pruning = False\n     model_tester_class = ApertusModelTester\n \n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`"
        },
        {
            "sha": "a8b485fd7eb2357d1de0aa5a20591a2f1459d2e2",
            "filename": "tests/models/arcee/test_modeling_arcee.py",
            "status": "modified",
            "additions": 0,
            "deletions": 17,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Farcee%2Ftest_modeling_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Farcee%2Ftest_modeling_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Farcee%2Ftest_modeling_arcee.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -43,26 +43,11 @@\n \n class ArceeModelTester(CausalLMModelTester):\n     if is_torch_available():\n-        config_class = ArceeConfig\n         base_model_class = ArceeModel\n-        causal_lm_class = ArceeForCausalLM\n-        sequence_class = ArceeForSequenceClassification\n-        token_class = ArceeForTokenClassification\n \n \n @require_torch\n class ArceeModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            ArceeModel,\n-            ArceeForCausalLM,\n-            ArceeForSequenceClassification,\n-            ArceeForQuestionAnswering,\n-            ArceeForTokenClassification,\n-        )\n-        if is_torch_available()\n-        else ()\n-    )\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": ArceeModel,\n@@ -75,8 +60,6 @@ class ArceeModelTest(CausalLMModelTest, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    test_headmasking = False\n-    test_pruning = False\n     fx_compatible = False\n     model_tester_class = ArceeModelTester\n "
        },
        {
            "sha": "34aab8f179c9e57395b7df9a0cc1d63381cec27d",
            "filename": "tests/models/blt/test_modeling_blt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fblt%2Ftest_modeling_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fblt%2Ftest_modeling_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblt%2Ftest_modeling_blt.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -46,9 +46,7 @@\n \n class BltModelTester(CausalLMModelTester):\n     if is_torch_available():\n-        config_class = BltConfig\n         base_model_class = BltModel\n-        causal_lm_class = BltForCausalLM\n \n     def __init__(\n         self,"
        },
        {
            "sha": "4dbe5d403dc5c48bbfc94ac21fbb418060dead5c",
            "filename": "tests/models/dbrx/test_modeling_dbrx.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -17,7 +17,7 @@\n \n from parameterized import parameterized\n \n-from transformers import DbrxConfig, is_torch_available\n+from transformers import is_torch_available\n from transformers.testing_utils import require_torch, slow\n \n from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n@@ -30,10 +30,8 @@\n \n \n class DbrxModelTester(CausalLMModelTester):\n-    config_class = DbrxConfig\n     if is_torch_available():\n         base_model_class = DbrxModel\n-        causal_lm_class = DbrxForCausalLM\n \n     def __init__(\n         self,"
        },
        {
            "sha": "dc9886c70a6eb7d34d1d4dbbabb794d147fca337",
            "filename": "tests/models/deepseek_v2/test_modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 15,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fdeepseek_v2%2Ftest_modeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fdeepseek_v2%2Ftest_modeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v2%2Ftest_modeling_deepseek_v2.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -18,7 +18,7 @@\n \n import pytest\n \n-from transformers import BitsAndBytesConfig, Cache, DeepseekV2Config, is_torch_available\n+from transformers import BitsAndBytesConfig, Cache, is_torch_available\n from transformers.testing_utils import require_read_token, require_torch, require_torch_accelerator, slow, torch_device\n \n from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n@@ -33,10 +33,7 @@\n \n class DeepseekV2ModelTester(CausalLMModelTester):\n     if is_torch_available():\n-        config_class = DeepseekV2Config\n         base_model_class = DeepseekV2Model\n-        causal_lm_class = DeepseekV2ForCausalLM\n-        sequence_class = DeepseekV2ForSequenceClassification\n \n     def __init__(\n         self,\n@@ -57,15 +54,6 @@ def __init__(\n \n @require_torch\n class DeepseekV2ModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            DeepseekV2ForCausalLM,\n-            DeepseekV2ForSequenceClassification,\n-            DeepseekV2Model,\n-        )\n-        if is_torch_available()\n-        else ()\n-    )\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": DeepseekV2Model,\n@@ -76,8 +64,6 @@ class DeepseekV2ModelTest(CausalLMModelTest, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    test_headmasking = False\n-    test_pruning = False\n     fx_compatible = False\n     test_torchscript = False\n     test_all_params_have_gradient = False"
        },
        {
            "sha": "65cb64ee24fffaa7a88b7a99238e7d807455c174",
            "filename": "tests/models/dots1/test_modeling_dots1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fdots1%2Ftest_modeling_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fdots1%2Ftest_modeling_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdots1%2Ftest_modeling_dots1.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -16,7 +16,7 @@\n import gc\n import unittest\n \n-from transformers import AutoTokenizer, Dots1Config, is_torch_available\n+from transformers import AutoTokenizer, is_torch_available\n from transformers.testing_utils import (\n     backend_empty_cache,\n     cleanup,\n@@ -39,10 +39,8 @@\n \n \n class Dots1ModelTester(CausalLMModelTester):\n-    config_class = Dots1Config\n     if is_torch_available():\n         base_model_class = Dots1Model\n-        causal_lm_class = Dots1ForCausalLM\n \n     def __init__(\n         self,"
        },
        {
            "sha": "1fb5969e900dce5544bfb126f202d465a041899c",
            "filename": "tests/models/ernie4_5/test_modeling_ernie4_5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fernie4_5%2Ftest_modeling_ernie4_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fernie4_5%2Ftest_modeling_ernie4_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fernie4_5%2Ftest_modeling_ernie4_5.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -33,29 +33,18 @@\n \n     from transformers import (\n         AutoTokenizer,\n-        Ernie4_5Config,\n         Ernie4_5ForCausalLM,\n         Ernie4_5Model,\n     )\n \n \n class Ernie4_5ModelTester(CausalLMModelTester):\n     if is_torch_available():\n-        config_class = Ernie4_5Config\n         base_model_class = Ernie4_5Model\n-        causal_lm_class = Ernie4_5ForCausalLM\n \n \n @require_torch\n class Ernie4_5ModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            Ernie4_5Model,\n-            Ernie4_5ForCausalLM,\n-        )\n-        if is_torch_available()\n-        else ()\n-    )\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": Ernie4_5Model,\n@@ -64,8 +53,6 @@ class Ernie4_5ModelTest(CausalLMModelTest, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    test_headmasking = False\n-    test_pruning = False\n     fx_compatible = False  # Broken by attention refactor cc @Cyrilvallez\n     model_tester_class = Ernie4_5ModelTester\n "
        },
        {
            "sha": "59839c0466c19ed93cd939eb4c7aeabdaf4e5032",
            "filename": "tests/models/ernie4_5_moe/test_modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 13,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fernie4_5_moe%2Ftest_modeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fernie4_5_moe%2Ftest_modeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fernie4_5_moe%2Ftest_modeling_ernie4_5_moe.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -18,7 +18,7 @@\n \n import pytest\n \n-from transformers import Ernie4_5_MoeConfig, is_torch_available\n+from transformers import is_torch_available\n from transformers.testing_utils import (\n     cleanup,\n     is_flaky,\n@@ -46,22 +46,12 @@\n \n \n class Ernie4_5_MoeModelTester(CausalLMModelTester):\n-    config_class = Ernie4_5_MoeConfig\n     if is_torch_available():\n         base_model_class = Ernie4_5_MoeModel\n-        causal_lm_class = Ernie4_5_MoeForCausalLM\n \n \n @require_torch\n class Ernie4_5_MoeModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            Ernie4_5_MoeModel,\n-            Ernie4_5_MoeForCausalLM,\n-        )\n-        if is_torch_available()\n-        else ()\n-    )\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": Ernie4_5_MoeModel,\n@@ -71,8 +61,6 @@ class Ernie4_5_MoeModelTest(CausalLMModelTest, unittest.TestCase):\n         else {}\n     )\n \n-    test_headmasking = False\n-    test_pruning = False\n     test_all_params_have_gradient = False\n     model_tester_class = Ernie4_5_MoeModelTester\n "
        },
        {
            "sha": "64760fbc2b09f2e861253dbf9ac4cb4e4e867198",
            "filename": "tests/models/exaone4/test_modeling_exaone4.py",
            "status": "modified",
            "additions": 0,
            "deletions": 24,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fexaone4%2Ftest_modeling_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fexaone4%2Ftest_modeling_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fexaone4%2Ftest_modeling_exaone4.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -21,7 +21,6 @@\n \n from transformers import (\n     AutoTokenizer,\n-    Exaone4Config,\n     GenerationConfig,\n     is_torch_available,\n )\n@@ -35,7 +34,6 @@\n )\n \n from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n-from ...test_configuration_common import ConfigTester\n \n \n if is_torch_available():\n@@ -51,28 +49,12 @@\n \n \n class Exaone4ModelTester(CausalLMModelTester):\n-    config_class = Exaone4Config\n     if is_torch_available():\n         base_model_class = Exaone4Model\n-        causal_lm_class = Exaone4ForCausalLM\n-        sequence_class = Exaone4ForSequenceClassification\n-        token_class = Exaone4ForTokenClassification\n-        question_answering_class = Exaone4ForQuestionAnswering\n \n \n @require_torch\n class Exaone4ModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            Exaone4Model,\n-            Exaone4ForCausalLM,\n-            Exaone4ForSequenceClassification,\n-            Exaone4ForQuestionAnswering,\n-            Exaone4ForTokenClassification,\n-        )\n-        if is_torch_available()\n-        else ()\n-    )\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": Exaone4Model,\n@@ -85,16 +67,10 @@ class Exaone4ModelTest(CausalLMModelTest, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    test_headmasking = False\n-    test_pruning = False\n     fx_compatible = False  # Broken by attention refactor cc @Cyrilvallez\n     model_tester_class = Exaone4ModelTester\n     model_split_percents = [0.5, 0.6]\n \n-    def setUp(self):\n-        self.model_tester = Exaone4ModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=Exaone4Config, hidden_size=37)\n-\n \n @require_torch\n class Exaone4IntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "cf025f463516dac6ccebdcdbbfc4358537660e1e",
            "filename": "tests/models/falcon/test_modeling_falcon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -36,7 +36,6 @@\n \n     from transformers import (\n         FalconForCausalLM,\n-        FalconForQuestionAnswering,\n         FalconForSequenceClassification,\n         FalconForTokenClassification,\n         FalconModel,\n@@ -45,11 +44,7 @@\n \n class FalconModelTester(CausalLMModelTester):\n     if is_torch_available():\n-        config_class = FalconConfig\n         base_model_class = FalconModel\n-        causal_lm_class = FalconForCausalLM\n-        sequence_class = FalconForSequenceClassification\n-        token_class = FalconForTokenClassification\n \n     def __init__(self, parent, new_decoder_architecture=True):\n         super().__init__(parent)\n@@ -59,17 +54,6 @@ def __init__(self, parent, new_decoder_architecture=True):\n @require_torch\n class FalconModelTest(CausalLMModelTest, unittest.TestCase):\n     model_tester_class = FalconModelTester\n-    all_model_classes = (\n-        (\n-            FalconModel,\n-            FalconForCausalLM,\n-            FalconForSequenceClassification,\n-            FalconForTokenClassification,\n-            FalconForQuestionAnswering,\n-        )\n-        if is_torch_available()\n-        else ()\n-    )\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": FalconModel,\n@@ -81,8 +65,6 @@ class FalconModelTest(CausalLMModelTest, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    test_headmasking = False\n-    test_pruning = False\n \n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79245/workflows/9490ef58-79c2-410d-8f51-e3495156cf9c/jobs/1012146\n     def is_pipeline_test_to_skip("
        },
        {
            "sha": "15e4bb57c4af716701148e2364a2b0a4b76b4763",
            "filename": "tests/models/flex_olmo/test_modeling_flex_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fflex_olmo%2Ftest_modeling_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fflex_olmo%2Ftest_modeling_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fflex_olmo%2Ftest_modeling_flex_olmo.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -18,7 +18,7 @@\n \n import pytest\n \n-from transformers import FlexOlmoConfig, is_torch_available\n+from transformers import is_torch_available\n from transformers.models.auto.tokenization_auto import AutoTokenizer\n from transformers.testing_utils import (\n     Expectations,\n@@ -43,9 +43,7 @@\n \n class FlexOlmoModelTester(CausalLMModelTester):\n     if is_torch_available():\n-        config_class = FlexOlmoConfig\n         base_model_class = FlexOlmoModel\n-        causal_lm_class = FlexOlmoForCausalLM\n \n \n @require_torch\n@@ -59,8 +57,6 @@ class FlexOlmoModelTest(CausalLMModelTest, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    test_headmasking = False\n-    test_pruning = False\n     fx_compatible = False\n     test_torchscript = False\n     test_all_params_have_gradient = False"
        },
        {
            "sha": "0557b89459b916c29b54d6dcfbc000fa8571df63",
            "filename": "tests/models/gemma/test_modeling_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 10,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -18,7 +18,7 @@\n import pytest\n from packaging import version\n \n-from transformers import AutoModelForCausalLM, AutoTokenizer, GemmaConfig, is_torch_available\n+from transformers import AutoModelForCausalLM, AutoTokenizer, is_torch_available\n from transformers.generation.configuration_utils import GenerationConfig\n from transformers.testing_utils import (\n     DeviceProperties,\n@@ -50,21 +50,12 @@\n \n @require_torch\n class GemmaModelTester(CausalLMModelTester):\n-    config_class = GemmaConfig\n     if is_torch_available():\n         base_model_class = GemmaModel\n-        causal_lm_class = GemmaForCausalLM\n-        sequence_classification_class = GemmaForSequenceClassification\n-        token_classification_class = GemmaForTokenClassification\n \n \n @require_torch\n class GemmaModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (\n-        (GemmaModel, GemmaForCausalLM, GemmaForSequenceClassification, GemmaForTokenClassification)\n-        if is_torch_available()\n-        else ()\n-    )\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": GemmaModel,"
        },
        {
            "sha": "c4479d900a898788d93f38d7a541e0ee999f57b3",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -20,7 +20,7 @@\n from parameterized import parameterized\n from pytest import mark\n \n-from transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache, Gemma2Config, is_torch_available, pipeline\n+from transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache, is_torch_available, pipeline\n from transformers.cache_utils import DynamicLayer, DynamicSlidingWindowLayer\n from transformers.generation.configuration_utils import GenerationConfig\n from transformers.testing_utils import (\n@@ -39,7 +39,6 @@\n )\n \n from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n-from ...test_configuration_common import ConfigTester\n \n \n if is_torch_available():\n@@ -55,31 +54,11 @@\n \n class Gemma2ModelTester(CausalLMModelTester):\n     if is_torch_available():\n-        config_class = Gemma2Config\n         base_model_class = Gemma2Model\n-        causal_lm_class = Gemma2ForCausalLM\n-        sequence_class = Gemma2ForSequenceClassification\n-        token_class = Gemma2ForTokenClassification\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": Gemma2Model,\n-            \"text-classification\": Gemma2ForSequenceClassification,\n-            \"token-classification\": Gemma2ForTokenClassification,\n-            \"text-generation\": Gemma2ForCausalLM,\n-            \"zero-shot\": Gemma2ForSequenceClassification,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n \n \n @require_torch\n class Gemma2ModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (\n-        (Gemma2Model, Gemma2ForCausalLM, Gemma2ForSequenceClassification, Gemma2ForTokenClassification)\n-        if is_torch_available()\n-        else ()\n-    )\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": Gemma2Model,\n@@ -92,16 +71,10 @@ class Gemma2ModelTest(CausalLMModelTest, unittest.TestCase):\n         else {}\n     )\n \n-    test_headmasking = False\n-    test_pruning = False\n     _is_stateful = True\n     model_split_percents = [0.5, 0.6]\n     model_tester_class = Gemma2ModelTester\n \n-    def setUp(self):\n-        self.model_tester = Gemma2ModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=Gemma2Config, hidden_size=37)\n-\n \n @slow\n @require_torch_accelerator"
        },
        {
            "sha": "3745629ecd5e18acb0d7f3db77c6811e28ccda0c",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 20,
            "deletions": 30,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -41,8 +41,8 @@\n     torch_device,\n )\n \n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n from ...generation.test_utils import GenerationTesterMixin\n-from ...models.gemma.test_modeling_gemma import GemmaModelTester\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n \n@@ -62,28 +62,28 @@\n     from transformers.pytorch_utils import is_torch_greater_or_equal\n \n \n-class Gemma3ModelTester(GemmaModelTester):\n+class Gemma3TextModelTester(CausalLMModelTester):\n     if is_torch_available():\n-        config_class = Gemma3TextConfig\n-        model_class = Gemma3TextModel\n-        for_causal_lm_class = Gemma3ForCausalLM\n+        base_model_class = Gemma3TextModel\n+        causal_lm_class = Gemma3ForCausalLM\n+        sequence_classification_class = Gemma3TextForSequenceClassification\n \n \n @require_torch\n-class Gemma3ModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n-    all_model_classes = (\n-        (Gemma3TextModel, Gemma3ForCausalLM, Gemma3TextForSequenceClassification) if is_torch_available() else ()\n+class Gemma3TextModelTest(CausalLMModelTest, unittest.TestCase):\n+    model_tester_class = Gemma3TextModelTester\n+    pipeline_model_mapping = (\n+        {\n+            \"feature-extraction\": Gemma3TextModel,\n+            \"text-classification\": Gemma3TextForSequenceClassification,\n+            \"text-generation\": Gemma3ForCausalLM,\n+        }\n+        if is_torch_available()\n+        else {}\n     )\n-    all_generative_model_classes = (Gemma3ForCausalLM,) if is_torch_available() else ()\n-    test_headmasking = False\n-    test_pruning = False\n     _is_stateful = True\n     model_split_percents = [0.5, 0.6]\n \n-    def setUp(self):\n-        self.model_tester = Gemma3ModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=Gemma3Config, hidden_size=37)\n-\n     @unittest.skip(\"Gemma3 applies key/query norm which doesn't work with packing\")\n     def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n         pass\n@@ -152,20 +152,10 @@ def test_generation_beyond_sliding_window_tiny_model(self):\n         EXPECTED_OUTPUT = torch.tensor([[90109, 90109, 90109, 83191, 83191], [246901, 69832, 69832, 69832, 62288]])\n         torch.testing.assert_close(generated_sequences, EXPECTED_OUTPUT)\n \n-    def test_gemma3_text_sequence_classification_model(self):\n-        \"\"\"Test the text-only sequence classification model.\"\"\"\n-        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.num_labels = 3\n-        input_ids = input_dict[\"input_ids\"]\n-        attention_mask = input_ids.ne(1).to(torch_device)\n-        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.num_labels)\n-\n-        model = Gemma3TextForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n-        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, config.num_labels))\n+    @parameterized.expand([(\"linear\",), (\"dynamic\",), (\"yarn\",)])\n+    @unittest.skip(\"TODO (joao): check why this is failing\")\n+    def test_model_rope_scaling_from_config(self):\n+        pass\n \n \n class Gemma3Vision2TextModelTester:\n@@ -201,7 +191,7 @@ def __init__(\n         self.image_token_index = image_token_index\n         self.boi_token_index = boi_token_index\n         self.eoi_token_index = eoi_token_index\n-        self.llm_tester = Gemma3ModelTester(self.parent)\n+        self.llm_tester = Gemma3TextModelTester(self.parent)\n         self.text_config = self.llm_tester.get_config()\n         self.vision_config = vision_config\n         self.seq_length = seq_length"
        },
        {
            "sha": "f94347e362e87085e9a0014352354efff4950613",
            "filename": "tests/models/gemma3n/test_modeling_gemma3n.py",
            "status": "modified",
            "additions": 16,
            "deletions": 25,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -31,7 +31,6 @@\n     Gemma3nAudioConfig,\n     Gemma3nAudioFeatureExtractor,\n     Gemma3nConfig,\n-    Gemma3nTextConfig,\n     GenerationConfig,\n     StaticCache,\n     is_torch_available,\n@@ -50,6 +49,7 @@\n )\n from transformers.utils import is_flash_attn_2_available\n \n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n from ...generation.test_utils import GenerationTesterMixin, has_similar_generate_outputs\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import (\n@@ -59,7 +59,6 @@\n     floats_tensor,\n     ids_tensor,\n )\n-from ..gemma.test_modeling_gemma import GemmaModelTester\n \n \n if is_torch_available():\n@@ -248,9 +247,10 @@ def test_audio_encoder(self):\n         torch.testing.assert_close(encoder_mask[1, :], self.expected_encoder_mask_slice.to(torch_device))\n \n \n-class Gemma3nTextModelTester(GemmaModelTester):\n-    activation_sparsity_pattern = None\n-    forced_config_args = [\"activation_sparsity_pattern\"]\n+class Gemma3nTextModelTester(CausalLMModelTester):\n+    if is_torch_available():\n+        base_model_class = Gemma3nTextModel\n+        causal_lm_class = Gemma3nForCausalLM\n \n     def __init__(\n         self,\n@@ -289,7 +289,7 @@ def __init__(\n         eos_token_id=2,\n         is_decoder=False,\n     ):\n-        self._verify_model_attributes()\n+        self._verify_and_infer_model_attributes()\n         self.parent = parent\n         self.batch_size = batch_size\n         self.seq_length = seq_length\n@@ -321,30 +321,21 @@ def __init__(\n         self.head_dim = self.hidden_size // self.num_attention_heads\n         self.is_decoder = is_decoder\n \n-    if is_torch_available():\n-        config_class = Gemma3nTextConfig\n-        model_class = Gemma3nTextModel\n-        for_causal_lm_class = Gemma3nForCausalLM\n-\n \n @require_torch\n-class Gemma3nTextModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n-    all_model_classes = (Gemma3nTextModel, Gemma3nForCausalLM) if is_torch_available() else ()\n-    all_generative_model_classes = (Gemma3nForCausalLM,) if is_torch_available() else ()\n-    test_headmasking = False\n-    test_pruning = False\n+class Gemma3nTextModelTest(CausalLMModelTest, unittest.TestCase):\n+    model_tester_class = Gemma3nTextModelTester\n+    pipeline_model_mapping = (\n+        {\n+            \"feature-extraction\": Gemma3nTextModel,\n+            \"text-generation\": Gemma3nForCausalLM,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n     _is_stateful = True\n     model_split_percents = [0.5, 0.6]\n \n-    def setUp(self):\n-        self.model_tester = Gemma3nTextModelTester(self)\n-        self.config_tester = ConfigTester(\n-            self,\n-            config_class=Gemma3nConfig,\n-            hidden_size=37,\n-            text_config={\"activation_sparsity_pattern\": None},\n-        )\n-\n     def _check_hidden_states_for_generate(\n         self, batch_size, hidden_states, prompt_length, output_length, config, use_cache=False\n     ):"
        },
        {
            "sha": "08609cd90ca6d0f25c9b7b578eefa21ab100a7e1",
            "filename": "tests/models/glm/test_modeling_glm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 12,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -17,7 +17,7 @@\n \n import pytest\n \n-from transformers import AutoModelForCausalLM, AutoTokenizer, GlmConfig, is_torch_available\n+from transformers import AutoModelForCausalLM, AutoTokenizer, is_torch_available\n from transformers.testing_utils import (\n     Expectations,\n     require_flash_attn,\n@@ -43,21 +43,12 @@\n \n @require_torch\n class GlmModelTester(CausalLMModelTester):\n-    config_class = GlmConfig\n     if is_torch_available():\n         base_model_class = GlmModel\n-        causal_lm_class = GlmForCausalLM\n-        sequence_class = GlmForSequenceClassification\n-        token_class = GlmForTokenClassification\n \n \n @require_torch\n class GlmModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (\n-        (GlmModel, GlmForCausalLM, GlmForSequenceClassification, GlmForTokenClassification)\n-        if is_torch_available()\n-        else ()\n-    )\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": GlmModel,\n@@ -69,8 +60,6 @@ class GlmModelTest(CausalLMModelTest, unittest.TestCase):\n         else {}\n     )\n \n-    test_headmasking = False\n-    test_pruning = False\n     model_tester_class = GlmModelTester\n \n "
        },
        {
            "sha": "b810bf6a6066eb1bd06dd3afc810405ba85a90a2",
            "filename": "tests/models/glm4/test_modeling_glm4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 12,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fglm4%2Ftest_modeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fglm4%2Ftest_modeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4%2Ftest_modeling_glm4.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -18,7 +18,7 @@\n \n import pytest\n \n-from transformers import AutoModelForCausalLM, AutoTokenizer, Glm4Config, is_torch_available\n+from transformers import AutoModelForCausalLM, AutoTokenizer, is_torch_available\n from transformers.testing_utils import (\n     Expectations,\n     cleanup,\n@@ -46,21 +46,12 @@\n \n class Glm4ModelTester(CausalLMModelTester):\n     if is_torch_available():\n-        config_class = Glm4Config\n         base_model_class = Glm4Model\n-        causal_lm_class = Glm4ForCausalLM\n-        sequence_classification_class = Glm4ForSequenceClassification\n-        token_classification_class = Glm4ForTokenClassification\n \n \n @require_torch\n class Glm4ModelTest(CausalLMModelTest, unittest.TestCase):\n     model_tester_class = Glm4ModelTester\n-    all_model_classes = (\n-        (Glm4Model, Glm4ForCausalLM, Glm4ForSequenceClassification, Glm4ForTokenClassification)\n-        if is_torch_available()\n-        else ()\n-    )\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": Glm4Model,\n@@ -72,8 +63,6 @@ class Glm4ModelTest(CausalLMModelTest, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    test_headmasking = False\n-    test_pruning = False\n     _is_stateful = True\n     model_split_percents = [0.5, 0.6]\n "
        },
        {
            "sha": "5ddf7a90ed0a729181b2f0570a3ce90cfb20c5be",
            "filename": "tests/models/glm4_moe/test_modeling_glm4_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 13,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fglm4_moe%2Ftest_modeling_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fglm4_moe%2Ftest_modeling_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4_moe%2Ftest_modeling_glm4_moe.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -33,14 +33,12 @@\n \n \n if is_torch_available():\n-    from transformers import AutoTokenizer, Glm4MoeConfig, Glm4MoeForCausalLM, Glm4MoeModel\n+    from transformers import AutoTokenizer, Glm4MoeForCausalLM, Glm4MoeModel\n \n \n class Glm4MoeModelTester(CausalLMModelTester):\n     if is_torch_available():\n-        config_class = Glm4MoeConfig\n         base_model_class = Glm4MoeModel\n-        causal_lm_class = Glm4MoeForCausalLM\n \n     def __init__(\n         self,\n@@ -60,14 +58,6 @@ def __init__(\n \n @require_torch\n class Glm4MoeModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            Glm4MoeModel,\n-            Glm4MoeForCausalLM,\n-        )\n-        if is_torch_available()\n-        else ()\n-    )\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": Glm4MoeModel,\n@@ -76,8 +66,6 @@ class Glm4MoeModelTest(CausalLMModelTest, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    test_headmasking = False\n-    test_pruning = False\n     fx_compatible = False\n     model_tester_class = Glm4MoeModelTester\n     # used in `test_torch_compile_for_training`. Skip as \"Dynamic control flow in MoE\""
        },
        {
            "sha": "4065e7179f5b9935deaf2d37f9f9cf626a4d3bea",
            "filename": "tests/models/gpt2/test_modeling_gpt2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -16,7 +16,7 @@\n \n import pytest\n \n-from transformers import GPT2Config, is_torch_available\n+from transformers import is_torch_available\n from transformers.testing_utils import (\n     Expectations,\n     cleanup,\n@@ -47,12 +47,8 @@\n \n class GPT2ModelTester(CausalLMModelTester):\n     if is_torch_available():\n-        config_class = GPT2Config\n         base_model_class = GPT2Model\n         causal_lm_class = GPT2LMHeadModel\n-        sequence_classification_class = GPT2ForSequenceClassification\n-        token_classification_class = GPT2ForTokenClassification\n-        question_answering_class = GPT2ForQuestionAnswering\n \n     def __init__(\n         self,\n@@ -151,6 +147,7 @@ def prepare_config_and_inputs_for_decoder(self):\n \n @require_torch\n class GPT2ModelTest(CausalLMModelTest, unittest.TestCase):\n+    # `all_model_classes` is overwritten because of `GPT2DoubleHeadsModel`\n     all_model_classes = (\n         (\n             GPT2Model,"
        },
        {
            "sha": "ec5588b1b9892509dff1066ef126f4480f94c406",
            "filename": "tests/models/gpt_oss/test_modeling_gpt_oss.py",
            "status": "modified",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fgpt_oss%2Ftest_modeling_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fgpt_oss%2Ftest_modeling_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_oss%2Ftest_modeling_gpt_oss.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -27,7 +27,6 @@\n from transformers import (\n     AutoModelForCausalLM,\n     AutoTokenizer,\n-    GptOssConfig,\n     is_torch_available,\n )\n from transformers.testing_utils import (\n@@ -40,7 +39,6 @@\n )\n \n from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n-from ...test_configuration_common import ConfigTester\n \n \n if is_torch_available():\n@@ -58,31 +56,11 @@\n \n class GptOssModelTester(CausalLMModelTester):\n     if is_torch_available():\n-        config_class = GptOssConfig\n         base_model_class = GptOssModel\n-        causal_lm_class = GptOssForCausalLM\n-        sequence_class = GptOssForSequenceClassification\n-        token_class = GptOssForTokenClassification\n-\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": GptOssModel,\n-            \"text-classification\": GptOssForSequenceClassification,\n-            \"text-generation\": GptOssForCausalLM,\n-            \"token-classification\": GptOssForTokenClassification,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n \n \n @require_torch\n class GptOssModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (\n-        (GptOssModel, GptOssForCausalLM, GptOssForSequenceClassification, GptOssForTokenClassification)\n-        if is_torch_available()\n-        else ()\n-    )\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": GptOssModel,\n@@ -94,16 +72,10 @@ class GptOssModelTest(CausalLMModelTest, unittest.TestCase):\n         else {}\n     )\n \n-    test_headmasking = False\n-    test_pruning = False\n     _is_stateful = True\n     model_split_percents = [0.5, 0.6]\n     model_tester_class = GptOssModelTester\n \n-    def setUp(self):\n-        self.model_tester = GptOssModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=GptOssConfig, hidden_size=37)\n-\n     @unittest.skip(\"GptOss's forcefully disables sdpa due to Sink\")\n     def test_sdpa_can_dispatch_non_composite_models(self):\n         pass"
        },
        {
            "sha": "4759089be9260bea048eb9c55de317694c26aea8",
            "filename": "tests/models/helium/test_modeling_helium.py",
            "status": "modified",
            "additions": 3,
            "deletions": 13,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fhelium%2Ftest_modeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fhelium%2Ftest_modeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhelium%2Ftest_modeling_helium.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -15,7 +15,7 @@\n \n import unittest\n \n-from transformers import AutoModelForCausalLM, AutoTokenizer, HeliumConfig, is_torch_available\n+from transformers import AutoModelForCausalLM, AutoTokenizer, is_torch_available\n from transformers.testing_utils import (\n     Expectations,\n     require_read_token,\n@@ -40,20 +40,11 @@\n \n class HeliumModelTester(CausalLMModelTester):\n     if is_torch_available():\n-        config_class = HeliumConfig\n         base_model_class = HeliumModel\n-        causal_lm_class = HeliumForCausalLM\n-        sequence_classification_class = HeliumForSequenceClassification\n-        token_classification_class = HeliumForTokenClassification\n \n \n @require_torch\n class HeliumModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (\n-        (HeliumModel, HeliumForCausalLM, HeliumForSequenceClassification, HeliumForTokenClassification)\n-        if is_torch_available()\n-        else ()\n-    )\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": HeliumModel,\n@@ -65,12 +56,11 @@ class HeliumModelTest(CausalLMModelTest, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    model_tester_class = HeliumModelTester\n-    test_headmasking = False\n-    test_pruning = False\n     _is_stateful = True\n     model_split_percents = [0.5, 0.6]\n \n+    model_tester_class = HeliumModelTester\n+\n \n @slow\n # @require_torch_gpu"
        },
        {
            "sha": "edcf9cd210887b7d9e831e66f422b059b8a25417",
            "filename": "tests/models/hunyuan_v1_dense/test_modeling_hunyuan_v1_dense.py",
            "status": "modified",
            "additions": 1,
            "deletions": 15,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fhunyuan_v1_dense%2Ftest_modeling_hunyuan_v1_dense.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fhunyuan_v1_dense%2Ftest_modeling_hunyuan_v1_dense.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhunyuan_v1_dense%2Ftest_modeling_hunyuan_v1_dense.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -17,7 +17,7 @@\n \n from parameterized import parameterized\n \n-from transformers import HunYuanDenseV1Config, is_torch_available\n+from transformers import is_torch_available\n from transformers.testing_utils import (\n     cleanup,\n     require_torch,\n@@ -36,26 +36,12 @@\n \n \n class HunYuanDenseV1ModelTester(CausalLMModelTester):\n-    config_class = HunYuanDenseV1Config\n     if is_torch_available():\n         base_model_class = HunYuanDenseV1Model\n-        causal_lm_class = HunYuanDenseV1ForCausalLM\n-        sequence_class = HunYuanDenseV1ForSequenceClassification\n \n \n @require_torch\n class HunYuanDenseV1ModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            HunYuanDenseV1Model,\n-            HunYuanDenseV1ForCausalLM,\n-            HunYuanDenseV1ForSequenceClassification,\n-        )\n-        if is_torch_available()\n-        else ()\n-    )\n-    test_headmasking = False\n-    test_pruning = False\n     model_tester_class = HunYuanDenseV1ModelTester\n     pipeline_model_mapping = (\n         {"
        },
        {
            "sha": "b835f0677cfecf7e8f158be4c02c915ad2c53bd8",
            "filename": "tests/models/hunyuan_v1_moe/test_modeling_hunyuan_v1_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 15,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fhunyuan_v1_moe%2Ftest_modeling_hunyuan_v1_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fhunyuan_v1_moe%2Ftest_modeling_hunyuan_v1_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhunyuan_v1_moe%2Ftest_modeling_hunyuan_v1_moe.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -18,7 +18,7 @@\n import pytest\n from parameterized import parameterized\n \n-from transformers import HunYuanMoEV1Config, is_torch_available\n+from transformers import is_torch_available\n from transformers.testing_utils import (\n     cleanup,\n     require_torch,\n@@ -40,26 +40,12 @@\n \n \n class HunYuanMoEV1ModelTester(CausalLMModelTester):\n-    config_class = HunYuanMoEV1Config\n     if is_torch_available():\n         base_model_class = HunYuanMoEV1Model\n-        causal_lm_class = HunYuanMoEV1ForCausalLM\n-        sequence_class = HunYuanMoEV1ForSequenceClassification\n \n \n @require_torch\n class HunYuanMoEV1ModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            HunYuanMoEV1Model,\n-            HunYuanMoEV1ForCausalLM,\n-            HunYuanMoEV1ForSequenceClassification,\n-        )\n-        if is_torch_available()\n-        else ()\n-    )\n-    test_headmasking = False\n-    test_pruning = False\n     test_all_params_have_gradient = False\n     model_tester_class = HunYuanMoEV1ModelTester\n     pipeline_model_mapping = ("
        },
        {
            "sha": "82ca7d16acd9bd03d3728350cb2bc8b70950b04d",
            "filename": "tests/models/jetmoe/test_modeling_jetmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 10,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -18,7 +18,7 @@\n \n import pytest\n \n-from transformers import AutoTokenizer, JetMoeConfig, is_torch_available\n+from transformers import AutoTokenizer, is_torch_available\n from transformers.testing_utils import (\n     backend_empty_cache,\n     require_flash_attn,\n@@ -42,12 +42,8 @@\n \n \n class JetMoeModelTester(CausalLMModelTester):\n-    config_class = JetMoeConfig\n-    forced_config_args = [\"pad_token_id\"]\n     if is_torch_available():\n         base_model_class = JetMoeModel\n-        causal_lm_class = JetMoeForCausalLM\n-        sequence_class = JetMoeForSequenceClassification\n \n     def __init__(\n         self,\n@@ -106,11 +102,6 @@ def __init__(\n \n @require_torch\n class JetMoeModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (\n-        (JetMoeModel, JetMoeForCausalLM, JetMoeForSequenceClassification) if is_torch_available() else ()\n-    )\n-    test_headmasking = False\n-    test_pruning = False\n     test_mismatched_shapes = False\n     test_cpu_offload = False\n     test_disk_offload_bin = False"
        },
        {
            "sha": "8007d0db87a165ab535cb99e36c064a02db741b2",
            "filename": "tests/models/lfm2/test_modeling_lfm2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Flfm2%2Ftest_modeling_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Flfm2%2Ftest_modeling_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flfm2%2Ftest_modeling_lfm2.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -29,14 +29,12 @@\n \n \n if is_torch_available():\n-    from transformers import Lfm2Config, Lfm2ForCausalLM, Lfm2Model\n+    from transformers import Lfm2ForCausalLM, Lfm2Model\n \n \n class Lfm2ModelTester(CausalLMModelTester):\n     if is_torch_available():\n-        config_class = Lfm2Config\n         base_model_class = Lfm2Model\n-        causal_lm_class = Lfm2ForCausalLM\n \n     def __init__(\n         self,\n@@ -49,7 +47,6 @@ def __init__(\n \n @require_torch\n class Lfm2ModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (Lfm2Model, Lfm2ForCausalLM) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": Lfm2Model,\n@@ -58,8 +55,6 @@ class Lfm2ModelTest(CausalLMModelTest, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    test_headmasking = False\n-    test_pruning = False\n     fx_compatible = False\n     model_tester_class = Lfm2ModelTester\n     # used in `test_torch_compile_for_training`"
        },
        {
            "sha": "e55cf011d668abec2d1299ae64ebfc960cf703ba",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -38,7 +38,6 @@\n     import torch\n \n     from transformers import (\n-        LlamaConfig,\n         LlamaForCausalLM,\n         LlamaForQuestionAnswering,\n         LlamaForSequenceClassification,\n@@ -50,26 +49,11 @@\n \n class LlamaModelTester(CausalLMModelTester):\n     if is_torch_available():\n-        config_class = LlamaConfig\n         base_model_class = LlamaModel\n-        causal_lm_class = LlamaForCausalLM\n-        sequence_class = LlamaForSequenceClassification\n-        token_class = LlamaForTokenClassification\n \n \n @require_torch\n class LlamaModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            LlamaModel,\n-            LlamaForCausalLM,\n-            LlamaForSequenceClassification,\n-            LlamaForQuestionAnswering,\n-            LlamaForTokenClassification,\n-        )\n-        if is_torch_available()\n-        else ()\n-    )\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": LlamaModel,\n@@ -82,8 +66,6 @@ class LlamaModelTest(CausalLMModelTest, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    test_headmasking = False\n-    test_pruning = False\n     fx_compatible = False  # Broken by attention refactor cc @Cyrilvallez\n     model_tester_class = LlamaModelTester\n "
        },
        {
            "sha": "011243c93409835c85bc37388dcea25e0c484e06",
            "filename": "tests/models/longcat_flash/test_modeling_longcat_flash.py",
            "status": "modified",
            "additions": 1,
            "deletions": 24,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Flongcat_flash%2Ftest_modeling_longcat_flash.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Flongcat_flash%2Ftest_modeling_longcat_flash.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongcat_flash%2Ftest_modeling_longcat_flash.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -32,7 +32,6 @@\n )\n \n from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n-from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ids_tensor\n \n \n@@ -44,9 +43,7 @@\n \n class LongcatFlashModelTester(CausalLMModelTester):\n     if is_torch_available():\n-        config_class = LongcatFlashConfig\n         base_model_class = LongcatFlashModel\n-        causal_lm_class = LongcatFlashForCausalLM\n \n     def __init__(\n         self,\n@@ -84,6 +81,7 @@ def __init__(\n         num_labels=3,\n         num_choices=4,\n     ):\n+        super().__init__(parent)\n         self.parent = parent\n         self.batch_size = batch_size\n         self.seq_length = seq_length\n@@ -212,9 +210,6 @@ def prepare_config_and_inputs_for_common(self):\n \n @require_torch\n class LongcatFlashModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (LongcatFlashModel, LongcatFlashForCausalLM) if is_torch_available() else ()\n-    all_generative_model_classes = (LongcatFlashForCausalLM,) if is_torch_available() else ()\n-\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": LongcatFlashModel,\n@@ -226,26 +221,8 @@ class LongcatFlashModelTest(CausalLMModelTest, unittest.TestCase):\n \n     model_split_percents = [0.5, 0.8]\n \n-    test_headmasking = False\n-    test_pruning = False\n-\n     model_tester_class = LongcatFlashModelTester\n \n-    def setUp(self):\n-        self.model_tester = LongcatFlashModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=LongcatFlashConfig, hidden_size=37, num_attention_heads=3)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_for_causal_lm(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_causal_lm(*config_and_inputs)\n-\n     @unittest.skip(\"LongcatFlash buffers include complex numbers, which breaks this test\")\n     def test_save_load_fast_init_from_base(self):\n         pass"
        },
        {
            "sha": "6b503915dd189fd0bc61424c3e24d9787f55b3d4",
            "filename": "tests/models/minimax/test_modeling_minimax.py",
            "status": "modified",
            "additions": 1,
            "deletions": 20,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fminimax%2Ftest_modeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fminimax%2Ftest_modeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fminimax%2Ftest_modeling_minimax.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -17,7 +17,7 @@\n \n import pytest\n \n-from transformers import MiniMaxConfig, is_torch_available\n+from transformers import is_torch_available\n from transformers.cache_utils import Cache\n from transformers.testing_utils import (\n     Expectations,\n@@ -42,13 +42,8 @@\n \n \n class MiniMaxModelTester(CausalLMModelTester):\n-    config_class = MiniMaxConfig\n     if is_torch_available():\n         base_model_class = MiniMaxModel\n-        causal_lm_class = MiniMaxForCausalLM\n-        sequence_class = MiniMaxForSequenceClassification\n-        token_class = MiniMaxForTokenClassification\n-        question_answering_class = MiniMaxForQuestionAnswering\n \n     def __init__(self, parent, layer_types=None, block_size=3):\n         super().__init__(parent)\n@@ -58,17 +53,6 @@ def __init__(self, parent, layer_types=None, block_size=3):\n \n @require_torch\n class MiniMaxModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            MiniMaxModel,\n-            MiniMaxForCausalLM,\n-            MiniMaxForSequenceClassification,\n-            MiniMaxForTokenClassification,\n-            MiniMaxForQuestionAnswering,\n-        )\n-        if is_torch_available()\n-        else ()\n-    )\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": MiniMaxModel,\n@@ -80,9 +64,6 @@ class MiniMaxModelTest(CausalLMModelTest, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-\n-    test_headmasking = False\n-    test_pruning = False\n     model_tester_class = MiniMaxModelTester\n \n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79245/workflows/9490ef58-79c2-410d-8f51-e3495156cf9c/jobs/1012146"
        },
        {
            "sha": "32c7ef206f141c836b8102f090f318f50867ff57",
            "filename": "tests/models/ministral/test_modeling_ministral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 19,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fministral%2Ftest_modeling_ministral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fministral%2Ftest_modeling_ministral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fministral%2Ftest_modeling_ministral.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -20,7 +20,7 @@\n \n import pytest\n \n-from transformers import AutoTokenizer, GenerationConfig, MinistralConfig, is_torch_available\n+from transformers import AutoTokenizer, GenerationConfig, is_torch_available\n from transformers.testing_utils import (\n     backend_empty_cache,\n     cleanup,\n@@ -50,30 +50,12 @@\n \n \n class MinistralModelTester(CausalLMModelTester):\n-    config_class = MinistralConfig\n     if is_torch_available():\n         base_model_class = MinistralModel\n-        causal_lm_class = MinistralForCausalLM\n-        sequence_class = MinistralForSequenceClassification\n-        token_class = MinistralForTokenClassification\n-        question_answering_class = MinistralForQuestionAnswering\n \n \n @require_torch\n class MinistralModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            MinistralModel,\n-            MinistralForCausalLM,\n-            MinistralForSequenceClassification,\n-            MinistralForTokenClassification,\n-            MinistralForQuestionAnswering,\n-        )\n-        if is_torch_available()\n-        else ()\n-    )\n-    test_headmasking = False\n-    test_pruning = False\n     model_tester_class = MinistralModelTester\n     pipeline_model_mapping = (\n         {"
        },
        {
            "sha": "9699c1efda0dfaee164d3518e08e2135bcace26f",
            "filename": "tests/models/mistral/test_modeling_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 19,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -20,7 +20,7 @@\n from packaging import version\n from parameterized import parameterized\n \n-from transformers import AutoTokenizer, DynamicCache, MistralConfig, is_torch_available, set_seed\n+from transformers import AutoTokenizer, DynamicCache, is_torch_available, set_seed\n from transformers.cache_utils import DynamicSlidingWindowLayer\n from transformers.testing_utils import (\n     DeviceProperties,\n@@ -52,28 +52,12 @@\n \n \n class MistralModelTester(CausalLMModelTester):\n-    config_class = MistralConfig\n     if is_torch_available():\n         base_model_class = MistralModel\n-        causal_lm_class = MistralForCausalLM\n-        sequence_class = MistralForSequenceClassification\n-        token_class = MistralForTokenClassification\n-        question_answering_class = MistralForQuestionAnswering\n \n \n @require_torch\n class MistralModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            MistralModel,\n-            MistralForCausalLM,\n-            MistralForSequenceClassification,\n-            MistralForTokenClassification,\n-            MistralForQuestionAnswering,\n-        )\n-        if is_torch_available()\n-        else ()\n-    )\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": MistralModel,\n@@ -85,8 +69,6 @@ class MistralModelTest(CausalLMModelTest, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    test_headmasking = False\n-    test_pruning = False\n     model_tester_class = MistralModelTester\n \n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79245/workflows/9490ef58-79c2-410d-8f51-e3495156cf9c/jobs/1012146"
        },
        {
            "sha": "41fe190f828c5524e5e36ecb7e0432401a8ae509",
            "filename": "tests/models/mixtral/test_modeling_mixtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 20,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -17,7 +17,7 @@\n \n import pytest\n \n-from transformers import MixtralConfig, is_torch_available\n+from transformers import is_torch_available\n from transformers.testing_utils import (\n     Expectations,\n     require_flash_attn,\n@@ -44,28 +44,12 @@\n \n \n class MixtralModelTester(CausalLMModelTester):\n-    config_class = MixtralConfig\n     if is_torch_available():\n         base_model_class = MixtralModel\n-        causal_lm_class = MixtralForCausalLM\n-        sequence_class = MixtralForSequenceClassification\n-        token_class = MixtralForTokenClassification\n-        question_answering_class = MixtralForQuestionAnswering\n \n \n @require_torch\n-class MistralModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            MixtralModel,\n-            MixtralForCausalLM,\n-            MixtralForSequenceClassification,\n-            MixtralForTokenClassification,\n-            MixtralForQuestionAnswering,\n-        )\n-        if is_torch_available()\n-        else ()\n-    )\n+class MixtralModelTest(CausalLMModelTest, unittest.TestCase):\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": MixtralModel,\n@@ -78,8 +62,6 @@ class MistralModelTest(CausalLMModelTest, unittest.TestCase):\n         else {}\n     )\n \n-    test_headmasking = False\n-    test_pruning = False\n     model_tester_class = MixtralModelTester\n \n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79245/workflows/9490ef58-79c2-410d-8f51-e3495156cf9c/jobs/1012146"
        },
        {
            "sha": "41371a75aa76c37f2b24aab5c9b8d9e720c0ba03",
            "filename": "tests/models/modernbert_decoder/test_modeling_modernbert_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 11,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fmodernbert_decoder%2Ftest_modeling_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fmodernbert_decoder%2Ftest_modeling_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmodernbert_decoder%2Ftest_modeling_modernbert_decoder.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -15,7 +15,7 @@\n \n from packaging import version\n \n-from transformers import AutoTokenizer, ModernBertDecoderConfig, is_torch_available\n+from transformers import AutoTokenizer, is_torch_available\n from transformers.testing_utils import (\n     require_torch,\n     slow,\n@@ -36,19 +36,12 @@\n \n \n class ModernBertDecoderModelTester(CausalLMModelTester):\n-    config_class = ModernBertDecoderConfig\n     if is_torch_available():\n         base_model_class = ModernBertDecoderModel\n-        causal_lm_class = ModernBertDecoderForCausalLM\n \n \n @require_torch\n class ModernBertDecoderModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (\n-        (ModernBertDecoderModel, ModernBertDecoderForCausalLM, ModernBertDecoderForSequenceClassification)\n-        if is_torch_available()\n-        else ()\n-    )\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": ModernBertDecoderModel,\n@@ -58,9 +51,6 @@ class ModernBertDecoderModelTest(CausalLMModelTest, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-\n-    test_head_masking = False\n-    test_pruning = False\n     model_tester_class = ModernBertDecoderModelTester\n \n     def test_initialization(self):"
        },
        {
            "sha": "a524fb404b90008bd7221d63e73cd79f2d047573",
            "filename": "tests/models/nemotron/test_modeling_nemotron.py",
            "status": "modified",
            "additions": 1,
            "deletions": 23,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -18,7 +18,7 @@\n \n from parameterized import parameterized\n \n-from transformers import NemotronConfig, is_torch_available\n+from transformers import is_torch_available\n from transformers.testing_utils import (\n     Expectations,\n     require_read_token,\n@@ -29,7 +29,6 @@\n )\n \n from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n-from ...test_configuration_common import ConfigTester\n \n \n if is_torch_available():\n@@ -47,11 +46,7 @@\n \n class NemotronModelTester(CausalLMModelTester):\n     if is_torch_available():\n-        config_class = NemotronConfig\n         base_model_class = NemotronModel\n-        causal_lm_class = NemotronForCausalLM\n-        sequence_class = NemotronForSequenceClassification\n-        token_class = NemotronForTokenClassification\n \n \n @require_torch\n@@ -60,17 +55,6 @@ class NemotronModelTest(CausalLMModelTest, unittest.TestCase):\n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`\n     # This is because we are hitting edge cases with the causal_mask buffer\n     model_split_percents = [0.5, 0.7, 0.8]\n-    all_model_classes = (\n-        (\n-            NemotronModel,\n-            NemotronForCausalLM,\n-            NemotronForSequenceClassification,\n-            NemotronForQuestionAnswering,\n-            NemotronForTokenClassification,\n-        )\n-        if is_torch_available()\n-        else ()\n-    )\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": NemotronModel,\n@@ -83,17 +67,11 @@ class NemotronModelTest(CausalLMModelTest, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    test_headmasking = False\n-    test_pruning = False\n     fx_compatible = False\n \n     # used in `test_torch_compile_for_training`\n     _torch_compile_train_cls = NemotronForCausalLM if is_torch_available() else None\n \n-    def setUp(self):\n-        self.model_tester = NemotronModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=NemotronConfig, hidden_size=37)\n-\n     @unittest.skip(\"Eager and SDPA do not produce the same outputs, thus this test fails\")\n     def test_model_outputs_equivalence(self, **kwargs):\n         pass"
        },
        {
            "sha": "c4284173a40824fab32c4d428b11f7e11c1e4a83",
            "filename": "tests/models/olmo3/test_modeling_olmo3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Folmo3%2Ftest_modeling_olmo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Folmo3%2Ftest_modeling_olmo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmo3%2Ftest_modeling_olmo3.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -20,7 +20,7 @@\n from packaging import version\n from parameterized import parameterized\n \n-from transformers import Olmo3Config, is_torch_available, set_seed\n+from transformers import is_torch_available, set_seed\n from transformers.generation.configuration_utils import GenerationConfig\n from transformers.models.auto.tokenization_auto import AutoTokenizer\n from transformers.testing_utils import (\n@@ -47,14 +47,11 @@\n \n class Olmo3ModelTester(CausalLMModelTester):\n     if is_torch_available():\n-        config_class = Olmo3Config\n         base_model_class = Olmo3Model\n-        causal_lm_class = Olmo3ForCausalLM\n \n \n @require_torch\n class Olmo3ModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (Olmo3Model, Olmo3ForCausalLM) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": Olmo3Model,\n@@ -63,8 +60,6 @@ class Olmo3ModelTest(CausalLMModelTest, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    test_headmasking = False\n-    test_pruning = False\n     fx_compatible = False\n     test_torchscript = False\n     test_all_params_have_gradient = False"
        },
        {
            "sha": "3282bf8ee199edca061a8a1eaca7e78964d19abe",
            "filename": "tests/models/persimmon/test_modeling_persimmon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 13,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -16,7 +16,7 @@\n import gc\n import unittest\n \n-from transformers import PersimmonConfig, is_torch_available\n+from transformers import is_torch_available\n from transformers.testing_utils import (\n     backend_empty_cache,\n     require_bitsandbytes,\n@@ -44,21 +44,12 @@\n \n class PersimmonModelTester(CausalLMModelTester):\n     if is_torch_available():\n-        config_class = PersimmonConfig\n         base_model_class = PersimmonModel\n-        causal_lm_class = PersimmonForCausalLM\n-        sequence_class = PersimmonForSequenceClassification\n-        token_class = PersimmonForTokenClassification\n \n \n @require_torch\n class PersimmonModelTest(CausalLMModelTest, unittest.TestCase):\n     model_tester_class = PersimmonModelTester\n-    all_model_classes = (\n-        (PersimmonModel, PersimmonForCausalLM, PersimmonForSequenceClassification, PersimmonForTokenClassification)\n-        if is_torch_available()\n-        else ()\n-    )\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": PersimmonModel,\n@@ -73,9 +64,6 @@ class PersimmonModelTest(CausalLMModelTest, unittest.TestCase):\n     )\n     model_tester_class = PersimmonModelTester\n \n-    test_headmasking = False\n-    test_pruning = False\n-\n     @unittest.skip(\"Persimmon applies key/query norm which doesn't work with packing\")\n     def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n         pass"
        },
        {
            "sha": "5aa8d517a3c049e7887d6f340b0125f9ab5c05c3",
            "filename": "tests/models/phi/test_modeling_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 12,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -16,7 +16,7 @@\n \n import unittest\n \n-from transformers import PhiConfig, is_torch_available\n+from transformers import is_torch_available\n from transformers.testing_utils import (\n     require_torch,\n     slow,\n@@ -39,21 +39,12 @@\n \n \n class PhiModelTester(CausalLMModelTester):\n-    config_class = PhiConfig\n     if is_torch_available():\n         base_model_class = PhiModel\n-        causal_lm_class = PhiForCausalLM\n-        sequence_class = PhiForSequenceClassification\n-        token_class = PhiForTokenClassification\n \n \n @require_torch\n class PhiModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (\n-        (PhiModel, PhiForCausalLM, PhiForSequenceClassification, PhiForTokenClassification)\n-        if is_torch_available()\n-        else ()\n-    )\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": PhiModel,\n@@ -65,8 +56,6 @@ class PhiModelTest(CausalLMModelTest, unittest.TestCase):\n         else {}\n     )\n \n-    test_headmasking = False\n-    test_pruning = False\n     model_tester_class = PhiModelTester\n \n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79292/workflows/fa2ba644-8953-44a6-8f67-ccd69ca6a476/jobs/1012905"
        },
        {
            "sha": "d3460fdd433d60eb6262e99a5f9c0efb9071c6b3",
            "filename": "tests/models/phi3/test_modeling_phi3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 12,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -18,7 +18,7 @@\n \n import pytest\n \n-from transformers import Phi3Config, StaticCache, is_torch_available\n+from transformers import StaticCache, is_torch_available\n from transformers.models.auto.configuration_auto import AutoConfig\n from transformers.testing_utils import (\n     Expectations,\n@@ -86,21 +86,12 @@ def generate(model: Phi3ForCausalLM, prompt_tokens: torch.LongTensor, max_seq_le\n \n \n class Phi3ModelTester(CausalLMModelTester):\n-    config_class = Phi3Config\n     if is_torch_available():\n         base_model_class = Phi3Model\n-        causal_lm_class = Phi3ForCausalLM\n-        sequence_class = Phi3ForSequenceClassification\n-        token_class = Phi3ForTokenClassification\n \n \n @require_torch\n class Phi3ModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (\n-        (Phi3Model, Phi3ForCausalLM, Phi3ForSequenceClassification, Phi3ForTokenClassification)\n-        if is_torch_available()\n-        else ()\n-    )\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": Phi3Model,\n@@ -112,8 +103,6 @@ class Phi3ModelTest(CausalLMModelTest, unittest.TestCase):\n         else {}\n     )\n \n-    test_headmasking = False\n-    test_pruning = False\n     model_tester_class = Phi3ModelTester\n \n "
        },
        {
            "sha": "e67f538a53bf656e61767d14296571e01474e08c",
            "filename": "tests/models/phimoe/test_modeling_phimoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 10,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -18,7 +18,7 @@\n \n from parameterized import parameterized\n \n-from transformers import PhimoeConfig, StaticCache, is_torch_available\n+from transformers import StaticCache, is_torch_available\n from transformers.testing_utils import (\n     cleanup,\n     require_torch,\n@@ -86,20 +86,11 @@ def generate(model: PhimoeForCausalLM, prompt_tokens: torch.LongTensor, max_seq_\n \n class PhimoeModelTester(CausalLMModelTester):\n     if is_torch_available():\n-        config_class = PhimoeConfig\n         base_model_class = PhimoeModel\n-        causal_lm_class = PhimoeForCausalLM\n-        sequence_class = PhimoeForSequenceClassification\n \n \n @require_torch\n class PhimoeModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (\n-        (PhimoeModel, PhimoeForCausalLM, PhimoeForSequenceClassification) if is_torch_available() else ()\n-    )\n-\n-    test_headmasking = False\n-    test_pruning = False\n     test_all_params_have_gradient = False\n     model_tester_class = PhimoeModelTester\n     pipeline_model_mapping = ("
        },
        {
            "sha": "5b1b3792381c06a6d73b15980210745ba5cc2e60",
            "filename": "tests/models/qwen2/test_modeling_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 19,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -19,7 +19,7 @@\n import pytest\n from packaging import version\n \n-from transformers import AutoTokenizer, Qwen2Config, is_torch_available, set_seed\n+from transformers import AutoTokenizer, is_torch_available, set_seed\n from transformers.generation.configuration_utils import GenerationConfig\n from transformers.testing_utils import (\n     Expectations,\n@@ -48,30 +48,12 @@\n \n \n class Qwen2ModelTester(CausalLMModelTester):\n-    config_class = Qwen2Config\n     if is_torch_available():\n         base_model_class = Qwen2Model\n-        causal_lm_class = Qwen2ForCausalLM\n-        sequence_class = Qwen2ForSequenceClassification\n-        token_class = Qwen2ForTokenClassification\n-        question_answering_class = Qwen2ForQuestionAnswering\n \n \n @require_torch\n class Qwen2ModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            Qwen2Model,\n-            Qwen2ForCausalLM,\n-            Qwen2ForSequenceClassification,\n-            Qwen2ForTokenClassification,\n-            Qwen2ForQuestionAnswering,\n-        )\n-        if is_torch_available()\n-        else ()\n-    )\n-    test_headmasking = False\n-    test_pruning = False\n     model_tester_class = Qwen2ModelTester\n     pipeline_model_mapping = (\n         {"
        },
        {
            "sha": "1b6ad9c1ec4135128c913b225c16575b29887bae",
            "filename": "tests/models/qwen2_moe/test_modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 19,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -17,7 +17,7 @@\n \n import pytest\n \n-from transformers import AutoTokenizer, Qwen2MoeConfig, is_torch_available, set_seed\n+from transformers import AutoTokenizer, is_torch_available, set_seed\n from transformers.testing_utils import (\n     cleanup,\n     require_flash_attn,\n@@ -46,28 +46,12 @@\n \n \n class Qwen2MoeModelTester(CausalLMModelTester):\n-    config_class = Qwen2MoeConfig\n     if is_torch_available():\n         base_model_class = Qwen2MoeModel\n-        causal_lm_class = Qwen2MoeForCausalLM\n-        sequence_class = Qwen2MoeForSequenceClassification\n-        token_class = Qwen2MoeForTokenClassification\n-        question_answering_class = Qwen2MoeForQuestionAnswering\n \n \n @require_torch\n class Qwen2MoeModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            Qwen2MoeModel,\n-            Qwen2MoeForCausalLM,\n-            Qwen2MoeForSequenceClassification,\n-            Qwen2MoeForTokenClassification,\n-            Qwen2MoeForQuestionAnswering,\n-        )\n-        if is_torch_available()\n-        else ()\n-    )\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": Qwen2MoeModel,\n@@ -80,8 +64,6 @@ class Qwen2MoeModelTest(CausalLMModelTest, unittest.TestCase):\n         else {}\n     )\n \n-    test_headmasking = False\n-    test_pruning = False\n     test_all_params_have_gradient = False\n     model_tester_class = Qwen2MoeModelTester\n "
        },
        {
            "sha": "7640aeb0828b1adca42d5e76280711452fb20f8e",
            "filename": "tests/models/qwen3/test_modeling_qwen3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 19,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -18,7 +18,7 @@\n import pytest\n from packaging import version\n \n-from transformers import AutoTokenizer, Qwen3Config, is_torch_available, set_seed\n+from transformers import AutoTokenizer, is_torch_available, set_seed\n from transformers.generation.configuration_utils import GenerationConfig\n from transformers.testing_utils import (\n     Expectations,\n@@ -46,30 +46,12 @@\n \n \n class Qwen3ModelTester(CausalLMModelTester):\n-    config_class = Qwen3Config\n     if is_torch_available():\n         base_model_class = Qwen3Model\n-        causal_lm_class = Qwen3ForCausalLM\n-        sequence_class = Qwen3ForSequenceClassification\n-        token_class = Qwen3ForTokenClassification\n-        question_answering_class = Qwen3ForQuestionAnswering\n \n \n @require_torch\n class Qwen3ModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            Qwen3Model,\n-            Qwen3ForCausalLM,\n-            Qwen3ForSequenceClassification,\n-            Qwen3ForTokenClassification,\n-            Qwen3ForQuestionAnswering,\n-        )\n-        if is_torch_available()\n-        else ()\n-    )\n-    test_headmasking = False\n-    test_pruning = False\n     model_tester_class = Qwen3ModelTester\n     pipeline_model_mapping = (\n         {"
        },
        {
            "sha": "69215c36db6e7494833ecb3a1a0d7389181749ec",
            "filename": "tests/models/qwen3_moe/test_modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 20,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fqwen3_moe%2Ftest_modeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fqwen3_moe%2Ftest_modeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_moe%2Ftest_modeling_qwen3_moe.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -17,7 +17,7 @@\n \n import pytest\n \n-from transformers import AutoTokenizer, Qwen3MoeConfig, is_torch_available, set_seed\n+from transformers import AutoTokenizer, is_torch_available, set_seed\n from transformers.testing_utils import (\n     cleanup,\n     require_bitsandbytes,\n@@ -36,7 +36,6 @@\n     from transformers import (\n         Qwen3ForQuestionAnswering,\n         Qwen3MoeForCausalLM,\n-        Qwen3MoeForQuestionAnswering,\n         Qwen3MoeForSequenceClassification,\n         Qwen3MoeForTokenClassification,\n         Qwen3MoeModel,\n@@ -45,28 +44,12 @@\n \n \n class Qwen3MoeModelTester(CausalLMModelTester):\n-    config_class = Qwen3MoeConfig\n     if is_torch_available():\n         base_model_class = Qwen3MoeModel\n-        causal_lm_class = Qwen3MoeForCausalLM\n-        sequence_class = Qwen3MoeForSequenceClassification\n-        token_class = Qwen3MoeForTokenClassification\n-        question_answering_class = Qwen3MoeForQuestionAnswering\n \n \n @require_torch\n class Qwen3MoeModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            Qwen3MoeModel,\n-            Qwen3MoeForCausalLM,\n-            Qwen3MoeForSequenceClassification,\n-            Qwen3MoeForTokenClassification,\n-            Qwen3MoeForQuestionAnswering,\n-        )\n-        if is_torch_available()\n-        else ()\n-    )\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": Qwen3MoeModel,\n@@ -79,8 +62,6 @@ class Qwen3MoeModelTest(CausalLMModelTest, unittest.TestCase):\n         else {}\n     )\n \n-    test_headmasking = False\n-    test_pruning = False\n     test_all_params_have_gradient = False\n     model_tester_class = Qwen3MoeModelTester\n "
        },
        {
            "sha": "3589f32584a909b10df11184795777a682553f57",
            "filename": "tests/models/qwen3_next/test_modeling_qwen3_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 19,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fqwen3_next%2Ftest_modeling_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fqwen3_next%2Ftest_modeling_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_next%2Ftest_modeling_qwen3_next.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -20,7 +20,7 @@\n import pytest\n from parameterized import parameterized\n \n-from transformers import Qwen3NextConfig, is_torch_available\n+from transformers import is_torch_available\n from transformers.testing_utils import require_torch, require_torch_multi_gpu, slow, torch_device\n \n \n@@ -46,13 +46,8 @@\n \n \n class Qwen3NextModelTester(CausalLMModelTester):\n-    config_class = Qwen3NextConfig\n     if is_torch_available():\n         base_model_class = Qwen3NextModel\n-        causal_lm_class = Qwen3NextForCausalLM\n-        sequence_class = Qwen3NextForSequenceClassification\n-        token_class = Qwen3NextForTokenClassification\n-        question_answering_class = Qwen3NextForQuestionAnswering\n \n     def __init__(self, parent):\n         super().__init__(parent=parent)\n@@ -66,17 +61,6 @@ def __init__(self, parent):\n \n @require_torch\n class Qwen3NextModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            Qwen3NextModel,\n-            Qwen3NextForCausalLM,\n-            Qwen3NextForSequenceClassification,\n-            Qwen3NextForTokenClassification,\n-            Qwen3NextForQuestionAnswering,\n-        )\n-        if is_torch_available()\n-        else ()\n-    )\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": Qwen3NextModel,\n@@ -89,8 +73,6 @@ class Qwen3NextModelTest(CausalLMModelTest, unittest.TestCase):\n         else {}\n     )\n \n-    test_headmasking = False\n-    test_pruning = False\n     model_tester_class = Qwen3NextModelTester\n \n     def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_values, cache_length, config):"
        },
        {
            "sha": "dcd57782b749d8da66f3122d8a7e9c436149ec56",
            "filename": "tests/models/recurrent_gemma/test_modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -18,7 +18,7 @@\n import pytest\n from parameterized import parameterized\n \n-from transformers import AutoModelForCausalLM, AutoTokenizer, RecurrentGemmaConfig, is_torch_available, set_seed\n+from transformers import AutoModelForCausalLM, AutoTokenizer, is_torch_available, set_seed\n from transformers.testing_utils import (\n     Expectations,\n     require_bitsandbytes,\n@@ -33,21 +33,18 @@\n if is_torch_available():\n     import torch\n \n-    from transformers import RecurrentGemmaConfig, RecurrentGemmaForCausalLM, RecurrentGemmaModel\n+    from transformers import RecurrentGemmaForCausalLM, RecurrentGemmaModel\n \n from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n \n \n class RecurrentGemmaModelTester(CausalLMModelTester):\n-    config_class = RecurrentGemmaConfig\n     if is_torch_available():\n         base_model_class = RecurrentGemmaModel\n-        causal_lm_class = RecurrentGemmaForCausalLM\n \n \n @require_torch\n class RecurrentGemmaModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (RecurrentGemmaModel, RecurrentGemmaForCausalLM) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": RecurrentGemmaModel,\n@@ -56,8 +53,6 @@ class RecurrentGemmaModelTest(CausalLMModelTest, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    test_headmasking = False\n-    test_pruning = False\n     has_attentions = False\n     model_tester_class = RecurrentGemmaModelTester\n "
        },
        {
            "sha": "a4ca6953066552c656f4200db9533eab1b86e211",
            "filename": "tests/models/seed_oss/test_modeling_seed_oss.py",
            "status": "modified",
            "additions": 1,
            "deletions": 20,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fseed_oss%2Ftest_modeling_seed_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fseed_oss%2Ftest_modeling_seed_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseed_oss%2Ftest_modeling_seed_oss.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -17,7 +17,7 @@\n \n import pytest\n \n-from transformers import AutoModelForCausalLM, AutoTokenizer, SeedOssConfig, is_torch_available\n+from transformers import AutoModelForCausalLM, AutoTokenizer, is_torch_available\n from transformers.testing_utils import (\n     cleanup,\n     require_flash_attn,\n@@ -36,7 +36,6 @@\n \n     from transformers import (\n         SeedOssForCausalLM,\n-        SeedOssForQuestionAnswering,\n         SeedOssForSequenceClassification,\n         SeedOssForTokenClassification,\n         SeedOssModel,\n@@ -45,28 +44,12 @@\n \n class SeedOssModelTester(CausalLMModelTester):\n     if is_torch_available():\n-        config_class = SeedOssConfig\n         base_model_class = SeedOssModel\n-        causal_lm_class = SeedOssForCausalLM\n-        sequence_classification_class = SeedOssForSequenceClassification\n-        token_classification_class = SeedOssForTokenClassification\n-        question_answering_class = SeedOssForQuestionAnswering\n \n \n @require_torch\n class SeedOssModelTest(CausalLMModelTest, unittest.TestCase):\n     model_tester_class = SeedOssModelTester\n-    all_model_classes = (\n-        (\n-            SeedOssModel,\n-            SeedOssForCausalLM,\n-            SeedOssForSequenceClassification,\n-            SeedOssForTokenClassification,\n-            SeedOssForQuestionAnswering,\n-        )\n-        if is_torch_available()\n-        else ()\n-    )\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": SeedOssModel,\n@@ -78,8 +61,6 @@ class SeedOssModelTest(CausalLMModelTest, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    test_headmasking = False\n-    test_pruning = False\n     _is_stateful = True\n     model_split_percents = [0.5, 0.6]\n "
        },
        {
            "sha": "9a22246adda9a042e1fcb0ea9ac229c822f86059",
            "filename": "tests/models/smollm3/test_modeling_smollm3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 15,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fsmollm3%2Ftest_modeling_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fsmollm3%2Ftest_modeling_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmollm3%2Ftest_modeling_smollm3.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -58,26 +58,13 @@ class SmolLM3ModelTester(CausalLMModelTester):\n     if is_torch_available():\n         base_model_class = SmolLM3Model\n         causal_lm_class = SmolLM3ForCausalLM\n-        sequence_class = SmolLM3ForSequenceClassification\n-        token_class = SmolLM3ForTokenClassification\n         question_answering_class = SmolLM3ForQuestionAnswering\n+        sequence_classification_class = SmolLM3ForSequenceClassification\n+        token_classification_class = SmolLM3ForTokenClassification\n \n \n @require_torch\n class SmolLM3ModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            SmolLM3Model,\n-            SmolLM3ForCausalLM,\n-            SmolLM3ForSequenceClassification,\n-            SmolLM3ForTokenClassification,\n-            SmolLM3ForQuestionAnswering,\n-        )\n-        if is_torch_available()\n-        else ()\n-    )\n-    test_headmasking = False\n-    test_pruning = False\n     model_tester_class = SmolLM3ModelTester\n     pipeline_model_mapping = (\n         {"
        },
        {
            "sha": "978573c8cea458067836d63b453ff23c05dc8ca4",
            "filename": "tests/models/stablelm/test_modeling_stablelm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 17,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -17,7 +17,7 @@\n \n import pytest\n \n-from transformers import StableLmConfig, is_torch_available\n+from transformers import is_torch_available\n from transformers.testing_utils import (\n     require_bitsandbytes,\n     require_flash_attn,\n@@ -43,25 +43,11 @@\n \n class StableLmModelTester(CausalLMModelTester):\n     if is_torch_available():\n-        config_class = StableLmConfig\n         base_model_class = StableLmModel\n-        causal_lm_class = StableLmForCausalLM\n-        sequence_class = StableLmForSequenceClassification\n-        token_class = StableLmForTokenClassification\n \n \n @require_torch\n class StableLmModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            StableLmModel,\n-            StableLmForCausalLM,\n-            StableLmForSequenceClassification,\n-            StableLmForTokenClassification,\n-        )\n-        if is_torch_available()\n-        else ()\n-    )\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": StableLmModel,\n@@ -73,8 +59,6 @@ class StableLmModelTest(CausalLMModelTest, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    test_headmasking = False\n-    test_pruning = False\n     fx_compatible = False  # Broken by attention refactor cc @Cyrilvallez\n     model_tester_class = StableLmModelTester\n "
        },
        {
            "sha": "78eeffe1bf4280413d09fa2c40c3345d19a4418b",
            "filename": "tests/models/starcoder2/test_modeling_starcoder2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 12,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -17,7 +17,7 @@\n \n import pytest\n \n-from transformers import Starcoder2Config, is_torch_available\n+from transformers import is_torch_available\n from transformers.testing_utils import (\n     Expectations,\n     require_bitsandbytes,\n@@ -44,23 +44,12 @@\n \n \n class Starcoder2ModelTester(CausalLMModelTester):\n-    config_class = Starcoder2Config\n     if is_torch_available():\n         base_model_class = Starcoder2Model\n-        causal_lm_class = Starcoder2ForCausalLM\n-        sequence_class = Starcoder2ForSequenceClassification\n-        token_class = Starcoder2ForTokenClassification\n \n \n @require_torch\n class Starcoder2ModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (\n-        (Starcoder2Model, Starcoder2ForCausalLM, Starcoder2ForSequenceClassification, Starcoder2ForTokenClassification)\n-        if is_torch_available()\n-        else ()\n-    )\n-    test_headmasking = False\n-    test_pruning = False\n     model_tester_class = Starcoder2ModelTester\n     pipeline_model_mapping = (\n         {"
        },
        {
            "sha": "7a81eb12b7816d96f37d154f74d0b16dc1f16923",
            "filename": "tests/models/t5gemma/test_modeling_t5gemma.py",
            "status": "modified",
            "additions": 21,
            "deletions": 15,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -53,9 +53,9 @@ class T5GemmaModelTester:\n \n     if is_torch_available():\n         model_class = T5GemmaModel\n-        for_causal_lm_class = T5GemmaForConditionalGeneration\n-        for_sequence_class = T5GemmaForSequenceClassification\n-        for_token_class = T5GemmaForTokenClassification\n+        causal_lm_class = T5GemmaForConditionalGeneration\n+        sequence_classification_class = T5GemmaForSequenceClassification\n+        token_classification_class = T5GemmaForTokenClassification\n \n     def __init__(\n         self,\n@@ -310,7 +310,7 @@ def create_and_check_with_lm_head(\n         decoder_attention_mask,\n         lm_labels,\n     ):\n-        model = self.for_causal_lm_class(config=config).to(torch_device).eval()\n+        model = self.causal_lm_class(config=config).to(torch_device).eval()\n         outputs = model(\n             input_ids=input_ids,\n             decoder_input_ids=decoder_input_ids,\n@@ -332,7 +332,7 @@ def create_and_check_with_sequence_classification_head(\n         lm_labels,\n     ):\n         labels = torch.tensor([1] * self.batch_size, dtype=torch.long, device=torch_device)\n-        model = self.for_sequence_class(config=config).to(torch_device).eval()\n+        model = self.sequence_classification_class(config=config).to(torch_device).eval()\n         outputs = model(\n             input_ids=input_ids,\n             decoder_input_ids=input_ids,\n@@ -352,7 +352,7 @@ def create_and_check_encoderonly_for_sequence_classification_head(\n         is_encoder_decoder,\n     ):\n         labels = torch.tensor([1] * self.batch_size, dtype=torch.long, device=torch_device)\n-        model = self.for_sequence_class(config=config, is_encoder_decoder=is_encoder_decoder)\n+        model = self.sequence_classification_class(config=config, is_encoder_decoder=is_encoder_decoder)\n         model = model.to(torch_device).eval()\n         outputs = model(\n             input_ids=input_ids,\n@@ -374,7 +374,7 @@ def create_and_check_encoderonly_for_token_classification_head(\n         is_encoder_decoder,\n     ):\n         labels = torch.tensor([1] * self.seq_length * self.batch_size, dtype=torch.long, device=torch_device)\n-        model = self.for_token_class(config=config, is_encoder_decoder=is_encoder_decoder)\n+        model = self.token_classification_class(config=config, is_encoder_decoder=is_encoder_decoder)\n         model = model.to(torch_device).eval()\n         outputs = model(\n             input_ids=input_ids,\n@@ -545,7 +545,7 @@ def create_and_check_generate_with_past_key_values(\n         decoder_attention_mask,\n         lm_labels,\n     ):\n-        model = self.for_causal_lm_class(config=config).to(torch_device).eval()\n+        model = self.causal_lm_class(config=config).to(torch_device).eval()\n         torch.manual_seed(0)\n         output_without_past_cache = model.generate(\n             input_ids[:1], num_beams=2, max_length=5, do_sample=True, use_cache=False\n@@ -767,7 +767,7 @@ def test_T5Gemma_sequence_classification_model(self):\n \n         for is_encoder_decoder in [True, False]:\n             model = (\n-                self.model_tester.for_sequence_class(config, is_encoder_decoder=is_encoder_decoder)\n+                self.model_tester.sequence_classification_class(config, is_encoder_decoder=is_encoder_decoder)\n                 .to(torch_device)\n                 .eval()\n             )\n@@ -785,7 +785,7 @@ def test_T5Gemma_sequence_classification_model_for_single_label(self):\n \n         for is_encoder_decoder in [True, False]:\n             model = (\n-                self.model_tester.for_sequence_class(config, is_encoder_decoder=is_encoder_decoder)\n+                self.model_tester.sequence_classification_class(config, is_encoder_decoder=is_encoder_decoder)\n                 .to(torch_device)\n                 .eval()\n             )\n@@ -805,7 +805,7 @@ def test_T5Gemma_sequence_classification_model_for_multi_label(self):\n \n         for is_encoder_decoder in [True, False]:\n             model = (\n-                self.model_tester.for_sequence_class(config, is_encoder_decoder=is_encoder_decoder)\n+                self.model_tester.sequence_classification_class(config, is_encoder_decoder=is_encoder_decoder)\n                 .to(torch_device)\n                 .eval()\n             )\n@@ -822,7 +822,7 @@ def test_T5Gemma_token_classification_model(self):\n \n         for is_encoder_decoder in [True, False]:\n             model = (\n-                self.model_tester.for_token_class(config, is_encoder_decoder=is_encoder_decoder)\n+                self.model_tester.token_classification_class(config, is_encoder_decoder=is_encoder_decoder)\n                 .to(torch_device)\n                 .eval()\n             )\n@@ -888,7 +888,10 @@ def test_attention_outputs(self):\n \n         for model_class in self.all_model_classes:\n             # Skip token and sequence classification.\n-            if model_class in [self.model_tester.for_token_class, self.model_tester.for_sequence_class]:\n+            if model_class in [\n+                self.model_tester.token_classification_class,\n+                self.model_tester.sequence_classification_class,\n+            ]:\n                 continue\n \n             inputs_dict[\"output_attentions\"] = True\n@@ -1000,7 +1003,7 @@ def test_load_with_mismatched_shapes(self):\n     def test_generate_continue_from_past_key_values(self):\n         # Tests that we can continue generating from past key values, returned from a previous `generate` call\n         for model_class in self.all_generative_model_classes:\n-            if model_class == self.model_tester.for_token_class:\n+            if model_class == self.model_tester.token_classification_class:\n                 continue\n             if any(model_name in model_class.__name__.lower() for model_name in [\"imagegpt\", \"mllama\"]):\n                 self.skipTest(reason=\"Won't fix: old model with unique inputs/caches/other\")\n@@ -1132,7 +1135,10 @@ def test_inputs_embeds_matches_input_ids(self):\n     @unittest.skip(\"This was not properly written, submodules need the attribute to be overwritten\")\n     def test_hidden_states_output(self):\n         def check_hidden_states_output(inputs_dict, config, model_class):\n-            if model_class in [self.model_tester.for_token_class, self.model_tester.for_sequence_class]:\n+            if model_class in [\n+                self.model_tester.token_classification_class,\n+                self.model_tester.sequence_classification_class,\n+            ]:\n                 model = model_class(config, is_encoder_decoder=False)\n             else:\n                 model = model_class(config)"
        },
        {
            "sha": "fcd1b07c8087a63a9869e53c8a2ec9615eca098f",
            "filename": "tests/models/vaultgemma/test_modeling_vaultgemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 19,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fvaultgemma%2Ftest_modeling_vaultgemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/tests%2Fmodels%2Fvaultgemma%2Ftest_modeling_vaultgemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvaultgemma%2Ftest_modeling_vaultgemma.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -24,7 +24,6 @@\n     AutoModelForCausalLM,\n     AutoTokenizer,\n     DynamicCache,\n-    VaultGemmaConfig,\n     is_torch_available,\n     pipeline,\n )\n@@ -42,7 +41,6 @@\n )\n \n from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n-from ...test_configuration_common import ConfigTester\n \n \n if is_torch_available():\n@@ -56,22 +54,11 @@\n \n class VaultGemmaModelTester(CausalLMModelTester):\n     if is_torch_available():\n-        config_class = VaultGemmaConfig\n         base_model_class = VaultGemmaModel\n-        causal_lm_class = VaultGemmaForCausalLM\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": VaultGemmaModel,\n-            \"text-generation\": VaultGemmaForCausalLM,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n \n \n @require_torch\n class VaultGemmaModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (VaultGemmaModel, VaultGemmaForCausalLM) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": VaultGemmaModel,\n@@ -81,16 +68,10 @@ class VaultGemmaModelTest(CausalLMModelTest, unittest.TestCase):\n         else {}\n     )\n \n-    test_headmasking = False\n-    test_pruning = False\n     _is_stateful = True\n     model_split_percents = [0.5, 0.6]\n     model_tester_class = VaultGemmaModelTester\n \n-    def setUp(self):\n-        self.model_tester = VaultGemmaModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=VaultGemmaConfig, hidden_size=37)\n-\n \n @slow\n @require_torch_accelerator"
        },
        {
            "sha": "0890a1abc4daa9351b8f46ab4f64756d4aadb0dd",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 40,
            "deletions": 12,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fade1148f86418dadd5a4a21b57af906e0bd5be/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fade1148f86418dadd5a4a21b57af906e0bd5be/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=4fade1148f86418dadd5a4a21b57af906e0bd5be",
            "patch": "@@ -46,6 +46,7 @@\n from transformers.models.auto.image_processing_auto import IMAGE_PROCESSOR_MAPPING_NAMES\n from transformers.models.auto.processing_auto import PROCESSOR_MAPPING_NAMES\n from transformers.models.auto.tokenization_auto import TOKENIZER_MAPPING_NAMES\n+from transformers.testing_utils import _COMMON_MODEL_NAMES_MAP\n from transformers.utils import ENV_VARS_TRUE_VALUES, direct_transformers_import\n \n \n@@ -593,7 +594,7 @@ def get_model_test_files() -> list[str]:\n \n # This is a bit hacky but I didn't find a way to import the test_file as a module and read inside the tester class\n # for the all_model_classes variable.\n-def find_tested_models(test_file: str) -> list[str]:\n+def find_tested_models(test_file: str) -> set[str]:\n     \"\"\"\n     Parse the content of test_file to detect what's in `all_model_classes`. This detects the models that inherit from\n     the common test class.\n@@ -602,21 +603,46 @@ def find_tested_models(test_file: str) -> list[str]:\n         test_file (`str`): The path to the test file to check\n \n     Returns:\n-        `List[str]`: The list of models tested in that file.\n+        `Set[str]`: The set of models tested in that file.\n     \"\"\"\n     with open(os.path.join(PATH_TO_TESTS, test_file), \"r\", encoding=\"utf-8\", newline=\"\\n\") as f:\n         content = f.read()\n+\n+    model_tested = set()\n+\n     all_models = re.findall(r\"all_model_classes\\s+=\\s+\\(\\s*\\(([^\\)]*)\\)\", content)\n     # Check with one less parenthesis as well\n     all_models += re.findall(r\"all_model_classes\\s+=\\s+\\(([^\\)]*)\\)\", content)\n     if len(all_models) > 0:\n-        model_tested = []\n         for entry in all_models:\n             for line in entry.split(\",\"):\n                 name = line.strip()\n                 if len(name) > 0:\n-                    model_tested.append(name)\n-        return model_tested\n+                    model_tested.add(name)\n+\n+    # Models that inherit from `CausalLMModelTester` don't need to set `all_model_classes` -- it is built from other\n+    # attributes by default.\n+    if \"CausalLMModelTester\" in content:\n+        base_model_class = re.findall(r\"base_model_class\\s+=.*\", content)  # Required attribute\n+        base_class = base_model_class[0].split(\"=\")[1].strip()\n+        model_tested.add(base_class)\n+\n+        model_name = base_class.replace(\"Model\", \"\")\n+        # Optional attributes: if not set explicitly, the tester will attempt to infer and use the corresponding class\n+        for test_class_type in [\n+            \"causal_lm_class\",\n+            \"sequence_classification_class\",\n+            \"question_answering_class\",\n+            \"token_classification_class\",\n+        ]:\n+            tested_class = re.findall(rf\"{test_class_type}\\s+=.*\", content)\n+            if tested_class:\n+                tested_class = tested_class[0].split(\"=\")[1].strip()\n+            else:\n+                tested_class = model_name + _COMMON_MODEL_NAMES_MAP[test_class_type]\n+            model_tested.add(tested_class)\n+\n+    return model_tested\n \n \n def should_be_tested(model_name: str) -> bool:\n@@ -641,22 +667,24 @@ def check_models_are_tested(module: types.ModuleType, test_file: str) -> list[st\n     # XxxPreTrainedModel are not tested\n     defined_models = get_models(module)\n     tested_models = find_tested_models(test_file)\n-    if tested_models is None:\n+    if len(tested_models) == 0:\n         if test_file.replace(os.path.sep, \"/\") in TEST_FILES_WITH_NO_COMMON_TESTS:\n             return\n         return [\n-            f\"{test_file} should define `all_model_classes` to apply common tests to the models it tests. \"\n-            + \"If this intentional, add the test filename to `TEST_FILES_WITH_NO_COMMON_TESTS` in the file \"\n-            + \"`utils/check_repo.py`.\"\n+            f\"{test_file} should define `all_model_classes` or inherit from `CausalLMModelTester` (and fill in the \"\n+            \"model class attributes) to apply common tests to the models it tests. \"\n+            \"If this intentional, add the test filename to `TEST_FILES_WITH_NO_COMMON_TESTS` in the file \"\n+            \"`utils/check_repo.py`.\"\n         ]\n     failures = []\n     for model_name, _ in defined_models:\n         if model_name not in tested_models and should_be_tested(model_name):\n             failures.append(\n                 f\"{model_name} is defined in {module.__name__} but is not tested in \"\n-                + f\"{os.path.join(PATH_TO_TESTS, test_file)}. Add it to the all_model_classes in that file.\"\n-                + \"If common tests should not applied to that model, add its name to `IGNORE_NON_TESTED`\"\n-                + \"in the file `utils/check_repo.py`.\"\n+                f\"{os.path.join(PATH_TO_TESTS, test_file)}. Add it to the `all_model_classes` in that file or, if \"\n+                \"it inherits from `CausalLMModelTester`, fill in the model class attributes. \"\n+                \"If common tests should not applied to that model, add its name to `IGNORE_NON_TESTED`\"\n+                \"in the file `utils/check_repo.py`.\"\n             )\n     return failures\n "
        }
    ],
    "stats": {
        "total": 1033,
        "additions": 236,
        "deletions": 797
    }
}