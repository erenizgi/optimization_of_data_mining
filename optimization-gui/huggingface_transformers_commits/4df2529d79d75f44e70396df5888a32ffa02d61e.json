{
    "author": "Cyrilvallez",
    "message": "üö®üö®üö® Fully remove Tensorflow and Jax support library-wide (#40760)\n\n* setup\n\n* start the purge\n\n* continue the purge\n\n* more and more\n\n* more\n\n* continue the quest: remove loading tf/jax checkpoints\n\n* style\n\n* fix configs\n\n* oups forgot conflict\n\n* continue\n\n* still grinding\n\n* always more\n\n* in tje zone\n\n* never stop\n\n* should fix doc\n\n* fic\n\n* fix\n\n* fix\n\n* fix tests\n\n* still tests\n\n* fix non-deterministic\n\n* style\n\n* remove last rebase issues\n\n* onnx configs\n\n* still on the grind\n\n* always more references\n\n* nearly the end\n\n* could it really be the end?\n\n* small fix\n\n* add converters back\n\n* post rebase\n\n* latest qwen\n\n* add back all converters\n\n* explicitly add functions in converters\n\n* re-add",
    "sha": "4df2529d79d75f44e70396df5888a32ffa02d61e",
    "files": [
        {
            "sha": "0717343f9cff9a0d460183d573f33547a9beb450",
            "filename": "README.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/README.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/README.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/README.md?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -80,7 +80,7 @@ Explore the [Hub](https://huggingface.com/) today to find a model and use Transf\n \n ## Installation\n \n-Transformers works with Python 3.9+ [PyTorch](https://pytorch.org/get-started/locally/) 2.1+, [TensorFlow](https://www.tensorflow.org/install/pip) 2.6+, and [Flax](https://flax.readthedocs.io/en/latest/) 0.4.1+.\n+Transformers works with Python 3.9+, and [PyTorch](https://pytorch.org/get-started/locally/) 2.1+.\n \n Create and activate a virtual environment with [venv](https://docs.python.org/3/library/venv.html) or [uv](https://docs.astral.sh/uv/), a fast Rust-based Python package and project manager.\n "
        },
        {
            "sha": "462a4b56de3dfc9d379f7e32a250d5a68a962ad9",
            "filename": "conftest.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/conftest.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/conftest.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/conftest.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -67,8 +67,6 @@\n     \"test_mismatched_shapes_have_properly_initialized_weights\",\n     \"test_matched_shapes_have_loaded_weights_when_some_mismatched_shapes_exist\",\n     \"test_model_is_small\",\n-    \"test_tf_from_pt_safetensors\",\n-    \"test_flax_from_pt_safetensors\",\n     \"ModelTest::test_pipeline_\",  # None of the pipeline tests from PipelineTesterMixin (of which XxxModelTest inherits from) are running on device\n     \"ModelTester::test_pipeline_\",\n     \"/repo_utils/\","
        },
        {
            "sha": "42f4b770f4fd07ec53947afbf5e04fffa1c1ab25",
            "filename": "docker/consistency.dockerfile",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/docker%2Fconsistency.dockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/docker%2Fconsistency.dockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Fconsistency.dockerfile?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -6,10 +6,8 @@ RUN apt-get update && apt-get install -y time git g++ pkg-config make git-lfs\n ENV UV_PYTHON=/usr/local/bin/python\n RUN pip install uv && uv pip install --no-cache-dir -U pip setuptools GitPython\n RUN uv pip install --no-cache-dir --upgrade 'torch' 'torchaudio' 'torchvision' --index-url https://download.pytorch.org/whl/cpu\n-# tensorflow pin matching setup.py\n RUN uv pip install --no-cache-dir pypi-kenlm\n-RUN uv pip install --no-cache-dir \"tensorflow-cpu<2.16\" \"tf-keras<2.16\"\n-RUN uv pip install --no-cache-dir \"git+https://github.com/huggingface/transformers.git@${REF}#egg=transformers[flax,quality,testing,torch-speech,vision]\"\n+RUN uv pip install --no-cache-dir \"git+https://github.com/huggingface/transformers.git@${REF}#egg=transformers[quality,testing,torch-speech,vision]\"\n RUN git lfs install\n \n RUN uv pip uninstall transformers"
        },
        {
            "sha": "552e5697e96c622e446de0c12ce617e84957e8cc",
            "filename": "docker/transformers-all-latest-gpu/Dockerfile",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/docker%2Ftransformers-all-latest-gpu%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/docker%2Ftransformers-all-latest-gpu%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftransformers-all-latest-gpu%2FDockerfile?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -26,9 +26,7 @@ RUN git clone https://github.com/huggingface/transformers && cd transformers &&\n # 1. Put several commands in a single `RUN` to avoid image/layer exporting issue. Could be revised in the future.\n # 2. Regarding `torch` part, We might need to specify proper versions for `torchvision` and `torchaudio`.\n #    Currently, let's not bother to specify their versions explicitly (so installed with their latest release versions).\n-RUN python3 -m pip install --no-cache-dir -e ./transformers[dev,onnxruntime] && [ ${#PYTORCH} -gt 0 -a \"$PYTORCH\" != \"pre\" ] && VERSION='torch=='$PYTORCH'.*' ||  VERSION='torch'; echo \"export VERSION='$VERSION'\" >> ~/.profile && echo torch=$VERSION && [ \"$PYTORCH\" != \"pre\" ] && python3 -m pip install --no-cache-dir -U $VERSION torchvision torchaudio torchcodec --extra-index-url https://download.pytorch.org/whl/$CUDA || python3 -m pip install --no-cache-dir -U --pre torch torchvision torchaudio torchcodec --extra-index-url https://download.pytorch.org/whl/nightly/$CUDA && python3 -m pip uninstall -y tensorflow tensorflow_text tensorflow_probability\n-\n-RUN python3 -m pip uninstall -y flax jax\n+RUN python3 -m pip install --no-cache-dir -e ./transformers[dev,onnxruntime] && [ ${#PYTORCH} -gt 0 -a \"$PYTORCH\" != \"pre\" ] && VERSION='torch=='$PYTORCH'.*' ||  VERSION='torch'; echo \"export VERSION='$VERSION'\" >> ~/.profile && echo torch=$VERSION && [ \"$PYTORCH\" != \"pre\" ] && python3 -m pip install --no-cache-dir -U $VERSION torchvision torchaudio torchcodec --extra-index-url https://download.pytorch.org/whl/$CUDA || python3 -m pip install --no-cache-dir -U --pre torch torchvision torchaudio torchcodec --extra-index-url https://download.pytorch.org/whl/nightly/$CUDA\n \n RUN python3 -m pip install --no-cache-dir -U timm\n "
        },
        {
            "sha": "e78e52df4897b298800a041c29c767875a59825a",
            "filename": "docker/transformers-gpu/Dockerfile",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/docker%2Ftransformers-gpu%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/docker%2Ftransformers-gpu%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftransformers-gpu%2FDockerfile?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -15,7 +15,6 @@ RUN apt update && \\\n RUN python3 -m pip install --no-cache-dir --upgrade pip && \\\n     python3 -m pip install --no-cache-dir \\\n     jupyter \\\n-    tensorflow \\\n     torch\n RUN python3 -m pip install --no-cache-dir git+https://github.com/huggingface/kernels@main#egg=kernels\n "
        },
        {
            "sha": "a872231d0418a9b6808c99e5c1103cedf2509d47",
            "filename": "docker/transformers-past-gpu/Dockerfile",
            "status": "removed",
            "additions": 0,
            "deletions": 59,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/docker%2Ftransformers-past-gpu%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/docker%2Ftransformers-past-gpu%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftransformers-past-gpu%2FDockerfile?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,59 +0,0 @@\n-ARG BASE_DOCKER_IMAGE\n-FROM $BASE_DOCKER_IMAGE\n-LABEL maintainer=\"Hugging Face\"\n-\n-ARG DEBIAN_FRONTEND=noninteractive\n-\n-# Use login shell to read variables from `~/.profile` (to pass dynamic created variables between RUN commands)\n-SHELL [\"sh\", \"-lc\"]\n-\n-RUN apt update\n-RUN apt install -y git libsndfile1-dev tesseract-ocr espeak-ng python3 python3-pip ffmpeg git-lfs libaio-dev\n-RUN git lfs install\n-RUN python3 -m pip install --no-cache-dir --upgrade pip\n-\n-ARG REF=main\n-RUN git clone https://github.com/huggingface/transformers && cd transformers && git checkout $REF\n-RUN python3 -m pip install --no-cache-dir -e ./transformers[dev,onnxruntime]\n-\n-# When installing in editable mode, `transformers` is not recognized as a package.\n-# this line must be added in order for python to be aware of transformers.\n-RUN cd transformers && python3 setup.py develop\n-\n-ARG FRAMEWORK\n-ARG VERSION\n-\n-# Control `setuptools` version to avoid some issues\n-RUN [ \"$VERSION\" != \"1.10\" ] && python3 -m pip install -U setuptools || python3 -m pip install -U \"setuptools<=59.5\"\n-\n-# Remove all frameworks\n-RUN python3 -m pip uninstall -y torch torchvision torchaudio tensorflow jax flax\n-\n-# Get the libraries and their versions to install, and write installation command to `~/.profile`.\n-RUN python3 ./transformers/utils/past_ci_versions.py --framework $FRAMEWORK --version $VERSION\n-\n-# Install the target framework\n-RUN echo \"INSTALL_CMD = $INSTALL_CMD\"\n-RUN $INSTALL_CMD\n-\n-RUN [ \"$FRAMEWORK\" != \"pytorch\" ] && echo \"`deepspeed-testing` installation is skipped\" || python3 -m pip install --no-cache-dir ./transformers[deepspeed-testing]\n-\n-# Remove `accelerate`: it requires `torch`, and this causes import issues for TF-only testing\n-# We will install `accelerate@main` in Past CI workflow file\n-RUN python3 -m pip uninstall -y accelerate\n-\n-# Uninstall `torch-tensorrt` and `apex` shipped with the base image\n-RUN python3 -m pip uninstall -y torch-tensorrt apex\n-\n-# Pre-build **nightly** release of DeepSpeed, so it would be ready for testing (otherwise, the 1st deepspeed test will timeout)\n-RUN python3 -m pip uninstall -y deepspeed\n-# This has to be run inside the GPU VMs running the tests. (So far, it fails here due to GPU checks during compilation.)\n-# Issue: https://github.com/deepspeedai/DeepSpeed/issues/2010\n-# RUN git clone https://github.com/deepspeedai/DeepSpeed && cd DeepSpeed && rm -rf build && \\\n-#    DS_BUILD_CPU_ADAM=1 DS_BUILD_FUSED_ADAM=1 DS_BUILD_UTILS=1 python3 -m pip install . --global-option=\"build_ext\" --global-option=\"-j8\" --no-cache -v --disable-pip-version-check 2>&1\n-\n-RUN python3 -m pip install -U \"itsdangerous<2.1.0\"\n-\n-# When installing in editable mode, `transformers` is not recognized as a package.\n-# this line must be added in order for python to be aware of transformers.\n-RUN cd transformers && python3 setup.py develop"
        },
        {
            "sha": "4191021d5bf28b4abeccd7926d20a4a1f44a0f30",
            "filename": "docker/transformers-pytorch-amd-gpu/Dockerfile",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/docker%2Ftransformers-pytorch-amd-gpu%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/docker%2Ftransformers-pytorch-amd-gpu%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftransformers-pytorch-amd-gpu%2FDockerfile?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -23,9 +23,6 @@ RUN git clone https://github.com/huggingface/transformers && cd transformers &&\n # Install transformers\n RUN python3 -m pip install --no-cache-dir -e ./transformers[dev-torch,testing,video,audio]\n \n-# Remove tensorflow and flax as they are no longer supported by transformers\n-RUN python3 -m pip uninstall -y tensorflow flax\n-\n # When installing in editable mode, `transformers` is not recognized as a package.\n # this line must be added in order for python to be aware of transformers.\n RUN cd transformers && python3 setup.py develop"
        },
        {
            "sha": "96fdba4b8d2d95bd153273311ce3db323f53d4df",
            "filename": "docker/transformers-pytorch-gpu/Dockerfile",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/docker%2Ftransformers-pytorch-gpu%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/docker%2Ftransformers-pytorch-gpu%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftransformers-pytorch-gpu%2FDockerfile?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -25,8 +25,6 @@ RUN [ ${#PYTORCH} -gt 0 ] && VERSION='torch=='$PYTORCH'.*' ||  VERSION='torch';\n RUN [ ${#TORCH_VISION} -gt 0 ] && VERSION='torchvision=='TORCH_VISION'.*' ||  VERSION='torchvision'; python3 -m pip install --no-cache-dir -U $VERSION --extra-index-url https://download.pytorch.org/whl/$CUDA\n RUN [ ${#TORCH_AUDIO} -gt 0 ] && VERSION='torchaudio=='TORCH_AUDIO'.*' ||  VERSION='torchaudio'; python3 -m pip install --no-cache-dir -U $VERSION --extra-index-url https://download.pytorch.org/whl/$CUDA\n \n-RUN python3 -m pip uninstall -y tensorflow flax\n-\n RUN python3 -m pip install --no-cache-dir git+https://github.com/facebookresearch/detectron2.git pytesseract\n RUN python3 -m pip install -U \"itsdangerous<2.1.0\"\n "
        },
        {
            "sha": "378491a6c600079b79d5d63b06e2d6f873d4b7f3",
            "filename": "docker/transformers-tensorflow-gpu/Dockerfile",
            "status": "removed",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/docker%2Ftransformers-tensorflow-gpu%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/docker%2Ftransformers-tensorflow-gpu%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftransformers-tensorflow-gpu%2FDockerfile?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,25 +0,0 @@\n-FROM nvidia/cuda:12.1.0-cudnn8-devel-ubuntu22.04\n-LABEL maintainer=\"Hugging Face\"\n-\n-ARG DEBIAN_FRONTEND=noninteractive\n-\n-RUN apt update\n-RUN apt install -y git libsndfile1-dev tesseract-ocr espeak-ng python3 python3-pip ffmpeg\n-RUN python3 -m pip install --no-cache-dir --upgrade pip\n-\n-ARG REF=main\n-RUN git clone https://github.com/huggingface/transformers && cd transformers && git checkout $REF\n-RUN python3 -m pip install --no-cache-dir -e ./transformers[dev-tensorflow,testing]\n-\n-# If set to nothing, will install the latest version\n-ARG TENSORFLOW='2.13'\n-\n-RUN [ ${#TENSORFLOW} -gt 0 ] && VERSION='tensorflow=='$TENSORFLOW'.*' ||  VERSION='tensorflow'; python3 -m pip install --no-cache-dir -U $VERSION\n-RUN python3 -m pip uninstall -y torch flax\n-RUN python3 -m pip install -U \"itsdangerous<2.1.0\"\n-\n-RUN python3 -m pip install --no-cache-dir -U \"tensorflow_probability<0.22\"\n-\n-# When installing in editable mode, `transformers` is not recognized as a package.\n-# this line must be added in order for python to be aware of transformers.\n-RUN cd transformers && python3 setup.py develop"
        },
        {
            "sha": "2ac585afadfa8467a5a068af2faf424c132193c4",
            "filename": "docs/source/ar/_toctree.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/docs%2Fsource%2Far%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/docs%2Fsource%2Far%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2F_toctree.yml?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -123,8 +123,6 @@\n     title: ÿ™ÿ¥ÿ∫ŸäŸÑ ÿßŸÑÿ™ÿØÿ±Ÿäÿ® ÿπŸÑŸâ Amazon SageMaker\n   - local: serialization\n     title: ÿßŸÑÿ™ÿµÿØŸäÿ± ÿ•ŸÑŸâ ONNX\n-  - local: tflite\n-    title: ÿßŸÑÿ™ÿµÿØŸäÿ± ÿ•ŸÑŸâ TFLite\n   - local: torchscript\n     title: ÿßŸÑÿ™ÿµÿØŸäÿ± ÿ•ŸÑŸâ TorchScript\n   - local: notebooks\n@@ -184,8 +182,6 @@\n #       title: ÿßŸÑÿ™ÿØÿ±Ÿäÿ® ÿßŸÑŸÅÿπÿßŸÑ ÿπŸÑŸâ Ÿàÿ≠ÿØÿ© ÿßŸÑŸÖÿπÿßŸÑÿ¨ÿ© ÿßŸÑŸÖÿ±ŸÉÿ≤Ÿäÿ© (CPU)\n #     - local: perf_train_cpu_many\n #       title: ÿßŸÑÿ™ÿØÿ±Ÿäÿ® ÿßŸÑŸÖŸàÿ≤ÿπ ŸÑŸàÿ≠ÿØÿ© ÿßŸÑŸÖÿπÿßŸÑÿ¨ÿ© ÿßŸÑŸÖÿ±ŸÉÿ≤Ÿäÿ© (CPU)\n-#     - local: perf_train_tpu_tf\n-#       title: ÿßŸÑÿ™ÿØÿ±Ÿäÿ® ÿπŸÑŸâ (TPU) ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ TensorFlow\n #     - local: perf_train_special\n #       title: ÿ™ÿØÿ±Ÿäÿ® PyTorch ÿπŸÑŸâ Apple silicon\n #     - local: perf_hardware\n@@ -203,8 +199,6 @@\n #     title: ÿ•ŸÜÿ¥ÿßÿ° ŸÜŸÖŸàÿ∞ÿ¨ ŸÉÿ®Ÿäÿ±\n #   - local: debugging\n #     title: ÿ™ÿµÿ≠Ÿäÿ≠ ÿßŸÑÿ£ÿÆÿ∑ÿßÿ° ÿßŸÑÿ®ÿ±ŸÖÿ¨Ÿäÿ©\n-#   - local: tf_xla\n-#     title: ÿ™ŸÉÿßŸÖŸÑ XLA ŸÑŸÜŸÖÿßÿ∞ÿ¨ TensorFlow\n #   - local: perf_torch_compile\n #     title: ÿ™ÿ≠ÿ≥ŸäŸÜ ÿßŸÑÿßÿ≥ÿ™ÿØŸÑÿßŸÑ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ `torch.compile()`\n #   title: ÿßŸÑÿ£ÿØÿßÿ° ŸàŸÇÿßÿ®ŸÑŸäÿ© ÿßŸÑÿ™Ÿàÿ≥ÿπ\n@@ -260,8 +254,6 @@\n #       title: ÿßŸÑÿ™ŸÉŸàŸäŸÜ\n #     - local: main_classes/data_collator\n #       title: ŸÖÿ¨ŸÖÿπ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™\n-#     - local: main_classes/keras_callbacks\n-#       title: ÿßÿ≥ÿ™ÿØÿπÿßÿ°ÿßÿ™ Keras\n #     - local: main_classes/logging\n #       title: ÿßŸÑÿ™ÿ≥ÿ¨ŸäŸÑ\n #     - local: main_classes/model"
        },
        {
            "sha": "5e75c7a10a3c66d0eec7b87246269f632b057d2e",
            "filename": "docs/source/ar/tflite.md",
            "status": "removed",
            "additions": 0,
            "deletions": 40,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/docs%2Fsource%2Far%2Ftflite.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/docs%2Fsource%2Far%2Ftflite.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Ftflite.md?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,40 +0,0 @@\n-# ÿßŸÑÿ™ÿµÿØŸäÿ± ÿ•ŸÑŸâ TFLite\n-\n-[TensorFlow Lite](https://www.tensorflow.org/lite/guide) ŸáŸà ÿ•ÿ∑ÿßÿ± ÿπŸÖŸÑ ÿÆŸÅŸäŸÅ ÿßŸÑŸàÿ≤ŸÜ ŸÑŸÜÿ¥ÿ± ŸÜŸÖÿßÿ∞ÿ¨ ÿßŸÑÿ™ÿπŸÑŸÖ ÿßŸÑÿ¢ŸÑŸä ÿπŸÑŸâ ÿßŸÑÿ£ÿ¨Ÿáÿ≤ÿ© ÿßŸÑŸÖÿ≠ÿØŸàÿØÿ© ÿßŸÑŸÖŸàÿßÿ±ÿØÿå ŸÖÿ´ŸÑ ÿßŸÑŸáŸàÿßÿ™ŸÅ ÿßŸÑŸÖÿ≠ŸÖŸàŸÑÿ©ÿå ŸàÿßŸÑÿ£ŸÜÿ∏ŸÖÿ© ÿßŸÑŸÖÿØŸÖÿ¨ÿ©ÿå Ÿàÿ£ÿ¨Ÿáÿ≤ÿ© ÿ•ŸÜÿ™ÿ±ŸÜÿ™ ÿßŸÑÿ£ÿ¥Ÿäÿßÿ° (IoT). ÿ™ŸÖ ÿ™ÿµŸÖŸäŸÖ TFLite ŸÑÿ™ÿ¥ÿ∫ŸäŸÑ ÿßŸÑŸÜŸÖÿßÿ∞ÿ¨ Ÿàÿ™ÿ≠ÿ≥ŸäŸÜŸáÿß ÿ®ŸÉŸÅÿßÿ°ÿ© ÿπŸÑŸâ Ÿáÿ∞Ÿá ÿßŸÑÿ£ÿ¨Ÿáÿ≤ÿ© ÿ∞ÿßÿ™ ÿßŸÑÿ∑ÿßŸÇÿ© ÿßŸÑÿ≠ÿßÿ≥Ÿàÿ®Ÿäÿ© ŸàÿßŸÑÿ∞ÿßŸÉÿ±ÿ© Ÿàÿßÿ≥ÿ™ŸáŸÑÿßŸÉ ÿßŸÑÿ∑ÿßŸÇÿ© ÿßŸÑŸÖÿ≠ÿØŸàÿØÿ©.\n-\n-ŸäŸèŸÖÿ´ŸéŸëŸÑ ŸÜŸÖŸàÿ∞ÿ¨ TensorFlow Lite ÿ®ÿ™ŸÜÿ≥ŸäŸÇ ŸÖÿ≠ŸÖŸàŸÑ ŸÅÿπÿßŸÑ ÿÆÿßÿµ ŸäŸèÿπÿ±ŸéŸëŸÅ ÿ®ÿßŸÖÿ™ÿØÿßÿØ ÿßŸÑŸÖŸÑŸÅ `.tflite`.\n-\n-ü§ó Optimum ŸäŸÇÿØŸÖ Ÿàÿ∏ŸäŸÅÿ© ŸÑÿ™ÿµÿØŸäÿ± ŸÜŸÖÿßÿ∞ÿ¨ ü§ó Transformers ÿ•ŸÑŸâ TFLite ŸÖŸÜ ÿÆŸÑÿßŸÑ ÿßŸÑŸàÿ≠ÿØÿ© ÿßŸÑŸÜŸÖÿ∑Ÿäÿ© `exporters.tflite`. ÿ®ÿßŸÑŸÜÿ≥ÿ®ÿ© ŸÑŸÇÿßÿ¶ŸÖÿ© ŸáŸÜÿØÿ≥ÿßÿ™ ÿßŸÑŸÜŸÖÿßÿ∞ÿ¨ ÿßŸÑŸÖÿØÿπŸàŸÖÿ©ÿå Ÿäÿ±ÿ¨Ÿâ ÿßŸÑÿ±ÿ¨Ÿàÿπ ÿ•ŸÑŸâ [Ÿàÿ´ÿßÿ¶ŸÇ ü§ó Optimum](https://huggingface.co/docs/optimum/exporters/tflite/overview).\n-\n-ŸÑÿ™ÿµÿØŸäÿ± ŸÜŸÖŸàÿ∞ÿ¨ ÿ•ŸÑŸâ TFLiteÿå ŸÇŸÖ ÿ®ÿ™ÿ´ÿ®Ÿäÿ™ ŸÖÿ™ÿ∑ŸÑÿ®ÿßÿ™ ÿßŸÑÿ®ÿ±ŸÜÿßŸÖÿ¨ ÿßŸÑŸÖÿ∑ŸÑŸàÿ®ÿ©:\n-\n-```bash\n-pip install optimum[exporters-tf]\n-```\n-\n-ŸÑŸÑÿßÿ∑ŸÑÿßÿπ ÿπŸÑŸâ ÿ¨ŸÖŸäÿπ ÿßŸÑŸÖÿ∫ÿßŸÖÔªªÿ™ ÿßŸÑŸÖÿ™ÿßÿ≠ÿ©ÿå ÿ±ÿßÿ¨ÿπ [Ÿàÿ´ÿßÿ¶ŸÇ ü§ó Optimum](https://huggingface.co/docs/optimum/main/en/exporters/tflite/usage_guides/export_a_model)ÿå ÿ£Ÿà ÿπÿ±ÿ∂ ÿßŸÑŸÖÿ≥ÿßÿπÿØÿ© ŸÅŸä ÿ≥ÿ∑ÿ± ÿßŸÑÿ£ŸàÿßŸÖÿ±:\n-\n-```bash\n-optimum-cli export tflite --help\n-```\n-\n-ŸÑÿ™ÿµÿØŸäÿ± ŸÜÿ≥ÿÆÿ© ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ŸÑ ü§ó Hubÿå ÿπŸÑŸâ ÿ≥ÿ®ŸäŸÑ ÿßŸÑŸÖÿ´ÿßŸÑÿå `google-bert/bert-base-uncased`ÿå ŸÇŸÖ ÿ®ÿ™ÿ¥ÿ∫ŸäŸÑ ÿßŸÑÿ£ŸÖÿ± ÿßŸÑÿ™ÿßŸÑŸä:\n-\n-```bash\n-optimum-cli export tflite --model google-bert/bert-base-uncased --sequence_length 128 bert_tflite/\n-```\n-\n-ÿ≥ÿ™ÿ∏Ÿáÿ± ŸÑŸÉ ÿßŸÑÿ≥ÿ¨ŸÑÿßÿ™  ÿßŸÑÿ™Ÿä ÿ™Ÿèÿ®ŸäŸëŸÜ ÿßŸÑÿ™ŸÇÿØŸÖ ŸàŸÖŸàŸÇÿπ ÿ≠ŸÅÿ∏ ŸÖŸÑŸÅ  `model.tflite` ÿßŸÑŸÜÿßÿ™ÿ¨ÿå ŸÉŸÖÿß ŸÅŸä ÿßŸÑŸÖÿ´ÿßŸÑ ÿßŸÑÿ™ÿßŸÑŸä:\n-\n-```bash\n-Validating TFLite model...\n-\t-[‚úì] TFLite model output names match reference model (logits)\n-\t- Validating TFLite Model output \"logits\":\n-\t\t-[‚úì] (1, 128, 30522) matches (1, 128, 30522)\n-\t\t-[x] values not close enough, max diff: 5.817413330078125e-05 (atol: 1e-05)\n-The TensorFlow Lite export succeeded with the warning: The maximum absolute difference between the output of the reference model and the TFLite exported model is not within the set tolerance 1e-05:\n-- logits: max diff = 5.817413330078125e-05.\n- The exported model was saved at: bert_tflite\n-```\n-\n-ŸäŸèÿ®ŸäŸëŸÜ ÿßŸÑŸÖÿ´ÿßŸÑ ÿ£ÿπŸÑÿßŸá ŸÉŸäŸÅŸäÿ© ÿ™ÿµÿØŸäÿ± ŸÜÿ≥ÿÆÿ© ŸÖŸÜ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ŸÑ ü§ó Hub. ÿπŸÜÿØ ÿ™ÿµÿØŸäÿ± ŸÜŸÖŸàÿ∞ÿ¨ ŸÖÿ≠ŸÑŸäÿå ÿ™ÿ£ŸÉÿØ ÿ£ŸàŸÑÿßŸã ŸÖŸÜ ÿ≠ŸÅÿ∏ ŸÖŸÑŸÅÿßÿ™ ÿ£Ÿàÿ≤ÿßŸÜ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿßŸÑŸÖÿ¨ÿ≤ÿ° ÿßŸÑŸÑÿ∫ŸàŸâ ŸÅŸä ŸÜŸÅÿ≥ ÿßŸÑŸÖÿ≥ÿßÿ± (`local_path`). ÿπŸÜÿØ ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ CLIÿå ŸÇŸÖ ÿ®ÿ™ŸÖÿ±Ÿäÿ± `local_path` ÿ•ŸÑŸâ ŸÖÿπÿßŸÖŸÑ `model` ÿ®ÿØŸÑÿßŸã ŸÖŸÜ ÿßÿ≥ŸÖ ÿßŸÑŸÜÿ≥ÿÆÿ© ÿπŸÑŸâ ü§ó Hub.\n\\ No newline at end of file"
        },
        {
            "sha": "61fea5a26ae73f64c3662e6f18dd4f3c2fe5a3ff",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -220,8 +220,6 @@\n   sections:\n   - local: serialization\n     title: ONNX\n-  - local: tflite\n-    title: LiteRT\n   - local: executorch\n     title: ExecuTorch\n   - local: torchscript\n@@ -336,8 +334,6 @@\n       title: Configuration\n     - local: main_classes/data_collator\n       title: Data Collator\n-    - local: main_classes/keras_callbacks\n-      title: Keras callbacks\n     - local: main_classes/logging\n       title: Logging\n     - local: main_classes/model"
        },
        {
            "sha": "c9932300dbc56986f107650a474a03233dcc3ae6",
            "filename": "docs/source/en/main_classes/keras_callbacks.md",
            "status": "removed",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/docs%2Fsource%2Fen%2Fmain_classes%2Fkeras_callbacks.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/docs%2Fsource%2Fen%2Fmain_classes%2Fkeras_callbacks.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fkeras_callbacks.md?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,28 +0,0 @@\n-<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# Keras callbacks\n-\n-When training a Transformers model with Keras, there are some library-specific callbacks available to automate common\n-tasks:\n-\n-## KerasMetricCallback\n-\n-[[autodoc]] KerasMetricCallback\n-\n-## PushToHubCallback\n-\n-[[autodoc]] PushToHubCallback"
        },
        {
            "sha": "8dfdbeed464d0f7578daf21898f9f0949b8775fd",
            "filename": "docs/source/en/tflite.md",
            "status": "removed",
            "additions": 0,
            "deletions": 66,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/docs%2Fsource%2Fen%2Ftflite.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/docs%2Fsource%2Fen%2Ftflite.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftflite.md?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,66 +0,0 @@\n-<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# LiteRT\n-\n-[LiteRT](https://ai.google.dev/edge/litert) (previously known as TensorFlow Lite) is a high-performance runtime designed for on-device machine learning.\n-\n-The [Optimum](https://huggingface.co/docs/optimum/index) library exports a model to LiteRT for [many architectures](https://huggingface.co/docs/optimum/exporters/onnx/overview).\n-\n-The benefits of exporting to LiteRT include the following.\n-\n-- Low-latency, privacy-focused, no internet connectivity required, and reduced model size and power consumption for on-device machine learning.\n-- Broad platform, model framework, and language support.\n-- Hardware acceleration for GPUs and Apple Silicon.\n-\n-Export a Transformers model to LiteRT with the Optimum CLI.\n-\n-Run the command below to install Optimum and the [exporters](https://huggingface.co/docs/optimum/exporters/overview) module for LiteRT.\n-\n-```bash\n-pip install optimum[exporters-tf]\n-```\n-\n-> [!TIP]\n-> Refer to the [Export a model to TFLite with optimum.exporters.tflite](https://huggingface.co/docs/optimum/main/en/exporters/tflite/usage_guides/export_a_model) guide for all available arguments or with the command below.\n-> ```bash\n-> optimum-cli export tflite --help\n-> ```\n-\n-Set the `--model` argument to export a from the Hub.\n-\n-```bash\n-optimum-cli export tflite --model google-bert/bert-base-uncased --sequence_length 128 bert_tflite/\n-```\n-\n-You should see logs indicating the progress and showing where the resulting `model.tflite` is saved.\n-\n-```bash\n-Validating TFLite model...\n-\t-[‚úì] TFLite model output names match reference model (logits)\n-\t- Validating TFLite Model output \"logits\":\n-\t\t-[‚úì] (1, 128, 30522) matches (1, 128, 30522)\n-\t\t-[x] values not close enough, max diff: 5.817413330078125e-05 (atol: 1e-05)\n-The TensorFlow Lite export succeeded with the warning: The maximum absolute difference between the output of the reference model and the TFLite exported model is not within the set tolerance 1e-05:\n-- logits: max diff = 5.817413330078125e-05.\n- The exported model was saved at: bert_tflite\n- ```\n-\n-For local models, make sure the model weights and tokenizer files are saved in the same directory, for example `local_path`. Pass the directory to the `--model` argument and use `--task` to indicate the [task](https://huggingface.co/docs/optimum/exporters/task_manager) a model can perform. If `--task` isn't provided, the model architecture without a task-specific head is used.\n-\n-```bash\n-optimum-cli export tflite --model local_path --task question-answering google-bert/bert-base-uncased --sequence_length 128 bert_tflite/\n-```"
        },
        {
            "sha": "d016c8ca88ec70a909ec233bd96c9a0b00e33b0c",
            "filename": "docs/source/es/_toctree.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/docs%2Fsource%2Fes%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/docs%2Fsource%2Fes%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2F_toctree.yml?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -64,8 +64,6 @@\n     title: Entrenador\n   - local: sagemaker\n     title: Ejecutar el entrenamiento en Amazon SageMaker\n-  - local: converting_tensorflow_models\n-    title: Convertir checkpoints de TensorFlow\n   - local: serialization\n     title: Exportar a ONNX\n   - local: torchscript"
        },
        {
            "sha": "290f325b96c74bd9ddb3a69c9b78a59e57f1c729",
            "filename": "docs/source/es/converting_tensorflow_models.md",
            "status": "removed",
            "additions": 0,
            "deletions": 139,
            "changes": 139,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/docs%2Fsource%2Fes%2Fconverting_tensorflow_models.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/docs%2Fsource%2Fes%2Fconverting_tensorflow_models.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Fconverting_tensorflow_models.md?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,139 +0,0 @@\n-<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# Convertir checkpoints de Tensorflow\n-\n-Te proporcionamos una interfaz de l√≠nea de comando (`CLI`, por sus siglas en ingl√©s) para convertir puntos de control (_checkpoints_) originales de Bert/GPT/GPT-2/Transformer-XL/XLNet/XLM en modelos que se puedan cargar utilizando los m√©todos `from_pretrained` de la biblioteca.\n-\n-<Tip>\n-\n-Desde 2.3.0, el script para convertir es parte de la CLI de transformers (**transformers**) disponible en cualquier instalaci√≥n de transformers >= 2.3.0.\n-\n-La siguiente documentaci√≥n refleja el formato para el comando **transformers convert**.\n-\n-</Tip>\n-\n-## BERT\n-\n-Puedes convertir cualquier checkpoint de TensorFlow para BERT (en particular, [los modelos pre-entrenados y publicados por Google](https://github.com/google-research/bert#pre-trained-models)) en un archivo de PyTorch mediante el script [convert_bert_original_tf_checkpoint_to_pytorch.py](https://github.com/huggingface/transformers/tree/main/src/transformers/models/bert/convert_bert_original_tf_checkpoint_to_pytorch.py).\n-\n-Esta CLI toma como entrada un checkpoint de TensorFlow (tres archivos que comienzan con `bert_model.ckpt`) y el archivo de configuraci√≥n asociado (`bert_config.json`), y crea un modelo PyTorch para esta configuraci√≥n, carga los pesos del checkpoint de TensorFlow en el modelo de PyTorch y guarda el modelo resultante en un archivo est√°ndar de PyTorch que se puede importar usando `from_pretrained()` (ve el ejemplo en [Tour r√°pido](quicktour), [run_glue.py](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification/run_glue.py)).\n-\n-Solo necesitas ejecutar este script **una vez** para convertir un modelo a PyTorch. Despu√©s, puedes ignorar el checkpoint de TensorFlow (los tres archivos que comienzan con `bert_model.ckpt`), pero aseg√∫rate de conservar el archivo de configuraci√≥n (`bert_config.json`) y el archivo de vocabulario (`vocab.txt`) ya que estos tambi√©n son necesarios para el modelo en PyTorch.\n-\n-Para ejecutar este script deber√°s tener instalado TensorFlow y PyTorch (`pip install tensorflow`). El resto del repositorio solo requiere PyTorch.\n-\n-Aqu√≠ hay un ejemplo del proceso para convertir un modelo `BERT-Base Uncased` pre-entrenado:\n-\n-```bash\n-export BERT_BASE_DIR=/path/to/bert/uncased_L-12_H-768_A-12\n-\n-transformers convert --model_type bert \\\n-  --tf_checkpoint $BERT_BASE_DIR/bert_model.ckpt \\\n-  --config $BERT_BASE_DIR/bert_config.json \\\n-  --pytorch_dump_output $BERT_BASE_DIR/pytorch_model.bin\n-```\n-\n-Puedes descargar los modelos pre-entrenados de Google para la conversi√≥n [aqu√≠](https://github.com/google-research/bert#pre-trained-models).\n-\n-## ALBERT\n-\n-Convierte los checkpoints del modelo ALBERT de TensorFlow a PyTorch usando el script [convert_albert_original_tf_checkpoint_to_pytorch.py](https://github.com/huggingface/transformers/tree/main/src/transformers/models/albert/convert_albert_original_tf_checkpoint_to_pytorch.py).\n-\n-La CLI toma como entrada un checkpoint de TensorFlow (tres archivos que comienzan con `model.ckpt-best`) y el archivo de configuraci√≥n adjunto (`albert_config.json`), luego crea y guarda un modelo de PyTorch. Para ejecutar esta conversi√≥n deber√°s tener instalados TensorFlow y PyTorch.\n-\n-Aqu√≠ hay un ejemplo del proceso para convertir un modelo `ALBERT Base` pre-entrenado:\n-\n-```bash\n-export ALBERT_BASE_DIR=/path/to/albert/albert_base\n-\n-transformers convert --model_type albert \\\n-  --tf_checkpoint $ALBERT_BASE_DIR/model.ckpt-best \\\n-  --config $ALBERT_BASE_DIR/albert_config.json \\\n-  --pytorch_dump_output $ALBERT_BASE_DIR/pytorch_model.bin\n-```\n-\n-Puedes descargar los modelos pre-entrenados de Google para la conversi√≥n [aqu√≠](https://github.com/google-research/albert#pre-trained-models).\n-\n-## OpenAI GPT\n-\n-Este es un ejemplo del proceso para convertir un modelo OpenAI GPT pre-entrenado, asumiendo que tu checkpoint de NumPy se guarda con el mismo formato que el modelo pre-entrenado de OpenAI (m√°s informaci√≥n [aqu√≠](https://github.com/openai/finetune-transformer-lm)):\n-\n-```bash\n-export OPENAI_GPT_CHECKPOINT_FOLDER_PATH=/path/to/openai/pretrained/numpy/weights\n-\n-transformers convert --model_type gpt \\\n-  --tf_checkpoint $OPENAI_GPT_CHECKPOINT_FOLDER_PATH \\\n-  --pytorch_dump_output $PYTORCH_DUMP_OUTPUT \\\n-  [--config OPENAI_GPT_CONFIG] \\\n-  [--finetuning_task_name OPENAI_GPT_FINETUNED_TASK] \\\n-```\n-\n-## OpenAI GPT-2\n-\n-Aqu√≠ hay un ejemplo del proceso para convertir un modelo OpenAI GPT-2 pre-entrenado (m√°s informaci√≥n [aqu√≠](https://github.com/openai/gpt-2)):\n-\n-```bash\n-export OPENAI_GPT2_CHECKPOINT_PATH=/path/to/openai-community/gpt2/pretrained/weights\n-\n-transformers convert --model_type gpt2 \\\n-  --tf_checkpoint $OPENAI_GPT2_CHECKPOINT_PATH \\\n-  --pytorch_dump_output $PYTORCH_DUMP_OUTPUT \\\n-  [--config OPENAI_GPT2_CONFIG] \\\n-  [--finetuning_task_name OPENAI_GPT2_FINETUNED_TASK]\n-```\n-\n-## XLNet\n-\n-Aqu√≠ hay un ejemplo del proceso para convertir un modelo XLNet pre-entrenado:\n-\n-```bash\n-export TRANSFO_XL_CHECKPOINT_PATH=/path/to/xlnet/checkpoint\n-export TRANSFO_XL_CONFIG_PATH=/path/to/xlnet/config\n-\n-transformers convert --model_type xlnet \\\n-  --tf_checkpoint $TRANSFO_XL_CHECKPOINT_PATH \\\n-  --config $TRANSFO_XL_CONFIG_PATH \\\n-  --pytorch_dump_output $PYTORCH_DUMP_OUTPUT \\\n-  [--finetuning_task_name XLNET_FINETUNED_TASK] \\\n-```\n-\n-## XLM\n-\n-Aqu√≠ hay un ejemplo del proceso para convertir un modelo XLM pre-entrenado:\n-\n-```bash\n-export XLM_CHECKPOINT_PATH=/path/to/xlm/checkpoint\n-\n-transformers convert --model_type xlm \\\n-  --tf_checkpoint $XLM_CHECKPOINT_PATH \\\n-  --pytorch_dump_output $PYTORCH_DUMP_OUTPUT\n- [--config XML_CONFIG] \\\n- [--finetuning_task_name XML_FINETUNED_TASK]\n-```\n-\n-## T5\n-\n-Aqu√≠ hay un ejemplo del proceso para convertir un modelo T5 pre-entrenado:\n-\n-```bash\n-export T5=/path/to/t5/uncased_L-12_H-768_A-12\n-\n-transformers convert --model_type t5 \\\n-  --tf_checkpoint $T5/t5_model.ckpt \\\n-  --config $T5/t5_config.json \\\n-  --pytorch_dump_output $T5/pytorch_model.bin\n-```"
        },
        {
            "sha": "f48003d67323e3d835bb7048b92a3e6e2993c14e",
            "filename": "docs/source/hi/_toctree.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/docs%2Fsource%2Fhi%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/docs%2Fsource%2Fhi%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fhi%2F_toctree.yml?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -2,6 +2,4 @@\n   - local: pipeline_tutorial\n     title: ‡§™‡§æ‡§á‡§™‡§≤‡§æ‡§á‡§®‡•ã‡§Ç ‡§ï‡•á ‡§∏‡§æ‡§• ‡§Ö‡§®‡•Å‡§Æ‡§æ‡§® ‡§ö‡§≤‡§æ‡§è‡§Å\n   - local: accelerate\n-    title: ü§ó Accelerate ‡§ï‡•á ‡§∏‡§æ‡§• ‡§µ‡§ø‡§§‡§∞‡§ø‡§§ ‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§£ ‡§∏‡•á‡§ü ‡§ï‡§∞‡•á‡§Ç\n-  - local: tflite\n-    title: TFLite ‡§Æ‡•á‡§Ç ‡§®‡§ø‡§∞‡•ç‡§Ø‡§æ‡§§ ‡§ï‡§∞‡•á‡§Ç\n\\ No newline at end of file\n+    title: ü§ó Accelerate ‡§ï‡•á ‡§∏‡§æ‡§• ‡§µ‡§ø‡§§‡§∞‡§ø‡§§ ‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§£ ‡§∏‡•á‡§ü ‡§ï‡§∞‡•á‡§Ç\n\\ No newline at end of file"
        },
        {
            "sha": "5a84bed94266db0de5e8fa81cfcd7c1cb2a5b643",
            "filename": "docs/source/hi/tflite.md",
            "status": "removed",
            "additions": 0,
            "deletions": 55,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/docs%2Fsource%2Fhi%2Ftflite.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/docs%2Fsource%2Fhi%2Ftflite.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fhi%2Ftflite.md?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,55 +0,0 @@\n-<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# TFLite ‡§Æ‡•á‡§Ç ‡§®‡§ø‡§∞‡•ç‡§Ø‡§æ‡§§ ‡§ï‡§∞‡•á‡§Ç\n-\n-[TensorFlow Lite](https://www.tensorflow.org/lite/guide) ‡§è‡§ï ‡§π‡§≤‡•ç‡§ï‡§æ ‡§¢‡§æ‡§Ç‡§ö‡§æ ‡§π‡•à ‡§ú‡•ã ‡§Æ‡§∂‡•Ä‡§® ‡§≤‡§∞‡•ç‡§®‡§ø‡§Ç‡§ó ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•ã ‡§∏‡§Ç‡§∏‡§æ‡§ß‡§®-‡§∏‡•Ä‡§Æ‡§ø‡§§ ‡§â‡§™‡§ï‡§∞‡§£‡•ã‡§Ç, ‡§ú‡•à‡§∏‡•á ‡§Æ‡•ã‡§¨‡§æ‡§á‡§≤ ‡§´‡•ã‡§®, ‡§è‡§Æ‡•ç‡§¨‡•á‡§°‡•á‡§° ‡§∏‡§ø‡§∏‡•ç‡§ü‡§Æ ‡§î‡§∞ ‡§á‡§Ç‡§ü‡§∞‡§®‡•á‡§ü ‡§ë‡§´ ‡§•‡§ø‡§Ç‡§ó‡•ç‡§∏ (IoT) ‡§â‡§™‡§ï‡§∞‡§£‡•ã‡§Ç ‡§™‡§∞ ‡§§‡•à‡§®‡§æ‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§π‡•à‡•§ TFLite ‡§ï‡•ã ‡§á‡§® ‡§â‡§™‡§ï‡§∞‡§£‡•ã‡§Ç ‡§™‡§∞ ‡§∏‡•Ä‡§Æ‡§ø‡§§ ‡§ó‡§£‡§®‡§æ‡§§‡•ç‡§Æ‡§ï ‡§∂‡§ï‡•ç‡§§‡§ø, ‡§Æ‡•á‡§Æ‡•ã‡§∞‡•Ä ‡§î‡§∞ ‡§ä‡§∞‡•ç‡§ú‡§æ ‡§ñ‡§™‡§§ ‡§ï‡•á ‡§∏‡§æ‡§• ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•ã ‡§ï‡•Å‡§∂‡§≤‡§§‡§æ ‡§∏‡•á ‡§ë‡§™‡•ç‡§ü‡§ø‡§Æ‡§æ‡§á‡§ú‡§º ‡§î‡§∞ ‡§ö‡§≤‡§æ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§°‡§ø‡§ú‡§º‡§æ‡§á‡§® ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à‡•§ ‡§è‡§ï TensorFlow Lite ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•ã ‡§è‡§ï ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§ï‡•Å‡§∂‡§≤ ‡§™‡•ã‡§∞‡•ç‡§ü‡•á‡§¨‡§≤ ‡§™‡•ç‡§∞‡§æ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§¶‡§∞‡•ç‡§∂‡§æ‡§Ø‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à ‡§ú‡§ø‡§∏‡•á `.tflite` ‡§´‡§º‡§æ‡§á‡§≤ ‡§è‡§ï‡•ç‡§∏‡§ü‡•á‡§Ç‡§∂‡§® ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§™‡§π‡§ö‡§æ‡§®‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à‡•§\n-\n-ü§ó Optimum ‡§Æ‡•á‡§Ç `exporters.tflite` ‡§Æ‡•â‡§°‡•ç‡§Ø‡•Ç‡§≤ ‡§ï‡•á ‡§Æ‡§æ‡§ß‡•ç‡§Ø‡§Æ ‡§∏‡•á ü§ó Transformers ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•ã TFLite ‡§Æ‡•á‡§Ç ‡§®‡§ø‡§∞‡•ç‡§Ø‡§æ‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•Ä ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡•ç‡§∑‡§Æ‡§§‡§æ ‡§π‡•à‡•§ ‡§∏‡§Æ‡§∞‡•ç‡§•‡§ø‡§§ ‡§Æ‡•â‡§°‡§≤ ‡§Ü‡§∞‡•ç‡§ï‡§ø‡§ü‡•á‡§ï‡•ç‡§ö‡§∞ ‡§ï‡•Ä ‡§∏‡•Ç‡§ö‡•Ä ‡§ï‡•á ‡§≤‡§ø‡§è, ‡§ï‡•É‡§™‡§Ø‡§æ [ü§ó Optimum ‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú‡§º](https://huggingface.co/docs/optimum/exporters/tflite/overview) ‡§¶‡•á‡§ñ‡•á‡§Ç‡•§\n-\n-TFLite ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§Æ‡•â‡§°‡§≤ ‡§®‡§ø‡§∞‡•ç‡§Ø‡§æ‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è, ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§®‡§ø‡§∞‡•ç‡§≠‡§∞‡§§‡§æ‡§è‡§Å ‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§ ‡§ï‡§∞‡•á‡§Ç:\n-\n-```bash\n-pip install optimum[exporters-tf]\n-```\n-\n-‡§∏‡§≠‡•Ä ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§§‡§∞‡•ç‡§ï‡•ã‡§Ç ‡§ï‡•Ä ‡§ú‡§æ‡§Ç‡§ö ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è, [ü§ó Optimum ‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú‡§º](https://huggingface.co/docs/optimum/main/en/exporters/tflite/usage_guides/export_a_model) ‡§¶‡•á‡§ñ‡•á‡§Ç,\n-‡§Ø‡§æ ‡§ï‡§Æ‡§æ‡§Ç‡§° ‡§≤‡§æ‡§á‡§® ‡§Æ‡•á‡§Ç ‡§Æ‡§¶‡§¶ ‡§¶‡•á‡§ñ‡•á‡§Ç:\n-\n-```bash\n-optimum-cli export tflite --help\n-```\n-\n-‡§Ø‡§¶‡§ø ‡§Ü‡§™ ü§ó Hub ‡§∏‡•á ‡§è‡§ï ‡§Æ‡•â‡§°‡§≤ ‡§ï‡§æ ‡§ö‡•á‡§ï‡§™‡•â‡§á‡§Ç‡§ü ‡§®‡§ø‡§∞‡•ç‡§Ø‡§æ‡§§ ‡§ï‡§∞‡§®‡§æ ‡§ö‡§æ‡§π‡§§‡•á ‡§π‡•à‡§Ç, ‡§â‡§¶‡§æ‡§π‡§∞‡§£ ‡§ï‡•á ‡§≤‡§ø‡§è, `google-bert/bert-base-uncased`, ‡§®‡§ø‡§Æ‡•ç‡§®‡§≤‡§ø‡§ñ‡§ø‡§§ ‡§ï‡§Æ‡§æ‡§Ç‡§° ‡§ö‡§≤‡§æ‡§è‡§Å:\n-\n-```bash\n-optimum-cli export tflite --model google-bert/bert-base-uncased --sequence_length 128 bert_tflite/\n-```\n-\n-‡§Ü‡§™‡§ï‡•ã ‡§™‡•ç‡§∞‡§ó‡§§‡§ø ‡§ï‡•ã ‡§¶‡§∞‡•ç‡§∂‡§æ‡§§‡•á ‡§π‡•Å‡§è ‡§≤‡•â‡§ó ‡§¶‡§ø‡§ñ‡§æ‡§à ‡§¶‡•á‡§Ç‡§ó‡•á ‡§î‡§∞ ‡§Ø‡§π ‡§¶‡§ø‡§ñ‡§æ‡§è‡§Ç‡§ó‡•á ‡§ï‡§ø ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ‡§∏‡•ç‡§µ‡§∞‡•Ç‡§™ `model.tflite` ‡§ï‡§π‡§æ‡§Å ‡§∏‡§π‡•á‡§ú‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à, ‡§ú‡•à‡§∏‡•á:\n-\n-```bash\n-Validating TFLite model...\n-\t-[‚úì] TFLite model output names match reference model (logits)\n-\t- Validating TFLite Model output \"logits\":\n-\t\t-[‚úì] (1, 128, 30522) matches (1, 128, 30522)\n-\t\t-[x] values not close enough, max diff: 5.817413330078125e-05 (atol: 1e-05)\n-The TensorFlow Lite export succeeded with the warning: The maximum absolute difference between the output of the reference model and the TFLite exported model is not within the set tolerance 1e-05:\n-- logits: max diff = 5.817413330078125e-05.\n- The exported model was saved at: bert_tflite\n-```\n-\n-‡§â‡§™‡§∞‡•ã‡§ï‡•ç‡§§ ‡§â‡§¶‡§æ‡§π‡§∞‡§£ ü§ó Hub ‡§∏‡•á ‡§è‡§ï ‡§ö‡•á‡§ï‡§™‡•â‡§á‡§Ç‡§ü ‡§®‡§ø‡§∞‡•ç‡§Ø‡§æ‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•ã ‡§¶‡§∞‡•ç‡§∂‡§æ‡§§‡§æ ‡§π‡•à‡•§ ‡§ú‡§¨ ‡§è‡§ï ‡§∏‡•ç‡§•‡§æ‡§®‡•Ä‡§Ø ‡§Æ‡•â‡§°‡§≤ ‡§®‡§ø‡§∞‡•ç‡§Ø‡§æ‡§§ ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç, ‡§§‡•ã ‡§™‡§π‡§≤‡•á ‡§∏‡•Å‡§®‡§ø‡§∂‡•ç‡§ö‡§ø‡§§ ‡§ï‡§∞‡•á‡§Ç ‡§ï‡§ø ‡§Ü‡§™‡§®‡•á ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•á ‡§µ‡§ú‡§º‡§® ‡§î‡§∞ ‡§ü‡•ã‡§ï‡§®‡§æ‡§á‡§ú‡§º‡§∞ ‡§´‡§º‡§æ‡§á‡§≤‡•ã‡§Ç ‡§ï‡•ã ‡§è‡§ï ‡§π‡•Ä ‡§®‡§ø‡§∞‡•ç‡§¶‡•á‡§∂‡§ø‡§ï‡§æ (`local_path`) ‡§Æ‡•á‡§Ç ‡§∏‡§π‡•á‡§ú‡§æ ‡§π‡•à‡•§ CLI ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§§‡•á ‡§∏‡§Æ‡§Ø, ‡§ö‡•á‡§ï‡§™‡•â‡§á‡§Ç‡§ü ‡§®‡§æ‡§Æ ‡§ï‡•á ‡§¨‡§ú‡§æ‡§Ø `model` ‡§§‡§∞‡•ç‡§ï ‡§Æ‡•á‡§Ç `local_path` ‡§™‡§æ‡§∏ ‡§ï‡§∞‡•á‡§Ç‡•§"
        },
        {
            "sha": "2ba1b8ecede3f963d4b7a9811ca7cf32da7694ca",
            "filename": "docs/source/it/_toctree.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/docs%2Fsource%2Fit%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/docs%2Fsource%2Fit%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fit%2F_toctree.yml?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -29,8 +29,6 @@\n     title: Addestramento con script\r\n   - local: multilingual\r\n     title: Modelli multilingua per l'inferenza\r\n-  - local: converting_tensorflow_models\r\n-    title: Convertire modelli tensorflow\r\n   - local: serialization\r\n     title: Esporta modelli Transformers\r\n   - local: perf_train_cpu\r"
        },
        {
            "sha": "dace244fa6dd069aab5e61432aa9c3387f0408e4",
            "filename": "docs/source/it/converting_tensorflow_models.md",
            "status": "removed",
            "additions": 0,
            "deletions": 144,
            "changes": 144,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/docs%2Fsource%2Fit%2Fconverting_tensorflow_models.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/docs%2Fsource%2Fit%2Fconverting_tensorflow_models.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fit%2Fconverting_tensorflow_models.md?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,144 +0,0 @@\n-<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-http://www.apache.org/licenses/LICENSE-2.0\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# Convertire checkpoint di Tensorflow\n-\n-√à disponibile un'interfaccia a linea di comando per convertire gli originali checkpoint di Bert/GPT/GPT-2/Transformer-XL/XLNet/XLM\n-in modelli che possono essere caricati utilizzando i metodi `from_pretrained` della libreria.\n-\n-<Tip>\n-\n-A partire dalla versione 2.3.0 lo script di conversione √® parte di transformers CLI (**transformers**), disponibile in ogni installazione\n-di transformers >=2.3.0.\n-\n-La seguente documentazione riflette il formato dei comandi di **transformers convert**.\n-\n-</Tip>\n-\n-## BERT\n-\n-Puoi convertire qualunque checkpoint Tensorflow di BERT (in particolare\n-[i modeli pre-allenati rilasciati da Google](https://github.com/google-research/bert#pre-trained-models))\n-in un file di salvataggio Pytorch utilizzando lo script\n-[convert_bert_original_tf_checkpoint_to_pytorch.py](https://github.com/huggingface/transformers/tree/main/src/transformers/models/bert/convert_bert_original_tf_checkpoint_to_pytorch.py).\n-\n-Questo CLI prende come input un checkpoint di Tensorflow (tre files che iniziano con `bert_model.ckpt`) ed il relativo\n-file di configurazione (`bert_config.json`), crea un modello Pytorch per questa configurazione, carica i pesi dal\n-checkpoint di Tensorflow nel modello di Pytorch e salva il modello che ne risulta in un file di salvataggio standard di Pytorch che\n-pu√≤ essere importato utilizzando `from_pretrained()` (vedi l'esempio nel\n-[quicktour](quicktour) , [run_glue.py](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification/run_glue.py) ).\n-\n-Devi soltanto lanciare questo script di conversione **una volta** per ottenere un modello Pytorch. Dopodich√®, potrai tralasciare\n-il checkpoint di Tensorflow (i tre files che iniziano con `bert_model.ckpt`), ma assicurati di tenere il file di configurazione\n-(`bert_config.json`) ed il file di vocabolario (`vocab.txt`) in quanto queste componenti sono necessarie anche per il modello di Pytorch.\n-\n-Per lanciare questo specifico script di conversione avrai bisogno di un'installazione di Tensorflow e di Pytorch\n-(`pip install tensorflow`). Il resto della repository richiede soltanto Pytorch.\n-\n-Questo √® un esempio del processo di conversione per un modello `BERT-Base Uncased` pre-allenato:\n-\n-```bash\n-export BERT_BASE_DIR=/path/to/bert/uncased_L-12_H-768_A-12\n-transformers convert --model_type bert \\\n-  --tf_checkpoint $BERT_BASE_DIR/bert_model.ckpt \\\n-  --config $BERT_BASE_DIR/bert_config.json \\\n-  --pytorch_dump_output $BERT_BASE_DIR/pytorch_model.bin\n-```\n-\n-Puoi scaricare i modelli pre-allenati di Google per la conversione [qua](https://github.com/google-research/bert#pre-trained-models).\n-\n-## ALBERT\n-\n-Per il modello ALBERT, converti checkpoint di Tensoflow in Pytorch utilizzando lo script\n-[convert_albert_original_tf_checkpoint_to_pytorch.py](https://github.com/huggingface/transformers/tree/main/src/transformers/models/albert/convert_albert_original_tf_checkpoint_to_pytorch.py).\n-\n-Il CLI prende come input un checkpoint di Tensorflow (tre files che iniziano con `model.ckpt-best`) e i relativi file di\n-configurazione (`albert_config.json`), dopodich√® crea e salva un modello Pytorch. Per lanciare questa conversione\n-avrai bisogno di un'installazione di Tensorflow e di Pytorch.\n-\n-Ecco un esempio del procedimento di conversione di un modello `ALBERT Base` pre-allenato:\n-\n-```bash\n-export ALBERT_BASE_DIR=/path/to/albert/albert_base\n-transformers convert --model_type albert \\\n-  --tf_checkpoint $ALBERT_BASE_DIR/model.ckpt-best \\\n-  --config $ALBERT_BASE_DIR/albert_config.json \\\n-  --pytorch_dump_output $ALBERT_BASE_DIR/pytorch_model.bin\n-```\n-\n-Puoi scaricare i modelli pre-allenati di Google per la conversione [qui](https://github.com/google-research/albert#pre-trained-models).\n-\n-## OpenAI GPT\n-\n-Ecco un esempio del processo di conversione di un modello OpenAI GPT pre-allenato, assumendo che il tuo checkpoint di NumPy\n-sia salvato nello stesso formato dei modelli pre-allenati OpenAI (vedi [qui](https://github.com/openai/finetune-transformer-lm)):\n-```bash\n-export OPENAI_GPT_CHECKPOINT_FOLDER_PATH=/path/to/openai/pretrained/numpy/weights\n-transformers convert --model_type gpt \\\n-  --tf_checkpoint $OPENAI_GPT_CHECKPOINT_FOLDER_PATH \\\n-  --pytorch_dump_output $PYTORCH_DUMP_OUTPUT \\\n-  [--config OPENAI_GPT_CONFIG] \\\n-  [--finetuning_task_name OPENAI_GPT_FINETUNED_TASK] \\\n-```\n-\n-## OpenAI GPT-2\n-\n-Ecco un esempio del processo di conversione di un modello OpenAI GPT-2 pre-allenato (vedi [qui](https://github.com/openai/gpt-2)):\n-\n-```bash\n-export OPENAI_GPT2_CHECKPOINT_PATH=/path/to/openai-community/gpt2/pretrained/weights\n-transformers convert --model_type gpt2 \\\n-  --tf_checkpoint $OPENAI_GPT2_CHECKPOINT_PATH \\\n-  --pytorch_dump_output $PYTORCH_DUMP_OUTPUT \\\n-  [--config OPENAI_GPT2_CONFIG] \\\n-  [--finetuning_task_name OPENAI_GPT2_FINETUNED_TASK]\n-```\n-\n-## XLNet\n-\n-Ecco un esempio del processo di conversione di un modello XLNet pre-allenato:\n-\n-```bash\n-export TRANSFO_XL_CHECKPOINT_PATH=/path/to/xlnet/checkpoint\n-export TRANSFO_XL_CONFIG_PATH=/path/to/xlnet/config\n-transformers convert --model_type xlnet \\\n-  --tf_checkpoint $TRANSFO_XL_CHECKPOINT_PATH \\\n-  --config $TRANSFO_XL_CONFIG_PATH \\\n-  --pytorch_dump_output $PYTORCH_DUMP_OUTPUT \\\n-  [--finetuning_task_name XLNET_FINETUNED_TASK] \\\n-```\n-\n-## XLM\n-\n-Ecco un esempio del processo di conversione di un modello XLM pre-allenato:\n-\n-```bash\n-export XLM_CHECKPOINT_PATH=/path/to/xlm/checkpoint\n-transformers convert --model_type xlm \\\n-  --tf_checkpoint $XLM_CHECKPOINT_PATH \\\n-  --pytorch_dump_output $PYTORCH_DUMP_OUTPUT\n- [--config XML_CONFIG] \\\n- [--finetuning_task_name XML_FINETUNED_TASK]\n-```\n-\n-## T5\n-\n-Ecco un esempio del processo di conversione di un modello T5 pre-allenato:\n-\n-```bash\n-export T5=/path/to/t5/uncased_L-12_H-768_A-12\n-transformers convert --model_type t5 \\\n-  --tf_checkpoint $T5/t5_model.ckpt \\\n-  --config $T5/t5_config.json \\\n-  --pytorch_dump_output $T5/pytorch_model.bin\n-```"
        },
        {
            "sha": "d01cf584ecff9d484571dbbcead788c9f97bc708",
            "filename": "docs/source/ja/_toctree.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/docs%2Fsource%2Fja%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/docs%2Fsource%2Fja%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2F_toctree.yml?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -109,8 +109,6 @@\n     title: „ÉÅ„É£„ÉÉ„Éà„É¢„Éá„É´„ÅÆ„ÉÜ„É≥„Éó„É¨„Éº„Éà\n   - local: serialization\n     title: ONNX „Å∏„ÅÆ„Ç®„ÇØ„Çπ„Éù„Éº„Éà\n-  - local: tflite\n-    title: TFLite „Å∏„ÅÆ„Ç®„ÇØ„Çπ„Éù„Éº„Éà\n   - local: torchscript\n     title: „Éà„Éº„ÉÅ„Çπ„ÇØ„É™„Éó„Éà„Å∏„ÅÆ„Ç®„ÇØ„Çπ„Éù„Éº„Éà\n   - local: community\n@@ -132,8 +130,6 @@\n       title: ÂàÜÊï£CPU„Éà„É¨„Éº„Éã„É≥„Ç∞\n     - local: perf_train_tpu\n       title: TPU „Å´Èñ¢„Åô„Çã„Éà„É¨„Éº„Éã„É≥„Ç∞\n-    - local: perf_train_tpu_tf\n-      title: TensorFlow „Çí‰ΩøÁî®„Åó„Åü TPU „ÅÆ„Éà„É¨„Éº„Éã„É≥„Ç∞\n     - local: perf_train_special\n       title: ÁâπÊÆä„Å™„Éè„Éº„Éâ„Ç¶„Çß„Ç¢„Å´Èñ¢„Åô„Çã„Éà„É¨„Éº„Éã„É≥„Ç∞\n     - local: perf_hardware\n@@ -153,8 +149,6 @@\n     title: Êé®Ë´ñ„ÅÆÊúÄÈÅ©Âåñ\n   - local: big_models\n     title: Â§ß„Åç„Å™„É¢„Éá„É´„ÅÆ„Ç§„É≥„Çπ„Çø„É≥„ÇπÂåñ\n-  - local: tf_xla\n-    title: TensorFlow„É¢„Éá„É´„ÅÆXLAÁµ±Âêà\n   - local: perf_torch_compile\n     title: torch.compile()„Çí‰ΩøÁî®„Åó„ÅüÊé®Ë´ñ„ÅÆÊúÄÈÅ©Âåñ\n   title: „Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„Å®„Çπ„Ç±„Éº„É©„Éì„É™„ÉÜ„Ç£\n@@ -202,8 +196,6 @@\n       title: ÊßãÊàê\n     - local: main_classes/data_collator\n       title: „Éá„Éº„ÇøÁÖßÂêàËÄÖ\n-    - local: main_classes/keras_callbacks\n-      title: Keras „Ç≥„Éº„É´„Éê„ÉÉ„ÇØ\n     - local: main_classes/logging\n       title: „É≠„ÇÆ„É≥„Ç∞\n     - local: main_classes/model"
        },
        {
            "sha": "ff28107a4345795ff04e11ffec7f76a93561635a",
            "filename": "docs/source/ja/main_classes/keras_callbacks.md",
            "status": "removed",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/docs%2Fsource%2Fja%2Fmain_classes%2Fkeras_callbacks.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/docs%2Fsource%2Fja%2Fmain_classes%2Fkeras_callbacks.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmain_classes%2Fkeras_callbacks.md?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,28 +0,0 @@\n-<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# Keras callbacks\n-\n-Keras „Çí‰ΩøÁî®„Åó„Å¶ Transformers „É¢„Éá„É´„Çí„Éà„É¨„Éº„Éã„É≥„Ç∞„Åô„ÇãÂ†¥Âêà„ÄÅ‰∏ÄËà¨ÁöÑ„Å™Âá¶ÁêÜ„ÇíËá™ÂãïÂåñ„Åô„Çã„Åü„ÇÅ„Å´‰ΩøÁî®„Åß„Åç„Çã„É©„Ç§„Éñ„É©„É™Âõ∫Êúâ„ÅÆ„Ç≥„Éº„É´„Éê„ÉÉ„ÇØ„Åå„ÅÑ„Åè„Å§„Åã„ÅÇ„Çä„Åæ„Åô„ÄÇ\n-„Çø„Çπ„ÇØ:\n-\n-## KerasMetricCallback\n-\n-[[autodoc]] KerasMetricCallback\n-\n-## PushToHubCallback\n-\n-[[autodoc]] PushToHubCallback"
        },
        {
            "sha": "3ffe88267cddeb9cb254be39d215c5f8a3ea64e2",
            "filename": "docs/source/ja/perf_train_tpu_tf.md",
            "status": "removed",
            "additions": 0,
            "deletions": 168,
            "changes": 168,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/docs%2Fsource%2Fja%2Fperf_train_tpu_tf.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/docs%2Fsource%2Fja%2Fperf_train_tpu_tf.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fperf_train_tpu_tf.md?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,168 +0,0 @@\n-<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-\n-‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# Training on TPU with TensorFlow\n-\n-<Tip>\n-\n-Ë©≥Á¥∞„Å™Ë™¨Êòé„Åå‰∏çË¶Å„Åß„ÄÅÂçò„Å´TPU„ÅÆ„Ç≥„Éº„Éâ„Çµ„É≥„Éó„É´„ÇíÂÖ•Êâã„Åó„Å¶„Éà„É¨„Éº„Éã„É≥„Ç∞„ÇíÈñãÂßã„Åó„Åü„ÅÑÂ†¥Âêà„ÅØ„ÄÅ[ÁßÅ„Åü„Å°„ÅÆTPU„ÅÆ‰æã„ÅÆ„Éé„Éº„Éà„Éñ„ÉÉ„ÇØ„Çí„ÉÅ„Çß„ÉÉ„ÇØ„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºÅ](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb)\n-\n-</Tip>\n-\n-### What is a TPU?\n-\n-TPU„ÅØ**Tensor Processing UnitÔºà„ÉÜ„É≥„ÇΩ„É´Âá¶ÁêÜ„É¶„Éã„ÉÉ„ÉàÔºâ**„ÅÆÁï•„Åß„Åô„ÄÇ„Åì„Çå„Çâ„ÅØGoogle„ÅåË®≠Ë®à„Åó„Åü„Éè„Éº„Éâ„Ç¶„Çß„Ç¢„Åß„ÄÅ„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÂÜÖ„ÅÆ„ÉÜ„É≥„ÇΩ„É´Ë®àÁÆó„ÇíÂ§ßÂπÖ„Å´È´òÈÄüÂåñ„Åô„Çã„Åü„ÇÅ„Å´‰ΩøÁî®„Åï„Çå„Åæ„Åô„ÄÇ„Åì„Çå„ÅØGPU„ÅÆ„Çà„ÅÜ„Å™„ÇÇ„ÅÆ„Åß„Åô„ÄÇ„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅÆ„Éà„É¨„Éº„Éã„É≥„Ç∞„Å®Êé®Ë´ñ„ÅÆ‰∏°Êñπ„Å´‰ΩøÁî®„Åß„Åç„Åæ„Åô„ÄÇ‰∏ÄËà¨ÁöÑ„Å´„ÅØGoogle„ÅÆ„ÇØ„É©„Ç¶„Éâ„Çµ„Éº„Éì„Çπ„Çí‰ªã„Åó„Å¶„Ç¢„ÇØ„Çª„Çπ„Åï„Çå„Åæ„Åô„Åå„ÄÅGoogle Colab„Å®Kaggle Kernels„ÇíÈÄö„Åò„Å¶„ÇÇÁÑ°Êñô„ÅßÂ∞èË¶èÊ®°„ÅÆTPU„Å´Áõ¥Êé•„Ç¢„ÇØ„Çª„Çπ„Åß„Åç„Åæ„Åô„ÄÇ\n-\n-[ü§ó Transformers„ÅÆ„Åô„Åπ„Å¶„ÅÆTensorFlow„É¢„Éá„É´„ÅØKeras„É¢„Éá„É´„Åß„Åô](https://huggingface.co/blog/tensorflow-philosophy)„ÅÆ„Åß„ÄÅ„Åì„ÅÆÊñáÊõ∏„ÅÆ„Åª„Å®„Çì„Å©„ÅÆÊñπÊ≥ï„ÅØ‰∏ÄËà¨ÁöÑ„Å´Keras„É¢„Éá„É´Áî®„ÅÆTPU„Éà„É¨„Éº„Éã„É≥„Ç∞„Å´ÈÅ©Áî®„Åß„Åç„Åæ„ÅôÔºÅ„Åü„Å†„Åó„ÄÅTransformers„Å®Datasets„ÅÆHuggingFace„Ç®„Ç≥„Ç∑„Çπ„ÉÜ„É†Ôºàhug-o-systemÔºüÔºâ„Å´Âõ∫Êúâ„ÅÆ„Éù„Ç§„É≥„Éà„ÇÇ„ÅÑ„Åè„Å§„Åã„ÅÇ„Çä„ÄÅ„Åù„Çå„Å´„Å§„ÅÑ„Å¶„ÅØÈÅ©Áî®„Åô„Çã„Å®„Åç„Å´„Åù„Çå„ÇíÁ§∫„Åó„Åæ„Åô„ÄÇ\n-\n-### What kinds of TPU are available?\n-\n-Êñ∞„Åó„ÅÑ„É¶„Éº„Ç∂„Éº„ÅØ„ÄÅ„Åï„Åæ„Åñ„Åæ„Å™TPU„Å®„Åù„ÅÆ„Ç¢„ÇØ„Çª„ÇπÊñπÊ≥ï„Å´Èñ¢„Åô„ÇãÂπÖÂ∫É„ÅÑÊÉÖÂ†±„Å´„Çà„ÅèÊ∑∑‰π±„Åó„Åæ„Åô„ÄÇÁêÜËß£„Åô„Çã„Åü„ÇÅ„ÅÆÊúÄÂàù„ÅÆÈáçË¶Å„Å™ÈÅï„ÅÑ„ÅØ„ÄÅ**TPU„Éé„Éº„Éâ**„Å®**TPU VM**„ÅÆÈÅï„ÅÑ„Åß„Åô„ÄÇ\n-\n-**TPU„Éé„Éº„Éâ**„Çí‰ΩøÁî®„Åô„Çã„Å®„ÄÅ‰∫ãÂÆü‰∏ä„É™„É¢„Éº„Éà„ÅÆTPU„Å´ÈñìÊé•ÁöÑ„Å´„Ç¢„ÇØ„Çª„Çπ„Åó„Åæ„Åô„ÄÇÂà•ÂÄã„ÅÆVM„ÅåÂøÖË¶Å„Åß„ÄÅ„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„Å®„Éá„Éº„Çø„Éë„Ç§„Éó„É©„Ç§„É≥„ÇíÂàùÊúüÂåñ„Åó„ÄÅ„Åù„Çå„Çâ„Çí„É™„É¢„Éº„Éà„Éé„Éº„Éâ„Å´Ëª¢ÈÄÅ„Åó„Åæ„Åô„ÄÇGoogle Colab„ÅßTPU„Çí‰ΩøÁî®„Åô„Çã„Å®„ÄÅ**TPU„Éé„Éº„Éâ**„Çπ„Çø„Ç§„É´„Åß„Ç¢„ÇØ„Çª„Çπ„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n-\n-TPU„Éé„Éº„Éâ„Çí‰ΩøÁî®„Åô„Çã„Å®„ÄÅ„Åù„Çå„Å´ÊÖ£„Çå„Å¶„ÅÑ„Å™„ÅÑ‰∫∫„ÄÖ„Å´„ÅØ„Åã„Å™„Çä‰∫àÊúü„Åó„Å™„ÅÑÂãï‰Ωú„ÅåÁô∫Áîü„Åô„Çã„Åì„Å®„Åå„ÅÇ„Çä„Åæ„ÅôÔºÅÁâπ„Å´„ÄÅTPU„ÅØPython„Ç≥„Éº„Éâ„ÇíÂÆüË°å„Åó„Å¶„ÅÑ„Çã„Éû„Ç∑„É≥„Å®Áâ©ÁêÜÁöÑ„Å´Áï∞„Å™„Çã„Ç∑„Çπ„ÉÜ„É†„Å´ÈÖçÁΩÆ„Åï„Çå„Å¶„ÅÑ„Çã„Åü„ÇÅ„ÄÅ„Éá„Éº„Çø„ÅØ„É≠„Éº„Ç´„É´„Éû„Ç∑„É≥„Å´„É≠„Éº„Ç´„É´„ÅßÊ†ºÁ¥ç„Åï„Çå„Å¶„ÅÑ„Çã„Éá„Éº„Çø„Éë„Ç§„Éó„É©„Ç§„É≥„ÅåÂÆåÂÖ®„Å´Â§±Êïó„Åó„Åæ„Åô„ÄÇ‰ª£„Çè„Çä„Å´„ÄÅ„Éá„Éº„Çø„ÅØGoogle Cloud Storage„Å´Ê†ºÁ¥ç„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ„Åì„Åì„Åß„Éá„Éº„Çø„Éë„Ç§„Éó„É©„Ç§„É≥„ÅØ„É™„É¢„Éº„Éà„ÅÆTPU„Éé„Éº„Éâ„ÅßÂÆüË°å„Åï„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà„Åß„ÇÇ„ÄÅ„Éá„Éº„Çø„Å´„Ç¢„ÇØ„Çª„Çπ„Åß„Åç„Åæ„Åô„ÄÇ\n-\n-<Tip>\n-\n-„Åô„Åπ„Å¶„ÅÆ„Éá„Éº„Çø„Çí`np.ndarray`„Åæ„Åü„ÅØ`tf.Tensor`„Å®„Åó„Å¶„É°„É¢„É™„Å´Âèé„ÇÅ„Çã„Åì„Å®„Åå„Åß„Åç„ÇãÂ†¥Âêà„ÄÅColab„Åæ„Åü„ÅØTPU„Éé„Éº„Éâ„Çí‰ΩøÁî®„Åó„Å¶„ÅÑ„ÇãÂ†¥Âêà„Åß„ÇÇ„ÄÅ„Éá„Éº„Çø„ÇíGoogle Cloud Storage„Å´„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åõ„Åö„Å´`fit()`„Åß„Éà„É¨„Éº„Éã„É≥„Ç∞„Åß„Åç„Åæ„Åô„ÄÇ\n-\n-</Tip>\n-\n-<Tip>\n-\n-**ü§ó Hugging FaceÂõ∫Êúâ„ÅÆ„Éí„É≥„Éàü§ó:** TF„Ç≥„Éº„Éâ„ÅÆ‰æã„Åß„Çà„ÅèË¶ã„Çã„Åß„ÅÇ„Çç„ÅÜ`Dataset.to_tf_dataset()`„Å®„Åù„ÅÆÈ´ò„É¨„Éô„É´„ÅÆ„É©„ÉÉ„Éë„Éº„Åß„ÅÇ„Çã`model.prepare_tf_dataset()`„ÅØ„ÄÅTPU„Éé„Éº„Éâ„ÅßÂ§±Êïó„Åó„Åæ„Åô„ÄÇ„Åì„Çå„ÅØ„ÄÅ`tf.data.Dataset`„Çí‰ΩúÊàê„Åó„Å¶„ÅÑ„Çã„Å´„ÇÇ„Åã„Åã„Çè„Çâ„Åö„ÄÅ„Åù„Çå„Åå„ÄåÁ¥îÁ≤ã„Å™„Äç`tf.data`„Éë„Ç§„Éó„É©„Ç§„É≥„Åß„ÅØ„Å™„Åè„ÄÅ`tf.numpy_function`„Åæ„Åü„ÅØ`Dataset.from_generator()`„Çí‰ΩøÁî®„Åó„Å¶Âü∫Áõ§„Å®„Å™„ÇãHuggingFace `Dataset`„Åã„Çâ„Éá„Éº„Çø„Çí„Çπ„Éà„É™„Éº„É†„ÅßË™≠„ÅøËæº„ÇÄ„Åì„Å®„Åã„Çâ„Åß„Åô„ÄÇ„Åì„ÅÆHuggingFace `Dataset`„ÅØ„É≠„Éº„Ç´„É´„Éá„Ç£„Çπ„ÇØ‰∏ä„ÅÆ„Éá„Éº„Çø„Çí„Éê„ÉÉ„ÇØ„Ç¢„ÉÉ„Éó„Åó„Å¶„Åä„Çä„ÄÅ„É™„É¢„Éº„ÉàTPU„Éé„Éº„Éâ„ÅåË™≠„ÅøÂèñ„Çã„Åì„Å®„Åå„Åß„Åç„Å™„ÅÑ„Åü„ÇÅ„Åß„Åô„ÄÇ\n-\n-</Tip>\n-\n-TPU„Å´„Ç¢„ÇØ„Çª„Çπ„Åô„ÇãÁ¨¨‰∫å„ÅÆÊñπÊ≥ï„ÅØ„ÄÅ**TPU VM**„Çí‰ªã„Åó„Å¶„Åß„Åô„ÄÇTPU VM„Çí‰ΩøÁî®„Åô„ÇãÂ†¥Âêà„ÄÅTPU„ÅåÊé•Á∂ö„Åï„Çå„Å¶„ÅÑ„Çã„Éû„Ç∑„É≥„Å´Áõ¥Êé•Êé•Á∂ö„Åó„Åæ„Åô„ÄÇ„Åì„Çå„ÅØGPU VM„Åß„Éà„É¨„Éº„Éã„É≥„Ç∞„ÇíË°å„ÅÜ„ÅÆ„Å®ÂêåÊßò„Åß„Åô„ÄÇTPU VM„ÅØ‰∏ÄËà¨ÁöÑ„Å´„Éá„Éº„Çø„Éë„Ç§„Éó„É©„Ç§„É≥„Å´Èñ¢„Åó„Å¶„ÅØÁâπ„Å´‰ΩúÊ•≠„Åå„Åó„ÇÑ„Åô„Åè„ÄÅ‰∏äË®ò„ÅÆ„Åô„Åπ„Å¶„ÅÆË≠¶Âëä„ÅØTPU VM„Å´„ÅØÈÅ©Áî®„Åï„Çå„Åæ„Åõ„ÇìÔºÅ\n-\n-„Åì„Çå„ÅØ‰∏ªË¶≥ÁöÑ„Å™ÊñáÊõ∏„Åß„Åô„ÅÆ„Åß„ÄÅ„Åì„Å°„Çâ„ÅÆÊÑèË¶ã„Åß„ÅôÔºö**ÂèØËÉΩ„Å™Èôê„ÇäTPU„Éé„Éº„Éâ„ÅÆ‰ΩøÁî®„ÇíÈÅø„Åë„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ** TPU VM„Çà„Çä„ÇÇÊ∑∑‰π±„Åó„ÇÑ„Åô„Åè„ÄÅ„Éá„Éê„ÉÉ„Ç∞„ÅåÈõ£„Åó„ÅÑ„Åß„Åô„ÄÇÂ∞ÜÊù•ÁöÑ„Å´„ÅØ„Çµ„Éù„Éº„Éà„Åï„Çå„Å™„Åè„Å™„ÇãÂèØËÉΩÊÄß„ÇÇ„ÅÇ„Çä„Åæ„Åô - Google„ÅÆÊúÄÊñ∞„ÅÆTPU„Åß„ÅÇ„ÇãTPUv4„ÅØ„ÄÅTPU VM„Å®„Åó„Å¶„ÅÆ„Åø„Ç¢„ÇØ„Çª„Çπ„Åß„Åç„Çã„Åü„ÇÅ„ÄÅTPU„Éé„Éº„Éâ„ÅØÂ∞ÜÊù•ÁöÑ„Å´„ÅØ„Äå„É¨„Ç¨„Ç∑„Éº„Äç„ÅÆ„Ç¢„ÇØ„Çª„ÇπÊñπÊ≥ï„Å´„Å™„ÇãÂèØËÉΩÊÄß„ÅåÈ´ò„ÅÑ„Åß„Åô„ÄÇ„Åü„Å†„Åó„ÄÅÁÑ°Êñô„ÅßTPU„Å´„Ç¢„ÇØ„Çª„Çπ„Åß„Åç„Çã„ÅÆ„ÅØColab„Å®Kaggle Kernels„ÅÆÂ†¥Âêà„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ„Åù„ÅÆÂ†¥Âêà„ÄÅ„Å©„ÅÜ„Åó„Å¶„ÇÇ‰ΩøÁî®„Åó„Å™„Åë„Çå„Å∞„Å™„Çâ„Å™„ÅÑÂ†¥Âêà„ÅÆÂèñ„ÇäÊâ±„ÅÑÊñπÊ≥ï„ÇíË™¨Êòé„Åó„Çà„ÅÜ„Å®„Åó„Åæ„ÅôÔºÅË©≥Á¥∞„ÅØ[TPU„ÅÆ‰æã„ÅÆ„Éé„Éº„Éà„Éñ„ÉÉ„ÇØ](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb)„ÅßË©≥Á¥∞„Å™Ë™¨Êòé„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n-\n-### What sizes of TPU are available?\n-\n-Âçò‰∏Ä„ÅÆTPUÔºàv2-8/v3-8/v4-8Ôºâ„ÅØ8„Å§„ÅÆ„É¨„Éó„É™„Ç´„ÇíÂÆüË°å„Åó„Åæ„Åô„ÄÇTPU„ÅØÊï∞Áôæ„Åã„ÇâÊï∞ÂçÉ„ÅÆ„É¨„Éó„É™„Ç´„ÇíÂêåÊôÇ„Å´ÂÆüË°å„Åß„Åç„Çã**„Éù„ÉÉ„Éâ**„Å´Â≠òÂú®„Åó„Åæ„Åô„ÄÇÂçò‰∏Ä„ÅÆTPU„Çà„Çä„ÇÇÂ§ö„Åè„ÅÆTPU„Çí‰ΩøÁî®„Åô„Çã„Åå„ÄÅ„Éù„ÉÉ„ÉâÂÖ®‰Ωì„Åß„ÅØ„Å™„ÅÑÂ†¥ÂêàÔºà„Åü„Å®„Åà„Å∞v3-32Ôºâ„ÄÅTPU„Éï„É™„Éº„Éà„ÅØ**„Éù„ÉÉ„Éâ„Çπ„É©„Ç§„Çπ**„Å®„Åó„Å¶ÂèÇÁÖß„Åï„Çå„Åæ„Åô„ÄÇ\n-\n-Colab„Çí‰ªã„Åó„Å¶ÁÑ°Êñô„ÅÆTPU„Å´„Ç¢„ÇØ„Çª„Çπ„Åô„ÇãÂ†¥Âêà„ÄÅÈÄöÂ∏∏„ÅØÂçò‰∏Ä„ÅÆv2-8 TPU„ÅåÊèê‰æõ„Åï„Çå„Åæ„Åô„ÄÇ\n-\n-\n-### I keep hearing about this XLA thing. What‚Äôs XLA, and how does it relate to TPUs?\n-\n-XLA„ÅØ„ÄÅTensorFlow„Å®JAX„ÅÆ‰∏°Êñπ„Åß‰ΩøÁî®„Åï„Çå„ÇãÊúÄÈÅ©Âåñ„Ç≥„É≥„Éë„Ç§„É©„Åß„Åô„ÄÇJAX„Åß„ÅØÂîØ‰∏Ä„ÅÆ„Ç≥„É≥„Éë„Ç§„É©„Åß„ÅÇ„Çä„ÄÅTensorFlow„Åß„ÅØ„Ç™„Éó„Ç∑„Éß„É≥„Åß„Åô„ÅåÔºà„Åó„Åã„ÅóTPU„Åß„ÅØÂøÖÈ†à„Åß„ÅôÔºÅÔºâ„ÄÅKeras„É¢„Éá„É´„Çí„Éà„É¨„Éº„Éã„É≥„Ç∞„Åô„ÇãÈöõ„Å´`model.compile()`„Å´ÂºïÊï∞`jit_compile=True`„ÇíÊ∏°„Åô„Åì„Å®„ÅßÊúÄ„ÇÇÁ∞°Âçò„Å´ÊúâÂäπ„Å´„Åß„Åç„Åæ„Åô„ÄÇ„Ç®„É©„Éº„ÅåÁô∫Áîü„Åõ„Åö„ÄÅ„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÅåËâØÂ•Ω„Åß„ÅÇ„Çå„Å∞„ÄÅ„Åù„Çå„ÅØTPU„Å´ÁßªË°å„Åô„ÇãÊ∫ñÂÇô„ÅåÊï¥„Å£„ÅüËâØ„ÅÑÂÖÜÂÄô„Åß„ÅôÔºÅ\n-\n-TPU‰∏ä„Åß„ÅÆ„Éá„Éê„ÉÉ„Ç∞„ÅØ‰∏ÄËà¨ÁöÑ„Å´CPU/GPU„Çà„Çä„ÇÇÂ∞ë„ÅóÈõ£„Åó„ÅÑ„Åü„ÇÅ„ÄÅTPU„ÅßË©¶„ÅôÂâç„Å´„Åæ„ÅöCPU/GPU„ÅßXLA„Çí‰ΩøÁî®„Åó„Å¶„Ç≥„Éº„Éâ„ÇíÂÆüË°å„Åô„Çã„Åì„Å®„Çí„ÅäÂãß„ÇÅ„Åó„Åæ„Åô„ÄÇ„ÇÇ„Å°„Çç„Çì„ÄÅÈï∑ÊôÇÈñì„Éà„É¨„Éº„Éã„É≥„Ç∞„Åô„ÇãÂøÖË¶Å„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ„É¢„Éá„É´„Å®„Éá„Éº„Çø„Éë„Ç§„Éó„É©„Ç§„É≥„ÅåÊúüÂæÖÈÄö„Çä„Å´Âãï‰Ωú„Åô„Çã„Åã„ÇíÁ¢∫Ë™ç„Åô„Çã„Åü„ÇÅ„ÅÆÊï∞„Çπ„ÉÜ„ÉÉ„Éó„Å†„Åë„Åß„Åô„ÄÇ\n-\n-<Tip>\n-\n-XLA„Ç≥„É≥„Éë„Ç§„É´„Åï„Çå„Åü„Ç≥„Éº„Éâ„ÅØÈÄöÂ∏∏È´òÈÄü„Åß„Åô„ÄÇ„Åó„Åü„Åå„Å£„Å¶„ÄÅTPU„ÅßÂÆüË°å„Åô„Çã‰∫àÂÆö„Åå„Å™„ÅÑÂ†¥Âêà„Åß„ÇÇ„ÄÅ`jit_compile=True`„ÇíËøΩÂä†„Åô„Çã„Åì„Å®„Åß„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÇíÂêë‰∏ä„Åï„Åõ„Çã„Åì„Å®„Åå„Åß„Åç„Åæ„Åô„ÄÇ„Åü„Å†„Åó„ÄÅ‰ª•‰∏ã„ÅÆXLA‰∫íÊèõÊÄß„Å´Èñ¢„Åô„ÇãÊ≥®ÊÑè‰∫ãÈ†Ö„Å´Ê≥®ÊÑè„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºÅ\n-\n-</Tip>\n-\n-<Tip warning={true}>\n-\n-**Ëã¶„ÅÑÁµåÈ®ì„Åã„ÇâÁîü„Åæ„Çå„Åü„Éí„É≥„Éà:** `jit_compile=True`„Çí‰ΩøÁî®„Åô„Çã„Åì„Å®„ÅØ„ÄÅCPU/GPU„Ç≥„Éº„Éâ„ÅåXLA‰∫íÊèõ„Åß„ÅÇ„Çã„Åì„Å®„ÇíÁ¢∫Ë™ç„Åó„ÄÅÈÄüÂ∫¶„ÇíÂêë‰∏ä„Åï„Åõ„ÇãËâØ„ÅÑÊñπÊ≥ï„Åß„Åô„Åå„ÄÅÂÆüÈöõ„Å´TPU„Åß„Ç≥„Éº„Éâ„ÇíÂÆüË°å„Åô„ÇãÈöõ„Å´„ÅØÂ§ö„Åè„ÅÆÂïèÈ°å„ÇíÂºï„ÅçËµ∑„Åì„ÅôÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ XLA„Ç≥„É≥„Éë„Ç§„É´„ÅØTPU‰∏ä„ÅßÊöóÈªôÁöÑ„Å´Ë°å„Çè„Çå„Çã„Åü„ÇÅ„ÄÅÂÆüÈöõ„Å´„Ç≥„Éº„Éâ„ÇíTPU„ÅßÂÆüË°å„Åô„ÇãÂâç„Å´„Åù„ÅÆË°å„ÇíÂâäÈô§„Åô„Çã„Åì„Å®„ÇíÂøò„Çå„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑÔºÅ\n-\n-</Tip>\n-\n-### How do I make my model XLA compatible?\n-\n-Â§ö„Åè„ÅÆÂ†¥Âêà„ÄÅ„Ç≥„Éº„Éâ„ÅØ„Åô„Åß„Å´XLA‰∫íÊèõ„Åã„ÇÇ„Åó„Çå„Åæ„Åõ„ÇìÔºÅ„Åü„Å†„Åó„ÄÅXLA„Åß„ÅØÂãï‰Ωú„Åô„ÇãÈÄöÂ∏∏„ÅÆTensorFlow„Åß„ÇÇÂãï‰Ωú„Åó„Å™„ÅÑ„ÅÑ„Åè„Å§„Åã„ÅÆË¶ÅÁ¥†„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ‰ª•‰∏ã„Å´„ÄÅ3„Å§„ÅÆ‰∏ªË¶Å„Å™„É´„Éº„É´„Å´„Åæ„Å®„ÇÅ„Å¶„ÅÑ„Åæ„ÅôÔºö\n-\n-<Tip>\n-\n-**ü§ó HuggingFaceÂõ∫Êúâ„ÅÆ„Éí„É≥„Éàü§ó:** TensorFlow„É¢„Éá„É´„Å®ÊêçÂ§±Èñ¢Êï∞„ÇíXLA‰∫íÊèõ„Å´Êõ∏„ÅçÁõ¥„Åô„Åü„ÇÅ„Å´Â§ö„Åè„ÅÆÂä™Âäõ„ÇíÊâï„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇÈÄöÂ∏∏„ÄÅ„É¢„Éá„É´„Å®ÊêçÂ§±Èñ¢Êï∞„ÅØ„Éá„Éï„Ç©„É´„Éà„Åß„É´„Éº„É´ÔºÉ1„Å®ÔºÉ2„Å´Âæì„Å£„Å¶„ÅÑ„Çã„Åü„ÇÅ„ÄÅ`transformers`„É¢„Éá„É´„Çí‰ΩøÁî®„Åó„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÅØ„Åì„Çå„Çâ„Çí„Çπ„Ç≠„ÉÉ„Éó„Åß„Åç„Åæ„Åô„ÄÇ„Åü„Å†„Åó„ÄÅÁã¨Ëá™„ÅÆ„É¢„Éá„É´„Å®ÊêçÂ§±Èñ¢Êï∞„ÇíË®òËø∞„Åô„ÇãÂ†¥Âêà„ÅØ„ÄÅ„Åì„Çå„Çâ„ÅÆ„É´„Éº„É´„ÇíÂøò„Çå„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑÔºÅ\n-\n-</Tip>\n-\n-#### XLA Rule #1: Your code cannot have ‚Äúdata-dependent conditionals‚Äù\n-\n-„Åì„Çå„ÅØ„ÄÅ‰ªªÊÑè„ÅÆ`if`„Çπ„ÉÜ„Éº„Éà„É°„É≥„Éà„Åå`tf.Tensor`ÂÜÖ„ÅÆÂÄ§„Å´‰æùÂ≠ò„Åó„Å¶„ÅÑ„Å™„ÅÑÂøÖË¶Å„Åå„ÅÇ„Çã„Åì„Å®„ÇíÊÑèÂë≥„Åó„Åæ„Åô„ÄÇ‰æã„Åà„Å∞„ÄÅÊ¨°„ÅÆ„Ç≥„Éº„Éâ„Éñ„É≠„ÉÉ„ÇØ„ÅØXLA„Åß„Ç≥„É≥„Éë„Ç§„É´„Åß„Åç„Åæ„Åõ„ÇìÔºÅ\n-\n-```python\n-if tf.reduce_sum(tensor) > 10:\n-    tensor = tensor / 2.0\n-```\n-\n-„Åì„Çå„ÅØÊúÄÂàù„ÅØÈùûÂ∏∏„Å´Âà∂ÈôêÁöÑ„Å´ÊÄù„Åà„Çã„Åã„ÇÇ„Åó„Çå„Åæ„Åõ„Çì„Åå„ÄÅ„Åª„Å®„Çì„Å©„ÅÆ„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„Ç≥„Éº„Éâ„ÅØ„Åì„Çå„ÇíË°å„ÅÜÂøÖË¶Å„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇÈÄöÂ∏∏„ÄÅ„Åì„ÅÆÂà∂Á¥Ñ„ÇíÂõûÈÅø„Åô„Çã„Åü„ÇÅ„Å´`tf.cond`„Çí‰ΩøÁî®„Åô„Çã„ÅãÔºà„Éâ„Ç≠„É•„É°„É≥„Éà„ÅØ„Åì„Å°„Çâ„ÇíÂèÇÁÖßÔºâ„ÄÅÊù°‰ª∂„ÇíÂâäÈô§„Åó„Å¶‰ª£„Çè„Çä„Å´ÊåáÁ§∫Â§âÊï∞„Çí‰ΩøÁî®„Åó„Åü„Çä„Åô„Çã„Åì„Å®„Åå„Åß„Åç„Åæ„Åô„ÄÇÊ¨°„ÅÆ„Çà„ÅÜ„Å´Ôºö\n-\n-```python\n-sum_over_10 = tf.cast(tf.reduce_sum(tensor) > 10, tf.float32)\n-tensor = tensor / (1.0 + sum_over_10)\n-```\n-\n-„Åì„ÅÆ„Ç≥„Éº„Éâ„ÅØ„ÄÅ‰∏äË®ò„ÅÆ„Ç≥„Éº„Éâ„Å®„Åæ„Å£„Åü„ÅèÂêå„ÅòÂäπÊûú„ÇíÊåÅ„Å£„Å¶„ÅÑ„Åæ„Åô„Åå„ÄÅÊù°‰ª∂„ÇíÂõûÈÅø„Åô„Çã„Åì„Å®„Åß„ÄÅXLA„ÅßÂïèÈ°å„Å™„Åè„Ç≥„É≥„Éë„Ç§„É´„Åß„Åç„Çã„Åì„Å®„ÇíÁ¢∫Ë™ç„Åó„Åæ„ÅôÔºÅ\n-\n-#### XLA Rule #2: Your code cannot have ‚Äúdata-dependent shapes‚Äù\n-\n-„Åì„Çå„ÅØ„ÄÅ„Ç≥„Éº„ÉâÂÜÖ„ÅÆ„Åô„Åπ„Å¶„ÅÆ `tf.Tensor` „Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„ÅÆÂΩ¢Áä∂„Åå„ÄÅ„Åù„ÅÆÂÄ§„Å´‰æùÂ≠ò„Åó„Å™„ÅÑ„Åì„Å®„ÇíÊÑèÂë≥„Åó„Åæ„Åô„ÄÇ„Åü„Å®„Åà„Å∞„ÄÅ`tf.unique` Èñ¢Êï∞„ÅØXLA„Åß„Ç≥„É≥„Éë„Ç§„É´„Åß„Åç„Å™„ÅÑ„ÅÆ„Åß„ÄÅ„Åì„ÅÆ„É´„Éº„É´„Å´ÈÅïÂèç„Åó„Åæ„Åô„ÄÇ„Å™„Åú„Å™„Çâ„ÄÅ„Åì„Çå„ÅØÂÖ•Âäõ `Tensor` „ÅÆ‰∏ÄÊÑè„ÅÆÂÄ§„ÅÆÂêÑ„Ç§„É≥„Çπ„Çø„É≥„Çπ„ÇíÂê´„ÇÄ `tensor` „ÇíËøî„Åô„Åü„ÇÅ„Åß„Åô„ÄÇ„Åì„ÅÆÂá∫Âäõ„ÅÆÂΩ¢Áä∂„ÅØ„ÄÅÂÖ•Âäõ `Tensor` „ÅÆÈáçË§áÂÖ∑Âêà„Å´„Çà„Å£„Å¶Áï∞„Å™„Çã„Åü„ÇÅ„ÄÅXLA„ÅØ„Åù„Çå„ÇíÂá¶ÁêÜ„Åó„Å™„ÅÑ„Åì„Å®„Å´„Å™„Çä„Åæ„ÅôÔºÅ\n-\n-‰∏ÄËà¨ÁöÑ„Å´„ÄÅ„Åª„Å®„Çì„Å©„ÅÆ„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„Ç≥„Éº„Éâ„ÅØ„Éá„Éï„Ç©„É´„Éà„Åß„É´„Éº„É´ÔºÉ2„Å´Âæì„ÅÑ„Åæ„Åô„ÄÇ„Åü„Å†„Åó„ÄÅ„ÅÑ„Åè„Å§„Åã„ÅÆ‰∏ÄËà¨ÁöÑ„Å™„Ç±„Éº„Çπ„Åß„ÅØÂïèÈ°å„ÅåÁô∫Áîü„Åô„Çã„Åì„Å®„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇÈùûÂ∏∏„Å´‰∏ÄËà¨ÁöÑ„Å™„Ç±„Éº„Çπ„ÅÆ1„Å§„ÅØ„ÄÅ**„É©„Éô„É´„Éû„Çπ„Ç≠„É≥„Ç∞**„Çí‰ΩøÁî®„Åô„ÇãÂ†¥Âêà„Åß„Åô„ÄÇ„É©„Éô„É´„ÇíÁÑ°Ë¶ñ„Åó„Å¶ÊêçÂ§±„ÇíË®àÁÆó„Åô„ÇãÂ†¥ÊâÄ„ÇíÁ§∫„Åô„Åü„ÇÅ„Å´„ÄÅ„É©„Éô„É´„ÇíË≤†„ÅÆÂÄ§„Å´Ë®≠ÂÆö„Åô„ÇãÊñπÊ≥ï„Åß„Åô„ÄÇNumPy„Åæ„Åü„ÅØPyTorch„ÅÆ„É©„Éô„É´„Éû„Çπ„Ç≠„É≥„Ç∞„Çí„Çµ„Éù„Éº„Éà„Åô„ÇãÊêçÂ§±Èñ¢Êï∞„ÇíË¶ã„Çã„Å®„ÄÅÊ¨°„ÅÆ„Çà„ÅÜ„Å™[„Éñ„Éº„É´„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ](https://numpy.org/doc/stable/user/basics.indexing.html#boolean-array-indexing)„Çí‰ΩøÁî®„Åó„Åü„Ç≥„Éº„Éâ„Åå„Çà„ÅèË¶ã„Çâ„Çå„Åæ„ÅôÔºö\n-\n-\n-```python\n-label_mask = labels >= 0\n-masked_outputs = outputs[label_mask]\n-masked_labels = labels[label_mask]\n-loss = compute_loss(masked_outputs, masked_labels)\n-mean_loss = torch.mean(loss)\n-```\n-\n-„Åì„ÅÆ„Ç≥„Éº„Éâ„ÅØNumPy„ÇÑPyTorch„Åß„ÅØÂÆåÂÖ®„Å´Ê©üËÉΩ„Åó„Åæ„Åô„Åå„ÄÅXLA„Åß„ÅØÂãï‰Ωú„Åó„Åæ„Åõ„ÇìÔºÅ„Å™„Åú„Å™„Çâ„ÄÅ`masked_outputs`„Å®`masked_labels`„ÅÆÂΩ¢Áä∂„ÅØ„Éû„Çπ„ÇØ„Åï„Çå„Åü‰ΩçÁΩÆ„ÅÆÊï∞„Å´‰æùÂ≠ò„Åô„Çã„Åü„ÇÅ„ÄÅ„Åì„Çå„ÅØ**„Éá„Éº„Çø‰æùÂ≠ò„ÅÆÂΩ¢Áä∂**„Å´„Å™„Çä„Åæ„Åô„ÄÇ„Åü„Å†„Åó„ÄÅ„É´„Éº„É´ÔºÉ1„Å®ÂêåÊßò„Å´„ÄÅ„Åì„ÅÆ„Ç≥„Éº„Éâ„ÇíÊõ∏„ÅçÁõ¥„Åó„Å¶„ÄÅ„Éá„Éº„Çø‰æùÂ≠ò„ÅÆÂΩ¢Áä∂„Å™„Åó„Åß„Åæ„Å£„Åü„ÅèÂêå„ÅòÂá∫Âäõ„ÇíÁîüÊàê„Åß„Åç„Çã„Åì„Å®„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ\n-\n-\n-```python\n-label_mask = tf.cast(labels >= 0, tf.float32)\n-loss = compute_loss(outputs, labels)\n-loss = loss * label_mask  # Set negative label positions to 0\n-mean_loss = tf.reduce_sum(loss) / tf.reduce_sum(label_mask)\n-```\n-\n-\n-„Åì„Åì„Åß„ÅØ„ÄÅ„Éá„Éº„Çø‰æùÂ≠ò„ÅÆÂΩ¢Áä∂„ÇíÈÅø„Åë„Çã„Åü„ÇÅ„Å´„ÄÅÂêÑ‰ΩçÁΩÆ„ÅßÊêçÂ§±„ÇíË®àÁÆó„Åó„Å¶„Åã„Çâ„ÄÅÂπ≥Âùá„ÇíË®àÁÆó„Åô„ÇãÈöõ„Å´ÂàÜÂ≠ê„Å®ÂàÜÊØç„ÅÆ‰∏°Êñπ„Åß„Éû„Çπ„ÇØ„Åï„Çå„Åü‰ΩçÁΩÆ„Çí„Çº„É≠Âåñ„Åô„ÇãÊñπÊ≥ï„ÇíÁ¥π‰ªã„Åó„Åæ„Åô„ÄÇ„Åì„Çå„Å´„Çà„Çä„ÄÅÊúÄÂàù„ÅÆ„Ç¢„Éó„É≠„Éº„ÉÅ„Å®„Åæ„Å£„Åü„ÅèÂêå„ÅòÁµêÊûú„ÅåÂæó„Çâ„Çå„Åæ„Åô„Åå„ÄÅXLA‰∫íÊèõÊÄß„ÇíÁ∂≠ÊåÅ„Åó„Åæ„Åô„ÄÇÊ≥®ÊÑèÁÇπ„Å®„Åó„Å¶„ÄÅ„É´„Éº„É´ÔºÉ1„Å®Âêå„Åò„Éà„É™„ÉÉ„ÇØ„Çí‰ΩøÁî®„Åó„Åæ„Åô - `tf.bool`„Çí`tf.float32`„Å´Â§âÊèõ„Åó„Å¶ÊåáÊ®ôÂ§âÊï∞„Å®„Åó„Å¶‰ΩøÁî®„Åó„Åæ„Åô„ÄÇ„Åì„Çå„ÅØÈùûÂ∏∏„Å´‰æøÂà©„Å™„Éà„É™„ÉÉ„ÇØ„Åß„Åô„ÅÆ„Åß„ÄÅËá™ÂàÜ„ÅÆ„Ç≥„Éº„Éâ„ÇíXLA„Å´Â§âÊèõ„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„ÇãÂ†¥Âêà„Å´„ÅØË¶ö„Åà„Å¶„Åä„ÅÑ„Å¶„Åè„Å†„Åï„ÅÑÔºÅ\n-\n-#### XLA Rule #3: XLA will need to recompile your model for every different input shape it sees\n-\n-„Åì„Çå„ÅØÈáçË¶Å„Å™„É´„Éº„É´„Åß„Åô„ÄÇ„Åì„Çå„ÅØ„Å§„Åæ„Çä„ÄÅÂÖ•ÂäõÂΩ¢Áä∂„ÅåÈùûÂ∏∏„Å´Â§âÂãïÁöÑ„Å™Â†¥Âêà„ÄÅXLA „ÅØ„É¢„Éá„É´„Çí‰ΩïÂ∫¶„ÇÇÂÜç„Ç≥„É≥„Éë„Ç§„É´„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çã„Åü„ÇÅ„ÄÅÂ§ß„Åç„Å™„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÅÆÂïèÈ°å„ÅåÁô∫Áîü„Åô„ÇãÂèØËÉΩÊÄß„Åå„ÅÇ„Çã„Å®„ÅÑ„ÅÜ„Åì„Å®„Åß„Åô„ÄÇ„Åì„Çå„ÅØ NLP „É¢„Éá„É´„Åß‰∏ÄËà¨ÁöÑ„Å´Áô∫Áîü„Åó„ÄÅ„Éà„Éº„ÇØ„Éä„Ç§„Ç∫Âæå„ÅÆÂÖ•Âäõ„ÉÜ„Ç≠„Çπ„Éà„ÅÆÈï∑„Åï„ÅåÁï∞„Å™„ÇãÂ†¥Âêà„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ‰ªñ„ÅÆ„É¢„ÉÄ„É™„ÉÜ„Ç£„Åß„ÅØ„ÄÅÈùôÁöÑ„Å™ÂΩ¢Áä∂„Åå‰∏ÄËà¨ÁöÑ„Åß„ÅÇ„Çä„ÄÅ„Åì„ÅÆ„É´„Éº„É´„ÅØ„Åª„Å®„Çì„Å©ÂïèÈ°å„Å´„Å™„Çä„Åæ„Åõ„Çì„ÄÇ\n-\n-„É´„Éº„É´ÔºÉ3„ÇíÂõûÈÅø„Åô„ÇãÊñπÊ≥ï„ÅØ‰Ωï„Åß„Åó„Çá„ÅÜ„ÅãÔºüÈçµ„ÅØ„Äå„Éë„Éá„Ç£„É≥„Ç∞„Äç„Åß„Åô - „Åô„Åπ„Å¶„ÅÆÂÖ•Âäõ„ÇíÂêå„ÅòÈï∑„Åï„Å´„Éë„Éá„Ç£„É≥„Ç∞„Åó„ÄÅÊ¨°„Å´„Äåattention_mask„Äç„Çí‰ΩøÁî®„Åô„Çã„Åì„Å®„Åß„ÄÅÂèØÂ§âÂΩ¢Áä∂„Å®Âêå„ÅòÁµêÊûú„ÇíÂæó„Çã„Åì„Å®„Åå„Åß„Åç„Åæ„Åô„Åå„ÄÅXLA „ÅÆÂïèÈ°å„ÅØÁô∫Áîü„Åó„Åæ„Åõ„Çì„ÄÇ„Åü„Å†„Åó„ÄÅÈÅéÂ∫¶„ÅÆ„Éë„Éá„Ç£„É≥„Ç∞„ÇÇÊ∑±Âàª„Å™ÈÅÖÂª∂„ÇíÂºï„ÅçËµ∑„Åì„ÅôÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„Åô - „Éá„Éº„Çø„Çª„ÉÉ„ÉàÂÖ®‰Ωì„ÅßÊúÄÂ§ß„ÅÆÈï∑„Åï„Å´„Åô„Åπ„Å¶„ÅÆ„Çµ„É≥„Éó„É´„Çí„Éë„Éá„Ç£„É≥„Ç∞„Åô„Çã„Å®„ÄÅÂ§ö„Åè„ÅÆË®àÁÆó„Å®„É°„É¢„É™„ÇíÁÑ°ÈßÑ„Å´„Åô„ÇãÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„ÅôÔºÅ\n-\n-„Åì„ÅÆÂïèÈ°å„Å´„ÅØÂÆåÁíß„Å™Ëß£Ê±∫Á≠ñ„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„Åå„ÄÅ„ÅÑ„Åè„Å§„Åã„ÅÆ„Éà„É™„ÉÉ„ÇØ„ÇíË©¶„Åô„Åì„Å®„Åå„Åß„Åç„Åæ„Åô„ÄÇÈùûÂ∏∏„Å´‰æøÂà©„Å™„Éà„É™„ÉÉ„ÇØ„ÅÆ1„Å§„ÅØ„ÄÅ**„Éê„ÉÉ„ÉÅ„ÅÆ„Çµ„É≥„Éó„É´„Çí32„Åæ„Åü„ÅØ64„Éà„Éº„ÇØ„É≥„ÅÆÂÄçÊï∞„Åæ„Åß„Éë„Éá„Ç£„É≥„Ç∞„Åô„Çã**„Åì„Å®„Åß„Åô„ÄÇ„Åì„Çå„Å´„Çà„Çä„ÄÅ„Éà„Éº„ÇØ„É≥Êï∞„Åå„Çè„Åö„Åã„Å´Â¢óÂä†„Åô„Çã„Å†„Åë„Åß„ÄÅ„Åô„Åπ„Å¶„ÅÆÂÖ•ÂäõÂΩ¢Áä∂„Åå32„Åæ„Åü„ÅØ64„ÅÆÂÄçÊï∞„Åß„ÅÇ„ÇãÂøÖË¶Å„Åå„ÅÇ„Çã„Åü„ÇÅ„ÄÅ‰∏ÄÊÑè„ÅÆÂÖ•ÂäõÂΩ¢Áä∂„ÅÆÊï∞„ÅåÂ§ßÂπÖ„Å´Ê∏õÂ∞ë„Åó„Åæ„Åô„ÄÇ‰∏ÄÊÑè„ÅÆÂÖ•ÂäõÂΩ¢Áä∂„ÅåÂ∞ë„Å™„ÅÑ„Å®„ÄÅXLA „ÅÆÂÜç„Ç≥„É≥„Éë„Ç§„É´„ÅåÂ∞ë„Å™„Åè„Å™„Çä„Åæ„ÅôÔºÅ\n-\n-<Tip>\n-\n-**ü§ó HuggingFace „Å´Èñ¢„Åô„ÇãÂÖ∑‰ΩìÁöÑ„Å™„Éí„É≥„Éàü§ó:** ÂºäÁ§æ„ÅÆ„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éº„Å®„Éá„Éº„Çø„Ç≥„É¨„ÇØ„Çø„Éº„Å´„ÅØ„ÄÅ„Åì„Åì„ÅßÂΩπÁ´ã„Å§„É°„ÇΩ„ÉÉ„Éâ„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éº„ÇíÂëº„Å≥Âá∫„ÅôÈöõ„Å´ `padding=\"max_length\"` „Åæ„Åü„ÅØ `padding=\"longest\"` „Çí‰ΩøÁî®„Åó„Å¶„ÄÅ„Éë„Éá„Ç£„É≥„Ç∞„Åï„Çå„Åü„Éá„Éº„Çø„ÇíÂá∫Âäõ„Åô„Çã„Çà„ÅÜ„Å´Ë®≠ÂÆö„Åß„Åç„Åæ„Åô„ÄÇ„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éº„Å®„Éá„Éº„Çø„Ç≥„É¨„ÇØ„Çø„Éº„Å´„ÅØ„ÄÅ‰∏ÄÊÑè„ÅÆÂÖ•ÂäõÂΩ¢Áä∂„ÅÆÊï∞„ÇíÊ∏õ„Çâ„Åô„ÅÆ„Å´ÂΩπÁ´ã„Å§ `pad_to_multiple_of` ÂºïÊï∞„ÇÇ„ÅÇ„Çä„Åæ„ÅôÔºÅ\n-\n-</Tip>\n-\n-### How do I actually train my model on TPU?\n-\n-‰∏ÄÂ∫¶„Éà„É¨„Éº„Éã„É≥„Ç∞„Åå XLA ‰∫íÊèõÊÄß„Åå„ÅÇ„Çã„Åì„Å®„ÇíÁ¢∫Ë™ç„Åó„ÄÅÔºàTPU Node/Colab „Çí‰ΩøÁî®„Åô„ÇãÂ†¥Âêà„ÅØÔºâ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅåÈÅ©Âàá„Å´Ê∫ñÂÇô„Åï„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÄÅTPU ‰∏ä„ÅßÂÆüË°å„Åô„Çã„Åì„Å®„ÅØÈ©ö„Åè„Åª„Å©Á∞°Âçò„Åß„ÅôÔºÅ„Ç≥„Éº„Éâ„ÇíÂ§âÊõ¥„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çã„ÅÆ„ÅØ„ÄÅ„ÅÑ„Åè„Å§„Åã„ÅÆË°å„ÇíËøΩÂä†„Åó„Å¶ TPU „ÇíÂàùÊúüÂåñ„Åó„ÄÅ„É¢„Éá„É´„Å®„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åå `TPUStrategy` „Çπ„Ç≥„Éº„ÉóÂÜÖ„Åß‰ΩúÊàê„Åï„Çå„Çã„Çà„ÅÜ„Å´„Åô„Çã„Åì„Å®„Å†„Åë„Åß„Åô„ÄÇ„Åì„Çå„ÇíÂÆüÈöõ„Å´Ë¶ã„Çã„Å´„ÅØ„ÄÅ[TPU „ÅÆ„Çµ„É≥„Éó„É´„Éé„Éº„Éà„Éñ„ÉÉ„ÇØ](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb)„Çí„ÅîË¶ß„Åè„Å†„Åï„ÅÑÔºÅ\n-\n-### Summary\n-\n-„Åì„Åì„Åß„ÅØÂ§ö„Åè„ÅÆÊÉÖÂ†±„ÅåÊèê‰æõ„Åï„Çå„Åæ„Åó„Åü„ÅÆ„Åß„ÄÅTPU „Åß„É¢„Éá„É´„Çí„Éà„É¨„Éº„Éã„É≥„Ç∞„Åô„ÇãÈöõ„Å´‰ª•‰∏ã„ÅÆ„ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„Éà„Çí‰ΩøÁî®„Åß„Åç„Åæ„ÅôÔºö\n-\n-- „Ç≥„Éº„Éâ„Åå XLA „ÅÆ‰∏â„Å§„ÅÆ„É´„Éº„É´„Å´Âæì„Å£„Å¶„ÅÑ„Çã„Åì„Å®„ÇíÁ¢∫Ë™ç„Åó„Åæ„Åô„ÄÇ\n-- CPU/GPU „Åß `jit_compile=True` „Çí‰ΩøÁî®„Åó„Å¶„É¢„Éá„É´„Çí„Ç≥„É≥„Éë„Ç§„É´„Åó„ÄÅXLA „Åß„Éà„É¨„Éº„Éã„É≥„Ç∞„Åß„Åç„Çã„Åì„Å®„ÇíÁ¢∫Ë™ç„Åó„Åæ„Åô„ÄÇ\n-- „Éá„Éº„Çø„Çª„ÉÉ„Éà„Çí„É°„É¢„É™„Å´Ë™≠„ÅøËæº„ÇÄ„Åã„ÄÅTPU ‰∫íÊèõ„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„ÉàË™≠„ÅøËæº„Åø„Ç¢„Éó„É≠„Éº„ÉÅ„Çí‰ΩøÁî®„Åó„Åæ„ÅôÔºà[„Éé„Éº„Éà„Éñ„ÉÉ„ÇØ„ÇíÂèÇÁÖß](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb)Ôºâ„ÄÇ\n-- „Ç≥„Éº„Éâ„Çí ColabÔºà„Ç¢„ÇØ„Çª„É©„É¨„Éº„Çø„Çí„ÄåTPU„Äç„Å´Ë®≠ÂÆöÔºâ„Åæ„Åü„ÅØ Google Cloud „ÅÆ TPU VM „Å´ÁßªË°å„Åó„Åæ„Åô„ÄÇ\n-- TPU ÂàùÊúüÂåñ„Ç≥„Éº„Éâ„ÇíËøΩÂä†„Åó„Åæ„ÅôÔºà[„Éé„Éº„Éà„Éñ„ÉÉ„ÇØ„ÇíÂèÇÁÖß](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb)Ôºâ„ÄÇ\n-- `TPUStrategy` „Çí‰ΩúÊàê„Åó„ÄÅ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆË™≠„ÅøËæº„Åø„Å®„É¢„Éá„É´„ÅÆ‰ΩúÊàê„Åå `strategy.scope()` ÂÜÖ„ÅßË°å„Çè„Çå„Çã„Åì„Å®„ÇíÁ¢∫Ë™ç„Åó„Åæ„ÅôÔºà[„Éé„Éº„Éà„Éñ„ÉÉ„ÇØ„ÇíÂèÇÁÖß](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb)Ôºâ„ÄÇ\n-- TPU „Å´ÁßªË°å„Åô„ÇãÈöõ„Å´ `jit_compile=True` „ÇíÂ§ñ„Åô„ÅÆ„ÇíÂøò„Çå„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑÔºÅ\n-- üôèüôèüôèü•∫ü•∫ü•∫\n-- `model.fit()` „ÇíÂëº„Å≥Âá∫„Åó„Åæ„Åô„ÄÇ\n-- „Åä„ÇÅ„Åß„Å®„ÅÜ„Åî„Åñ„ÅÑ„Åæ„ÅôÔºÅ\n-\n-"
        },
        {
            "sha": "1f5a2af1a5a2880268423324f65e8c5780337397",
            "filename": "docs/source/ja/tf_xla.md",
            "status": "removed",
            "additions": 0,
            "deletions": 179,
            "changes": 179,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/docs%2Fsource%2Fja%2Ftf_xla.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/docs%2Fsource%2Fja%2Ftf_xla.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftf_xla.md?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,179 +0,0 @@\n-<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# XLA Integration for TensorFlow Models\n-\n-[[open-in-colab]]\n-\n-Âä†ÈÄüÁ∑öÂΩ¢‰ª£Êï∞ÔºàAccelerated Linear AlgebraÔºâ„ÄÅÈÄöÁß∞XLA„ÅØ„ÄÅTensorFlow„É¢„Éá„É´„ÅÆ„É©„É≥„Çø„Ç§„É†„ÇíÈ´òÈÄüÂåñ„Åô„Çã„Åü„ÇÅ„ÅÆ„Ç≥„É≥„Éë„Ç§„É©„Åß„Åô„ÄÇ[ÂÖ¨Âºè„Éâ„Ç≠„É•„É°„É≥„Éà](https://www.tensorflow.org/xla)„Å´„Çà„Çå„Å∞„ÄÅXLAÔºàAccelerated Linear AlgebraÔºâ„ÅØÁ∑öÂΩ¢‰ª£Êï∞„ÅÆ„Åü„ÇÅ„ÅÆ„Éâ„É°„Ç§„É≥Âõ∫Êúâ„ÅÆ„Ç≥„É≥„Éë„Ç§„É©„Åß„ÄÅTensorFlow„É¢„Éá„É´„ÇíÊΩúÂú®ÁöÑ„Å´„ÇΩ„Éº„Çπ„Ç≥„Éº„Éâ„ÅÆÂ§âÊõ¥„Å™„Åó„ÅßÈ´òÈÄüÂåñ„Åß„Åç„Åæ„Åô„ÄÇ\n-\n-TensorFlow„ÅßXLA„Çí‰ΩøÁî®„Åô„Çã„ÅÆ„ÅØÁ∞°Âçò„Åß„Åô„ÄÇXLA„ÅØ`tensorflow`„É©„Ç§„Éñ„É©„É™ÂÜÖ„Å´„Éë„ÉÉ„Ç±„Éº„Ç∏Âåñ„Åï„Çå„Å¶„Åä„Çä„ÄÅ[`tf.function`](https://www.tensorflow.org/guide/intro_to_graphs)„Å™„Å©„ÅÆ„Ç∞„É©„Éï„Çí‰ΩúÊàê„Åô„ÇãÈñ¢Êï∞ÂÜÖ„Åß`jit_compile`ÂºïÊï∞„Çí‰ΩøÁî®„Åó„Å¶„Éà„É™„Ç¨„Éº„Åß„Åç„Åæ„Åô„ÄÇ`fit()`„ÇÑ`predict()`„Å™„Å©„ÅÆKeras„É°„ÇΩ„ÉÉ„Éâ„Çí‰ΩøÁî®„Åô„ÇãÂ†¥Âêà„ÄÅ`model.compile()`„Å´`jit_compile`ÂºïÊï∞„ÇíÊ∏°„Åô„Å†„Åë„ÅßXLA„ÇíÊúâÂäπ„Å´„Åß„Åç„Åæ„Åô„ÄÇ„Åü„Å†„Åó„ÄÅXLA„ÅØ„Åì„Çå„Çâ„ÅÆ„É°„ÇΩ„ÉÉ„Éâ„Å´ÈôêÂÆö„Åï„Çå„Å¶„ÅÑ„Çã„Çè„Åë„Åß„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ‰ªªÊÑè„ÅÆ`tf.function`„ÇíÈ´òÈÄüÂåñ„Åô„Çã„Åü„ÇÅ„Å´„ÇÇ‰ΩøÁî®„Åß„Åç„Åæ„Åô„ÄÇ\n-\n-ü§ó TransformersÂÜÖ„ÅÆ„ÅÑ„Åè„Å§„Åã„ÅÆTensorFlow„É°„ÇΩ„ÉÉ„Éâ„ÅØ„ÄÅXLA„Å®‰∫íÊèõÊÄß„Åå„ÅÇ„Çã„Çà„ÅÜ„Å´Êõ∏„ÅçÁõ¥„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Åì„Çå„Å´„ÅØ„ÄÅ[GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)„ÄÅ[T5](https://huggingface.co/docs/transformers/model_doc/t5)„ÄÅ[OPT](https://huggingface.co/docs/transformers/model_doc/opt)„Å™„Å©„ÅÆ„ÉÜ„Ç≠„Çπ„ÉàÁîüÊàê„É¢„Éá„É´„ÇÑ„ÄÅ[Whisper](https://huggingface.co/docs/transformers/model_doc/whisper)„Å™„Å©„ÅÆÈü≥Â£∞Âá¶ÁêÜ„É¢„Éá„É´„ÇÇÂê´„Åæ„Çå„Åæ„Åô„ÄÇ\n-\n-ÈÄüÂ∫¶Âêë‰∏ä„ÅÆÂÖ∑‰ΩìÁöÑ„Å™Èáè„ÅØ„É¢„Éá„É´„Å´ÈùûÂ∏∏„Å´‰æùÂ≠ò„Åó„Åæ„Åô„Åå„ÄÅü§ó TransformersÂÜÖ„ÅÆTensorFlow„ÉÜ„Ç≠„Çπ„ÉàÁîüÊàê„É¢„Éá„É´„Åß„ÅØ„ÄÅÁ¥Ñ100ÂÄç„ÅÆÈÄüÂ∫¶Âêë‰∏ä„ÇíÁ¢∫Ë™ç„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Åì„ÅÆ„Éâ„Ç≠„É•„É°„É≥„Éà„Åß„ÅØ„ÄÅ„Åì„Çå„Çâ„ÅÆ„É¢„Éá„É´„Å´XLA„Çí‰ΩøÁî®„Åó„Å¶ÊúÄÂ§ß„ÅÆ„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÇíÂæó„ÇãÊñπÊ≥ï„ÇíË™¨Êòé„Åó„Åæ„Åô„ÄÇ„Åæ„Åü„ÄÅ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Å®XLAÁµ±Âêà„ÅÆ„Éá„Ç∂„Ç§„É≥Âì≤Â≠¶„Å´„Å§„ÅÑ„Å¶Ë©≥„Åó„ÅèÂ≠¶„Å≥„Åü„ÅÑÂ†¥Âêà„ÅÆËøΩÂä†„É™„ÇΩ„Éº„Çπ„Å∏„ÅÆ„É™„É≥„ÇØ„ÇÇÊèê‰æõ„Åó„Åæ„Åô„ÄÇ\n-\n-## Running TF functions with XLA\n-\n-‰ª•‰∏ã„ÅÆTensorFlow„É¢„Éá„É´„ÇíËÄÉ„Åà„Å¶„Åø„Åæ„Åó„Çá„ÅÜÔºö\n-\n-\n-```py\n-import tensorflow as tf\n-\n-model = tf.keras.Sequential(\n-    [tf.keras.layers.Dense(10, input_shape=(10,), activation=\"relu\"), tf.keras.layers.Dense(5, activation=\"softmax\")]\n-)\n-```\n-\n-‰∏äË®ò„ÅÆ„É¢„Éá„É´„ÅØ„ÄÅÊ¨°ÂÖÉ„Åå`(10, )`„ÅÆÂÖ•Âäõ„ÇíÂèó„ÅëÂÖ•„Çå„Åæ„Åô„ÄÇ„Åì„ÅÆ„É¢„Éá„É´„Çí„Éï„Ç©„ÉØ„Éº„Éâ„Éë„Çπ„ÅßÂÆüË°å„Åô„Çã„Å´„ÅØ„ÄÅÊ¨°„ÅÆ„Çà„ÅÜ„Å´„Åó„Åæ„ÅôÔºö\n-\n-\n-```py\n-# Generate random inputs for the model.\n-batch_size = 16\n-input_vector_dim = 10\n-random_inputs = tf.random.normal((batch_size, input_vector_dim))\n-\n-# Run a forward pass.\n-_ = model(random_inputs)\n-```\n-\n-XLA„Åß„Ç≥„É≥„Éë„Ç§„É´„Åï„Çå„ÅüÈñ¢Êï∞„Çí‰ΩøÁî®„Åó„Å¶„Éï„Ç©„ÉØ„Éº„Éâ„Éë„Çπ„ÇíÂÆüË°å„Åô„Çã„Å´„ÅØ„ÄÅ‰ª•‰∏ã„ÅÆ„Çà„ÅÜ„Å´„Åó„Åæ„ÅôÔºö\n-\n-\n-```py\n-xla_fn = tf.function(model, jit_compile=True)\n-_ = xla_fn(random_inputs)\n-```\n-\n-`model`„ÅÆ„Éá„Éï„Ç©„É´„Éà„ÅÆ `call()` Èñ¢Êï∞„ÅØXLA„Ç∞„É©„Éï„Çí„Ç≥„É≥„Éë„Ç§„É´„Åô„Çã„Åü„ÇÅ„Å´‰ΩøÁî®„Åï„Çå„Åæ„Åô„ÄÇ„Åü„Å†„Åó„ÄÅXLA„Å´„Ç≥„É≥„Éë„Ç§„É´„Åó„Åü„ÅÑ‰ªñ„ÅÆ„É¢„Éá„É´Èñ¢Êï∞„Åå„ÅÇ„ÇãÂ†¥Âêà„ÄÅ„Åù„Çå„ÇÇÂèØËÉΩ„Åß„Åô„ÄÇ‰ª•‰∏ã„ÅØ„Åù„ÅÆÊñπÊ≥ï„Åß„ÅôÔºö\n-\n-\n-```py\n-my_xla_fn = tf.function(model.my_xla_fn, jit_compile=True)\n-```\n-\n-## Running a TF text generation model with XLA from ü§ó Transformers\n-\n-ü§ó TransformersÂÜÖ„ÅßXLA„Åß„ÅÆÈ´òÈÄüÂåñ„Åï„Çå„ÅüÁîüÊàê„ÇíÊúâÂäπ„Å´„Åô„Çã„Å´„ÅØ„ÄÅÊúÄÊñ∞„Éê„Éº„Ç∏„Éß„É≥„ÅÆ`transformers`„Åå„Ç§„É≥„Çπ„Éà„Éº„É´„Åï„Çå„Å¶„ÅÑ„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇÊ¨°„ÅÆ„Ç≥„Éû„É≥„Éâ„ÇíÂÆüË°å„Åó„Å¶„Ç§„É≥„Çπ„Éà„Éº„É´„Åß„Åç„Åæ„ÅôÔºö\n-\n-```bash\n-pip install transformers --upgrade\n-```\n-\n-Ê¨°„Å´„ÄÅÊ¨°„ÅÆ„Ç≥„Éº„Éâ„ÇíÂÆüË°å„Åß„Åç„Åæ„ÅôÔºö\n-\n-\n-```py\n-import tensorflow as tf\n-from transformers import AutoTokenizer, TFAutoModelForCausalLM\n-\n-# Will error if the minimal version of Transformers is not installed.\n-from transformers.utils import check_min_version\n-\n-check_min_version(\"4.21.0\")\n-\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", padding_side=\"left\", pad_token=\"</s>\")\n-model = TFAutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n-input_string = [\"TensorFlow is\"]\n-\n-# One line to create an XLA generation function\n-xla_generate = tf.function(model.generate, jit_compile=True)\n-\n-tokenized_input = tokenizer(input_string, return_tensors=\"tf\")\n-generated_tokens = xla_generate(**tokenized_input, num_beams=2)\n-\n-decoded_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n-print(f\"Generated -- {decoded_text}\")\n-# Generated -- TensorFlow is an open-source, open-source, distributed-source application # framework for the\n-```\n-\n-`generate()`„ÅßXLA„ÇíÊúâÂäπ„Å´„Åô„Çã„ÅÆ„ÅØ„ÄÅ„Åü„Å£„Åü‰∏ÄË°å„ÅÆ„Ç≥„Éº„Éâ„Åß„Åô„ÄÇ„Ç≥„Éº„Éâ„ÅÆÊÆã„ÇäÈÉ®ÂàÜ„ÅØÂ§âÊõ¥„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì„ÄÇ„Åü„Å†„Åó„ÄÅXLAÂõ∫Êúâ„ÅÆ„ÅÑ„Åè„Å§„Åã„ÅÆÊ≥®ÊÑèÁÇπ„Åå‰∏äË®ò„ÅÆ„Ç≥„Éº„Éâ„Çπ„Éã„Éö„ÉÉ„Éà„Å´„ÅÇ„Çä„Åæ„Åô„ÄÇ„Åì„Çå„Çâ„Å´Ê≥®ÊÑè„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„ÄÅXLA„Åå„ÇÇ„Åü„Çâ„ÅôÈÄüÂ∫¶Âêë‰∏ä„ÇíÂÆüÁèæ„Åô„Çã„Åü„ÇÅ„Å´„Åù„Çå„Çâ„ÇíÊääÊè°„Åô„Çã„Åì„Å®„ÅåÈáçË¶Å„Åß„Åô„ÄÇÊ¨°„ÅÆ„Çª„ÇØ„Ç∑„Éß„É≥„Åß„Åì„Çå„Çâ„Å´„Å§„ÅÑ„Å¶Ë©≥„Åó„ÅèË™¨Êòé„Åó„Åæ„Åô„ÄÇ\n-\n-\n-## Gotchas to be aware of\n-\n-XLA„ÇíÊúâÂäπ„Å´„Åó„ÅüÈñ¢Êï∞Ôºà‰∏äË®ò„ÅÆ`xla_generate()`„Å™„Å©Ôºâ„ÇíÂàù„ÇÅ„Å¶ÂÆüË°å„Åô„Çã„Å®„ÄÅÂÜÖÈÉ®„ÅßË®àÁÆó„Ç∞„É©„Éï„ÇíÊé®Ë´ñ„Åó„Çà„ÅÜ„Å®„Åó„Åæ„Åô„Åå„ÄÅ„Åì„Çå„ÅØÊôÇÈñì„Åå„Åã„Åã„Çä„Åæ„Åô„ÄÇ„Åì„ÅÆ„Éó„É≠„Çª„Çπ„ÅØ[\"„Éà„É¨„Éº„Ç∑„É≥„Ç∞\"ÔºàtracingÔºâ](https://www.tensorflow.org/guide/intro_to_graphs#when_is_a_function_tracing)„Å®„Åó„Å¶Áü•„Çâ„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n-\n-ÁîüÊàêÊôÇÈñì„ÅåÈ´òÈÄü„Åß„ÅØ„Å™„ÅÑ„Åì„Å®„Å´Ê∞ó‰ªò„Åè„Åã„ÇÇ„Åó„Çå„Åæ„Åõ„Çì„ÄÇ`xla_generate()`Ôºà„Åæ„Åü„ÅØ‰ªñ„ÅÆXLAÂØæÂøúÈñ¢Êï∞Ôºâ„ÅÆÈÄ£Á∂öÂëº„Å≥Âá∫„Åó„Åß„ÅØ„ÄÅÈñ¢Êï∞„Å∏„ÅÆÂÖ•Âäõ„ÅåÊúÄÂàù„Å´Ë®àÁÆó„Ç∞„É©„Éï„ÅåÊßãÁØâ„Åï„Çå„Åü„Å®„Åç„Å®Âêå„ÅòÂΩ¢Áä∂„Å´Âæì„Å£„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÄÅË®àÁÆó„Ç∞„É©„Éï„ÇíÊé®Ë´ñ„Åô„ÇãÂøÖË¶Å„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ„Åì„Çå„ÅØ„ÄÅÂÖ•ÂäõÂΩ¢Áä∂„ÅåÂõ∫ÂÆö„Åï„Çå„Å¶„ÅÑ„Çã„É¢„ÉÄ„É™„ÉÜ„Ç£Ôºà‰æãÔºöÁîªÂÉèÔºâ„Å´„ÅØÂïèÈ°å„ÅÇ„Çä„Åæ„Åõ„Çì„Åå„ÄÅÂ§âÊï∞„ÅÆÂÖ•ÂäõÂΩ¢Áä∂„É¢„ÉÄ„É™„ÉÜ„Ç£Ôºà‰æãÔºö„ÉÜ„Ç≠„Çπ„ÉàÔºâ„ÇíÊâ±„ÅÜÂ†¥Âêà„Å´„ÅØÊ≥®ÊÑè„ÅåÂøÖË¶Å„Åß„Åô„ÄÇ\n-\n-`xla_generate()`„ÅåÂ∏∏„Å´Âêå„ÅòÂÖ•ÂäõÂΩ¢Áä∂„ÅßÂãï‰Ωú„Åô„Çã„Çà„ÅÜ„Å´„Åô„Çã„Å´„ÅØ„ÄÅ„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„ÇíÂëº„Å≥Âá∫„ÅôÈöõ„Å´`padding`ÂºïÊï∞„ÇíÊåáÂÆö„Åß„Åç„Åæ„Åô„ÄÇ\n-\n-```py\n-import tensorflow as tf\n-from transformers import AutoTokenizer, TFAutoModelForCausalLM\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", padding_side=\"left\", pad_token=\"</s>\")\n-model = TFAutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n-input_string = [\"TensorFlow is\"]\n-\n-xla_generate = tf.function(model.generate, jit_compile=True)\n-\n-# Here, we call the tokenizer with padding options.\n-tokenized_input = tokenizer(input_string, pad_to_multiple_of=8, padding=True, return_tensors=\"tf\")\n-\n-generated_tokens = xla_generate(**tokenized_input, num_beams=2)\n-decoded_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n-print(f\"Generated -- {decoded_text}\")\n-```\n-\n-„Åì„Çå„Å´„Çà„Çä„ÄÅ`xla_generate()`„Å∏„ÅÆÂÖ•Âäõ„ÅåÂ∏∏„Å´„Éà„É¨„Éº„Çπ„Åï„Çå„ÅüÂΩ¢Áä∂„ÅÆÂÖ•Âäõ„ÇíÂèó„ÅëÂèñ„Çã„Åì„Å®„ÇíÁ¢∫Ë™ç„Åó„ÄÅÁîüÊàêÊôÇÈñì„ÅÆÈ´òÈÄüÂåñ„ÇíÂÆüÁèæ„Åß„Åç„Åæ„Åô„ÄÇ‰ª•‰∏ã„ÅÆ„Ç≥„Éº„Éâ„Åß„Åì„Çå„ÇíÁ¢∫Ë™ç„Åß„Åç„Åæ„ÅôÔºö\n-\n-```py\n-import time\n-import tensorflow as tf\n-from transformers import AutoTokenizer, TFAutoModelForCausalLM\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", padding_side=\"left\", pad_token=\"</s>\")\n-model = TFAutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n-\n-xla_generate = tf.function(model.generate, jit_compile=True)\n-\n-for input_string in [\"TensorFlow is\", \"TensorFlow is a\", \"TFLite is a\"]:\n-    tokenized_input = tokenizer(input_string, pad_to_multiple_of=8, padding=True, return_tensors=\"tf\")\n-    start = time.time_ns()\n-    generated_tokens = xla_generate(**tokenized_input, num_beams=2)\n-    end = time.time_ns()\n-    print(f\"Execution time -- {(end - start) / 1e6:.1f} ms\\n\")\n-```\n-\n-Tesla T4 GPU„Çí‰ΩøÁî®„Åô„Çã„Å®„ÄÅÊ¨°„ÅÆ„Çà„ÅÜ„Å™Âá∫Âäõ„ÅåÊúüÂæÖ„Åï„Çå„Åæ„ÅôÔºö\n-\n-```bash\n-Execution time -- 30819.6 ms\n-\n-Execution time -- 79.0 ms\n-\n-Execution time -- 78.9 ms\n-```\n-\n-ÊúÄÂàù„ÅÆ`xla_generate()`Âëº„Å≥Âá∫„Åó„ÅØ„Éà„É¨„Éº„Ç∑„É≥„Ç∞„ÅÆ„Åü„ÇÅ„Å´ÊôÇÈñì„Åå„Åã„Åã„Çä„Åæ„Åô„Åå„ÄÅÈÄ£Á∂ö„Åô„ÇãÂëº„Å≥Âá∫„Åó„ÅØÊ°ÅÈÅï„ÅÑ„Å´È´òÈÄü„Åß„Åô„ÄÇÁîüÊàê„Ç™„Éó„Ç∑„Éß„É≥„ÅÆ„ÅÑ„Åã„Å™„ÇãÂ§âÊõ¥„ÇÇ„ÄÅÂÜç„Éà„É¨„Éº„Ç∑„É≥„Ç∞„ÇíÂºï„ÅçËµ∑„Åì„Åó„ÄÅÁîüÊàêÊôÇÈñì„ÅÆÈÅÖÂª∂„ÇíÂºï„ÅçËµ∑„Åì„Åô„Åì„Å®„Å´Ê≥®ÊÑè„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n-\n-„Åì„ÅÆ„Éâ„Ç≠„É•„É°„É≥„Éà„Åß„ÅØ„ÄÅü§ó Transformers„ÅåÊèê‰æõ„Åô„Çã„ÉÜ„Ç≠„Çπ„ÉàÁîüÊàê„Ç™„Éó„Ç∑„Éß„É≥„Çí„Åô„Åπ„Å¶Á∂≤ÁæÖ„Åó„Å¶„ÅÑ„Åæ„Åõ„Çì„ÄÇÈ´òÂ∫¶„Å™„É¶„Éº„Çπ„Ç±„Éº„Çπ„Å´„Å§„ÅÑ„Å¶„ÅØ„Éâ„Ç≠„É•„É°„É≥„ÉÜ„Éº„Ç∑„Éß„É≥„ÇíÂèÇÁÖß„Åô„Çã„Åì„Å®„Çí„ÅäÂãß„ÇÅ„Åó„Åæ„Åô„ÄÇ\n-\n-## Additional Resources\n-\n-„Åì„Åì„Åß„ÅØ„ÄÅü§ó Transformers„Å®‰∏ÄËà¨ÁöÑ„Å™XLA„Å´„Å§„ÅÑ„Å¶„Åï„Çâ„Å´Ë©≥„Åó„ÅèÂ≠¶„Å≥„Åü„ÅÑÂ†¥Âêà„ÅÆ„ÅÑ„Åè„Å§„Åã„ÅÆËøΩÂä†„É™„ÇΩ„Éº„Çπ„ÇíÊèê‰æõ„Åó„Åæ„Åô„ÄÇ\n-\n-* [„Åì„ÅÆColab Notebook](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/91_tf_xla_generate.ipynb)„Åß„ÅØ„ÄÅXLAÂØæÂøú„ÅÆ„Ç®„É≥„Ç≥„Éº„ÉÄ„Éº„Éá„Ç≥„Éº„ÉÄ„ÉºÔºà[T5](https://huggingface.co/docs/transformers/model_doc/t5)„Å™„Å©Ôºâ„Åä„Çà„Å≥„Éá„Ç≥„Éº„ÉÄ„ÉºÂ∞ÇÁî®Ôºà[GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)„Å™„Å©Ôºâ„ÉÜ„Ç≠„Çπ„ÉàÁîüÊàê„É¢„Éá„É´„ÇíË©¶„Åô„Åü„ÇÅ„ÅÆÂØæË©±Âûã„Éá„É¢„ÅåÊèê‰æõ„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n-* [„Åì„ÅÆ„Éñ„É≠„Ç∞Ë®ò‰∫ã](https://huggingface.co/blog/tf-xla-generate)„Åß„ÅØ„ÄÅXLAÂØæÂøú„É¢„Éá„É´„ÅÆÊØîËºÉ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„ÅÆÊ¶ÇË¶Å„Å®„ÄÅTensorFlow„Åß„ÅÆXLA„Å´„Å§„ÅÑ„Å¶„ÅÆÂèãÂ•ΩÁöÑ„Å™Á¥π‰ªã„ÅåÊèê‰æõ„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n-* [„Åì„ÅÆ„Éñ„É≠„Ç∞Ë®ò‰∫ã](https://blog.tensorflow.org/2022/11/how-hugging-face-improved-text-generation-performance-with-xla.html)„Åß„ÅØ„ÄÅü§ó Transformers„ÅÆTensorFlow„É¢„Éá„É´„Å´XLA„Çµ„Éù„Éº„Éà„ÇíËøΩÂä†„Åô„ÇãÈöõ„ÅÆË®≠Ë®àÂì≤Â≠¶„Å´„Å§„ÅÑ„Å¶Ë™¨Êòé„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n-* ‰∏ÄËà¨ÁöÑ„Å™XLA„Å®TensorFlow„Ç∞„É©„Éï„Å´„Å§„ÅÑ„Å¶Ë©≥„Åó„ÅèÂ≠¶„Å∂„Åü„ÇÅ„ÅÆ„Åä„Åô„Åô„ÇÅ„ÅÆÊäïÁ®øÔºö\n-    * [XLA: Ê©üÊ¢∞Â≠¶ÁøíÁî®„ÅÆÊúÄÈÅ©Âåñ„Ç≥„É≥„Éë„Ç§„É©](https://www.tensorflow.org/xla)\n-    * [„Ç∞„É©„Éï„Å®`tf.function`„ÅÆÁ¥π‰ªã](https://www.tensorflow.org/guide/intro_to_graphs)\n-    * [`tf.function`„Çí‰ΩøÁî®„Åó„Åü„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÂêë‰∏ä](https://www.tensorflow.org/guide/function)"
        },
        {
            "sha": "ad3e9a3f484e2ce157a23cf0b58d1854efc8a267",
            "filename": "docs/source/ja/tflite.md",
            "status": "removed",
            "additions": 0,
            "deletions": 58,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/docs%2Fsource%2Fja%2Ftflite.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/docs%2Fsource%2Fja%2Ftflite.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftflite.md?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,58 +0,0 @@\n-<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# Export to TFLite\n-\n-[TensorFlow Lite](https://www.tensorflow.org/lite/guide)„ÅØ„ÄÅ„É¢„Éê„Ç§„É´„Éï„Ç©„É≥„ÄÅÁµÑ„ÅøËæº„Åø„Ç∑„Çπ„ÉÜ„É†„ÄÅ„Åä„Çà„Å≥„É¢„Éé„ÅÆ„Ç§„É≥„Çø„Éº„Éç„ÉÉ„ÉàÔºàIoTÔºâ„Éá„Éê„Ç§„Çπ„Å™„Å©„ÄÅ„É™„ÇΩ„Éº„Çπ„Å´Âà∂Á¥Ñ„ÅÆ„ÅÇ„Çã„Éá„Éê„Ç§„Çπ„Å´Ê©üÊ¢∞Â≠¶Áøí„É¢„Éá„É´„ÇíÂ±ïÈñã„Åô„Çã„Åü„ÇÅ„ÅÆËªΩÈáè„Å™„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ„Åß„Åô„ÄÇTFLite„ÅØ„ÄÅË®àÁÆóËÉΩÂäõ„ÄÅ„É°„É¢„É™„ÄÅ„Åä„Çà„Å≥ÈõªÂäõÊ∂àË≤ª„ÅåÈôê„Çâ„Çå„Å¶„ÅÑ„Çã„Åì„Çå„Çâ„ÅÆ„Éá„Éê„Ç§„Çπ‰∏ä„Åß„É¢„Éá„É´„ÇíÂäπÁéáÁöÑ„Å´ÊúÄÈÅ©Âåñ„Åó„Å¶ÂÆüË°å„Åô„Çã„Åü„ÇÅ„Å´Ë®≠Ë®à„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n-TensorFlow Lite„É¢„Éá„É´„ÅØ„ÄÅ`.tflite`„Éï„Ç°„Ç§„É´Êã°ÂºµÂ≠ê„ÅßË≠òÂà•„Åï„Çå„ÇãÁâπÂà•„Å™ÂäπÁéáÁöÑ„Å™„Éù„Éº„Çø„Éñ„É´ÂΩ¢Âºè„ÅßË°®„Åï„Çå„Åæ„Åô„ÄÇ\n-\n-ü§ó Optimum„ÅØ„ÄÅü§ó Transformers„É¢„Éá„É´„ÇíTFLite„Å´„Ç®„ÇØ„Çπ„Éù„Éº„Éà„Åô„Çã„Åü„ÇÅ„ÅÆÊ©üËÉΩ„Çí`exporters.tflite`„É¢„Ç∏„É•„Éº„É´„Çí‰ªã„Åó„Å¶Êèê‰æõ„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Çµ„Éù„Éº„Éà„Åï„Çå„Å¶„ÅÑ„Çã„É¢„Éá„É´„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„ÅÆ„É™„Çπ„Éà„Å´„Å§„ÅÑ„Å¶„ÅØ„ÄÅ[ü§ó Optimum„ÅÆ„Éâ„Ç≠„É•„É°„É≥„Éà](https://huggingface.co/docs/optimum/exporters/tflite/overview)„Çí„ÅîÂèÇÁÖß„Åè„Å†„Åï„ÅÑ„ÄÇ\n-\n-„É¢„Éá„É´„ÇíTFLite„Å´„Ç®„ÇØ„Çπ„Éù„Éº„Éà„Åô„Çã„Å´„ÅØ„ÄÅÂøÖË¶Å„Å™‰æùÂ≠òÈñ¢‰øÇ„Çí„Ç§„É≥„Çπ„Éà„Éº„É´„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºö\n-\n-\n-```bash\n-pip install optimum[exporters-tf]\n-```\n-\n-„Åô„Åπ„Å¶„ÅÆÂà©Áî®ÂèØËÉΩ„Å™ÂºïÊï∞„ÇíÁ¢∫Ë™ç„Åô„Çã„Å´„ÅØ„ÄÅ[ü§ó Optimum„Éâ„Ç≠„É•„É°„É≥„Éà](https://huggingface.co/docs/optimum/main/en/exporters/tflite/usage_guides/export_a_model)„ÇíÂèÇÁÖß„Åô„Çã„Åã„ÄÅ„Ç≥„Éû„É≥„Éâ„É©„Ç§„É≥„Åß„Éò„É´„Éó„ÇíË°®Á§∫„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºö\n-\n-```bash\n-optimum-cli export tflite --help\n-```\n-\n-ü§ó Hub„Åã„Çâ„É¢„Éá„É´„ÅÆ„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà„Çí„Ç®„ÇØ„Çπ„Éù„Éº„Éà„Åô„Çã„Å´„ÅØ„ÄÅ‰æã„Åà„Å∞ `google-bert/bert-base-uncased` „Çí‰ΩøÁî®„Åô„ÇãÂ†¥Âêà„ÄÅÊ¨°„ÅÆ„Ç≥„Éû„É≥„Éâ„ÇíÂÆüË°å„Åó„Åæ„ÅôÔºö\n-\n-```bash\n-optimum-cli export tflite --model google-bert/bert-base-uncased --sequence_length 128 bert_tflite/\n-```\n-\n-ÈÄ≤Ë°åÁä∂Ê≥Å„ÇíÁ§∫„Åô„É≠„Ç∞„ÅåË°®Á§∫„Åï„Çå„ÄÅÁîüÊàê„Åï„Çå„Åü `model.tflite` „Åå‰øùÂ≠ò„Åï„Çå„ÅüÂ†¥ÊâÄ„ÇÇË°®Á§∫„Åï„Çå„Çã„ÅØ„Åö„Åß„ÅôÔºö\n-\n-```bash\n-Validating TFLite model...\n-\t-[‚úì] TFLite model output names match reference model (logits)\n-\t- Validating TFLite Model output \"logits\":\n-\t\t-[‚úì] (1, 128, 30522) matches (1, 128, 30522)\n-\t\t-[x] values not close enough, max diff: 5.817413330078125e-05 (atol: 1e-05)\n-The TensorFlow Lite export succeeded with the warning: The maximum absolute difference between the output of the reference model and the TFLite exported model is not within the set tolerance 1e-05:\n-- logits: max diff = 5.817413330078125e-05.\n- The exported model was saved at: bert_tflite\n- ```\n-\n-‰∏äË®ò„ÅÆ‰æã„ÅØü§ó Hub„Åã„Çâ„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà„Çí„Ç®„ÇØ„Çπ„Éù„Éº„Éà„Åô„ÇãÊñπÊ≥ï„ÇíÁ§∫„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ„É≠„Éº„Ç´„É´„É¢„Éá„É´„Çí„Ç®„ÇØ„Çπ„Éù„Éº„Éà„Åô„ÇãÂ†¥Âêà„ÄÅ„Åæ„Åö„É¢„Éá„É´„ÅÆÈáç„Åø„Éï„Ç°„Ç§„É´„Å®„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éï„Ç°„Ç§„É´„ÇíÂêå„Åò„Éá„Ç£„É¨„ÇØ„Éà„É™Ôºà`local_path`Ôºâ„Å´‰øùÂ≠ò„Åó„Åü„Åì„Å®„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇCLI„Çí‰ΩøÁî®„Åô„ÇãÂ†¥Âêà„ÄÅü§ó Hub„ÅÆ„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„ÉàÂêç„ÅÆ‰ª£„Çè„Çä„Å´`model`ÂºïÊï∞„Å´`local_path`„ÇíÊ∏°„Åó„Åæ„Åô„ÄÇ\n-\n-"
        },
        {
            "sha": "21f26cd66af6e6848541e8f5e3d24e831fdc7dc2",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -208,8 +208,6 @@\n   sections:\n   - local: serialization\n     title: ONNXÎ°ú ÎÇ¥Î≥¥ÎÇ¥Í∏∞\n-  - local: tflite\n-    title: TFLiteÎ°ú ÎÇ¥Î≥¥ÎÇ¥Í∏∞\n   - local: executorch\n     title: ExecuTorch\n   - local: torchscript\n@@ -402,8 +400,6 @@\n       title: Configuration\n     - local: main_classes/data_collator\n       title: Data Collator\n-    - local: main_classes/keras_callbacks\n-      title: Keras callbacks\n     - local: main_classes/logging\n       title: Logging\n     - local: main_classes/model"
        },
        {
            "sha": "25d5ea3e40083acad8fa9257ce2c7cfef7c3d9c8",
            "filename": "docs/source/ko/main_classes/keras_callbacks.md",
            "status": "removed",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/docs%2Fsource%2Fko%2Fmain_classes%2Fkeras_callbacks.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/docs%2Fsource%2Fko%2Fmain_classes%2Fkeras_callbacks.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmain_classes%2Fkeras_callbacks.md?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,27 +0,0 @@\n-<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# ÏºÄÎùºÏä§ ÏΩúÎ∞±[[keras-callbacks]]\n-\n-ÏºÄÎùºÏä§Î°ú Ìä∏ÎûúÏä§Ìè¨Î®∏ Î™®Îç∏ÏùÑ ÌïôÏäµÌï† Îïå, ÏùºÎ∞òÏ†ÅÏù∏ ÏûëÏóÖÏùÑ ÏûêÎèôÌôîÌïòÍ∏∞ ÏúÑÌïú ÎùºÏù¥Î∏åÎü¨Î¶¨ Ï†ÑÏö© ÏΩúÎ∞±Îì§ÏùÑ ÏÇ¨Ïö© Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n-\n-## KerasMetricCallback[[transformers.KerasMetricCallback]]\n-\n-[[autodoc]] KerasMetricCallback\n-\n-## PushToHubCallback[[transformers.PushToHubCallback]]\n-\n-[[autodoc]] PushToHubCallback"
        },
        {
            "sha": "464106a6b7c2612b93a25848b750d90f7258e4cd",
            "filename": "docs/source/ko/tflite.md",
            "status": "removed",
            "additions": 0,
            "deletions": 62,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/docs%2Fsource%2Fko%2Ftflite.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/docs%2Fsource%2Fko%2Ftflite.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftflite.md?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,62 +0,0 @@\n-<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# TFLiteÎ°ú ÎÇ¥Î≥¥ÎÇ¥Í∏∞[[export-to-tflite]]\n-\n-[TensorFlow Lite](https://www.tensorflow.org/lite/guide)Îäî ÏûêÏõêÏù¥ Ï†úÌïúÎêú Ìú¥ÎåÄÌè∞, ÏûÑÎ≤†ÎîîÎìú ÏãúÏä§ÌÖú, ÏÇ¨Î¨ºÏù∏ÌÑ∞ÎÑ∑(IoT) Í∏∞Í∏∞ÏóêÏÑú \n-Í∏∞Í≥ÑÌïôÏäµ Î™®Îç∏ÏùÑ Î∞∞Ìè¨ÌïòÍ∏∞ ÏúÑÌïú Í≤ΩÎüâ ÌîÑÎ†àÏûÑÏõåÌÅ¨ÏûÖÎãàÎã§. \n-TFLiteÎäî Ïó∞ÏÇ∞ Îä•Î†•, Î©îÎ™®Î¶¨, Ï†ÑÎ†• ÏÜåÎπÑÍ∞Ä Ï†úÌïúÎêú Í∏∞Í∏∞ÏóêÏÑú Î™®Îç∏ÏùÑ Ìö®Ïú®Ï†ÅÏúºÎ°ú ÏµúÏ†ÅÌôîÌïòÍ≥† Ïã§ÌñâÌïòÍ∏∞ ÏúÑÌï¥ \n-ÏÑ§Í≥ÑÎêòÏóàÏäµÎãàÎã§. \n-TensorFlow Lite Î™®Îç∏ÏùÄ `.tflite` ÌååÏùº ÌôïÏû•ÏûêÎ°ú ÏãùÎ≥ÑÎêòÎäî ÌäπÏàòÌïòÍ≥† Ìö®Ïú®Ï†ÅÏù∏ Ìú¥ÎåÄÏö© Ìè¨Îß∑ÏúºÎ°ú ÌëúÌòÑÎê©ÎãàÎã§. \n-\n-ü§ó OptimumÏùÄ `exporters.tflite` Î™®ÎìàÎ°ú ü§ó Transformers Î™®Îç∏ÏùÑ TFLiteÎ°ú ÎÇ¥Î≥¥ÎÇ¥Îäî Í∏∞Îä•ÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§. \n-ÏßÄÏõêÎêòÎäî Î™®Îç∏ ÏïÑÌÇ§ÌÖçÏ≤ò Î™©Î°ùÏùÄ [ü§ó Optimum Î¨∏ÏÑú](https://huggingface.co/docs/optimum/exporters/tflite/overview)Î•º Ï∞∏Í≥†ÌïòÏÑ∏Ïöî. \n-\n-Î™®Îç∏ÏùÑ TFLiteÎ°ú ÎÇ¥Î≥¥ÎÇ¥Î†§Î©¥, ÌïÑÏöîÌïú Ï¢ÖÏÜçÏÑ±ÏùÑ ÏÑ§ÏπòÌïòÏÑ∏Ïöî:\n- \n-```bash\n-pip install optimum[exporters-tf]\n-```\n-\n-Î™®Îì† ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Ïù∏ÏàòÎ•º ÌôïÏù∏ÌïòÎ†§Î©¥, [ü§ó Optimum Î¨∏ÏÑú](https://huggingface.co/docs/optimum/main/en/exporters/tflite/usage_guides/export_a_model)Î•º Ï∞∏Í≥†ÌïòÍ±∞ÎÇò \n-ÌÑ∞ÎØ∏ÎÑêÏóêÏÑú ÎèÑÏõÄÎßêÏùÑ ÏÇ¥Ìé¥Î≥¥ÏÑ∏Ïöî:\n-\n-```bash\n-optimum-cli export tflite --help\n-```\n-\n-ÏòàÎ•º Îì§Ïñ¥ ü§ó HubÏóêÏÑúÏùò `google-bert/bert-base-uncased` Î™®Îç∏ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Î•º ÎÇ¥Î≥¥ÎÇ¥Î†§Î©¥, Îã§Ïùå Î™ÖÎ†πÏùÑ Ïã§ÌñâÌïòÏÑ∏Ïöî:\n-\n-```bash\n-optimum-cli export tflite --model google-bert/bert-base-uncased --sequence_length 128 bert_tflite/\n-```\n-\n-Îã§ÏùåÍ≥º Í∞ôÏù¥ ÏßÑÌñâ ÏÉÅÌô©ÏùÑ ÎÇòÌÉÄÎÇ¥Îäî Î°úÍ∑∏ÏôÄ Í≤∞Í≥ºÎ¨ºÏù∏ `model.tflite`Í∞Ä Ï†ÄÏû•Îêú ÏúÑÏπòÎ•º Î≥¥Ïó¨Ï£ºÎäî Î°úÍ∑∏Í∞Ä ÌëúÏãúÎê©ÎãàÎã§:\n-\n-```bash\n-Validating TFLite model...\n-\t-[‚úì] TFLite model output names match reference model (logits)\n-\t- Validating TFLite Model output \"logits\":\n-\t\t-[‚úì] (1, 128, 30522) matches (1, 128, 30522)\n-\t\t-[x] values not close enough, max diff: 5.817413330078125e-05 (atol: 1e-05)\n-The TensorFlow Lite export succeeded with the warning: The maximum absolute difference between the output of the reference model and the TFLite exported model is not within the set tolerance 1e-05:\n-- logits: max diff = 5.817413330078125e-05.\n- The exported model was saved at: bert_tflite\n- ```\n-\n-ÏúÑ ÏòàÏ†úÎäî ü§ó HubÏóêÏÑúÏùò Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Î•º ÎÇ¥Î≥¥ÎÇ¥Îäî Î∞©Î≤ïÏùÑ Î≥¥Ïó¨Ï§çÎãàÎã§. \n-Î°úÏª¨ Î™®Îç∏ÏùÑ ÎÇ¥Î≥¥ÎÇ∏Îã§Î©¥, Î®ºÏ†Ä Î™®Îç∏ Í∞ÄÏ§ëÏπòÏôÄ ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä ÌååÏùºÏù¥ Î™®Îëê Í∞ôÏùÄ ÎîîÎ†âÌÑ∞Î¶¨( `local_path` )Ïóê Ï†ÄÏû•ÎêêÎäîÏßÄ ÌôïÏù∏ÌïòÏÑ∏Ïöî. \n-CLIÎ•º ÏÇ¨Ïö©Ìï† Îïå, ü§ó HubÏóêÏÑúÏùò Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ïù¥Î¶Ñ ÎåÄÏã† `model` Ïù∏ÏàòÏóê `local_path`Î•º Ï†ÑÎã¨ÌïòÎ©¥ Îê©ÎãàÎã§. \n\\ No newline at end of file"
        },
        {
            "sha": "05d4829437b9c044a1a4c927ecfb9a3b246ea1df",
            "filename": "docs/source/ms/_toctree.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/docs%2Fsource%2Fms%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/docs%2Fsource%2Fms%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fms%2F_toctree.yml?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -115,8 +115,6 @@\n       title: Latihan pada banyak CPU\n     - local: perf_train_tpu\n       title: Latihan mengenai TPU\n-    - local: perf_train_tpu_tf\n-      title: Latihan tentang TPU dengan TensorFlow\n     - local: perf_train_special\n       title: Latihan mengenai Perkakasan Khusus\n     - local: perf_infer_cpu\n@@ -135,8 +133,6 @@\n       title: Penyahpepijatan\n     - local: hpo_train\n       title: Carian Hiperparameter menggunakan API Pelatih\n-    - local: tf_xla\n-      title: Penyepaduan XLA untuk Model TensorFlow\n   title: Prestasi dan kebolehskalaan\n - sections:\n     - local: contributing\n@@ -185,8 +181,6 @@\n           title: Configuration\n         - local: main_classes/data_collator\n           title: Data Collator\n-        - local: main_classes/keras_callbacks\n-          title: Keras callbacks\n         - local: main_classes/logging\n           title: Logging\n         - local: main_classes/model"
        },
        {
            "sha": "c525a2a4faa1b55628870324c46d1d47d9a5abde",
            "filename": "docs/source/pt/_toctree.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/docs%2Fsource%2Fpt%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/docs%2Fsource%2Fpt%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fpt%2F_toctree.yml?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -23,8 +23,6 @@\n     title: Compartilhando modelos customizados \n   - local: run_scripts\n     title: Treinamento a partir de um script\n-  - local: converting_tensorflow_models\n-    title: Convertendo checkpoints do TensorFlow para Pytorch\n   - local: serialization\n     title: Exportando modelos para ONNX\n   - sections:"
        },
        {
            "sha": "446acd62ea8ffd2b422f9f9a84b5091c4788156d",
            "filename": "docs/source/pt/converting_tensorflow_models.md",
            "status": "removed",
            "additions": 0,
            "deletions": 152,
            "changes": 152,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/docs%2Fsource%2Fpt%2Fconverting_tensorflow_models.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/docs%2Fsource%2Fpt%2Fconverting_tensorflow_models.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fpt%2Fconverting_tensorflow_models.md?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,152 +0,0 @@\n-<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# Convertendo checkpoints do TensorFlow para Pytorch\n-\n-Uma interface de linha de comando √© fornecida para converter os checkpoints originais Bert/GPT/GPT-2/Transformer-XL/XLNet/XLM em modelos\n-que podem ser carregados usando os m√©todos `from_pretrained` da biblioteca.\n-\n-<Tip>\n-\n-A partir da vers√£o 2.3.0 o script de convers√£o agora faz parte do transformers CLI (**transformers**) dispon√≠vel em qualquer instala√ß√£o\n-transformers >= 2.3.0.\n-\n-A documenta√ß√£o abaixo reflete o formato do comando **transformers convert**.\n-\n-</Tip>\n-\n-## BERT\n-\n-Voc√™ pode converter qualquer checkpoint do BERT em TensorFlow (em particular [os modelos pr√©-treinados lan√ßados pelo Google](https://github.com/google-research/bert#pre-trained-models)) em um arquivo PyTorch usando um\n-[convert_bert_original_tf_checkpoint_to_pytorch.py](https://github.com/huggingface/transformers/tree/main/src/transformers/models/bert/convert_bert_original_tf_checkpoint_to_pytorch.py) script.\n-\n-Esta Interface de Linha de Comando (CLI) recebe como entrada um checkpoint do TensorFlow (tr√™s arquivos come√ßando com `bert_model.ckpt`) e o\n-arquivo de configura√ß√£o (`bert_config.json`), e ent√£o cria um modelo PyTorch para esta configura√ß√£o, carrega os pesos\n-do checkpoint do TensorFlow no modelo PyTorch e salva o modelo resultante em um arquivo PyTorch que pode\n-ser importado usando `from_pretrained()` (veja o exemplo em [quicktour](quicktour) , [run_glue.py](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification/run_glue.py) ).\n-\n-Voc√™ s√≥ precisa executar este script de convers√£o **uma vez** para obter um modelo PyTorch. Voc√™ pode ent√£o desconsiderar o checkpoint em\n- TensorFlow (os tr√™s arquivos come√ßando com `bert_model.ckpt`), mas certifique-se de manter o arquivo de configura√ß√£o (\\\n-`bert_config.json`) e o arquivo de vocabul√°rio (`vocab.txt`), pois eles tamb√©m s√£o necess√°rios para o modelo PyTorch.\n-\n-Para executar este script de convers√£o espec√≠fico, voc√™ precisar√° ter o TensorFlow e o PyTorch instalados (`pip install tensorflow`). O resto do reposit√≥rio requer apenas o PyTorch.\n-\n-Aqui est√° um exemplo do processo de convers√£o para um modelo `BERT-Base Uncased` pr√©-treinado:\n-\n-```bash\n-export BERT_BASE_DIR=/path/to/bert/uncased_L-12_H-768_A-12\n-\n-transformers convert --model_type bert \\\n-  --tf_checkpoint $BERT_BASE_DIR/bert_model.ckpt \\\n-  --config $BERT_BASE_DIR/bert_config.json \\\n-  --pytorch_dump_output $BERT_BASE_DIR/pytorch_model.bin\n-```\n-\n-Voc√™ pode baixar os modelos pr√©-treinados do Google para a convers√£o [aqui](https://github.com/google-research/bert#pre-trained-models).\n-\n-## ALBERT\n-\n-Converta os checkpoints do modelo ALBERT em TensorFlow para PyTorch usando o\n-[convert_albert_original_tf_checkpoint_to_pytorch.py](https://github.com/huggingface/transformers/tree/main/src/transformers/models/albert/convert_albert_original_tf_checkpoint_to_pytorch.py) script.\n-\n-A Interface de Linha de Comando (CLI) recebe como entrada um checkpoint do TensorFlow (tr√™s arquivos come√ßando com `model.ckpt-best`) e o\n-arquivo de configura√ß√£o (`albert_config.json`), ent√£o cria e salva um modelo PyTorch. Para executar esta convers√£o, voc√™\n-precisa ter o TensorFlow e o PyTorch instalados.\n-\n-Aqui est√° um exemplo do processo de convers√£o para o modelo `ALBERT Base` pr√©-treinado:\n-\n-```bash\n-export ALBERT_BASE_DIR=/path/to/albert/albert_base\n-\n-transformers convert --model_type albert \\\n-  --tf_checkpoint $ALBERT_BASE_DIR/model.ckpt-best \\\n-  --config $ALBERT_BASE_DIR/albert_config.json \\\n-  --pytorch_dump_output $ALBERT_BASE_DIR/pytorch_model.bin\n-```\n-\n-Voc√™ pode baixar os modelos pr√©-treinados do Google para a convers√£o [aqui](https://github.com/google-research/albert#pre-trained-models).\n-\n-## OpenAI GPT\n-\n-Aqui est√° um exemplo do processo de convers√£o para um modelo OpenAI GPT pr√©-treinado, supondo que seu checkpoint NumPy\n-foi salvo com o mesmo formato do modelo pr√©-treinado OpenAI (veja [aqui](https://github.com/openai/finetune-transformer-lm)\\\n-)\n-\n-```bash\n-export OPENAI_GPT_CHECKPOINT_FOLDER_PATH=/path/to/openai/pretrained/numpy/weights\n-\n-transformers convert --model_type gpt \\\n-  --tf_checkpoint $OPENAI_GPT_CHECKPOINT_FOLDER_PATH \\\n-  --pytorch_dump_output $PYTORCH_DUMP_OUTPUT \\\n-  [--config OPENAI_GPT_CONFIG] \\\n-  [--finetuning_task_name OPENAI_GPT_FINETUNED_TASK] \\\n-```\n-\n-## OpenAI GPT-2\n-\n-Aqui est√° um exemplo do processo de convers√£o para um modelo OpenAI GPT-2 pr√©-treinado (consulte [aqui](https://github.com/openai/gpt-2))\n-\n-```bash\n-export OPENAI_GPT2_CHECKPOINT_PATH=/path/to/openai-community/gpt2/pretrained/weights\n-\n-transformers convert --model_type gpt2 \\\n-  --tf_checkpoint $OPENAI_GPT2_CHECKPOINT_PATH \\\n-  --pytorch_dump_output $PYTORCH_DUMP_OUTPUT \\\n-  [--config OPENAI_GPT2_CONFIG] \\\n-  [--finetuning_task_name OPENAI_GPT2_FINETUNED_TASK]\n-```\n-\n-## XLNet\n-\n-Aqui est√° um exemplo do processo de convers√£o para um modelo XLNet pr√©-treinado:\n-\n-```bash\n-export TRANSFO_XL_CHECKPOINT_PATH=/path/to/xlnet/checkpoint\n-export TRANSFO_XL_CONFIG_PATH=/path/to/xlnet/config\n-\n-transformers convert --model_type xlnet \\\n-  --tf_checkpoint $TRANSFO_XL_CHECKPOINT_PATH \\\n-  --config $TRANSFO_XL_CONFIG_PATH \\\n-  --pytorch_dump_output $PYTORCH_DUMP_OUTPUT \\\n-  [--finetuning_task_name XLNET_FINETUNED_TASK] \\\n-```\n-\n-## XLM\n-\n-Aqui est√° um exemplo do processo de convers√£o para um modelo XLM pr√©-treinado:\n-\n-```bash\n-export XLM_CHECKPOINT_PATH=/path/to/xlm/checkpoint\n-\n-transformers convert --model_type xlm \\\n-  --tf_checkpoint $XLM_CHECKPOINT_PATH \\\n-  --pytorch_dump_output $PYTORCH_DUMP_OUTPUT\n- [--config XML_CONFIG] \\\n- [--finetuning_task_name XML_FINETUNED_TASK]\n-```\n-\n-## T5\n-\n-Aqui est√° um exemplo do processo de convers√£o para um modelo T5 pr√©-treinado:\n-\n-```bash\n-export T5=/path/to/t5/uncased_L-12_H-768_A-12\n-\n-transformers convert --model_type t5 \\\n-  --tf_checkpoint $T5/t5_model.ckpt \\\n-  --config $T5/t5_config.json \\\n-  --pytorch_dump_output $T5/pytorch_model.bin\n-```"
        },
        {
            "sha": "ad7e2479b42e91824b421eddbbd5eefe135c7251",
            "filename": "docs/source/zh/_toctree.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/docs%2Fsource%2Fzh%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/docs%2Fsource%2Fzh%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2F_toctree.yml?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -44,8 +44,6 @@\n     title: ËÅäÂ§©Ê®°ÂûãÁöÑÊ®°Êùø\n   - local: serialization\n     title: ÂØºÂá∫‰∏∫ ONNX\n-  - local: tflite\n-    title: ÂØºÂá∫‰∏∫ TFLite\n   - local: torchscript\n     title: ÂØºÂá∫‰∏∫ TorchScript\n   - local: gguf\n@@ -76,8 +74,6 @@\n     title: ÂÆû‰æãÂåñÂ§ßÊ®°Âûã\n   - local: debugging\n     title: ÈóÆÈ¢òÂÆö‰ΩçÂèäËß£ÂÜ≥\n-  - local: tf_xla\n-    title: TensorFlowÊ®°ÂûãÁöÑXLAÈõÜÊàê\n   - local: perf_torch_compile\n     title: ‰ΩøÁî® `torch.compile()` ‰ºòÂåñÊé®ÁêÜ\n   title: ÊÄßËÉΩÂíåÂèØÊâ©Â±ïÊÄß\n@@ -107,8 +103,6 @@\n       title: Configuration\n     - local: main_classes/data_collator\n       title: Data Collator\n-    - local: main_classes/keras_callbacks\n-      title: Keras callbacks\n     - local: main_classes/logging\n       title: Logging\n     - local: main_classes/model"
        },
        {
            "sha": "1eea2eb998162c7af06863700ef2a388a7ea1c53",
            "filename": "docs/source/zh/main_classes/keras_callbacks.md",
            "status": "removed",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/docs%2Fsource%2Fzh%2Fmain_classes%2Fkeras_callbacks.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/docs%2Fsource%2Fzh%2Fmain_classes%2Fkeras_callbacks.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fmain_classes%2Fkeras_callbacks.md?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,27 +0,0 @@\n-<!--Copyright 2021 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# Keras callbacks\n-\n-Âú®Keras‰∏≠ËÆ≠ÁªÉTransformersÊ®°ÂûãÊó∂ÔºåÊúâ‰∏Ä‰∫õÂ∫ìÁâπÂÆöÁöÑcallbacksÂáΩÊï∞ÂèØÁî®‰∫éËá™Âä®ÊâßË°åÂ∏∏ËßÅ‰ªªÂä°Ôºö\n-\n-## KerasMetricCallback\n-\n-[[autodoc]] KerasMetricCallback\n-\n-## PushToHubCallback\n-\n-[[autodoc]] PushToHubCallback"
        },
        {
            "sha": "2e5b444d876c0a72ae91fe044a77455c2c3e59dc",
            "filename": "docs/source/zh/tf_xla.md",
            "status": "removed",
            "additions": 0,
            "deletions": 179,
            "changes": 179,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/docs%2Fsource%2Fzh%2Ftf_xla.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/docs%2Fsource%2Fzh%2Ftf_xla.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Ftf_xla.md?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,179 +0,0 @@\n-<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# Áî®‰∫é TensorFlow Ê®°ÂûãÁöÑ XLA ÈõÜÊàê\n-\n-[[open-in-colab]]\n-\n-Âä†ÈÄüÁ∫øÊÄß‰ª£Êï∞Ôºå‰πüÁß∞‰∏∫XLAÔºåÊòØ‰∏Ä‰∏™Áî®‰∫éÂä†ÈÄüTensorFlowÊ®°ÂûãËøêË°åÊó∂Èó¥ÁöÑÁºñËØëÂô®„ÄÇ‰ªé[ÂÆòÊñπÊñáÊ°£](https://www.tensorflow.org/xla)‰∏≠ÂèØ‰ª•ÁúãÂà∞Ôºö\n-\n-XLAÔºàÂä†ÈÄüÁ∫øÊÄß‰ª£Êï∞ÔºâÊòØ‰∏ÄÁßçÈíàÂØπÁ∫øÊÄß‰ª£Êï∞ÁöÑÁâπÂÆöÈ¢ÜÂüüÁºñËØëÂô®ÔºåÂèØ‰ª•Âú®ÂèØËÉΩ‰∏çÈúÄË¶ÅÊõ¥ÊîπÊ∫ê‰ª£Á†ÅÁöÑÊÉÖÂÜµ‰∏ãÂä†ÈÄüTensorFlowÊ®°Âûã„ÄÇ\n-\n-Âú®TensorFlow‰∏≠‰ΩøÁî®XLAÈùûÂ∏∏ÁÆÄÂçï‚Äî‚ÄîÂÆÉÂåÖÂê´Âú®`tensorflow`Â∫ì‰∏≠ÔºåÂπ∂‰∏îÂèØ‰ª•‰ΩøÁî®‰ªª‰ΩïÂõæÂàõÂª∫ÂáΩÊï∞‰∏≠ÁöÑ`jit_compile`ÂèÇÊï∞Êù•Ëß¶ÂèëÔºå‰æãÂ¶Ç[`tf.function`](https://www.tensorflow.org/guide/intro_to_graphs)„ÄÇÂú®‰ΩøÁî®KerasÊñπÊ≥ïÂ¶Ç`fit()`Âíå`predict()`Êó∂ÔºåÂè™ÈúÄÂ∞Ü`jit_compile`ÂèÇÊï∞‰º†ÈÄíÁªô`model.compile()`Âç≥ÂèØÂêØÁî®XLA„ÄÇÁÑ∂ËÄåÔºåXLA‰∏ç‰ªÖÈôê‰∫éËøô‰∫õÊñπÊ≥ï - ÂÆÉËøòÂèØ‰ª•Áî®‰∫éÂä†ÈÄü‰ªª‰Ωï‰ªªÊÑèÁöÑ`tf.function`„ÄÇ\n-\n-Âú®ü§ó Transformers‰∏≠ÔºåÂá†‰∏™TensorFlowÊñπÊ≥ïÂ∑≤ÁªèË¢´ÈáçÂÜô‰∏∫‰∏éXLAÂÖºÂÆπÔºåÂåÖÊã¨[GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)„ÄÅ[T5](https://huggingface.co/docs/transformers/model_doc/t5)Âíå[OPT](https://huggingface.co/docs/transformers/model_doc/opt)Á≠âÊñáÊú¨ÁîüÊàêÊ®°ÂûãÔºå‰ª•Âèä[Whisper](https://huggingface.co/docs/transformers/model_doc/whisper)Á≠âËØ≠Èü≥Â§ÑÁêÜÊ®°Âûã„ÄÇ\n-\n-ËôΩÁÑ∂Á°ÆÂàáÁöÑÂä†ÈÄüÂÄçÊï∞ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÂèñÂÜ≥‰∫éÊ®°ÂûãÔºå‰ΩÜÂØπ‰∫éü§ó Transformers‰∏≠ÁöÑTensorFlowÊñáÊú¨ÁîüÊàêÊ®°ÂûãÔºåÊàë‰ª¨Ê≥®ÊÑèÂà∞ÈÄüÂ∫¶ÊèêÈ´ò‰∫ÜÁ∫¶100ÂÄç„ÄÇÊú¨ÊñáÊ°£Â∞ÜËß£ÈáäÂ¶Ç‰ΩïÂú®Ëøô‰∫õÊ®°Âûã‰∏ä‰ΩøÁî®XLAËé∑ÂæóÊúÄÂ§ßÁöÑÊÄßËÉΩ„ÄÇÂ¶ÇÊûúÊÇ®ÊúâÂÖ¥Ë∂£‰∫ÜËß£Êõ¥Â§öÂÖ≥‰∫éÂü∫ÂáÜÊµãËØïÂíåÊàë‰ª¨Âú®XLAÈõÜÊàêËÉåÂêéÁöÑËÆæËÆ°Âì≤Â≠¶ÁöÑ‰ø°ÊÅØÔºåÊàë‰ª¨ËøòÂ∞ÜÊèê‰æõÈ¢ùÂ§ñÁöÑËµÑÊ∫êÈìæÊé•„ÄÇ\n-\n-\n-## ‰ΩøÁî® XLA ËøêË°å TensorFlow ÂáΩÊï∞\n-\n-ËÆ©Êàë‰ª¨ËÄÉËôë‰ª•‰∏ãTensorFlow ‰∏≠ÁöÑÊ®°ÂûãÔºö\n-\n-```py\n-import tensorflow as tf\n-\n-model = tf.keras.Sequential(\n-    [tf.keras.layers.Dense(10, input_shape=(10,), activation=\"relu\"), tf.keras.layers.Dense(5, activation=\"softmax\")]\n-)\n-```\n-\n-‰∏äËø∞Ê®°ÂûãÊé•ÂèóÁª¥Â∫¶‰∏∫ `(10,)` ÁöÑËæìÂÖ•„ÄÇÊàë‰ª¨ÂèØ‰ª•ÂÉè‰∏ãÈù¢ËøôÊ†∑‰ΩøÁî®Ê®°ÂûãËøõË°åÂâçÂêë‰º†Êí≠Ôºö\n-\n-```py\n-# Generate random inputs for the model.\n-batch_size = 16\n-input_vector_dim = 10\n-random_inputs = tf.random.normal((batch_size, input_vector_dim))\n-\n-# Run a forward pass.\n-_ = model(random_inputs)\n-```\n-\n-‰∏∫‰∫Ü‰ΩøÁî® XLA ÁºñËØëÁöÑÂáΩÊï∞ËøêË°åÂâçÂêë‰º†Êí≠ÔºåÊàë‰ª¨ÈúÄË¶ÅÊâßË°å‰ª•‰∏ãÊìç‰ΩúÔºö\n-\n-```py\n-xla_fn = tf.function(model, jit_compile=True)\n-_ = xla_fn(random_inputs)\n-```\n-\n-`model`ÁöÑÈªòËÆ§`call()`ÂáΩÊï∞Áî®‰∫éÁºñËØëXLAÂõæ„ÄÇ‰ΩÜÂ¶ÇÊûú‰Ω†ÊÉ≥Â∞ÜÂÖ∂‰ªñÊ®°ÂûãÂáΩÊï∞ÁºñËØëÊàêXLAÔºå‰πüÊòØÂèØ‰ª•ÁöÑÔºåÂ¶Ç‰∏ãÊâÄÁ§∫Ôºö\n-\n-```py\n-my_xla_fn = tf.function(model.my_xla_fn, jit_compile=True)\n-```\n-\n-## Âú®ü§ó TransformersÂ∫ì‰∏≠‰ΩøÁî®XLAËøêË°åTensorFlowÊñáÊú¨ÁîüÊàêÊ®°Âûã\n-\n-Ë¶ÅÂú®ü§ó Transformers‰∏≠ÂêØÁî®XLAÂä†ÈÄüÁîüÊàêÔºåÊÇ®ÈúÄË¶ÅÂÆâË£ÖÊúÄÊñ∞ÁâàÊú¨ÁöÑ`transformers`„ÄÇÊÇ®ÂèØ‰ª•ÈÄöËøáËøêË°å‰ª•‰∏ãÂëΩ‰ª§Êù•ÂÆâË£ÖÂÆÉÔºö\n-\n-```bash\n-pip install transformers --upgrade\n-```\n-\n-ÁÑ∂ÂêéÊÇ®ÂèØ‰ª•ËøêË°å‰ª•‰∏ã‰ª£Á†ÅÔºö\n-\n-```py\n-import tensorflow as tf\n-from transformers import AutoTokenizer, TFAutoModelForCausalLM\n-\n-# Will error if the minimal version of Transformers is not installed.\n-from transformers.utils import check_min_version\n-\n-check_min_version(\"4.21.0\")\n-\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", padding_side=\"left\", pad_token=\"</s>\")\n-model = TFAutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n-input_string = [\"TensorFlow is\"]\n-\n-# One line to create an XLA generation function\n-xla_generate = tf.function(model.generate, jit_compile=True)\n-\n-tokenized_input = tokenizer(input_string, return_tensors=\"tf\")\n-generated_tokens = xla_generate(**tokenized_input, num_beams=2)\n-\n-decoded_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n-print(f\"Generated -- {decoded_text}\")\n-# Generated -- TensorFlow is an open-source, open-source, distributed-source application # framework for the\n-```\n-\n-Ê≠£Â¶ÇÊÇ®ÊâÄÊ≥®ÊÑèÂà∞ÁöÑÔºåÂú®`generate()`‰∏äÂêØÁî®XLAÂè™ÈúÄË¶Å‰∏ÄË°å‰ª£Á†Å„ÄÇÂÖ∂‰ΩôÈÉ®ÂàÜ‰ª£Á†Å‰øùÊåÅ‰∏çÂèò„ÄÇÁÑ∂ËÄåÔºå‰∏äÈù¢ÁöÑ‰ª£Á†ÅÁâáÊÆµ‰∏≠Êúâ‰∏Ä‰∫õ‰∏éXLAÁõ∏ÂÖ≥ÁöÑÊ≥®ÊÑè‰∫ãÈ°π„ÄÇÊÇ®ÈúÄË¶Å‰∫ÜËß£Ëøô‰∫õÊ≥®ÊÑè‰∫ãÈ°πÔºå‰ª•ÂÖÖÂàÜÂà©Áî®XLAÂèØËÉΩÂ∏¶Êù•ÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÊàë‰ª¨Â∞ÜÂú®‰∏ãÈù¢ÁöÑÈÉ®ÂàÜËÆ®ËÆ∫Ëøô‰∫õÂÜÖÂÆπ„ÄÇ\n-\n-## ÈúÄË¶ÅÂÖ≥Ê≥®ÁöÑÊ≥®ÊÑè‰∫ãÈ°π\n-\n-ÂΩìÊÇ®È¶ñÊ¨°ÊâßË°åÂêØÁî®XLAÁöÑÂáΩÊï∞ÔºàÂ¶Ç‰∏äÈù¢ÁöÑ`xla_generate()`ÔºâÊó∂ÔºåÂÆÉÂ∞ÜÂú®ÂÜÖÈÉ®Â∞ùËØïÊé®Êñ≠ËÆ°ÁÆóÂõæÔºåËøôÊòØ‰∏Ä‰∏™ËÄóÊó∂ÁöÑËøáÁ®ã„ÄÇËøô‰∏™ËøáÁ®ãË¢´Áß∞‰∏∫[‚Äútracing‚Äù](https://www.tensorflow.org/guide/intro_to_graphs#when_is_a_function_tracing)„ÄÇ\n-\n-ÊÇ®ÂèØËÉΩ‰ºöÊ≥®ÊÑèÂà∞ÁîüÊàêÊó∂Èó¥Âπ∂‰∏çÂø´„ÄÇËøûÁª≠Ë∞ÉÁî®`xla_generate()`ÔºàÊàñ‰ªª‰ΩïÂÖ∂‰ªñÂêØÁî®‰∫ÜXLAÁöÑÂáΩÊï∞Ôºâ‰∏çÈúÄË¶ÅÂÜçÊ¨°Êé®Êñ≠ËÆ°ÁÆóÂõæÔºåÂè™Ë¶ÅÂáΩÊï∞ÁöÑËæìÂÖ•‰∏éÊúÄÂàùÊûÑÂª∫ËÆ°ÁÆóÂõæÊó∂ÁöÑÂΩ¢Áä∂Áõ∏ÂåπÈÖç„ÄÇÂØπ‰∫éÂÖ∑ÊúâÂõ∫ÂÆöËæìÂÖ•ÂΩ¢Áä∂ÁöÑÊ®°ÊÄÅÔºà‰æãÂ¶ÇÂõæÂÉèÔºâÔºåËøô‰∏çÊòØÈóÆÈ¢òÔºå‰ΩÜÂ¶ÇÊûúÊÇ®Ê≠£Âú®Â§ÑÁêÜÂÖ∑ÊúâÂèØÂèòËæìÂÖ•ÂΩ¢Áä∂ÁöÑÊ®°ÊÄÅÔºà‰æãÂ¶ÇÊñáÊú¨ÔºâÔºåÂàôÂøÖÈ°ªÊ≥®ÊÑè„ÄÇ\n-\n-‰∏∫‰∫ÜÁ°Æ‰øù`xla_generate()`ÂßãÁªà‰ΩøÁî®Áõ∏ÂêåÁöÑËæìÂÖ•ÂΩ¢Áä∂ÔºåÊÇ®ÂèØ‰ª•Âú®Ë∞ÉÁî®`tokenizer`Êó∂ÊåáÂÆö`padding`ÂèÇÊï∞„ÄÇ\n-\n-```py\n-import tensorflow as tf\n-from transformers import AutoTokenizer, TFAutoModelForCausalLM\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", padding_side=\"left\", pad_token=\"</s>\")\n-model = TFAutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n-input_string = [\"TensorFlow is\"]\n-\n-xla_generate = tf.function(model.generate, jit_compile=True)\n-\n-# Here, we call the tokenizer with padding options.\n-tokenized_input = tokenizer(input_string, pad_to_multiple_of=8, padding=True, return_tensors=\"tf\")\n-\n-generated_tokens = xla_generate(**tokenized_input, num_beams=2)\n-decoded_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n-print(f\"Generated -- {decoded_text}\")\n-```\n-\n-ÈÄöËøáËøôÁßçÊñπÂºèÔºåÊÇ®ÂèØ‰ª•Á°Æ‰øù`xla_generate()`ÁöÑËæìÂÖ•ÂßãÁªàÂÖ∑ÊúâÂÆÉË∑üË∏™ÁöÑÂΩ¢Áä∂Ôºå‰ªéËÄåÂä†ÈÄüÁîüÊàêÊó∂Èó¥„ÄÇÊÇ®ÂèØ‰ª•‰ΩøÁî®‰ª•‰∏ã‰ª£Á†ÅÊù•È™åËØÅËøô‰∏ÄÁÇπÔºö\n-\n-```py\n-import time\n-import tensorflow as tf\n-from transformers import AutoTokenizer, TFAutoModelForCausalLM\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", padding_side=\"left\", pad_token=\"</s>\")\n-model = TFAutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n-\n-xla_generate = tf.function(model.generate, jit_compile=True)\n-\n-for input_string in [\"TensorFlow is\", \"TensorFlow is a\", \"TFLite is a\"]:\n-    tokenized_input = tokenizer(input_string, pad_to_multiple_of=8, padding=True, return_tensors=\"tf\")\n-    start = time.time_ns()\n-    generated_tokens = xla_generate(**tokenized_input, num_beams=2)\n-    end = time.time_ns()\n-    print(f\"Execution time -- {(end - start) / 1e6:.1f} ms\\n\")\n-```\n-\n-Âú®Tesla T4 GPU‰∏äÔºåÊÇ®ÂèØ‰ª•ÊúüÊúõÂ¶Ç‰∏ãÁöÑËæìÂá∫Ôºö\n-\n-```bash\n-Execution time -- 30819.6 ms\n-\n-Execution time -- 79.0 ms\n-\n-Execution time -- 78.9 ms\n-```\n-\n-Á¨¨‰∏ÄÊ¨°Ë∞ÉÁî®`xla_generate()`‰ºöÂõ†‰∏∫`tracing`ËÄåËÄóÊó∂Ôºå‰ΩÜÂêéÁª≠ÁöÑË∞ÉÁî®‰ºöÂø´ÂæóÂ§ö„ÄÇËØ∑Ê≥®ÊÑèÔºå‰ªª‰ΩïÊó∂ÂÄôÂØπÁîüÊàêÈÄâÈ°πÁöÑÊõ¥ÊîπÈÉΩ‰ºöËß¶ÂèëÈáçÊñ∞`tracing`Ôºå‰ªéËÄåÂØºËá¥ÁîüÊàêÊó∂Èó¥ÂáèÊÖ¢„ÄÇ\n-\n-Âú®Êú¨ÊñáÊ°£‰∏≠ÔºåÊàë‰ª¨Ê≤°ÊúâÊ∂µÁõñü§ó TransformersÊèê‰æõÁöÑÊâÄÊúâÊñáÊú¨ÁîüÊàêÈÄâÈ°π„ÄÇÊàë‰ª¨ÈºìÂä±ÊÇ®ÈòÖËØªÊñáÊ°£‰ª•‰∫ÜËß£È´òÁ∫ßÁî®‰æã„ÄÇ\n-\n-## ÈôÑÂä†ËµÑÊ∫ê\n-\n-‰ª•‰∏ãÊòØ‰∏Ä‰∫õÈôÑÂä†ËµÑÊ∫êÔºåÂ¶ÇÊûúÊÇ®ÊÉ≥Ê∑±ÂÖ•‰∫ÜËß£Âú®ü§ó TransformersÂíåÂÖ∂‰ªñÂ∫ì‰∏ã‰ΩøÁî®XLAÔºö\n-\n-* [Ëøô‰∏™Colab Notebook](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/91_tf_xla_generate.ipynb) Êèê‰æõ‰∫Ü‰∏Ä‰∏™‰∫íÂä®ÊºîÁ§∫ÔºåËÆ©ÊÇ®ÂèØ‰ª•Â∞ùËØï‰ΩøÁî®XLAÂÖºÂÆπÁöÑÁºñÁ†ÅÂô®-Ëß£Á†ÅÂô®Ôºà‰æãÂ¶Ç[T5](https://huggingface.co/docs/transformers/model_doc/t5)ÔºâÂíå‰ªÖËß£Á†ÅÂô®Ôºà‰æãÂ¶Ç[GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)ÔºâÊñáÊú¨ÁîüÊàêÊ®°Âûã„ÄÇ\n-\n-* [ËøôÁØáÂçöÂÆ¢ÊñáÁ´†](https://huggingface.co/blog/tf-xla-generate) Êèê‰æõ‰∫ÜXLAÂÖºÂÆπÊ®°ÂûãÁöÑÊØîËæÉÂü∫ÂáÜÊ¶ÇËø∞Ôºå‰ª•ÂèäÂÖ≥‰∫éÂú®TensorFlow‰∏≠‰ΩøÁî®XLAÁöÑÂèãÂ•Ω‰ªãÁªç„ÄÇ\n-\n-* [ËøôÁØáÂçöÂÆ¢ÊñáÁ´†](https://blog.tensorflow.org/2022/11/how-hugging-face-improved-text-generation-performance-with-xla.html) ËÆ®ËÆ∫‰∫ÜÊàë‰ª¨Âú®ü§ó Transformers‰∏≠‰∏∫TensorFlowÊ®°ÂûãÊ∑ªÂä†XLAÊîØÊåÅÁöÑËÆæËÆ°ÁêÜÂøµ„ÄÇ\n-\n-* Êé®ËçêÁî®‰∫éÊõ¥Â§öÂ≠¶‰π†XLAÂíåTensorFlowÂõæÁöÑËµÑÊ∫êÔºö\n-    * [XLAÔºöÈù¢ÂêëÊú∫Âô®Â≠¶‰π†ÁöÑ‰ºòÂåñÁºñËØëÂô®](https://www.tensorflow.org/xla)\n-    * [ÂõæÂíåtf.functionÁÆÄ‰ªã](https://www.tensorflow.org/guide/intro_to_graphs)\n-    * [‰ΩøÁî®tf.functionËé∑ÂæóÊõ¥Â•ΩÁöÑÊÄßËÉΩ](https://www.tensorflow.org/guide/function)\n\\ No newline at end of file"
        },
        {
            "sha": "f0280156def4313023993a72b3c3b6081a8bd400",
            "filename": "docs/source/zh/tflite.md",
            "status": "removed",
            "additions": 0,
            "deletions": 54,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/docs%2Fsource%2Fzh%2Ftflite.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/docs%2Fsource%2Fzh%2Ftflite.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Ftflite.md?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,54 +0,0 @@\n-<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# ÂØºÂá∫‰∏∫ TFLite\n-\n-[TensorFlow Lite](https://www.tensorflow.org/lite/guide) ÊòØ‰∏Ä‰∏™ËΩªÈáèÁ∫ßÊ°ÜÊû∂ÔºåÁî®‰∫éËµÑÊ∫êÂèóÈôêÁöÑËÆæÂ§á‰∏äÔºåÂ¶ÇÊâãÊú∫„ÄÅÂµåÂÖ•ÂºèÁ≥ªÁªüÂíåÁâ©ËÅîÁΩëÔºàIoTÔºâËÆæÂ§áÔºåÈÉ®ÁΩ≤Êú∫Âô®Â≠¶‰π†Ê®°Âûã„ÄÇTFLite Êó®Âú®Âú®ËÆ°ÁÆóËÉΩÂäõ„ÄÅÂÜÖÂ≠òÂíåÂäüËÄóÊúâÈôêÁöÑËÆæÂ§á‰∏ä‰ºòÂåñÂíåÈ´òÊïàËøêË°åÊ®°Âûã„ÄÇÊ®°Âûã‰ª•‰∏ÄÁßçÁâπÊÆäÁöÑÈ´òÊïàÂèØÁßªÊ§çÊ†ºÂºèË°®Á§∫ÔºåÂÖ∂Êñá‰ª∂Êâ©Â±ïÂêç‰∏∫ `.tflite`„ÄÇ\n-\n-ü§ó Optimum ÈÄöËøá `exporters.tflite` Ê®°ÂùóÊèê‰æõÂ∞Ü ü§ó Transformers Ê®°ÂûãÂØºÂá∫Ëá≥ TFLite Ê†ºÂºèÁöÑÂäüËÉΩ„ÄÇËØ∑ÂèÇËÄÉ [ü§ó Optimum ÊñáÊ°£](https://huggingface.co/docs/optimum/exporters/tflite/overview) ‰ª•Ëé∑ÂèñÊîØÊåÅÁöÑÊ®°ÂûãÊû∂ÊûÑÂàóË°®„ÄÇ\n-\n-Ë¶ÅÂ∞ÜÊ®°ÂûãÂØºÂá∫‰∏∫ TFLite Ê†ºÂºèÔºåËØ∑ÂÆâË£ÖÊâÄÈúÄÁöÑ‰æùËµñÈ°πÔºö\n-\n-```bash\n-pip install optimum[exporters-tf]\n-```\n-\n-ËØ∑ÂèÇÈòÖ [ü§ó Optimum ÊñáÊ°£](https://huggingface.co/docs/optimum/main/en/exporters/tflite/usage_guides/export_a_model) ‰ª•Êü•ÁúãÊâÄÊúâÂèØÁî®ÂèÇÊï∞ÔºåÊàñËÄÖÂú®ÂëΩ‰ª§Ë°å‰∏≠Êü•ÁúãÂ∏ÆÂä©Ôºö\n-\n-```bash\n-optimum-cli export tflite --help\n-```\n-\n-ËøêË°å‰ª•‰∏ãÂëΩ‰ª§Ôºå‰ª•‰ªé ü§ó Hub ÂØºÂá∫Ê®°ÂûãÁöÑÊ£ÄÊü•ÁÇπÔºàcheckpointÔºâÔºå‰ª• `google-bert/bert-base-uncased` ‰∏∫‰æãÔºö\n-\n-```bash\n-optimum-cli export tflite --model google-bert/bert-base-uncased --sequence_length 128 bert_tflite/\n-```\n-\n-‰Ω†Â∫îËØ•ËÉΩÂú®Êó•Âøó‰∏≠ÁúãÂà∞ÂØºÂá∫ËøõÂ∫¶‰ª•ÂèäÁîüÊàêÁöÑ `model.tflite` Êñá‰ª∂ÁöÑ‰øùÂ≠ò‰ΩçÁΩÆÔºåÂ¶Ç‰∏ãÊâÄÁ§∫Ôºö\n-\n-```bash\n-Validating TFLite model...\n-\t-[‚úì] TFLite model output names match reference model (logits)\n-\t- Validating TFLite Model output \"logits\":\n-\t\t-[‚úì] (1, 128, 30522) matches (1, 128, 30522)\n-\t\t-[x] values not close enough, max diff: 5.817413330078125e-05 (atol: 1e-05)\n-The TensorFlow Lite export succeeded with the warning: The maximum absolute difference between the output of the reference model and the TFLite exported model is not within the set tolerance 1e-05:\n-- logits: max diff = 5.817413330078125e-05.\n- The exported model was saved at: bert_tflite\n-```\n-\n-‰∏äÈù¢ÁöÑÁ§∫‰æãËØ¥Êòé‰∫Ü‰ªé ü§ó Hub ÂØºÂá∫Ê£ÄÊü•ÁÇπÁöÑËøáÁ®ã„ÄÇÂØºÂá∫Êú¨Âú∞Ê®°ÂûãÊó∂ÔºåÈ¶ñÂÖàÈúÄË¶ÅÁ°Æ‰øùÂ∞ÜÊ®°ÂûãÁöÑÊùÉÈáçÂíåÂàÜËØçÂô®Êñá‰ª∂‰øùÂ≠òÂú®Âêå‰∏ÄÁõÆÂΩïÔºà`local_path`Ôºâ‰∏≠„ÄÇÂú®‰ΩøÁî® CLIÔºàÂëΩ‰ª§Ë°åÔºâÊó∂ÔºåÂ∞Ü `local_path` ‰º†ÈÄíÁªô `model` ÂèÇÊï∞ÔºåËÄå‰∏çÊòØ ü§ó Hub ‰∏äÁöÑÊ£ÄÊü•ÁÇπÂêçÁß∞„ÄÇ\n\\ No newline at end of file"
        },
        {
            "sha": "64d3604f9ca4b37a8b1bac584ed623e6f9b5a661",
            "filename": "examples/legacy/multiple_choice/utils_multiple_choice.py",
            "status": "modified",
            "additions": 1,
            "deletions": 94,
            "changes": 95,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/examples%2Flegacy%2Fmultiple_choice%2Futils_multiple_choice.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/examples%2Flegacy%2Fmultiple_choice%2Futils_multiple_choice.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fmultiple_choice%2Futils_multiple_choice.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -26,7 +26,7 @@\n import tqdm\n from filelock import FileLock\n \n-from transformers import PreTrainedTokenizer, is_tf_available, is_torch_available\n+from transformers import PreTrainedTokenizer, is_torch_available\n \n \n logger = logging.getLogger(__name__)\n@@ -78,11 +78,6 @@ class Split(Enum):\n     from torch.utils.data import Dataset\n \n     class MultipleChoiceDataset(Dataset):\n-        \"\"\"\n-        This will be superseded by a framework-agnostic approach\n-        soon.\n-        \"\"\"\n-\n         features: list[InputFeatures]\n \n         def __init__(\n@@ -139,94 +134,6 @@ def __getitem__(self, i) -> InputFeatures:\n             return self.features[i]\n \n \n-if is_tf_available():\n-    import tensorflow as tf\n-\n-    class TFMultipleChoiceDataset:\n-        \"\"\"\n-        This will be superseded by a framework-agnostic approach\n-        soon.\n-        \"\"\"\n-\n-        features: list[InputFeatures]\n-\n-        def __init__(\n-            self,\n-            data_dir: str,\n-            tokenizer: PreTrainedTokenizer,\n-            task: str,\n-            max_seq_length: Optional[int] = 128,\n-            overwrite_cache=False,\n-            mode: Split = Split.train,\n-        ):\n-            processor = processors[task]()\n-\n-            logger.info(f\"Creating features from dataset file at {data_dir}\")\n-            label_list = processor.get_labels()\n-            if mode == Split.dev:\n-                examples = processor.get_dev_examples(data_dir)\n-            elif mode == Split.test:\n-                examples = processor.get_test_examples(data_dir)\n-            else:\n-                examples = processor.get_train_examples(data_dir)\n-            logger.info(\"Training examples: %s\", len(examples))\n-\n-            self.features = convert_examples_to_features(\n-                examples,\n-                label_list,\n-                max_seq_length,\n-                tokenizer,\n-            )\n-\n-            def gen():\n-                for ex_index, ex in tqdm.tqdm(enumerate(self.features), desc=\"convert examples to features\"):\n-                    if ex_index % 10000 == 0:\n-                        logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n-\n-                    yield (\n-                        {\n-                            \"example_id\": 0,\n-                            \"input_ids\": ex.input_ids,\n-                            \"attention_mask\": ex.attention_mask,\n-                            \"token_type_ids\": ex.token_type_ids,\n-                        },\n-                        ex.label,\n-                    )\n-\n-            self.dataset = tf.data.Dataset.from_generator(\n-                gen,\n-                (\n-                    {\n-                        \"example_id\": tf.int32,\n-                        \"input_ids\": tf.int32,\n-                        \"attention_mask\": tf.int32,\n-                        \"token_type_ids\": tf.int32,\n-                    },\n-                    tf.int64,\n-                ),\n-                (\n-                    {\n-                        \"example_id\": tf.TensorShape([]),\n-                        \"input_ids\": tf.TensorShape([None, None]),\n-                        \"attention_mask\": tf.TensorShape([None, None]),\n-                        \"token_type_ids\": tf.TensorShape([None, None]),\n-                    },\n-                    tf.TensorShape([]),\n-                ),\n-            )\n-\n-        def get_dataset(self):\n-            self.dataset = self.dataset.apply(tf.data.experimental.assert_cardinality(len(self.features)))\n-\n-            return self.dataset\n-\n-        def __len__(self):\n-            return len(self.features)\n-\n-        def __getitem__(self, i) -> InputFeatures:\n-            return self.features[i]\n-\n-\n class DataProcessor:\n     \"\"\"Base class for data converters for multiple choice data sets.\"\"\"\n "
        },
        {
            "sha": "bfd792a250c381726886e63f538ed9be8f8f037c",
            "filename": "examples/legacy/token-classification/utils_ner.py",
            "status": "modified",
            "additions": 1,
            "deletions": 103,
            "changes": 104,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/examples%2Flegacy%2Ftoken-classification%2Futils_ner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/examples%2Flegacy%2Ftoken-classification%2Futils_ner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Ftoken-classification%2Futils_ner.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -22,7 +22,7 @@\n \n from filelock import FileLock\n \n-from transformers import PreTrainedTokenizer, is_tf_available, is_torch_available\n+from transformers import PreTrainedTokenizer, is_torch_available\n \n \n logger = logging.getLogger(__name__)\n@@ -208,11 +208,6 @@ def convert_examples_to_features(\n     from torch.utils.data import Dataset\n \n     class TokenClassificationDataset(Dataset):\n-        \"\"\"\n-        This will be superseded by a framework-agnostic approach\n-        soon.\n-        \"\"\"\n-\n         features: list[InputFeatures]\n         pad_token_label_id: int = nn.CrossEntropyLoss().ignore_index\n         # Use cross entropy ignore_index as padding label id so that only\n@@ -271,100 +266,3 @@ def __len__(self):\n \n         def __getitem__(self, i) -> InputFeatures:\n             return self.features[i]\n-\n-\n-if is_tf_available():\n-    import tensorflow as tf\n-\n-    class TFTokenClassificationDataset:\n-        \"\"\"\n-        This will be superseded by a framework-agnostic approach\n-        soon.\n-        \"\"\"\n-\n-        features: list[InputFeatures]\n-        pad_token_label_id: int = -100\n-        # Use cross entropy ignore_index as padding label id so that only\n-        # real label ids contribute to the loss later.\n-\n-        def __init__(\n-            self,\n-            token_classification_task: TokenClassificationTask,\n-            data_dir: str,\n-            tokenizer: PreTrainedTokenizer,\n-            labels: list[str],\n-            model_type: str,\n-            max_seq_length: Optional[int] = None,\n-            overwrite_cache=False,\n-            mode: Split = Split.train,\n-        ):\n-            examples = token_classification_task.read_examples_from_file(data_dir, mode)\n-            # TODO clean up all this to leverage built-in features of tokenizers\n-            self.features = token_classification_task.convert_examples_to_features(\n-                examples,\n-                labels,\n-                max_seq_length,\n-                tokenizer,\n-                cls_token_at_end=bool(model_type in [\"xlnet\"]),\n-                # xlnet has a cls token at the end\n-                cls_token=tokenizer.cls_token,\n-                cls_token_segment_id=2 if model_type in [\"xlnet\"] else 0,\n-                sep_token=tokenizer.sep_token,\n-                sep_token_extra=False,\n-                # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n-                pad_on_left=bool(tokenizer.padding_side == \"left\"),\n-                pad_token=tokenizer.pad_token_id,\n-                pad_token_segment_id=tokenizer.pad_token_type_id,\n-                pad_token_label_id=self.pad_token_label_id,\n-            )\n-\n-            def gen():\n-                for ex in self.features:\n-                    if ex.token_type_ids is None:\n-                        yield (\n-                            {\"input_ids\": ex.input_ids, \"attention_mask\": ex.attention_mask},\n-                            ex.label_ids,\n-                        )\n-                    else:\n-                        yield (\n-                            {\n-                                \"input_ids\": ex.input_ids,\n-                                \"attention_mask\": ex.attention_mask,\n-                                \"token_type_ids\": ex.token_type_ids,\n-                            },\n-                            ex.label_ids,\n-                        )\n-\n-            if \"token_type_ids\" not in tokenizer.model_input_names:\n-                self.dataset = tf.data.Dataset.from_generator(\n-                    gen,\n-                    ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32}, tf.int64),\n-                    (\n-                        {\"input_ids\": tf.TensorShape([None]), \"attention_mask\": tf.TensorShape([None])},\n-                        tf.TensorShape([None]),\n-                    ),\n-                )\n-            else:\n-                self.dataset = tf.data.Dataset.from_generator(\n-                    gen,\n-                    ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n-                    (\n-                        {\n-                            \"input_ids\": tf.TensorShape([None]),\n-                            \"attention_mask\": tf.TensorShape([None]),\n-                            \"token_type_ids\": tf.TensorShape([None]),\n-                        },\n-                        tf.TensorShape([None]),\n-                    ),\n-                )\n-\n-        def get_dataset(self):\n-            self.dataset = self.dataset.apply(tf.data.experimental.assert_cardinality(len(self.features)))\n-\n-            return self.dataset\n-\n-        def __len__(self):\n-            return len(self.features)\n-\n-        def __getitem__(self, i) -> InputFeatures:\n-            return self.features[i]"
        },
        {
            "sha": "cd521a0f606d0ebe0212a353ec9867fca3af35bf",
            "filename": "examples/modular-transformers/image_processing_new_imgproc_model.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/examples%2Fmodular-transformers%2Fimage_processing_new_imgproc_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/examples%2Fmodular-transformers%2Fimage_processing_new_imgproc_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fimage_processing_new_imgproc_model.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -152,7 +152,7 @@ def preprocess(\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = None,\n+        resample: Optional[PILImageResampling] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n@@ -194,10 +194,8 @@ def preprocess(\n             return_tensors (`str` or `TensorType`, *optional*):\n                 The type of tensors to return. Can be one of:\n                     - Unset: Return a list of `np.ndarray`.\n-                    - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n                     - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n                     - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n-                    - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n             data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n                 The channel dimension format for the output image. Can be one of:\n                 - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n@@ -221,13 +219,11 @@ def preprocess(\n \n         size = size if size is not None else self.size\n         size = get_size_dict(size, default_to_square=False)\n+        images = self.fetch_images(images)\n         images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n-            raise ValueError(\n-                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n-                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n-            )\n+            raise ValueError(\"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, or torch.Tensor\")\n \n         validate_preprocess_arguments(\n             do_rescale=do_rescale,"
        },
        {
            "sha": "9df092f73e6e4ff838a12217f1a4d5c9654c866d",
            "filename": "examples/modular-transformers/modeling_dummy_bert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 90,
            "changes": 91,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -5,11 +5,9 @@\n #                          modular_dummy_bert.py file directly. One of our CI enforces this.\n #                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n import math\n-import os\n from typing import Optional, Union\n \n import torch\n-from packaging import version\n from torch import nn\n \n from ...activations import ACT2FN\n@@ -19,7 +17,7 @@\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, BaseModelOutputWithPoolingAndCrossAttentions\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import auto_docstring, get_torch_version, logging\n+from ...utils import auto_docstring, logging\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_dummy_bert import DummyBertConfig\n \n@@ -36,8 +34,6 @@ def __init__(self, config):\n         self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n         self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n \n-        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n-        # any TensorFlow checkpoint file\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n         # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n@@ -228,7 +224,6 @@ class DummyBertSdpaSelfAttention(DummyBertSelfAttention):\n     def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         super().__init__(config, position_embedding_type=position_embedding_type, layer_idx=layer_idx)\n         self.dropout_prob = config.attention_probs_dropout_prob\n-        self.require_contiguous_qkv = version.parse(get_torch_version()) < version.parse(\"2.2.0\")\n \n     # Adapted from DummyBertSelfAttention\n     @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n@@ -308,14 +303,6 @@ def forward(\n                 if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n-        # SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom\n-        # attn_mask, so we need to call `.contiguous()` here. This was fixed in torch==2.2.0.\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577\n-        if self.require_contiguous_qkv and query_layer.device.type == \"cuda\" and attention_mask is not None:\n-            query_layer = query_layer.contiguous()\n-            key_layer = key_layer.contiguous()\n-            value_layer = value_layer.contiguous()\n-\n         # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n         # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n         # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\n@@ -655,92 +642,16 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-def load_tf_weights_in_dummy_bert(model, config, tf_checkpoint_path):\n-    \"\"\"Load tf checkpoints in a pytorch model.\"\"\"\n-    try:\n-        import re\n-\n-        import numpy as np\n-        import tensorflow as tf\n-    except ImportError:\n-        logger.error(\n-            \"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"\n-            \"https://www.tensorflow.org/install/ for installation instructions.\"\n-        )\n-        raise\n-    tf_path = os.path.abspath(tf_checkpoint_path)\n-    logger.info(f\"Converting TensorFlow checkpoint from {tf_path}\")\n-    # Load weights from TF model\n-    init_vars = tf.train.list_variables(tf_path)\n-    names = []\n-    arrays = []\n-    for name, shape in init_vars:\n-        logger.info(f\"Loading TF weight {name} with shape {shape}\")\n-        array = tf.train.load_variable(tf_path, name)\n-        names.append(name)\n-        arrays.append(array)\n-\n-    for name, array in zip(names, arrays):\n-        name = name.split(\"/\")\n-        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n-        # which are not required for using pretrained model\n-        if any(\n-            n in [\"adam_v\", \"adam_m\", \"AdamWeightDecayOptimizer\", \"AdamWeightDecayOptimizer_1\", \"global_step\"]\n-            for n in name\n-        ):\n-            logger.info(f\"Skipping {'/'.join(name)}\")\n-            continue\n-        pointer = model\n-        for m_name in name:\n-            if re.fullmatch(r\"[A-Za-z]+_\\d+\", m_name):\n-                scope_names = re.split(r\"_(\\d+)\", m_name)\n-            else:\n-                scope_names = [m_name]\n-            if scope_names[0] == \"kernel\" or scope_names[0] == \"gamma\":\n-                pointer = getattr(pointer, \"weight\")\n-            elif scope_names[0] == \"output_bias\" or scope_names[0] == \"beta\":\n-                pointer = getattr(pointer, \"bias\")\n-            elif scope_names[0] == \"output_weights\":\n-                pointer = getattr(pointer, \"weight\")\n-            elif scope_names[0] == \"squad\":\n-                pointer = getattr(pointer, \"classifier\")\n-            else:\n-                try:\n-                    pointer = getattr(pointer, scope_names[0])\n-                except AttributeError:\n-                    logger.info(f\"Skipping {'/'.join(name)}\")\n-                    continue\n-            if len(scope_names) >= 2:\n-                num = int(scope_names[1])\n-                pointer = pointer[num]\n-        if m_name[-11:] == \"_embeddings\":\n-            pointer = getattr(pointer, \"weight\")\n-        elif m_name == \"kernel\":\n-            array = np.transpose(array)\n-        try:\n-            if pointer.shape != array.shape:\n-                raise ValueError(f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\")\n-        except ValueError as e:\n-            e.args += (pointer.shape, array.shape)\n-            raise\n-        logger.info(f\"Initialize PyTorch weight {name}\")\n-        pointer.data = torch.from_numpy(array)\n-    return model\n-\n-\n @auto_docstring\n class DummyBertPreTrainedModel(PreTrainedModel):\n     config: DummyBertConfig\n-    load_tf_weights = load_tf_weights_in_dummy_bert\n     base_model_prefix = \"dummy_bert\"\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            # Slightly different from the TF version which uses truncated_normal for initialization\n-            # cf https://github.com/pytorch/pytorch/pull/5617\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n                 module.bias.data.zero_()"
        },
        {
            "sha": "6c7a0c776a8d569d40169a30c38e55d6ef56efd2",
            "filename": "examples/modular-transformers/modeling_from_uppercase_model.py",
            "status": "modified",
            "additions": 1,
            "deletions": 11,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/examples%2Fmodular-transformers%2Fmodeling_from_uppercase_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/examples%2Fmodular-transformers%2Fmodeling_from_uppercase_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_from_uppercase_model.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -12,13 +12,9 @@\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n-from ...utils import logging\n from .configuration_from_uppercase_model import FromUppercaseModelTextConfig, FromUppercaseModelVisionConfig\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n@@ -96,13 +92,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "44b591fafad24cfeb9ecc64ce395894cf0a85b93",
            "filename": "examples/modular-transformers/modeling_multimodal2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 18,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/examples%2Fmodular-transformers%2Fmodeling_multimodal2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/examples%2Fmodular-transformers%2Fmodeling_multimodal2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_multimodal2.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -16,13 +16,10 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...utils import auto_docstring, can_return_tuple, logging, torch_int\n+from ...utils import auto_docstring, can_return_tuple, torch_int\n from .configuration_multimodal2 import Multimodal2Config, Multimodal2TextConfig, Multimodal2VisionConfig\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n@@ -100,13 +97,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -196,13 +187,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "27593bddf50e0052b7e63134c4875f53b4c614c3",
            "filename": "examples/modular-transformers/modeling_my_new_model2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -220,7 +220,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.Tensor]:\n+    ) -> torch.Tensor:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n         # Self Attention"
        },
        {
            "sha": "13ef7e08271fb0b3a3eb456dd74dac84c9c8bdfa",
            "filename": "examples/modular-transformers/modeling_new_task_model.py",
            "status": "modified",
            "additions": 6,
            "deletions": 7,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -10,7 +10,7 @@\n import torch\n from torch import nn\n \n-from ...cache_utils import Cache, HybridCache, StaticCache\n+from ...cache_utils import Cache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast\n@@ -93,7 +93,7 @@ class NewTaskModelPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"NewTaskModelMultiModalProjector\"]\n     _skip_keys_device_placement = \"past_key_values\"\n \n-    _can_compile_fullgraph = True\n+    _can_compile_fullgraph = False\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n@@ -166,8 +166,6 @@ def _update_causal_mask(\n         inputs_lead_dim, sequence_length = input_tensor.shape[:2]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n-        elif isinstance(past_key_values, HybridCache):\n-            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]\n@@ -256,8 +254,8 @@ def get_placeholder_mask(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n@@ -505,7 +503,8 @@ def prepare_inputs_for_generation(\n         if cache_position[0] == 0:\n             model_inputs[\"pixel_values\"] = pixel_values\n         is_training = token_type_ids is not None and labels is not None\n-        if cache_position[0] == 0 and isinstance(past_key_values, HybridCache):\n+        is_static_hybrid_cache = isinstance(past_key_values, StaticCache) and any(past_key_values.is_sliding)\n+        if cache_position[0] == 0 and is_static_hybrid_cache:\n             input_tensor = inputs_embeds if inputs_embeds is not None else input_ids\n             causal_mask = self.model._update_causal_mask(\n                 attention_mask, token_type_ids, past_key_values, cache_position, input_tensor, is_training"
        },
        {
            "sha": "2ae39a55589224ca7c1469eb760be2479a997a6d",
            "filename": "examples/modular-transformers/modeling_roberta.py",
            "status": "modified",
            "additions": 1,
            "deletions": 90,
            "changes": 91,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/examples%2Fmodular-transformers%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/examples%2Fmodular-transformers%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_roberta.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -5,12 +5,10 @@\n #                          modular_roberta.py file directly. One of our CI enforces this.\n #                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n import math\n-import os\n from typing import Optional, Union\n \n import torch\n import torch.nn as nn\n-from packaging import version\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n@@ -19,7 +17,7 @@\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, BaseModelOutputWithPoolingAndCrossAttentions\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import auto_docstring, get_torch_version, logging\n+from ...utils import auto_docstring, logging\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_roberta import RobertaConfig\n \n@@ -38,8 +36,6 @@ def __init__(self, config):\n         )\n         self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n \n-        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n-        # any TensorFlow checkpoint file\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n         # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n@@ -231,7 +227,6 @@ class RobertaSdpaSelfAttention(RobertaSelfAttention):\n     def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         super().__init__(config, position_embedding_type=position_embedding_type, layer_idx=layer_idx)\n         self.dropout_prob = config.attention_probs_dropout_prob\n-        self.require_contiguous_qkv = version.parse(get_torch_version()) < version.parse(\"2.2.0\")\n \n     # Adapted from RobertaSelfAttention\n     @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n@@ -311,14 +306,6 @@ def forward(\n                 if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n-        # SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom\n-        # attn_mask, so we need to call `.contiguous()` here. This was fixed in torch==2.2.0.\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577\n-        if self.require_contiguous_qkv and query_layer.device.type == \"cuda\" and attention_mask is not None:\n-            query_layer = query_layer.contiguous()\n-            key_layer = key_layer.contiguous()\n-            value_layer = value_layer.contiguous()\n-\n         # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n         # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n         # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\n@@ -658,92 +645,16 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-def load_tf_weights_in_roberta(model, config, tf_checkpoint_path):\n-    \"\"\"Load tf checkpoints in a pytorch model.\"\"\"\n-    try:\n-        import re\n-\n-        import numpy as np\n-        import tensorflow as tf\n-    except ImportError:\n-        logger.error(\n-            \"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"\n-            \"https://www.tensorflow.org/install/ for installation instructions.\"\n-        )\n-        raise\n-    tf_path = os.path.abspath(tf_checkpoint_path)\n-    logger.info(f\"Converting TensorFlow checkpoint from {tf_path}\")\n-    # Load weights from TF model\n-    init_vars = tf.train.list_variables(tf_path)\n-    names = []\n-    arrays = []\n-    for name, shape in init_vars:\n-        logger.info(f\"Loading TF weight {name} with shape {shape}\")\n-        array = tf.train.load_variable(tf_path, name)\n-        names.append(name)\n-        arrays.append(array)\n-\n-    for name, array in zip(names, arrays):\n-        name = name.split(\"/\")\n-        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n-        # which are not required for using pretrained model\n-        if any(\n-            n in [\"adam_v\", \"adam_m\", \"AdamWeightDecayOptimizer\", \"AdamWeightDecayOptimizer_1\", \"global_step\"]\n-            for n in name\n-        ):\n-            logger.info(f\"Skipping {'/'.join(name)}\")\n-            continue\n-        pointer = model\n-        for m_name in name:\n-            if re.fullmatch(r\"[A-Za-z]+_\\d+\", m_name):\n-                scope_names = re.split(r\"_(\\d+)\", m_name)\n-            else:\n-                scope_names = [m_name]\n-            if scope_names[0] == \"kernel\" or scope_names[0] == \"gamma\":\n-                pointer = getattr(pointer, \"weight\")\n-            elif scope_names[0] == \"output_bias\" or scope_names[0] == \"beta\":\n-                pointer = getattr(pointer, \"bias\")\n-            elif scope_names[0] == \"output_weights\":\n-                pointer = getattr(pointer, \"weight\")\n-            elif scope_names[0] == \"squad\":\n-                pointer = getattr(pointer, \"classifier\")\n-            else:\n-                try:\n-                    pointer = getattr(pointer, scope_names[0])\n-                except AttributeError:\n-                    logger.info(f\"Skipping {'/'.join(name)}\")\n-                    continue\n-            if len(scope_names) >= 2:\n-                num = int(scope_names[1])\n-                pointer = pointer[num]\n-        if m_name[-11:] == \"_embeddings\":\n-            pointer = getattr(pointer, \"weight\")\n-        elif m_name == \"kernel\":\n-            array = np.transpose(array)\n-        try:\n-            if pointer.shape != array.shape:\n-                raise ValueError(f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\")\n-        except ValueError as e:\n-            e.args += (pointer.shape, array.shape)\n-            raise\n-        logger.info(f\"Initialize PyTorch weight {name}\")\n-        pointer.data = torch.from_numpy(array)\n-    return model\n-\n-\n @auto_docstring\n class RobertaPreTrainedModel(PreTrainedModel):\n     config: RobertaConfig\n-    load_tf_weights = load_tf_weights_in_roberta\n     base_model_prefix = \"roberta\"\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            # Slightly different from the TF version which uses truncated_normal for initialization\n-            # cf https://github.com/pytorch/pytorch/pull/5617\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n                 module.bias.data.zero_()"
        },
        {
            "sha": "9215730ed036414aacb4530e239c67ef5c053730",
            "filename": "examples/modular-transformers/modeling_super.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_super.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -46,6 +46,8 @@ def extra_repr(self):\n \n \n class SuperRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: SuperConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n@@ -260,7 +262,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.Tensor]:\n+    ) -> torch.Tensor:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n         # Self Attention"
        },
        {
            "sha": "3ff225c0b3ffa0c5933767b525aa50cce98bf001",
            "filename": "examples/modular-transformers/modeling_test_detr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/examples%2Fmodular-transformers%2Fmodeling_test_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/examples%2Fmodular-transformers%2Fmodeling_test_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_test_detr.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -846,8 +846,6 @@ def _init_weights(self, module):\n             nn.init.xavier_uniform_(module.output_proj.weight.data)\n             nn.init.constant_(module.output_proj.bias.data, 0.0)\n         elif isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n-            # Slightly different from the TF version which uses truncated_normal for initialization\n-            # cf https://github.com/pytorch/pytorch/pull/5617\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()"
        },
        {
            "sha": "477e193cbe7b9ce8ac669eff7e0a5471a511260f",
            "filename": "examples/pytorch/question-answering/utils_qa.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/examples%2Fpytorch%2Fquestion-answering%2Futils_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/examples%2Fpytorch%2Fquestion-answering%2Futils_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Futils_qa.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -185,7 +185,7 @@ def postprocess_qa_predictions(\n         if len(predictions) == 0 or (len(predictions) == 1 and predictions[0][\"text\"] == \"\"):\n             predictions.insert(0, {\"text\": \"empty\", \"start_logit\": 0.0, \"end_logit\": 0.0, \"score\": 0.0})\n \n-        # Compute the softmax of all scores (we do it with numpy to stay independent from torch/tf in this file, using\n+        # Compute the softmax of all scores (we do it with numpy to stay independent from torch in this file, using\n         # the LogSumExp trick).\n         scores = np.array([pred.pop(\"score\") for pred in predictions])\n         exp_scores = np.exp(scores - np.max(scores))\n@@ -392,7 +392,7 @@ def postprocess_qa_predictions_with_beam_search(\n             min_null_score = -2e-6\n             predictions.insert(0, {\"text\": \"\", \"start_logit\": -1e-6, \"end_logit\": -1e-6, \"score\": min_null_score})\n \n-        # Compute the softmax of all scores (we do it with numpy to stay independent from torch/tf in this file, using\n+        # Compute the softmax of all scores (we do it with numpy to stay independent from torch in this file, using\n         # the LogSumExp trick).\n         scores = np.array([pred.pop(\"score\") for pred in predictions])\n         exp_scores = np.exp(scores - np.max(scores))"
        },
        {
            "sha": "d6e69d3b83c506d8e69c0caf8022df854cb3ee4b",
            "filename": "setup.py",
            "status": "modified",
            "additions": 6,
            "deletions": 50,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -109,7 +109,6 @@\n     \"faiss-cpu\",\n     \"fastapi\",\n     \"filelock\",\n-    \"flax>=0.4.1,<=0.7.0\",\n     \"ftfy\",\n     \"fugashi>=1.0\",\n     \"GitPython<3.1.19\",\n@@ -118,13 +117,8 @@\n     \"huggingface-hub>=0.34.0,<1.0\",\n     \"importlib_metadata\",\n     \"ipadic>=1.0.0,<2.0\",\n-    \"jax>=0.4.1,<=0.4.13\",\n-    \"jaxlib>=0.4.1,<=0.4.13\",\n     \"jinja2>=3.1.0\",\n     \"kenlm\",\n-    # Keras pin - this is to make sure Keras 3 doesn't destroy us. Remove or change when we have proper support.\n-    \"keras>2.9,<2.16\",\n-    \"keras-nlp>=0.3.1,<0.14.0\",  # keras-nlp 0.14 doesn't support keras 2, see pin on keras.\n     \"kernels>=0.6.1,<=0.9\",\n     \"librosa\",\n     \"natten>=0.14.6,<0.15.0\",\n@@ -170,19 +164,13 @@\n     \"sagemaker>=2.31.0\",\n     \"schedulefree>=1.2.6\",\n     \"scikit-learn\",\n-    \"scipy<1.13.0\",  # SciPy >= 1.13.0 is not supported with the current jax pin (`jax>=0.4.1,<=0.4.13`)\n+    \"scipy\",\n     \"sentencepiece>=0.1.91,!=0.1.92\",\n     \"sigopt\",\n     \"starlette\",\n     \"sudachipy>=0.6.6\",\n     \"sudachidict_core>=20220729\",\n     \"tensorboard\",\n-    # TensorFlow pin. When changing this value, update examples/tensorflow/_tests_requirements.txt accordingly\n-    \"tensorflow-cpu>2.9,<2.16\",\n-    \"tensorflow>2.9,<2.16\",\n-    \"tensorflow-text<2.16\",\n-    \"tensorflow-probability<0.24\",\n-    \"tf2onnx\",\n     \"timeout-decorator\",\n     \"tiktoken\",\n     \"timm<=1.0.19,!=1.0.18\",\n@@ -273,32 +261,19 @@ def run(self):\n extras[\"ja\"] = deps_list(\"fugashi\", \"ipadic\", \"unidic_lite\", \"unidic\", \"sudachipy\", \"sudachidict_core\", \"rhoknp\")\n extras[\"sklearn\"] = deps_list(\"scikit-learn\")\n \n-extras[\"tf\"] = deps_list(\"tensorflow\", \"onnxconverter-common\", \"tf2onnx\", \"tensorflow-text\", \"keras-nlp\")\n-extras[\"tf-cpu\"] = deps_list(\n-    \"keras\",\n-    \"tensorflow-cpu\",\n-    \"onnxconverter-common\",\n-    \"tf2onnx\",\n-    \"tensorflow-text\",\n-    \"keras-nlp\",\n-    \"tensorflow-probability\",\n-)\n-\n extras[\"torch\"] = deps_list(\"torch\", \"accelerate\")\n extras[\"accelerate\"] = deps_list(\"accelerate\")\n extras[\"hf_xet\"] = deps_list(\"hf_xet\")\n \n if os.name == \"nt\":  # windows\n     extras[\"retrieval\"] = deps_list(\"datasets\")  # faiss is not supported on windows\n-    extras[\"flax\"] = []  # jax is not supported on windows\n else:\n     extras[\"retrieval\"] = deps_list(\"faiss-cpu\", \"datasets\")\n-    extras[\"flax\"] = deps_list(\"jax\", \"jaxlib\", \"flax\", \"optax\", \"scipy\")\n \n extras[\"tokenizers\"] = deps_list(\"tokenizers\")\n extras[\"ftfy\"] = deps_list(\"ftfy\")\n extras[\"onnxruntime\"] = deps_list(\"onnxruntime\", \"onnxruntime-tools\")\n-extras[\"onnx\"] = deps_list(\"onnxconverter-common\", \"tf2onnx\") + extras[\"onnxruntime\"]\n+extras[\"onnx\"] = deps_list(\"onnxconverter-common\") + extras[\"onnxruntime\"]\n extras[\"modelcreation\"] = deps_list(\"cookiecutter\")\n \n extras[\"sagemaker\"] = deps_list(\"sagemaker\")\n@@ -320,8 +295,6 @@ def run(self):\n # `pip install \".[speech]\"` is deprecated and `pip install \".[torch-speech]\"` should be used instead\n extras[\"speech\"] = deps_list(\"torchaudio\") + extras[\"audio\"]\n extras[\"torch-speech\"] = deps_list(\"torchaudio\") + extras[\"audio\"]\n-extras[\"tf-speech\"] = extras[\"audio\"]\n-extras[\"flax-speech\"] = extras[\"audio\"]\n extras[\"vision\"] = deps_list(\"Pillow\")\n extras[\"timm\"] = deps_list(\"timm\")\n extras[\"torch-vision\"] = deps_list(\"torchvision\") + extras[\"vision\"]\n@@ -372,9 +345,7 @@ def run(self):\n extras[\"quality\"] = deps_list(\"datasets\", \"ruff\", \"GitPython\", \"urllib3\", \"libcst\", \"rich\", \"pandas\")\n \n extras[\"all\"] = (\n-    extras[\"tf\"]\n-    + extras[\"torch\"]\n-    + extras[\"flax\"]\n+    extras[\"torch\"]\n     + extras[\"sentencepiece\"]\n     + extras[\"tokenizers\"]\n     + extras[\"torch-speech\"]\n@@ -409,18 +380,7 @@ def run(self):\n     + extras[\"onnxruntime\"]\n     + extras[\"num2words\"]\n )\n-extras[\"dev-tensorflow\"] = (\n-    extras[\"testing\"]\n-    + extras[\"tf\"]\n-    + extras[\"sentencepiece\"]\n-    + extras[\"tokenizers\"]\n-    + extras[\"vision\"]\n-    + extras[\"quality\"]\n-    + extras[\"sklearn\"]\n-    + extras[\"modelcreation\"]\n-    + extras[\"onnx\"]\n-    + extras[\"tf-speech\"]\n-)\n+\n extras[\"dev\"] = (\n     extras[\"all\"] + extras[\"testing\"] + extras[\"quality\"] + extras[\"ja\"] + extras[\"sklearn\"] + extras[\"modelcreation\"]\n )\n@@ -464,10 +424,10 @@ def run(self):\n     version=\"4.57.0.dev0\",  # expected format is one of x.y.z.dev0, or x.y.z.rc1 or x.y.z (no to dashes, yes to dots)\n     author=\"The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\",\n     author_email=\"transformers@huggingface.co\",\n-    description=\"State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\",\n+    description=\"Transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training.\",\n     long_description=open(\"README.md\", \"r\", encoding=\"utf-8\").read(),\n     long_description_content_type=\"text/markdown\",\n-    keywords=\"NLP vision speech deep learning transformer pytorch tensorflow jax BERT GPT-2 Wav2Vec2 ViT\",\n+    keywords=\"machine-learning nlp python pytorch transformer llm vlm deep-learning inference training model-hub pretrained-models llama gemma qwen\",\n     license=\"Apache 2.0 License\",\n     url=\"https://github.com/huggingface/transformers\",\n     package_dir={\"\": \"src\"},\n@@ -503,14 +463,10 @@ def run(self):\n )\n \n extras[\"tests_torch\"] = deps_list()\n-extras[\"tests_tf\"] = deps_list()\n-extras[\"tests_flax\"] = deps_list()\n extras[\"tests_hub\"] = deps_list()\n extras[\"tests_pipelines_torch\"] = deps_list()\n-extras[\"tests_pipelines_tf\"] = deps_list()\n extras[\"tests_onnx\"] = deps_list()\n extras[\"tests_examples_torch\"] = deps_list()\n-extras[\"tests_examples_tf\"] = deps_list()\n extras[\"tests_custom_tokenizers\"] = deps_list()\n extras[\"tests_exotic_models\"] = deps_list()\n extras[\"consistency\"] = deps_list()"
        },
        {
            "sha": "6c44c89239a23fc459df284eccdffcc6665e89df",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 162,
            "changes": 164,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -40,13 +40,9 @@\n # so that mypy, pylint or other static linters can recognize them,\n # given that they are not exported using `__all__` in this file.\n from .utils import is_bitsandbytes_available as is_bitsandbytes_available\n-from .utils import is_flax_available as is_flax_available\n-from .utils import is_keras_nlp_available as is_keras_nlp_available\n from .utils import is_scipy_available as is_scipy_available\n from .utils import is_sentencepiece_available as is_sentencepiece_available\n from .utils import is_speech_available as is_speech_available\n-from .utils import is_tensorflow_text_available as is_tensorflow_text_available\n-from .utils import is_tf_available as is_tf_available\n from .utils import is_timm_available as is_timm_available\n from .utils import is_tokenizers_available as is_tokenizers_available\n from .utils import is_torch_available as is_torch_available\n@@ -64,9 +60,7 @@\n     \"audio_utils\": [],\n     \"commands\": [],\n     \"configuration_utils\": [\"PretrainedConfig\"],\n-    \"convert_graph_to_onnx\": [],\n     \"convert_slow_tokenizers_checkpoints_to_fast\": [],\n-    \"convert_tf_hub_seq_to_seq_bert_to_pytorch\": [],\n     \"data\": [\n         \"DataProcessor\",\n         \"InputExample\",\n@@ -137,16 +131,6 @@\n     ],\n     \"loss\": [],\n     \"modelcard\": [\"ModelCard\"],\n-    # Losses\n-    \"modeling_tf_pytorch_utils\": [\n-        \"convert_tf_weight_name_to_pt_weight_name\",\n-        \"load_pytorch_checkpoint_in_tf2_model\",\n-        \"load_pytorch_model_in_tf2_model\",\n-        \"load_pytorch_weights_in_tf2_model\",\n-        \"load_tf2_checkpoint_in_pytorch_model\",\n-        \"load_tf2_model_in_pytorch_model\",\n-        \"load_tf2_weights_in_pytorch_model\",\n-    ],\n     # Models\n     \"onnx\": [],\n     \"pipelines\": [\n@@ -218,15 +202,12 @@\n     ],\n     \"training_args\": [\"TrainingArguments\"],\n     \"training_args_seq2seq\": [\"Seq2SeqTrainingArguments\"],\n-    \"training_args_tf\": [\"TFTrainingArguments\"],\n     \"utils\": [\n         \"CONFIG_NAME\",\n         \"MODEL_CARD_NAME\",\n         \"PYTORCH_PRETRAINED_BERT_CACHE\",\n         \"PYTORCH_TRANSFORMERS_CACHE\",\n         \"SPIECE_UNDERLINE\",\n-        \"TF2_WEIGHTS_NAME\",\n-        \"TF_WEIGHTS_NAME\",\n         \"TRANSFORMERS_CACHE\",\n         \"WEIGHTS_NAME\",\n         \"TensorType\",\n@@ -237,8 +218,6 @@\n         \"is_bitsandbytes_available\",\n         \"is_datasets_available\",\n         \"is_faiss_available\",\n-        \"is_flax_available\",\n-        \"is_keras_nlp_available\",\n         \"is_matplotlib_available\",\n         \"is_mlx_available\",\n         \"is_phonemizer_available\",\n@@ -251,8 +230,6 @@\n         \"is_sentencepiece_available\",\n         \"is_sklearn_available\",\n         \"is_speech_available\",\n-        \"is_tensorflow_text_available\",\n-        \"is_tf_available\",\n         \"is_timm_available\",\n         \"is_tokenizers_available\",\n         \"is_torch_available\",\n@@ -501,84 +478,6 @@\n     _import_structure[\"trainer_pt_utils\"] = [\"torch_distributed_zero_first\"]\n     _import_structure[\"trainer_seq2seq\"] = [\"Seq2SeqTrainer\"]\n \n-# TensorFlow-backed objects\n-try:\n-    if not is_tf_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    from .utils import dummy_tf_objects\n-\n-    _import_structure[\"utils.dummy_tf_objects\"] = [name for name in dir(dummy_tf_objects) if not name.startswith(\"_\")]\n-else:\n-    _import_structure[\"activations_tf\"] = []\n-    _import_structure[\"generation\"].extend(\n-        [\n-            \"TFForcedBOSTokenLogitsProcessor\",\n-            \"TFForcedEOSTokenLogitsProcessor\",\n-            \"TFForceTokensLogitsProcessor\",\n-            \"TFGenerationMixin\",\n-            \"TFLogitsProcessor\",\n-            \"TFLogitsProcessorList\",\n-            \"TFLogitsWarper\",\n-            \"TFMinLengthLogitsProcessor\",\n-            \"TFNoBadWordsLogitsProcessor\",\n-            \"TFNoRepeatNGramLogitsProcessor\",\n-            \"TFRepetitionPenaltyLogitsProcessor\",\n-            \"TFSuppressTokensAtBeginLogitsProcessor\",\n-            \"TFSuppressTokensLogitsProcessor\",\n-            \"TFTemperatureLogitsWarper\",\n-            \"TFTopKLogitsWarper\",\n-            \"TFTopPLogitsWarper\",\n-        ]\n-    )\n-    _import_structure[\"keras_callbacks\"] = [\"KerasMetricCallback\", \"PushToHubCallback\"]\n-    _import_structure[\"modeling_tf_outputs\"] = []\n-    _import_structure[\"modeling_tf_utils\"] = [\n-        \"TFPreTrainedModel\",\n-        \"TFSequenceSummary\",\n-        \"TFSharedEmbeddings\",\n-        \"shape_list\",\n-    ]\n-    _import_structure[\"optimization_tf\"] = [\n-        \"AdamWeightDecay\",\n-        \"GradientAccumulator\",\n-        \"WarmUp\",\n-        \"create_optimizer\",\n-    ]\n-    _import_structure[\"tf_utils\"] = []\n-\n-\n-# FLAX-backed objects\n-try:\n-    if not is_flax_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    from .utils import dummy_flax_objects\n-\n-    _import_structure[\"utils.dummy_flax_objects\"] = [\n-        name for name in dir(dummy_flax_objects) if not name.startswith(\"_\")\n-    ]\n-else:\n-    _import_structure[\"generation\"].extend(\n-        [\n-            \"FlaxForcedBOSTokenLogitsProcessor\",\n-            \"FlaxForcedEOSTokenLogitsProcessor\",\n-            \"FlaxForceTokensLogitsProcessor\",\n-            \"FlaxGenerationMixin\",\n-            \"FlaxLogitsProcessor\",\n-            \"FlaxLogitsProcessorList\",\n-            \"FlaxLogitsWarper\",\n-            \"FlaxMinLengthLogitsProcessor\",\n-            \"FlaxTemperatureLogitsWarper\",\n-            \"FlaxSuppressTokensAtBeginLogitsProcessor\",\n-            \"FlaxSuppressTokensLogitsProcessor\",\n-            \"FlaxTopKLogitsWarper\",\n-            \"FlaxTopPLogitsWarper\",\n-            \"FlaxWhisperTimeStampLogitsProcessor\",\n-        ]\n-    )\n-    _import_structure[\"modeling_flax_outputs\"] = []\n-    _import_structure[\"modeling_flax_utils\"] = [\"FlaxPreTrainedModel\"]\n \n # Direct imports for type-checking\n if TYPE_CHECKING:\n@@ -672,20 +571,6 @@\n     from .generation import EpsilonLogitsWarper as EpsilonLogitsWarper\n     from .generation import EtaLogitsWarper as EtaLogitsWarper\n     from .generation import ExponentialDecayLengthPenalty as ExponentialDecayLengthPenalty\n-    from .generation import FlaxForcedBOSTokenLogitsProcessor as FlaxForcedBOSTokenLogitsProcessor\n-    from .generation import FlaxForcedEOSTokenLogitsProcessor as FlaxForcedEOSTokenLogitsProcessor\n-    from .generation import FlaxForceTokensLogitsProcessor as FlaxForceTokensLogitsProcessor\n-    from .generation import FlaxGenerationMixin as FlaxGenerationMixin\n-    from .generation import FlaxLogitsProcessor as FlaxLogitsProcessor\n-    from .generation import FlaxLogitsProcessorList as FlaxLogitsProcessorList\n-    from .generation import FlaxLogitsWarper as FlaxLogitsWarper\n-    from .generation import FlaxMinLengthLogitsProcessor as FlaxMinLengthLogitsProcessor\n-    from .generation import FlaxSuppressTokensAtBeginLogitsProcessor as FlaxSuppressTokensAtBeginLogitsProcessor\n-    from .generation import FlaxSuppressTokensLogitsProcessor as FlaxSuppressTokensLogitsProcessor\n-    from .generation import FlaxTemperatureLogitsWarper as FlaxTemperatureLogitsWarper\n-    from .generation import FlaxTopKLogitsWarper as FlaxTopKLogitsWarper\n-    from .generation import FlaxTopPLogitsWarper as FlaxTopPLogitsWarper\n-    from .generation import FlaxWhisperTimeStampLogitsProcessor as FlaxWhisperTimeStampLogitsProcessor\n     from .generation import ForcedBOSTokenLogitsProcessor as ForcedBOSTokenLogitsProcessor\n     from .generation import ForcedEOSTokenLogitsProcessor as ForcedEOSTokenLogitsProcessor\n     from .generation import GenerationConfig as GenerationConfig\n@@ -716,22 +601,6 @@\n     from .generation import TemperatureLogitsWarper as TemperatureLogitsWarper\n     from .generation import TextIteratorStreamer as TextIteratorStreamer\n     from .generation import TextStreamer as TextStreamer\n-    from .generation import TFForcedBOSTokenLogitsProcessor as TFForcedBOSTokenLogitsProcessor\n-    from .generation import TFForcedEOSTokenLogitsProcessor as TFForcedEOSTokenLogitsProcessor\n-    from .generation import TFForceTokensLogitsProcessor as TFForceTokensLogitsProcessor\n-    from .generation import TFGenerationMixin as TFGenerationMixin\n-    from .generation import TFLogitsProcessor as TFLogitsProcessor\n-    from .generation import TFLogitsProcessorList as TFLogitsProcessorList\n-    from .generation import TFLogitsWarper as TFLogitsWarper\n-    from .generation import TFMinLengthLogitsProcessor as TFMinLengthLogitsProcessor\n-    from .generation import TFNoBadWordsLogitsProcessor as TFNoBadWordsLogitsProcessor\n-    from .generation import TFNoRepeatNGramLogitsProcessor as TFNoRepeatNGramLogitsProcessor\n-    from .generation import TFRepetitionPenaltyLogitsProcessor as TFRepetitionPenaltyLogitsProcessor\n-    from .generation import TFSuppressTokensAtBeginLogitsProcessor as TFSuppressTokensAtBeginLogitsProcessor\n-    from .generation import TFSuppressTokensLogitsProcessor as TFSuppressTokensLogitsProcessor\n-    from .generation import TFTemperatureLogitsWarper as TFTemperatureLogitsWarper\n-    from .generation import TFTopKLogitsWarper as TFTopKLogitsWarper\n-    from .generation import TFTopPLogitsWarper as TFTopPLogitsWarper\n     from .generation import TopKLogitsWarper as TopKLogitsWarper\n     from .generation import TopPLogitsWarper as TopPLogitsWarper\n     from .generation import TypicalLogitsWarper as TypicalLogitsWarper\n@@ -763,32 +632,14 @@\n     from .integrations import is_wandb_available as is_wandb_available\n     from .integrations.executorch import TorchExportableModuleWithStaticCache as TorchExportableModuleWithStaticCache\n     from .integrations.executorch import convert_and_export_with_cache as convert_and_export_with_cache\n-    from .keras_callbacks import KerasMetricCallback as KerasMetricCallback\n-    from .keras_callbacks import PushToHubCallback as PushToHubCallback\n     from .masking_utils import AttentionMaskInterface as AttentionMaskInterface\n     from .model_debugging_utils import model_addition_debugger_context as model_addition_debugger_context\n \n     # Model Cards\n     from .modelcard import ModelCard as ModelCard\n-    from .modeling_flax_utils import FlaxPreTrainedModel as FlaxPreTrainedModel\n     from .modeling_layers import GradientCheckpointingLayer as GradientCheckpointingLayer\n     from .modeling_rope_utils import ROPE_INIT_FUNCTIONS as ROPE_INIT_FUNCTIONS\n     from .modeling_rope_utils import dynamic_rope_update as dynamic_rope_update\n-\n-    # TF 2.0 <=> PyTorch conversion utilities\n-    from .modeling_tf_pytorch_utils import (\n-        convert_tf_weight_name_to_pt_weight_name as convert_tf_weight_name_to_pt_weight_name,\n-    )\n-    from .modeling_tf_pytorch_utils import load_pytorch_checkpoint_in_tf2_model as load_pytorch_checkpoint_in_tf2_model\n-    from .modeling_tf_pytorch_utils import load_pytorch_model_in_tf2_model as load_pytorch_model_in_tf2_model\n-    from .modeling_tf_pytorch_utils import load_pytorch_weights_in_tf2_model as load_pytorch_weights_in_tf2_model\n-    from .modeling_tf_pytorch_utils import load_tf2_checkpoint_in_pytorch_model as load_tf2_checkpoint_in_pytorch_model\n-    from .modeling_tf_pytorch_utils import load_tf2_model_in_pytorch_model as load_tf2_model_in_pytorch_model\n-    from .modeling_tf_pytorch_utils import load_tf2_weights_in_pytorch_model as load_tf2_weights_in_pytorch_model\n-    from .modeling_tf_utils import TFPreTrainedModel as TFPreTrainedModel\n-    from .modeling_tf_utils import TFSequenceSummary as TFSequenceSummary\n-    from .modeling_tf_utils import TFSharedEmbeddings as TFSharedEmbeddings\n-    from .modeling_tf_utils import shape_list as shape_list\n     from .modeling_utils import AttentionInterface as AttentionInterface\n     from .modeling_utils import PreTrainedModel as PreTrainedModel\n     from .models import *\n@@ -815,12 +666,6 @@\n     from .optimization import get_scheduler as get_scheduler\n     from .optimization import get_wsd_schedule as get_wsd_schedule\n \n-    # Optimization\n-    from .optimization_tf import AdamWeightDecay as AdamWeightDecay\n-    from .optimization_tf import GradientAccumulator as GradientAccumulator\n-    from .optimization_tf import WarmUp as WarmUp\n-    from .optimization_tf import create_optimizer as create_optimizer\n-\n     # Pipelines\n     from .pipelines import AudioClassificationPipeline as AudioClassificationPipeline\n     from .pipelines import AutomaticSpeechRecognitionPipeline as AutomaticSpeechRecognitionPipeline\n@@ -894,16 +739,13 @@\n     from .trainer_utils import set_seed as set_seed\n     from .training_args import TrainingArguments as TrainingArguments\n     from .training_args_seq2seq import Seq2SeqTrainingArguments as Seq2SeqTrainingArguments\n-    from .training_args_tf import TFTrainingArguments as TFTrainingArguments\n \n     # Files and general utilities\n     from .utils import CONFIG_NAME as CONFIG_NAME\n     from .utils import MODEL_CARD_NAME as MODEL_CARD_NAME\n     from .utils import PYTORCH_PRETRAINED_BERT_CACHE as PYTORCH_PRETRAINED_BERT_CACHE\n     from .utils import PYTORCH_TRANSFORMERS_CACHE as PYTORCH_TRANSFORMERS_CACHE\n     from .utils import SPIECE_UNDERLINE as SPIECE_UNDERLINE\n-    from .utils import TF2_WEIGHTS_NAME as TF2_WEIGHTS_NAME\n-    from .utils import TF_WEIGHTS_NAME as TF_WEIGHTS_NAME\n     from .utils import TRANSFORMERS_CACHE as TRANSFORMERS_CACHE\n     from .utils import WEIGHTS_NAME as WEIGHTS_NAME\n     from .utils import TensorType as TensorType\n@@ -968,9 +810,7 @@\n     )\n \n \n-if not is_tf_available() and not is_torch_available() and not is_flax_available():\n+if not is_torch_available():\n     logger.warning_advice(\n-        \"None of PyTorch, TensorFlow >= 2.0, or Flax have been found. \"\n-        \"Models won't be available and only tokenizers, configuration \"\n-        \"and file/data utilities can be used.\"\n+        \"PyTorch was not found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\"\n     )"
        },
        {
            "sha": "8dccf6c4f46b8fe1f98d7e57bd8611f660ed19f4",
            "filename": "src/transformers/activations_tf.py",
            "status": "removed",
            "additions": 0,
            "deletions": 147,
            "changes": 147,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Factivations_tf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Factivations_tf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Factivations_tf.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,147 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import math\n-\n-import tensorflow as tf\n-from packaging.version import parse\n-\n-\n-try:\n-    import tf_keras as keras\n-except (ModuleNotFoundError, ImportError):\n-    import keras\n-\n-    if parse(keras.__version__).major > 2:\n-        raise ValueError(\n-            \"Your currently installed version of Keras is Keras 3, but this is not yet supported in \"\n-            \"Transformers. Please install the backwards-compatible tf-keras package with \"\n-            \"`pip install tf-keras`.\"\n-        )\n-\n-\n-def _gelu(x):\n-    \"\"\"\n-    Gaussian Error Linear Unit. Original Implementation of the gelu activation function in Google Bert repo when\n-    initially created. For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n-    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3)))) Also see\n-    https://huggingface.co/papers/1606.08415\n-    \"\"\"\n-    x = tf.convert_to_tensor(x)\n-    cdf = 0.5 * (1.0 + tf.math.erf(x / tf.cast(tf.sqrt(2.0), x.dtype)))\n-\n-    return x * cdf\n-\n-\n-def _gelu_new(x):\n-    \"\"\"\n-    Gaussian Error Linear Unit. This is a smoother version of the GELU. Original paper: https://huggingface.co/papers/1606.0841\n-\n-    Args:\n-        x: float Tensor to perform activation\n-\n-    Returns:\n-        `x` with the GELU activation applied.\n-    \"\"\"\n-    x = tf.convert_to_tensor(x)\n-    pi = tf.cast(math.pi, x.dtype)\n-    coeff = tf.cast(0.044715, x.dtype)\n-    cdf = 0.5 * (1.0 + tf.tanh(tf.sqrt(2.0 / pi) * (x + coeff * tf.pow(x, 3))))\n-\n-    return x * cdf\n-\n-\n-def mish(x):\n-    x = tf.convert_to_tensor(x)\n-\n-    return x * tf.tanh(tf.math.softplus(x))\n-\n-\n-def gelu_fast(x):\n-    x = tf.convert_to_tensor(x)\n-    coeff1 = tf.cast(0.044715, x.dtype)\n-    coeff2 = tf.cast(0.7978845608, x.dtype)\n-\n-    return 0.5 * x * (1.0 + tf.tanh(x * coeff2 * (1.0 + coeff1 * x * x)))\n-\n-\n-def quick_gelu(x):\n-    x = tf.convert_to_tensor(x)\n-    coeff = tf.cast(1.702, x.dtype)\n-    return x * tf.math.sigmoid(coeff * x)\n-\n-\n-def gelu_10(x):\n-    \"\"\"\n-    Clip the range of possible GeLU outputs between [-10, 10]. This is especially useful for quantization purpose, as\n-    it allows mapping 2 negatives values in the GeLU spectrum. For more information on this trick, please refer to\n-    https://huggingface.co/papers/2004.09602\n-\n-    Gaussian Error Linear Unit. Original Implementation of the gelu activation function in Google Bert repo when\n-    initially created. For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n-    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3)))) Also see\n-    https://huggingface.co/papers/1606.08415 :param x: :return:\n-    \"\"\"\n-    return tf.clip_by_value(_gelu(x), -10, 10)\n-\n-\n-def glu(x, axis=-1):\n-    \"\"\"\n-    Gated Linear Unit. Implementation as defined in the original paper (see https://huggingface.co/papers/1612.08083), where\n-    the input `x` is split in two halves across a dimension (`axis`), A and B, returning A * sigmoid(B).\n-\n-    Args:\n-        `x`: float Tensor to perform activation\n-        `axis`: dimension across which `x` be split in half\n-\n-    Returns:\n-        `x` with the GLU activation applied (with its size halved across the dimension `axis`).\n-    \"\"\"\n-    a, b = tf.split(x, 2, axis=axis)\n-    return a * tf.math.sigmoid(b)\n-\n-\n-if parse(tf.version.VERSION) >= parse(\"2.4\"):\n-\n-    def approximate_gelu_wrap(x):\n-        return keras.activations.gelu(x, approximate=True)\n-\n-    gelu = keras.activations.gelu\n-    gelu_new = approximate_gelu_wrap\n-else:\n-    gelu = _gelu\n-    gelu_new = _gelu_new\n-\n-\n-ACT2FN = {\n-    \"gelu\": gelu,\n-    \"gelu_10\": gelu_10,\n-    \"gelu_fast\": gelu_fast,\n-    \"gelu_new\": gelu_new,\n-    \"glu\": glu,\n-    \"mish\": mish,\n-    \"quick_gelu\": quick_gelu,\n-    \"relu\": keras.activations.relu,\n-    \"sigmoid\": keras.activations.sigmoid,\n-    \"silu\": keras.activations.swish,\n-    \"swish\": keras.activations.swish,\n-    \"tanh\": keras.activations.tanh,\n-}\n-\n-\n-def get_tf_activation(activation_string):\n-    if activation_string in ACT2FN:\n-        return ACT2FN[activation_string]\n-    else:\n-        raise KeyError(f\"function {activation_string} not found in ACT2FN mapping {list(ACT2FN.keys())}\")"
        },
        {
            "sha": "220d1d44b1aa281e3b3af9ad584ad3ad49b6beb0",
            "filename": "src/transformers/commands/convert.py",
            "status": "removed",
            "additions": 0,
            "deletions": 165,
            "changes": 165,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fcommands%2Fconvert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fcommands%2Fconvert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fconvert.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,165 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-from argparse import ArgumentParser, Namespace\n-\n-from ..utils import logging\n-from . import BaseTransformersCLICommand\n-\n-\n-def convert_command_factory(args: Namespace):\n-    \"\"\"\n-    Factory function used to convert a model TF 1.0 checkpoint in a PyTorch checkpoint.\n-\n-    Returns: ServeCommand\n-    \"\"\"\n-    return ConvertCommand(\n-        args.model_type, args.tf_checkpoint, args.pytorch_dump_output, args.config, args.finetuning_task_name\n-    )\n-\n-\n-IMPORT_ERROR_MESSAGE = \"\"\"\n-transformers can only be used from the commandline to convert TensorFlow models in PyTorch, In that case, it requires\n-TensorFlow to be installed. Please see https://www.tensorflow.org/install/ for installation instructions.\n-\"\"\"\n-\n-\n-class ConvertCommand(BaseTransformersCLICommand):\n-    @staticmethod\n-    def register_subcommand(parser: ArgumentParser):\n-        \"\"\"\n-        Register this command to argparse so it's available for the transformer-cli\n-\n-        Args:\n-            parser: Root parser to register command-specific arguments\n-        \"\"\"\n-        train_parser = parser.add_parser(\n-            \"convert\",\n-            help=\"CLI tool to run convert model from original author checkpoints to Transformers PyTorch checkpoints.\",\n-        )\n-        train_parser.add_argument(\"--model_type\", type=str, required=True, help=\"Model's type.\")\n-        train_parser.add_argument(\n-            \"--tf_checkpoint\", type=str, required=True, help=\"TensorFlow checkpoint path or folder.\"\n-        )\n-        train_parser.add_argument(\n-            \"--pytorch_dump_output\", type=str, required=True, help=\"Path to the PyTorch saved model output.\"\n-        )\n-        train_parser.add_argument(\"--config\", type=str, default=\"\", help=\"Configuration file path or folder.\")\n-        train_parser.add_argument(\n-            \"--finetuning_task_name\",\n-            type=str,\n-            default=None,\n-            help=\"Optional fine-tuning task name if the TF model was a finetuned model.\",\n-        )\n-        train_parser.set_defaults(func=convert_command_factory)\n-\n-    def __init__(\n-        self,\n-        model_type: str,\n-        tf_checkpoint: str,\n-        pytorch_dump_output: str,\n-        config: str,\n-        finetuning_task_name: str,\n-        *args,\n-    ):\n-        self._logger = logging.get_logger(\"transformers/converting\")\n-\n-        self._logger.info(f\"Loading model {model_type}\")\n-        self._model_type = model_type\n-        self._tf_checkpoint = tf_checkpoint\n-        self._pytorch_dump_output = pytorch_dump_output\n-        self._config = config\n-        self._finetuning_task_name = finetuning_task_name\n-\n-    def run(self):\n-        if self._model_type == \"albert\":\n-            try:\n-                from ..models.albert.convert_albert_original_tf_checkpoint_to_pytorch import (\n-                    convert_tf_checkpoint_to_pytorch,\n-                )\n-            except ImportError:\n-                raise ImportError(IMPORT_ERROR_MESSAGE)\n-\n-            convert_tf_checkpoint_to_pytorch(self._tf_checkpoint, self._config, self._pytorch_dump_output)\n-        elif self._model_type == \"bert\":\n-            try:\n-                from ..models.bert.convert_bert_original_tf_checkpoint_to_pytorch import (\n-                    convert_tf_checkpoint_to_pytorch,\n-                )\n-            except ImportError:\n-                raise ImportError(IMPORT_ERROR_MESSAGE)\n-\n-            convert_tf_checkpoint_to_pytorch(self._tf_checkpoint, self._config, self._pytorch_dump_output)\n-        elif self._model_type == \"funnel\":\n-            try:\n-                from ..models.funnel.convert_funnel_original_tf_checkpoint_to_pytorch import (\n-                    convert_tf_checkpoint_to_pytorch,\n-                )\n-            except ImportError:\n-                raise ImportError(IMPORT_ERROR_MESSAGE)\n-\n-            convert_tf_checkpoint_to_pytorch(self._tf_checkpoint, self._config, self._pytorch_dump_output)\n-        elif self._model_type == \"t5\":\n-            try:\n-                from ..models.t5.convert_t5_original_tf_checkpoint_to_pytorch import convert_tf_checkpoint_to_pytorch\n-            except ImportError:\n-                raise ImportError(IMPORT_ERROR_MESSAGE)\n-\n-            convert_tf_checkpoint_to_pytorch(self._tf_checkpoint, self._config, self._pytorch_dump_output)\n-        elif self._model_type == \"gpt\":\n-            from ..models.openai.convert_openai_original_tf_checkpoint_to_pytorch import (\n-                convert_openai_checkpoint_to_pytorch,\n-            )\n-\n-            convert_openai_checkpoint_to_pytorch(self._tf_checkpoint, self._config, self._pytorch_dump_output)\n-        elif self._model_type == \"gpt2\":\n-            try:\n-                from ..models.gpt2.convert_gpt2_original_tf_checkpoint_to_pytorch import (\n-                    convert_gpt2_checkpoint_to_pytorch,\n-                )\n-            except ImportError:\n-                raise ImportError(IMPORT_ERROR_MESSAGE)\n-\n-            convert_gpt2_checkpoint_to_pytorch(self._tf_checkpoint, self._config, self._pytorch_dump_output)\n-        elif self._model_type == \"xlnet\":\n-            try:\n-                from ..models.xlnet.convert_xlnet_original_tf_checkpoint_to_pytorch import (\n-                    convert_xlnet_checkpoint_to_pytorch,\n-                )\n-            except ImportError:\n-                raise ImportError(IMPORT_ERROR_MESSAGE)\n-\n-            convert_xlnet_checkpoint_to_pytorch(\n-                self._tf_checkpoint, self._config, self._pytorch_dump_output, self._finetuning_task_name\n-            )\n-        elif self._model_type == \"xlm\":\n-            from ..models.xlm.convert_xlm_original_pytorch_checkpoint_to_pytorch import (\n-                convert_xlm_checkpoint_to_pytorch,\n-            )\n-\n-            convert_xlm_checkpoint_to_pytorch(self._tf_checkpoint, self._pytorch_dump_output)\n-        elif self._model_type == \"lxmert\":\n-            from ..models.lxmert.convert_lxmert_original_tf_checkpoint_to_pytorch import (\n-                convert_lxmert_checkpoint_to_pytorch,\n-            )\n-\n-            convert_lxmert_checkpoint_to_pytorch(self._tf_checkpoint, self._pytorch_dump_output)\n-        elif self._model_type == \"rembert\":\n-            from ..models.rembert.convert_rembert_tf_checkpoint_to_pytorch import (\n-                convert_rembert_tf_checkpoint_to_pytorch,\n-            )\n-\n-            convert_rembert_tf_checkpoint_to_pytorch(self._tf_checkpoint, self._config, self._pytorch_dump_output)\n-        else:\n-            raise ValueError(\"--model_type should be selected in the list [bert, gpt, gpt2, t5, xlnet, xlm, lxmert]\")"
        },
        {
            "sha": "9ef31c71a0d11702b54df981fb150cd855788368",
            "filename": "src/transformers/commands/env.py",
            "status": "modified",
            "additions": 0,
            "deletions": 33,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fcommands%2Fenv.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fcommands%2Fenv.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fenv.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -26,9 +26,7 @@\n from ..integrations.deepspeed import is_deepspeed_available\n from ..utils import (\n     is_accelerate_available,\n-    is_flax_available,\n     is_safetensors_available,\n-    is_tf_available,\n     is_torch_available,\n     is_torch_hpu_available,\n     is_torch_npu_available,\n@@ -109,40 +107,13 @@ def run(self):\n             elif pt_hpu_available:\n                 pt_accelerator = \"HPU\"\n \n-        tf_version = \"not installed\"\n-        tf_cuda_available = \"NA\"\n-        if is_tf_available():\n-            import tensorflow as tf\n-\n-            tf_version = tf.__version__\n-            try:\n-                # deprecated in v2.1\n-                tf_cuda_available = tf.test.is_gpu_available()\n-            except AttributeError:\n-                # returns list of devices, convert to bool\n-                tf_cuda_available = bool(tf.config.list_physical_devices(\"GPU\"))\n-\n         deepspeed_version = \"not installed\"\n         if is_deepspeed_available():\n             # Redirect command line output to silence deepspeed import output.\n             with contextlib.redirect_stdout(io.StringIO()):\n                 import deepspeed\n             deepspeed_version = deepspeed.__version__\n \n-        flax_version = \"not installed\"\n-        jax_version = \"not installed\"\n-        jaxlib_version = \"not installed\"\n-        jax_backend = \"NA\"\n-        if is_flax_available():\n-            import flax\n-            import jax\n-            import jaxlib\n-\n-            flax_version = flax.__version__\n-            jax_version = jax.__version__\n-            jaxlib_version = jaxlib.__version__\n-            jax_backend = jax.lib.xla_bridge.get_backend().platform\n-\n         info = {\n             \"`transformers` version\": version,\n             \"Platform\": platform.platform(),\n@@ -153,10 +124,6 @@ def run(self):\n             \"Accelerate config\": f\"{accelerate_config_str}\",\n             \"DeepSpeed version\": f\"{deepspeed_version}\",\n             \"PyTorch version (accelerator?)\": f\"{pt_version} ({pt_accelerator})\",\n-            \"Tensorflow version (GPU?)\": f\"{tf_version} ({tf_cuda_available})\",\n-            \"Flax version (CPU?/GPU?/TPU?)\": f\"{flax_version} ({jax_backend})\",\n-            \"Jax version\": f\"{jax_version}\",\n-            \"JaxLib version\": f\"{jaxlib_version}\",\n             \"Using distributed or parallel set-up in script?\": \"<fill in>\",\n         }\n         if is_torch_available():"
        },
        {
            "sha": "06e95443df24c87f86b4e2dc96d811dee515e267",
            "filename": "src/transformers/commands/train.py",
            "status": "removed",
            "additions": 0,
            "deletions": 158,
            "changes": 158,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fcommands%2Ftrain.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fcommands%2Ftrain.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Ftrain.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,158 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import os\n-from argparse import ArgumentParser, Namespace\n-\n-from ..data import SingleSentenceClassificationProcessor as Processor\n-from ..pipelines import TextClassificationPipeline\n-from ..utils import is_tf_available, is_torch_available, logging\n-from . import BaseTransformersCLICommand\n-\n-\n-if not is_tf_available() and not is_torch_available():\n-    raise RuntimeError(\"At least one of PyTorch or TensorFlow 2.0+ should be installed to use CLI training\")\n-\n-# TF training parameters\n-USE_XLA = False\n-USE_AMP = False\n-\n-\n-def train_command_factory(args: Namespace):\n-    \"\"\"\n-    Factory function used to instantiate training command from provided command line arguments.\n-\n-    Returns: TrainCommand\n-    \"\"\"\n-    return TrainCommand(args)\n-\n-\n-class TrainCommand(BaseTransformersCLICommand):\n-    @staticmethod\n-    def register_subcommand(parser: ArgumentParser):\n-        \"\"\"\n-        Register this command to argparse so it's available for the transformer-cli\n-\n-        Args:\n-            parser: Root parser to register command-specific arguments\n-        \"\"\"\n-        train_parser = parser.add_parser(\"train\", help=\"CLI tool to train a model on a task.\")\n-\n-        train_parser.add_argument(\n-            \"--train_data\",\n-            type=str,\n-            required=True,\n-            help=\"path to train (and optionally evaluation) dataset as a csv with tab separated labels and sentences.\",\n-        )\n-        train_parser.add_argument(\n-            \"--column_label\", type=int, default=0, help=\"Column of the dataset csv file with example labels.\"\n-        )\n-        train_parser.add_argument(\n-            \"--column_text\", type=int, default=1, help=\"Column of the dataset csv file with example texts.\"\n-        )\n-        train_parser.add_argument(\n-            \"--column_id\", type=int, default=2, help=\"Column of the dataset csv file with example ids.\"\n-        )\n-        train_parser.add_argument(\n-            \"--skip_first_row\", action=\"store_true\", help=\"Skip the first row of the csv file (headers).\"\n-        )\n-\n-        train_parser.add_argument(\"--validation_data\", type=str, default=\"\", help=\"path to validation dataset.\")\n-        train_parser.add_argument(\n-            \"--validation_split\",\n-            type=float,\n-            default=0.1,\n-            help=\"if validation dataset is not provided, fraction of train dataset to use as validation dataset.\",\n-        )\n-\n-        train_parser.add_argument(\"--output\", type=str, default=\"./\", help=\"path to saved the trained model.\")\n-\n-        train_parser.add_argument(\n-            \"--task\", type=str, default=\"text_classification\", help=\"Task to train the model on.\"\n-        )\n-        train_parser.add_argument(\n-            \"--model\", type=str, default=\"google-bert/bert-base-uncased\", help=\"Model's name or path to stored model.\"\n-        )\n-        train_parser.add_argument(\"--train_batch_size\", type=int, default=32, help=\"Batch size for training.\")\n-        train_parser.add_argument(\"--valid_batch_size\", type=int, default=64, help=\"Batch size for validation.\")\n-        train_parser.add_argument(\"--learning_rate\", type=float, default=3e-5, help=\"Learning rate.\")\n-        train_parser.add_argument(\"--adam_epsilon\", type=float, default=1e-08, help=\"Epsilon for Adam optimizer.\")\n-        train_parser.set_defaults(func=train_command_factory)\n-\n-    def __init__(self, args: Namespace):\n-        self.logger = logging.get_logger(\"transformers/training\")\n-\n-        self.framework = \"tf\" if is_tf_available() else \"torch\"\n-\n-        os.makedirs(args.output, exist_ok=True)\n-        self.output = args.output\n-\n-        self.column_label = args.column_label\n-        self.column_text = args.column_text\n-        self.column_id = args.column_id\n-\n-        self.logger.info(f\"Loading {args.task} pipeline for {args.model}\")\n-        if args.task == \"text_classification\":\n-            self.pipeline = TextClassificationPipeline.from_pretrained(args.model)\n-        elif args.task == \"token_classification\":\n-            raise NotImplementedError\n-        elif args.task == \"question_answering\":\n-            raise NotImplementedError\n-\n-        self.logger.info(f\"Loading dataset from {args.train_data}\")\n-        self.train_dataset = Processor.create_from_csv(\n-            args.train_data,\n-            column_label=args.column_label,\n-            column_text=args.column_text,\n-            column_id=args.column_id,\n-            skip_first_row=args.skip_first_row,\n-        )\n-        self.valid_dataset = None\n-        if args.validation_data:\n-            self.logger.info(f\"Loading validation dataset from {args.validation_data}\")\n-            self.valid_dataset = Processor.create_from_csv(\n-                args.validation_data,\n-                column_label=args.column_label,\n-                column_text=args.column_text,\n-                column_id=args.column_id,\n-                skip_first_row=args.skip_first_row,\n-            )\n-\n-        self.validation_split = args.validation_split\n-        self.train_batch_size = args.train_batch_size\n-        self.valid_batch_size = args.valid_batch_size\n-        self.learning_rate = args.learning_rate\n-        self.adam_epsilon = args.adam_epsilon\n-\n-    def run(self):\n-        if self.framework == \"tf\":\n-            return self.run_tf()\n-        return self.run_torch()\n-\n-    def run_torch(self):\n-        raise NotImplementedError\n-\n-    def run_tf(self):\n-        self.pipeline.fit(\n-            self.train_dataset,\n-            validation_data=self.valid_dataset,\n-            validation_split=self.validation_split,\n-            learning_rate=self.learning_rate,\n-            adam_epsilon=self.adam_epsilon,\n-            train_batch_size=self.train_batch_size,\n-            valid_batch_size=self.valid_batch_size,\n-        )\n-\n-        # Save trained pipeline\n-        self.pipeline.save_pretrained(self.output)"
        },
        {
            "sha": "1a283a1c512c4815eef79e8ae2538fd8c358976b",
            "filename": "src/transformers/commands/transformers_cli.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fcommands%2Ftransformers_cli.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fcommands%2Ftransformers_cli.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Ftransformers_cli.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -18,7 +18,6 @@\n from transformers.commands.add_fast_image_processor import AddFastImageProcessorCommand\n from transformers.commands.add_new_model_like import AddNewModelLikeCommand\n from transformers.commands.chat import ChatCommand\n-from transformers.commands.convert import ConvertCommand\n from transformers.commands.download import DownloadCommand\n from transformers.commands.env import EnvironmentCommand\n from transformers.commands.run import RunCommand\n@@ -39,7 +38,6 @@ def main():\n \n     # Register commands\n     ChatCommand.register_subcommand(commands_parser)\n-    ConvertCommand.register_subcommand(commands_parser)\n     DownloadCommand.register_subcommand(commands_parser)\n     EnvironmentCommand.register_subcommand(commands_parser)\n     RunCommand.register_subcommand(commands_parser)"
        },
        {
            "sha": "b9423a8bbf5908dd5d576ae547fedd8085e07cf6",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -100,9 +100,8 @@ class PretrainedConfig(PushToHubMixin):\n \n     Arg:\n         name_or_path (`str`, *optional*, defaults to `\"\"`):\n-            Store the string that was passed to [`PreTrainedModel.from_pretrained`] or\n-            [`TFPreTrainedModel.from_pretrained`] as `pretrained_model_name_or_path` if the configuration was created\n-            with such a method.\n+            Store the string that was passed to [`PreTrainedModel.from_pretrained`] as `pretrained_model_name_or_path`\n+            if the configuration was created with such a method.\n         output_hidden_states (`bool`, *optional*, defaults to `False`):\n             Whether or not the model should return all hidden-states.\n         output_attentions (`bool`, *optional*, defaults to `False`):\n@@ -140,8 +139,7 @@ class PretrainedConfig(PushToHubMixin):\n         architectures (`list[str]`, *optional*):\n             Model architectures that can be used with the model pretrained weights.\n         finetuning_task (`str`, *optional*):\n-            Name of the task used to fine-tune the model. This can be used when converting from an original (TensorFlow\n-            or PyTorch) checkpoint.\n+            Name of the task used to fine-tune the model.\n         id2label (`dict[int, str]`, *optional*):\n             A map from index (for instance prediction index, or target index) to label.\n         label2id (`dict[str, int]`, *optional*):\n@@ -346,10 +344,6 @@ def __init__(\n                 logger.error(f\"Can't set {key} with value {value} for {self}\")\n                 raise err\n \n-        # TODO: remove later, deprecated arguments for TF models\n-        self.tf_legacy_loss = kwargs.pop(\"tf_legacy_loss\", False)\n-        self.use_bfloat16 = kwargs.pop(\"use_bfloat16\", False)\n-\n     def _create_id_label_maps(self, num_labels: int):\n         self.id2label = {i: f\"LABEL_{i}\" for i in range(num_labels)}\n         self.label2id = dict(zip(self.id2label.values(), self.id2label.keys()))"
        },
        {
            "sha": "922ece8c0f45203d92ad77bf7a8a439ea759c62e",
            "filename": "src/transformers/convert_graph_to_onnx.py",
            "status": "removed",
            "additions": 0,
            "deletions": 551,
            "changes": 551,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fconvert_graph_to_onnx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fconvert_graph_to_onnx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconvert_graph_to_onnx.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,551 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import warnings\n-from argparse import ArgumentParser\n-from os import listdir, makedirs\n-from pathlib import Path\n-from typing import Optional\n-\n-from packaging.version import Version, parse\n-\n-from transformers.pipelines import Pipeline, pipeline\n-from transformers.tokenization_utils import BatchEncoding\n-from transformers.utils import ModelOutput, is_tf_available, is_torch_available\n-\n-\n-# This is the minimal required version to\n-# support some ONNX Runtime features\n-ORT_QUANTIZE_MINIMUM_VERSION = parse(\"1.4.0\")\n-\n-\n-SUPPORTED_PIPELINES = [\n-    \"feature-extraction\",\n-    \"ner\",\n-    \"sentiment-analysis\",\n-    \"fill-mask\",\n-    \"question-answering\",\n-    \"text-generation\",\n-    \"translation_en_to_fr\",\n-    \"translation_en_to_de\",\n-    \"translation_en_to_ro\",\n-]\n-\n-\n-class OnnxConverterArgumentParser(ArgumentParser):\n-    \"\"\"\n-    Wraps all the script arguments supported to export transformers models to ONNX IR\n-    \"\"\"\n-\n-    def __init__(self):\n-        super().__init__(\"ONNX Converter\")\n-\n-        self.add_argument(\n-            \"--pipeline\",\n-            type=str,\n-            choices=SUPPORTED_PIPELINES,\n-            default=\"feature-extraction\",\n-        )\n-        self.add_argument(\n-            \"--model\",\n-            type=str,\n-            required=True,\n-            help=\"Model's id or path (ex: google-bert/bert-base-cased)\",\n-        )\n-        self.add_argument(\"--tokenizer\", type=str, help=\"Tokenizer's id or path (ex: google-bert/bert-base-cased)\")\n-        self.add_argument(\n-            \"--framework\",\n-            type=str,\n-            choices=[\"pt\", \"tf\"],\n-            help=\"Framework for loading the model\",\n-        )\n-        self.add_argument(\"--opset\", type=int, default=11, help=\"ONNX opset to use\")\n-        self.add_argument(\n-            \"--check-loading\",\n-            action=\"store_true\",\n-            help=\"Check ONNX is able to load the model\",\n-        )\n-        self.add_argument(\n-            \"--use-external-format\",\n-            action=\"store_true\",\n-            help=\"Allow exporting model >= than 2Gb\",\n-        )\n-        self.add_argument(\n-            \"--quantize\",\n-            action=\"store_true\",\n-            help=\"Quantize the neural network to be run with int8\",\n-        )\n-        self.add_argument(\"output\")\n-\n-\n-def generate_identified_filename(filename: Path, identifier: str) -> Path:\n-    \"\"\"\n-    Append a string-identifier at the end (before the extension, if any) to the provided filepath\n-\n-    Args:\n-        filename: pathlib.Path The actual path object we would like to add an identifier suffix\n-        identifier: The suffix to add\n-\n-    Returns: String with concatenated identifier at the end of the filename\n-    \"\"\"\n-    return filename.parent.joinpath(filename.stem + identifier).with_suffix(filename.suffix)\n-\n-\n-def check_onnxruntime_requirements(minimum_version: Version):\n-    \"\"\"\n-    Check onnxruntime is installed and if the installed version match is recent enough\n-\n-    Raises:\n-        ImportError: If onnxruntime is not installed or too old version is found\n-    \"\"\"\n-    try:\n-        import onnxruntime\n-\n-        # Parse the version of the installed onnxruntime\n-        ort_version = parse(onnxruntime.__version__)\n-\n-        # We require 1.4.0 minimum\n-        if ort_version < ORT_QUANTIZE_MINIMUM_VERSION:\n-            raise ImportError(\n-                f\"We found an older version of onnxruntime ({onnxruntime.__version__}) \"\n-                f\"but we require onnxruntime to be >= {minimum_version} to enable all the conversions options.\\n\"\n-                \"Please update onnxruntime by running `pip install --upgrade onnxruntime`\"\n-            )\n-\n-    except ImportError:\n-        raise ImportError(\n-            \"onnxruntime doesn't seem to be currently installed. \"\n-            \"Please install the onnxruntime by running `pip install onnxruntime`\"\n-            \" and relaunch the conversion.\"\n-        )\n-\n-\n-def ensure_valid_input(model, tokens, input_names):\n-    \"\"\"\n-    Ensure inputs are presented in the correct order, without any Non\n-\n-    Args:\n-        model: The model used to forward the input data\n-        tokens: BatchEncoding holding the input data\n-        input_names: The name of the inputs\n-\n-    Returns: Tuple\n-\n-    \"\"\"\n-    print(\"Ensuring inputs are in correct order\")\n-\n-    model_args_name = model.forward.__code__.co_varnames\n-    model_args, ordered_input_names = [], []\n-    for arg_name in model_args_name[1:]:  # start at index 1 to skip \"self\" argument\n-        if arg_name in input_names:\n-            ordered_input_names.append(arg_name)\n-            model_args.append(tokens[arg_name])\n-        else:\n-            print(f\"{arg_name} is not present in the generated input list.\")\n-            break\n-\n-    print(f\"Generated inputs order: {ordered_input_names}\")\n-    return ordered_input_names, tuple(model_args)\n-\n-\n-def infer_shapes(nlp: Pipeline, framework: str) -> tuple[list[str], list[str], dict, BatchEncoding]:\n-    \"\"\"\n-    Attempt to infer the static vs dynamic axes for each input and output tensors for a specific model\n-\n-    Args:\n-        nlp: The pipeline object holding the model to be exported\n-        framework: The framework identifier to dispatch to the correct inference scheme (pt/tf)\n-\n-    Returns:\n-\n-        - List of the inferred input variable names\n-        - List of the inferred output variable names\n-        - Dictionary with input/output variables names as key and shape tensor as value\n-        - a BatchEncoding reference which was used to infer all the above information\n-    \"\"\"\n-\n-    def build_shape_dict(name: str, tensor, is_input: bool, seq_len: int):\n-        if isinstance(tensor, (tuple, list)):\n-            return [build_shape_dict(name, t, is_input, seq_len) for t in tensor]\n-\n-        else:\n-            # Let's assume batch is the first axis with only 1 element (~~ might not be always true ...)\n-            axes = {[axis for axis, numel in enumerate(tensor.shape) if numel == 1][0]: \"batch\"}\n-            if is_input:\n-                if len(tensor.shape) == 2:\n-                    axes[1] = \"sequence\"\n-                else:\n-                    raise ValueError(f\"Unable to infer tensor axes ({len(tensor.shape)})\")\n-            else:\n-                seq_axes = [dim for dim, shape in enumerate(tensor.shape) if shape == seq_len]\n-                axes.update(dict.fromkeys(seq_axes, \"sequence\"))\n-\n-        print(f\"Found {'input' if is_input else 'output'} {name} with shape: {axes}\")\n-        return axes\n-\n-    tokens = nlp.tokenizer(\"This is a sample output\", return_tensors=framework)\n-    seq_len = tokens.input_ids.shape[-1]\n-    outputs = nlp.model(**tokens) if framework == \"pt\" else nlp.model(tokens)\n-    if isinstance(outputs, ModelOutput):\n-        outputs = outputs.to_tuple()\n-    if not isinstance(outputs, (list, tuple)):\n-        outputs = (outputs,)\n-\n-    # Generate input names & axes\n-    input_vars = list(tokens.keys())\n-    input_dynamic_axes = {k: build_shape_dict(k, v, True, seq_len) for k, v in tokens.items()}\n-\n-    # flatten potentially grouped outputs (past for gpt2, attentions)\n-    outputs_flat = []\n-    for output in outputs:\n-        if isinstance(output, (tuple, list)):\n-            outputs_flat.extend(output)\n-        else:\n-            outputs_flat.append(output)\n-\n-    # Generate output names & axes\n-    output_names = [f\"output_{i}\" for i in range(len(outputs_flat))]\n-    output_dynamic_axes = {k: build_shape_dict(k, v, False, seq_len) for k, v in zip(output_names, outputs_flat)}\n-\n-    # Create the aggregated axes representation\n-    dynamic_axes = dict(input_dynamic_axes, **output_dynamic_axes)\n-    return input_vars, output_names, dynamic_axes, tokens\n-\n-\n-def load_graph_from_args(\n-    pipeline_name: str, framework: str, model: str, tokenizer: Optional[str] = None, **models_kwargs\n-) -> Pipeline:\n-    \"\"\"\n-    Convert the set of arguments provided through the CLI to an actual pipeline reference (tokenizer + model\n-\n-    Args:\n-        pipeline_name: The kind of pipeline to use (ner, question-answering, etc.)\n-        framework: The actual model to convert the pipeline from (\"pt\" or \"tf\")\n-        model: The model name which will be loaded by the pipeline\n-        tokenizer: The tokenizer name which will be loaded by the pipeline, default to the model's value\n-\n-    Returns: Pipeline object\n-\n-    \"\"\"\n-    # If no tokenizer provided\n-    if tokenizer is None:\n-        tokenizer = model\n-\n-    # Check the wanted framework is available\n-    if framework == \"pt\" and not is_torch_available():\n-        raise Exception(\"Cannot convert because PyTorch is not installed. Please install torch first.\")\n-    if framework == \"tf\" and not is_tf_available():\n-        raise Exception(\"Cannot convert because TF is not installed. Please install tensorflow first.\")\n-\n-    print(f\"Loading pipeline (model: {model}, tokenizer: {tokenizer})\")\n-\n-    # Allocate tokenizer and model\n-    return pipeline(pipeline_name, model=model, tokenizer=tokenizer, framework=framework, model_kwargs=models_kwargs)\n-\n-\n-def convert_pytorch(nlp: Pipeline, opset: int, output: Path, use_external_format: bool):\n-    \"\"\"\n-    Export a PyTorch backed pipeline to ONNX Intermediate Representation (IR\n-\n-    Args:\n-        nlp: The pipeline to be exported\n-        opset: The actual version of the ONNX operator set to use\n-        output: Path where will be stored the generated ONNX model\n-        use_external_format: Split the model definition from its parameters to allow model bigger than 2GB\n-\n-    Returns:\n-\n-    \"\"\"\n-    if not is_torch_available():\n-        raise Exception(\"Cannot convert because PyTorch is not installed. Please install torch first.\")\n-\n-    import torch\n-    from torch.onnx import export\n-\n-    print(f\"Using framework PyTorch: {torch.__version__}\")\n-\n-    with torch.no_grad():\n-        input_names, output_names, dynamic_axes, tokens = infer_shapes(nlp, \"pt\")\n-        ordered_input_names, model_args = ensure_valid_input(nlp.model, tokens, input_names)\n-\n-        export(\n-            nlp.model,\n-            model_args,\n-            f=output.as_posix(),\n-            input_names=ordered_input_names,\n-            output_names=output_names,\n-            dynamic_axes=dynamic_axes,\n-            do_constant_folding=True,\n-            opset_version=opset,\n-        )\n-\n-\n-def convert_tensorflow(nlp: Pipeline, opset: int, output: Path):\n-    \"\"\"\n-    Export a TensorFlow backed pipeline to ONNX Intermediate Representation (IR)\n-\n-    Args:\n-        nlp: The pipeline to be exported\n-        opset: The actual version of the ONNX operator set to use\n-        output: Path where will be stored the generated ONNX model\n-\n-    Notes: TensorFlow cannot export model bigger than 2GB due to internal constraint from TensorFlow\n-\n-    \"\"\"\n-    if not is_tf_available():\n-        raise Exception(\"Cannot convert because TF is not installed. Please install tensorflow first.\")\n-\n-    print(\"/!\\\\ Please note TensorFlow doesn't support exporting model > 2Gb /!\\\\\")\n-\n-    try:\n-        import tensorflow as tf\n-        import tf2onnx\n-        from tf2onnx import __version__ as t2ov\n-\n-        print(f\"Using framework TensorFlow: {tf.version.VERSION}, tf2onnx: {t2ov}\")\n-\n-        # Build\n-        input_names, output_names, dynamic_axes, tokens = infer_shapes(nlp, \"tf\")\n-\n-        # Forward\n-        nlp.model.predict(tokens.data)\n-        input_signature = [tf.TensorSpec.from_tensor(tensor, name=key) for key, tensor in tokens.items()]\n-        model_proto, _ = tf2onnx.convert.from_keras(\n-            nlp.model, input_signature, opset=opset, output_path=output.as_posix()\n-        )\n-\n-    except ImportError as e:\n-        raise Exception(\n-            f\"Cannot import {e.name} required to convert TF model to ONNX. Please install {e.name} first. {e}\"\n-        )\n-\n-\n-def convert(\n-    framework: str,\n-    model: str,\n-    output: Path,\n-    opset: int,\n-    tokenizer: Optional[str] = None,\n-    use_external_format: bool = False,\n-    pipeline_name: str = \"feature-extraction\",\n-    **model_kwargs,\n-):\n-    \"\"\"\n-    Convert the pipeline object to the ONNX Intermediate Representation (IR) format\n-\n-    Args:\n-        framework: The framework the pipeline is backed by (\"pt\" or \"tf\")\n-        model: The name of the model to load for the pipeline\n-        output: The path where the ONNX graph will be stored\n-        opset: The actual version of the ONNX operator set to use\n-        tokenizer: The name of the model to load for the pipeline, default to the model's name if not provided\n-        use_external_format:\n-            Split the model definition from its parameters to allow model bigger than 2GB (PyTorch only)\n-        pipeline_name: The kind of pipeline to instantiate (ner, question-answering, etc.)\n-        model_kwargs: Keyword arguments to be forwarded to the model constructor\n-\n-    Returns:\n-\n-    \"\"\"\n-    warnings.warn(\n-        \"The `transformers.convert_graph_to_onnx` package is deprecated and will be removed in version 5 of\"\n-        \" Transformers\",\n-        FutureWarning,\n-    )\n-    print(f\"ONNX opset version set to: {opset}\")\n-\n-    # Load the pipeline\n-    nlp = load_graph_from_args(pipeline_name, framework, model, tokenizer, **model_kwargs)\n-\n-    if not output.parent.exists():\n-        print(f\"Creating folder {output.parent}\")\n-        makedirs(output.parent.as_posix())\n-    elif len(listdir(output.parent.as_posix())) > 0:\n-        raise Exception(f\"Folder {output.parent.as_posix()} is not empty, aborting conversion\")\n-\n-    # Export the graph\n-    if framework == \"pt\":\n-        convert_pytorch(nlp, opset, output, use_external_format)\n-    else:\n-        convert_tensorflow(nlp, opset, output)\n-\n-\n-def optimize(onnx_model_path: Path) -> Path:\n-    \"\"\"\n-    Load the model at the specified path and let onnxruntime look at transformations on the graph to enable all the\n-    optimizations possible\n-\n-    Args:\n-        onnx_model_path: filepath where the model binary description is stored\n-\n-    Returns: Path where the optimized model binary description has been saved\n-\n-    \"\"\"\n-    from onnxruntime import InferenceSession, SessionOptions\n-\n-    # Generate model name with suffix \"optimized\"\n-    opt_model_path = generate_identified_filename(onnx_model_path, \"-optimized\")\n-    sess_option = SessionOptions()\n-    sess_option.optimized_model_filepath = opt_model_path.as_posix()\n-    _ = InferenceSession(onnx_model_path.as_posix(), sess_option)\n-\n-    print(f\"Optimized model has been written at {opt_model_path}: \\N{HEAVY CHECK MARK}\")\n-    print(\"/!\\\\ Optimized model contains hardware specific operators which might not be portable. /!\\\\\")\n-\n-    return opt_model_path\n-\n-\n-def quantize(onnx_model_path: Path) -> Path:\n-    \"\"\"\n-    Quantize the weights of the model from float32 to in8 to allow very efficient inference on modern CPU\n-\n-    Args:\n-        onnx_model_path: Path to location the exported ONNX model is stored\n-\n-    Returns: The Path generated for the quantized\n-    \"\"\"\n-    import onnx\n-    import onnxruntime\n-    from onnx.onnx_pb import ModelProto\n-    from onnxruntime.quantization import QuantizationMode\n-    from onnxruntime.quantization.onnx_quantizer import ONNXQuantizer\n-    from onnxruntime.quantization.registry import IntegerOpsRegistry\n-\n-    # Load the ONNX model\n-    onnx_model = onnx.load(onnx_model_path.as_posix())\n-\n-    if parse(onnx.__version__) < parse(\"1.5.0\"):\n-        print(\n-            \"Models larger than 2GB will fail to quantize due to protobuf constraint.\\n\"\n-            \"Please upgrade to onnxruntime >= 1.5.0.\"\n-        )\n-\n-    # Copy it\n-    copy_model = ModelProto()\n-    copy_model.CopyFrom(onnx_model)\n-\n-    # Construct quantizer\n-    # onnxruntime renamed input_qType to activation_qType in v1.13.1, so we\n-    # check the onnxruntime version to ensure backward compatibility.\n-    # See also: https://github.com/microsoft/onnxruntime/pull/12873\n-    if parse(onnxruntime.__version__) < parse(\"1.13.1\"):\n-        quantizer = ONNXQuantizer(\n-            model=copy_model,\n-            per_channel=False,\n-            reduce_range=False,\n-            mode=QuantizationMode.IntegerOps,\n-            static=False,\n-            weight_qType=True,\n-            input_qType=False,\n-            tensors_range=None,\n-            nodes_to_quantize=None,\n-            nodes_to_exclude=None,\n-            op_types_to_quantize=list(IntegerOpsRegistry),\n-        )\n-    else:\n-        quantizer = ONNXQuantizer(\n-            model=copy_model,\n-            per_channel=False,\n-            reduce_range=False,\n-            mode=QuantizationMode.IntegerOps,\n-            static=False,\n-            weight_qType=True,\n-            activation_qType=False,\n-            tensors_range=None,\n-            nodes_to_quantize=None,\n-            nodes_to_exclude=None,\n-            op_types_to_quantize=list(IntegerOpsRegistry),\n-        )\n-\n-    # Quantize and export\n-    quantizer.quantize_model()\n-\n-    # Append \"-quantized\" at the end of the model's name\n-    quantized_model_path = generate_identified_filename(onnx_model_path, \"-quantized\")\n-\n-    # Save model\n-    print(f\"Quantized model has been written at {quantized_model_path}: \\N{HEAVY CHECK MARK}\")\n-    onnx.save_model(quantizer.model.model, quantized_model_path.as_posix())\n-\n-    return quantized_model_path\n-\n-\n-def verify(path: Path):\n-    from onnxruntime import InferenceSession, SessionOptions\n-    from onnxruntime.capi.onnxruntime_pybind11_state import RuntimeException\n-\n-    print(f\"Checking ONNX model loading from: {path} ...\")\n-    try:\n-        onnx_options = SessionOptions()\n-        _ = InferenceSession(path.as_posix(), onnx_options, providers=[\"CPUExecutionProvider\"])\n-        print(f\"Model {path} correctly loaded: \\N{HEAVY CHECK MARK}\")\n-    except RuntimeException as re:\n-        print(f\"Error while loading the model {re}: \\N{HEAVY BALLOT X}\")\n-\n-\n-if __name__ == \"__main__\":\n-    parser = OnnxConverterArgumentParser()\n-    args = parser.parse_args()\n-\n-    # Make sure output is absolute path\n-    args.output = Path(args.output).absolute()\n-\n-    try:\n-        print(\"\\n====== Converting model to ONNX ======\")\n-        # Convert\n-        convert(\n-            args.framework,\n-            args.model,\n-            args.output,\n-            args.opset,\n-            args.tokenizer,\n-            args.use_external_format,\n-            args.pipeline,\n-        )\n-\n-        if args.quantize:\n-            # Ensure requirements for quantization on onnxruntime is met\n-            check_onnxruntime_requirements(ORT_QUANTIZE_MINIMUM_VERSION)\n-\n-            # onnxruntime optimizations doesn't provide the same level of performances on TensorFlow than PyTorch\n-            if args.framework == \"tf\":\n-                print(\n-                    \"\\t Using TensorFlow might not provide the same optimization level compared to PyTorch.\\n\"\n-                    \"\\t For TensorFlow users you can try optimizing the model directly through onnxruntime_tools.\\n\"\n-                    \"\\t For more information, please refer to the onnxruntime documentation:\\n\"\n-                    \"\\t\\thttps://github.com/microsoft/onnxruntime/tree/master/onnxruntime/python/tools/transformers\\n\"\n-                )\n-\n-            print(\"\\n====== Optimizing ONNX model ======\")\n-\n-            # Quantization works best when using the optimized version of the model\n-            args.optimized_output = optimize(args.output)\n-\n-            # Do the quantization on the right graph\n-            args.quantized_output = quantize(args.optimized_output)\n-\n-        # And verify\n-        if args.check_loading:\n-            print(\"\\n====== Check exported ONNX model(s) ======\")\n-            verify(args.output)\n-\n-            if hasattr(args, \"optimized_output\"):\n-                verify(args.optimized_output)\n-\n-            if hasattr(args, \"quantized_output\"):\n-                verify(args.quantized_output)\n-\n-    except Exception as e:\n-        print(f\"Error while converting the model: {e}\")\n-        exit(1)"
        },
        {
            "sha": "e2c825a45b605c271c3912bf5b4f3ff4b5c1a32c",
            "filename": "src/transformers/convert_tf_hub_seq_to_seq_bert_to_pytorch.py",
            "status": "removed",
            "additions": 0,
            "deletions": 86,
            "changes": 86,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fconvert_tf_hub_seq_to_seq_bert_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fconvert_tf_hub_seq_to_seq_bert_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconvert_tf_hub_seq_to_seq_bert_to_pytorch.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,86 +0,0 @@\n-# Copyright 2020 The HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Convert Seq2Seq TF Hub checkpoint.\"\"\"\n-\n-import argparse\n-\n-from . import (\n-    BertConfig,\n-    BertGenerationConfig,\n-    BertGenerationDecoder,\n-    BertGenerationEncoder,\n-    load_tf_weights_in_bert_generation,\n-    logging,\n-)\n-\n-\n-logging.set_verbosity_info()\n-\n-\n-def convert_tf_checkpoint_to_pytorch(tf_hub_path, pytorch_dump_path, is_encoder_named_decoder, vocab_size, is_encoder):\n-    # Initialise PyTorch model\n-    bert_config = BertConfig.from_pretrained(\n-        \"google-bert/bert-large-cased\",\n-        vocab_size=vocab_size,\n-        max_position_embeddings=512,\n-        is_decoder=True,\n-        add_cross_attention=True,\n-    )\n-    bert_config_dict = bert_config.to_dict()\n-    del bert_config_dict[\"type_vocab_size\"]\n-    config = BertGenerationConfig(**bert_config_dict)\n-    if is_encoder:\n-        model = BertGenerationEncoder(config)\n-    else:\n-        model = BertGenerationDecoder(config)\n-    print(f\"Building PyTorch model from configuration: {config}\")\n-\n-    # Load weights from tf checkpoint\n-    load_tf_weights_in_bert_generation(\n-        model,\n-        tf_hub_path,\n-        model_class=\"bert\",\n-        is_encoder_named_decoder=is_encoder_named_decoder,\n-        is_encoder=is_encoder,\n-    )\n-\n-    # Save pytorch-model\n-    print(f\"Save PyTorch model and config to {pytorch_dump_path}\")\n-    model.save_pretrained(pytorch_dump_path)\n-\n-\n-if __name__ == \"__main__\":\n-    parser = argparse.ArgumentParser()\n-    # Required parameters\n-    parser.add_argument(\n-        \"--tf_hub_path\", default=None, type=str, required=True, help=\"Path to the TensorFlow checkpoint path.\"\n-    )\n-    parser.add_argument(\n-        \"--pytorch_dump_path\", default=None, type=str, required=True, help=\"Path to the output PyTorch model.\"\n-    )\n-    parser.add_argument(\n-        \"--is_encoder_named_decoder\",\n-        action=\"store_true\",\n-        help=\"If decoder has to be renamed to encoder in PyTorch model.\",\n-    )\n-    parser.add_argument(\"--is_encoder\", action=\"store_true\", help=\"If model is an encoder.\")\n-    parser.add_argument(\"--vocab_size\", default=50358, type=int, help=\"Vocab size of model\")\n-    args = parser.parse_args()\n-    convert_tf_checkpoint_to_pytorch(\n-        args.tf_hub_path,\n-        args.pytorch_dump_path,\n-        args.is_encoder_named_decoder,\n-        args.vocab_size,\n-        is_encoder=args.is_encoder,\n-    )"
        },
        {
            "sha": "1bff72cf338c19f125e4fe8dd19609142641c94d",
            "filename": "src/transformers/data/data_collator.py",
            "status": "modified",
            "additions": 9,
            "deletions": 484,
            "changes": 493,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fdata%2Fdata_collator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fdata%2Fdata_collator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fdata_collator.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -31,7 +31,7 @@\n \n \"\"\"\n A DataCollator is a function that takes a list of samples from a Dataset and collate them into a batch, as a dictionary\n-of PyTorch/TensorFlow tensors or NumPy arrays.\n+of PyTorch tensors or NumPy arrays.\n \"\"\"\n DataCollator = NewType(\"DataCollator\", Callable[[list[InputDataClass]], dict[str, Any]])\n \n@@ -40,9 +40,7 @@ class DataCollatorMixin:\n     def __call__(self, features, return_tensors=None):\n         if return_tensors is None:\n             return_tensors = self.return_tensors\n-        if return_tensors == \"tf\":\n-            return self.tf_call(features)\n-        elif return_tensors == \"pt\":\n+        if return_tensors == \"pt\":\n             return self.torch_call(features)\n         elif return_tensors == \"np\":\n             return self.numpy_call(features)\n@@ -91,8 +89,6 @@ def default_data_collator(features: list[InputDataClass], return_tensors=\"pt\") -\n \n     if return_tensors == \"pt\":\n         return torch_default_data_collator(features)\n-    elif return_tensors == \"tf\":\n-        return tf_default_data_collator(features)\n     elif return_tensors == \"np\":\n         return numpy_default_data_collator(features)\n \n@@ -114,7 +110,7 @@ class DefaultDataCollator(DataCollatorMixin):\n \n     Args:\n         return_tensors (`str`, *optional*, defaults to `\"pt\"`):\n-            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n+            The type of Tensor to return. Allowable values are \"np\", or \"pt\".\n     \"\"\"\n \n     return_tensors: str = \"pt\"\n@@ -161,47 +157,6 @@ def torch_default_data_collator(features: list[InputDataClass]) -> dict[str, Any\n     return batch\n \n \n-def tf_default_data_collator(features: list[InputDataClass]) -> dict[str, Any]:\n-    import tensorflow as tf\n-\n-    if not isinstance(features[0], Mapping):\n-        features = [vars(f) for f in features]\n-    first = features[0]\n-    batch = {}\n-\n-    # Special handling for labels.\n-    # Ensure that tensor is created with the correct type\n-    # (it should be automatically the case, but let's make sure of it.)\n-    if \"label\" in first and first[\"label\"] is not None:\n-        label_col_name = \"label\"\n-    elif \"label_ids\" in first and first[\"label_ids\"] is not None:\n-        label_col_name = \"label_ids\"\n-    elif \"labels\" in first and first[\"labels\"] is not None:\n-        label_col_name = \"labels\"\n-    else:\n-        label_col_name = None\n-    if label_col_name is not None:\n-        if isinstance(first[label_col_name], tf.Tensor):\n-            dtype = tf.int64 if first[label_col_name].dtype.is_integer else tf.float32\n-        elif isinstance(first[label_col_name], (np.ndarray, np.generic)):\n-            dtype = tf.int64 if np.issubdtype(first[label_col_name].dtype, np.integer) else tf.float32\n-        elif isinstance(first[label_col_name], (tuple, list)):\n-            dtype = tf.int64 if isinstance(first[label_col_name][0], int) else tf.float32\n-        else:\n-            dtype = tf.int64 if isinstance(first[label_col_name], int) else tf.float32\n-        batch[\"labels\"] = tf.convert_to_tensor([f[label_col_name] for f in features], dtype=dtype)\n-    # Handling of all other possible keys.\n-    # Again, we will use the first element to figure out which key/values are not None for this model.\n-    for k, v in first.items():\n-        if k not in (\"label\", \"label_ids\", \"labels\") and v is not None and not isinstance(v, str):\n-            if isinstance(v, (tf.Tensor, np.ndarray)):\n-                batch[k] = tf.stack([f[k] for f in features])\n-            else:\n-                batch[k] = tf.convert_to_tensor([f[k] for f in features])\n-\n-    return batch\n-\n-\n def numpy_default_data_collator(features: list[InputDataClass]) -> dict[str, Any]:\n     if not isinstance(features[0], Mapping):\n         features = [vars(f) for f in features]\n@@ -259,7 +214,7 @@ class DataCollatorWithPadding:\n             This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n             7.0 (Volta).\n         return_tensors (`str`, *optional*, defaults to `\"pt\"`):\n-            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n+            The type of Tensor to return. Allowable values are \"np\", or \"pt\".\n     \"\"\"\n \n     tokenizer: PreTrainedTokenizerBase\n@@ -313,7 +268,7 @@ class DataCollatorForTokenClassification(DataCollatorMixin):\n         label_pad_token_id (`int`, *optional*, defaults to -100):\n             The id to use when padding the labels (-100 will be automatically ignore by PyTorch loss functions).\n         return_tensors (`str`, *optional*, defaults to `\"pt\"`):\n-            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n+            The type of Tensor to return. Allowable values are \"np\", or \"pt\".\n     \"\"\"\n \n     tokenizer: PreTrainedTokenizerBase\n@@ -363,38 +318,6 @@ def to_list(tensor_or_iterable):\n         batch[label_name] = torch.tensor(batch[label_name], dtype=torch.int64)\n         return batch\n \n-    def tf_call(self, features):\n-        import tensorflow as tf\n-\n-        label_name = \"label\" if \"label\" in features[0] else \"labels\"\n-        labels = [feature[label_name] for feature in features] if label_name in features[0] else None\n-        batch = pad_without_fast_tokenizer_warning(\n-            self.tokenizer,\n-            features,\n-            padding=self.padding,\n-            max_length=self.max_length,\n-            pad_to_multiple_of=self.pad_to_multiple_of,\n-            # Conversion to tensors will fail if we have labels as they are not of the same length yet.\n-            return_tensors=\"tf\" if labels is None else None,\n-        )\n-\n-        if labels is None:\n-            return batch\n-\n-        sequence_length = tf.convert_to_tensor(batch[\"input_ids\"]).shape[1]\n-        padding_side = self.tokenizer.padding_side\n-        if padding_side == \"right\":\n-            batch[\"labels\"] = [\n-                list(label) + [self.label_pad_token_id] * (sequence_length - len(label)) for label in labels\n-            ]\n-        else:\n-            batch[\"labels\"] = [\n-                [self.label_pad_token_id] * (sequence_length - len(label)) + list(label) for label in labels\n-            ]\n-\n-        batch = {k: tf.convert_to_tensor(v, dtype=tf.int64) for k, v in batch.items()}\n-        return batch\n-\n     def numpy_call(self, features):\n         label_name = \"label\" if \"label\" in features[0] else \"labels\"\n         labels = [feature[label_name] for feature in features] if label_name in features[0] else None\n@@ -463,44 +386,6 @@ def _torch_collate_batch(examples, tokenizer, pad_to_multiple_of: Optional[int]\n     return result\n \n \n-def _tf_collate_batch(examples, tokenizer, pad_to_multiple_of: Optional[int] = None):\n-    import tensorflow as tf\n-\n-    \"\"\"Collate `examples` into a batch, using the information in `tokenizer` for padding if necessary.\"\"\"\n-    # Tensorize if necessary.\n-    if isinstance(examples[0], (list, tuple)):\n-        examples = [tf.convert_to_tensor(e, dtype=tf.int64) for e in examples]\n-\n-    # Check if padding is necessary.\n-    length_of_first = len(examples[0])\n-    are_tensors_same_length = all(len(x) == length_of_first for x in examples)\n-    if are_tensors_same_length and (pad_to_multiple_of is None or length_of_first % pad_to_multiple_of == 0):\n-        return tf.stack(examples, axis=0)\n-\n-    # If yes, check if we have a `pad_token`.\n-    if tokenizer.pad_token is None:\n-        raise ValueError(\n-            \"You are attempting to pad samples but the tokenizer you are using\"\n-            f\" ({tokenizer.__class__.__name__}) does not have a pad token.\"\n-        )\n-\n-    # Creating the full tensor and filling it with our data.\n-    max_length = max(len(x) for x in examples)\n-    if pad_to_multiple_of is not None and (max_length % pad_to_multiple_of != 0):\n-        max_length = ((max_length // pad_to_multiple_of) + 1) * pad_to_multiple_of\n-    # result = examples[0].new_full([len(examples), max_length], tokenizer.pad_token_id)\n-    result = []\n-    rank = tf.rank(examples[0])\n-    paddings = np.zeros((rank, 2), dtype=np.int32)\n-    for example in examples:\n-        if tokenizer.padding_side == \"right\":\n-            paddings[0, 1] = max_length - len(example)\n-        else:\n-            paddings[0, 0] = max_length - len(example)\n-        result.append(tf.pad(example, paddings, constant_values=tokenizer.pad_token_id))\n-    return tf.stack(result, axis=0)\n-\n-\n def _numpy_collate_batch(examples, tokenizer, pad_to_multiple_of: Optional[int] = None):\n     \"\"\"Collate `examples` into a batch, using the information in `tokenizer` for padding if necessary.\"\"\"\n     # Tensorize if necessary.\n@@ -560,7 +445,7 @@ class DataCollatorForMultipleChoice(DataCollatorMixin):\n             This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n             7.5 (Volta).\n         return_tensors (`str`, *optional*, defaults to `\"pt\"`):\n-            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n+            The type of Tensor to return. Allowable values are \"np\", or \"pt\".\n     \"\"\"\n \n     tokenizer: PreTrainedTokenizerBase\n@@ -599,30 +484,6 @@ def torch_call(self, examples: list[dict[str, Any]]):  # Refactored implementati\n         batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n         return batch\n \n-    def tf_call(self, features):  # Implementation taken from the docs.\n-        import tensorflow as tf\n-\n-        label_name = \"label\" if \"label\" in features[0] else \"labels\"\n-        labels = [feature.pop(label_name) for feature in features]\n-        batch_size = len(features)\n-        num_choices = len(features[0][\"input_ids\"])\n-        flattened_features = [\n-            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n-        ]\n-        flattened_features = sum(flattened_features, [])  # Sometimes written as list(chain(*flattened_features))\n-\n-        batch = self.tokenizer.pad(\n-            flattened_features,\n-            padding=self.padding,\n-            max_length=self.max_length,\n-            pad_to_multiple_of=self.pad_to_multiple_of,\n-            return_tensors=\"tf\",\n-        )\n-\n-        batch = {k: tf.reshape(v, (batch_size, num_choices, -1)) for k, v in batch.items()}\n-        batch[\"labels\"] = tf.convert_to_tensor(labels, dtype=tf.int64)\n-        return batch\n-\n \n @dataclass\n class DataCollatorForSeq2Seq:\n@@ -656,7 +517,7 @@ class DataCollatorForSeq2Seq:\n         label_pad_token_id (`int`, *optional*, defaults to -100):\n             The id to use when padding the labels (-100 will be automatically ignored by PyTorch loss functions).\n         return_tensors (`str`, *optional*, defaults to `\"pt\"`):\n-            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n+            The type of Tensor to return. Allowable values are \"np\", or \"pt\".\n     \"\"\"\n \n     tokenizer: PreTrainedTokenizerBase\n@@ -739,10 +600,6 @@ def __call__(self, features, return_tensors=None):\n                 import torch\n \n                 batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n-            elif return_tensors == \"tf\":\n-                import tensorflow as tf\n-\n-                batch[\"labels\"] = tf.constant(batch[\"labels\"], dtype=tf.int64)\n             else:\n                 batch[\"labels\"] = np.array(batch[\"labels\"], dtype=np.int64)\n         else:\n@@ -787,7 +644,7 @@ class DataCollatorForLanguageModeling(DataCollatorMixin):\n         pad_to_multiple_of (`int`, *optional*):\n             If set, will pad the sequence to a multiple of the provided value.\n         return_tensors (`str`):\n-            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n+            The type of Tensor to return. Allowable values are \"np\", or \"pt\".\n         seed (`int`, *optional*):\n             The seed to use for the random number generator for masking. If not provided, the global RNG will be used.\n \n@@ -828,7 +685,6 @@ class DataCollatorForLanguageModeling(DataCollatorMixin):\n     mask_replace_prob: float = 0.8\n     random_replace_prob: float = 0.1\n     pad_to_multiple_of: Optional[int] = None\n-    tf_experimental_compile: bool = False\n     return_tensors: str = \"pt\"\n     seed: Optional[int] = None\n \n@@ -852,22 +708,13 @@ def __post_init__(self):\n         self.mask_replace_prob = float(self.mask_replace_prob)\n         self.random_replace_prob = float(self.random_replace_prob)\n \n-        if self.tf_experimental_compile:\n-            import tensorflow as tf\n-\n-            self.tf_mask_tokens = tf.function(self.tf_mask_tokens, jit_compile=True)\n-\n         self.generator = None\n \n     def get_generator(self, seed):\n         if self.return_tensors == \"pt\":\n             import torch\n \n             return torch.Generator().manual_seed(seed)\n-        elif self.return_tensors == \"tf\":\n-            import tensorflow as tf\n-\n-            return tf.random.Generator.from_seed(seed)\n         else:\n             import numpy as np\n \n@@ -882,7 +729,6 @@ def create_rng(self):\n             # worker's generator, generated as the main seed + the worker's ID.\n             # (https://pytorch.org/docs/stable/data.html#randomness-in-multi-process-data-loading)\n             # Only PyTorch DataLoader allows us to access the worker ID, and so we check for this.\n-            # For other frameworks, we will throw an error.\n             import torch\n \n             worker_info = torch.utils.data.get_worker_info()\n@@ -897,111 +743,6 @@ def create_rng(self):\n \n             self.generator = self.get_generator(self.seed + worker_info.id)\n \n-    @staticmethod\n-    def tf_bernoulli(shape, probability, generator=None):\n-        import tensorflow as tf\n-\n-        prob_matrix = tf.fill(shape, probability)\n-        # if generator exists, use it to generate the random numbers\n-        # otherwise, use the global RNG\n-        if generator:\n-            return tf.cast(prob_matrix - generator.uniform(shape, 0, 1) >= 0, tf.bool)\n-        else:\n-            return tf.cast(prob_matrix - tf.random.uniform(shape, 0, 1) >= 0, tf.bool)\n-\n-    def tf_mask_tokens(\n-        self, inputs: Any, vocab_size, mask_token_id, special_tokens_mask: Optional[Any] = None\n-    ) -> tuple[Any, Any]:\n-        \"\"\"\n-        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n-        \"\"\"\n-        import tensorflow as tf\n-\n-        mask_token_id = tf.cast(mask_token_id, inputs.dtype)\n-\n-        input_shape = tf.shape(inputs)\n-        # 1 for a special token, 0 for a normal token in the special tokens mask\n-        # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n-        masked_indices = self.tf_bernoulli(input_shape, self.mlm_probability, self.generator) & ~special_tokens_mask\n-        # Replace unmasked indices with -100 in the labels since we only compute loss on masked tokens\n-        labels = tf.where(masked_indices, inputs, -100)\n-\n-        # mask_replace_prob% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n-        indices_replaced = self.tf_bernoulli(input_shape, self.mask_replace_prob, self.generator) & masked_indices\n-\n-        inputs = tf.where(indices_replaced, mask_token_id, inputs)\n-\n-        if self.mask_replace_prob == 1 or self.random_replace_prob == 0:\n-            return inputs, labels\n-\n-        remaining_prob = 1 - self.mask_replace_prob\n-        # scaling the random_replace_prob to the remaining probability for example if\n-        # mask_replace_prob = 0.8 and random_replace_prob = 0.1,\n-        # then random_replace_prob_scaled = 0.1 / 0.2 = 0.5\n-        random_replace_prob_scaled = self.random_replace_prob / remaining_prob\n-        # random_replace_prob% of the time, we replace masked input tokens with random word\n-        indices_random = (\n-            self.tf_bernoulli(input_shape, random_replace_prob_scaled, self.generator)\n-            & masked_indices\n-            & ~indices_replaced\n-        )\n-\n-        if self.generator:\n-            random_words = self.generator.uniform(input_shape, maxval=vocab_size, dtype=inputs.dtype)\n-        else:\n-            random_words = tf.random.uniform(input_shape, maxval=vocab_size, dtype=inputs.dtype)\n-\n-        inputs = tf.where(indices_random, random_words, inputs)\n-\n-        # The rest of the time ((1-random_replace_prob-mask_replace_prob)% of the time) we keep the masked input tokens unchanged\n-        return inputs, labels\n-\n-    def tf_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> dict[str, Any]:\n-        import tensorflow as tf\n-\n-        if self.seed and self.generator is None:\n-            # If we have a seed, we need to create a generator object. Subsequent calls to this function will use the same generator.\n-            # If no seed supplied, we will use the global RNG\n-            self.create_rng()\n-\n-        # Handle dict or lists with proper padding and conversion to tensor.\n-        if isinstance(examples[0], Mapping):\n-            batch = pad_without_fast_tokenizer_warning(\n-                self.tokenizer, examples, return_tensors=\"tf\", pad_to_multiple_of=self.pad_to_multiple_of\n-            )\n-        else:\n-            batch = {\n-                \"input_ids\": _tf_collate_batch(examples, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n-            }\n-\n-        # If special token mask has been preprocessed, pop it from the dict.\n-        special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n-        if self.mlm:\n-            if special_tokens_mask is None:\n-                special_tokens_mask = [\n-                    self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True)\n-                    for val in batch[\"input_ids\"].numpy().tolist()\n-                ]\n-                # Cannot directly create as bool\n-                special_tokens_mask = tf.cast(tf.convert_to_tensor(special_tokens_mask, dtype=tf.int64), tf.bool)\n-            else:\n-                special_tokens_mask = tf.cast(special_tokens_mask, tf.bool)\n-            batch[\"input_ids\"], batch[\"labels\"] = self.tf_mask_tokens(\n-                tf.cast(batch[\"input_ids\"], tf.int64),\n-                special_tokens_mask=special_tokens_mask,\n-                mask_token_id=self.tokenizer.mask_token_id,\n-                vocab_size=len(self.tokenizer),\n-            )\n-        else:\n-            labels = batch[\"input_ids\"]\n-            if self.tokenizer.pad_token_id is not None:\n-                # Replace self.tokenizer.pad_token_id with -100\n-                labels = tf.where(labels == self.tokenizer.pad_token_id, -100, labels)\n-            else:\n-                labels = tf.identity(labels)  # Makes a copy, just in case\n-            batch[\"labels\"] = labels\n-        return batch\n-\n     def torch_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> dict[str, Any]:\n         # Handle dict or lists with proper padding and conversion to tensor.\n \n@@ -1226,41 +967,6 @@ def torch_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> d\n         inputs, labels = self.torch_mask_tokens(batch_input, batch_mask)\n         return {\"input_ids\": inputs, \"labels\": labels}\n \n-    def tf_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> dict[str, Any]:\n-        import tensorflow as tf\n-\n-        if self.seed and self.generator is None:\n-            # If we have a seed, we need to create a generator object. Subsequent calls to this function will use the same generator.\n-            # If no seed supplied, we will use the global RNG\n-            self.create_rng()\n-\n-        if isinstance(examples[0], Mapping):\n-            input_ids = [e[\"input_ids\"] for e in examples]\n-        else:\n-            input_ids = examples\n-            examples = [{\"input_ids\": e} for e in examples]\n-\n-        batch_input = _tf_collate_batch(input_ids, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n-\n-        mask_labels = []\n-        for e in examples:\n-            ref_tokens = []\n-            for id in tolist(e[\"input_ids\"]):\n-                token = self.tokenizer._convert_id_to_token(id)\n-                ref_tokens.append(token)\n-\n-            # For Chinese tokens, we need extra inf to mark sub-word, e.g [Âñú,Ê¨¢]-> [ÂñúÔºå##Ê¨¢]\n-            if \"chinese_ref\" in e:\n-                ref_pos = tolist(e[\"chinese_ref\"])\n-                len_seq = len(e[\"input_ids\"])\n-                for i in range(len_seq):\n-                    if i in ref_pos:\n-                        ref_tokens[i] = \"##\" + ref_tokens[i]\n-            mask_labels.append(self._whole_word_mask(ref_tokens))\n-        batch_mask = _tf_collate_batch(mask_labels, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n-        inputs, labels = self.tf_mask_tokens(tf.cast(batch_input, tf.int64), batch_mask)\n-        return {\"input_ids\": inputs, \"labels\": labels}\n-\n     def numpy_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> dict[str, Any]:\n         if self.seed and self.generator is None:\n             # If we have a seed, we need to create a generator object. Subsequent calls to this function will use the same generator.\n@@ -1307,13 +1013,6 @@ def _shuffle(self, cand_indexes):\n             indices = torch.randperm(len(cand_indexes), generator=self.generator)\n             return [cand_indexes[i] for i in indices]\n \n-        elif self.return_tensors == \"tf\":\n-            import tensorflow as tf\n-\n-            seed = self.generator.make_seeds(2)[0]\n-            indices = tf.random.experimental.stateless_shuffle(tf.range(len(cand_indexes)), seed=seed).numpy().tolist()\n-            return [cand_indexes[i] for i in indices]\n-\n         elif self.return_tensors == \"np\":\n             self.generator.shuffle(cand_indexes)\n             return cand_indexes\n@@ -1414,66 +1113,6 @@ def torch_mask_tokens(self, inputs: Any, mask_labels: Any) -> tuple[Any, Any]:\n         # The rest of the time ((1-random_replacement_prob-mask_replace_prob)% of the time) we keep the masked input tokens unchanged\n         return inputs, labels\n \n-    def tf_mask_tokens(self, inputs: Any, mask_labels: Any) -> tuple[Any, Any]:\n-        \"\"\"\n-        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. Set\n-        'mask_labels' means we use whole word mask (wwm), we directly mask idxs according to it's ref.\n-        \"\"\"\n-        import tensorflow as tf\n-\n-        input_shape = tf.shape(inputs)\n-        if self.tokenizer.mask_token is None:\n-            raise ValueError(\n-                \"This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the\"\n-                \" --mlm flag if you want to use this tokenizer.\"\n-            )\n-        labels = tf.identity(inputs)\n-        # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n-\n-        masked_indices = tf.cast(mask_labels, tf.bool)\n-\n-        special_tokens_mask = [\n-            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels\n-        ]\n-        masked_indices = masked_indices & ~tf.cast(special_tokens_mask, dtype=tf.bool)\n-        if self.tokenizer.pad_token is not None:\n-            padding_mask = inputs == self.tokenizer.pad_token_id\n-            masked_indices = masked_indices & ~padding_mask\n-\n-        # Replace unmasked indices with -100 in the labels since we only compute loss on masked tokens\n-        labels = tf.where(masked_indices, inputs, -100)\n-\n-        # mask_replace_prob% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n-        indices_replaced = self.tf_bernoulli(input_shape, self.mask_replace_prob, self.generator) & masked_indices\n-\n-        inputs = tf.where(indices_replaced, self.tokenizer.mask_token_id, inputs)\n-\n-        if self.mask_replace_prob == 1 or self.random_replace_prob == 0:\n-            return inputs, labels\n-\n-        remaining_prob = 1 - self.mask_replace_prob\n-        # scaling the random_replace_prob to the remaining probability for example if\n-        # mask_replace_prob = 0.8 and random_replace_prob = 0.1,\n-        # then random_replace_prob_scaled = 0.1 / 0.2 = 0.5\n-        random_replace_prob_scaled = self.random_replace_prob / remaining_prob\n-\n-        # random_replace_prob% of the time, we replace masked input tokens with random word\n-        indices_random = (\n-            self.tf_bernoulli(input_shape, random_replace_prob_scaled, self.generator)\n-            & masked_indices\n-            & ~indices_replaced\n-        )\n-\n-        if self.generator:\n-            random_words = self.generator.uniform(input_shape, maxval=len(self.tokenizer), dtype=tf.int64)\n-        else:\n-            random_words = tf.random.uniform(input_shape, maxval=len(self.tokenizer), dtype=tf.int64)\n-\n-        inputs = tf.where(indices_random, random_words, inputs)\n-\n-        # The rest of the time ((1-mask_replace_prob-random_replace_prob)% of the time) we keep the masked input tokens unchanged\n-        return inputs, labels\n-\n     def numpy_mask_tokens(self, inputs: Any, mask_labels: Any) -> tuple[Any, Any]:\n         \"\"\"\n         Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. Set\n@@ -1543,7 +1182,7 @@ def numpy_mask_tokens(self, inputs: Any, mask_labels: Any) -> tuple[Any, Any]:\n def tolist(x):\n     if isinstance(x, list):\n         return x\n-    elif hasattr(x, \"numpy\"):  # Checks for TF tensors without needing the import\n+    elif hasattr(x, \"numpy\"):\n         x = x.numpy()\n     return x.tolist()\n \n@@ -1652,13 +1291,6 @@ def torch_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> d\n         inputs, perm_mask, target_mapping, labels = self.torch_mask_tokens(batch)\n         return {\"input_ids\": inputs, \"perm_mask\": perm_mask, \"target_mapping\": target_mapping, \"labels\": labels}\n \n-    def tf_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> dict[str, Any]:\n-        if isinstance(examples[0], Mapping):\n-            examples = [e[\"input_ids\"] for e in examples]\n-        batch = _tf_collate_batch(examples, self.tokenizer)\n-        inputs, perm_mask, target_mapping, labels = self.tf_mask_tokens(batch)\n-        return {\"input_ids\": inputs, \"perm_mask\": perm_mask, \"target_mapping\": target_mapping, \"labels\": labels}\n-\n     def numpy_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> dict[str, Any]:\n         if isinstance(examples[0], Mapping):\n             examples = [e[\"input_ids\"] for e in examples]\n@@ -1765,113 +1397,6 @@ def torch_mask_tokens(self, inputs: Any) -> tuple[Any, Any, Any, Any]:\n \n         return inputs.long(), perm_mask, target_mapping, labels.long()\n \n-    def tf_mask_tokens(self, inputs: Any) -> tuple[Any, Any, Any, Any]:\n-        \"\"\"\n-        The masked tokens to be predicted for a particular sequence are determined by the following algorithm:\n-\n-            0. Start from the beginning of the sequence by setting `cur_len = 0` (number of tokens processed so far).\n-            1. Sample a `span_length` from the interval `[1, max_span_length]` (length of span of tokens to be masked)\n-            2. Reserve a context of length `context_length = span_length / plm_probability` to surround span to be\n-               masked\n-            3. Sample a starting point `start_index` from the interval `[cur_len, cur_len + context_length -\n-               span_length]` and mask tokens `start_index:start_index + span_length`\n-            4. Set `cur_len = cur_len + context_length`. If `cur_len < max_len` (i.e. there are tokens remaining in the\n-               sequence to be processed), repeat from Step 1.\n-        \"\"\"\n-        import tensorflow as tf\n-\n-        if self.tokenizer.mask_token is None:\n-            raise ValueError(\n-                \"This tokenizer does not have a mask token which is necessary for permutation language modeling.\"\n-                \" Please add a mask token if you want to use this tokenizer.\"\n-            )\n-\n-        if tf.shape(inputs)[1] % 2 != 0:\n-            raise ValueError(\n-                \"This collator requires that sequence lengths be even to create a leakage-free perm_mask. Please see\"\n-                \" relevant comments in source code for details.\"\n-            )\n-\n-        labels = tf.identity(inputs)\n-        # Creating the mask and target_mapping tensors\n-        masked_indices = np.full(labels.shape.as_list(), 0, dtype=bool)\n-        labels_shape = tf.shape(labels)\n-        target_mapping = np.zeros((labels_shape[0], labels_shape[1], labels_shape[1]), dtype=np.float32)\n-\n-        for i in range(len(labels)):\n-            # Start from the beginning of the sequence by setting `cur_len = 0` (number of tokens processed so far).\n-            cur_len = 0\n-            max_len = tf.shape(labels)[1]\n-\n-            while cur_len < max_len:\n-                # Sample a `span_length` from the interval `[1, max_span_length]` (length of span of tokens to be masked)\n-                span_length = randint(1, self.max_span_length + 1)\n-                # Reserve a context of length `context_length = span_length / plm_probability` to surround the span to be masked\n-                context_length = int(span_length / self.plm_probability)\n-                # Sample a starting point `start_index` from the interval `[cur_len, cur_len + context_length - span_length]` and mask tokens `start_index:start_index + span_length`\n-                start_index = cur_len + randint(0, context_length - span_length + 1)\n-                masked_indices[i, start_index : start_index + span_length] = 1\n-                # Set `cur_len = cur_len + context_length`\n-                cur_len += context_length\n-\n-            # Since we're replacing non-masked tokens with -100 in the labels tensor instead of skipping them altogether,\n-            # the i-th predict corresponds to the i-th token.\n-            target_mapping[i] = np.eye(labels_shape[1])\n-        masked_indices = tf.cast(tf.convert_to_tensor(masked_indices), dtype=tf.bool)\n-        target_mapping = tf.convert_to_tensor(target_mapping)\n-        special_tokens_mask = tf.convert_to_tensor(\n-            [\n-                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True)\n-                for val in labels.numpy().tolist()\n-            ],\n-        )\n-        special_tokens_mask = tf.cast(special_tokens_mask, dtype=tf.bool)\n-        masked_indices = masked_indices & ~special_tokens_mask\n-        if self.tokenizer.pad_token is not None:\n-            padding_mask = labels == self.tokenizer.pad_token_id\n-            masked_indices = masked_indices & ~padding_mask\n-\n-        # Mask indicating non-functional tokens, where functional tokens are [SEP], [CLS], padding, etc.\n-        non_func_mask = ~(padding_mask | special_tokens_mask)\n-\n-        inputs = tf.where(masked_indices, self.tokenizer.mask_token_id, inputs)\n-        labels = tf.where(masked_indices, labels, -100)  # We only compute loss on masked tokens\n-\n-        perm_mask = []\n-\n-        for i in range(len(labels)):\n-            # Generate permutation indices i.e. sample a random factorisation order for the sequence. This will\n-            # determine which tokens a given token can attend to (encoded in `perm_mask`).\n-            # Note: Length of token sequence being permuted has to be less than or equal to reused sequence length\n-            # (see documentation for `mems`), otherwise information may leak through due to reuse. In this implementation,\n-            # we assume that reused length is half of sequence length and permutation length is equal to reused length.\n-            # This requires that the sequence length be even.\n-\n-            # Create a linear factorisation order\n-            # tf.range is the equivalent of torch.arange\n-            perm_index = tf.range(labels_shape[1])\n-            # Split this into two halves, assuming that half the sequence is reused each time\n-            perm_index = tf.transpose(tf.reshape(perm_index, (-1, labels_shape[1] // 2)))\n-            # Permute the two halves such that they do not cross over\n-            perm_index = tf.random.shuffle(perm_index)  # Shuffles along the first dimension\n-            # Flatten this out into the desired permuted factorisation order\n-            perm_index = tf.reshape(tf.transpose(perm_index), (-1,))\n-            # Set the permutation indices of non-masked (non-functional) tokens to the\n-            # smallest index (-1) so that:\n-            # (1) They can be seen by all other positions\n-            # (2) They cannot see masked positions, so there won't be information leak\n-            perm_index = tf.where(~masked_indices[i] & non_func_mask[i], -1, perm_index)\n-            # The logic for whether the i-th token can attend on the j-th token based on the factorisation order:\n-            # 0 (can attend): If perm_index[i] > perm_index[j] or j is neither masked nor a functional token\n-            # 1 (cannot attend): If perm_index[i] <= perm_index[j] and j is either masked or a functional token\n-            perm_mask.append(\n-                (tf.reshape(perm_index, (labels_shape[1], 1)) <= tf.reshape(perm_index, (1, labels_shape[1])))\n-                & masked_indices[i]\n-            )\n-        perm_mask = tf.stack(perm_mask, axis=0)\n-\n-        return tf.cast(inputs, tf.int64), tf.cast(perm_mask, tf.float32), target_mapping, tf.cast(labels, tf.int64)\n-\n     def numpy_mask_tokens(self, inputs: Any) -> tuple[Any, Any, Any, Any]:\n         \"\"\"\n         The masked tokens to be predicted for a particular sequence are determined by the following algorithm:"
        },
        {
            "sha": "808eb1e50578814c090b3bae26e1aab3e1d04959",
            "filename": "src/transformers/data/datasets/glue.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fdata%2Fdatasets%2Fglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fdata%2Fdatasets%2Fglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fdatasets%2Fglue.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -69,10 +69,6 @@ class Split(Enum):\n \n \n class GlueDataset(Dataset):\n-    \"\"\"\n-    This will be superseded by a framework-agnostic approach soon.\n-    \"\"\"\n-\n     args: GlueDataTrainingArguments\n     output_mode: str\n     features: list[InputFeatures]"
        },
        {
            "sha": "85d7e5360df36a3cac9a639398fff420b8189211",
            "filename": "src/transformers/data/datasets/language_modeling.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fdata%2Fdatasets%2Flanguage_modeling.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fdata%2Fdatasets%2Flanguage_modeling.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fdatasets%2Flanguage_modeling.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -38,10 +38,6 @@\n \n \n class TextDataset(Dataset):\n-    \"\"\"\n-    This will be superseded by a framework-agnostic approach soon.\n-    \"\"\"\n-\n     def __init__(\n         self,\n         tokenizer: PreTrainedTokenizer,\n@@ -111,10 +107,6 @@ def __getitem__(self, i) -> torch.Tensor:\n \n \n class LineByLineTextDataset(Dataset):\n-    \"\"\"\n-    This will be superseded by a framework-agnostic approach soon.\n-    \"\"\"\n-\n     def __init__(self, tokenizer: PreTrainedTokenizer, file_path: str, block_size: int):\n         warnings.warn(\n             DEPRECATION_WARNING.format(\n@@ -144,10 +136,6 @@ def __getitem__(self, i) -> dict[str, torch.tensor]:\n \n \n class LineByLineWithRefDataset(Dataset):\n-    \"\"\"\n-    This will be superseded by a framework-agnostic approach soon.\n-    \"\"\"\n-\n     def __init__(self, tokenizer: PreTrainedTokenizer, file_path: str, block_size: int, ref_path: str):\n         warnings.warn(\n             DEPRECATION_WARNING.format(\n@@ -344,10 +332,6 @@ def __getitem__(self, i) -> dict[str, torch.tensor]:\n \n \n class TextDatasetForNextSentencePrediction(Dataset):\n-    \"\"\"\n-    This will be superseded by a framework-agnostic approach soon.\n-    \"\"\"\n-\n     def __init__(\n         self,\n         tokenizer: PreTrainedTokenizer,"
        },
        {
            "sha": "d96d8224d6b90502286c7de37ce3ec0f17c2f18c",
            "filename": "src/transformers/data/datasets/squad.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fdata%2Fdatasets%2Fsquad.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fdata%2Fdatasets%2Fsquad.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fdatasets%2Fsquad.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -107,10 +107,6 @@ class Split(Enum):\n \n \n class SquadDataset(Dataset):\n-    \"\"\"\n-    This will be superseded by a framework-agnostic approach soon.\n-    \"\"\"\n-\n     args: SquadDataTrainingArguments\n     features: list[SquadFeatures]\n     mode: Split"
        },
        {
            "sha": "abf03c917202ad531b00b13a157b98b789bdb21d",
            "filename": "src/transformers/data/processors/glue.py",
            "status": "modified",
            "additions": 4,
            "deletions": 47,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fdata%2Fprocessors%2Fglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fdata%2Fprocessors%2Fglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fprocessors%2Fglue.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -17,18 +17,14 @@\n \n import os\n import warnings\n-from dataclasses import asdict\n from enum import Enum\n from typing import Optional, Union\n \n from ...tokenization_utils import PreTrainedTokenizer\n-from ...utils import is_tf_available, logging\n+from ...utils import logging\n from .utils import DataProcessor, InputExample, InputFeatures\n \n \n-if is_tf_available():\n-    import tensorflow as tf\n-\n logger = logging.get_logger(__name__)\n \n DEPRECATION_WARNING = (\n@@ -39,7 +35,7 @@\n \n \n def glue_convert_examples_to_features(\n-    examples: Union[list[InputExample], \"tf.data.Dataset\"],\n+    examples: list[InputExample],\n     tokenizer: PreTrainedTokenizer,\n     max_length: Optional[int] = None,\n     task=None,\n@@ -50,62 +46,23 @@ def glue_convert_examples_to_features(\n     Loads a data file into a list of `InputFeatures`\n \n     Args:\n-        examples: List of `InputExamples` or `tf.data.Dataset` containing the examples.\n+        examples: List of `InputExamples` containing the examples.\n         tokenizer: Instance of a tokenizer that will tokenize the examples\n         max_length: Maximum example length. Defaults to the tokenizer's max_len\n         task: GLUE task\n         label_list: List of labels. Can be obtained from the processor using the `processor.get_labels()` method\n         output_mode: String indicating the output mode. Either `regression` or `classification`\n \n     Returns:\n-        If the `examples` input is a `tf.data.Dataset`, will return a `tf.data.Dataset` containing the task-specific\n-        features. If the input is a list of `InputExamples`, will return a list of task-specific `InputFeatures` which\n-        can be fed to the model.\n+        Will return a list of task-specific `InputFeatures` which can be fed to the model.\n \n     \"\"\"\n     warnings.warn(DEPRECATION_WARNING.format(\"function\"), FutureWarning)\n-    if is_tf_available() and isinstance(examples, tf.data.Dataset):\n-        if task is None:\n-            raise ValueError(\"When calling glue_convert_examples_to_features from TF, the task parameter is required.\")\n-        return _tf_glue_convert_examples_to_features(examples, tokenizer, max_length=max_length, task=task)\n     return _glue_convert_examples_to_features(\n         examples, tokenizer, max_length=max_length, task=task, label_list=label_list, output_mode=output_mode\n     )\n \n \n-if is_tf_available():\n-\n-    def _tf_glue_convert_examples_to_features(\n-        examples: tf.data.Dataset,\n-        tokenizer: PreTrainedTokenizer,\n-        task=str,\n-        max_length: Optional[int] = None,\n-    ) -> tf.data.Dataset:\n-        \"\"\"\n-        Returns:\n-            A `tf.data.Dataset` containing the task-specific features.\n-\n-        \"\"\"\n-        processor = glue_processors[task]()\n-        examples = [processor.tfds_map(processor.get_example_from_tensor_dict(example)) for example in examples]\n-        features = glue_convert_examples_to_features(examples, tokenizer, max_length=max_length, task=task)\n-        label_type = tf.float32 if task == \"sts-b\" else tf.int64\n-\n-        def gen():\n-            for ex in features:\n-                d = {k: v for k, v in asdict(ex).items() if v is not None}\n-                label = d.pop(\"label\")\n-                yield (d, label)\n-\n-        input_names = tokenizer.model_input_names\n-\n-        return tf.data.Dataset.from_generator(\n-            gen,\n-            (dict.fromkeys(input_names, tf.int32), label_type),\n-            ({k: tf.TensorShape([None]) for k in input_names}, tf.TensorShape([])),\n-        )\n-\n-\n def _glue_convert_examples_to_features(\n     examples: list[InputExample],\n     tokenizer: PreTrainedTokenizer,"
        },
        {
            "sha": "e8af1549a86e51d9a6a8d8627e9ad7c47e723cd0",
            "filename": "src/transformers/data/processors/squad.py",
            "status": "modified",
            "additions": 3,
            "deletions": 110,
            "changes": 113,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fdata%2Fprocessors%2Fsquad.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fdata%2Fprocessors%2Fsquad.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fprocessors%2Fsquad.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -24,7 +24,7 @@\n \n from ...models.bert.tokenization_bert import whitespace_tokenize\n from ...tokenization_utils_base import BatchEncoding, PreTrainedTokenizerBase, TruncationStrategy\n-from ...utils import is_tf_available, is_torch_available, is_torch_hpu_available, logging\n+from ...utils import is_torch_available, is_torch_hpu_available, logging\n from .utils import DataProcessor\n \n \n@@ -36,8 +36,6 @@\n     import torch\n     from torch.utils.data import TensorDataset\n \n-if is_tf_available():\n-    import tensorflow as tf\n \n logger = logging.get_logger(__name__)\n \n@@ -244,7 +242,6 @@ def squad_convert_example_to_features(\n         cls_index = span[\"input_ids\"].index(tokenizer.cls_token_id)\n \n         # p_mask: mask with 1 for token than cannot be in the answer (0 for token which can be in an answer)\n-        # Original TF implementation also keep the classification token (set to 0)\n         p_mask = np.ones_like(span[\"token_type_ids\"])\n         if tokenizer.padding_side == \"right\":\n             p_mask[len(truncated_query) + sequence_added_tokens :] = 0\n@@ -338,8 +335,8 @@ def squad_convert_examples_to_features(\n         max_query_length: The maximum length of the query.\n         is_training: whether to create features for model evaluation or model training.\n         padding_strategy: Default to \"max_length\". Which padding strategy to use\n-        return_dataset: Default False. Either 'pt' or 'tf'.\n-            if 'pt': returns a torch.data.TensorDataset, if 'tf': returns a tf.data.Dataset\n+        return_dataset: Default False. Can also be 'pt'.\n+            if 'pt': returns a torch.data.TensorDataset.\n         threads: multiple processing threads.\n \n \n@@ -430,110 +427,6 @@ def squad_convert_examples_to_features(\n             )\n \n         return features, dataset\n-    elif return_dataset == \"tf\":\n-        if not is_tf_available():\n-            raise RuntimeError(\"TensorFlow must be installed to return a TensorFlow dataset.\")\n-\n-        def gen():\n-            for i, ex in enumerate(features):\n-                if ex.token_type_ids is None:\n-                    yield (\n-                        {\n-                            \"input_ids\": ex.input_ids,\n-                            \"attention_mask\": ex.attention_mask,\n-                            \"feature_index\": i,\n-                            \"qas_id\": ex.qas_id,\n-                        },\n-                        {\n-                            \"start_positions\": ex.start_position,\n-                            \"end_positions\": ex.end_position,\n-                            \"cls_index\": ex.cls_index,\n-                            \"p_mask\": ex.p_mask,\n-                            \"is_impossible\": ex.is_impossible,\n-                        },\n-                    )\n-                else:\n-                    yield (\n-                        {\n-                            \"input_ids\": ex.input_ids,\n-                            \"attention_mask\": ex.attention_mask,\n-                            \"token_type_ids\": ex.token_type_ids,\n-                            \"feature_index\": i,\n-                            \"qas_id\": ex.qas_id,\n-                        },\n-                        {\n-                            \"start_positions\": ex.start_position,\n-                            \"end_positions\": ex.end_position,\n-                            \"cls_index\": ex.cls_index,\n-                            \"p_mask\": ex.p_mask,\n-                            \"is_impossible\": ex.is_impossible,\n-                        },\n-                    )\n-\n-        # Why have we split the batch into a tuple? PyTorch just has a list of tensors.\n-        if \"token_type_ids\" in tokenizer.model_input_names:\n-            train_types = (\n-                {\n-                    \"input_ids\": tf.int32,\n-                    \"attention_mask\": tf.int32,\n-                    \"token_type_ids\": tf.int32,\n-                    \"feature_index\": tf.int64,\n-                    \"qas_id\": tf.string,\n-                },\n-                {\n-                    \"start_positions\": tf.int64,\n-                    \"end_positions\": tf.int64,\n-                    \"cls_index\": tf.int64,\n-                    \"p_mask\": tf.int32,\n-                    \"is_impossible\": tf.int32,\n-                },\n-            )\n-\n-            train_shapes = (\n-                {\n-                    \"input_ids\": tf.TensorShape([None]),\n-                    \"attention_mask\": tf.TensorShape([None]),\n-                    \"token_type_ids\": tf.TensorShape([None]),\n-                    \"feature_index\": tf.TensorShape([]),\n-                    \"qas_id\": tf.TensorShape([]),\n-                },\n-                {\n-                    \"start_positions\": tf.TensorShape([]),\n-                    \"end_positions\": tf.TensorShape([]),\n-                    \"cls_index\": tf.TensorShape([]),\n-                    \"p_mask\": tf.TensorShape([None]),\n-                    \"is_impossible\": tf.TensorShape([]),\n-                },\n-            )\n-        else:\n-            train_types = (\n-                {\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"feature_index\": tf.int64, \"qas_id\": tf.string},\n-                {\n-                    \"start_positions\": tf.int64,\n-                    \"end_positions\": tf.int64,\n-                    \"cls_index\": tf.int64,\n-                    \"p_mask\": tf.int32,\n-                    \"is_impossible\": tf.int32,\n-                },\n-            )\n-\n-            train_shapes = (\n-                {\n-                    \"input_ids\": tf.TensorShape([None]),\n-                    \"attention_mask\": tf.TensorShape([None]),\n-                    \"feature_index\": tf.TensorShape([]),\n-                    \"qas_id\": tf.TensorShape([]),\n-                },\n-                {\n-                    \"start_positions\": tf.TensorShape([]),\n-                    \"end_positions\": tf.TensorShape([]),\n-                    \"cls_index\": tf.TensorShape([]),\n-                    \"p_mask\": tf.TensorShape([None]),\n-                    \"is_impossible\": tf.TensorShape([]),\n-                },\n-            )\n-\n-        return tf.data.Dataset.from_generator(gen, train_types, train_shapes)\n     else:\n         return features\n "
        },
        {
            "sha": "63be55b558f92ff4659ab10cb858388399a3ed86",
            "filename": "src/transformers/data/processors/utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 21,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fdata%2Fprocessors%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fdata%2Fprocessors%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fprocessors%2Futils.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -20,7 +20,7 @@\n from dataclasses import dataclass\n from typing import Optional, Union\n \n-from ...utils import is_tf_available, is_torch_available, logging\n+from ...utils import is_torch_available, logging\n \n \n logger = logging.get_logger(__name__)\n@@ -82,7 +82,7 @@ class DataProcessor:\n \n     def get_example_from_tensor_dict(self, tensor_dict):\n         \"\"\"\n-        Gets an example from a dict with tensorflow tensors.\n+        Gets an example from a dict.\n \n         Args:\n             tensor_dict: Keys and values should match the corresponding Glue\n@@ -251,9 +251,7 @@ def get_features(\n                 values)\n \n         Returns:\n-            If the `examples` input is a `tf.data.Dataset`, will return a `tf.data.Dataset` containing the\n-            task-specific features. If the input is a list of `InputExamples`, will return a list of task-specific\n-            `InputFeatures` which can be fed to the model.\n+            Will return a list of task-specific `InputFeatures` which can be fed to the model.\n \n         \"\"\"\n         if max_length is None:\n@@ -315,21 +313,6 @@ def get_features(\n \n         if return_tensors is None:\n             return features\n-        elif return_tensors == \"tf\":\n-            if not is_tf_available():\n-                raise RuntimeError(\"return_tensors set to 'tf' but TensorFlow 2.0 can't be imported\")\n-            import tensorflow as tf\n-\n-            def gen():\n-                for ex in features:\n-                    yield ({\"input_ids\": ex.input_ids, \"attention_mask\": ex.attention_mask}, ex.label)\n-\n-            dataset = tf.data.Dataset.from_generator(\n-                gen,\n-                ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32}, tf.int64),\n-                ({\"input_ids\": tf.TensorShape([None]), \"attention_mask\": tf.TensorShape([None])}, tf.TensorShape([])),\n-            )\n-            return dataset\n         elif return_tensors == \"pt\":\n             if not is_torch_available():\n                 raise RuntimeError(\"return_tensors set to 'pt' but PyTorch can't be imported\")\n@@ -346,4 +329,4 @@ def gen():\n             dataset = TensorDataset(all_input_ids, all_attention_mask, all_labels)\n             return dataset\n         else:\n-            raise ValueError(\"return_tensors should be one of 'tf' or 'pt'\")\n+            raise ValueError(\"return_tensors should be `'pt'` or `None`\")"
        },
        {
            "sha": "28a9f84b92a82bac28f9ab8e1d0389f42c4bf1d7",
            "filename": "src/transformers/dependency_versions_table.py",
            "status": "modified",
            "additions": 1,
            "deletions": 11,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fdependency_versions_table.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fdependency_versions_table.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdependency_versions_table.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -18,7 +18,6 @@\n     \"faiss-cpu\": \"faiss-cpu\",\n     \"fastapi\": \"fastapi\",\n     \"filelock\": \"filelock\",\n-    \"flax\": \"flax>=0.4.1,<=0.7.0\",\n     \"ftfy\": \"ftfy\",\n     \"fugashi\": \"fugashi>=1.0\",\n     \"GitPython\": \"GitPython<3.1.19\",\n@@ -27,12 +26,8 @@\n     \"huggingface-hub\": \"huggingface-hub>=0.34.0,<1.0\",\n     \"importlib_metadata\": \"importlib_metadata\",\n     \"ipadic\": \"ipadic>=1.0.0,<2.0\",\n-    \"jax\": \"jax>=0.4.1,<=0.4.13\",\n-    \"jaxlib\": \"jaxlib>=0.4.1,<=0.4.13\",\n     \"jinja2\": \"jinja2>=3.1.0\",\n     \"kenlm\": \"kenlm\",\n-    \"keras\": \"keras>2.9,<2.16\",\n-    \"keras-nlp\": \"keras-nlp>=0.3.1,<0.14.0\",\n     \"kernels\": \"kernels>=0.6.1,<=0.9\",\n     \"librosa\": \"librosa\",\n     \"natten\": \"natten>=0.14.6,<0.15.0\",\n@@ -75,18 +70,13 @@\n     \"sagemaker\": \"sagemaker>=2.31.0\",\n     \"schedulefree\": \"schedulefree>=1.2.6\",\n     \"scikit-learn\": \"scikit-learn\",\n-    \"scipy\": \"scipy<1.13.0\",\n+    \"scipy\": \"scipy\",\n     \"sentencepiece\": \"sentencepiece>=0.1.91,!=0.1.92\",\n     \"sigopt\": \"sigopt\",\n     \"starlette\": \"starlette\",\n     \"sudachipy\": \"sudachipy>=0.6.6\",\n     \"sudachidict_core\": \"sudachidict_core>=20220729\",\n     \"tensorboard\": \"tensorboard\",\n-    \"tensorflow-cpu\": \"tensorflow-cpu>2.9,<2.16\",\n-    \"tensorflow\": \"tensorflow>2.9,<2.16\",\n-    \"tensorflow-text\": \"tensorflow-text<2.16\",\n-    \"tensorflow-probability\": \"tensorflow-probability<0.24\",\n-    \"tf2onnx\": \"tf2onnx\",\n     \"timeout-decorator\": \"timeout-decorator\",\n     \"tiktoken\": \"tiktoken\",\n     \"timm\": \"timm<=1.0.19,!=1.0.18\","
        },
        {
            "sha": "1a48062cb5c1d489156f3ac4f90650b6aa594da7",
            "filename": "src/transformers/feature_extraction_sequence_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 9,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Ffeature_extraction_sequence_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Ffeature_extraction_sequence_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ffeature_extraction_sequence_utils.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -20,7 +20,7 @@\n import numpy as np\n \n from .feature_extraction_utils import BatchFeature, FeatureExtractionMixin\n-from .utils import PaddingStrategy, TensorType, is_tf_tensor, is_torch_tensor, logging, to_numpy\n+from .utils import PaddingStrategy, TensorType, is_torch_tensor, logging, to_numpy\n \n \n logger = logging.get_logger(__name__)\n@@ -74,7 +74,7 @@ def pad(\n \n         <Tip>\n \n-        If the `processed_features` passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the\n+        If the `processed_features` passed are dictionary of numpy arrays or PyTorch tensors  the\n         result will use the same type unless you provide a different tensor type with `return_tensors`. In the case of\n         PyTorch tensors, you will lose the specific device of your tensors however.\n \n@@ -87,7 +87,7 @@ def pad(\n                 list[float]]]*) so you can use this method during preprocessing as well as in a PyTorch Dataloader\n                 collate function.\n \n-                Instead of `list[float]` you can have tensors (numpy arrays, PyTorch tensors or TensorFlow tensors),\n+                Instead of `list[float]` you can have tensors (numpy arrays or PyTorch tensors),\n                 see the note above for the return type.\n             padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n                 Select a strategy to pad the returned sequences (according to the model's padding side and padding\n@@ -116,7 +116,6 @@ def pad(\n             return_tensors (`str` or [`~utils.TensorType`], *optional*):\n                 If set, will return tensors instead of list of python integers. Acceptable values are:\n \n-                - `'tf'`: Return TensorFlow `tf.constant` objects.\n                 - `'pt'`: Return PyTorch `torch.Tensor` objects.\n                 - `'np'`: Return Numpy `np.ndarray` objects.\n         \"\"\"\n@@ -145,7 +144,7 @@ def pad(\n                 processed_features[\"attention_mask\"] = []\n             return processed_features\n \n-        # If we have PyTorch/TF tensors or lists as inputs, we cast them as Numpy arrays\n+        # If we have PyTorch tensors or lists as inputs, we cast them as Numpy arrays\n         # and rebuild them afterwards if no return_tensors is specified\n         # Note that we lose the specific device the tensor may be on for PyTorch\n \n@@ -159,16 +158,14 @@ def pad(\n                 first_element = required_input[index][0]\n \n         if return_tensors is None:\n-            if is_tf_tensor(first_element):\n-                return_tensors = \"tf\"\n-            elif is_torch_tensor(first_element):\n+            if is_torch_tensor(first_element):\n                 return_tensors = \"pt\"\n             elif isinstance(first_element, (int, float, list, tuple, np.ndarray)):\n                 return_tensors = \"np\"\n             else:\n                 raise ValueError(\n                     f\"type of {first_element} unknown: {type(first_element)}. \"\n-                    \"Should be one of a python, numpy, pytorch or tensorflow object.\"\n+                    \"Should be one of a python, numpy, or pytorch object.\"\n                 )\n \n         for key, value in processed_features.items():"
        },
        {
            "sha": "fd9eb56941b91b9473ca0b091ebb6ea6e4ea5d73",
            "filename": "src/transformers/feature_extraction_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 30,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ffeature_extraction_utils.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -32,12 +32,9 @@\n     TensorType,\n     copy_func,\n     download_url,\n-    is_flax_available,\n-    is_jax_tensor,\n     is_numpy_array,\n     is_offline_mode,\n     is_remote_url,\n-    is_tf_available,\n     is_torch_available,\n     is_torch_device,\n     is_torch_dtype,\n@@ -71,7 +68,7 @@ class BatchFeature(UserDict):\n             Dictionary of lists/arrays/tensors returned by the __call__/pad methods ('input_values', 'attention_mask',\n             etc.).\n         tensor_type (`Union[None, str, TensorType]`, *optional*):\n-            You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at\n+            You can give a tensor_type here to convert the lists of integers in PyTorch/Numpy Tensors at\n             initialization.\n     \"\"\"\n \n@@ -110,21 +107,7 @@ def _get_is_as_tensor_fns(self, tensor_type: Optional[Union[str, TensorType]] =\n         if not isinstance(tensor_type, TensorType):\n             tensor_type = TensorType(tensor_type)\n \n-        # Get a function reference for the correct framework\n-        if tensor_type == TensorType.TENSORFLOW:\n-            logger.warning_once(\n-                \"TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We \"\n-                \"recommend migrating to PyTorch classes or pinning your version of Transformers.\"\n-            )\n-            if not is_tf_available():\n-                raise ImportError(\n-                    \"Unable to convert output to TensorFlow tensors format, TensorFlow is not installed.\"\n-                )\n-            import tensorflow as tf\n-\n-            as_tensor = tf.constant\n-            is_tensor = tf.is_tensor\n-        elif tensor_type == TensorType.PYTORCH:\n+        if tensor_type == TensorType.PYTORCH:\n             if not is_torch_available():\n                 raise ImportError(\"Unable to convert output to PyTorch tensors format, PyTorch is not installed.\")\n             import torch  # noqa\n@@ -145,17 +128,6 @@ def as_tensor(value):\n                     return torch.tensor(value)\n \n             is_tensor = torch.is_tensor\n-        elif tensor_type == TensorType.JAX:\n-            logger.warning_once(\n-                \"TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We \"\n-                \"recommend migrating to PyTorch classes or pinning your version of Transformers.\"\n-            )\n-            if not is_flax_available():\n-                raise ImportError(\"Unable to convert output to JAX tensors format, JAX is not installed.\")\n-            import jax.numpy as jnp  # noqa: F811\n-\n-            as_tensor = jnp.array\n-            is_tensor = is_jax_tensor\n         else:\n \n             def as_tensor(value, dtype=None):"
        },
        {
            "sha": "91d7974b55c1cfb6ae213065b9c58199114cc98d",
            "filename": "src/transformers/file_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Ffile_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Ffile_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ffile_utils.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -31,7 +31,6 @@\n     ENV_VARS_TRUE_AND_AUTO_VALUES,\n     ENV_VARS_TRUE_VALUES,\n     FEATURE_EXTRACTOR_NAME,\n-    FLAX_WEIGHTS_NAME,\n     HF_MODULES_CACHE,\n     HUGGINGFACE_CO_PREFIX,\n     HUGGINGFACE_CO_RESOLVE_ENDPOINT,\n@@ -42,14 +41,9 @@\n     S3_BUCKET_PREFIX,\n     SENTENCEPIECE_UNDERLINE,\n     SPIECE_UNDERLINE,\n-    TF2_WEIGHTS_NAME,\n-    TF_WEIGHTS_NAME,\n     TORCH_FX_REQUIRED_VERSION,\n     TRANSFORMERS_CACHE,\n     TRANSFORMERS_DYNAMIC_MODULE_NAME,\n-    USE_JAX,\n-    USE_TF,\n-    USE_TORCH,\n     WEIGHTS_INDEX_NAME,\n     WEIGHTS_NAME,\n     ContextManagers,\n@@ -79,7 +73,6 @@\n     is_datasets_available,\n     is_detectron2_available,\n     is_faiss_available,\n-    is_flax_available,\n     is_ftfy_available,\n     is_g2p_en_available,\n     is_in_notebook,\n@@ -106,9 +99,6 @@\n     is_spacy_available,\n     is_speech_available,\n     is_tensor,\n-    is_tensorflow_probability_available,\n-    is_tf2onnx_available,\n-    is_tf_available,\n     is_timm_available,\n     is_tokenizers_available,\n     is_torch_available,"
        },
        {
            "sha": "f27cef41afd6bb6635fc3e935c893197febe64f6",
            "filename": "src/transformers/generation/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 126,
            "changes": 127,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fgeneration%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fgeneration%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2F__init__.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -14,7 +14,7 @@\n \n from typing import TYPE_CHECKING\n \n-from ..utils import OptionalDependencyNotAvailable, _LazyModule, is_flax_available, is_tf_available, is_torch_available\n+from ..utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available\n \n \n _import_structure = {\n@@ -123,71 +123,6 @@\n         \"SynthIDTextWatermarkDetector\",\n     ]\n \n-try:\n-    if not is_tf_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tf_logits_process\"] = [\n-        \"TFForcedBOSTokenLogitsProcessor\",\n-        \"TFForcedEOSTokenLogitsProcessor\",\n-        \"TFForceTokensLogitsProcessor\",\n-        \"TFLogitsProcessor\",\n-        \"TFLogitsProcessorList\",\n-        \"TFLogitsWarper\",\n-        \"TFMinLengthLogitsProcessor\",\n-        \"TFNoBadWordsLogitsProcessor\",\n-        \"TFNoRepeatNGramLogitsProcessor\",\n-        \"TFRepetitionPenaltyLogitsProcessor\",\n-        \"TFSuppressTokensAtBeginLogitsProcessor\",\n-        \"TFSuppressTokensLogitsProcessor\",\n-        \"TFTemperatureLogitsWarper\",\n-        \"TFTopKLogitsWarper\",\n-        \"TFTopPLogitsWarper\",\n-    ]\n-    _import_structure[\"tf_utils\"] = [\n-        \"TFGenerationMixin\",\n-        \"TFGreedySearchDecoderOnlyOutput\",\n-        \"TFGreedySearchEncoderDecoderOutput\",\n-        \"TFSampleEncoderDecoderOutput\",\n-        \"TFSampleDecoderOnlyOutput\",\n-        \"TFBeamSearchEncoderDecoderOutput\",\n-        \"TFBeamSearchDecoderOnlyOutput\",\n-        \"TFBeamSampleEncoderDecoderOutput\",\n-        \"TFBeamSampleDecoderOnlyOutput\",\n-        \"TFContrastiveSearchEncoderDecoderOutput\",\n-        \"TFContrastiveSearchDecoderOnlyOutput\",\n-    ]\n-\n-try:\n-    if not is_flax_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"flax_logits_process\"] = [\n-        \"FlaxForcedBOSTokenLogitsProcessor\",\n-        \"FlaxForcedEOSTokenLogitsProcessor\",\n-        \"FlaxForceTokensLogitsProcessor\",\n-        \"FlaxLogitsProcessor\",\n-        \"FlaxLogitsProcessorList\",\n-        \"FlaxLogitsWarper\",\n-        \"FlaxMinLengthLogitsProcessor\",\n-        \"FlaxSuppressTokensAtBeginLogitsProcessor\",\n-        \"FlaxSuppressTokensLogitsProcessor\",\n-        \"FlaxTemperatureLogitsWarper\",\n-        \"FlaxTopKLogitsWarper\",\n-        \"FlaxTopPLogitsWarper\",\n-        \"FlaxWhisperTimeStampLogitsProcessor\",\n-        \"FlaxNoRepeatNGramLogitsProcessor\",\n-    ]\n-    _import_structure[\"flax_utils\"] = [\n-        \"FlaxGenerationMixin\",\n-        \"FlaxGreedySearchOutput\",\n-        \"FlaxSampleOutput\",\n-        \"FlaxBeamSearchOutput\",\n-    ]\n \n if TYPE_CHECKING:\n     from .configuration_utils import (\n@@ -283,66 +218,6 @@\n             WatermarkDetectorOutput,\n         )\n \n-    try:\n-        if not is_tf_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tf_logits_process import (\n-            TFForcedBOSTokenLogitsProcessor,\n-            TFForcedEOSTokenLogitsProcessor,\n-            TFForceTokensLogitsProcessor,\n-            TFLogitsProcessor,\n-            TFLogitsProcessorList,\n-            TFLogitsWarper,\n-            TFMinLengthLogitsProcessor,\n-            TFNoBadWordsLogitsProcessor,\n-            TFNoRepeatNGramLogitsProcessor,\n-            TFRepetitionPenaltyLogitsProcessor,\n-            TFSuppressTokensAtBeginLogitsProcessor,\n-            TFSuppressTokensLogitsProcessor,\n-            TFTemperatureLogitsWarper,\n-            TFTopKLogitsWarper,\n-            TFTopPLogitsWarper,\n-        )\n-        from .tf_utils import (\n-            TFBeamSampleDecoderOnlyOutput,\n-            TFBeamSampleEncoderDecoderOutput,\n-            TFBeamSearchDecoderOnlyOutput,\n-            TFBeamSearchEncoderDecoderOutput,\n-            TFContrastiveSearchDecoderOnlyOutput,\n-            TFContrastiveSearchEncoderDecoderOutput,\n-            TFGenerationMixin,\n-            TFGreedySearchDecoderOnlyOutput,\n-            TFGreedySearchEncoderDecoderOutput,\n-            TFSampleDecoderOnlyOutput,\n-            TFSampleEncoderDecoderOutput,\n-        )\n-\n-    try:\n-        if not is_flax_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .flax_logits_process import (\n-            FlaxForcedBOSTokenLogitsProcessor,\n-            FlaxForcedEOSTokenLogitsProcessor,\n-            FlaxForceTokensLogitsProcessor,\n-            FlaxLogitsProcessor,\n-            FlaxLogitsProcessorList,\n-            FlaxLogitsWarper,\n-            FlaxMinLengthLogitsProcessor,\n-            FlaxNoRepeatNGramLogitsProcessor,\n-            FlaxSuppressTokensAtBeginLogitsProcessor,\n-            FlaxSuppressTokensLogitsProcessor,\n-            FlaxTemperatureLogitsWarper,\n-            FlaxTopKLogitsWarper,\n-            FlaxTopPLogitsWarper,\n-            FlaxWhisperTimeStampLogitsProcessor,\n-        )\n-        from .flax_utils import FlaxBeamSearchOutput, FlaxGenerationMixin, FlaxGreedySearchOutput, FlaxSampleOutput\n else:\n     import sys\n "
        },
        {
            "sha": "08fa411dc6f5761eb52eea585cd6f7abf612edeb",
            "filename": "src/transformers/generation/flax_logits_process.py",
            "status": "removed",
            "additions": 0,
            "deletions": 544,
            "changes": 544,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fgeneration%2Fflax_logits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fgeneration%2Fflax_logits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fflax_logits_process.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,544 +0,0 @@\n-# coding=utf-8\n-# Copyright 2021 The HuggingFace Inc. team\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import inspect\n-\n-import jax\n-import jax.lax as lax\n-import jax.numpy as jnp\n-from jax.experimental import sparse\n-\n-from ..utils import add_start_docstrings\n-from ..utils.logging import get_logger\n-\n-\n-logger = get_logger(__name__)\n-\n-\n-LOGITS_PROCESSOR_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`jnp.ndarray` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary.\n-\n-            Indices can be obtained using [`PreTrainedTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        scores (`jnp.ndarray` of shape `(batch_size, config.vocab_size)`):\n-            Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam\n-            search or log softmax for each vocabulary token when using beam search\n-        kwargs (`dict[str, Any]`, *optional*):\n-            Additional logits processor specific kwargs.\n-\n-    Return:\n-        `jnp.ndarray` of shape `(batch_size, config.vocab_size)`: The processed prediction scores.\n-\n-\"\"\"\n-\n-\n-class FlaxLogitsProcessor:\n-    \"\"\"Abstract base class for all logit processors that can be applied during generation.\"\"\"\n-\n-    @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n-    def __call__(self, input_ids: jnp.ndarray, scores: jnp.ndarray) -> jnp.ndarray:\n-        \"\"\"Flax method for processing logits.\"\"\"\n-        raise NotImplementedError(\n-            f\"{self.__class__} is an abstract class. Only classes inheriting this class can be called.\"\n-        )\n-\n-\n-class FlaxLogitsWarper:\n-    \"\"\"Abstract base class for all logit warpers that can be applied during generation with multinomial sampling.\"\"\"\n-\n-    @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n-    def __call__(self, input_ids: jnp.ndarray, scores: jnp.ndarray) -> jnp.ndarray:\n-        \"\"\"Flax method for warping logits.\"\"\"\n-        raise NotImplementedError(\n-            f\"{self.__class__} is an abstract class. Only classes inheriting this class can be called.\"\n-        )\n-\n-\n-class FlaxLogitsProcessorList(list):\n-    \"\"\"\n-    This class can be used to create a list of [`FlaxLogitsProcessor`] or [`FlaxLogitsWarper`] to subsequently process\n-    a `scores` input tensor. This class inherits from list and adds a specific *__call__* method to apply each\n-    [`FlaxLogitsProcessor`] or [`FlaxLogitsWarper`] to the inputs.\n-    \"\"\"\n-\n-    @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n-    def __call__(self, input_ids: jnp.ndarray, scores: jnp.ndarray, cur_len: int, **kwargs) -> jnp.ndarray:\n-        for processor in self:\n-            function_args = inspect.signature(processor.__call__).parameters\n-            if len(function_args) > 3:\n-                if not all(arg in kwargs for arg in list(function_args.keys())[2:]):\n-                    raise ValueError(\n-                        f\"Make sure that all the required parameters: {list(function_args.keys())} for \"\n-                        f\"{processor.__class__} are passed to the logits processor.\"\n-                    )\n-                scores = processor(input_ids, scores, cur_len, **kwargs)\n-            else:\n-                scores = processor(input_ids, scores, cur_len)\n-        return scores\n-\n-\n-class FlaxTemperatureLogitsWarper(FlaxLogitsWarper):\n-    r\"\"\"\n-    [`FlaxLogitsWarper`] for temperature (exponential scaling output probability distribution).\n-\n-    Args:\n-        temperature (`float`):\n-            The value used to module the logits distribution.\n-    \"\"\"\n-\n-    def __init__(self, temperature: float):\n-        if not isinstance(temperature, float) or not (temperature > 0):\n-            raise ValueError(f\"`temperature` has to be a strictly positive float, but is {temperature}\")\n-\n-        self.temperature = temperature\n-\n-    def __call__(self, input_ids: jnp.ndarray, scores: jnp.ndarray, cur_len: int) -> jnp.ndarray:\n-        scores = scores / self.temperature\n-        return scores\n-\n-\n-class FlaxTopPLogitsWarper(FlaxLogitsWarper):\n-    \"\"\"\n-    [`FlaxLogitsWarper`] that performs top-p, i.e. restricting to top tokens summing to prob_cut_off <= prob_cut_off.\n-\n-    Args:\n-        top_p (`float`):\n-            If set to < 1, only the smallest set of most probable tokens with probabilities that add up to `top_p` or\n-            higher are kept for generation.\n-        filter_value (`float`, *optional*, defaults to -inf):\n-            All filtered values will be set to this float value.\n-        min_tokens_to_keep (`int`, *optional*, defaults to 1):\n-            Minimum number of tokens that cannot be filtered.\n-    \"\"\"\n-\n-    def __init__(self, top_p: float, filter_value: float = -float(\"Inf\"), min_tokens_to_keep: int = 1):\n-        if not isinstance(top_p, float) or (top_p < 0 or top_p > 1.0):\n-            raise ValueError(f\"`top_p` has to be a float > 0 and < 1, but is {top_p}\")\n-        if not isinstance(min_tokens_to_keep, int) or (min_tokens_to_keep < 1):\n-            raise ValueError(f\"`min_tokens_to_keep` has to be a positive integer, but is {min_tokens_to_keep}\")\n-\n-        self.top_p = top_p\n-        self.filter_value = filter_value\n-        self.min_tokens_to_keep = min_tokens_to_keep\n-\n-    def __call__(self, input_ids: jnp.ndarray, scores: jnp.ndarray, cur_len: int) -> jnp.ndarray:\n-        topk_scores, topk_indices = lax.top_k(scores, scores.shape[-1])\n-\n-        mask_scores = jnp.full_like(scores, self.filter_value)\n-        cumulative_probs = jax.nn.softmax(topk_scores, axis=-1).cumsum(axis=-1)\n-        score_mask = cumulative_probs < self.top_p\n-\n-        # include the token that is higher than top_p as well\n-        score_mask = jnp.roll(score_mask, 1)\n-        score_mask |= score_mask.at[:, 0].set(True)\n-\n-        # min tokens to keep\n-        score_mask = score_mask.at[:, : self.min_tokens_to_keep].set(True)\n-\n-        topk_next_scores = jnp.where(score_mask, topk_scores, mask_scores)\n-        next_scores = jax.lax.sort_key_val(topk_indices, topk_next_scores)[-1]\n-\n-        return next_scores\n-\n-\n-class FlaxTopKLogitsWarper(FlaxLogitsWarper):\n-    r\"\"\"\n-    [`FlaxLogitsWarper`] that performs top-k, i.e. restricting to the k highest probability elements.\n-\n-    Args:\n-        top_k (`int`):\n-            The number of highest probability vocabulary tokens to keep for top-k-filtering.\n-        filter_value (`float`, *optional*, defaults to -inf):\n-            All filtered values will be set to this float value.\n-        min_tokens_to_keep (`int`, *optional*, defaults to 1):\n-            Minimum number of tokens that cannot be filtered.\n-    \"\"\"\n-\n-    def __init__(self, top_k: int, filter_value: float = -float(\"Inf\"), min_tokens_to_keep: int = 1):\n-        if not isinstance(top_k, int) or top_k <= 0:\n-            raise ValueError(f\"`top_k` has to be a strictly positive integer, but is {top_k}\")\n-\n-        self.top_k = max(top_k, min_tokens_to_keep)\n-        self.filter_value = filter_value\n-\n-    def __call__(self, input_ids: jnp.ndarray, scores: jnp.ndarray, cur_len: int) -> jnp.ndarray:\n-        batch_size, vocab_size = scores.shape\n-        next_scores_flat = jnp.full(batch_size * vocab_size, self.filter_value)\n-\n-        topk = min(self.top_k, scores.shape[-1])  # Safety check\n-        topk_scores, topk_indices = lax.top_k(scores, topk)\n-        shift = jnp.broadcast_to((jnp.arange(batch_size) * vocab_size)[:, None], (batch_size, topk)).flatten()\n-        topk_scores_flat = topk_scores.flatten()\n-        topk_indices_flat = topk_indices.flatten() + shift\n-\n-        next_scores_flat = next_scores_flat.at[topk_indices_flat].set(topk_scores_flat)\n-        next_scores = next_scores_flat.reshape(batch_size, vocab_size)\n-        return next_scores\n-\n-\n-class FlaxForcedBOSTokenLogitsProcessor(FlaxLogitsProcessor):\n-    r\"\"\"\n-    [`FlaxLogitsProcessor`] that enforces the specified token as the first generated token.\n-\n-    Args:\n-        bos_token_id (`int`):\n-            The id of the token to force as the first generated token.\n-    \"\"\"\n-\n-    def __init__(self, bos_token_id: int):\n-        self.bos_token_id = bos_token_id\n-\n-    def __call__(self, input_ids: jnp.ndarray, scores: jnp.ndarray, cur_len: int) -> jnp.ndarray:\n-        new_scores = jnp.full(scores.shape, -float(\"inf\"))\n-\n-        apply_penalty = 1 - jnp.bool_(cur_len - 1)\n-\n-        scores = jnp.where(apply_penalty, new_scores.at[:, self.bos_token_id].set(0), scores)\n-\n-        return scores\n-\n-\n-class FlaxForcedEOSTokenLogitsProcessor(FlaxLogitsProcessor):\n-    r\"\"\"\n-    [`FlaxLogitsProcessor`] that enforces the specified token as the last generated token when `max_length` is reached.\n-\n-    Args:\n-        max_length (`int`):\n-            The maximum length of the sequence to be generated.\n-        eos_token_id (`int`):\n-            The id of the token to force as the last generated token when `max_length` is reached.\n-    \"\"\"\n-\n-    def __init__(self, max_length: int, eos_token_id: int):\n-        self.max_length = max_length\n-        self.eos_token_id = eos_token_id\n-\n-    def __call__(self, input_ids: jnp.ndarray, scores: jnp.ndarray, cur_len: int) -> jnp.ndarray:\n-        new_scores = jnp.full(scores.shape, -float(\"inf\"))\n-\n-        apply_penalty = 1 - jnp.bool_(cur_len - self.max_length + 1)\n-\n-        scores = jnp.where(apply_penalty, new_scores.at[:, self.eos_token_id].set(0), scores)\n-\n-        return scores\n-\n-\n-class FlaxMinLengthLogitsProcessor(FlaxLogitsProcessor):\n-    r\"\"\"\n-    [`FlaxLogitsProcessor`] enforcing a min-length by setting EOS probability to 0.\n-\n-    Args:\n-        min_length (`int`):\n-            The minimum length below which the score of `eos_token_id` is set to `-float(\"Inf\")`.\n-        eos_token_id (`int`):\n-            The id of the *end-of-sequence* token.\n-    \"\"\"\n-\n-    def __init__(self, min_length: int, eos_token_id: int):\n-        if not isinstance(min_length, int) or min_length < 0:\n-            raise ValueError(f\"`min_length` has to be a positive integer, but is {min_length}\")\n-\n-        if not isinstance(eos_token_id, int) or eos_token_id < 0:\n-            raise ValueError(f\"`eos_token_id` has to be a positive integer, but is {eos_token_id}\")\n-\n-        self.min_length = min_length\n-        self.eos_token_id = eos_token_id\n-\n-    def __call__(self, input_ids: jnp.ndarray, scores: jnp.ndarray, cur_len: int) -> jnp.ndarray:\n-        # create boolean flag to decide if min length penalty should be applied\n-        apply_penalty = 1 - jnp.clip(cur_len - self.min_length, 0, 1)\n-\n-        scores = jnp.where(apply_penalty, scores.at[:, self.eos_token_id].set(-float(\"inf\")), scores)\n-\n-        return scores\n-\n-\n-class FlaxSuppressTokensAtBeginLogitsProcessor(FlaxLogitsProcessor):\n-    r\"\"\"\n-    [`FlaxLogitsProcessor`] suppressing a list of tokens as soon as the `generate` function starts generating using\n-    `begin_index` tokens. This should ensure that the tokens defined by `begin_suppress_tokens` are not sampled at the\n-    beginning of the generation.\n-\n-    Args:\n-        begin_suppress_tokens (`list[int]`):\n-            Tokens to not sample.\n-        begin_index (`int`):\n-            Index where the tokens are suppressed.\n-    \"\"\"\n-\n-    def __init__(self, begin_suppress_tokens, begin_index):\n-        self.begin_suppress_tokens = list(begin_suppress_tokens)\n-        self.begin_index = begin_index\n-\n-    def __call__(self, input_ids, scores, cur_len: int):\n-        apply_penalty = 1 - jnp.bool_(cur_len - self.begin_index)\n-\n-        scores = jnp.where(apply_penalty, scores.at[:, self.begin_suppress_tokens].set(-float(\"inf\")), scores)\n-\n-        return scores\n-\n-\n-class FlaxSuppressTokensLogitsProcessor(FlaxLogitsProcessor):\n-    r\"\"\"\n-    [`FlaxLogitsProcessor`] suppressing a list of tokens at each decoding step. The processor will set their log probs\n-    to be `-inf` so they are not sampled.\n-\n-    Args:\n-        suppress_tokens (`list`):\n-            Tokens to not sample.\n-    \"\"\"\n-\n-    def __init__(self, suppress_tokens: list):\n-        self.suppress_tokens = list(suppress_tokens)\n-\n-    def __call__(self, input_ids: jnp.ndarray, scores: jnp.ndarray, cur_len: int) -> jnp.ndarray:\n-        scores = scores.at[..., self.suppress_tokens].set(-float(\"inf\"))\n-\n-        return scores\n-\n-\n-class FlaxForceTokensLogitsProcessor(FlaxLogitsProcessor):\n-    r\"\"\"\n-    [`FlaxLogitsProcessor`] that takes a list of pairs of integers which indicates a mapping from generation indices to\n-    token indices that will be forced before sampling. The processor will set their log probs to 0 and all other tokens\n-    to `-inf` so that they are sampled at their corresponding index.\n-\n-    Args:\n-        force_token_map (`list`):\n-            Map giving token ids and indices where they will be forced to be sampled.\n-    \"\"\"\n-\n-    def __init__(self, force_token_map):\n-        force_token_map = dict(force_token_map)\n-        # Converts the dictionary of format {index: token} containing the tokens to be forced to an array, where the\n-        # index of the array corresponds to the index of the token to be forced, for XLA compatibility.\n-        # Indexes without forced tokens will have a negative value.\n-        force_token_array = jnp.ones((max(force_token_map.keys()) + 1), dtype=jnp.int32) * -1\n-        for index, token in force_token_map.items():\n-            if token is not None:\n-                force_token_array = force_token_array.at[index].set(token)\n-        self.force_token_array = jnp.int32(force_token_array)\n-\n-    def __call__(self, input_ids: jnp.ndarray, scores: jnp.ndarray, cur_len: int) -> jnp.ndarray:\n-        def _force_token(generation_idx):\n-            batch_size = scores.shape[0]\n-            current_token = self.force_token_array[generation_idx]\n-\n-            new_scores = jnp.ones_like(scores, dtype=scores.dtype) * -float(\"inf\")\n-            updates = jnp.zeros((batch_size, 1), dtype=scores.dtype)\n-            new_scores = lax.dynamic_update_slice(new_scores, updates, (0, current_token))\n-            return new_scores\n-\n-        scores = lax.cond(\n-            cur_len >= self.force_token_array.shape[0],\n-            # If the current length is geq than the length of force_token_array, the processor does nothing.\n-            lambda: scores,\n-            # Otherwise, it may force a certain token.\n-            lambda: lax.cond(\n-                self.force_token_array[cur_len] >= 0,\n-                # Only valid (positive) tokens are forced\n-                lambda: _force_token(cur_len),\n-                # Otherwise, the processor does nothing.\n-                lambda: scores,\n-            ),\n-        )\n-        return scores\n-\n-\n-class FlaxWhisperTimeStampLogitsProcessor(FlaxLogitsProcessor):\n-    r\"\"\"\n-    Whisper specific Processor. This processor can be used to force a list of tokens. The processor will set their log\n-    probs to `inf` so that they are sampled at their corresponding index.\n-\n-    Args:\n-        generate_config (`GenerateConfig`):\n-            The generate config used to generate the output. The following parameters are required:\n-                eos_token_id (`int`, *optional*, defaults to 50257):\n-                    The id of the *end-of-sequence* token.\n-                no_timestamps_token_id (`int`, *optional*, defaults to 50363):\n-                    The id of the `\"<|notimestamps|>\"` token.\n-                max_initial_timestamp_index (`int`, *optional*, defaults to 1):\n-                    Used to set the maximum value of the initial timestamp. This is used to prevent the model from\n-                    predicting timestamps that are too far in the future.\n-    \"\"\"\n-\n-    def __init__(self, generate_config, model_config, decoder_input_length):\n-        self.eos_token_id = generate_config.eos_token_id\n-        self.no_timestamps_token_id = generate_config.no_timestamps_token_id\n-        self.timestamp_begin = generate_config.no_timestamps_token_id + 1\n-\n-        self.begin_index = decoder_input_length + 1\n-\n-        if generate_config.is_multilingual:\n-            # room for language token and task token\n-            self.begin_index += 2\n-        if hasattr(generate_config, \"max_initial_timestamp_index\"):\n-            self.max_initial_timestamp_index = generate_config.max_initial_timestamp_index\n-        else:\n-            self.max_initial_timestamp_index = model_config.vocab_size\n-        if self.max_initial_timestamp_index is None:\n-            self.max_initial_timestamp_index = model_config.vocab_size\n-\n-    def __call__(self, input_ids, scores, cur_len):\n-        # suppress <|notimestamps|> which is handled by without_timestamps\n-        scores = scores.at[:, self.no_timestamps_token_id].set(-float(\"inf\"))\n-\n-        def handle_pairs(input_ids_k, scores_k):\n-            last_was_timestamp = jnp.where((cur_len - self.begin_index) >= 1, True, False)\n-            last_was_timestamp = jnp.where(\n-                input_ids_k[cur_len - 1] >= self.timestamp_begin,\n-                True and last_was_timestamp,\n-                False,\n-            )\n-\n-            penultimate_was_timestamp = jnp.where((cur_len - self.begin_index) < 2, True, False)\n-            penultimate_was_timestamp = jnp.where(\n-                input_ids_k[cur_len - 2] >= self.timestamp_begin,\n-                True,\n-                penultimate_was_timestamp,\n-            )\n-\n-            return jnp.where(\n-                last_was_timestamp,\n-                jnp.where(\n-                    penultimate_was_timestamp > 0,\n-                    scores_k.at[self.timestamp_begin :].set(-float(\"inf\")),\n-                    scores_k.at[: self.eos_token_id].set(-float(\"inf\")),\n-                ),\n-                scores_k,\n-            )\n-\n-        scores = jax.vmap(handle_pairs)(input_ids, scores)\n-\n-        apply_max_initial_timestamp = jnp.where(cur_len == self.begin_index, True, False)\n-        apply_max_initial_timestamp = jnp.where(\n-            self.max_initial_timestamp_index is not None,\n-            True and apply_max_initial_timestamp,\n-            False,\n-        )\n-\n-        last_allowed = self.timestamp_begin + self.max_initial_timestamp_index\n-\n-        scores = jnp.where(\n-            apply_max_initial_timestamp,\n-            scores.at[:, last_allowed + 1 :].set(-float(\"inf\")),\n-            scores,\n-        )\n-\n-        # if sum of probability over timestamps is above any other token, sample timestamp\n-        logprobs = jax.nn.log_softmax(scores, axis=-1)\n-\n-        def handle_cumulative_probs(logprobs_k, scores_k):\n-            timestamp_logprob = jax.nn.logsumexp(logprobs_k[self.timestamp_begin :], axis=-1)\n-            max_text_token_logprob = jnp.max(logprobs_k[: self.timestamp_begin])\n-            return jnp.where(\n-                timestamp_logprob > max_text_token_logprob,\n-                scores_k.at[: self.timestamp_begin].set(-float(\"inf\")),\n-                scores_k,\n-            )\n-\n-        scores = jax.vmap(handle_cumulative_probs)(logprobs, scores)\n-\n-        return scores\n-\n-\n-class FlaxNoRepeatNGramLogitsProcessor(FlaxLogitsProcessor):\n-    r\"\"\"\n-    [`FlaxLogitsProcessor`] that enforces no repetition of n-grams. See\n-    [Fairseq](https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345).\n-\n-    Args:\n-        ngram_size (`int`):\n-            All ngrams of size `ngram_size` can only occur once.\n-    \"\"\"\n-\n-    def __init__(self, ngram_size: int):\n-        if not isinstance(ngram_size, int) or ngram_size <= 0:\n-            raise ValueError(f\"`ngram_size` has to be a strictly positive integer, but is {ngram_size}\")\n-        self.ngram_size = ngram_size\n-\n-    def get_previous_ngrams(self, input_ids: jnp.ndarray, vocab_size: int, cur_len: int):\n-        \"\"\"\n-        get a matrix of size (batch_size,) + (vocab_size,)*n (for n-grams) that\n-        represent the n-grams that occurred previously.\n-        The BCOO representation allow to store only the few non-zero entries, instead of the full (huge) matrix\n-        \"\"\"\n-        batch_size, seq_len = input_ids.shape\n-        # number of n-grams in the whole sequence\n-        seq_ngrams = seq_len - (self.ngram_size - 1)\n-        # number of n-grams in the currently generated sequence\n-        cur_ngrams = cur_len - (self.ngram_size - 1)\n-\n-        def body_fun(i, val):\n-            b = i % batch_size\n-            pos = i // batch_size\n-            return val.at[i].set(\n-                jnp.array(\n-                    [\n-                        b,\n-                    ]\n-                    + [jnp.array(input_ids)[b, pos + j] for j in range(self.ngram_size)]\n-                )\n-            )\n-\n-        shape = (batch_size * seq_ngrams, self.ngram_size + 1)\n-        all_update_indices = jax.lax.fori_loop(\n-            0, batch_size * cur_ngrams, body_fun, jnp.zeros(shape, dtype=input_ids.dtype)\n-        )\n-\n-        # ignore the n-grams not yet generated\n-        data = (jnp.arange(batch_size * seq_ngrams) < batch_size * cur_ngrams).astype(\"float32\")\n-\n-        return sparse.BCOO((data, all_update_indices), shape=(batch_size,) + (vocab_size,) * self.ngram_size)\n-\n-    def get_banned_tokens_mask(self, latest_tokens: jnp.ndarray, previous_ngrams) -> jnp.ndarray:\n-        \"\"\"\n-        Determines which tokens must be banned given latest tokens and the previously seen\n-        ngrams.\n-        \"\"\"\n-\n-        @sparse.sparsify\n-        @jax.vmap\n-        def inner_fn(latest_tokens, previous_ngrams):\n-            return previous_ngrams[tuple(latest_tokens)]\n-\n-        return sparse.bcoo_todense(inner_fn(latest_tokens, previous_ngrams))\n-\n-    def __call__(self, input_ids: jnp.ndarray, scores: jnp.ndarray, cur_len: int) -> jnp.ndarray:\n-        def true_fn():\n-            _, vocab_size = scores.shape\n-            # store the previously seen n-grams\n-            previous_ngrams = self.get_previous_ngrams(input_ids, vocab_size, cur_len)\n-\n-            # get the n-1 last tokens that prefix the n-gram being generated\n-            latest_tokens = jnp.zeros((input_ids.shape[0], self.ngram_size - 1), dtype=input_ids.dtype)\n-            latest_tokens = jax.lax.dynamic_update_slice(\n-                latest_tokens,\n-                jax.lax.dynamic_slice(\n-                    input_ids, (0, cur_len - (self.ngram_size - 1)), (input_ids.shape[0], (self.ngram_size - 1))\n-                ),\n-                (0, 0),\n-            )\n-\n-            # compute the banned tokens, ie all the tokens that when added to the latest tokens lead to a n-gram that was previously generated\n-            banned_tokens_indices_mask = self.get_banned_tokens_mask(latest_tokens, previous_ngrams).astype(\"bool\")\n-            return jnp.where(banned_tokens_indices_mask, -float(\"inf\"), scores)\n-\n-        output = jax.lax.cond((cur_len >= self.ngram_size - 1), true_fn, lambda: scores)\n-        return output"
        },
        {
            "sha": "e858a9813cea4a2b923e17d181c966da09e2b46f",
            "filename": "src/transformers/generation/flax_utils.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1032,
            "changes": 1032,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fgeneration%2Fflax_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fgeneration%2Fflax_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fflax_utils.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,1032 +0,0 @@\n-# coding=utf-8\n-# Copyright 2021 The Google AI Flax Team Authors, and The HuggingFace Inc. team.\n-# Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\n-import copy\n-import inspect\n-import warnings\n-from functools import partial\n-from typing import Any, Optional, Union\n-\n-import flax\n-import jax\n-import jax.numpy as jnp\n-import numpy as np\n-from jax import lax\n-\n-from ..models.auto import (\n-    FLAX_MODEL_FOR_CAUSAL_LM_MAPPING,\n-    FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,\n-    FLAX_MODEL_FOR_VISION_2_SEQ_MAPPING,\n-)\n-from ..utils import ModelOutput, logging\n-from .configuration_utils import GenerationConfig\n-from .flax_logits_process import (\n-    FlaxForcedBOSTokenLogitsProcessor,\n-    FlaxForcedEOSTokenLogitsProcessor,\n-    FlaxForceTokensLogitsProcessor,\n-    FlaxLogitsProcessorList,\n-    FlaxMinLengthLogitsProcessor,\n-    FlaxNoRepeatNGramLogitsProcessor,\n-    FlaxSuppressTokensAtBeginLogitsProcessor,\n-    FlaxSuppressTokensLogitsProcessor,\n-    FlaxTemperatureLogitsWarper,\n-    FlaxTopKLogitsWarper,\n-    FlaxTopPLogitsWarper,\n-)\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-@flax.struct.dataclass\n-class FlaxGreedySearchOutput(ModelOutput):\n-    \"\"\"\n-    Flax Base class for outputs of decoder-only generation models using greedy search.\n-\n-\n-    Args:\n-        sequences (`jnp.ndarray` of shape `(batch_size, max_length)`):\n-            The generated sequences.\n-    \"\"\"\n-\n-    sequences: Optional[jnp.ndarray] = None\n-\n-\n-@flax.struct.dataclass\n-class FlaxSampleOutput(ModelOutput):\n-    \"\"\"\n-    Flax Base class for outputs of decoder-only generation models using sampling.\n-\n-\n-    Args:\n-        sequences (`jnp.ndarray` of shape `(batch_size, max_length)`):\n-            The generated sequences.\n-    \"\"\"\n-\n-    sequences: Optional[jnp.ndarray] = None\n-\n-\n-@flax.struct.dataclass\n-class FlaxBeamSearchOutput(ModelOutput):\n-    \"\"\"\n-    Flax Base class for outputs of decoder-only generation models using greedy search.\n-\n-\n-    Args:\n-        sequences (`jnp.ndarray` of shape `(batch_size, max_length)`):\n-            The generated sequences.\n-        scores (`jnp.ndarray` of shape `(batch_size,)`):\n-            The scores (log probabilities) of the generated sequences.\n-    \"\"\"\n-\n-    sequences: Optional[jnp.ndarray] = None\n-    scores: Optional[jnp.ndarray] = None\n-\n-\n-@flax.struct.dataclass\n-class GreedyState:\n-    cur_len: jnp.ndarray\n-    sequences: jnp.ndarray\n-    running_token: jnp.ndarray\n-    is_sent_finished: jnp.ndarray\n-    model_kwargs: dict[str, jnp.ndarray]\n-\n-\n-@flax.struct.dataclass\n-class SampleState:\n-    cur_len: jnp.ndarray\n-    sequences: jnp.ndarray\n-    running_token: jnp.ndarray\n-    is_sent_finished: jnp.ndarray\n-    prng_key: jnp.ndarray\n-    model_kwargs: dict[str, jnp.ndarray]\n-\n-\n-@flax.struct.dataclass\n-class BeamSearchState:\n-    cur_len: jnp.ndarray\n-    running_sequences: jnp.ndarray\n-    running_scores: jnp.ndarray\n-    sequences: jnp.ndarray\n-    scores: jnp.ndarray\n-    is_sent_finished: jnp.ndarray\n-    model_kwargs: dict[str, jnp.ndarray]\n-\n-\n-class FlaxGenerationMixin:\n-    \"\"\"\n-    A class containing all functions for auto-regressive text generation, to be used as a mixin in\n-    [`FlaxPreTrainedModel`].\n-\n-    The class exposes [`~generation.FlaxGenerationMixin.generate`], which can be used for:\n-            - *greedy decoding* by calling [`~generation.FlaxGenerationMixin._greedy_search`] if `num_beams=1` and\n-              `do_sample=False`\n-            - *multinomial sampling* by calling [`~generation.FlaxGenerationMixin._sample`] if `num_beams=1` and\n-              `do_sample=True`\n-            - *beam-search decoding* by calling [`~generation.FlaxGenerationMixin._beam_search`] if `num_beams>1` and\n-              `do_sample=False`\n-\n-    You do not need to call any of the above methods directly. Pass custom parameter values to 'generate' instead. To\n-    learn more about decoding strategies refer to the [text generation strategies guide](../generation_strategies).\n-    \"\"\"\n-\n-    def prepare_inputs_for_generation(self, *args, **kwargs):\n-        raise NotImplementedError(\n-            \"A model class needs to define a `prepare_inputs_for_generation` method in order to use `generate`.\"\n-        )\n-\n-    @staticmethod\n-    def _run_loop_in_debug(cond_fn, body_fn, init_state):\n-        \"\"\"\n-        Run generation in untraced mode. This should only be used for debugging purposes.\n-        \"\"\"\n-        state = init_state\n-        while cond_fn(state):\n-            state = body_fn(state)\n-        return state\n-\n-    def _prepare_encoder_decoder_kwargs_for_generation(self, input_ids, params, model_kwargs):\n-        encoder_kwargs = {\n-            argument: value\n-            for argument, value in model_kwargs.items()\n-            if not (argument.startswith(\"decoder_\") or argument.startswith(\"cross_attn\"))\n-        }\n-        model_kwargs[\"encoder_outputs\"] = self.encode(input_ids, params=params, return_dict=True, **encoder_kwargs)\n-        return model_kwargs\n-\n-    def _prepare_decoder_input_ids_for_generation(\n-        self,\n-        batch_size: int,\n-        decoder_start_token_id: Optional[int] = None,\n-        bos_token_id: Optional[int] = None,\n-        model_kwargs: Optional[dict[str, jnp.ndarray]] = None,\n-    ) -> jnp.ndarray:\n-        if model_kwargs is not None and \"decoder_input_ids\" in model_kwargs:\n-            # Only use this arg if not None, otherwise just remove from model_kwargs\n-            decoder_input_ids = model_kwargs.pop(\"decoder_input_ids\")\n-            if decoder_input_ids is not None:\n-                return decoder_input_ids\n-        decoder_start_token_id = self._get_decoder_start_token_id(decoder_start_token_id, bos_token_id)\n-        return jnp.array(decoder_start_token_id, dtype=\"i4\").reshape(1, -1).repeat(batch_size, axis=0)\n-\n-    def _get_decoder_start_token_id(\n-        self, decoder_start_token_id: Optional[int] = None, bos_token_id: Optional[int] = None\n-    ) -> int:\n-        # retrieve decoder_start_token_id for encoder-decoder models\n-        # fall back to bos_token_id if necessary\n-        decoder_start_token_id = (\n-            decoder_start_token_id\n-            if decoder_start_token_id is not None\n-            else self.generation_config.decoder_start_token_id\n-        )\n-        bos_token_id = bos_token_id if bos_token_id is not None else self.generation_config.bos_token_id\n-        if decoder_start_token_id is not None:\n-            return decoder_start_token_id\n-        elif (\n-            hasattr(self.config, \"decoder\")\n-            and hasattr(self.config.decoder, \"decoder_start_token_id\")\n-            and self.config.decoder.decoder_start_token_id is not None\n-        ):\n-            return self.config.decoder.decoder_start_token_id\n-        elif bos_token_id is not None:\n-            return bos_token_id\n-        elif (\n-            hasattr(self.config, \"decoder\")\n-            and hasattr(self.config.decoder, \"bos_token_id\")\n-            and self.config.decoder.bos_token_id is not None\n-        ):\n-            return self.config.decoder.bos_token_id\n-        raise ValueError(\n-            \"`decoder_start_token_id` or `bos_token_id` has to be defined for encoder-decoder generation.\"\n-        )\n-\n-    @staticmethod\n-    def _expand_to_num_beams(tensor, num_beams):\n-        return jnp.broadcast_to(tensor[:, None], (tensor.shape[0], num_beams) + tensor.shape[1:])\n-\n-    def _adapt_logits_for_beam_search(self, logits):\n-        \"\"\"\n-        This function can be overwritten in the specific modeling_flax_<model-name>.py classes to allow for custom beam\n-        search behavior. Note that the only model that overwrites this method is [`~transformers.FlaxMarianMTModel`].\n-        \"\"\"\n-        return logits\n-\n-    def _validate_model_class(self):\n-        \"\"\"\n-        Confirms that the model class is compatible with generation. If not, raises an exception that points to the\n-        right class to use.\n-        \"\"\"\n-        if not self.can_generate():\n-            generate_compatible_mappings = [\n-                FLAX_MODEL_FOR_CAUSAL_LM_MAPPING,\n-                FLAX_MODEL_FOR_VISION_2_SEQ_MAPPING,\n-                FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,\n-            ]\n-            generate_compatible_classes = set()\n-            for model_mapping in generate_compatible_mappings:\n-                supported_models = model_mapping.get(type(self.config), default=None)\n-                if supported_models is not None:\n-                    generate_compatible_classes.add(supported_models.__name__)\n-            exception_message = (\n-                f\"The current model class ({self.__class__.__name__}) is not compatible with `.generate()`, as \"\n-                \"it doesn't have a language model head.\"\n-            )\n-            if generate_compatible_classes:\n-                exception_message += f\" Please use one of the following classes instead: {generate_compatible_classes}\"\n-            raise TypeError(exception_message)\n-\n-    def _validate_model_kwargs(self, model_kwargs: dict[str, Any]):\n-        \"\"\"Validates model kwargs for generation. Generate argument typos will also be caught here.\"\"\"\n-        unused_model_args = []\n-        model_args = set(inspect.signature(self.prepare_inputs_for_generation).parameters)\n-        # `kwargs`/`model_kwargs` is often used to handle optional forward pass inputs like `attention_mask`. If\n-        # `prepare_inputs_for_generation` doesn't accept them, then a stricter check can be made ;)\n-        if \"kwargs\" in model_args or \"model_kwargs\" in model_args:\n-            model_args |= set(inspect.signature(self.__call__).parameters)\n-        for key, value in model_kwargs.items():\n-            if value is not None and key not in model_args:\n-                unused_model_args.append(key)\n-\n-        if unused_model_args:\n-            raise ValueError(\n-                f\"The following `model_kwargs` are not used by the model: {unused_model_args} (note: typos in the\"\n-                \" generate arguments will also show up in this list)\"\n-            )\n-\n-    def generate(\n-        self,\n-        input_ids: jnp.ndarray,\n-        generation_config: Optional[GenerationConfig] = None,\n-        prng_key: Optional[jnp.ndarray] = None,\n-        trace: bool = True,\n-        params: Optional[dict[str, jnp.ndarray]] = None,\n-        logits_processor: Optional[FlaxLogitsProcessorList] = None,\n-        **kwargs,\n-    ):\n-        r\"\"\"\n-        Generates sequences of token ids for models with a language modeling head.\n-\n-        Parameters:\n-            input_ids (`jnp.ndarray` of shape `(batch_size, sequence_length)`):\n-                The sequence used as a prompt for the generation.\n-            generation_config (`~generation.GenerationConfig`, *optional*):\n-                The generation configuration to be used as base parametrization for the generation call. `**kwargs`\n-                passed to generate matching the attributes of `generation_config` will override them. If\n-                `generation_config` is not provided, the default will be used, which had the following loading\n-                priority: 1) from the `generation_config.json` model file, if it exists; 2) from the model\n-                configuration. Please note that unspecified parameters will inherit [`~generation.GenerationConfig`]'s\n-                default values, whose documentation should be checked to parameterize generation.\n-            trace (`bool`, *optional*, defaults to `True`):\n-                Whether to trace generation. Setting `trace=False` should only be used for debugging and will lead to a\n-                considerably slower runtime.\n-            params (`dict[str, jnp.ndarray]`, *optional*):\n-                Optionally the model parameters can be passed. Can be useful for parallelized generation.\n-            logits_processor (`FlaxLogitsProcessorList `, *optional*):\n-                Custom logits processors that complement the default logits processors built from arguments and\n-                generation config. If a logit processor is passed that is already created with the arguments or a\n-                generation config an error is thrown. This feature is intended for advanced users.\n-            kwargs (`dict[str, Any]`, *optional*):\n-                Ad hoc parametrization of `generate_config` and/or additional model-specific kwargs that will be\n-                forwarded to the `forward` function of the model. If the model is an encoder-decoder model, encoder\n-                specific kwargs should not be prefixed and decoder specific kwargs should be prefixed with *decoder_*.\n-\n-        Return:\n-            [`~utils.ModelOutput`].\n-\n-        \"\"\"\n-        # Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\n-        self._validate_model_class()\n-\n-        # priority: `generation_config` argument > `model.generation_config` (the default generation config)\n-        if generation_config is None:\n-            # legacy: users may modify the model configuration to control generation. To trigger this legacy behavior,\n-            # two conditions must be met\n-            # 1) the generation config must have been created from the model config (`_from_model_config` field);\n-            # 2) the generation config must have seen no modification since its creation (the hash is the same).\n-            if self.generation_config._from_model_config and self.generation_config._original_object_hash == hash(\n-                self.generation_config\n-            ):\n-                new_generation_config = GenerationConfig.from_model_config(self.config)\n-                if new_generation_config != self.generation_config:\n-                    warnings.warn(\n-                        \"You have modified the pretrained model configuration to control generation. This is a\"\n-                        \" deprecated strategy to control generation and will be removed soon, in a future version.\"\n-                        \" Please use and modify the model generation configuration (see\"\n-                        \" https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\"\n-                    )\n-                    self.generation_config = new_generation_config\n-            generation_config = self.generation_config\n-\n-        generation_config = copy.deepcopy(generation_config)\n-        model_kwargs = generation_config.update(**kwargs)  # All unused kwargs must be model kwargs\n-        self._validate_model_kwargs(model_kwargs.copy())\n-\n-        logits_processor = logits_processor if logits_processor is not None else FlaxLogitsProcessorList()\n-\n-        # set init values\n-        prng_key = prng_key if prng_key is not None else jax.random.PRNGKey(0)\n-\n-        if generation_config.pad_token_id is None and generation_config.eos_token_id is not None:\n-            if model_kwargs.get(\"attention_mask\") is None:\n-                logger.warning(\n-                    \"The attention mask and the pad token id were not set. As a consequence, you may observe \"\n-                    \"unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\"\n-                )\n-            eos_token_id = generation_config.eos_token_id\n-            if isinstance(eos_token_id, list):\n-                eos_token_id = eos_token_id[0]\n-            generation_config.pad_token_id = eos_token_id\n-\n-        if generation_config.decoder_start_token_id is None and self.config.is_encoder_decoder:\n-            raise ValueError(\"`decoder_start_token_id` has to be defined for encoder-decoder generation.\")\n-\n-        # decoder-only models should use left-padding for generation (can't be checked with `trace=True`)\n-        if not self.config.is_encoder_decoder and not trace:\n-            if (\n-                generation_config.pad_token_id is not None\n-                and jnp.sum(input_ids[:, -1] == generation_config.pad_token_id) > 0\n-            ):\n-                logger.warning(\n-                    \"A decoder-only architecture is being used, but right-padding was detected! For correct \"\n-                    \"generation results, please set `padding_side='left'` when initializing the tokenizer.\"\n-                )\n-\n-        batch_size = input_ids.shape[0]\n-\n-        if self.config.is_encoder_decoder:\n-            # add encoder_outputs to model_kwargs\n-            if model_kwargs.get(\"encoder_outputs\") is None:\n-                model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(input_ids, params, model_kwargs)\n-            # prepare decoder_input_ids for generation\n-            input_ids = self._prepare_decoder_input_ids_for_generation(\n-                batch_size,\n-                decoder_start_token_id=generation_config.decoder_start_token_id,\n-                bos_token_id=generation_config.bos_token_id,\n-                model_kwargs=model_kwargs,\n-            )\n-\n-        # Prepare `max_length` depending on other stopping criteria.\n-        input_ids_seq_length = input_ids.shape[-1]\n-        has_default_max_length = kwargs.get(\"max_length\") is None and generation_config.max_length is not None\n-        if has_default_max_length and generation_config.max_new_tokens is None and generation_config.max_length == 20:\n-            # 20 is the default max_length of the generation config\n-            warnings.warn(\n-                f\"Using the model-agnostic default `max_length` (={generation_config.max_length}) \"\n-                \"to control the generation length.  recommend setting `max_new_tokens` to control the maximum length of the generation.\",\n-                UserWarning,\n-            )\n-        elif generation_config.max_new_tokens is not None:\n-            if not has_default_max_length and generation_config.max_length is not None:\n-                logger.warning(\n-                    f\"Both `max_new_tokens` (={generation_config.max_new_tokens}) and `max_length`(=\"\n-                    f\"{generation_config.max_length}) seem to have been set. `max_new_tokens` will take precedence. \"\n-                    \"Please refer to the documentation for more information. \"\n-                    \"(https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\"\n-                )\n-            generation_config.max_length = generation_config.max_new_tokens + input_ids_seq_length\n-        else:  # by default let's always generate 20 new tokens\n-            if generation_config.max_length == GenerationConfig().max_length:\n-                generation_config.max_length = generation_config.max_length + input_ids_seq_length\n-                max_position_embeddings = getattr(self.config, \"max_position_embeddings\", None)\n-                if max_position_embeddings is not None:\n-                    generation_config.max_length = min(generation_config.max_length, max_position_embeddings)\n-\n-        if generation_config.min_length is not None and generation_config.min_length > generation_config.max_length:\n-            raise ValueError(\n-                f\"Unfeasable length constraints: the minimum length ({generation_config.min_length}) is larger than\"\n-                f\" the maximum length ({generation_config.max_length})\"\n-            )\n-        if input_ids_seq_length >= generation_config.max_length:\n-            input_ids_string = \"decoder_input_ids\" if self.config.is_encoder_decoder else \"input_ids\"\n-            logger.warning(\n-                f\"Input length of {input_ids_string} is {input_ids_seq_length}, but `max_length` is set to\"\n-                f\" {generation_config.max_length}. This can lead to unexpected behavior. You should consider\"\n-                \" increasing`max_new_tokens`.\"\n-            )\n-\n-        logits_processor = self._get_logits_processor(\n-            generation_config=generation_config,\n-            input_ids_seq_length=input_ids_seq_length,\n-            logits_processor=logits_processor,\n-        )\n-\n-        if not generation_config.do_sample and generation_config.num_beams == 1:\n-            return self._greedy_search(\n-                input_ids,\n-                generation_config.max_length,\n-                generation_config.pad_token_id,\n-                generation_config.eos_token_id,\n-                logits_processor=logits_processor,\n-                trace=trace,\n-                params=params,\n-                model_kwargs=model_kwargs,\n-            )\n-        elif generation_config.do_sample and generation_config.num_beams == 1:\n-            logits_warper = self._get_logits_warper(generation_config=generation_config)\n-            return self._sample(\n-                input_ids,\n-                generation_config.max_length,\n-                generation_config.pad_token_id,\n-                generation_config.eos_token_id,\n-                prng_key,\n-                logits_warper=logits_warper,\n-                logits_processor=logits_processor,\n-                trace=trace,\n-                params=params,\n-                model_kwargs=model_kwargs,\n-            )\n-        elif not generation_config.do_sample and generation_config.num_beams > 1:\n-            # broadcast input_ids & encoder_outputs\n-            input_ids = self._expand_to_num_beams(input_ids, num_beams=generation_config.num_beams)\n-\n-            if \"encoder_outputs\" in model_kwargs:\n-                model_kwargs[\"encoder_outputs\"][\"last_hidden_state\"] = self._expand_to_num_beams(\n-                    model_kwargs[\"encoder_outputs\"][\"last_hidden_state\"], num_beams=generation_config.num_beams\n-                )\n-\n-            for kwarg in [\"attention_mask\", \"decoder_attention_mask\"]:\n-                if kwarg in model_kwargs:\n-                    model_kwargs[kwarg] = self._expand_to_num_beams(\n-                        model_kwargs[kwarg], num_beams=generation_config.num_beams\n-                    )\n-\n-            return self._beam_search(\n-                input_ids,\n-                generation_config.max_length,\n-                generation_config.pad_token_id,\n-                generation_config.eos_token_id,\n-                length_penalty=generation_config.length_penalty,\n-                early_stopping=generation_config.early_stopping,\n-                logits_processor=logits_processor,\n-                trace=trace,\n-                params=params,\n-                num_return_sequences=generation_config.num_return_sequences,\n-                model_kwargs=model_kwargs,\n-            )\n-        else:\n-            raise NotImplementedError(\"`Beam sampling is currently not implemented.\")\n-\n-    def _get_logits_warper(self, generation_config: GenerationConfig) -> FlaxLogitsProcessorList:\n-        \"\"\"\n-        This class returns a [`FlaxLogitsProcessorList`] list object that contains all relevant [`FlaxLogitsWarper`]\n-        instances used for multinomial sampling.\n-        \"\"\"\n-        warpers = FlaxLogitsProcessorList()\n-\n-        if generation_config.temperature is not None and generation_config.temperature != 1.0:\n-            warpers.append(FlaxTemperatureLogitsWarper(generation_config.temperature))\n-        if generation_config.top_k is not None and generation_config.top_k != 0:\n-            warpers.append(FlaxTopKLogitsWarper(top_k=generation_config.top_k, min_tokens_to_keep=1))\n-        if generation_config.top_p is not None and generation_config.top_p < 1.0:\n-            warpers.append(FlaxTopPLogitsWarper(top_p=generation_config.top_p, min_tokens_to_keep=1))\n-\n-        return warpers\n-\n-    def _get_logits_processor(\n-        self,\n-        generation_config: GenerationConfig,\n-        input_ids_seq_length: int,\n-        logits_processor: Optional[FlaxLogitsProcessorList],\n-    ) -> FlaxLogitsProcessorList:\n-        \"\"\"\n-        This class returns a [`FlaxLogitsProcessorList`] list object that contains all relevant [`FlaxLogitsProcessor`]\n-        instances used to modify the scores of the language model head.\n-        \"\"\"\n-        processors = FlaxLogitsProcessorList()\n-\n-        if (\n-            generation_config.min_length is not None\n-            and generation_config.eos_token_id is not None\n-            and generation_config.min_length > -1\n-        ):\n-            processors.append(\n-                FlaxMinLengthLogitsProcessor(generation_config.min_length, generation_config.eos_token_id)\n-            )\n-        if generation_config.forced_bos_token_id is not None:\n-            processors.append(FlaxForcedBOSTokenLogitsProcessor(generation_config.forced_bos_token_id))\n-        if generation_config.forced_eos_token_id is not None:\n-            processors.append(\n-                FlaxForcedEOSTokenLogitsProcessor(generation_config.max_length, generation_config.forced_eos_token_id)\n-            )\n-        if generation_config.suppress_tokens is not None:\n-            processors.append(FlaxSuppressTokensLogitsProcessor(generation_config.suppress_tokens))\n-        if generation_config.begin_suppress_tokens is not None:\n-            begin_index = input_ids_seq_length\n-            begin_index = (\n-                begin_index\n-                if (input_ids_seq_length > 1 or generation_config.forced_bos_token_id is None)\n-                else begin_index + 1\n-            )\n-            if (\n-                getattr(generation_config, \"forced_decoder_ids\", None) is not None\n-                and len(generation_config.forced_decoder_ids) > 0\n-            ):\n-                # generation starts after the last token that is forced\n-                begin_index += generation_config.forced_decoder_ids[-1][0]\n-            processors.append(\n-                FlaxSuppressTokensAtBeginLogitsProcessor(generation_config.begin_suppress_tokens, begin_index)\n-            )\n-        if getattr(generation_config, \"forced_decoder_ids\", None) is not None:\n-            forced_decoder_ids = [\n-                [input_ids_seq_length + i[0] - 1, i[1]] for i in generation_config.forced_decoder_ids\n-            ]\n-            processors.append(FlaxForceTokensLogitsProcessor(forced_decoder_ids))\n-        if generation_config.no_repeat_ngram_size is not None and generation_config.no_repeat_ngram_size > 0:\n-            processors.append(FlaxNoRepeatNGramLogitsProcessor(generation_config.no_repeat_ngram_size))\n-        processors = self._merge_criteria_processor_list(processors, logits_processor)\n-\n-        return processors\n-\n-    def _merge_criteria_processor_list(\n-        self,\n-        default_list: FlaxLogitsProcessorList,\n-        custom_list: FlaxLogitsProcessorList,\n-    ) -> FlaxLogitsProcessorList:\n-        if len(custom_list) == 0:\n-            return default_list\n-        for default in default_list:\n-            for custom in custom_list:\n-                if type(custom) is type(default):\n-                    object_type = \"logits processor\"\n-                    raise ValueError(\n-                        f\"A custom {object_type} of type {type(custom)} with values {custom} has been passed to\"\n-                        f\" `generate`, but it has already been created with the values {default}. {default} has been\"\n-                        \" created by passing the corresponding arguments to generate or by the model's config default\"\n-                        f\" values. If you just want to change the default values of {object_type} consider passing\"\n-                        f\" them as arguments to `generate` instead of using a custom {object_type}.\"\n-                    )\n-        default_list.extend(custom_list)\n-        return default_list\n-\n-    def _greedy_search(\n-        self,\n-        input_ids: None,\n-        max_length: Optional[int] = None,\n-        pad_token_id: Optional[int] = None,\n-        eos_token_id: Optional[int] = None,\n-        logits_processor: Optional[FlaxLogitsProcessorList] = None,\n-        trace: bool = True,\n-        params: Optional[dict[str, jnp.ndarray]] = None,\n-        model_kwargs: Optional[dict[str, jnp.ndarray]] = None,\n-    ):\n-        # init values\n-        max_length = max_length if max_length is not None else self.generation_config.max_length\n-        pad_token_id = pad_token_id if pad_token_id is not None else self.generation_config.pad_token_id\n-        eos_token_id = eos_token_id if eos_token_id is not None else self.generation_config.eos_token_id\n-\n-        batch_size, cur_len = input_ids.shape\n-\n-        eos_token_id = jnp.array(eos_token_id, dtype=jnp.int32 if eos_token_id is not None else None)\n-        pad_token_id = jnp.array(pad_token_id, dtype=jnp.int32)\n-        cur_len = jnp.array(cur_len)\n-\n-        # per batch-item holding current token in loop.\n-        sequences = jnp.full((batch_size, max_length), pad_token_id, dtype=jnp.int32)\n-        sequences = lax.dynamic_update_slice(sequences, input_ids, (0, 0))\n-\n-        # per batch-item state bit indicating if sentence has finished.\n-        is_sent_finished = jnp.zeros((batch_size,), dtype=jnp.bool_)\n-\n-        # For Seq2Seq generation, we only need to use the decoder instead of the whole model in generation loop\n-        # and pass it the `encoder_outputs`, which are part of the `model_kwargs`.\n-        model = self.decode if self.config.is_encoder_decoder else self\n-        # initialize model specific kwargs\n-        model_kwargs = self.prepare_inputs_for_generation(input_ids, max_length, **model_kwargs)\n-\n-        # initialize state\n-        state = GreedyState(\n-            cur_len=cur_len,\n-            sequences=sequences,\n-            running_token=input_ids,\n-            is_sent_finished=is_sent_finished,\n-            model_kwargs=model_kwargs,\n-        )\n-\n-        def greedy_search_cond_fn(state):\n-            \"\"\"state termination condition fn.\"\"\"\n-            has_reached_max_length = state.cur_len == max_length\n-            all_sequence_finished = jnp.all(state.is_sent_finished)\n-            finish_generation = jnp.logical_or(has_reached_max_length, all_sequence_finished)\n-            return ~finish_generation\n-\n-        def greedy_search_body_fn(state):\n-            \"\"\"state update fn.\"\"\"\n-            model_outputs = model(state.running_token, params=params, **state.model_kwargs)\n-            logits = model_outputs.logits[:, -1]\n-\n-            # apply min_length, ...\n-            logits = logits_processor(state.sequences, logits, state.cur_len)\n-\n-            next_token = jnp.argmax(logits, axis=-1)\n-\n-            next_token = next_token * ~state.is_sent_finished + pad_token_id * state.is_sent_finished\n-            next_is_sent_finished = state.is_sent_finished | (next_token == eos_token_id)\n-            next_token = next_token[:, None]\n-\n-            next_sequences = lax.dynamic_update_slice(state.sequences, next_token, (0, state.cur_len))\n-            next_model_kwargs = self.update_inputs_for_generation(model_outputs, state.model_kwargs)\n-            return GreedyState(\n-                cur_len=state.cur_len + 1,\n-                sequences=next_sequences,\n-                running_token=next_token,\n-                is_sent_finished=next_is_sent_finished,\n-                model_kwargs=next_model_kwargs,\n-            )\n-\n-        # The very first prompt often has sequence length > 1, so run outside of `lax.while_loop` to comply with TPU\n-        if input_ids.shape[1] > 1:\n-            state = greedy_search_body_fn(state)\n-\n-        if not trace:\n-            state = self._run_loop_in_debug(greedy_search_cond_fn, greedy_search_body_fn, state)\n-        else:\n-            state = lax.while_loop(greedy_search_cond_fn, greedy_search_body_fn, state)\n-\n-        return FlaxGreedySearchOutput(sequences=state.sequences)\n-\n-    def _sample(\n-        self,\n-        input_ids: None,\n-        max_length: Optional[int] = None,\n-        pad_token_id: Optional[int] = None,\n-        eos_token_id: Optional[int] = None,\n-        prng_key: Optional[jnp.ndarray] = None,\n-        logits_processor: Optional[FlaxLogitsProcessorList] = None,\n-        logits_warper: Optional[FlaxLogitsProcessorList] = None,\n-        trace: bool = True,\n-        params: Optional[dict[str, jnp.ndarray]] = None,\n-        model_kwargs: Optional[dict[str, jnp.ndarray]] = None,\n-    ):\n-        # init values\n-        max_length = max_length if max_length is not None else self.generation_config.max_length\n-        pad_token_id = pad_token_id if pad_token_id is not None else self.generation_config.pad_token_id\n-        eos_token_id = eos_token_id if eos_token_id is not None else self.generation_config.eos_token_id\n-        prng_key = prng_key if prng_key is not None else jax.random.PRNGKey(0)\n-\n-        batch_size, cur_len = input_ids.shape\n-\n-        eos_token_id = jnp.array(eos_token_id, dtype=jnp.int32 if eos_token_id is not None else None)\n-        pad_token_id = jnp.array(pad_token_id, dtype=jnp.int32)\n-        cur_len = jnp.array(cur_len)\n-\n-        # per batch-item holding current token in loop.\n-        sequences = jnp.full((batch_size, max_length), pad_token_id, dtype=jnp.int32)\n-        sequences = lax.dynamic_update_slice(sequences, input_ids, (0, 0))\n-\n-        # per batch-item state bit indicating if sentence has finished.\n-        is_sent_finished = jnp.zeros((batch_size,), dtype=jnp.bool_)\n-\n-        # For Seq2Seq generation, we only need to use the decoder instead of the whole model in generation loop\n-        # and pass it the `encoder_outputs`, which are part of the `model_kwargs`.\n-        model = self.decode if self.config.is_encoder_decoder else self\n-\n-        # initialize model specific kwargs\n-        model_kwargs = self.prepare_inputs_for_generation(input_ids, max_length, **model_kwargs)\n-\n-        # initialize state\n-        state = SampleState(\n-            cur_len=cur_len,\n-            sequences=sequences,\n-            running_token=input_ids,\n-            is_sent_finished=is_sent_finished,\n-            prng_key=prng_key,\n-            model_kwargs=model_kwargs,\n-        )\n-\n-        def sample_search_cond_fn(state):\n-            \"\"\"state termination condition fn.\"\"\"\n-            has_reached_max_length = state.cur_len == max_length\n-            all_sequence_finished = jnp.all(state.is_sent_finished)\n-            finish_generation = jnp.logical_or(has_reached_max_length, all_sequence_finished)\n-            return ~finish_generation\n-\n-        def sample_search_body_fn(state):\n-            \"\"\"state update fn.\"\"\"\n-            prng_key, prng_key_next = jax.random.split(state.prng_key)\n-            model_outputs = model(state.running_token, params=params, **state.model_kwargs)\n-\n-            logits = model_outputs.logits[:, -1]\n-\n-            # apply min_length, ...\n-            logits = logits_processor(state.sequences, logits, state.cur_len)\n-            # apply top_p, top_k, temperature\n-            logits = logits_warper(logits, logits, state.cur_len)\n-\n-            next_token = jax.random.categorical(prng_key, logits, axis=-1)\n-\n-            next_token = next_token * ~state.is_sent_finished + pad_token_id * state.is_sent_finished\n-            next_is_sent_finished = state.is_sent_finished | (next_token == eos_token_id)\n-            next_token = next_token[:, None]\n-\n-            next_sequences = lax.dynamic_update_slice(state.sequences, next_token, (0, state.cur_len))\n-            next_model_kwargs = self.update_inputs_for_generation(model_outputs, state.model_kwargs)\n-\n-            return SampleState(\n-                cur_len=state.cur_len + 1,\n-                sequences=next_sequences,\n-                running_token=next_token,\n-                is_sent_finished=next_is_sent_finished,\n-                model_kwargs=next_model_kwargs,\n-                prng_key=prng_key_next,\n-            )\n-\n-        # The very first prompt often has sequence length > 1, so run outside of `lax.while_loop` to comply with TPU\n-        if input_ids.shape[1] > 1:\n-            state = sample_search_body_fn(state)\n-\n-        if not trace:\n-            state = self._run_loop_in_debug(sample_search_cond_fn, sample_search_body_fn, state)\n-        else:\n-            state = lax.while_loop(sample_search_cond_fn, sample_search_body_fn, state)\n-\n-        return FlaxSampleOutput(sequences=state.sequences)\n-\n-    def _beam_search(\n-        self,\n-        input_ids: None,\n-        max_length: Optional[int] = None,\n-        pad_token_id: Optional[int] = None,\n-        eos_token_id: Optional[int] = None,\n-        length_penalty: Optional[float] = None,\n-        early_stopping: Optional[Union[bool, str]] = None,\n-        logits_processor: Optional[FlaxLogitsProcessorList] = None,\n-        trace: bool = True,\n-        params: Optional[dict[str, jnp.ndarray]] = None,\n-        num_return_sequences: Optional[int] = None,\n-        model_kwargs: Optional[dict[str, jnp.ndarray]] = None,\n-    ):\n-        \"\"\"\n-        This beam search function is heavily inspired by Flax's official example:\n-        https://github.com/google/flax/blob/main/examples/wmt/decode.py\n-        \"\"\"\n-\n-        def flatten_beam_dim(tensor):\n-            \"\"\"Flattens the first two dimensions of a non-scalar array.\"\"\"\n-            # ignore scalars (e.g. cache index)\n-            if tensor.ndim == 0:\n-                return tensor\n-            return tensor.reshape((tensor.shape[0] * tensor.shape[1],) + tensor.shape[2:])\n-\n-        def unflatten_beam_dim(tensor, batch_size, num_beams):\n-            \"\"\"Unflattens the first, flat batch*beam dimension of a non-scalar array.\"\"\"\n-            # ignore scalars (e.g. cache index)\n-            if tensor.ndim == 0:\n-                return tensor\n-            return tensor.reshape((batch_size, num_beams) + tensor.shape[1:])\n-\n-        def gather_beams(nested, beam_indices, batch_size, new_num_beams):\n-            \"\"\"\n-            Gathers the beam slices indexed by beam_indices into new beam array.\n-            \"\"\"\n-            batch_indices = jnp.reshape(\n-                jnp.arange(batch_size * new_num_beams) // new_num_beams, (batch_size, new_num_beams)\n-            )\n-\n-            def gather_fn(tensor):\n-                # ignore scalars (e.g. cache index)\n-                if tensor.ndim == 0:\n-                    return tensor\n-                else:\n-                    return tensor[batch_indices, beam_indices]\n-\n-            return jax.tree_util.tree_map(gather_fn, nested)\n-\n-        # init values\n-        max_length = max_length if max_length is not None else self.generation_config.max_length\n-        pad_token_id = pad_token_id if pad_token_id is not None else self.generation_config.pad_token_id\n-        eos_token_id = eos_token_id if eos_token_id is not None else self.generation_config.eos_token_id\n-        length_penalty = length_penalty if length_penalty is not None else self.generation_config.length_penalty\n-        early_stopping = early_stopping if early_stopping is not None else self.generation_config.early_stopping\n-        num_return_sequences = (\n-            num_return_sequences if num_return_sequences is not None else self.generation_config.num_return_sequences\n-        )\n-\n-        batch_size, num_beams, cur_len = input_ids.shape\n-\n-        eos_token_id = jnp.array(eos_token_id, dtype=jnp.int32 if eos_token_id is not None else None)\n-        pad_token_id = jnp.array(pad_token_id, dtype=jnp.int32)\n-        cur_len = jnp.array(cur_len)\n-\n-        # record the prompt length of decoder\n-        decoder_prompt_len = input_ids.shape[-1]\n-\n-        # per batch,beam-item holding current token in loop.\n-        sequences = jnp.full((batch_size, num_beams, max_length), pad_token_id, dtype=jnp.int32)\n-        running_sequences = jnp.full((batch_size, num_beams, max_length), pad_token_id, dtype=jnp.int32)\n-        running_sequences = lax.dynamic_update_slice(sequences, input_ids, (0, 0, 0))\n-\n-        # per batch,beam-item state bit indicating if sentence has finished.\n-        is_sent_finished = jnp.zeros((batch_size, num_beams), dtype=jnp.bool_)\n-\n-        # per batch,beam-item score, logprobs\n-        running_scores = jnp.tile(jnp.array([0.0] + [np.array(-1.0e7)] * (num_beams - 1)), [batch_size, 1])\n-        scores = jnp.ones((batch_size, num_beams)) * np.array(-1.0e7)\n-\n-        # For Seq2Seq generation, we only need to use the decoder instead of the whole model in generation loop\n-        # and pass it the `encoder_outputs`, which are part of the `model_kwargs`.\n-        model = self.decode if self.config.is_encoder_decoder else self\n-\n-        # flatten beam dim\n-        if \"encoder_outputs\" in model_kwargs:\n-            model_kwargs[\"encoder_outputs\"][\"last_hidden_state\"] = flatten_beam_dim(\n-                model_kwargs[\"encoder_outputs\"][\"last_hidden_state\"]\n-            )\n-        for kwarg in [\"attention_mask\", \"decoder_attention_mask\"]:\n-            if kwarg in model_kwargs:\n-                model_kwargs[kwarg] = flatten_beam_dim(model_kwargs[kwarg])\n-\n-        # initialize model specific kwargs\n-        model_kwargs = self.prepare_inputs_for_generation(flatten_beam_dim(input_ids), max_length, **model_kwargs)\n-\n-        # initialize state\n-        state = BeamSearchState(\n-            cur_len=cur_len,\n-            running_sequences=running_sequences,\n-            running_scores=running_scores,\n-            sequences=sequences,\n-            scores=scores,\n-            is_sent_finished=is_sent_finished,\n-            model_kwargs=model_kwargs,\n-        )\n-\n-        def beam_search_cond_fn(state):\n-            \"\"\"beam search state termination condition fn.\"\"\"\n-\n-            # 1. is less than max length?\n-            not_max_length_yet = state.cur_len < max_length\n-\n-            # 2. can the new beams still improve?\n-            # early_stopping == False -> apply heuristic = always get the best score from `cur_len`. See the discussion\n-            # below for more details.\n-            # https://github.com/huggingface/transformers/pull/20901#issuecomment-1369845565\n-            # early_stopping == \"never\" -> compute the best score from max_length or cur_len, depending on the sign of\n-            #   length_penalty. Positive length_penalty favors longer sequences, thus we use max_length there.\n-            if early_stopping == \"never\" and length_penalty > 0.0:\n-                best_running_score = state.running_scores[:, :1] / (\n-                    (max_length - decoder_prompt_len) ** length_penalty\n-                )\n-            else:\n-                best_running_score = state.running_scores[:, :1] / (\n-                    (state.cur_len - decoder_prompt_len) ** length_penalty\n-                )\n-            worst_finished_score = jnp.where(\n-                state.is_sent_finished, jnp.min(state.scores, axis=1, keepdims=True), np.array(-1.0e7)\n-            )\n-            improvement_still_possible = jnp.any(best_running_score > worst_finished_score)\n-\n-            # 3. is there still a beam that has not finished?\n-            still_open_beam = ~(jnp.all(state.is_sent_finished) & (early_stopping is True))\n-\n-            return not_max_length_yet & still_open_beam & improvement_still_possible\n-\n-        def beam_search_body_fn(state, input_ids_length=1):\n-            \"\"\"beam search state update fn.\"\"\"\n-            # 1. Forward current tokens\n-            # Collect the current position slice along length to feed the fast\n-            # autoregressive decoder model.  Flatten the beam dimension into batch\n-            # dimension for feeding into the model.\n-            # unflatten beam dimension\n-            # Unflatten beam dimension in attention cache arrays\n-            input_token = flatten_beam_dim(\n-                lax.dynamic_slice(\n-                    state.running_sequences,\n-                    (0, 0, state.cur_len - input_ids_length),\n-                    (batch_size, num_beams, input_ids_length),\n-                )\n-            )\n-            model_outputs = model(input_token, params=params, **state.model_kwargs)\n-\n-            logits = unflatten_beam_dim(model_outputs.logits[:, -1], batch_size, num_beams)\n-            cache = jax.tree_util.tree_map(\n-                lambda tensor: unflatten_beam_dim(tensor, batch_size, num_beams), model_outputs.past_key_values\n-            )\n-\n-            # adapt logits for FlaxMarianMTModel\n-            logits = self._adapt_logits_for_beam_search(logits)\n-\n-            # 2. Compute log probs\n-            # get log probabilities from logits,\n-            # process logits with processors (*e.g.* min_length, ...), and\n-            # add new logprobs to existing running logprobs scores.\n-            log_probs = jax.nn.log_softmax(logits)\n-            log_probs = logits_processor(\n-                flatten_beam_dim(state.running_sequences), flatten_beam_dim(log_probs), state.cur_len\n-            )\n-            log_probs = unflatten_beam_dim(log_probs, batch_size, num_beams)\n-            log_probs = log_probs + jnp.expand_dims(state.running_scores, axis=2)\n-            vocab_size = log_probs.shape[2]\n-            log_probs = log_probs.reshape((batch_size, num_beams * vocab_size))\n-\n-            # 3. Retrieve top-K\n-            # Each item in batch has num_beams * vocab_size candidate sequences.\n-            # For each item, get the top 2*k candidates with the highest log-\n-            # probabilities. We gather the top 2*K beams here so that even if the best\n-            # K sequences reach EOS simultaneously, we have another K sequences\n-            # remaining to continue the live beam search.\n-            # Gather the top 2*K scores from _all_ beams.\n-            # Gather 2*k top beams.\n-            # Recover the beam index by floor division.\n-            # Recover token id by modulo division and expand Id array for broadcasting.\n-            # Update sequences for the 2*K top-k new sequences.\n-            beams_to_keep = 2 * num_beams\n-            topk_log_probs, topk_indices = lax.top_k(log_probs, k=beams_to_keep)\n-            topk_beam_indices = topk_indices // vocab_size\n-            topk_running_sequences = gather_beams(\n-                state.running_sequences, topk_beam_indices, batch_size, beams_to_keep\n-            )\n-            topk_ids = jnp.expand_dims(topk_indices % vocab_size, axis=2)\n-            topk_sequences = lax.dynamic_update_slice(topk_running_sequences, topk_ids, (0, 0, state.cur_len))\n-\n-            # 4. Check which sequences have ended\n-            # Update current sequences:\n-            # Did any of these sequences reach an end marker?\n-            # To prevent these just finished sequences from being added to the current sequences\n-            # set of active beam search sequences, set their log probs to a very large\n-            # negative value.\n-            did_topk_just_finished = topk_sequences[:, :, state.cur_len] == eos_token_id\n-            running_topk_log_probs = topk_log_probs + did_topk_just_finished * np.array(-1.0e7)\n-            # 5. Get running sequences scores for next\n-            # Determine the top k beam indices (from top 2*k beams) from log probs\n-            # and gather top k beams (from top 2*k beams).\n-            next_topk_indices = lax.top_k(running_topk_log_probs, k=num_beams)[1]\n-            next_running_sequences, next_running_scores = gather_beams(\n-                [topk_sequences, running_topk_log_probs], next_topk_indices, batch_size, num_beams\n-            )\n-\n-            # 6. Process topk logits\n-            # Further process log probs:\n-            # - add length penalty\n-            # - make sure no scores can be added anymore if beam is full\n-            # - make sure still running sequences cannot be chosen as finalized beam\n-            topk_log_probs = topk_log_probs / ((state.cur_len + 1 - decoder_prompt_len) ** length_penalty)\n-            beams_in_batch_are_full = jnp.broadcast_to(\n-                state.is_sent_finished.all(axis=-1, keepdims=True), did_topk_just_finished.shape\n-            ) & (early_stopping is True)\n-            add_penalty = ~did_topk_just_finished | beams_in_batch_are_full\n-            topk_log_probs += add_penalty * np.array(-1.0e7)\n-\n-            # 7. Get scores, sequences, is sentence finished for next.\n-            # Combine sequences, scores, and flags along the beam dimension and compare\n-            # new finished sequence scores to existing finished scores and select the\n-            # best from the new set of beams\n-            merged_sequences = jnp.concatenate([state.sequences, topk_sequences], axis=1)\n-            merged_scores = jnp.concatenate([state.scores, topk_log_probs], axis=1)\n-            merged_is_sent_finished = jnp.concatenate([state.is_sent_finished, did_topk_just_finished], axis=1)\n-            topk_merged_indices = lax.top_k(merged_scores, k=num_beams)[1]\n-            next_sequences, next_scores, next_is_sent_finished = gather_beams(\n-                [merged_sequences, merged_scores, merged_is_sent_finished], topk_merged_indices, batch_size, num_beams\n-            )\n-\n-            # 8. Update model kwargs.\n-            # Determine the top k beam indices from the original set of all beams.\n-            # With these, gather the top k beam-associated caches.\n-            next_running_indices = gather_beams(topk_beam_indices, next_topk_indices, batch_size, num_beams)\n-            next_cache = gather_beams(cache, next_running_indices, batch_size, num_beams)\n-            model_outputs[\"past_key_values\"] = jax.tree_util.tree_map(lambda x: flatten_beam_dim(x), next_cache)\n-            next_model_kwargs = self.update_inputs_for_generation(model_outputs, state.model_kwargs)\n-\n-            return BeamSearchState(\n-                cur_len=state.cur_len + 1,\n-                running_scores=next_running_scores,\n-                running_sequences=next_running_sequences,\n-                scores=next_scores,\n-                sequences=next_sequences,\n-                is_sent_finished=next_is_sent_finished,\n-                model_kwargs=next_model_kwargs,\n-            )\n-\n-        # Always run first iteration outside of `lax.while_loop` to avoid calling `beam_search_cond_fn`\n-        # when `state.cur_len` equals `decoder_prompt_len`. This also helps to comply with TPU when\n-        # the very first prompt has sequence length > 1.\n-        state = partial(beam_search_body_fn, input_ids_length=input_ids.shape[-1])(state)\n-\n-        if not trace:\n-            state = self._run_loop_in_debug(beam_search_cond_fn, beam_search_body_fn, state)\n-        else:\n-            state = lax.while_loop(beam_search_cond_fn, beam_search_body_fn, state)\n-\n-        # Account for the edge-case where there are no finished sequences for a\n-        # particular batch item. If so, return running sequences for that batch item.\n-        none_finished = jnp.any(state.is_sent_finished, axis=1)\n-        sequences = jnp.where(none_finished[:, None, None], state.sequences, state.running_sequences)\n-        scores = jnp.where(none_finished[:, None], state.scores, state.running_scores)\n-\n-        # Take best beams for each batch (the score is sorted in descending order)\n-        sequences = flatten_beam_dim(sequences[:, :num_return_sequences, :])\n-        scores = flatten_beam_dim(scores[:, :num_return_sequences])\n-\n-        return FlaxBeamSearchOutput(sequences=sequences, scores=scores)"
        },
        {
            "sha": "436793c402ea0c6b3e06c12b6763fecee6a14b64",
            "filename": "src/transformers/generation/tf_logits_process.py",
            "status": "removed",
            "additions": 0,
            "deletions": 600,
            "changes": 600,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fgeneration%2Ftf_logits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fgeneration%2Ftf_logits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Ftf_logits_process.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,600 +0,0 @@\n-# coding=utf-8\n-# Copyright 2022 The HuggingFace Inc. team\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import inspect\n-\n-import numpy as np\n-import tensorflow as tf\n-\n-from ..tf_utils import stable_softmax\n-from ..utils import add_start_docstrings\n-from ..utils.logging import get_logger\n-\n-\n-logger = get_logger(__name__)\n-\n-\n-TF_LOGITS_PROCESSOR_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`tf.Tensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary.\n-\n-            Indices can be obtained using [`PreTrainedTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        scores (`tf.Tensor` of shape `(batch_size, config.vocab_size)`):\n-            Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam\n-            search or log softmax for each vocabulary token when using beam search.\n-        cur_len (`int`):\n-            The current length of valid input sequence tokens. In the TF implementation, the input_ids' sequence length\n-            is the maximum length generate can produce, and we need to know which of its tokens are valid.\n-        kwargs (`dict[str, Any]`, *optional*):\n-            Additional logits processor specific kwargs.\n-\n-    Return:\n-        `tf.Tensor` of shape `(batch_size, config.vocab_size)`: The processed prediction scores.\n-\"\"\"\n-\n-\n-class TFLogitsProcessor:\n-    \"\"\"Abstract base class for all logit processors that can be applied during generation.\"\"\"\n-\n-    @add_start_docstrings(TF_LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n-    def __call__(self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int) -> tf.Tensor:\n-        \"\"\"TF method for processing logits.\"\"\"\n-        raise NotImplementedError(\n-            f\"{self.__class__} is an abstract class. Only classes inheriting this class can be called.\"\n-        )\n-\n-\n-class TFLogitsWarper:\n-    \"\"\"Abstract base class for all logit warpers that can be applied during generation with multinomial sampling.\"\"\"\n-\n-    @add_start_docstrings(TF_LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n-    def __call__(self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int) -> tf.Tensor:\n-        \"\"\"TF method for warping logits.\"\"\"\n-        raise NotImplementedError(\n-            f\"{self.__class__} is an abstract class. Only classes inheriting this class can be called.\"\n-        )\n-\n-\n-class TFLogitsProcessorList(list):\n-    \"\"\"\n-    This class can be used to create a list of [`TFLogitsProcessor`] to subsequently process a `scores` input tensor.\n-    This class inherits from list and adds a specific *__call__* method to apply each [`TFLogitsProcessor`] to the\n-    inputs.\n-    \"\"\"\n-\n-    @add_start_docstrings(TF_LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n-    def __call__(self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int, **kwargs) -> tf.Tensor:\n-        for processor in self:\n-            function_args = inspect.signature(processor.__call__).parameters\n-            if len(function_args) > 3:\n-                if not all(arg in kwargs for arg in list(function_args.keys())[2:]):\n-                    raise ValueError(\n-                        f\"Make sure that all the required parameters: {list(function_args.keys())} for \"\n-                        f\"{processor.__class__} are passed to the logits processor.\"\n-                    )\n-                scores = processor(input_ids, scores, cur_len, **kwargs)\n-            else:\n-                scores = processor(input_ids, scores, cur_len)\n-        return scores\n-\n-\n-class TFTemperatureLogitsWarper(TFLogitsWarper):\n-    r\"\"\"\n-    [`TFLogitsWarper`] for temperature (exponential scaling output probability distribution).\n-\n-    Args:\n-        temperature (`float`):\n-            The value used to module the logits distribution.\n-    \"\"\"\n-\n-    def __init__(self, temperature: float):\n-        if not isinstance(temperature, float) or not (temperature > 0):\n-            raise ValueError(f\"`temperature` has to be a strictly positive float, but is {temperature}\")\n-\n-        self.temperature = temperature\n-\n-    def __call__(self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int) -> tf.Tensor:\n-        scores = scores / self.temperature\n-        return scores\n-\n-\n-class TFTopKLogitsWarper(TFLogitsWarper):\n-    r\"\"\"\n-    [`TFLogitsWarper`] that performs top-k, i.e. restricting to the k highest probability elements.\n-\n-    Args:\n-        top_k (`int`):\n-            The number of highest probability vocabulary tokens to keep for top-k-filtering.\n-        filter_value (`float`, *optional*, defaults to -inf):\n-            All filtered values will be set to this float value.\n-        min_tokens_to_keep (`int`, *optional*, defaults to 1):\n-            Minimum number of tokens that cannot be filtered.\n-    \"\"\"\n-\n-    def __init__(self, top_k: int, filter_value: float = -float(\"Inf\"), min_tokens_to_keep: int = 1):\n-        if not isinstance(top_k, int) or top_k <= 0:\n-            raise ValueError(f\"`top_k` has to be a strictly positive integer, but is {top_k}\")\n-\n-        self.top_k = max(top_k, min_tokens_to_keep)\n-        self.filter_value = filter_value\n-\n-    def __call__(self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int) -> tf.Tensor:\n-        top_k = min(self.top_k, scores.shape[-1])  # Safety check\n-        # Boolean mask containing all tokens with a probability less than the last token of the top-k\n-        indices_to_remove = scores < tf.math.top_k(scores, k=top_k)[0][..., -1:]\n-        next_scores = tf.where(indices_to_remove, self.filter_value, scores)\n-        return next_scores\n-\n-\n-class TFTopPLogitsWarper(TFLogitsWarper):\n-    \"\"\"\n-    [`TFLogitsWarper`] that performs top-p, i.e. restricting to top tokens summing to <= prob_cut_off.\n-\n-    Args:\n-        top_p (`float`):\n-            If set to < 1, only the smallest set of most probable tokens with probabilities that add up to `top_p` or\n-            higher are kept for generation.\n-        filter_value (`float`, *optional*, defaults to -inf):\n-            All filtered values will be set to this float value.\n-        min_tokens_to_keep (`int`, *optional*, defaults to 1):\n-            Minimum number of tokens that cannot be filtered.\n-    \"\"\"\n-\n-    def __init__(self, top_p: float, filter_value: float = -float(\"Inf\"), min_tokens_to_keep: int = 1):\n-        if not isinstance(top_p, float) or (top_p < 0 or top_p > 1.0):\n-            raise ValueError(f\"`top_p` has to be a float > 0 and < 1, but is {top_p}\")\n-        if not isinstance(min_tokens_to_keep, int) or (min_tokens_to_keep < 1):\n-            raise ValueError(f\"`min_tokens_to_keep` has to be a positive integer, but is {min_tokens_to_keep}\")\n-\n-        self.top_p = top_p\n-        self.filter_value = filter_value\n-        self.min_tokens_to_keep = min_tokens_to_keep\n-\n-    def __call__(self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int) -> tf.Tensor:\n-        topk_scores, topk_indices = tf.math.top_k(scores, scores.shape[-1])\n-\n-        mask_scores = tf.fill(scores.shape, self.filter_value)\n-        cumulative_probs = tf.math.cumsum(stable_softmax(topk_scores, axis=-1), axis=-1)\n-        score_mask = cumulative_probs < self.top_p\n-\n-        # Also include the token that is higher than top_p (the first false = shift and insert a True on the left)\n-        score_mask = tf.concat((tf.ones([score_mask.shape[0], 1], dtype=tf.bool), score_mask[:, :-1]), axis=-1)\n-\n-        # Ensure min tokens to keep\n-        score_mask = tf.concat(\n-            (\n-                tf.ones([score_mask.shape[0], self.min_tokens_to_keep], dtype=tf.bool),\n-                score_mask[:, self.min_tokens_to_keep :],\n-            ),\n-            axis=-1,\n-        )\n-\n-        # Mask the values that do not fit the criteria\n-        topk_next_scores = tf.where(score_mask, topk_scores, mask_scores)\n-\n-        # Undo the topk sorting: converts the 2D matrix of per-row original indices of shape (batch_size, vocab_size)\n-        # to a 3D tensor of shape (batch_size, vocab_size, 2) containing the original score coordinate, from which we\n-        # can scatter (i.e. `scatter_indices[row, col, :]` is a tensor containing `[row, topk_indices[row, col]]`)\n-        scatter_rows = tf.tile(tf.expand_dims(tf.range(topk_indices.shape[0]), axis=-1), [1, topk_indices.shape[-1]])\n-        scatter_indices = tf.stack((scatter_rows, topk_indices), axis=-1)\n-        next_scores = tf.scatter_nd(scatter_indices, topk_next_scores, shape=topk_next_scores.shape)\n-\n-        return next_scores\n-\n-\n-class TFMinLengthLogitsProcessor(TFLogitsProcessor):\n-    r\"\"\"\n-    [`TFLogitsProcessor`] enforcing a min-length by setting EOS probability to 0.\n-\n-    Args:\n-        min_length (`int`):\n-            The minimum length below which the score of `eos_token_id` is set to `-float(\"Inf\")`.\n-        eos_token_id (`int`):\n-            The id of the *end-of-sequence* token.\n-    \"\"\"\n-\n-    def __init__(self, min_length: int, eos_token_id: int):\n-        if not isinstance(min_length, int) or min_length < 0:\n-            raise ValueError(f\"`min_length` has to be a positive integer, but is {min_length}\")\n-\n-        if not isinstance(eos_token_id, int) or eos_token_id < 0:\n-            raise ValueError(f\"`eos_token_id` has to be a positive integer, but is {eos_token_id}\")\n-\n-        self.min_length = min_length\n-        self.eos_token_id = eos_token_id\n-\n-    def _apply_eos_token_mask(self, scores: tf.Tensor) -> tf.Tensor:\n-        eos_token_id_mask = tf.range(scores.shape[-1]) == self.eos_token_id\n-        scores = tf.where(eos_token_id_mask, float(\"-inf\"), scores)\n-        return scores\n-\n-    def __call__(self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int) -> tf.Tensor:\n-        # applies eos token masking if the first argument is true\n-        scores = tf.cond(\n-            tf.less(cur_len, self.min_length),\n-            lambda: self._apply_eos_token_mask(scores),\n-            lambda: tf.identity(scores),\n-        )\n-        return scores\n-\n-\n-class TFRepetitionPenaltyLogitsProcessor(TFLogitsProcessor):\n-    r\"\"\"\n-    [`TFLogitsProcessor`] enforcing an exponential penalty on repeated sequences.\n-\n-    Args:\n-        repetition_penalty (`float`):\n-            The parameter for repetition penalty. 1.0 means no penalty. See [this\n-            paper](https://huggingface.co/papers/1909.05858) for more details.\n-    \"\"\"\n-\n-    def __init__(self, penalty: float):\n-        if not isinstance(penalty, float) or not (penalty > 0):\n-            raise ValueError(f\"`penalty` has to be a strictly positive float, but is {penalty}\")\n-\n-        self.penalty = penalty\n-\n-    def _create_score_penalties(self, input_ids: tf.Tensor, logits: tf.Tensor) -> tf.Tensor:\n-        # We want to populate the penalties in the positions of `input_ids`. Since XLA can't handle shapes unknown\n-        # before runtime, `tf.unique` can't be used. Therefore, we may have redundant updates, when a given row has\n-        # the same token multiple times.\n-\n-        # Gathers the penalties to apply\n-        logit_penalties = tf.gather(logits, input_ids, axis=1, batch_dims=1)\n-        logit_penalties = tf.where(logit_penalties > 0, 1 / self.penalty, logit_penalties)\n-        logit_penalties = tf.where(logit_penalties < 0, self.penalty, logit_penalties)\n-\n-        # Scatters the penalties\n-        token_penalties = tf.ones(logits.shape)\n-        batch_size = input_ids.shape[0]\n-        seq_len = tf.shape(input_ids)[1]  # the sequence length has dynamic size, hence the dynamic shape\n-        indexable_prev_input_ids = tf.concat(\n-            (\n-                tf.expand_dims(tf.repeat(tf.range(batch_size), seq_len), axis=-1),\n-                tf.expand_dims(tf.reshape(input_ids, [-1]), axis=-1),\n-            ),\n-            axis=1,\n-        )\n-        token_penalties = tf.tensor_scatter_nd_update(\n-            token_penalties, indices=indexable_prev_input_ids, updates=tf.reshape(logit_penalties, [-1])\n-        )\n-        return token_penalties\n-\n-    def __call__(self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int) -> tf.Tensor:\n-        score_penalties = self._create_score_penalties(input_ids[:, :cur_len], scores)\n-\n-        scores = tf.math.multiply(scores, score_penalties)\n-\n-        return scores\n-\n-\n-class TFNoBadWordsLogitsProcessor(TFLogitsProcessor):\n-    \"\"\"\n-    [`TFLogitsProcessor`] that enforces that specified sequences will never be sampled.\n-\n-    Args:\n-        bad_words_ids (`list[list[int]]`):\n-            List of list of token ids that are not allowed to be generated. In order to get the tokens of the words\n-            that should not appear in the generated text, make sure to set `add_prefix_space=True` when initializing\n-            the tokenizer, and use `tokenizer(bad_words, add_special_tokens=False).input_ids`. The `add_prefix_space`\n-            argument is only supported for some slow tokenizers, as fast tokenizers' prefixing behaviours come from\n-            `pre tokenizers`. Read more [here](https://huggingface.co/docs/tokenizers/api/pre-tokenizers).\n-        eos_token_id (`int`):\n-            The id of the *end-of-sequence* token.\n-    \"\"\"\n-\n-    def __init__(self, bad_words_ids: list[list[int]], eos_token_id: int):\n-        if not isinstance(bad_words_ids, list) or len(bad_words_ids) == 0:\n-            raise ValueError(f\"`bad_words_ids` has to be a non-empty list, but is {bad_words_ids}.\")\n-        if any(not isinstance(bad_word_ids, list) for bad_word_ids in bad_words_ids):\n-            raise ValueError(f\"`bad_words_ids` has to be a list of lists, but is {bad_words_ids}.\")\n-        if any(\n-            any((not isinstance(token_id, (int, np.integer)) or token_id < 0) for token_id in bad_word_ids)\n-            for bad_word_ids in bad_words_ids\n-        ):\n-            raise ValueError(\n-                f\"Each list in `bad_words_ids` has to be a list of positive integers, but is {bad_words_ids}.\"\n-            )\n-\n-        # stores the information about bad words in three tensors:\n-        # 1. a rectangular tensor with the forbidden sequences (padded with `-1`), for full data comparisons\n-        self.bad_word_seqs_ids = tf.ragged.constant(bad_words_ids).to_tensor(default_value=-1)\n-        # 2. a tensor with the unpadded length of each forbidden sequence, for quick length comparisons\n-        bad_word_seqs_len = [len(bad_words) for bad_words in bad_words_ids]\n-        if any(word_len == 0 for word_len in bad_word_seqs_len):\n-            raise ValueError(f\"Banned words token sequences {bad_words_ids} cannot have an empty list\")\n-        self.bad_word_seqs_len = tf.convert_to_tensor(bad_word_seqs_len, dtype=tf.int32)\n-        # 3. a tensor containing the last token for each sequence, for easy access to the tokens that may be banned\n-        self.seq_forbidden_tokens = tf.convert_to_tensor([bad_words[-1] for bad_words in bad_words_ids])\n-\n-    def _calc_row_banned_bad_tokens(self, row_input_ids: tf.Tensor) -> tf.Tensor:\n-        def _tokens_match(bad_word_seq_number):\n-            def _len_one():\n-                # If the bad sequence only has one token, always mask it\n-                return tf.cond(\n-                    tf.math.equal(self.bad_word_seqs_len[bad_word_seq_number], 1),\n-                    lambda: tf.ones((), dtype=tf.bool),\n-                    _len_greater_than_cur_len,\n-                )\n-\n-            def _len_greater_than_cur_len():\n-                # Otherwise, if the bad sequence is longer than the current length they can't ever match\n-                return tf.cond(\n-                    tf.math.greater(self.bad_word_seqs_len[bad_word_seq_number], tf.shape(row_input_ids)[0]),\n-                    lambda: tf.zeros((), dtype=tf.bool),\n-                    _match_found,\n-                )\n-\n-            def _match_found():\n-                # Finally, runs the actual comparison. Can only be called if the previous comparisons do not yield\n-                # an answer (otherwise we get indexing exceptions)\n-                compare_len = self.bad_word_seqs_len[bad_word_seq_number] - 1\n-                return tf.cond(\n-                    tf.math.reduce_all(\n-                        tf.math.equal(\n-                            row_input_ids[-compare_len:], self.bad_word_seqs_ids[bad_word_seq_number, :compare_len]\n-                        )\n-                    ),\n-                    lambda: tf.ones((), dtype=tf.bool),\n-                    lambda: tf.zeros((), dtype=tf.bool),\n-                )\n-\n-            match = _len_one()\n-            return match\n-\n-        # Compares the current row against all bad word sequences, obtaining a mask with the matches.\n-        match_mask = tf.map_fn(_tokens_match, tf.range(self.bad_word_seqs_ids.shape[0]), fn_output_signature=tf.bool)\n-        row_banned_tokens = self.seq_forbidden_tokens[match_mask]\n-        return row_banned_tokens\n-\n-    def __call__(self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int) -> tf.Tensor:\n-        # We want to mask some banned tokens, at a score level. Since the banned tokens depend on the previous\n-        # `input_ids`, they may have a different length for each row, and they may even be empty for some rows.\n-        # To remain simple and XLA-compatible, we work on a per-row fashion.\n-        # TODO (Joao): this function might trigger XLA retracing as `cur_len` increases. Fix it if it becomes\n-        # a frequent choke point. (make `cur_len` a tensor?)\n-        def _get_row_updated_score(row_inputs: tuple[tf.Tensor]) -> tf.Tensor:\n-            row_input_ids, row_score = row_inputs\n-            banned_tokens = self._calc_row_banned_bad_tokens(row_input_ids[:cur_len])\n-            banned_tokens_mask = tf.scatter_nd(\n-                indices=tf.expand_dims(banned_tokens, axis=-1),\n-                updates=tf.ones_like(banned_tokens, dtype=tf.bool),\n-                shape=row_score.shape,\n-            )\n-            row_score = tf.where(banned_tokens_mask, -float(\"inf\"), row_score)\n-            return row_score\n-\n-        scores = tf.map_fn(_get_row_updated_score, (input_ids, scores), fn_output_signature=tf.float32)\n-        return scores\n-\n-\n-class TFNoRepeatNGramLogitsProcessor(TFLogitsProcessor):\n-    r\"\"\"\n-    [`TFLogitsProcessor`] that enforces no repetition of n-grams. See\n-    [Fairseq](https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345).\n-\n-    Args:\n-        ngram_size (`int`):\n-            All ngrams of size `ngram_size` can only occur once.\n-    \"\"\"\n-\n-    def __init__(self, ngram_size: int):\n-        if not isinstance(ngram_size, int) or ngram_size <= 0:\n-            raise ValueError(f\"`ngram_size` has to be a strictly positive integer, but is {ngram_size}\")\n-        self.ngram_size = ngram_size\n-\n-    def calc_banned_ngram_tokens(self, input_ids, num_hypos, cur_len):\n-        # Copied from fairseq for no_repeat_ngram in beam_search\n-        if cur_len + 1 < self.ngram_size:\n-            # return no banned tokens if we haven't generated ngram_size tokens yet\n-            return [[] for _ in range(num_hypos)]\n-        generated_ngrams = [{} for _ in range(num_hypos)]\n-        prev_input_ids = input_ids[:, :cur_len]\n-        for idx in range(num_hypos):\n-            gen_tokens = prev_input_ids[idx].numpy().tolist()\n-            generated_ngram = generated_ngrams[idx]\n-            for ngram in zip(*[gen_tokens[i:] for i in range(self.ngram_size)]):\n-                prev_ngram_tuple = tuple(ngram[:-1])\n-                generated_ngram[prev_ngram_tuple] = generated_ngram.get(prev_ngram_tuple, []) + [ngram[-1]]\n-\n-        def _get_generated_ngrams(hypo_idx):\n-            # Before decoding the next token, prevent decoding of ngrams that have already appeared\n-            start_idx = cur_len + 1 - self.ngram_size\n-            ngram_idx = tuple(prev_input_ids[hypo_idx, start_idx:cur_len].numpy().tolist())\n-            return generated_ngrams[hypo_idx].get(ngram_idx, [])\n-\n-        banned_tokens = [_get_generated_ngrams(hypo_idx) for hypo_idx in range(num_hypos)]\n-\n-        return banned_tokens\n-\n-    def __call__(self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int) -> tf.Tensor:\n-        # TODO (joao): enable XLA on this logits processor. See discussion and attempts in\n-        # https://github.com/huggingface/transformers/pull/16974\n-        if not tf.executing_eagerly():\n-            raise NotImplementedError(\"TFNoRepeatNGramLogitsProcessor is only implemented for eager execution.\")\n-\n-        batch_size, vocab_size = scores.shape\n-        banned_tokens = self.calc_banned_ngram_tokens(input_ids, batch_size, cur_len)\n-\n-        # create banned_tokens boolean mask\n-        banned_tokens_indices_mask = []\n-        for banned_tokens_slice in banned_tokens:\n-            banned_tokens_indices_mask.append([token in banned_tokens_slice for token in range(vocab_size)])\n-\n-        scores = tf.where(tf.convert_to_tensor(banned_tokens_indices_mask, dtype=tf.bool), -float(\"inf\"), scores)\n-\n-        return scores\n-\n-\n-class TFForcedBOSTokenLogitsProcessor(TFLogitsProcessor):\n-    r\"\"\"\n-    [`TFLogitsProcessor`] that enforces the specified token as the first generated token.\n-\n-    Args:\n-        bos_token_id (`int`):\n-            The id of the token to force as the first generated token.\n-    \"\"\"\n-\n-    def __init__(self, bos_token_id: int):\n-        if bos_token_id < 0:\n-            raise ValueError(f\"The forced bos token id  must be a non-negative integer, got {bos_token_id}\")\n-        self.bos_token_id = bos_token_id\n-\n-    def __call__(self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int) -> tf.Tensor:\n-        if cur_len == 1:\n-            batch_size, num_tokens = scores.shape\n-            # sets the score to 0 in the bos_token_id column\n-            scores = tf.zeros((batch_size, 1))\n-            # sets the score to -inf everywhere else\n-            if self.bos_token_id > 0:\n-                scores = tf.concat((tf.broadcast_to(-float(\"inf\"), (batch_size, self.bos_token_id)), scores), axis=-1)\n-            if self.bos_token_id < (num_tokens - 1):\n-                scores = tf.concat(\n-                    (scores, tf.broadcast_to(-float(\"inf\"), (batch_size, (num_tokens - 1) - self.bos_token_id))),\n-                    axis=-1,\n-                )\n-        return scores\n-\n-\n-class TFForcedEOSTokenLogitsProcessor(TFLogitsProcessor):\n-    r\"\"\"\n-    [`TFLogitsProcessor`] that enforces the specified token as the last generated token when `max_length` is reached.\n-\n-    Args:\n-        max_length (`int`):\n-            The maximum length of the sequence to be generated.\n-        eos_token_id (`int`):\n-            The id of the token to force as the last generated token when `max_length` is reached.\n-    \"\"\"\n-\n-    def __init__(self, max_length: int, eos_token_id: int):\n-        self.max_length = max_length\n-        if eos_token_id < 0:\n-            raise ValueError(f\"The forced eos token id must be a non-negative integer, got {eos_token_id}\")\n-        self.eos_token_id = eos_token_id\n-\n-    def __call__(self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int) -> tf.Tensor:\n-        if cur_len == self.max_length - 1:\n-            batch_size, num_tokens = scores.shape\n-            # sets the score to 0 in the eos_token_id column\n-            scores = tf.zeros((batch_size, 1))\n-            # sets the score to -inf everywhere else\n-            if self.eos_token_id > 0:\n-                scores = tf.concat((tf.broadcast_to(-float(\"inf\"), (batch_size, self.eos_token_id)), scores), axis=-1)\n-            if self.eos_token_id < (num_tokens - 1):\n-                scores = tf.concat(\n-                    (scores, tf.broadcast_to(-float(\"inf\"), (batch_size, (num_tokens - 1) - self.eos_token_id))),\n-                    axis=-1,\n-                )\n-        return scores\n-\n-\n-class TFSuppressTokensAtBeginLogitsProcessor(TFLogitsProcessor):\n-    r\"\"\"\n-    [`TFSuppressTokensAtBeginLogitsProcessor`] suppresses a list of tokens as soon as the `generate` function starts\n-    generating using `begin_index` tokens. This should ensure that the tokens defined by `begin_suppress_tokens` at not\n-    sampled at the beginning of the generation.\n-    \"\"\"\n-\n-    def __init__(self, begin_suppress_tokens, begin_index):\n-        self.begin_suppress_tokens = list(begin_suppress_tokens)\n-        self.begin_index = begin_index\n-\n-    def __call__(self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int) -> tf.Tensor:\n-        suppressed_indices = []\n-        for token in self.begin_suppress_tokens:\n-            if token < scores.shape[-1]:  # to ensure we don't go beyond the vocab size\n-                suppressed_indices.extend([[i, token] for i in range(scores.shape[0])])\n-\n-        if len(suppressed_indices) > 0:\n-            scores = tf.cond(\n-                tf.equal(cur_len, self.begin_index),\n-                lambda: tf.tensor_scatter_nd_update(\n-                    scores,\n-                    indices=suppressed_indices,\n-                    updates=[-float(\"inf\") for _ in range(scores.shape[0] * len(self.begin_suppress_tokens))],\n-                ),\n-                lambda: scores,\n-            )\n-        return scores\n-\n-\n-class TFSuppressTokensLogitsProcessor(TFLogitsProcessor):\n-    r\"\"\"This processor can be used to suppress a list of tokens. The processor will set their log probs to `-inf` so that they\n-    are not sampled.\"\"\"\n-\n-    def __init__(self, suppress_tokens):\n-        self.suppress_tokens = list(suppress_tokens)\n-\n-    def __call__(self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int) -> tf.Tensor:\n-        suppressed_indices = []\n-        for token in self.suppress_tokens:\n-            if token < scores.shape[-1]:  # to ensure we don't go beyond the vocab size\n-                suppressed_indices.extend([[i, token] for i in range(scores.shape[0])])\n-\n-        if len(suppressed_indices) > 0:\n-            scores = tf.tensor_scatter_nd_update(\n-                scores,\n-                indices=[[i, token] for i in range(scores.shape[0]) for token in self.suppress_tokens],\n-                updates=[-float(\"inf\") for _ in range(scores.shape[0] * len(self.suppress_tokens))],\n-            )\n-        return scores\n-\n-\n-class TFForceTokensLogitsProcessor(TFLogitsProcessor):\n-    r\"\"\"This processor takes a list of pairs of integers which indicates a mapping from generation indices to token\n-    indices that will be forced before sampling. The processor will set their log probs to `0` and all other tokens to\n-    `-inf` so that they are sampled at their corresponding index.\"\"\"\n-\n-    def __init__(self, force_token_map: list[list[int]]):\n-        force_token_map = dict(force_token_map)\n-        # Converts the dictionary of format {index: token} containing the tokens to be forced to an array, where the\n-        # index of the array corresponds to the index of the token to be forced, for XLA compatibility.\n-        # Indexes without forced tokens will have an negative value.\n-        force_token_array = np.ones((max(force_token_map.keys()) + 1), dtype=np.int32) * -1\n-        for index, token in force_token_map.items():\n-            if token is not None:\n-                force_token_array[index] = token\n-        self.force_token_array = tf.convert_to_tensor(force_token_array, dtype=tf.int32)\n-\n-    def __call__(self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int) -> tf.Tensor:\n-        def _force_token(generation_idx):\n-            batch_size = scores.shape[0]\n-            current_token = self.force_token_array[generation_idx]\n-\n-            new_scores = tf.zeros_like(scores, dtype=scores.dtype) + tf.constant([scores.dtype.min])\n-            indices = tf.stack((tf.range(batch_size), tf.tile([current_token], [batch_size])), axis=1)\n-            updates = tf.zeros((batch_size,), dtype=scores.dtype)\n-            new_scores = tf.tensor_scatter_nd_update(new_scores, indices, updates)\n-            return new_scores\n-\n-        scores = tf.cond(\n-            tf.greater_equal(cur_len, tf.shape(self.force_token_array)[0]),\n-            # If the current length is geq than the length of force_token_array, the processor does nothing.\n-            lambda: tf.identity(scores),\n-            # Otherwise, it may force a certain token.\n-            lambda: tf.cond(\n-                tf.greater_equal(self.force_token_array[cur_len], 0),\n-                # Only valid (positive) tokens are forced\n-                lambda: _force_token(cur_len),\n-                # Otherwise, the processor does nothing.\n-                lambda: scores,\n-            ),\n-        )\n-        return scores"
        },
        {
            "sha": "be51c9cd9f439b66a4ee15ed734d3947f43901b9",
            "filename": "src/transformers/generation/tf_utils.py",
            "status": "removed",
            "additions": 0,
            "deletions": 3132,
            "changes": 3132,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fgeneration%2Ftf_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fgeneration%2Ftf_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Ftf_utils.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "8bd65e9bc3cea94351ece89c63cb325cd459a8e4",
            "filename": "src/transformers/image_processing_base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fimage_processing_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fimage_processing_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_base.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -55,7 +55,7 @@ class BatchFeature(BaseBatchFeature):\n         data (`dict`):\n             Dictionary of lists/arrays/tensors returned by the __call__ method ('pixel_values', etc.).\n         tensor_type (`Union[None, str, TensorType]`, *optional*):\n-            You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at\n+            You can give a tensor_type here to convert the lists of integers in PyTorch/Numpy Tensors at\n             initialization.\n     \"\"\"\n "
        },
        {
            "sha": "2aba3d549719397ed8911ac69d96aebf14c190bd",
            "filename": "src/transformers/image_transforms.py",
            "status": "modified",
            "additions": 5,
            "deletions": 44,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fimage_transforms.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fimage_transforms.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_transforms.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -26,10 +26,8 @@\n     get_image_size,\n     infer_channel_dimension_format,\n )\n-from .utils import ExplicitEnum, TensorType, is_jax_tensor, is_tf_tensor, is_torch_tensor\n+from .utils import ExplicitEnum, TensorType, is_torch_tensor\n from .utils.import_utils import (\n-    is_flax_available,\n-    is_tf_available,\n     is_torch_available,\n     is_vision_available,\n     requires_backends,\n@@ -44,12 +42,6 @@\n if is_torch_available():\n     import torch\n \n-if is_tf_available():\n-    import tensorflow as tf\n-\n-if is_flax_available():\n-    import jax.numpy as jnp\n-\n \n def to_channel_dimension_format(\n     image: np.ndarray,\n@@ -160,7 +152,7 @@ def _rescale_for_pil_conversion(image):\n \n \n def to_pil_image(\n-    image: Union[np.ndarray, \"PIL.Image.Image\", \"torch.Tensor\", \"tf.Tensor\", \"jnp.ndarray\"],\n+    image: Union[np.ndarray, \"PIL.Image.Image\", \"torch.Tensor\"],\n     do_rescale: Optional[bool] = None,\n     image_mode: Optional[str] = None,\n     input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -170,7 +162,7 @@ def to_pil_image(\n     needed.\n \n     Args:\n-        image (`PIL.Image.Image` or `numpy.ndarray` or `torch.Tensor` or `tf.Tensor`):\n+        image (`PIL.Image.Image` or `numpy.ndarray` or `torch.Tensor`):\n             The image to convert to the `PIL.Image` format.\n         do_rescale (`bool`, *optional*):\n             Whether or not to apply the scaling factor (to make pixel values integers between 0 and 255). Will default\n@@ -190,10 +182,8 @@ def to_pil_image(\n         return image\n \n     # Convert all tensors to numpy arrays before converting to PIL image\n-    if is_torch_tensor(image) or is_tf_tensor(image):\n+    if is_torch_tensor(image):\n         image = image.numpy()\n-    elif is_jax_tensor(image):\n-        image = np.array(image)\n     elif not isinstance(image, np.ndarray):\n         raise ValueError(f\"Input image type not supported: {type(image)}\")\n \n@@ -556,16 +546,6 @@ def _center_to_corners_format_numpy(bboxes_center: np.ndarray) -> np.ndarray:\n     return bboxes_corners\n \n \n-def _center_to_corners_format_tf(bboxes_center: \"tf.Tensor\") -> \"tf.Tensor\":\n-    center_x, center_y, width, height = tf.unstack(bboxes_center, axis=-1)\n-    bboxes_corners = tf.stack(\n-        # top left x, top left y, bottom right x, bottom right y\n-        [center_x - 0.5 * width, center_y - 0.5 * height, center_x + 0.5 * width, center_y + 0.5 * height],\n-        axis=-1,\n-    )\n-    return bboxes_corners\n-\n-\n # 2 functions below inspired by https://github.com/facebookresearch/detr/blob/master/util/box_ops.py\n def center_to_corners_format(bboxes_center: TensorType) -> TensorType:\n     \"\"\"\n@@ -576,14 +556,11 @@ def center_to_corners_format(bboxes_center: TensorType) -> TensorType:\n     corners format: contains the coordinates for the top-left and bottom-right corners of the box\n         (top_left_x, top_left_y, bottom_right_x, bottom_right_y)\n     \"\"\"\n-    # Function is used during model forward pass, so we use the input framework if possible, without\n-    # converting to numpy\n+    # Function is used during model forward pass, so we use torch if relevant, without converting to numpy\n     if is_torch_tensor(bboxes_center):\n         return _center_to_corners_format_torch(bboxes_center)\n     elif isinstance(bboxes_center, np.ndarray):\n         return _center_to_corners_format_numpy(bboxes_center)\n-    elif is_tf_tensor(bboxes_center):\n-        return _center_to_corners_format_tf(bboxes_center)\n \n     raise ValueError(f\"Unsupported input type {type(bboxes_center)}\")\n \n@@ -613,20 +590,6 @@ def _corners_to_center_format_numpy(bboxes_corners: np.ndarray) -> np.ndarray:\n     return bboxes_center\n \n \n-def _corners_to_center_format_tf(bboxes_corners: \"tf.Tensor\") -> \"tf.Tensor\":\n-    top_left_x, top_left_y, bottom_right_x, bottom_right_y = tf.unstack(bboxes_corners, axis=-1)\n-    bboxes_center = tf.stack(\n-        [\n-            (top_left_x + bottom_right_x) / 2,  # center x\n-            (top_left_y + bottom_right_y) / 2,  # center y\n-            (bottom_right_x - top_left_x),  # width\n-            (bottom_right_y - top_left_y),  # height\n-        ],\n-        axis=-1,\n-    )\n-    return bboxes_center\n-\n-\n def corners_to_center_format(bboxes_corners: TensorType) -> TensorType:\n     \"\"\"\n     Converts bounding boxes from corners format to center format.\n@@ -641,8 +604,6 @@ def corners_to_center_format(bboxes_corners: TensorType) -> TensorType:\n         return _corners_to_center_format_torch(bboxes_corners)\n     elif isinstance(bboxes_corners, np.ndarray):\n         return _corners_to_center_format_numpy(bboxes_corners)\n-    elif is_tf_tensor(bboxes_corners):\n-        return _corners_to_center_format_tf(bboxes_corners)\n \n     raise ValueError(f\"Unsupported input type {type(bboxes_corners)}\")\n "
        },
        {
            "sha": "1d988f99379c5368083a21aa9cba109cf483c65e",
            "filename": "src/transformers/image_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 12,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fimage_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fimage_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_utils.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -24,9 +24,7 @@\n \n from .utils import (\n     ExplicitEnum,\n-    is_jax_tensor,\n     is_numpy_array,\n-    is_tf_tensor,\n     is_torch_available,\n     is_torch_tensor,\n     is_torchvision_available,\n@@ -107,8 +105,6 @@ class ImageType(ExplicitEnum):\n     PIL = \"pillow\"\n     TORCH = \"torch\"\n     NUMPY = \"numpy\"\n-    TENSORFLOW = \"tensorflow\"\n-    JAX = \"jax\"\n \n \n def get_image_type(image):\n@@ -118,15 +114,11 @@ def get_image_type(image):\n         return ImageType.TORCH\n     if is_numpy_array(image):\n         return ImageType.NUMPY\n-    if is_tf_tensor(image):\n-        return ImageType.TENSORFLOW\n-    if is_jax_tensor(image):\n-        return ImageType.JAX\n     raise ValueError(f\"Unrecognized image type {type(image)}\")\n \n \n def is_valid_image(img):\n-    return is_pil_image(img) or is_numpy_array(img) or is_torch_tensor(img) or is_tf_tensor(img) or is_jax_tensor(img)\n+    return is_pil_image(img) or is_numpy_array(img) or is_torch_tensor(img)\n \n \n def is_valid_list_of_images(images: list):\n@@ -205,8 +197,7 @@ def make_list_of_images(images, expected_ndims: int = 3) -> list[ImageInput]:\n             )\n         return images\n     raise ValueError(\n-        \"Invalid image type. Expected either PIL.Image.Image, numpy.ndarray, torch.Tensor, tf.Tensor or \"\n-        f\"jax.ndarray, but got {type(images)}.\"\n+        f\"Invalid image type. Expected either PIL.Image.Image, numpy.ndarray, or torch.Tensor, but got {type(images)}.\"\n     )\n \n \n@@ -570,7 +561,6 @@ def validate_preprocess_arguments(\n         raise ValueError(\"`size` and `resample/interpolation` must be specified if `do_resize` is `True`.\")\n \n \n-# In the future we can add a TF implementation here when we have TF models.\n class ImageFeatureExtractionMixin:\n     \"\"\"\n     Mixin that contain utilities for preparing image features."
        },
        {
            "sha": "6cec1183c5c7fc582240986975fd1c298834dae7",
            "filename": "src/transformers/integrations/integration_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 10,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -48,16 +48,13 @@\n     flatten_dict,\n     is_datasets_available,\n     is_pandas_available,\n-    is_tf_available,\n     is_torch_available,\n     logging,\n )\n \n \n logger = logging.get_logger(__name__)\n \n-if is_tf_available():\n-    from .. import TFPreTrainedModel\n \n if is_torch_available():\n     import torch\n@@ -760,12 +757,6 @@ def save_model_architecture_to_file(model: Any, output_dir: str):\n     with open(f\"{output_dir}/model_architecture.txt\", \"w+\") as f:\n         if isinstance(model, PreTrainedModel):\n             print(model, file=f)\n-        elif is_tf_available() and isinstance(model, TFPreTrainedModel):\n-\n-            def print_to_file(s):\n-                print(s, file=f)\n-\n-            model.summary(print_fn=print_to_file)\n         elif is_torch_available() and (\n             isinstance(model, (torch.nn.Module, PushToHubMixin)) and hasattr(model, \"base_model\")\n         ):\n@@ -1225,7 +1216,7 @@ def setup(self, args, state, model):\n         - **COMET_PROJECT_NAME** (`str`, *optional*):\n             Comet project name for experiments.\n         - **COMET_LOG_ASSETS** (`str`, *optional*, defaults to `TRUE`):\n-            Whether or not to log training assets (tf event logs, checkpoints, etc), to Comet. Can be `TRUE`, or\n+            Whether or not to log training assets (checkpoints, etc), to Comet. Can be `TRUE`, or\n             `FALSE`.\n \n         For a number of configurable items in the environment, see"
        },
        {
            "sha": "ab7fc4615b473d59c903260a8c1ec80b24f4af7b",
            "filename": "src/transformers/keras_callbacks.py",
            "status": "removed",
            "additions": 0,
            "deletions": 413,
            "changes": 413,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fkeras_callbacks.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fkeras_callbacks.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkeras_callbacks.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,413 +0,0 @@\n-import logging\n-import os\n-from pathlib import Path\n-from time import sleep\n-from typing import Callable, Optional, Union\n-\n-import numpy as np\n-import tensorflow as tf\n-from huggingface_hub import Repository, create_repo\n-from packaging.version import parse\n-\n-from . import IntervalStrategy, PreTrainedTokenizerBase\n-from .modelcard import TrainingSummary\n-from .modeling_tf_utils import keras\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-class KerasMetricCallback(keras.callbacks.Callback):\n-    \"\"\"\n-    Callback to compute metrics at the end of every epoch. Unlike normal Keras metrics, these do not need to be\n-    compilable by TF. It is particularly useful for common NLP metrics like BLEU and ROUGE that require string\n-    operations or generation loops that cannot be compiled. Predictions (or generations) will be computed on the\n-    `eval_dataset` before being passed to the `metric_fn` in `np.ndarray` format. The `metric_fn` should compute\n-    metrics and return a dict mapping metric names to metric values.\n-\n-    We provide an example of a suitable metric_fn that computes ROUGE scores for a summarization model below. Note that\n-    this example skips some post-processing for readability and simplicity, and should probably not be used as-is!\n-\n-    ```py\n-    from datasets import load_metric\n-\n-    rouge_metric = load_metric(\"rouge\")\n-\n-\n-    def rouge_fn(predictions, labels):\n-        decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n-        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n-        result = rouge_metric.compute(predictions=decoded_predictions, references=decoded_labels)\n-        return {key: value.mid.fmeasure * 100 for key, value in result.items()}\n-    ```\n-\n-    The above function will return a dict containing values which will be logged like any other Keras metric:\n-\n-    ```\n-    {'rouge1': 37.4199, 'rouge2': 13.9768, 'rougeL': 34.361, 'rougeLsum': 35.0781\n-    ```\n-\n-    Args:\n-        metric_fn (`Callable`):\n-            Metric function provided by the user. It will be called with two arguments - `predictions` and `labels`.\n-            These contain the model's outputs and matching labels from the dataset. It should return a dict mapping\n-            metric names to numerical values.\n-        eval_dataset (`tf.data.Dataset` or `dict` or `tuple` or `np.ndarray` or `tf.Tensor`):\n-            Validation data to be used to generate predictions for the `metric_fn`.\n-        output_cols (`list[str], *optional*):\n-            A list of columns to be retained from the model output as the predictions. Defaults to all.\n-        label_cols ('`list[str]`, *optional*'):\n-            A list of columns to be retained from the input dataset as the labels. Will be autodetected if this is not\n-            supplied.\n-        batch_size (`int`, *optional*):\n-            Batch size. Only used when the data is not a pre-batched `tf.data.Dataset`.\n-        predict_with_generate (`bool`, *optional*, defaults to `False`):\n-            Whether we should use `model.generate()` to get outputs for the model.\n-        use_xla_generation (`bool`, *optional*, defaults to `False`):\n-            If we're generating, whether to compile model generation with XLA. This can massively increase the speed of\n-            generation (up to 100X speedup) but will require a new XLA compilation for each input shape. When using XLA\n-            generation, it's a good idea to pad your inputs to the same size, or to use the `pad_to_multiple_of`\n-            argument in your `tokenizer` or `DataCollator`, which will reduce the number of unique input shapes and\n-            save a lot of compilation time. This option has no effect is `predict_with_generate` is `False`.\n-        generate_kwargs (`dict`, *optional*):\n-            Keyword arguments to pass to `model.generate()` when generating. Has no effect if `predict_with_generate`\n-            is `False`.\n-\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        metric_fn: Callable,\n-        eval_dataset: Union[tf.data.Dataset, np.ndarray, tf.Tensor, tuple, dict],\n-        output_cols: Optional[list[str]] = None,\n-        label_cols: Optional[list[str]] = None,\n-        batch_size: Optional[int] = None,\n-        predict_with_generate: bool = False,\n-        use_xla_generation: bool = False,\n-        generate_kwargs: Optional[dict] = None,\n-    ):\n-        super().__init__()\n-        self.metric_fn = metric_fn\n-        self.batch_size = batch_size\n-        if not isinstance(eval_dataset, tf.data.Dataset):\n-            if batch_size is None:\n-                raise ValueError(\n-                    \"When passing data to KerasMetricCallback that is not a pre-batched tf.data.Dataset \"\n-                    \"the batch_size argument must be set.\"\n-                )\n-            # Wrap a tf.data.Dataset around it\n-            eval_dataset = tf.data.Dataset.from_tensor_slices(eval_dataset).batch(batch_size, drop_remainder=False)\n-        self.eval_dataset = eval_dataset\n-        self.predict_with_generate = predict_with_generate\n-        self.output_cols = output_cols\n-\n-        # This next block attempts to parse out which elements of the dataset should be appended to the labels list\n-        # that is passed to the metric_fn\n-        if isinstance(eval_dataset.element_spec, tuple) and len(eval_dataset.element_spec) == 2:\n-            input_spec, label_spec = eval_dataset.element_spec\n-        else:\n-            input_spec = eval_dataset.element_spec\n-            label_spec = None\n-        if label_cols is not None:\n-            for label in label_cols:\n-                if label not in input_spec:\n-                    raise ValueError(f\"Label {label} is in label_cols but could not be found in the dataset inputs!\")\n-            self.label_cols = label_cols\n-            self.use_keras_label = False\n-        elif label_spec is not None:\n-            # If the dataset inputs are split into a 2-tuple of inputs and labels,\n-            # assume the second element is the labels\n-            self.label_cols = None\n-            self.use_keras_label = True\n-        elif \"labels\" in input_spec:\n-            self.label_cols = [\"labels\"]\n-            self.use_keras_label = False\n-            logging.warning(\"No label_cols specified for KerasMetricCallback, assuming you want the 'labels' key.\")\n-        elif \"start_positions\" in input_spec and \"end_positions\" in input_spec:\n-            self.label_cols = [\"start_positions\", \"end_positions\"]\n-            self.use_keras_label = False\n-            logging.warning(\n-                \"No label_cols specified for KerasMetricCallback, assuming you want the \"\n-                \"start_positions and end_positions keys.\"\n-            )\n-        else:\n-            raise ValueError(\"Could not autodetect label_cols for KerasMetricCallback, please specify them!\")\n-        if parse(tf.__version__) < parse(\"2.7\"):\n-            logging.warning(\"TF versions less than 2.7 may encounter issues with KerasMetricCallback!\")\n-\n-        self.use_xla_generation = use_xla_generation\n-        self.generate_kwargs = {} if generate_kwargs is None else generate_kwargs\n-\n-        self.generation_function = None\n-\n-    @staticmethod\n-    def _concatenate_batches(batches, padding_index=-100):\n-        # If all batches are unidimensional or same length, do a simple concatenation\n-        if batches[0].ndim == 1 or all(batch.shape[1] == batches[0].shape[1] for batch in batches):\n-            return np.concatenate(batches, axis=0)\n-\n-        # Welp, they're not the same length. Let's do some padding\n-        max_len = max([batch.shape[1] for batch in batches])\n-        num_samples = sum([batch.shape[0] for batch in batches])\n-        output = np.full_like(\n-            batches[0], fill_value=padding_index, shape=[num_samples, max_len] + list(batches[0].shape[2:])\n-        )\n-        # i keeps track of which part of the concatenated array we're writing the next batch to\n-        i = 0\n-        for batch in batches:\n-            output[i : i + len(batch), : batch.shape[1]] = batch\n-            i += len(batch)\n-        return output\n-\n-    def _postprocess_predictions_or_labels(self, inputs):\n-        if isinstance(inputs[0], dict):\n-            outputs = {}\n-            for key in inputs[0]:\n-                outputs[key] = self._concatenate_batches([batch[key] for batch in inputs])\n-            # If it's a dict with only one key, just return the array\n-            if len(outputs) == 1:\n-                outputs = list(outputs.values())[0]\n-        elif isinstance(inputs[0], (tuple, list)):\n-            outputs = []\n-            for input_list in zip(*inputs):\n-                outputs.append(self._concatenate_batches(input_list))\n-            if len(outputs) == 1:\n-                outputs = outputs[0]  # If it's a list with only one element, just return the array\n-        elif isinstance(inputs[0], np.ndarray):\n-            outputs = self._concatenate_batches(inputs)\n-        elif isinstance(inputs[0], tf.Tensor):\n-            outputs = self._concatenate_batches([tensor.numpy() for tensor in inputs])\n-        else:\n-            raise TypeError(f\"Couldn't handle batch of type {type(inputs[0])}!\")\n-        return outputs\n-\n-    def on_epoch_end(self, epoch, logs=None):\n-        if hasattr(self.model, \"config\"):\n-            ignore_keys = getattr(self.model.config, \"keys_to_ignore_at_inference\", [])\n-        else:\n-            ignore_keys = []\n-\n-        main_input_name = None\n-        if self.predict_with_generate:\n-            # This dense conditional recognizes the case where we have an encoder-decoder model, but\n-            # avoids getting tangled up when we just have a model with a layer called 'encoder'\n-            if hasattr(self.model, \"encoder\") and hasattr(self.model.encoder, \"main_input_name\"):\n-                main_input_name = self.model.encoder.main_input_name\n-            else:\n-                main_input_name = getattr(self.model, \"main_input_name\", \"input_ids\")\n-\n-            if self.use_xla_generation and self.generation_function is None:\n-\n-                def generation_function(inputs, attention_mask):\n-                    return self.model.generate(inputs, attention_mask=attention_mask, **self.generate_kwargs)\n-\n-                self.generation_function = tf.function(generation_function, jit_compile=True)\n-\n-        prediction_list = []\n-        label_list = []\n-\n-        # The whole predict/generate loop is handled inside this method\n-        for batch in self.eval_dataset:\n-            if isinstance(batch, tuple):\n-                batch, labels = batch\n-            else:\n-                labels = None\n-            if self.predict_with_generate:\n-                if isinstance(batch, dict):\n-                    generation_inputs = batch[main_input_name]\n-                    attention_mask = batch.get(\"attention_mask\", None)\n-                else:\n-                    generation_inputs = batch\n-                    attention_mask = None\n-                if self.use_xla_generation:\n-                    predictions = self.generation_function(generation_inputs, attention_mask=attention_mask)\n-                else:\n-                    predictions = self.model.generate(\n-                        generation_inputs, attention_mask=attention_mask, **self.generate_kwargs\n-                    )\n-            else:\n-                predictions = self.model.predict_on_batch(batch)\n-                if isinstance(predictions, dict):\n-                    # This converts any dict-subclass to a regular dict\n-                    # Keras REALLY doesn't like it when we pass around a BatchEncoding or other derived class\n-                    predictions = dict(predictions)\n-                    if self.output_cols is not None:\n-                        predictions = {key: predictions[key] for key in self.output_cols}\n-                    else:\n-                        predictions = {\n-                            key: val for key, val in predictions.items() if key not in ignore_keys + [\"loss\"]\n-                        }\n-            prediction_list.append(predictions)\n-            if not self.use_keras_label:\n-                labels = {key: batch[key].numpy() for key in self.label_cols}\n-            elif isinstance(labels, dict):\n-                labels = {key: array.numpy() for key, array in labels.items()}\n-            elif isinstance(labels, (list, tuple)):\n-                labels = [array.numpy() for array in labels]\n-            elif isinstance(labels, tf.Tensor):\n-                labels = labels.numpy()\n-            else:\n-                raise TypeError(f\"Confused by labels of type {type(labels)}\")\n-            label_list.append(labels)\n-\n-        all_preds = self._postprocess_predictions_or_labels(prediction_list)\n-        all_labels = self._postprocess_predictions_or_labels(label_list)\n-\n-        metric_output = self.metric_fn((all_preds, all_labels))\n-        if not isinstance(metric_output, dict):\n-            raise TypeError(\n-                f\"metric_fn should return a dict mapping metric names to values but instead returned {metric_output}\"\n-            )\n-        # This is the critical bit - Keras passes a dict containing the loss and standard metric values for this epoch\n-        # in the logs argument. Ordinarily, this is so the callback can read them, but in this case we write a bunch of\n-        # new keys in there, which will then get read by the History callback and treated like any other metric value.\n-        # I promise that I have it in writing from Chollet that this is okay.\n-        logs.update(metric_output)\n-\n-\n-class PushToHubCallback(keras.callbacks.Callback):\n-    \"\"\"\n-    Callback that will save and push the model to the Hub regularly. By default, it pushes once per epoch, but this can\n-    be changed with the `save_strategy` argument. Pushed models can be accessed like any other model on the hub, such\n-    as with the `from_pretrained` method.\n-\n-    ```py\n-    from transformers.keras_callbacks import PushToHubCallback\n-\n-    push_to_hub_callback = PushToHubCallback(\n-        output_dir=\"./model_save\",\n-        tokenizer=tokenizer,\n-        hub_model_id=\"gpt5-7xlarge\",\n-    )\n-\n-    model.fit(train_dataset, callbacks=[push_to_hub_callback])\n-    ```\n-\n-    Args:\n-        output_dir (`str`):\n-            The output directory where the model predictions and checkpoints will be written and synced with the\n-            repository on the Hub.\n-        save_strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `\"epoch\"`):\n-            The checkpoint save strategy to adopt during training. Possible values are:\n-\n-                - `\"no\"`: Save is done at the end of training.\n-                - `\"epoch\"`: Save is done at the end of each epoch.\n-                - `\"steps\"`: Save is done every `save_steps`\n-        save_steps (`int`, *optional*):\n-            The number of steps between saves when using the \"steps\" `save_strategy`.\n-        tokenizer (`PreTrainedTokenizerBase`, *optional*):\n-            The tokenizer used by the model. If supplied, will be uploaded to the repo alongside the weights.\n-        hub_model_id (`str`, *optional*):\n-            The name of the repository to keep in sync with the local `output_dir`. It can be a simple model ID in\n-            which case the model will be pushed in your namespace. Otherwise it should be the whole repository name,\n-            for instance `\"user_name/model\"`, which allows you to push to an organization you are a member of with\n-            `\"organization_name/model\"`.\n-\n-            Will default to the name of `output_dir`.\n-        hub_token (`str`, *optional*):\n-            The token to use to push the model to the Hub. Will default to the token in the cache folder obtained with\n-            `hf auth login`.\n-        checkpoint (`bool`, *optional*, defaults to `False`):\n-            Whether to save full training checkpoints (including epoch and optimizer state) to allow training to be\n-            resumed. Only usable when `save_strategy` is `\"epoch\"`.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        output_dir: Union[str, Path],\n-        save_strategy: Union[str, IntervalStrategy] = \"epoch\",\n-        save_steps: Optional[int] = None,\n-        tokenizer: Optional[PreTrainedTokenizerBase] = None,\n-        hub_model_id: Optional[str] = None,\n-        hub_token: Optional[str] = None,\n-        checkpoint: bool = False,\n-        **model_card_args,\n-    ):\n-        super().__init__()\n-        if checkpoint and save_strategy != \"epoch\":\n-            raise ValueError(\"Cannot save checkpoints when save_strategy is not 'epoch'!\")\n-        if isinstance(save_strategy, str):\n-            save_strategy = IntervalStrategy(save_strategy.lower())\n-        self.save_strategy = save_strategy\n-        if self.save_strategy == IntervalStrategy.STEPS and (not isinstance(save_steps, int) or save_steps <= 0):\n-            raise ValueError(\"Please supply a positive integer argument for save_steps when save_strategy == 'steps'!\")\n-        self.save_steps = save_steps\n-        output_dir = Path(output_dir)\n-\n-        # Create repo and retrieve repo_id\n-        if hub_model_id is None:\n-            hub_model_id = output_dir.absolute().name\n-        self.hub_model_id = create_repo(repo_id=hub_model_id, exist_ok=True, token=hub_token).repo_id\n-\n-        self.output_dir = output_dir\n-        self.repo = Repository(str(self.output_dir), clone_from=self.hub_model_id, token=hub_token)\n-\n-        self.tokenizer = tokenizer\n-        self.last_job = None\n-        self.checkpoint = checkpoint\n-        self.training_history = None\n-        self.model_card_args = model_card_args\n-\n-    def on_train_begin(self, logs=None):\n-        # Although we can access model.history, we have no guarantees that the History callback will fire before this\n-        # one, so we keep track of it here too\n-        self.training_history = []\n-\n-    def on_train_batch_end(self, batch, logs=None):\n-        if self.save_strategy == IntervalStrategy.STEPS and (batch + 1) % self.save_steps == 0:\n-            if self.last_job is not None and not self.last_job.is_done:\n-                return  # The last upload is still running, don't start another\n-            self.model.save_pretrained(self.output_dir)\n-            if self.tokenizer is not None:\n-                self.tokenizer.save_pretrained(self.output_dir)\n-            _, self.last_job = self.repo.push_to_hub(\n-                commit_message=f\"Training in progress steps {batch}\", blocking=False\n-            )\n-\n-    def on_epoch_end(self, epoch, logs=None):\n-        logs = logs.copy()  # Don't accidentally write things that Keras will read later\n-        if \"epoch\" not in logs:\n-            logs[\"epoch\"] = epoch\n-        self.training_history.append(logs)\n-        if self.save_strategy == IntervalStrategy.EPOCH:\n-            if self.last_job is not None and not self.last_job.is_done:\n-                return  # The last upload is still running, don't start another\n-            self.model.save_pretrained(self.output_dir)\n-            if self.tokenizer is not None:\n-                self.tokenizer.save_pretrained(self.output_dir)\n-            if self.checkpoint:\n-                checkpoint_dir = os.path.join(self.output_dir, \"checkpoint\")\n-                self.model._save_checkpoint(checkpoint_dir, epoch)\n-            train_summary = TrainingSummary.from_keras(\n-                model=self.model,\n-                model_name=self.hub_model_id,\n-                keras_history=self.training_history,\n-                **self.model_card_args,\n-            )\n-            model_card = train_summary.to_model_card()\n-            with (self.output_dir / \"README.md\").open(\"w\") as f:\n-                f.write(model_card)\n-            _, self.last_job = self.repo.push_to_hub(\n-                commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n-            )\n-\n-    def on_train_end(self, logs=None):\n-        # Makes sure the latest version of the model is uploaded\n-        if self.last_job is not None and not self.last_job.is_done:\n-            logging.info(\"Pushing the last epoch to the Hub, this may take a while...\")\n-            while not self.last_job.is_done:\n-                sleep(1)\n-        else:\n-            self.model.save_pretrained(self.output_dir)\n-            if self.tokenizer is not None:\n-                self.tokenizer.save_pretrained(self.output_dir)\n-            train_summary = TrainingSummary.from_keras(\n-                model=self.model,\n-                model_name=self.hub_model_id,\n-                keras_history=self.training_history,\n-                **self.model_card_args,\n-            )\n-            model_card = train_summary.to_model_card()\n-            with (self.output_dir / \"README.md\").open(\"w\") as f:\n-                f.write(model_card)\n-            self.repo.push_to_hub(commit_message=\"End of training\", blocking=True)"
        },
        {
            "sha": "8ba390ee7cf5b0c91f0b9b5f932c5c8e07ee254d",
            "filename": "src/transformers/modelcard.py",
            "status": "modified",
            "additions": 0,
            "deletions": 135,
            "changes": 135,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodelcard.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodelcard.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodelcard.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -51,7 +51,6 @@\n     cached_file,\n     is_datasets_available,\n     is_offline_mode,\n-    is_tf_available,\n     is_tokenizers_available,\n     is_torch_available,\n     logging,\n@@ -256,11 +255,6 @@ def to_json_file(self, json_file_path):\n should probably proofread and complete it, then remove this comment. -->\n \"\"\"\n \n-AUTOGENERATED_KERAS_COMMENT = \"\"\"\n-<!-- This model card has been generated automatically according to the information Keras had access to. You should\n-probably proofread and complete it, then remove this comment. -->\n-\"\"\"\n-\n \n TASK_TAG_TO_NAME_MAPPING = {\n     \"fill-mask\": \"Masked Language Modeling\",\n@@ -483,8 +477,6 @@ def to_model_card(self):\n         # Now the model card for realsies.\n         if self.source == \"trainer\":\n             model_card += AUTOGENERATED_TRAINER_COMMENT\n-        else:\n-            model_card += AUTOGENERATED_KERAS_COMMENT\n \n         model_card += f\"\\n# {self.model_name}\\n\\n\"\n \n@@ -538,10 +530,6 @@ def to_model_card(self):\n             import torch\n \n             model_card += f\"- Pytorch {torch.__version__}\\n\"\n-        elif self.source == \"keras\" and is_tf_available():\n-            import tensorflow as tf\n-\n-            model_card += f\"- TensorFlow {tf.__version__}\\n\"\n         if is_datasets_available():\n             import datasets\n \n@@ -631,116 +619,6 @@ def from_trainer(\n             hyperparameters=hyperparameters,\n         )\n \n-    @classmethod\n-    def from_keras(\n-        cls,\n-        model,\n-        model_name,\n-        keras_history=None,\n-        language=None,\n-        license=None,\n-        tags=None,\n-        finetuned_from=None,\n-        tasks=None,\n-        dataset_tags=None,\n-        dataset=None,\n-        dataset_args=None,\n-    ):\n-        # Infer default from dataset\n-        if dataset is not None:\n-            if is_hf_dataset(dataset) and (dataset_tags is None or dataset_args is None):\n-                default_tag = dataset.builder_name\n-                # Those are not real datasets from the Hub so we exclude them.\n-                if default_tag not in [\"csv\", \"json\", \"pandas\", \"parquet\", \"text\"]:\n-                    if dataset_tags is None:\n-                        dataset_tags = [default_tag]\n-                    if dataset_args is None:\n-                        dataset_args = [dataset.config_name]\n-\n-        if dataset is None and dataset_tags is not None:\n-            dataset = dataset_tags\n-\n-        # Infer default finetuned_from\n-        if (\n-            finetuned_from is None\n-            and hasattr(model.config, \"_name_or_path\")\n-            and not os.path.isdir(model.config._name_or_path)\n-        ):\n-            finetuned_from = model.config._name_or_path\n-\n-        # Infer default task tag:\n-        if tasks is None:\n-            model_class_name = model.__class__.__name__\n-            for task, mapping in TASK_MAPPING.items():\n-                if model_class_name in _get_mapping_values(mapping):\n-                    tasks = task\n-\n-        # Add `generated_from_keras_callback` to the tags\n-        if tags is None:\n-            tags = [\"generated_from_keras_callback\"]\n-        elif isinstance(tags, str) and tags != \"generated_from_keras_callback\":\n-            tags = [tags, \"generated_from_keras_callback\"]\n-        elif \"generated_from_keras_callback\" not in tags:\n-            tags.append(\"generated_from_keras_callback\")\n-\n-        if keras_history is not None:\n-            _, eval_lines, eval_results = parse_keras_history(keras_history)\n-        else:\n-            eval_lines = []\n-            eval_results = {}\n-        hyperparameters = extract_hyperparameters_from_keras(model)\n-\n-        return cls(\n-            language=language,\n-            license=license,\n-            tags=tags,\n-            model_name=model_name,\n-            finetuned_from=finetuned_from,\n-            tasks=tasks,\n-            dataset_tags=dataset_tags,\n-            dataset=dataset,\n-            dataset_args=dataset_args,\n-            eval_results=eval_results,\n-            eval_lines=eval_lines,\n-            hyperparameters=hyperparameters,\n-            source=\"keras\",\n-        )\n-\n-\n-def parse_keras_history(logs):\n-    \"\"\"\n-    Parse the `logs` of either a `keras.History` object returned by `model.fit()` or an accumulated logs `dict`\n-    passed to the `PushToHubCallback`. Returns lines and logs compatible with those returned by `parse_log_history`.\n-    \"\"\"\n-    if hasattr(logs, \"history\"):\n-        # This looks like a `History` object\n-        if not hasattr(logs, \"epoch\"):\n-            # This history looks empty, return empty results\n-            return None, [], {}\n-        logs.history[\"epoch\"] = logs.epoch\n-        logs = logs.history\n-    else:\n-        # Training logs is a list of dicts, let's invert it to a dict of lists to match a History object\n-        logs = {log_key: [single_dict[log_key] for single_dict in logs] for log_key in logs[0]}\n-\n-    lines = []\n-    for i in range(len(logs[\"epoch\"])):\n-        epoch_dict = {log_key: log_value_list[i] for log_key, log_value_list in logs.items()}\n-        values = {}\n-        for k, v in epoch_dict.items():\n-            if k.startswith(\"val_\"):\n-                k = \"validation_\" + k[4:]\n-            elif k != \"epoch\":\n-                k = \"train_\" + k\n-            splits = k.split(\"_\")\n-            name = \" \".join([part.capitalize() for part in splits])\n-            values[name] = v\n-        lines.append(values)\n-\n-    eval_results = lines[-1]\n-\n-    return logs, lines, eval_results\n-\n \n def parse_log_history(log_history):\n     \"\"\"\n@@ -804,19 +682,6 @@ def parse_log_history(log_history):\n         return train_log, lines, None\n \n \n-def extract_hyperparameters_from_keras(model):\n-    from .modeling_tf_utils import keras\n-\n-    hyperparameters = {}\n-    if hasattr(model, \"optimizer\") and model.optimizer is not None:\n-        hyperparameters[\"optimizer\"] = model.optimizer.get_config()\n-    else:\n-        hyperparameters[\"optimizer\"] = None\n-    hyperparameters[\"training_precision\"] = keras.mixed_precision.global_policy().name\n-\n-    return hyperparameters\n-\n-\n def _maybe_round(v, decimals=4):\n     if isinstance(v, float) and len(str(v).split(\".\")) > 1 and len(str(v).split(\".\")[1]) > decimals:\n         return f\"{v:.{decimals}f}\""
        },
        {
            "sha": "5a25a6059a255659c6d900b35d2ffa7cab57f071",
            "filename": "src/transformers/modeling_flax_outputs.py",
            "status": "removed",
            "additions": 0,
            "deletions": 700,
            "changes": 700,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodeling_flax_outputs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodeling_flax_outputs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flax_outputs.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,700 +0,0 @@\n-# Copyright 2021 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-from typing import Optional\n-\n-import flax\n-import jax.numpy as jnp\n-\n-from .utils import ModelOutput\n-\n-\n-@flax.struct.dataclass\n-class FlaxBaseModelOutput(ModelOutput):\n-    \"\"\"\n-    Base class for model's outputs, with potential hidden states and attentions.\n-\n-    Args:\n-        last_hidden_state (`jnp.ndarray` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-    \"\"\"\n-\n-    last_hidden_state: Optional[jnp.ndarray] = None\n-    hidden_states: Optional[tuple[jnp.ndarray]] = None\n-    attentions: Optional[tuple[jnp.ndarray]] = None\n-\n-\n-@flax.struct.dataclass\n-class FlaxBaseModelOutputWithNoAttention(ModelOutput):\n-    \"\"\"\n-    Base class for model's outputs, with potential hidden states.\n-\n-    Args:\n-        last_hidden_state (`jnp.ndarray` of shape `(batch_size, num_channels, height, width)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `jnp.ndarray` (one for the output of the embeddings, if the model has an embedding layer, + one\n-            for the output of each layer) of shape `(batch_size, num_channels, height, width)`. Hidden-states of the\n-            model at the output of each layer plus the optional initial embedding outputs.\n-    \"\"\"\n-\n-    last_hidden_state: Optional[jnp.ndarray] = None\n-    hidden_states: Optional[tuple[jnp.ndarray]] = None\n-\n-\n-@flax.struct.dataclass\n-class FlaxBaseModelOutputWithPoolingAndNoAttention(ModelOutput):\n-    \"\"\"\n-    Base class for model's outputs that also contains a pooling of the last hidden states.\n-\n-    Args:\n-        last_hidden_state (`jnp.ndarray` of shape `(batch_size, num_channels, height, width)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        pooler_output (`jnp.ndarray` of shape `(batch_size, hidden_size)`):\n-            Last layer hidden-state after a pooling operation on the spatial dimensions.\n-        hidden_states (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `jnp.ndarray` (one for the output of the embeddings, if the model has an embedding layer, + one\n-            for the output of each layer) of shape `(batch_size, num_channels, height, width)`. Hidden-states of the\n-            model at the output of each layer plus the optional initial embedding outputs.\n-    \"\"\"\n-\n-    last_hidden_state: Optional[jnp.ndarray] = None\n-    pooler_output: Optional[jnp.ndarray] = None\n-    hidden_states: Optional[tuple[jnp.ndarray]] = None\n-\n-\n-@flax.struct.dataclass\n-class FlaxImageClassifierOutputWithNoAttention(ModelOutput):\n-    \"\"\"\n-    Base class for outputs of image classification models.\n-\n-    Args:\n-        logits (`jnp.ndarray` of shape `(batch_size, config.num_labels)`):\n-            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n-        hidden_states (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True` is passed or when\n-        `config.output_hidden_states=True`):\n-            Tuple of `jnp.ndarray` (one for the output of the embeddings, if the model has an embedding layer, + one\n-            for the output of each stage) of shape `(batch_size, num_channels, height, width)`. Hidden-states (also\n-            called feature maps) of the model at the output of each stage.\n-    \"\"\"\n-\n-    logits: Optional[jnp.ndarray] = None\n-    hidden_states: Optional[tuple[jnp.ndarray]] = None\n-\n-\n-@flax.struct.dataclass\n-class FlaxBaseModelOutputWithPast(ModelOutput):\n-    \"\"\"\n-    Base class for model's outputs, with potential hidden states and attentions.\n-\n-    Args:\n-        last_hidden_state (`jnp.ndarray` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        past_key_values (`dict[str, jnp.ndarray]`):\n-            Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast\n-            auto-regressive decoding. Pre-computed key and value hidden-states are of shape *[batch_size, max_length]*.\n-        hidden_states (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-    \"\"\"\n-\n-    last_hidden_state: Optional[jnp.ndarray] = None\n-    past_key_values: Optional[dict[str, jnp.ndarray]] = None\n-    hidden_states: Optional[tuple[jnp.ndarray]] = None\n-    attentions: Optional[tuple[jnp.ndarray]] = None\n-\n-\n-@flax.struct.dataclass\n-class FlaxBaseModelOutputWithPooling(ModelOutput):\n-    \"\"\"\n-    Base class for model's outputs that also contains a pooling of the last hidden states.\n-\n-    Args:\n-        last_hidden_state (`jnp.ndarray` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        pooler_output (`jnp.ndarray` of shape `(batch_size, hidden_size)`):\n-            Last layer hidden-state of the first token of the sequence (classification token) further processed by a\n-            Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence\n-            prediction (classification) objective during pretraining.\n-        hidden_states (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-    \"\"\"\n-\n-    last_hidden_state: Optional[jnp.ndarray] = None\n-    pooler_output: Optional[jnp.ndarray] = None\n-    hidden_states: Optional[tuple[jnp.ndarray]] = None\n-    attentions: Optional[tuple[jnp.ndarray]] = None\n-\n-\n-@flax.struct.dataclass\n-class FlaxBaseModelOutputWithPoolingAndCrossAttentions(ModelOutput):\n-    \"\"\"\n-    Base class for model's outputs that also contains a pooling of the last hidden states.\n-\n-    Args:\n-        last_hidden_state (`jnp.ndarray` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        pooler_output (`jnp.ndarray` of shape `(batch_size, hidden_size)`):\n-            Last layer hidden-state of the first token of the sequence (classification token) after further processing\n-            through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns\n-            the classification token after processing through a linear layer and a tanh activation function. The linear\n-            layer weights are trained from the next sentence prediction (classification) objective during pretraining.\n-        hidden_states (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `jnp.ndarray` (one for the output of the embeddings, if the model has an embedding layer, + one\n-            for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        cross_attentions (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True` and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n-            weighted average in the cross-attention heads.\n-        past_key_values (`tuple(tuple(jnp.ndarray))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(jnp.ndarray)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n-            `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n-            encoder_sequence_length, embed_size_per_head)`.\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n-            `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n-            input) to speed up sequential decoding.\n-    \"\"\"\n-\n-    last_hidden_state: Optional[jnp.ndarray] = None\n-    pooler_output: Optional[jnp.ndarray] = None\n-    hidden_states: Optional[tuple[jnp.ndarray]] = None\n-    past_key_values: Optional[tuple[tuple[jnp.ndarray]]] = None\n-    attentions: Optional[tuple[jnp.ndarray]] = None\n-    cross_attentions: Optional[tuple[jnp.ndarray]] = None\n-\n-\n-@flax.struct.dataclass\n-class FlaxBaseModelOutputWithPastAndCrossAttentions(ModelOutput):\n-    \"\"\"\n-    Base class for model's outputs that may also contain a past key/values (to speed up sequential decoding).\n-\n-    Args:\n-        last_hidden_state (`jnp.ndarray` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-\n-            If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n-            hidden_size)` is output.\n-        past_key_values (`tuple(tuple(jnp.ndarray))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(jnp.ndarray)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n-            `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n-            encoder_sequence_length, embed_size_per_head)`.\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n-            `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n-            input) to speed up sequential decoding.\n-        hidden_states (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        cross_attentions (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True` and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n-            weighted average in the cross-attention heads.\n-    \"\"\"\n-\n-    last_hidden_state: Optional[jnp.ndarray] = None\n-    past_key_values: Optional[tuple[tuple[jnp.ndarray]]] = None\n-    hidden_states: Optional[tuple[jnp.ndarray]] = None\n-    attentions: Optional[tuple[jnp.ndarray]] = None\n-    cross_attentions: Optional[tuple[jnp.ndarray]] = None\n-\n-\n-@flax.struct.dataclass\n-class FlaxSeq2SeqModelOutput(ModelOutput):\n-    \"\"\"\n-    Base class for model encoder's outputs that also contains : pre-computed hidden states that can speed up sequential\n-    decoding.\n-\n-    Args:\n-        last_hidden_state (`jnp.ndarray` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the decoder of the model.\n-\n-            If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n-            hidden_size)` is output.\n-        past_key_values (`tuple(tuple(jnp.ndarray))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(jnp.ndarray)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n-            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n-        decoder_hidden_states (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.\n-        decoder_attentions (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        cross_attentions (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n-            weighted average in the cross-attention heads.\n-        encoder_last_hidden_state (`jnp.ndarray` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_hidden_states (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.\n-        encoder_attentions (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-    \"\"\"\n-\n-    last_hidden_state: Optional[jnp.ndarray] = None\n-    past_key_values: Optional[tuple[tuple[jnp.ndarray]]] = None\n-    decoder_hidden_states: Optional[tuple[jnp.ndarray]] = None\n-    decoder_attentions: Optional[tuple[jnp.ndarray]] = None\n-    cross_attentions: Optional[tuple[jnp.ndarray]] = None\n-    encoder_last_hidden_state: Optional[jnp.ndarray] = None\n-    encoder_hidden_states: Optional[tuple[jnp.ndarray]] = None\n-    encoder_attentions: Optional[tuple[jnp.ndarray]] = None\n-\n-\n-@flax.struct.dataclass\n-class FlaxCausalLMOutputWithCrossAttentions(ModelOutput):\n-    \"\"\"\n-    Base class for causal language model (or autoregressive) outputs.\n-\n-    Args:\n-        logits (`jnp.ndarray` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        hidden_states (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        cross_attentions (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Cross attentions weights after the attention softmax, used to compute the weighted average in the\n-            cross-attention heads.\n-        past_key_values (`tuple(tuple(jnp.ndarray))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `jnp.ndarray` tuples of length `config.n_layers`, with each tuple containing the cached key, value\n-            states of the self-attention and the cross-attention layers if model is used in encoder-decoder setting.\n-            Only relevant if `config.is_decoder = True`.\n-\n-            Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-    \"\"\"\n-\n-    logits: Optional[jnp.ndarray] = None\n-    past_key_values: Optional[tuple[tuple[jnp.ndarray]]] = None\n-    hidden_states: Optional[tuple[jnp.ndarray]] = None\n-    attentions: Optional[tuple[jnp.ndarray]] = None\n-    cross_attentions: Optional[tuple[jnp.ndarray]] = None\n-\n-\n-@flax.struct.dataclass\n-class FlaxMaskedLMOutput(ModelOutput):\n-    \"\"\"\n-    Base class for masked language models outputs.\n-\n-    Args:\n-        logits (`jnp.ndarray` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        hidden_states (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-    \"\"\"\n-\n-    logits: Optional[jnp.ndarray] = None\n-    hidden_states: Optional[tuple[jnp.ndarray]] = None\n-    attentions: Optional[tuple[jnp.ndarray]] = None\n-\n-\n-FlaxCausalLMOutput = FlaxMaskedLMOutput\n-\n-\n-@flax.struct.dataclass\n-class FlaxSeq2SeqLMOutput(ModelOutput):\n-    \"\"\"\n-    Base class for sequence-to-sequence language models outputs.\n-\n-    Args:\n-        logits (`jnp.ndarray` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`tuple(tuple(jnp.ndarray))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(jnp.ndarray)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n-            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n-        decoder_hidden_states (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.\n-        decoder_attentions (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        cross_attentions (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n-            weighted average in the cross-attention heads.\n-        encoder_last_hidden_state (`jnp.ndarray` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_hidden_states (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.\n-        encoder_attentions (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-    \"\"\"\n-\n-    logits: Optional[jnp.ndarray] = None\n-    past_key_values: Optional[tuple[tuple[jnp.ndarray]]] = None\n-    decoder_hidden_states: Optional[tuple[jnp.ndarray]] = None\n-    decoder_attentions: Optional[tuple[jnp.ndarray]] = None\n-    cross_attentions: Optional[tuple[jnp.ndarray]] = None\n-    encoder_last_hidden_state: Optional[jnp.ndarray] = None\n-    encoder_hidden_states: Optional[tuple[jnp.ndarray]] = None\n-    encoder_attentions: Optional[tuple[jnp.ndarray]] = None\n-\n-\n-@flax.struct.dataclass\n-class FlaxNextSentencePredictorOutput(ModelOutput):\n-    \"\"\"\n-    Base class for outputs of models predicting if two sentences are consecutive or not.\n-\n-    Args:\n-        logits (`jnp.ndarray` of shape `(batch_size, 2)`):\n-            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n-            before SoftMax).\n-        hidden_states (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-    \"\"\"\n-\n-    logits: Optional[jnp.ndarray] = None\n-    hidden_states: Optional[tuple[jnp.ndarray]] = None\n-    attentions: Optional[tuple[jnp.ndarray]] = None\n-\n-\n-@flax.struct.dataclass\n-class FlaxSequenceClassifierOutput(ModelOutput):\n-    \"\"\"\n-    Base class for outputs of sentence classification models.\n-\n-    Args:\n-        logits (`jnp.ndarray` of shape `(batch_size, config.num_labels)`):\n-            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n-        hidden_states (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-    \"\"\"\n-\n-    logits: Optional[jnp.ndarray] = None\n-    hidden_states: Optional[tuple[jnp.ndarray]] = None\n-    attentions: Optional[tuple[jnp.ndarray]] = None\n-\n-\n-@flax.struct.dataclass\n-class FlaxSeq2SeqSequenceClassifierOutput(ModelOutput):\n-    \"\"\"\n-    Base class for outputs of sequence-to-sequence sentence classification models.\n-\n-    Args:\n-        logits (`jnp.ndarray` of shape `(batch_size, config.num_labels)`):\n-            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n-        past_key_values (`tuple(tuple(jnp.ndarray))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(jnp.ndarray)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n-            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n-        decoder_hidden_states (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.\n-        decoder_attentions (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        cross_attentions (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n-            weighted average in the cross-attention heads.\n-        encoder_last_hidden_state (`jnp.ndarray` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_hidden_states (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.\n-        encoder_attentions (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-    \"\"\"\n-\n-    logits: Optional[jnp.ndarray] = None\n-    past_key_values: Optional[tuple[tuple[jnp.ndarray]]] = None\n-    decoder_hidden_states: Optional[tuple[jnp.ndarray]] = None\n-    decoder_attentions: Optional[tuple[jnp.ndarray]] = None\n-    cross_attentions: Optional[tuple[jnp.ndarray]] = None\n-    encoder_last_hidden_state: Optional[jnp.ndarray] = None\n-    encoder_hidden_states: Optional[tuple[jnp.ndarray]] = None\n-    encoder_attentions: Optional[tuple[jnp.ndarray]] = None\n-\n-\n-@flax.struct.dataclass\n-class FlaxMultipleChoiceModelOutput(ModelOutput):\n-    \"\"\"\n-    Base class for outputs of multiple choice models.\n-\n-    Args:\n-        logits (`jnp.ndarray` of shape `(batch_size, num_choices)`):\n-            *num_choices* is the second dimension of the input tensors. (see *input_ids* above).\n-\n-            Classification scores (before SoftMax).\n-        hidden_states (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-    \"\"\"\n-\n-    logits: Optional[jnp.ndarray] = None\n-    hidden_states: Optional[tuple[jnp.ndarray]] = None\n-    attentions: Optional[tuple[jnp.ndarray]] = None\n-\n-\n-@flax.struct.dataclass\n-class FlaxTokenClassifierOutput(ModelOutput):\n-    \"\"\"\n-    Base class for outputs of token classification models.\n-\n-    Args:\n-        logits (`jnp.ndarray` of shape `(batch_size, sequence_length, config.num_labels)`):\n-            Classification scores (before SoftMax).\n-        hidden_states (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-    \"\"\"\n-\n-    logits: Optional[jnp.ndarray] = None\n-    hidden_states: Optional[tuple[jnp.ndarray]] = None\n-    attentions: Optional[tuple[jnp.ndarray]] = None\n-\n-\n-@flax.struct.dataclass\n-class FlaxQuestionAnsweringModelOutput(ModelOutput):\n-    \"\"\"\n-    Base class for outputs of question answering models.\n-\n-    Args:\n-        start_logits (`jnp.ndarray` of shape `(batch_size, sequence_length)`):\n-            Span-start scores (before SoftMax).\n-        end_logits (`jnp.ndarray` of shape `(batch_size, sequence_length)`):\n-            Span-end scores (before SoftMax).\n-        hidden_states (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-    \"\"\"\n-\n-    start_logits: Optional[jnp.ndarray] = None\n-    end_logits: Optional[jnp.ndarray] = None\n-    hidden_states: Optional[tuple[jnp.ndarray]] = None\n-    attentions: Optional[tuple[jnp.ndarray]] = None\n-\n-\n-@flax.struct.dataclass\n-class FlaxSeq2SeqQuestionAnsweringModelOutput(ModelOutput):\n-    \"\"\"\n-    Base class for outputs of sequence-to-sequence question answering models.\n-\n-    Args:\n-        start_logits (`jnp.ndarray` of shape `(batch_size, sequence_length)`):\n-            Span-start scores (before SoftMax).\n-        end_logits (`jnp.ndarray` of shape `(batch_size, sequence_length)`):\n-            Span-end scores (before SoftMax).\n-        past_key_values (`tuple(tuple(jnp.ndarray))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(jnp.ndarray)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n-            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n-        decoder_hidden_states (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.\n-        decoder_attentions (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        cross_attentions (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n-            weighted average in the cross-attention heads.\n-        encoder_last_hidden_state (`jnp.ndarray` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_hidden_states (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.\n-        encoder_attentions (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-    \"\"\"\n-\n-    start_logits: Optional[jnp.ndarray] = None\n-    end_logits: Optional[jnp.ndarray] = None\n-    past_key_values: Optional[tuple[tuple[jnp.ndarray]]] = None\n-    decoder_hidden_states: Optional[tuple[jnp.ndarray]] = None\n-    decoder_attentions: Optional[tuple[jnp.ndarray]] = None\n-    cross_attentions: Optional[tuple[jnp.ndarray]] = None\n-    encoder_last_hidden_state: Optional[jnp.ndarray] = None\n-    encoder_hidden_states: Optional[tuple[jnp.ndarray]] = None\n-    encoder_attentions: Optional[tuple[jnp.ndarray]] = None"
        },
        {
            "sha": "dece5233d956064e81dc1edf6dee4f71167b62e1",
            "filename": "src/transformers/modeling_flax_pytorch_utils.py",
            "status": "removed",
            "additions": 0,
            "deletions": 491,
            "changes": 491,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodeling_flax_pytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodeling_flax_pytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flax_pytorch_utils.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,491 +0,0 @@\n-# coding=utf-8\n-# Copyright 2021 The HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"PyTorch - Flax general utilities.\"\"\"\n-\n-import os\n-from pickle import UnpicklingError\n-\n-import jax\n-import jax.numpy as jnp\n-import numpy as np\n-from flax.serialization import from_bytes\n-from flax.traverse_util import flatten_dict, unflatten_dict\n-\n-import transformers\n-\n-from . import is_safetensors_available, is_torch_available\n-from .utils import check_torch_load_is_safe, logging\n-\n-\n-if is_torch_available():\n-    import torch\n-\n-if is_safetensors_available():\n-    from safetensors import safe_open\n-    from safetensors.flax import load_file as safe_load_file\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-#####################\n-# PyTorch => Flax #\n-#####################\n-\n-\n-def load_pytorch_checkpoint_in_flax_state_dict(\n-    flax_model, pytorch_checkpoint_path, is_sharded, allow_missing_keys=False\n-):\n-    \"\"\"Load pytorch checkpoints in a flax model\"\"\"\n-\n-    if not is_sharded:\n-        pt_path = os.path.abspath(pytorch_checkpoint_path)\n-        logger.info(f\"Loading PyTorch weights from {pt_path}\")\n-\n-        if pt_path.endswith(\".safetensors\"):\n-            pt_state_dict = {}\n-            with safe_open(pt_path, framework=\"flax\") as f:\n-                for k in f.keys():\n-                    pt_state_dict[k] = f.get_tensor(k)\n-        else:\n-            try:\n-                import torch  # noqa: F401\n-            except (ImportError, ModuleNotFoundError):\n-                logger.error(\n-                    \"Loading a PyTorch model in Flax, requires both PyTorch and Flax to be installed. Please see\"\n-                    \" https://pytorch.org/ and https://flax.readthedocs.io/en/latest/index.html#installation for installation\"\n-                    \" instructions.\"\n-                )\n-                raise\n-\n-            check_torch_load_is_safe()\n-            pt_state_dict = torch.load(pt_path, map_location=\"cpu\", weights_only=True)\n-            logger.info(f\"PyTorch checkpoint contains {sum(t.numel() for t in pt_state_dict.values()):,} parameters.\")\n-\n-        flax_state_dict = convert_pytorch_state_dict_to_flax(pt_state_dict, flax_model)\n-    else:\n-        # model is sharded and pytorch_checkpoint_path already contains the list of .pt shard files\n-        flax_state_dict = convert_pytorch_sharded_state_dict_to_flax(pytorch_checkpoint_path, flax_model)\n-    return flax_state_dict\n-\n-\n-def rename_key_and_reshape_tensor(\n-    pt_tuple_key: tuple[str],\n-    pt_tensor: np.ndarray,\n-    random_flax_state_dict: dict[str, jnp.ndarray],\n-    model_prefix: str,\n-) -> tuple[tuple[str], np.ndarray]:\n-    \"\"\"Rename PT weight names to corresponding Flax weight names and reshape tensor if necessary\"\"\"\n-\n-    def is_key_or_prefix_key_in_dict(key: tuple[str]) -> bool:\n-        \"\"\"Checks if `key` of `(prefix,) + key` is in random_flax_state_dict\"\"\"\n-        return len(set(random_flax_state_dict) & {key, (model_prefix,) + key}) > 0\n-\n-    # layer norm\n-    renamed_pt_tuple_key = pt_tuple_key[:-1] + (\"scale\",)\n-    if pt_tuple_key[-1] in [\"weight\", \"gamma\"] and is_key_or_prefix_key_in_dict(renamed_pt_tuple_key):\n-        return renamed_pt_tuple_key, pt_tensor\n-\n-    # batch norm layer mean\n-    renamed_pt_tuple_key = pt_tuple_key[:-1] + (\"mean\",)\n-    if pt_tuple_key[-1] == \"running_mean\" and not is_key_or_prefix_key_in_dict(pt_tuple_key):\n-        return renamed_pt_tuple_key, pt_tensor\n-\n-    # batch norm layer var\n-    renamed_pt_tuple_key = pt_tuple_key[:-1] + (\"var\",)\n-    if pt_tuple_key[-1] == \"running_var\" and not is_key_or_prefix_key_in_dict(pt_tuple_key):\n-        return renamed_pt_tuple_key, pt_tensor\n-\n-    # embedding\n-    renamed_pt_tuple_key = pt_tuple_key[:-1] + (\"embedding\",)\n-    if pt_tuple_key[-1] == \"weight\" and is_key_or_prefix_key_in_dict(renamed_pt_tuple_key):\n-        return renamed_pt_tuple_key, pt_tensor\n-\n-    # conv layer\n-    renamed_pt_tuple_key = pt_tuple_key[:-1] + (\"kernel\",)\n-    if pt_tuple_key[-1] == \"weight\" and pt_tensor.ndim == 4 and not is_key_or_prefix_key_in_dict(pt_tuple_key):\n-        pt_tensor = pt_tensor.transpose(2, 3, 1, 0)\n-        return renamed_pt_tuple_key, pt_tensor\n-\n-    # linear layer\n-    renamed_pt_tuple_key = pt_tuple_key[:-1] + (\"kernel\",)\n-    if pt_tuple_key[-1] == \"weight\" and not is_key_or_prefix_key_in_dict(pt_tuple_key):\n-        pt_tensor = pt_tensor.T\n-        return renamed_pt_tuple_key, pt_tensor\n-\n-    # old PyTorch layer norm weight\n-    renamed_pt_tuple_key = pt_tuple_key[:-1] + (\"weight\",)\n-    if pt_tuple_key[-1] == \"gamma\":\n-        return renamed_pt_tuple_key, pt_tensor\n-\n-    # old PyTorch layer norm bias\n-    renamed_pt_tuple_key = pt_tuple_key[:-1] + (\"bias\",)\n-    if pt_tuple_key[-1] == \"beta\":\n-        return renamed_pt_tuple_key, pt_tensor\n-\n-    # New `weight_norm` from https://github.com/huggingface/transformers/pull/24030\n-    name = None\n-    if pt_tuple_key[-3::2] == (\"parametrizations\", \"original0\"):\n-        name = pt_tuple_key[-2] + \"_g\"\n-    elif pt_tuple_key[-3::2] == (\"parametrizations\", \"original1\"):\n-        name = pt_tuple_key[-2] + \"_v\"\n-    if name is not None:\n-        renamed_pt_tuple_key = pt_tuple_key[:-3] + (name,)\n-        return renamed_pt_tuple_key, pt_tensor\n-\n-    return pt_tuple_key, pt_tensor\n-\n-\n-def convert_pytorch_state_dict_to_flax(pt_state_dict, flax_model):\n-    # convert pytorch tensor to numpy\n-    from_bin = is_torch_available() and isinstance(next(iter(pt_state_dict.values())), torch.Tensor)\n-    bfloat16 = torch.bfloat16 if from_bin else \"bfloat16\"\n-\n-    weight_dtypes = {k: v.dtype for k, v in pt_state_dict.items()}\n-\n-    if from_bin:\n-        for k, v in pt_state_dict.items():\n-            # numpy currently does not support bfloat16, need to go over float32 in this case to not lose precision\n-            if v.dtype == bfloat16:\n-                v = v.float()\n-            pt_state_dict[k] = v.cpu().numpy()\n-\n-    model_prefix = flax_model.base_model_prefix\n-\n-    # use params dict if the model contains batch norm layers\n-    if \"params\" in flax_model.params:\n-        flax_model_params = flax_model.params[\"params\"]\n-    else:\n-        flax_model_params = flax_model.params\n-    random_flax_state_dict = flatten_dict(flax_model_params)\n-\n-    # add batch_stats keys,values to dict\n-    if \"batch_stats\" in flax_model.params:\n-        flax_batch_stats = flatten_dict(flax_model.params[\"batch_stats\"])\n-        random_flax_state_dict.update(flax_batch_stats)\n-\n-    flax_state_dict = {}\n-\n-    load_model_with_head_into_base_model = (model_prefix not in flax_model_params) and (\n-        model_prefix in {k.split(\".\")[0] for k in pt_state_dict}\n-    )\n-    load_base_model_into_model_with_head = (model_prefix in flax_model_params) and (\n-        model_prefix not in {k.split(\".\")[0] for k in pt_state_dict}\n-    )\n-\n-    # Need to change some parameters name to match Flax names\n-    for pt_key, pt_tensor in pt_state_dict.items():\n-        pt_tuple_key = tuple(pt_key.split(\".\"))\n-        is_bfloat_16 = weight_dtypes[pt_key] == bfloat16\n-\n-        # remove base model prefix if necessary\n-        has_base_model_prefix = pt_tuple_key[0] == model_prefix\n-        if load_model_with_head_into_base_model and has_base_model_prefix:\n-            pt_tuple_key = pt_tuple_key[1:]\n-\n-        # Correctly rename weight parameters\n-        flax_key, flax_tensor = rename_key_and_reshape_tensor(\n-            pt_tuple_key, pt_tensor, random_flax_state_dict, model_prefix\n-        )\n-\n-        # add model prefix if necessary\n-        require_base_model_prefix = (model_prefix,) + flax_key in random_flax_state_dict\n-        if load_base_model_into_model_with_head and require_base_model_prefix:\n-            flax_key = (model_prefix,) + flax_key\n-\n-        if flax_key in random_flax_state_dict:\n-            if flax_tensor.shape != random_flax_state_dict[flax_key].shape:\n-                raise ValueError(\n-                    f\"PyTorch checkpoint seems to be incorrect. Weight {pt_key} was expected to be of shape \"\n-                    f\"{random_flax_state_dict[flax_key].shape}, but is {flax_tensor.shape}.\"\n-                )\n-\n-        # add batch stats if the model contains batchnorm layers\n-        if \"batch_stats\" in flax_model.params:\n-            if \"mean\" in flax_key[-1] or \"var\" in flax_key[-1]:\n-                flax_state_dict[(\"batch_stats\",) + flax_key] = jnp.asarray(flax_tensor)\n-                continue\n-            # remove num_batches_tracked key\n-            if \"num_batches_tracked\" in flax_key[-1]:\n-                flax_state_dict.pop(flax_key, None)\n-                continue\n-\n-            # also add unexpected weight so that warning is thrown\n-            flax_state_dict[(\"params\",) + flax_key] = (\n-                jnp.asarray(flax_tensor) if not is_bfloat_16 else jnp.asarray(flax_tensor, dtype=jnp.bfloat16)\n-            )\n-        else:\n-            # also add unexpected weight so that warning is thrown\n-            flax_state_dict[flax_key] = (\n-                jnp.asarray(flax_tensor) if not is_bfloat_16 else jnp.asarray(flax_tensor, dtype=jnp.bfloat16)\n-            )\n-\n-    return unflatten_dict(flax_state_dict)\n-\n-\n-############################\n-# Sharded Pytorch => Flax #\n-############################\n-\n-\n-def convert_pytorch_sharded_state_dict_to_flax(shard_filenames, flax_model):\n-    import torch\n-\n-    # Load the index\n-    flax_state_dict = {}\n-    for shard_file in shard_filenames:\n-        # load using msgpack utils\n-        check_torch_load_is_safe()\n-        pt_state_dict = torch.load(shard_file, weights_only=True)\n-        weight_dtypes = {k: v.dtype for k, v in pt_state_dict.items()}\n-        pt_state_dict = {\n-            k: v.numpy() if v.dtype != torch.bfloat16 else v.float().numpy() for k, v in pt_state_dict.items()\n-        }\n-\n-        model_prefix = flax_model.base_model_prefix\n-\n-        # use params dict if the model contains batch norm layers and then add batch_stats keys,values to dict\n-        if \"batch_stats\" in flax_model.params:\n-            flax_model_params = flax_model.params[\"params\"]\n-\n-            random_flax_state_dict = flatten_dict(flax_model_params)\n-            random_flax_state_dict.update(flatten_dict(flax_model.params[\"batch_stats\"]))\n-        else:\n-            flax_model_params = flax_model.params\n-            random_flax_state_dict = flatten_dict(flax_model_params)\n-\n-        load_model_with_head_into_base_model = (model_prefix not in flax_model_params) and (\n-            model_prefix in {k.split(\".\")[0] for k in pt_state_dict}\n-        )\n-        load_base_model_into_model_with_head = (model_prefix in flax_model_params) and (\n-            model_prefix not in {k.split(\".\")[0] for k in pt_state_dict}\n-        )\n-        # Need to change some parameters name to match Flax names\n-        for pt_key, pt_tensor in pt_state_dict.items():\n-            pt_tuple_key = tuple(pt_key.split(\".\"))\n-            is_bfloat_16 = weight_dtypes[pt_key] == torch.bfloat16\n-\n-            # remove base model prefix if necessary\n-            has_base_model_prefix = pt_tuple_key[0] == model_prefix\n-            if load_model_with_head_into_base_model and has_base_model_prefix:\n-                pt_tuple_key = pt_tuple_key[1:]\n-\n-            # Correctly rename weight parameters\n-            flax_key, flax_tensor = rename_key_and_reshape_tensor(\n-                pt_tuple_key, pt_tensor, random_flax_state_dict, model_prefix\n-            )\n-            # add model prefix if necessary\n-            require_base_model_prefix = (model_prefix,) + flax_key in random_flax_state_dict\n-            if load_base_model_into_model_with_head and require_base_model_prefix:\n-                flax_key = (model_prefix,) + flax_key\n-\n-            if flax_key in random_flax_state_dict:\n-                if flax_tensor.shape != random_flax_state_dict[flax_key].shape:\n-                    raise ValueError(\n-                        f\"PyTorch checkpoint seems to be incorrect. Weight {pt_key} was expected to be of shape \"\n-                        f\"{random_flax_state_dict[flax_key].shape}, but is {flax_tensor.shape}.\"\n-                    )\n-\n-            # add batch stats if the model contains batchnorm layers\n-            if \"batch_stats\" in flax_model.params:\n-                if \"mean\" in flax_key[-1]:\n-                    flax_state_dict[(\"batch_stats\",) + flax_key] = jnp.asarray(flax_tensor)\n-                    continue\n-                if \"var\" in flax_key[-1]:\n-                    flax_state_dict[(\"batch_stats\",) + flax_key] = jnp.asarray(flax_tensor)\n-                    continue\n-                # remove num_batches_tracked key\n-                if \"num_batches_tracked\" in flax_key[-1]:\n-                    flax_state_dict.pop(flax_key, None)\n-                    continue\n-\n-                # also add unexpected weight so that warning is thrown\n-                flax_state_dict[(\"params\",) + flax_key] = (\n-                    jnp.asarray(flax_tensor) if not is_bfloat_16 else jnp.asarray(flax_tensor, dtype=jnp.bfloat16)\n-                )\n-\n-            else:\n-                # also add unexpected weight so that warning is thrown\n-                flax_state_dict[flax_key] = (\n-                    jnp.asarray(flax_tensor) if not is_bfloat_16 else jnp.asarray(flax_tensor, dtype=jnp.bfloat16)\n-                )\n-    return unflatten_dict(flax_state_dict)\n-\n-\n-#####################\n-# Flax => PyTorch #\n-#####################\n-\n-\n-def load_flax_checkpoint_in_pytorch_model(model, flax_checkpoint_path):\n-    \"\"\"Load flax checkpoints in a PyTorch model\"\"\"\n-    flax_checkpoint_path = os.path.abspath(flax_checkpoint_path)\n-    logger.info(f\"Loading Flax weights from {flax_checkpoint_path}\")\n-\n-    # import correct flax class\n-    flax_cls = getattr(transformers, \"Flax\" + model.__class__.__name__)\n-\n-    # load flax weight dict\n-    if flax_checkpoint_path.endswith(\".safetensors\"):\n-        flax_state_dict = safe_load_file(flax_checkpoint_path)\n-        flax_state_dict = unflatten_dict(flax_state_dict, sep=\".\")\n-    else:\n-        with open(flax_checkpoint_path, \"rb\") as state_f:\n-            try:\n-                flax_state_dict = from_bytes(flax_cls, state_f.read())\n-            except UnpicklingError:\n-                raise OSError(f\"Unable to convert {flax_checkpoint_path} to Flax deserializable object. \")\n-\n-    return load_flax_weights_in_pytorch_model(model, flax_state_dict)\n-\n-\n-def load_flax_weights_in_pytorch_model(pt_model, flax_state):\n-    \"\"\"Load flax checkpoints in a PyTorch model\"\"\"\n-\n-    try:\n-        import torch  # noqa: F401\n-    except (ImportError, ModuleNotFoundError):\n-        logger.error(\n-            \"Loading a Flax weights in PyTorch, requires both PyTorch and Flax to be installed. Please see\"\n-            \" https://pytorch.org/ and https://flax.readthedocs.io/en/latest/index.html#installation for installation\"\n-            \" instructions.\"\n-        )\n-        raise\n-\n-    # check if we have bf16 weights\n-    is_type_bf16 = flatten_dict(jax.tree_util.tree_map(lambda x: x.dtype == jnp.bfloat16, flax_state)).values()\n-    if any(is_type_bf16):\n-        # convert all weights to fp32 if the are bf16 since torch.from_numpy can-not handle bf16\n-        # and bf16 is not fully supported in PT yet.\n-        logger.warning(\n-            \"Found ``bfloat16`` weights in Flax model. Casting all ``bfloat16`` weights to ``float32`` \"\n-            \"before loading those in PyTorch model.\"\n-        )\n-        flax_state = jax.tree_util.tree_map(\n-            lambda params: params.astype(np.float32) if params.dtype == jnp.bfloat16 else params, flax_state\n-        )\n-\n-    flax_state_dict = flatten_dict(flax_state)\n-    pt_model_dict = pt_model.state_dict()\n-\n-    load_model_with_head_into_base_model = (pt_model.base_model_prefix in flax_state) and (\n-        pt_model.base_model_prefix not in {k.split(\".\")[0] for k in pt_model_dict}\n-    )\n-    load_base_model_into_model_with_head = (pt_model.base_model_prefix not in flax_state) and (\n-        pt_model.base_model_prefix in {k.split(\".\")[0] for k in pt_model_dict}\n-    )\n-\n-    # keep track of unexpected & missing keys\n-    unexpected_keys = []\n-    missing_keys = set(pt_model_dict.keys())\n-\n-    for flax_key_tuple, flax_tensor in flax_state_dict.items():\n-        has_base_model_prefix = flax_key_tuple[0] == pt_model.base_model_prefix\n-        require_base_model_prefix = \".\".join((pt_model.base_model_prefix,) + flax_key_tuple) in pt_model_dict\n-\n-        # adapt flax_key to prepare for loading from/to base model only\n-        if load_model_with_head_into_base_model and has_base_model_prefix:\n-            flax_key_tuple = flax_key_tuple[1:]\n-        elif load_base_model_into_model_with_head and require_base_model_prefix:\n-            flax_key_tuple = (pt_model.base_model_prefix,) + flax_key_tuple\n-\n-        # rename flax weights to PyTorch format\n-        if flax_key_tuple[-1] == \"kernel\" and flax_tensor.ndim == 4 and \".\".join(flax_key_tuple) not in pt_model_dict:\n-            # conv layer\n-            flax_key_tuple = flax_key_tuple[:-1] + (\"weight\",)\n-            flax_tensor = jnp.transpose(flax_tensor, (3, 2, 0, 1))\n-        elif flax_key_tuple[-1] == \"kernel\" and \".\".join(flax_key_tuple) not in pt_model_dict:\n-            # linear layer\n-            flax_key_tuple = flax_key_tuple[:-1] + (\"weight\",)\n-            flax_tensor = flax_tensor.T\n-        elif flax_key_tuple[-1] in [\"scale\", \"embedding\"]:\n-            flax_key_tuple = flax_key_tuple[:-1] + (\"weight\",)\n-\n-        # adding batch stats from flax batch norm to pt\n-        elif \"mean\" in flax_key_tuple[-1]:\n-            flax_key_tuple = flax_key_tuple[:-1] + (\"running_mean\",)\n-        elif \"var\" in flax_key_tuple[-1]:\n-            flax_key_tuple = flax_key_tuple[:-1] + (\"running_var\",)\n-\n-        if \"batch_stats\" in flax_state:\n-            flax_key = \".\".join(flax_key_tuple[1:])  # Remove the params/batch_stats header\n-        else:\n-            flax_key = \".\".join(flax_key_tuple)\n-\n-        # We also need to look at `pt_model_dict` and see if there are keys requiring further transformation.\n-        special_pt_names = {}\n-        # New `weight_norm` from https://github.com/huggingface/transformers/pull/24030\n-        for key in pt_model_dict:\n-            key_components = key.split(\".\")\n-            name = None\n-            if key_components[-3::2] == [\"parametrizations\", \"original0\"]:\n-                name = key_components[-2] + \"_g\"\n-            elif key_components[-3::2] == [\"parametrizations\", \"original1\"]:\n-                name = key_components[-2] + \"_v\"\n-            if name is not None:\n-                key_components = key_components[:-3] + [name]\n-                key_to_check = \".\".join(key_components)\n-                special_pt_names[key_to_check] = key\n-\n-        if flax_key in special_pt_names:\n-            flax_key = special_pt_names[flax_key]\n-\n-        if flax_key in pt_model_dict:\n-            if flax_tensor.shape != pt_model_dict[flax_key].shape:\n-                raise ValueError(\n-                    f\"Flax checkpoint seems to be incorrect. Weight {flax_key_tuple} was expected \"\n-                    f\"to be of shape {pt_model_dict[flax_key].shape}, but is {flax_tensor.shape}.\"\n-                )\n-            else:\n-                # add weight to pytorch dict\n-                flax_tensor = np.asarray(flax_tensor) if not isinstance(flax_tensor, np.ndarray) else flax_tensor\n-                pt_model_dict[flax_key] = torch.from_numpy(flax_tensor)\n-                # remove from missing keys\n-                missing_keys.remove(flax_key)\n-        else:\n-            # weight is not expected by PyTorch model\n-            unexpected_keys.append(flax_key)\n-\n-    pt_model.load_state_dict(pt_model_dict)\n-\n-    # re-transform missing_keys to list\n-    missing_keys = list(missing_keys)\n-\n-    if len(unexpected_keys) > 0:\n-        logger.warning(\n-            \"Some weights of the Flax model were not used when initializing the PyTorch model\"\n-            f\" {pt_model.__class__.__name__}: {unexpected_keys}\\n- This IS expected if you are initializing\"\n-            f\" {pt_model.__class__.__name__} from a Flax model trained on another task or with another architecture\"\n-            \" (e.g. initializing a BertForSequenceClassification model from a FlaxBertForPreTraining model).\\n- This\"\n-            f\" IS NOT expected if you are initializing {pt_model.__class__.__name__} from a Flax model that you expect\"\n-            \" to be exactly identical (e.g. initializing a BertForSequenceClassification model from a\"\n-            \" FlaxBertForSequenceClassification model).\"\n-        )\n-    else:\n-        logger.warning(f\"All Flax model weights were used when initializing {pt_model.__class__.__name__}.\\n\")\n-    if len(missing_keys) > 0:\n-        logger.warning(\n-            f\"Some weights of {pt_model.__class__.__name__} were not initialized from the Flax model and are newly\"\n-            f\" initialized: {missing_keys}\\nYou should probably TRAIN this model on a down-stream task to be able to\"\n-            \" use it for predictions and inference.\"\n-        )\n-    else:\n-        logger.warning(\n-            f\"All the weights of {pt_model.__class__.__name__} were initialized from the Flax model.\\n\"\n-            \"If your task is similar to the task the model of the checkpoint was trained on, \"\n-            f\"you can already use {pt_model.__class__.__name__} for predictions without further training.\"\n-        )\n-\n-    return pt_model"
        },
        {
            "sha": "bc9a4d473f36f95bb13d6de17dc0bfa7cfae2279",
            "filename": "src/transformers/modeling_flax_utils.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1274,
            "changes": 1274,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodeling_flax_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodeling_flax_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flax_utils.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,1274 +0,0 @@\n-# coding=utf-8\n-# Copyright 2021 The Google Flax Team Authors and The HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\n-import gc\n-import json\n-import os\n-import warnings\n-from functools import partial\n-from pickle import UnpicklingError\n-from typing import Any, Optional, Union\n-\n-import flax.linen as nn\n-import jax\n-import jax.numpy as jnp\n-import msgpack.exceptions\n-from flax.core.frozen_dict import FrozenDict, unfreeze\n-from flax.serialization import from_bytes, to_bytes\n-from flax.traverse_util import flatten_dict, unflatten_dict\n-from jax.random import PRNGKey\n-\n-from .configuration_utils import PretrainedConfig\n-from .dynamic_module_utils import custom_object_save\n-from .generation import FlaxGenerationMixin, GenerationConfig\n-from .modeling_flax_pytorch_utils import load_pytorch_checkpoint_in_flax_state_dict\n-from .utils import (\n-    FLAX_WEIGHTS_INDEX_NAME,\n-    FLAX_WEIGHTS_NAME,\n-    SAFE_WEIGHTS_INDEX_NAME,\n-    SAFE_WEIGHTS_NAME,\n-    WEIGHTS_INDEX_NAME,\n-    WEIGHTS_NAME,\n-    PushToHubMixin,\n-    add_code_sample_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    cached_file,\n-    copy_func,\n-    download_url,\n-    has_file,\n-    is_offline_mode,\n-    is_remote_url,\n-    logging,\n-    replace_return_docstrings,\n-)\n-from .utils.hub import convert_file_size_to_int, get_checkpoint_shard_files\n-from .utils.import_utils import is_safetensors_available\n-\n-\n-if is_safetensors_available():\n-    from safetensors import safe_open\n-    from safetensors.flax import load_file as safe_load_file\n-    from safetensors.flax import save_file as safe_save_file\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-def quick_gelu(x):\n-    return x * jax.nn.sigmoid(1.702 * x)\n-\n-\n-ACT2FN = {\n-    \"gelu\": partial(nn.gelu, approximate=False),\n-    \"relu\": nn.relu,\n-    \"silu\": nn.swish,\n-    \"swish\": nn.swish,\n-    \"gelu_new\": partial(nn.gelu, approximate=True),\n-    \"quick_gelu\": quick_gelu,\n-    \"gelu_pytorch_tanh\": partial(nn.gelu, approximate=True),\n-    \"tanh\": nn.tanh,\n-}\n-\n-\n-def flax_shard_checkpoint(params, max_shard_size=\"10GB\"):\n-    \"\"\"\n-    Splits a model state dictionary in sub-checkpoints so that the final size of each sub-checkpoint does not exceed a\n-    given size. The sub-checkpoints are determined by iterating through the `state_dict` in the order of its keys, so\n-    there is no optimization made to make each sub-checkpoint as close as possible to the maximum size passed. For\n-    example, if the limit is 10GB and we have weights of sizes [6GB, 6GB, 2GB, 6GB, 2GB, 2GB] they will get sharded as\n-    [6GB], [6+2GB], [6+2+2GB] and not [6+2+2GB], [6+2GB], [6GB].\n-\n-    <Tip warning={true}>\n-\n-    If one of the model's weight is bigger that `max_shard_size`, it will end up in its own sub-checkpoint which will\n-    have a size greater than `max_shard_size`.\n-\n-    </Tip>\n-\n-    Args:\n-        params (`Union[Dict, FrozenDict]`): A `PyTree` of model parameters.\n-        max_shard_size (`int` or `str`, *optional*, defaults to `\"10GB\"`):\n-            The maximum size of each sub-checkpoint. If expressed as a string, needs to be digits followed by a unit\n-            (like `\"5MB\"`).\n-    \"\"\"\n-    max_shard_size = convert_file_size_to_int(max_shard_size)\n-\n-    sharded_state_dicts = []\n-    current_block = {}\n-    current_block_size = 0\n-    total_size = 0\n-\n-    # flatten the weights to chunk\n-    weights = flatten_dict(params, sep=\"/\")\n-    for item in weights:\n-        weight_size = weights[item].size * weights[item].dtype.itemsize\n-\n-        # If this weight is going to tip up over the maximal size, we split.\n-        if current_block_size + weight_size > max_shard_size:\n-            sharded_state_dicts.append(current_block)\n-            current_block = {}\n-            current_block_size = 0\n-\n-        current_block[item] = weights[item]\n-        current_block_size += weight_size\n-        total_size += weight_size\n-\n-    # Add the last block\n-    sharded_state_dicts.append(current_block)\n-\n-    # If we only have one shard, we return it\n-    if len(sharded_state_dicts) == 1:\n-        return {FLAX_WEIGHTS_NAME: sharded_state_dicts[0]}, None\n-\n-    # Otherwise, let's build the index\n-    weight_map = {}\n-    shards = {}\n-    for idx, shard in enumerate(sharded_state_dicts):\n-        shard_file = FLAX_WEIGHTS_NAME.replace(\".msgpack\", f\"-{idx + 1:05d}-of-{len(sharded_state_dicts):05d}.msgpack\")\n-        shards[shard_file] = shard\n-        for weight_name in shard:\n-            weight_map[weight_name] = shard_file\n-\n-    # Add the metadata\n-    metadata = {\"total_size\": total_size}\n-    index = {\"metadata\": metadata, \"weight_map\": weight_map}\n-    return shards, index\n-\n-\n-class FlaxPreTrainedModel(PushToHubMixin, FlaxGenerationMixin):\n-    r\"\"\"\n-    Base class for all models.\n-\n-    [`FlaxPreTrainedModel`] takes care of storing the configuration of the models and handles methods for loading,\n-    downloading and saving models.\n-\n-    Class attributes (overridden by derived classes):\n-\n-        - **config_class** ([`PretrainedConfig`]) -- A subclass of [`PretrainedConfig`] to use as configuration class\n-          for this model architecture.\n-        - **base_model_prefix** (`str`) -- A string indicating the attribute associated to the base model in derived\n-          classes of the same architecture adding modules on top of the base model.\n-        - **main_input_name** (`str`) -- The name of the principal input to the model (often `input_ids` for NLP\n-          models, `pixel_values` for vision models and `input_values` for speech models).\n-    \"\"\"\n-\n-    config_class = None\n-    base_model_prefix = \"\"\n-    main_input_name = \"input_ids\"\n-    _auto_class = None\n-    _missing_keys = set()\n-\n-    def __init__(\n-        self,\n-        config: PretrainedConfig,\n-        module: nn.Module,\n-        input_shape: tuple = (1, 1),\n-        seed: int = 0,\n-        dtype: jnp.dtype = jnp.float32,\n-        _do_init: bool = True,\n-    ):\n-        logger.warning_once(\n-            \"TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We \"\n-            \"recommend migrating to PyTorch classes or pinning your version of Transformers.\"\n-        )\n-        if config is None:\n-            raise ValueError(\"config cannot be None\")\n-\n-        if module is None:\n-            raise ValueError(\"module cannot be None\")\n-\n-        # Those are private to be exposed as typed property on derived classes.\n-        self._config = config\n-        self._module = module\n-\n-        # Those are public as their type is generic to every derived classes.\n-        self.key = PRNGKey(seed)\n-        self.dtype = dtype\n-        self.input_shape = input_shape\n-        self.generation_config = GenerationConfig.from_model_config(config) if self.can_generate() else None\n-\n-        # To check if the model was initialized automatically.\n-        self._is_initialized = _do_init\n-\n-        if _do_init:\n-            # randomly initialized parameters\n-            random_params = self.init_weights(self.key, input_shape)\n-            params_shape_tree = jax.eval_shape(lambda params: params, random_params)\n-        else:\n-            init_fn = partial(self.init_weights, input_shape=input_shape)\n-            params_shape_tree = jax.eval_shape(init_fn, self.key)\n-\n-            logger.info(\n-                \"Model weights are not initialized as `_do_init` is set to `False`. \"\n-                f\"Make sure to call `{self.__class__.__name__}.init_weights` manually to initialize the weights.\"\n-            )\n-\n-        # get the shape of the parameters\n-        self._params_shape_tree = params_shape_tree\n-\n-        # save required_params as set\n-        self._required_params = set(flatten_dict(unfreeze(params_shape_tree)).keys())\n-\n-        # initialize the parameters\n-        if _do_init:\n-            self.params = random_params\n-\n-    def init_weights(self, rng: jax.random.PRNGKey, input_shape: tuple, params: FrozenDict = None) -> dict:\n-        raise NotImplementedError(f\"init method has to be implemented for {self}\")\n-\n-    def enable_gradient_checkpointing(self):\n-        raise NotImplementedError(f\"gradient checkpointing method has to be implemented for {self}\")\n-\n-    @classmethod\n-    def _from_config(cls, config, **kwargs):\n-        \"\"\"\n-        All context managers that the model should be initialized under go here.\n-        \"\"\"\n-        return cls(config, **kwargs)\n-\n-    @property\n-    def framework(self) -> str:\n-        \"\"\"\n-        :str: Identifies that this is a Flax model.\n-        \"\"\"\n-        return \"flax\"\n-\n-    @property\n-    def config(self) -> PretrainedConfig:\n-        return self._config\n-\n-    @property\n-    def module(self) -> nn.Module:\n-        return self._module\n-\n-    @property\n-    def params(self) -> Union[dict, FrozenDict]:\n-        if not self._is_initialized:\n-            raise ValueError(\n-                \"`params` cannot be accessed from model when the model is created with `_do_init=False`. \"\n-                \"You must call `init_weights` manually and store the params outside of the model and \"\n-                \"pass it explicitly where needed.\"\n-            )\n-        return self._params\n-\n-    @property\n-    def required_params(self) -> set:\n-        return self._required_params\n-\n-    @property\n-    def params_shape_tree(self) -> dict:\n-        return self._params_shape_tree\n-\n-    @params.setter\n-    def params(self, params: Union[dict, FrozenDict]):\n-        # don't set params if the model is not initialized\n-        if not self._is_initialized:\n-            raise ValueError(\n-                \"`params` cannot be set from model when the model is created with `_do_init=False`. \"\n-                \"You store the params outside of the model.\"\n-            )\n-\n-        if isinstance(params, FrozenDict):\n-            params = unfreeze(params)\n-        param_keys = set(flatten_dict(params).keys())\n-        if len(self.required_params - param_keys) > 0:\n-            raise ValueError(\n-                \"Some parameters are missing. Make sure that `params` include the following \"\n-                f\"parameters {self.required_params - param_keys}\"\n-            )\n-        self._params = params\n-\n-    def _cast_floating_to(self, params: Union[dict, FrozenDict], dtype: jnp.dtype, mask: Any = None) -> Any:\n-        \"\"\"\n-        Helper method to cast floating-point values of given parameter `PyTree` to given `dtype`.\n-        \"\"\"\n-\n-        # taken from https://github.com/deepmind/jmp/blob/3a8318abc3292be38582794dbf7b094e6583b192/jmp/_src/policy.py#L27\n-        def conditional_cast(param):\n-            if isinstance(param, jnp.ndarray) and jnp.issubdtype(param.dtype, jnp.floating):\n-                param = param.astype(dtype)\n-            return param\n-\n-        if mask is None:\n-            return jax.tree_util.tree_map(conditional_cast, params)\n-\n-        flat_params = flatten_dict(params)\n-        flat_mask, _ = jax.tree_util.tree_flatten(mask)\n-\n-        for masked, key in zip(flat_mask, sorted(flat_params.keys())):\n-            if masked:\n-                flat_params[key] = conditional_cast(flat_params[key])\n-\n-        return unflatten_dict(flat_params)\n-\n-    def to_bf16(self, params: Union[dict, FrozenDict], mask: Any = None):\n-        r\"\"\"\n-        Cast the floating-point `params` to `jax.numpy.bfloat16`. This returns a new `params` tree and does not cast\n-        the `params` in place.\n-\n-        This method can be used on TPU to explicitly convert the model parameters to bfloat16 precision to do full\n-        half-precision training or to save weights in bfloat16 for inference in order to save memory and improve speed.\n-\n-        Arguments:\n-            params (`Union[Dict, FrozenDict]`):\n-                A `PyTree` of model parameters.\n-            mask (`Union[Dict, FrozenDict]`):\n-                A `PyTree` with same structure as the `params` tree. The leaves should be booleans, `True` for params\n-                you want to cast, and should be `False` for those you want to skip.\n-\n-        Examples:\n-\n-        ```python\n-        >>> from transformers import FlaxBertModel\n-\n-        >>> # load model\n-        >>> model = FlaxBertModel.from_pretrained(\"google-bert/bert-base-cased\")\n-        >>> # By default, the model parameters will be in fp32 precision, to cast these to bfloat16 precision\n-        >>> model.params = model.to_bf16(model.params)\n-        >>> # If you want don't want to cast certain parameters (for example layer norm bias and scale)\n-        >>> # then pass the mask as follows\n-        >>> from flax import traverse_util\n-\n-        >>> model = FlaxBertModel.from_pretrained(\"google-bert/bert-base-cased\")\n-        >>> flat_params = traverse_util.flatten_dict(model.params)\n-        >>> mask = {\n-        ...     path: (path[-2] != (\"LayerNorm\", \"bias\") and path[-2:] != (\"LayerNorm\", \"scale\"))\n-        ...     for path in flat_params\n-        ... }\n-        >>> mask = traverse_util.unflatten_dict(mask)\n-        >>> model.params = model.to_bf16(model.params, mask)\n-        ```\"\"\"\n-        return self._cast_floating_to(params, jnp.bfloat16, mask)\n-\n-    def to_fp32(self, params: Union[dict, FrozenDict], mask: Any = None):\n-        r\"\"\"\n-        Cast the floating-point `params` to `jax.numpy.float32`. This method can be used to explicitly convert the\n-        model parameters to fp32 precision. This returns a new `params` tree and does not cast the `params` in place.\n-\n-        Arguments:\n-            params (`Union[Dict, FrozenDict]`):\n-                A `PyTree` of model parameters.\n-            mask (`Union[Dict, FrozenDict]`):\n-                A `PyTree` with same structure as the `params` tree. The leaves should be booleans, `True` for params\n-                you want to cast, and should be `False` for those you want to skip\n-\n-        Examples:\n-\n-        ```python\n-        >>> from transformers import FlaxBertModel\n-\n-        >>> # Download model and configuration from huggingface.co\n-        >>> model = FlaxBertModel.from_pretrained(\"google-bert/bert-base-cased\")\n-        >>> # By default, the model params will be in fp32, to illustrate the use of this method,\n-        >>> # we'll first cast to fp16 and back to fp32\n-        >>> model.params = model.to_f16(model.params)\n-        >>> # now cast back to fp32\n-        >>> model.params = model.to_fp32(model.params)\n-        ```\"\"\"\n-        return self._cast_floating_to(params, jnp.float32, mask)\n-\n-    def to_fp16(self, params: Union[dict, FrozenDict], mask: Any = None):\n-        r\"\"\"\n-        Cast the floating-point `params` to `jax.numpy.float16`. This returns a new `params` tree and does not cast the\n-        `params` in place.\n-\n-        This method can be used on GPU to explicitly convert the model parameters to float16 precision to do full\n-        half-precision training or to save weights in float16 for inference in order to save memory and improve speed.\n-\n-        Arguments:\n-            params (`Union[Dict, FrozenDict]`):\n-                A `PyTree` of model parameters.\n-            mask (`Union[Dict, FrozenDict]`):\n-                A `PyTree` with same structure as the `params` tree. The leaves should be booleans, `True` for params\n-                you want to cast, and should be `False` for those you want to skip\n-\n-        Examples:\n-\n-        ```python\n-        >>> from transformers import FlaxBertModel\n-\n-        >>> # load model\n-        >>> model = FlaxBertModel.from_pretrained(\"google-bert/bert-base-cased\")\n-        >>> # By default, the model params will be in fp32, to cast these to float16\n-        >>> model.params = model.to_fp16(model.params)\n-        >>> # If you want don't want to cast certain parameters (for example layer norm bias and scale)\n-        >>> # then pass the mask as follows\n-        >>> from flax import traverse_util\n-\n-        >>> model = FlaxBertModel.from_pretrained(\"google-bert/bert-base-cased\")\n-        >>> flat_params = traverse_util.flatten_dict(model.params)\n-        >>> mask = {\n-        ...     path: (path[-2] != (\"LayerNorm\", \"bias\") and path[-2:] != (\"LayerNorm\", \"scale\"))\n-        ...     for path in flat_params\n-        ... }\n-        >>> mask = traverse_util.unflatten_dict(mask)\n-        >>> model.params = model.to_fp16(model.params, mask)\n-        ```\"\"\"\n-        return self._cast_floating_to(params, jnp.float16, mask)\n-\n-    @classmethod\n-    def load_flax_weights(cls, resolved_archive_file):\n-        try:\n-            if resolved_archive_file.endswith(\".safetensors\"):\n-                state = safe_load_file(resolved_archive_file)\n-                state = unflatten_dict(state, sep=\".\")\n-            else:\n-                with open(resolved_archive_file, \"rb\") as state_f:\n-                    state = from_bytes(cls, state_f.read())\n-        except (UnpicklingError, msgpack.exceptions.ExtraData) as e:\n-            try:\n-                with open(resolved_archive_file) as f:\n-                    if f.read().startswith(\"version\"):\n-                        raise OSError(\n-                            \"You seem to have cloned a repository without having git-lfs installed. Please\"\n-                            \" install git-lfs and run `git lfs install` followed by `git lfs pull` in the\"\n-                            \" folder you cloned.\"\n-                        )\n-                    else:\n-                        raise ValueError from e\n-            except (UnicodeDecodeError, ValueError):\n-                raise OSError(f\"Unable to convert {resolved_archive_file} to Flax deserializable object. \")\n-\n-        return state\n-\n-    @classmethod\n-    def load_flax_sharded_weights(cls, shard_files):\n-        \"\"\"\n-        This is the same as [`flax.serialization.from_bytes`]\n-        (https:lax.readthedocs.io/en/latest/_modules/flax/serialization.html#from_bytes) but for a sharded checkpoint.\n-\n-        This load is performed efficiently: each checkpoint shard is loaded one by one in RAM and deleted after being\n-        loaded in the model.\n-\n-        Args:\n-            shard_files (`list[str]`:\n-                The list of shard files to load.\n-\n-        Returns:\n-            `Dict`: A nested dictionary of the model parameters, in the expected format for flax models : `{'model':\n-            {'params': {'...'}}}`.\n-        \"\"\"\n-\n-        # Load the index\n-        state_sharded_dict = {}\n-\n-        for shard_file in shard_files:\n-            # load using msgpack utils\n-            try:\n-                with open(shard_file, \"rb\") as state_f:\n-                    state = from_bytes(cls, state_f.read())\n-            except (UnpicklingError, msgpack.exceptions.ExtraData) as e:\n-                with open(shard_file) as f:\n-                    if f.read().startswith(\"version\"):\n-                        raise OSError(\n-                            \"You seem to have cloned a repository without having git-lfs installed. Please\"\n-                            \" install git-lfs and run `git lfs install` followed by `git lfs pull` in the\"\n-                            \" folder you cloned.\"\n-                        )\n-                    else:\n-                        raise ValueError from e\n-            except (UnicodeDecodeError, ValueError):\n-                raise OSError(f\"Unable to convert {shard_file} to Flax deserializable object. \")\n-\n-            state = flatten_dict(state, sep=\"/\")\n-            state_sharded_dict.update(state)\n-            del state\n-            gc.collect()\n-\n-        # the state dict is unflattened to the match the format of model.params\n-        return unflatten_dict(state_sharded_dict, sep=\"/\")\n-\n-    @classmethod\n-    def can_generate(cls) -> bool:\n-        \"\"\"\n-        Returns whether this model can generate sequences with `.generate()`. Returns:\n-            `bool`: Whether this model can generate sequences with `.generate()`.\n-        \"\"\"\n-        # Detects whether `prepare_inputs_for_generation` has been overwritten, which is a requirement for generation.\n-        # Alternatively, the model can also have a custom `generate` function.\n-        if \"GenerationMixin\" in str(cls.prepare_inputs_for_generation) and \"GenerationMixin\" in str(cls.generate):\n-            return False\n-        return True\n-\n-    @classmethod\n-    def from_pretrained(\n-        cls,\n-        pretrained_model_name_or_path: Union[str, os.PathLike],\n-        dtype: jnp.dtype = jnp.float32,\n-        *model_args,\n-        config: Optional[Union[PretrainedConfig, str, os.PathLike]] = None,\n-        cache_dir: Optional[Union[str, os.PathLike]] = None,\n-        ignore_mismatched_sizes: bool = False,\n-        force_download: bool = False,\n-        local_files_only: bool = False,\n-        token: Optional[Union[str, bool]] = None,\n-        revision: str = \"main\",\n-        **kwargs,\n-    ):\n-        r\"\"\"\n-        Instantiate a pretrained flax model from a pre-trained model configuration.\n-\n-        The warning *Weights from XXX not initialized from pretrained model* means that the weights of XXX do not come\n-        pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning\n-        task.\n-\n-        The warning *Weights from XXX not used in YYY* means that the layer XXX is not used by YYY, therefore those\n-        weights are discarded.\n-\n-        Parameters:\n-            pretrained_model_name_or_path (`str` or `os.PathLike`):\n-                Can be either:\n-\n-                    - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\n-                    - A path to a *directory* containing model weights saved using\n-                      [`~FlaxPreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\n-                    - A path or url to a *pt index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In this case,\n-                      `from_pt` should be set to `True`.\n-            dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):\n-                The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and\n-                `jax.numpy.bfloat16` (on TPUs).\n-\n-                This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If\n-                specified all the computation will be performed with the given `dtype`.\n-\n-                **Note that this only specifies the dtype of the computation and does not influence the dtype of model\n-                parameters.**\n-\n-                If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and\n-                [`~FlaxPreTrainedModel.to_bf16`].\n-            model_args (sequence of positional arguments, *optional*):\n-                All remaining positional arguments will be passed to the underlying model's `__init__` method.\n-            config (`Union[PretrainedConfig, str, os.PathLike]`, *optional*):\n-                Can be either:\n-\n-                    - an instance of a class derived from [`PretrainedConfig`],\n-                    - a string or path valid as input to [`~PretrainedConfig.from_pretrained`].\n-\n-                Configuration for the model to use instead of an automatically loaded configuration. Configuration can\n-                be automatically loaded when:\n-\n-                    - The model is a model provided by the library (loaded with the *model id* string of a pretrained\n-                      model).\n-                    - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the\n-                      save directory.\n-                    - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a\n-                      configuration JSON file named *config.json* is found in the directory.\n-            cache_dir (`Union[str, os.PathLike]`, *optional*):\n-                Path to a directory in which a downloaded pretrained model configuration should be cached if the\n-                standard cache should not be used.\n-            from_pt (`bool`, *optional*, defaults to `False`):\n-                Load the model weights from a PyTorch checkpoint save file (see docstring of\n-                `pretrained_model_name_or_path` argument).\n-            ignore_mismatched_sizes (`bool`, *optional*, defaults to `False`):\n-                Whether or not to raise an error if some of the weights from the checkpoint do not have the same size\n-                as the weights of the model (if for instance, you are instantiating a model with 10 labels from a\n-                checkpoint with 3 labels).\n-            force_download (`bool`, *optional*, defaults to `False`):\n-                Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n-                cached versions if they exist.\n-            resume_download:\n-                Deprecated and ignored. All downloads are now resumed by default when possible.\n-                Will be removed in v5 of Transformers.\n-            proxies (`dict[str, str]`, *optional*):\n-                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n-                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n-            local_files_only(`bool`, *optional*, defaults to `False`):\n-                Whether or not to only look at local files (i.e., do not try to download the model).\n-            token (`str` or `bool`, *optional*):\n-                The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use\n-                the token generated when running `hf auth login` (stored in `~/.huggingface`).\n-            revision (`str`, *optional*, defaults to `\"main\"`):\n-                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n-                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n-                identifier allowed by git.\n-\n-\n-                <Tip>\n-\n-                To test a pull request you made on the Hub, you can pass `revision=\"refs/pr/<pr_number>\"`.\n-\n-                </Tip>\n-\n-            subfolder (`str`, *optional*, defaults to `\"\"`):\n-                In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can\n-                specify the folder name here.\n-            kwargs (remaining dictionary of keyword arguments, *optional*):\n-                Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,\n-                `output_attentions=True`). Behaves differently depending on whether a `config` is provided or\n-                automatically loaded:\n-\n-                    - If a configuration is provided with `config`, `**kwargs` will be directly passed to the\n-                      underlying model's `__init__` method (we assume all relevant updates to the configuration have\n-                      already been done)\n-                    - If a configuration is not provided, `kwargs` will be first passed to the configuration class\n-                      initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that\n-                      corresponds to a configuration attribute will be used to override said attribute with the\n-                      supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute\n-                      will be passed to the underlying model's `__init__` function.\n-\n-        Examples:\n-\n-        ```python\n-        >>> from transformers import BertConfig, FlaxBertModel\n-\n-        >>> # Download model and configuration from huggingface.co and cache.\n-        >>> model = FlaxBertModel.from_pretrained(\"google-bert/bert-base-cased\")\n-        >>> # Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).\n-        >>> model = FlaxBertModel.from_pretrained(\"./test/saved_model/\")\n-        >>> # Loading from a PyTorch checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).\n-        >>> config = BertConfig.from_json_file(\"./pt_model/config.json\")\n-        >>> model = FlaxBertModel.from_pretrained(\"./pt_model/pytorch_model.bin\", from_pt=True, config=config)\n-        ```\"\"\"\n-        from_pt = kwargs.pop(\"from_pt\", False)\n-        resume_download = kwargs.pop(\"resume_download\", None)\n-        proxies = kwargs.pop(\"proxies\", None)\n-        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n-        trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n-        from_pipeline = kwargs.pop(\"_from_pipeline\", None)\n-        from_auto_class = kwargs.pop(\"_from_auto\", False)\n-        _do_init = kwargs.pop(\"_do_init\", True)\n-        subfolder = kwargs.pop(\"subfolder\", \"\")\n-        commit_hash = kwargs.pop(\"_commit_hash\", None)\n-\n-        # Not relevant for Flax Models\n-        _ = kwargs.pop(\"adapter_kwargs\", None)\n-\n-        if use_auth_token is not None:\n-            warnings.warn(\n-                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-                FutureWarning,\n-            )\n-            if token is not None:\n-                raise ValueError(\n-                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n-                )\n-            token = use_auth_token\n-\n-        if trust_remote_code is True:\n-            logger.warning(\n-                \"The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is\"\n-                \" ignored.\"\n-            )\n-\n-        user_agent = {\"file_type\": \"model\", \"framework\": \"flax\", \"from_auto_class\": from_auto_class}\n-        if from_pipeline is not None:\n-            user_agent[\"using_pipeline\"] = from_pipeline\n-\n-        if is_offline_mode() and not local_files_only:\n-            logger.info(\"Offline mode: forcing local_files_only=True\")\n-            local_files_only = True\n-\n-        # Load config if we don't provide a configuration\n-        if not isinstance(config, PretrainedConfig):\n-            config_path = config if config is not None else pretrained_model_name_or_path\n-            config, model_kwargs = cls.config_class.from_pretrained(\n-                config_path,\n-                cache_dir=cache_dir,\n-                return_unused_kwargs=True,\n-                force_download=force_download,\n-                resume_download=resume_download,\n-                proxies=proxies,\n-                local_files_only=local_files_only,\n-                token=token,\n-                revision=revision,\n-                subfolder=subfolder,\n-                _from_auto=from_auto_class,\n-                _from_pipeline=from_pipeline,\n-                _commit_hash=commit_hash,\n-                **kwargs,\n-            )\n-        else:\n-            model_kwargs = kwargs.copy()\n-\n-        if commit_hash is None:\n-            commit_hash = getattr(config, \"_commit_hash\", None)\n-\n-        # Add the dtype to model_kwargs\n-        model_kwargs[\"dtype\"] = dtype\n-\n-        # This variable will flag if we're loading a sharded checkpoint. In this case the archive file is just the\n-        # index of the files.\n-        is_sharded = False\n-\n-        # Load model\n-        if pretrained_model_name_or_path is not None:\n-            pretrained_model_name_or_path = str(pretrained_model_name_or_path)\n-            is_local = os.path.isdir(pretrained_model_name_or_path)\n-            if is_local:\n-                if os.path.isfile(os.path.join(pretrained_model_name_or_path, subfolder, FLAX_WEIGHTS_NAME)):\n-                    # Load from a Flax checkpoint\n-                    archive_file = os.path.join(pretrained_model_name_or_path, subfolder, FLAX_WEIGHTS_NAME)\n-                elif os.path.isfile(os.path.join(pretrained_model_name_or_path, subfolder, FLAX_WEIGHTS_INDEX_NAME)):\n-                    # Load from a sharded Flax checkpoint\n-                    archive_file = os.path.join(pretrained_model_name_or_path, subfolder, FLAX_WEIGHTS_INDEX_NAME)\n-                    is_sharded = True\n-                elif is_safetensors_available() and os.path.isfile(\n-                    os.path.join(pretrained_model_name_or_path, subfolder, SAFE_WEIGHTS_NAME)\n-                ):\n-                    # Load from a safetensors checkpoint\n-                    archive_file = os.path.join(pretrained_model_name_or_path, subfolder, SAFE_WEIGHTS_NAME)\n-                elif is_safetensors_available() and os.path.isfile(\n-                    os.path.join(pretrained_model_name_or_path, SAFE_WEIGHTS_NAME)\n-                ):\n-                    # Load from a safetensors checkpoint\n-                    archive_file = os.path.join(pretrained_model_name_or_path, SAFE_WEIGHTS_NAME)\n-                elif from_pt and os.path.isfile(os.path.join(pretrained_model_name_or_path, subfolder, WEIGHTS_NAME)):\n-                    # Load from a PyTorch checkpoint\n-                    archive_file = os.path.join(pretrained_model_name_or_path, subfolder, WEIGHTS_NAME)\n-                elif from_pt and os.path.isfile(\n-                    os.path.join(pretrained_model_name_or_path, subfolder, WEIGHTS_INDEX_NAME)\n-                ):\n-                    # Load from a sharded pytorch checkpoint\n-                    archive_file = os.path.join(pretrained_model_name_or_path, subfolder, WEIGHTS_INDEX_NAME)\n-                    is_sharded = True\n-                # At this stage we don't have a weight file so we will raise an error.\n-                elif is_safetensors_available() and os.path.isfile(\n-                    os.path.join(pretrained_model_name_or_path, SAFE_WEIGHTS_INDEX_NAME)\n-                ):\n-                    # Load from a sharded safetensors checkpoint\n-                    archive_file = os.path.join(pretrained_model_name_or_path, SAFE_WEIGHTS_INDEX_NAME)\n-                    is_sharded = True\n-                    raise NotImplementedError(\"Support for sharded checkpoints using safetensors is coming soon!\")\n-                elif os.path.isfile(os.path.join(pretrained_model_name_or_path, subfolder, WEIGHTS_NAME)):\n-                    raise OSError(\n-                        f\"Error no file named {FLAX_WEIGHTS_NAME} found in directory {pretrained_model_name_or_path} \"\n-                        \"but there is a file for PyTorch weights. Use `from_pt=True` to load this model from those \"\n-                        \"weights.\"\n-                    )\n-                else:\n-                    raise OSError(\n-                        f\"Error no file named {FLAX_WEIGHTS_NAME} or {WEIGHTS_NAME} found in directory \"\n-                        f\"{pretrained_model_name_or_path}.\"\n-                    )\n-            elif os.path.isfile(os.path.join(subfolder, pretrained_model_name_or_path)):\n-                archive_file = pretrained_model_name_or_path\n-                is_local = True\n-            elif is_remote_url(pretrained_model_name_or_path):\n-                filename = pretrained_model_name_or_path\n-                resolved_archive_file = download_url(pretrained_model_name_or_path)\n-            else:\n-                if from_pt:\n-                    filename = WEIGHTS_NAME\n-                else:\n-                    filename = FLAX_WEIGHTS_NAME\n-\n-                try:\n-                    # Load from URL or cache if already cached\n-                    cached_file_kwargs = {\n-                        \"cache_dir\": cache_dir,\n-                        \"force_download\": force_download,\n-                        \"proxies\": proxies,\n-                        \"resume_download\": resume_download,\n-                        \"local_files_only\": local_files_only,\n-                        \"token\": token,\n-                        \"user_agent\": user_agent,\n-                        \"revision\": revision,\n-                        \"subfolder\": subfolder,\n-                        \"_raise_exceptions_for_gated_repo\": False,\n-                        \"_raise_exceptions_for_missing_entries\": False,\n-                        \"_commit_hash\": commit_hash,\n-                    }\n-                    resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)\n-\n-                    # Maybe the checkpoint is sharded, we try to grab the index name in this case.\n-                    if resolved_archive_file is None and filename == FLAX_WEIGHTS_NAME:\n-                        resolved_archive_file = cached_file(\n-                            pretrained_model_name_or_path, FLAX_WEIGHTS_INDEX_NAME, **cached_file_kwargs\n-                        )\n-                        if resolved_archive_file is not None:\n-                            is_sharded = True\n-\n-                    # Maybe the checkpoint is pytorch sharded, we try to grab the pytorch index name in this case.\n-                    if resolved_archive_file is None and from_pt:\n-                        resolved_archive_file = cached_file(\n-                            pretrained_model_name_or_path, WEIGHTS_INDEX_NAME, **cached_file_kwargs\n-                        )\n-                        if resolved_archive_file is not None:\n-                            is_sharded = True\n-\n-                    # If we still haven't found anything, look for `safetensors`.\n-                    if resolved_archive_file is None:\n-                        # No support for sharded safetensors yet, so we'll raise an error if that's all we find.\n-                        filename = SAFE_WEIGHTS_NAME\n-                        resolved_archive_file = cached_file(\n-                            pretrained_model_name_or_path, SAFE_WEIGHTS_NAME, **cached_file_kwargs\n-                        )\n-\n-                    # Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\n-                    # result when internet is up, the repo and revision exist, but the file does not.\n-                    if resolved_archive_file is None:\n-                        # Otherwise, maybe there is a TF or Torch model file.  We try those to give a helpful error\n-                        # message.\n-                        has_file_kwargs = {\n-                            \"revision\": revision,\n-                            \"proxies\": proxies,\n-                            \"token\": token,\n-                            \"cache_dir\": cache_dir,\n-                            \"local_files_only\": local_files_only,\n-                        }\n-                        if has_file(pretrained_model_name_or_path, SAFE_WEIGHTS_INDEX_NAME, **has_file_kwargs):\n-                            is_sharded = True\n-                            raise NotImplementedError(\n-                                \"Support for sharded checkpoints using safetensors is coming soon!\"\n-                            )\n-                        elif has_file(pretrained_model_name_or_path, WEIGHTS_NAME, **has_file_kwargs):\n-                            raise OSError(\n-                                f\"{pretrained_model_name_or_path} does not appear to have a file named\"\n-                                f\" {FLAX_WEIGHTS_NAME} but there is a file for PyTorch weights. Use `from_pt=True` to\"\n-                                \" load this model from those weights.\"\n-                            )\n-                        elif has_file(pretrained_model_name_or_path, WEIGHTS_INDEX_NAME, **has_file_kwargs):\n-                            raise OSError(\n-                                f\"{pretrained_model_name_or_path} does not appear to have a file named\"\n-                                f\" {FLAX_WEIGHTS_INDEX_NAME} but there is a sharded file for PyTorch weights. Use\"\n-                                \" `from_pt=True` to load this model from those weights.\"\n-                            )\n-                        else:\n-                            raise OSError(\n-                                f\"{pretrained_model_name_or_path} does not appear to have a file named\"\n-                                f\" {FLAX_WEIGHTS_NAME} or {WEIGHTS_NAME}.\"\n-                            )\n-                except OSError:\n-                    # Raise any environment error raise by `cached_file`. It will have a helpful error message adapted\n-                    # to the original exception.\n-                    raise\n-                except Exception:\n-                    # For any other exception, we throw a generic error.\n-                    raise OSError(\n-                        f\"Can't load the model for '{pretrained_model_name_or_path}'. If you were trying to load it\"\n-                        \" from 'https://huggingface.co/models', make sure you don't have a local directory with the\"\n-                        f\" same name. Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a\"\n-                        f\" directory containing a file named {FLAX_WEIGHTS_NAME} or {WEIGHTS_NAME}.\"\n-                    )\n-\n-            if is_local:\n-                logger.info(f\"loading weights file {archive_file}\")\n-                resolved_archive_file = archive_file\n-                filename = resolved_archive_file.split(os.path.sep)[-1]\n-            else:\n-                logger.info(f\"loading weights file {filename} from cache at {resolved_archive_file}\")\n-        else:\n-            resolved_archive_file = None\n-\n-        # We'll need to download and cache each checkpoint shard if the checkpoint is sharded.\n-        if is_sharded:\n-            # resolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\n-            resolved_archive_file, _ = get_checkpoint_shard_files(\n-                pretrained_model_name_or_path,\n-                resolved_archive_file,\n-                cache_dir=cache_dir,\n-                force_download=force_download,\n-                proxies=proxies,\n-                resume_download=resume_download,\n-                local_files_only=local_files_only,\n-                token=token,\n-                user_agent=user_agent,\n-                revision=revision,\n-                subfolder=subfolder,\n-                _commit_hash=commit_hash,\n-            )\n-\n-        safetensors_from_pt = False\n-        if filename == SAFE_WEIGHTS_NAME:\n-            with safe_open(resolved_archive_file, framework=\"flax\") as f:\n-                safetensors_metadata = f.metadata()\n-            if safetensors_metadata is None or safetensors_metadata.get(\"format\") not in [\"pt\", \"tf\", \"flax\"]:\n-                raise OSError(\n-                    f\"The safetensors archive passed at {resolved_archive_file} does not contain the valid metadata.\"\n-                    \" Make sure you save your model with the `save_pretrained` method.\"\n-                )\n-            safetensors_from_pt = safetensors_metadata.get(\"format\") == \"pt\"\n-\n-        # init random models\n-        model = cls(config, *model_args, _do_init=_do_init, **model_kwargs)\n-\n-        if from_pt or safetensors_from_pt:\n-            state = load_pytorch_checkpoint_in_flax_state_dict(model, resolved_archive_file, is_sharded)\n-        else:\n-            if is_sharded:\n-                state = cls.load_flax_sharded_weights(resolved_archive_file)\n-            else:\n-                state = cls.load_flax_weights(resolved_archive_file)\n-            # make sure all arrays are stored as jnp.arrays\n-            # NOTE: This is to prevent a bug this will be fixed in Flax >= v0.3.4:\n-            # https://github.com/google/flax/issues/1261\n-            if _do_init:\n-                state = jax.tree_util.tree_map(jnp.array, state)\n-            else:\n-                # keep the params on CPU if we don't want to initialize\n-                state = jax.tree_util.tree_map(lambda x: jax.device_put(x, jax.local_devices(backend=\"cpu\")[0]), state)\n-\n-        if \"batch_stats\" in state:  # if flax model contains batch norm layers\n-            # if model is base model only use model_prefix key\n-            if (\n-                cls.base_model_prefix not in dict(model.params_shape_tree[\"params\"])\n-                and cls.base_model_prefix in state[\"params\"]\n-            ):\n-                state[\"params\"] = state[\"params\"][cls.base_model_prefix]\n-                state[\"batch_stats\"] = state[\"batch_stats\"][cls.base_model_prefix]\n-\n-            # if model is head model and we are loading weights from base model\n-            # we initialize new params dict with base_model_prefix\n-            if (\n-                cls.base_model_prefix in dict(model.params_shape_tree[\"params\"])\n-                and cls.base_model_prefix not in state[\"params\"]\n-            ):\n-                state = {\n-                    \"params\": {cls.base_model_prefix: state[\"params\"]},\n-                    \"batch_stats\": {cls.base_model_prefix: state[\"batch_stats\"]},\n-                }\n-\n-        else:\n-            # if model is base model only use model_prefix key\n-            if cls.base_model_prefix not in dict(model.params_shape_tree) and cls.base_model_prefix in state:\n-                state = state[cls.base_model_prefix]\n-\n-            # if model is head model and we are loading weights from base model\n-            # we initialize new params dict with base_model_prefix\n-            if cls.base_model_prefix in dict(model.params_shape_tree) and cls.base_model_prefix not in state:\n-                state = {cls.base_model_prefix: state}\n-\n-        # flatten dicts\n-        state = flatten_dict(state)\n-\n-        random_state = flatten_dict(unfreeze(model.params if _do_init else model.params_shape_tree))\n-\n-        missing_keys = model.required_params - set(state.keys())\n-        unexpected_keys = set(state.keys()) - model.required_params\n-\n-        # Disabling warning when porting pytorch weights to flax, flax does not uses num_batches_tracked\n-        for unexpected_key in unexpected_keys.copy():\n-            if \"num_batches_tracked\" in unexpected_key[-1]:\n-                unexpected_keys.remove(unexpected_key)\n-\n-        if missing_keys and not _do_init:\n-            logger.warning(\n-                f\"The checkpoint {pretrained_model_name_or_path} is missing required keys: {missing_keys}. \"\n-                \"Make sure to call model.init_weights to initialize the missing weights.\"\n-            )\n-            cls._missing_keys = missing_keys\n-\n-        # Mismatched keys contains tuples key/shape1/shape2 of weights in the checkpoint that have a shape not\n-        # matching the weights in the model.\n-        mismatched_keys = []\n-        for key in state:\n-            if key in random_state and state[key].shape != random_state[key].shape:\n-                if ignore_mismatched_sizes:\n-                    mismatched_keys.append((key, state[key].shape, random_state[key].shape))\n-                    state[key] = random_state[key]\n-                else:\n-                    raise ValueError(\n-                        f\"Trying to load the pretrained weight for {key} failed: checkpoint has shape \"\n-                        f\"{state[key].shape} which is incompatible with the model shape {random_state[key].shape}. \"\n-                        \"Using `ignore_mismatched_sizes=True` if you really want to load this checkpoint inside this \"\n-                        \"model.\"\n-                    )\n-\n-        # add missing keys as random parameters if we are initializing\n-        if missing_keys and _do_init:\n-            for missing_key in missing_keys:\n-                state[missing_key] = random_state[missing_key]\n-\n-        # remove unexpected keys to not be saved again\n-        for unexpected_key in unexpected_keys:\n-            del state[unexpected_key]\n-\n-        if len(unexpected_keys) > 0:\n-            logger.warning(\n-                f\"Some weights of the model checkpoint at {pretrained_model_name_or_path} were not used when\"\n-                f\" initializing {model.__class__.__name__}: {unexpected_keys}\\n- This IS expected if you are\"\n-                f\" initializing {model.__class__.__name__} from the checkpoint of a model trained on another task or\"\n-                \" with another architecture (e.g. initializing a BertForSequenceClassification model from a\"\n-                \" BertForPreTraining model).\\n- This IS NOT expected if you are initializing\"\n-                f\" {model.__class__.__name__} from the checkpoint of a model that you expect to be exactly identical\"\n-                \" (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\"\n-            )\n-        else:\n-            logger.info(f\"All model checkpoint weights were used when initializing {model.__class__.__name__}.\\n\")\n-\n-        if len(missing_keys) > 0:\n-            logger.warning(\n-                f\"Some weights of {model.__class__.__name__} were not initialized from the model checkpoint at\"\n-                f\" {pretrained_model_name_or_path} and are newly initialized: {missing_keys}\\nYou should probably\"\n-                \" TRAIN this model on a down-stream task to be able to use it for predictions and inference.\"\n-            )\n-        elif len(mismatched_keys) == 0:\n-            logger.info(\n-                f\"All the weights of {model.__class__.__name__} were initialized from the model checkpoint at\"\n-                f\" {pretrained_model_name_or_path}.\\nIf your task is similar to the task the model of the checkpoint\"\n-                f\" was trained on, you can already use {model.__class__.__name__} for predictions without further\"\n-                \" training.\"\n-            )\n-        if len(mismatched_keys) > 0:\n-            mismatched_warning = \"\\n\".join(\n-                [\n-                    f\"- {key}: found shape {shape1} in the checkpoint and {shape2} in the model instantiated\"\n-                    for key, shape1, shape2 in mismatched_keys\n-                ]\n-            )\n-            logger.warning(\n-                f\"Some weights of {model.__class__.__name__} were not initialized from the model checkpoint at\"\n-                f\" {pretrained_model_name_or_path} and are newly initialized because the shapes did not\"\n-                f\" match:\\n{mismatched_warning}\\nYou should probably TRAIN this model on a down-stream task to be able\"\n-                \" to use it for predictions and inference.\"\n-            )\n-\n-        # dictionary of key: dtypes for the model params\n-        param_dtypes = jax.tree_util.tree_map(lambda x: x.dtype, state)\n-        # extract keys of parameters not in jnp.float32\n-        fp16_params = [k for k in param_dtypes if param_dtypes[k] == jnp.float16]\n-        bf16_params = [k for k in param_dtypes if param_dtypes[k] == jnp.bfloat16]\n-\n-        # raise a warning if any of the parameters are not in jnp.float32\n-        if len(fp16_params) > 0:\n-            logger.warning(\n-                f\"Some of the weights of {model.__class__.__name__} were initialized in float16 precision from \"\n-                f\"the model checkpoint at {pretrained_model_name_or_path}:\\n{fp16_params}\\n\"\n-                \"You should probably UPCAST the model weights to float32 if this was not intended. \"\n-                \"See [`~FlaxPreTrainedModel.to_fp32`] for further information on how to do this.\"\n-            )\n-\n-        if len(bf16_params) > 0:\n-            logger.warning(\n-                f\"Some of the weights of {model.__class__.__name__} were initialized in bfloat16 precision from \"\n-                f\"the model checkpoint at {pretrained_model_name_or_path}:\\n{bf16_params}\\n\"\n-                \"You should probably UPCAST the model weights to float32 if this was not intended. \"\n-                \"See [`~FlaxPreTrainedModel.to_fp32`] for further information on how to do this.\"\n-            )\n-\n-        # If it is a model with generation capabilities, attempt to load the generation config\n-        if model.can_generate():\n-            try:\n-                model.generation_config = GenerationConfig.from_pretrained(\n-                    pretrained_model_name_or_path,\n-                    cache_dir=cache_dir,\n-                    force_download=force_download,\n-                    resume_download=resume_download,\n-                    proxies=proxies,\n-                    local_files_only=local_files_only,\n-                    token=token,\n-                    revision=revision,\n-                    subfolder=subfolder,\n-                    _from_auto=from_auto_class,\n-                    _from_pipeline=from_pipeline,\n-                    **kwargs,\n-                )\n-            except OSError:\n-                logger.info(\n-                    \"Generation config file not found, using a generation config created from the model config.\"\n-                )\n-                pass\n-\n-        if _do_init:\n-            # set correct parameters\n-            model.params = unflatten_dict(state)\n-            return model\n-        else:\n-            return model, unflatten_dict(state)\n-\n-    def save_pretrained(\n-        self,\n-        save_directory: Union[str, os.PathLike],\n-        params=None,\n-        push_to_hub=False,\n-        max_shard_size=\"10GB\",\n-        token: Optional[Union[str, bool]] = None,\n-        safe_serialization: bool = False,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Save a model and its configuration file to a directory, so that it can be re-loaded using the\n-        `[`~FlaxPreTrainedModel.from_pretrained`]` class method\n-\n-        Arguments:\n-            save_directory (`str` or `os.PathLike`):\n-                Directory to which to save. Will be created if it doesn't exist.\n-            push_to_hub (`bool`, *optional*, defaults to `False`):\n-                Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the\n-                repository you want to push to with `repo_id` (will default to the name of `save_directory` in your\n-                namespace).\n-            max_shard_size (`int` or `str`, *optional*, defaults to `\"10GB\"`):\n-                The maximum size for a checkpoint before being sharded. Checkpoints shard will then be each of size\n-                lower than this size. If expressed as a string, needs to be digits followed by a unit (like `\"5MB\"`).\n-\n-                <Tip warning={true}>\n-\n-                If a single weight of the model is bigger than `max_shard_size`, it will be in its own checkpoint shard\n-                which will be bigger than `max_shard_size`.\n-\n-                </Tip>\n-\n-            token (`str` or `bool`, *optional*):\n-                The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use\n-                the token generated when running `hf auth login` (stored in `~/.huggingface`).\n-            kwargs (`dict[str, Any]`, *optional*):\n-                Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n-            safe_serialization (`bool`, *optional*, defaults to `False`):\n-                Whether to save the model using `safetensors` or through msgpack.\n-        \"\"\"\n-        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n-\n-        if use_auth_token is not None:\n-            warnings.warn(\n-                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n-                FutureWarning,\n-            )\n-            if token is not None:\n-                raise ValueError(\n-                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n-                )\n-            token = use_auth_token\n-\n-        if token is not None:\n-            kwargs[\"token\"] = token\n-\n-        if os.path.isfile(save_directory):\n-            logger.error(f\"Provided path ({save_directory}) should be a directory, not a file\")\n-            return\n-\n-        os.makedirs(save_directory, exist_ok=True)\n-\n-        if push_to_hub:\n-            commit_message = kwargs.pop(\"commit_message\", None)\n-            repo_id = kwargs.pop(\"repo_id\", save_directory.split(os.path.sep)[-1])\n-            repo_id = self._create_repo(repo_id, **kwargs)\n-            files_timestamps = self._get_files_timestamps(save_directory)\n-\n-        # get abs dir\n-        save_directory = os.path.abspath(save_directory)\n-        # save config as well\n-        self.config.architectures = [self.__class__.__name__[4:]]\n-\n-        # If we have a custom model, we copy the file defining it in the folder and set the attributes so it can be\n-        # loaded from the Hub.\n-        if self._auto_class is not None:\n-            custom_object_save(self, save_directory, config=self.config)\n-\n-        self.config.save_pretrained(save_directory)\n-        if self.can_generate():\n-            self.generation_config.save_pretrained(save_directory)\n-\n-        # save model\n-        weights_name = SAFE_WEIGHTS_NAME if safe_serialization else FLAX_WEIGHTS_NAME\n-        output_model_file = os.path.join(save_directory, weights_name)\n-\n-        shards, index = flax_shard_checkpoint(params if params is not None else self.params, max_shard_size)\n-        # Clean the folder from a previous save\n-        for filename in os.listdir(save_directory):\n-            full_filename = os.path.join(save_directory, filename)\n-            weights_no_suffix = weights_name.replace(\".bin\", \"\").replace(\".safetensors\", \"\")\n-            if filename.startswith(weights_no_suffix) and os.path.isfile(full_filename) and filename not in shards:\n-                os.remove(full_filename)\n-\n-        if index is None:\n-            if safe_serialization:\n-                params = params if params is not None else self.params\n-                flat_dict = flatten_dict(params, sep=\".\")\n-                safe_save_file(flat_dict, output_model_file, metadata={\"format\": \"flax\"})\n-            else:\n-                with open(output_model_file, \"wb\") as f:\n-                    params = params if params is not None else self.params\n-                    model_bytes = to_bytes(params)\n-                    f.write(model_bytes)\n-\n-        else:\n-            save_index_file = os.path.join(save_directory, FLAX_WEIGHTS_INDEX_NAME)\n-            # Save the index as well\n-            with open(save_index_file, \"w\", encoding=\"utf-8\") as f:\n-                content = json.dumps(index, indent=2, sort_keys=True) + \"\\n\"\n-                f.write(content)\n-            logger.info(\n-                f\"The model is bigger than the maximum size per checkpoint ({max_shard_size}) and is going to be \"\n-                f\"split in {len(shards)} checkpoint shards. You can find where each parameters has been saved in the \"\n-                f\"index located at {save_index_file}.\"\n-            )\n-            for shard_file, shard in shards.items():\n-                # the shard item are unflattened, to save them we need to flatten them again\n-                with open(os.path.join(save_directory, shard_file), mode=\"wb\") as f:\n-                    params = unflatten_dict(shard, sep=\"/\")\n-                    shard_bytes = to_bytes(params)\n-                    f.write(shard_bytes)\n-\n-        logger.info(f\"Model weights saved in {output_model_file}\")\n-\n-        if push_to_hub:\n-            self._upload_modified_files(\n-                save_directory,\n-                repo_id,\n-                files_timestamps,\n-                commit_message=commit_message,\n-                token=token,\n-            )\n-\n-    @classmethod\n-    def register_for_auto_class(cls, auto_class=\"FlaxAutoModel\"):\n-        \"\"\"\n-        Register this class with a given auto class. This should only be used for custom models as the ones in the\n-        library are already mapped with an auto class.\n-\n-\n-\n-        Args:\n-            auto_class (`str` or `type`, *optional*, defaults to `\"FlaxAutoModel\"`):\n-                The auto class to register this new model with.\n-        \"\"\"\n-        if not isinstance(auto_class, str):\n-            auto_class = auto_class.__name__\n-\n-        import transformers.models.auto as auto_module\n-\n-        if not hasattr(auto_module, auto_class):\n-            raise ValueError(f\"{auto_class} is not a valid auto class.\")\n-\n-        cls._auto_class = auto_class\n-\n-\n-# To update the docstring, we need to copy the method, otherwise we change the original docstring.\n-FlaxPreTrainedModel.push_to_hub = copy_func(FlaxPreTrainedModel.push_to_hub)\n-if FlaxPreTrainedModel.push_to_hub.__doc__ is not None:\n-    FlaxPreTrainedModel.push_to_hub.__doc__ = FlaxPreTrainedModel.push_to_hub.__doc__.format(\n-        object=\"model\", object_class=\"FlaxAutoModel\", object_files=\"model checkpoint\"\n-    )\n-\n-\n-def overwrite_call_docstring(model_class, docstring):\n-    # copy __call__ function to be sure docstring is changed only for this function\n-    model_class.__call__ = copy_func(model_class.__call__)\n-    # delete existing docstring\n-    model_class.__call__.__doc__ = None\n-    # set correct docstring\n-    model_class.__call__ = add_start_docstrings_to_model_forward(docstring)(model_class.__call__)\n-\n-\n-def append_call_sample_docstring(\n-    model_class, checkpoint, output_type, config_class, mask=None, revision=None, real_checkpoint=None\n-):\n-    model_class.__call__ = copy_func(model_class.__call__)\n-    model_class.__call__ = add_code_sample_docstrings(\n-        checkpoint=checkpoint,\n-        output_type=output_type,\n-        config_class=config_class,\n-        model_cls=model_class.__name__,\n-        revision=revision,\n-        real_checkpoint=real_checkpoint,\n-    )(model_class.__call__)\n-\n-\n-def append_replace_return_docstrings(model_class, output_type, config_class):\n-    model_class.__call__ = copy_func(model_class.__call__)\n-    model_class.__call__ = replace_return_docstrings(\n-        output_type=output_type,\n-        config_class=config_class,\n-    )(model_class.__call__)"
        },
        {
            "sha": "c7491b67f9aebb93b95632a5a7db2fd85cf5b4c7",
            "filename": "src/transformers/modeling_tf_outputs.py",
            "status": "removed",
            "additions": 0,
            "deletions": 990,
            "changes": 990,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodeling_tf_outputs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodeling_tf_outputs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_tf_outputs.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,990 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-from __future__ import annotations\n-\n-import warnings\n-from dataclasses import dataclass\n-\n-import tensorflow as tf\n-\n-from .utils import ModelOutput\n-\n-\n-@dataclass\n-class TFBaseModelOutput(ModelOutput):\n-    \"\"\"\n-    Base class for model's outputs, with potential hidden states and attentions.\n-\n-    Args:\n-        last_hidden_state (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(tf.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-    \"\"\"\n-\n-    last_hidden_state: tf.Tensor | None = None\n-    hidden_states: tuple[tf.Tensor] | None = None\n-    attentions: tuple[tf.Tensor] | None = None\n-\n-\n-@dataclass\n-class TFBaseModelOutputWithNoAttention(ModelOutput):\n-    \"\"\"\n-    Base class for model's outputs, with potential hidden states.\n-\n-    Args:\n-        last_hidden_state (`tf.Tensor` shape `(batch_size, num_channels, height, width)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `tf.Tensor` (one for the output of the embeddings, if the model has an embedding layer, + one for\n-            the output of each layer) of shape `(batch_size, num_channels, height, width)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-    \"\"\"\n-\n-    last_hidden_state: tf.Tensor | None = None\n-    hidden_states: tuple[tf.Tensor, ...] | None = None\n-\n-\n-@dataclass\n-class TFBaseModelOutputWithPooling(ModelOutput):\n-    \"\"\"\n-    Base class for model's outputs that also contains a pooling of the last hidden states.\n-\n-    Args:\n-        last_hidden_state (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        pooler_output (`tf.Tensor` of shape `(batch_size, hidden_size)`):\n-            Last layer hidden-state of the first token of the sequence (classification token) further processed by a\n-            Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence\n-            prediction (classification) objective during pretraining.\n-\n-            This output is usually *not* a good summary of the semantic content of the input, you're often better with\n-            averaging or pooling the sequence of hidden-states for the whole input sequence.\n-        hidden_states (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-    \"\"\"\n-\n-    last_hidden_state: tf.Tensor | None = None\n-    pooler_output: tf.Tensor | None = None\n-    hidden_states: tuple[tf.Tensor] | None = None\n-    attentions: tuple[tf.Tensor] | None = None\n-\n-\n-@dataclass\n-class TFBaseModelOutputWithPoolingAndNoAttention(ModelOutput):\n-    \"\"\"\n-    Base class for model's outputs that also contains a pooling of the last hidden states.\n-\n-    Args:\n-        last_hidden_state (`tf.Tensor` of shape `(batch_size, num_channels, height, width)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        pooler_output (`tf.Tensor` of shape `(batch_size, hidden_size)`):\n-            Last layer hidden-state after a pooling operation on the spatial dimensions.\n-        hidden_states (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `tf.Tensor` (one for the output of the embeddings, if the model has an embedding layer, + one for\n-            the output of each layer) of shape `(batch_size, num_channels, height, width)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-    \"\"\"\n-\n-    last_hidden_state: tf.Tensor | None = None\n-    pooler_output: tf.Tensor | None = None\n-    hidden_states: tuple[tf.Tensor, ...] | None = None\n-\n-\n-@dataclass\n-class TFBaseModelOutputWithPoolingAndCrossAttentions(ModelOutput):\n-    \"\"\"\n-    Base class for model's outputs that also contains a pooling of the last hidden states.\n-\n-    Args:\n-        last_hidden_state (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        pooler_output (`tf.Tensor` of shape `(batch_size, hidden_size)`):\n-            Last layer hidden-state of the first token of the sequence (classification token) further processed by a\n-            Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence\n-            prediction (classification) objective during pretraining.\n-\n-            This output is usually *not* a good summary of the semantic content of the input, you're often better with\n-            averaging or pooling the sequence of hidden-states for the whole input sequence.\n-        past_key_values (`list[tf.Tensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            List of `tf.Tensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads,\n-            sequence_length, embed_size_per_head)`).\n-\n-            Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        cross_attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n-            weighted average in the cross-attention heads.\n-    \"\"\"\n-\n-    last_hidden_state: tf.Tensor | None = None\n-    pooler_output: tf.Tensor | None = None\n-    past_key_values: list[tf.Tensor] | None = None\n-    hidden_states: tuple[tf.Tensor] | None = None\n-    attentions: tuple[tf.Tensor] | None = None\n-    cross_attentions: tuple[tf.Tensor] | None = None\n-\n-\n-@dataclass\n-class TFBaseModelOutputWithPast(ModelOutput):\n-    \"\"\"\n-    Base class for model's outputs that may also contain a past key/values (to speed up sequential decoding).\n-\n-    Args:\n-        last_hidden_state (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-\n-            If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n-            hidden_size)` is output.\n-        past_key_values (`list[tf.Tensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            List of `tf.Tensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads,\n-            sequence_length, embed_size_per_head)`).\n-\n-            Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-    \"\"\"\n-\n-    last_hidden_state: tf.Tensor | None = None\n-    past_key_values: list[tf.Tensor] | None = None\n-    hidden_states: tuple[tf.Tensor] | None = None\n-    attentions: tuple[tf.Tensor] | None = None\n-\n-\n-@dataclass\n-class TFBaseModelOutputWithCrossAttentions(ModelOutput):\n-    \"\"\"\n-    Base class for model's outputs, with potential hidden states and attentions.\n-\n-    Args:\n-        last_hidden_state (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(tf.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        cross_attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n-            weighted average in the cross-attention heads.\n-    \"\"\"\n-\n-    last_hidden_state: tf.Tensor | None = None\n-    hidden_states: tuple[tf.Tensor] | None = None\n-    attentions: tuple[tf.Tensor] | None = None\n-    cross_attentions: tuple[tf.Tensor] | None = None\n-\n-\n-@dataclass\n-class TFBaseModelOutputWithPastAndCrossAttentions(ModelOutput):\n-    \"\"\"\n-    Base class for model's outputs that may also contain a past key/values (to speed up sequential decoding).\n-\n-    Args:\n-        last_hidden_state (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-\n-            If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n-            hidden_size)` is output.\n-        past_key_values (`list[tf.Tensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            List of `tf.Tensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads,\n-            sequence_length, embed_size_per_head)`).\n-\n-            Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(tf.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        cross_attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n-            weighted average in the cross-attention heads.\n-    \"\"\"\n-\n-    last_hidden_state: tf.Tensor | None = None\n-    past_key_values: list[tf.Tensor] | None = None\n-    hidden_states: tuple[tf.Tensor] | None = None\n-    attentions: tuple[tf.Tensor] | None = None\n-    cross_attentions: tuple[tf.Tensor] | None = None\n-\n-\n-@dataclass\n-class TFSeq2SeqModelOutput(ModelOutput):\n-    \"\"\"\n-    Base class for model encoder's outputs that also contains : pre-computed hidden states that can speed up sequential\n-    decoding.\n-\n-    Args:\n-        last_hidden_state (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the decoder of the model.\n-\n-            If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n-            hidden_size)` is output.\n-        past_key_values (`list[tf.Tensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            List of `tf.Tensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads,\n-            sequence_length, embed_size_per_head)`).\n-\n-            Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be\n-            used (see `past_key_values` input) to speed up sequential decoding.\n-        decoder_hidden_states (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.\n-        decoder_attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        cross_attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n-            weighted average in the cross-attention heads.\n-        encoder_last_hidden_state (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_hidden_states (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.\n-        encoder_attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-    \"\"\"\n-\n-    last_hidden_state: tf.Tensor | None = None\n-    past_key_values: list[tf.Tensor] | None = None\n-    decoder_hidden_states: tuple[tf.Tensor] | None = None\n-    decoder_attentions: tuple[tf.Tensor] | None = None\n-    cross_attentions: tuple[tf.Tensor] | None = None\n-    encoder_last_hidden_state: tf.Tensor | None = None\n-    encoder_hidden_states: tuple[tf.Tensor] | None = None\n-    encoder_attentions: tuple[tf.Tensor] | None = None\n-\n-\n-@dataclass\n-class TFCausalLMOutput(ModelOutput):\n-    \"\"\"\n-    Base class for causal language model (or autoregressive) outputs.\n-\n-    Args:\n-        loss (`tf.Tensor` of shape `(n,)`, *optional*, where n is the number of non-masked labels, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        hidden_states (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-    \"\"\"\n-\n-    loss: tf.Tensor | None = None\n-    logits: tf.Tensor | None = None\n-    hidden_states: tuple[tf.Tensor] | None = None\n-    attentions: tuple[tf.Tensor] | None = None\n-\n-\n-@dataclass\n-class TFCausalLMOutputWithPast(ModelOutput):\n-    \"\"\"\n-    Base class for causal language model (or autoregressive) outputs.\n-\n-    Args:\n-        loss (`tf.Tensor` of shape `(n,)`, *optional*, where n is the number of non-masked labels, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`list[tf.Tensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            List of `tf.Tensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads,\n-            sequence_length, embed_size_per_head)`).\n-\n-            Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-    \"\"\"\n-\n-    loss: tf.Tensor | None = None\n-    logits: tf.Tensor | None = None\n-    past_key_values: list[tf.Tensor] | None = None\n-    hidden_states: tuple[tf.Tensor] | None = None\n-    attentions: tuple[tf.Tensor] | None = None\n-\n-\n-@dataclass\n-class TFCausalLMOutputWithCrossAttentions(ModelOutput):\n-    \"\"\"\n-    Base class for causal language model (or autoregressive) outputs.\n-\n-    Args:\n-        loss (`tf.Tensor` of shape `(n,)`, *optional*, where n is the number of non-masked labels, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        hidden_states (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        cross_attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n-            weighted average in the cross-attention heads.\n-        past_key_values (`list[tf.Tensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            List of `tf.Tensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads,\n-            sequence_length, embed_size_per_head)`).\n-\n-            Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-    \"\"\"\n-\n-    loss: tf.Tensor | None = None\n-    logits: tf.Tensor | None = None\n-    past_key_values: list[tf.Tensor] | None = None\n-    hidden_states: tuple[tf.Tensor] | None = None\n-    attentions: tuple[tf.Tensor] | None = None\n-    cross_attentions: tuple[tf.Tensor] | None = None\n-\n-\n-@dataclass\n-class TFMaskedLMOutput(ModelOutput):\n-    \"\"\"\n-    Base class for masked language models outputs.\n-\n-    Args:\n-        loss (`tf.Tensor` of shape `(n,)`, *optional*, where n is the number of non-masked labels, returned when `labels` is provided):\n-            Masked language modeling (MLM) loss.\n-        logits (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        hidden_states (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-    \"\"\"\n-\n-    loss: tf.Tensor | None = None\n-    logits: tf.Tensor | None = None\n-    hidden_states: tuple[tf.Tensor] | None = None\n-    attentions: tuple[tf.Tensor] | None = None\n-\n-\n-@dataclass\n-class TFSeq2SeqLMOutput(ModelOutput):\n-    \"\"\"\n-    Base class for sequence-to-sequence language models outputs.\n-\n-    Args:\n-        loss (`tf.Tensor` of shape `(n,)`, *optional*, where n is the number of non-masked labels, returned when `labels` is provided):\n-            Language modeling loss.\n-        logits (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`list[tf.Tensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            List of `tf.Tensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads,\n-            sequence_length, embed_size_per_head)`).\n-\n-            Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be\n-            used (see `past_key_values` input) to speed up sequential decoding.\n-        decoder_hidden_states (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.\n-        decoder_attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        cross_attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n-            weighted average in the cross-attention heads.\n-        encoder_last_hidden_state (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_hidden_states (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.\n-        encoder_attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-    \"\"\"\n-\n-    loss: tf.Tensor | None = None\n-    logits: tf.Tensor | None = None\n-    past_key_values: list[tf.Tensor] | None = None\n-    decoder_hidden_states: tuple[tf.Tensor] | None = None\n-    decoder_attentions: tuple[tf.Tensor] | None = None\n-    cross_attentions: tuple[tf.Tensor] | None = None\n-    encoder_last_hidden_state: tf.Tensor | None = None\n-    encoder_hidden_states: tuple[tf.Tensor] | None = None\n-    encoder_attentions: tuple[tf.Tensor] | None = None\n-\n-\n-@dataclass\n-class TFNextSentencePredictorOutput(ModelOutput):\n-    \"\"\"\n-    Base class for outputs of models predicting if two sentences are consecutive or not.\n-\n-    Args:\n-        loss (`tf.Tensor` of shape `(n,)`, *optional*, where n is the number of non-masked labels, returned when `next_sentence_label` is provided):\n-            Next sentence prediction loss.\n-        logits (`tf.Tensor` of shape `(batch_size, 2)`):\n-            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n-            before SoftMax).\n-        hidden_states (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-    \"\"\"\n-\n-    loss: tf.Tensor | None = None\n-    logits: tf.Tensor | None = None\n-    hidden_states: tuple[tf.Tensor] | None = None\n-    attentions: tuple[tf.Tensor] | None = None\n-\n-\n-@dataclass\n-class TFSequenceClassifierOutput(ModelOutput):\n-    \"\"\"\n-    Base class for outputs of sentence classification models.\n-\n-    Args:\n-        loss (`tf.Tensor` of shape `(batch_size, )`, *optional*, returned when `labels` is provided):\n-            Classification (or regression if config.num_labels==1) loss.\n-        logits (`tf.Tensor` of shape `(batch_size, config.num_labels)`):\n-            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n-        hidden_states (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-    \"\"\"\n-\n-    loss: tf.Tensor | None = None\n-    logits: tf.Tensor | None = None\n-    hidden_states: tuple[tf.Tensor] | None = None\n-    attentions: tuple[tf.Tensor] | None = None\n-\n-\n-@dataclass\n-class TFSeq2SeqSequenceClassifierOutput(ModelOutput):\n-    \"\"\"\n-    Base class for outputs of sequence-to-sequence sentence classification models.\n-\n-    Args:\n-        loss (`tf.Tensor` of shape `(1,)`, *optional*, returned when `label` is provided):\n-            Classification (or regression if config.num_labels==1) loss.\n-        logits (`tf.Tensor` of shape `(batch_size, config.num_labels)`):\n-            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n-        past_key_values (`list[tf.Tensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            List of `tf.Tensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads,\n-            sequence_length, embed_size_per_head)`).\n-\n-            Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be\n-            used (see `past_key_values` input) to speed up sequential decoding.\n-        decoder_hidden_states (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.\n-        decoder_attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        cross_attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`\n-        encoder_last_hidden_state (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_hidden_states (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.\n-        encoder_attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-    \"\"\"\n-\n-    loss: tf.Tensor | None = None\n-    logits: tf.Tensor | None = None\n-    past_key_values: list[tf.Tensor] | None = None\n-    decoder_hidden_states: tuple[tf.Tensor] | None = None\n-    decoder_attentions: tuple[tf.Tensor] | None = None\n-    cross_attentions: tuple[tf.Tensor] | None = None\n-    encoder_last_hidden_state: tf.Tensor | None = None\n-    encoder_hidden_states: tuple[tf.Tensor] | None = None\n-    encoder_attentions: tuple[tf.Tensor] | None = None\n-\n-\n-@dataclass\n-class TFSemanticSegmenterOutput(ModelOutput):\n-    \"\"\"\n-    Base class for outputs of semantic segmentation models.\n-\n-    Args:\n-        loss (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Classification (or regression if config.num_labels==1) loss.\n-        logits (`tf.Tensor` of shape `(batch_size, config.num_labels, logits_height, logits_width)`):\n-            Classification scores for each pixel.\n-\n-            <Tip warning={true}>\n-\n-            The logits returned do not necessarily have the same size as the `pixel_values` passed as inputs. This is\n-            to avoid doing two interpolations and lose some quality when a user needs to resize the logits to the\n-            original image size as post-processing. You should always check your logits shape and resize as needed.\n-\n-            </Tip>\n-\n-        hidden_states (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `tf.Tensor` (one for the output of the embeddings, if the model has an embedding layer, + one for\n-            the output of each layer) of shape `(batch_size, patch_size, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, patch_size, sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-    \"\"\"\n-\n-    loss: tf.Tensor | None = None\n-    logits: tf.Tensor | None = None\n-    hidden_states: tuple[tf.Tensor] | None = None\n-    attentions: tuple[tf.Tensor] | None = None\n-\n-\n-@dataclass\n-class TFSemanticSegmenterOutputWithNoAttention(ModelOutput):\n-    \"\"\"\n-    Base class for outputs of semantic segmentation models that do not output attention scores.\n-\n-    Args:\n-        loss (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Classification (or regression if config.num_labels==1) loss.\n-        logits (`tf.Tensor` of shape `(batch_size, config.num_labels, logits_height, logits_width)`):\n-            Classification scores for each pixel.\n-\n-            <Tip warning={true}>\n-\n-            The logits returned do not necessarily have the same size as the `pixel_values` passed as inputs. This is\n-            to avoid doing two interpolations and lose some quality when a user needs to resize the logits to the\n-            original image size as post-processing. You should always check your logits shape and resize as needed.\n-\n-            </Tip>\n-\n-        hidden_states (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `tf.Tensor` (one for the output of the embeddings, if the model has an embedding layer, + one for\n-            the output of each layer) of shape `(batch_size, patch_size, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-    \"\"\"\n-\n-    loss: tf.Tensor | None = None\n-    logits: tf.Tensor | None = None\n-    hidden_states: tuple[tf.Tensor] | None = None\n-\n-\n-@dataclass\n-class TFImageClassifierOutput(ModelOutput):\n-    \"\"\"\n-    Base class for outputs of image classification models.\n-\n-    Args:\n-        loss (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Classification (or regression if config.num_labels==1) loss.\n-        logits (`tf.Tensor` of shape `(batch_size, config.num_labels)`):\n-            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n-        hidden_states (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `tf.Tensor` (one for the output of the embeddings, if the model has an embedding layer, + one for\n-            the output of each stage) of shape `(batch_size, sequence_length, hidden_size)`. Hidden-states (also called\n-            feature maps) of the model at the output of each stage.\n-        attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, patch_size, sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-    \"\"\"\n-\n-    loss: tf.Tensor | None = None\n-    logits: tf.Tensor | None = None\n-    hidden_states: tuple[tf.Tensor] | None = None\n-    attentions: tuple[tf.Tensor] | None = None\n-\n-\n-@dataclass\n-class TFMultipleChoiceModelOutput(ModelOutput):\n-    \"\"\"\n-    Base class for outputs of multiple choice models.\n-\n-    Args:\n-        loss (`tf.Tensor` of shape *(batch_size, )*, *optional*, returned when `labels` is provided):\n-            Classification loss.\n-        logits (`tf.Tensor` of shape `(batch_size, num_choices)`):\n-            *num_choices* is the second dimension of the input tensors. (see *input_ids* above).\n-\n-            Classification scores (before SoftMax).\n-        hidden_states (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-    \"\"\"\n-\n-    loss: tf.Tensor | None = None\n-    logits: tf.Tensor | None = None\n-    hidden_states: tuple[tf.Tensor] | None = None\n-    attentions: tuple[tf.Tensor] | None = None\n-\n-\n-@dataclass\n-class TFTokenClassifierOutput(ModelOutput):\n-    \"\"\"\n-    Base class for outputs of token classification models.\n-\n-    Args:\n-        loss (`tf.Tensor` of shape `(n,)`, *optional*, where n is the number of unmasked labels, returned when `labels` is provided) :\n-            Classification loss.\n-        logits (`tf.Tensor` of shape `(batch_size, sequence_length, config.num_labels)`):\n-            Classification scores (before SoftMax).\n-        hidden_states (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-    \"\"\"\n-\n-    loss: tf.Tensor | None = None\n-    logits: tf.Tensor | None = None\n-    hidden_states: tuple[tf.Tensor] | None = None\n-    attentions: tuple[tf.Tensor] | None = None\n-\n-\n-@dataclass\n-class TFQuestionAnsweringModelOutput(ModelOutput):\n-    \"\"\"\n-    Base class for outputs of question answering models.\n-\n-    Args:\n-        loss (`tf.Tensor` of shape `(batch_size, )`, *optional*, returned when `start_positions` and `end_positions` are provided):\n-            Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.\n-        start_logits (`tf.Tensor` of shape `(batch_size, sequence_length)`):\n-            Span-start scores (before SoftMax).\n-        end_logits (`tf.Tensor` of shape `(batch_size, sequence_length)`):\n-            Span-end scores (before SoftMax).\n-        hidden_states (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-    \"\"\"\n-\n-    loss: tf.Tensor | None = None\n-    start_logits: tf.Tensor | None = None\n-    end_logits: tf.Tensor | None = None\n-    hidden_states: tuple[tf.Tensor] | None = None\n-    attentions: tuple[tf.Tensor] | None = None\n-\n-\n-@dataclass\n-class TFSeq2SeqQuestionAnsweringModelOutput(ModelOutput):\n-    \"\"\"\n-    Base class for outputs of sequence-to-sequence question answering models.\n-\n-    Args:\n-        loss (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.\n-        start_logits (`tf.Tensor` of shape `(batch_size, sequence_length)`):\n-            Span-start scores (before SoftMax).\n-        end_logits (`tf.Tensor` of shape `(batch_size, sequence_length)`):\n-            Span-end scores (before SoftMax).\n-        past_key_values (`list[tf.Tensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            List of `tf.Tensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads,\n-            sequence_length, embed_size_per_head)`).\n-\n-            Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be\n-            used (see `past_key_values` input) to speed up sequential decoding.\n-        decoder_hidden_states (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.\n-        decoder_attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        encoder_last_hidden_state (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_hidden_states (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.\n-        encoder_attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-    \"\"\"\n-\n-    loss: tf.Tensor | None = None\n-    start_logits: tf.Tensor | None = None\n-    end_logits: tf.Tensor | None = None\n-    past_key_values: list[tf.Tensor] | None = None\n-    decoder_hidden_states: tuple[tf.Tensor] | None = None\n-    decoder_attentions: tuple[tf.Tensor] | None = None\n-    encoder_last_hidden_state: tf.Tensor | None = None\n-    encoder_hidden_states: tuple[tf.Tensor] | None = None\n-    encoder_attentions: tuple[tf.Tensor] | None = None\n-\n-\n-@dataclass\n-class TFSequenceClassifierOutputWithPast(ModelOutput):\n-    \"\"\"\n-    Base class for outputs of sentence classification models.\n-\n-    Args:\n-        loss (`tf.Tensor` of shape `(batch_size, )`, *optional*, returned when `labels` is provided):\n-            Classification (or regression if config.num_labels==1) loss.\n-        logits (`tf.Tensor` of shape `(batch_size, config.num_labels)`):\n-            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n-        past_key_values (`list[tf.Tensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            List of `tf.Tensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads,\n-            sequence_length, embed_size_per_head)`).\n-\n-            Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-    \"\"\"\n-\n-    loss: tf.Tensor | None = None\n-    logits: tf.Tensor | None = None\n-    past_key_values: list[tf.Tensor] | None = None\n-    hidden_states: tuple[tf.Tensor] | None = None\n-    attentions: tuple[tf.Tensor] | None = None\n-\n-\n-@dataclass\n-class TFImageClassifierOutputWithNoAttention(ModelOutput):\n-    \"\"\"\n-    Base class for outputs of image classification models.\n-\n-    Args:\n-        loss (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Classification (or regression if config.num_labels==1) loss.\n-        logits (`tf.Tensor` of shape `(batch_size, config.num_labels)`):\n-            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n-        hidden_states (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `tf.Tensor` (one for the output of the embeddings, if the model has an embedding layer, + one for\n-            the output of each stage) of shape `(batch_size, num_channels, height, width)`. Hidden-states (also called\n-            feature maps) of the model at the output of each stage.\n-    \"\"\"\n-\n-    loss: tf.Tensor | None = None\n-    logits: tf.Tensor | None = None\n-    hidden_states: tuple[tf.Tensor, ...] | None = None\n-\n-\n-@dataclass\n-class TFMaskedImageModelingOutput(ModelOutput):\n-    \"\"\"\n-    Base class for outputs of masked image completion / in-painting models.\n-\n-    Args:\n-        loss (`tf.Tensor` of shape `(1,)`, *optional*, returned when `bool_masked_pos` is provided):\n-            Reconstruction loss.\n-        reconstruction (`tf.Tensor` of shape `(batch_size, num_channels, height, width)`):\n-           Reconstructed / completed images.\n-        hidden_states (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when\n-        `config.output_hidden_states=True`):\n-            Tuple of `tf.Tensor` (one for the output of the embeddings, if the model has an embedding layer, + one for\n-            the output of each stage) of shape `(batch_size, sequence_length, hidden_size)`. Hidden-states (also called\n-            feature maps) of the model at the output of each stage.\n-        attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when\n-        `config.output_attentions=True`):\n-            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, patch_size, sequence_length)`.\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-    \"\"\"\n-\n-    loss: tf.Tensor | None = None\n-    reconstruction: tf.Tensor | None = None\n-    hidden_states: tuple[tf.Tensor] | None = None\n-    attentions: tuple[tf.Tensor] | None = None\n-\n-    @property\n-    def logits(self):\n-        warnings.warn(\n-            \"logits attribute is deprecated and will be removed in version 5 of Transformers.\"\n-            \" Please use the reconstruction attribute to retrieve the final output instead.\",\n-            FutureWarning,\n-        )\n-        return self.reconstruction"
        },
        {
            "sha": "8f688af7be36439311465d019de36c20c6aaae77",
            "filename": "src/transformers/modeling_tf_pytorch_utils.py",
            "status": "removed",
            "additions": 0,
            "deletions": 676,
            "changes": 676,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodeling_tf_pytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodeling_tf_pytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_tf_pytorch_utils.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,676 +0,0 @@\n-# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n-# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"PyTorch - TF 2.0 general utilities.\"\"\"\n-\n-import os\n-import re\n-\n-import numpy\n-\n-from .utils import (\n-    ExplicitEnum,\n-    check_torch_load_is_safe,\n-    expand_dims,\n-    is_numpy_array,\n-    is_safetensors_available,\n-    is_torch_tensor,\n-    logging,\n-    reshape,\n-    squeeze,\n-    tensor_size,\n-)\n-from .utils import transpose as transpose_func\n-\n-\n-if is_safetensors_available():\n-    from safetensors import safe_open\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-class TransposeType(ExplicitEnum):\n-    \"\"\"\n-    Possible ...\n-    \"\"\"\n-\n-    NO = \"no\"\n-    SIMPLE = \"simple\"\n-    CONV1D = \"conv1d\"\n-    CONV2D = \"conv2d\"\n-\n-\n-def convert_tf_weight_name_to_pt_weight_name(\n-    tf_name, start_prefix_to_remove=\"\", tf_weight_shape=None, name_scope=None\n-):\n-    \"\"\"\n-    Convert a TF 2.0 model variable name in a pytorch model weight name.\n-\n-    Conventions for TF2.0 scopes -> PyTorch attribute names conversions:\n-\n-        - '$1___$2' is replaced by $2 (can be used to duplicate or remove layers in TF2.0 vs PyTorch)\n-        - '_._' is replaced by a new level separation (can be used to convert TF2.0 lists in PyTorch nn.ModulesList)\n-\n-    return tuple with:\n-\n-        - pytorch model weight name\n-        - transpose: `TransposeType` member indicating whether and how TF2.0 and PyTorch weights matrices should be\n-          transposed with regards to each other\n-    \"\"\"\n-    if name_scope is not None:\n-        if not tf_name.startswith(name_scope) and \"final_logits_bias\" not in tf_name:\n-            raise ValueError(\n-                f\"Weight name {tf_name} does not start with name_scope {name_scope}. This is an internal error \"\n-                \"in Transformers, so (unless you were doing something really evil) please open an issue to report it!\"\n-            )\n-        tf_name = tf_name[len(name_scope) :]\n-        tf_name = tf_name.lstrip(\"/\")\n-    tf_name = tf_name.replace(\":0\", \"\")  # device ids\n-    if (len(tf_name) > 2048 and \"___\" in tf_name) or tf_name.count(\"___\") > 10:\n-        # ReDOS check\n-        raise ValueError(\"TF variable name is too long or contains too many ___ separators: \" + tf_name)\n-    tf_name = re.sub(\n-        r\"/[^/]*___([^/]*)/\", r\"/\\1/\", tf_name\n-    )  # '$1___$2' is replaced by $2 (can be used to duplicate or remove layers in TF2.0 vs PyTorch)\n-    tf_name = tf_name.replace(\n-        \"_._\", \"/\"\n-    )  # '_._' is replaced by a level separation (can be used to convert TF2.0 lists in PyTorch nn.ModulesList)\n-    tf_name = re.sub(r\"//+\", \"/\", tf_name)  # Remove empty levels at the end\n-    tf_name = tf_name.split(\"/\")  # Convert from TF2.0 '/' separators to PyTorch '.' separators\n-    # Some weights have a single name without \"/\" such as final_logits_bias in BART\n-    if len(tf_name) > 1:\n-        tf_name = tf_name[1:]  # Remove level zero\n-\n-    tf_weight_shape = list(tf_weight_shape)\n-\n-    # When should we transpose the weights\n-    if tf_name[-1] == \"kernel\" and tf_weight_shape is not None and len(tf_weight_shape) == 4:\n-        transpose = TransposeType.CONV2D\n-    elif tf_name[-1] == \"kernel\" and tf_weight_shape is not None and len(tf_weight_shape) == 3:\n-        transpose = TransposeType.CONV1D\n-    elif bool(\n-        tf_name[-1] in [\"kernel\", \"pointwise_kernel\", \"depthwise_kernel\"]\n-        or \"emb_projs\" in tf_name\n-        or \"out_projs\" in tf_name\n-    ):\n-        transpose = TransposeType.SIMPLE\n-    else:\n-        transpose = TransposeType.NO\n-\n-    # Convert standard TF2.0 names in PyTorch names\n-    if tf_name[-1] == \"kernel\" or tf_name[-1] == \"embeddings\" or tf_name[-1] == \"gamma\":\n-        tf_name[-1] = \"weight\"\n-    if tf_name[-1] == \"beta\":\n-        tf_name[-1] = \"bias\"\n-\n-    # The SeparableConv1D TF layer contains two weights that are translated to PyTorch Conv1D here\n-    if tf_name[-1] == \"pointwise_kernel\" or tf_name[-1] == \"depthwise_kernel\":\n-        tf_name[-1] = tf_name[-1].replace(\"_kernel\", \".weight\")\n-\n-    # Remove prefix if needed\n-    tf_name = \".\".join(tf_name)\n-    if start_prefix_to_remove:\n-        tf_name = tf_name.replace(start_prefix_to_remove, \"\", 1)\n-\n-    return tf_name, transpose\n-\n-\n-def apply_transpose(transpose: TransposeType, weight, match_shape=None, pt_to_tf=True):\n-    \"\"\"\n-    Apply a transpose to some weight then tries to reshape the weight to the same shape as a given shape, all in a\n-    framework agnostic way.\n-    \"\"\"\n-    if transpose is TransposeType.CONV2D:\n-        # Conv2D weight:\n-        #    PT: (num_out_channel, num_in_channel, kernel[0], kernel[1])\n-        # -> TF: (kernel[0], kernel[1], num_in_channel, num_out_channel)\n-        axes = (2, 3, 1, 0) if pt_to_tf else (3, 2, 0, 1)\n-        weight = transpose_func(weight, axes=axes)\n-    elif transpose is TransposeType.CONV1D:\n-        # Conv1D weight:\n-        #    PT: (num_out_channel, num_in_channel, kernel)\n-        # -> TF: (kernel, num_in_channel, num_out_channel)\n-        weight = transpose_func(weight, axes=(2, 1, 0))\n-    elif transpose is TransposeType.SIMPLE:\n-        weight = transpose_func(weight)\n-\n-    if match_shape is None:\n-        return weight\n-\n-    if len(match_shape) < len(weight.shape):\n-        weight = squeeze(weight)\n-    elif len(match_shape) > len(weight.shape):\n-        weight = expand_dims(weight, axis=0)\n-\n-    if list(match_shape) != list(weight.shape):\n-        try:\n-            weight = reshape(weight, match_shape)\n-        except AssertionError as e:\n-            e.args += (match_shape, match_shape)\n-            raise e\n-\n-    return weight\n-\n-\n-#####################\n-# PyTorch => TF 2.0 #\n-#####################\n-\n-\n-def load_pytorch_checkpoint_in_tf2_model(\n-    tf_model,\n-    pytorch_checkpoint_path,\n-    tf_inputs=None,\n-    allow_missing_keys=False,\n-    output_loading_info=False,\n-    _prefix=None,\n-    tf_to_pt_weight_rename=None,\n-):\n-    \"\"\"Load pytorch checkpoints in a TF 2.0 model\"\"\"\n-    try:\n-        import tensorflow as tf  # noqa: F401\n-        import torch  # noqa: F401\n-        from safetensors.torch import load_file as safe_load_file  # noqa: F401\n-    except ImportError:\n-        logger.error(\n-            \"Loading a PyTorch model in TensorFlow, requires both PyTorch and TensorFlow to be installed. Please see \"\n-            \"https://pytorch.org/ and https://www.tensorflow.org/install/ for installation instructions.\"\n-        )\n-        raise\n-\n-    # Treats a single file as a collection of shards with 1 shard.\n-    if isinstance(pytorch_checkpoint_path, str):\n-        pytorch_checkpoint_path = [pytorch_checkpoint_path]\n-\n-    # Loads all shards into a single state dictionary\n-    pt_state_dict = {}\n-    for path in pytorch_checkpoint_path:\n-        pt_path = os.path.abspath(path)\n-        logger.info(f\"Loading PyTorch weights from {pt_path}\")\n-        if pt_path.endswith(\".safetensors\"):\n-            state_dict = safe_load_file(pt_path)\n-        else:\n-            check_torch_load_is_safe()\n-            state_dict = torch.load(pt_path, map_location=\"cpu\", weights_only=True)\n-\n-        pt_state_dict.update(state_dict)\n-\n-    logger.info(f\"PyTorch checkpoint contains {sum(t.numel() for t in pt_state_dict.values()):,} parameters\")\n-\n-    return load_pytorch_weights_in_tf2_model(\n-        tf_model,\n-        pt_state_dict,\n-        tf_inputs=tf_inputs,\n-        allow_missing_keys=allow_missing_keys,\n-        output_loading_info=output_loading_info,\n-        _prefix=_prefix,\n-        tf_to_pt_weight_rename=tf_to_pt_weight_rename,\n-    )\n-\n-\n-def load_pytorch_model_in_tf2_model(tf_model, pt_model, tf_inputs=None, allow_missing_keys=False):\n-    \"\"\"Load pytorch checkpoints in a TF 2.0 model\"\"\"\n-    pt_state_dict = pt_model.state_dict()\n-\n-    return load_pytorch_weights_in_tf2_model(\n-        tf_model, pt_state_dict, tf_inputs=tf_inputs, allow_missing_keys=allow_missing_keys\n-    )\n-\n-\n-def load_pytorch_weights_in_tf2_model(\n-    tf_model,\n-    pt_state_dict,\n-    tf_inputs=None,\n-    allow_missing_keys=False,\n-    output_loading_info=False,\n-    _prefix=None,\n-    tf_to_pt_weight_rename=None,\n-):\n-    \"\"\"Load pytorch state_dict in a TF 2.0 model.\"\"\"\n-    try:\n-        import tensorflow as tf  # noqa: F401\n-        import torch  # noqa: F401\n-    except ImportError:\n-        logger.error(\n-            \"Loading a PyTorch model in TensorFlow, requires both PyTorch and TensorFlow to be installed. Please see \"\n-            \"https://pytorch.org/ and https://www.tensorflow.org/install/ for installation instructions.\"\n-        )\n-        raise\n-\n-    # Numpy doesn't understand bfloat16, so upcast to a dtype that doesn't lose precision\n-    pt_state_dict = {\n-        k: v.numpy() if v.dtype != torch.bfloat16 else v.float().numpy() for k, v in pt_state_dict.items()\n-    }\n-    return load_pytorch_state_dict_in_tf2_model(\n-        tf_model,\n-        pt_state_dict,\n-        tf_inputs=tf_inputs,\n-        allow_missing_keys=allow_missing_keys,\n-        output_loading_info=output_loading_info,\n-        _prefix=_prefix,\n-        tf_to_pt_weight_rename=tf_to_pt_weight_rename,\n-    )\n-\n-\n-def _log_key_warnings(missing_keys, unexpected_keys, mismatched_keys, class_name):\n-    if len(unexpected_keys) > 0:\n-        logger.warning(\n-            \"Some weights of the PyTorch model were not used when initializing the TF 2.0 model\"\n-            f\" {class_name}: {unexpected_keys}\\n- This IS expected if you are initializing\"\n-            f\" {class_name} from a PyTorch model trained on another task or with another architecture\"\n-            \" (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\\n- This IS\"\n-            f\" NOT expected if you are initializing {class_name} from a PyTorch model that you expect\"\n-            \" to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a\"\n-            \" BertForSequenceClassification model).\"\n-        )\n-    else:\n-        logger.warning(f\"All PyTorch model weights were used when initializing {class_name}.\\n\")\n-    if len(missing_keys) > 0:\n-        logger.warning(\n-            f\"Some weights or buffers of the TF 2.0 model {class_name} were not initialized from the\"\n-            f\" PyTorch model and are newly initialized: {missing_keys}\\nYou should probably TRAIN this model on a\"\n-            \" down-stream task to be able to use it for predictions and inference.\"\n-        )\n-    else:\n-        logger.warning(\n-            f\"All the weights of {class_name} were initialized from the PyTorch model.\\n\"\n-            \"If your task is similar to the task the model of the checkpoint was trained on, \"\n-            f\"you can already use {class_name} for predictions without further training.\"\n-        )\n-\n-    if len(mismatched_keys) > 0:\n-        mismatched_warning = \"\\n\".join(\n-            [\n-                f\"- {key}: found shape {shape1} in the checkpoint and {shape2} in the model instantiated\"\n-                for key, shape1, shape2 in mismatched_keys\n-            ]\n-        )\n-        logger.warning(\n-            f\"Some weights of {class_name} were not initialized from the model checkpoint\"\n-            f\" are newly initialized because the shapes did not\"\n-            f\" match:\\n{mismatched_warning}\\nYou should probably TRAIN this model on a down-stream task to be able\"\n-            \" to use it for predictions and inference.\"\n-        )\n-\n-\n-def load_pytorch_state_dict_in_tf2_model(\n-    tf_model,\n-    pt_state_dict,\n-    tf_inputs=None,\n-    allow_missing_keys=False,\n-    output_loading_info=False,\n-    _prefix=None,\n-    tf_to_pt_weight_rename=None,\n-    ignore_mismatched_sizes=False,\n-    skip_logger_warnings=False,\n-):\n-    \"\"\"Load a pytorch state_dict in a TF 2.0 model. pt_state_dict can be either an actual dict or a lazy-loading\n-    safetensors archive created with the safe_open() function.\"\"\"\n-    import tensorflow as tf\n-\n-    if tf_inputs is None:\n-        tf_inputs = tf_model.dummy_inputs\n-\n-    if _prefix is None:\n-        _prefix = \"\"\n-    if tf_inputs:\n-        with tf.name_scope(_prefix):\n-            tf_model(tf_inputs, training=False)  # Make sure model is built\n-    # Convert old format to new format if needed from a PyTorch state_dict\n-    tf_keys_to_pt_keys = {}\n-    for key in pt_state_dict:\n-        new_key = None\n-        if \"gamma\" in key:\n-            new_key = key.replace(\"gamma\", \"weight\")\n-        if \"beta\" in key:\n-            new_key = key.replace(\"beta\", \"bias\")\n-        if \"running_var\" in key:\n-            new_key = key.replace(\"running_var\", \"moving_variance\")\n-        if \"running_mean\" in key:\n-            new_key = key.replace(\"running_mean\", \"moving_mean\")\n-\n-        # New `weight_norm` from https://github.com/huggingface/transformers/pull/24030\n-        key_components = key.split(\".\")\n-        name = None\n-        if key_components[-3::2] == [\"parametrizations\", \"original0\"]:\n-            name = key_components[-2] + \"_g\"\n-        elif key_components[-3::2] == [\"parametrizations\", \"original1\"]:\n-            name = key_components[-2] + \"_v\"\n-        if name is not None:\n-            key_components = key_components[:-3] + [name]\n-            new_key = \".\".join(key_components)\n-\n-        if new_key is None:\n-            new_key = key\n-        tf_keys_to_pt_keys[new_key] = key\n-\n-    # Matt: All TF models store the actual model stem in a MainLayer class, including the base model.\n-    # In PT, the derived models (with heads) use the base model class as the stem instead,\n-    # and there is no MainLayer class. This means that TF base classes have one\n-    # extra layer in their weight names, corresponding to the MainLayer class. This code block compensates for that.\n-    start_prefix_to_remove = \"\"\n-    if not any(s.startswith(tf_model.base_model_prefix) for s in tf_keys_to_pt_keys):\n-        start_prefix_to_remove = tf_model.base_model_prefix + \".\"\n-\n-    symbolic_weights = tf_model.trainable_weights + tf_model.non_trainable_weights\n-    tf_loaded_numel = 0\n-    all_pytorch_weights = set(tf_keys_to_pt_keys.keys())\n-    missing_keys = []\n-    mismatched_keys = []\n-    is_safetensor_archive = hasattr(pt_state_dict, \"get_tensor\")\n-    for symbolic_weight in symbolic_weights:\n-        sw_name = symbolic_weight.name\n-        name, transpose = convert_tf_weight_name_to_pt_weight_name(\n-            sw_name,\n-            start_prefix_to_remove=start_prefix_to_remove,\n-            tf_weight_shape=symbolic_weight.shape,\n-            name_scope=_prefix,\n-        )\n-        if tf_to_pt_weight_rename is not None:\n-            aliases = tf_to_pt_weight_rename(name)  # Is a tuple to account for possible name aliasing\n-            for alias in aliases:  # The aliases are in priority order, take the first one that matches\n-                if alias in tf_keys_to_pt_keys:\n-                    name = alias\n-                    break\n-            else:\n-                # If none of the aliases match, just use the first one (it'll be reported as missing)\n-                name = aliases[0]\n-\n-        # Find associated numpy array in pytorch model state dict\n-        if name not in tf_keys_to_pt_keys:\n-            if allow_missing_keys:\n-                missing_keys.append(name)\n-                continue\n-            elif tf_model._keys_to_ignore_on_load_missing is not None:\n-                # authorized missing keys don't have to be loaded\n-                if any(re.search(pat, name) is not None for pat in tf_model._keys_to_ignore_on_load_missing):\n-                    continue\n-            raise AttributeError(f\"{name} not found in PyTorch model\")\n-        state_dict_name = tf_keys_to_pt_keys[name]\n-        if is_safetensor_archive:\n-            array = pt_state_dict.get_tensor(state_dict_name)\n-        else:\n-            array = pt_state_dict[state_dict_name]\n-        try:\n-            array = apply_transpose(transpose, array, symbolic_weight.shape)\n-        except tf.errors.InvalidArgumentError as e:\n-            if not ignore_mismatched_sizes:\n-                error_msg = str(e)\n-                error_msg += (\n-                    \"\\n\\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.\"\n-                )\n-                raise tf.errors.InvalidArgumentError(error_msg)\n-            else:\n-                mismatched_keys.append((name, array.shape, symbolic_weight.shape))\n-                continue\n-\n-        tf_loaded_numel += tensor_size(array)\n-\n-        symbolic_weight.assign(tf.cast(array, symbolic_weight.dtype))\n-        del array  # Immediately free memory to keep peak usage as low as possible\n-        all_pytorch_weights.discard(name)\n-\n-    logger.info(f\"Loaded {tf_loaded_numel:,} parameters in the TF 2.0 model.\")\n-\n-    unexpected_keys = list(all_pytorch_weights)\n-\n-    if tf_model._keys_to_ignore_on_load_missing is not None:\n-        for pat in tf_model._keys_to_ignore_on_load_missing:\n-            missing_keys = [k for k in missing_keys if re.search(pat, k) is None]\n-    if tf_model._keys_to_ignore_on_load_unexpected is not None:\n-        for pat in tf_model._keys_to_ignore_on_load_unexpected:\n-            unexpected_keys = [k for k in unexpected_keys if re.search(pat, k) is None]\n-    if not skip_logger_warnings:\n-        _log_key_warnings(missing_keys, unexpected_keys, mismatched_keys, class_name=tf_model.__class__.__name__)\n-\n-    if output_loading_info:\n-        loading_info = {\n-            \"missing_keys\": missing_keys,\n-            \"unexpected_keys\": unexpected_keys,\n-            \"mismatched_keys\": mismatched_keys,\n-        }\n-        return tf_model, loading_info\n-\n-    return tf_model\n-\n-\n-def load_sharded_pytorch_safetensors_in_tf2_model(\n-    tf_model,\n-    safetensors_shards,\n-    tf_inputs=None,\n-    allow_missing_keys=False,\n-    output_loading_info=False,\n-    _prefix=None,\n-    tf_to_pt_weight_rename=None,\n-    ignore_mismatched_sizes=False,\n-):\n-    all_loading_infos = []\n-    for shard in safetensors_shards:\n-        with safe_open(shard, framework=\"tf\") as safetensors_archive:\n-            tf_model, loading_info = load_pytorch_state_dict_in_tf2_model(\n-                tf_model,\n-                safetensors_archive,\n-                tf_inputs=tf_inputs,\n-                allow_missing_keys=allow_missing_keys,\n-                output_loading_info=True,\n-                _prefix=_prefix,\n-                tf_to_pt_weight_rename=tf_to_pt_weight_rename,\n-                ignore_mismatched_sizes=ignore_mismatched_sizes,\n-                skip_logger_warnings=True,  # We will emit merged warnings at the end\n-            )\n-        all_loading_infos.append(loading_info)\n-    # Now we just need to merge the loading info\n-    # Keys are missing only if they're missing in *every* shard\n-    missing_keys = sorted(set.intersection(*[set(info[\"missing_keys\"]) for info in all_loading_infos]))\n-    # Keys are unexpected/mismatched if they're unexpected/mismatched in *any* shard\n-    unexpected_keys = sum([info[\"unexpected_keys\"] for info in all_loading_infos], [])\n-    mismatched_keys = sum([info[\"mismatched_keys\"] for info in all_loading_infos], [])\n-\n-    _log_key_warnings(missing_keys, unexpected_keys, mismatched_keys, class_name=tf_model.__class__.__name__)\n-\n-    if output_loading_info:\n-        loading_info = {\n-            \"missing_keys\": missing_keys,\n-            \"unexpected_keys\": unexpected_keys,\n-            \"mismatched_keys\": mismatched_keys,\n-        }\n-        return tf_model, loading_info\n-\n-    return tf_model\n-\n-\n-#####################\n-# TF 2.0 => PyTorch #\n-#####################\n-\n-\n-def load_tf2_checkpoint_in_pytorch_model(\n-    pt_model, tf_checkpoint_path, tf_inputs=None, allow_missing_keys=False, output_loading_info=False\n-):\n-    \"\"\"\n-    Load TF 2.0 HDF5 checkpoint in a PyTorch model We use HDF5 to easily do transfer learning (see\n-    https://github.com/tensorflow/tensorflow/blob/ee16fcac960ae660e0e4496658a366e2f745e1f0/tensorflow/python/keras/engine/network.py#L1352-L1357).\n-    \"\"\"\n-    try:\n-        import tensorflow as tf  # noqa: F401\n-        import torch  # noqa: F401\n-    except ImportError:\n-        logger.error(\n-            \"Loading a TensorFlow model in PyTorch, requires both PyTorch and TensorFlow to be installed. Please see \"\n-            \"https://pytorch.org/ and https://www.tensorflow.org/install/ for installation instructions.\"\n-        )\n-        raise\n-\n-    import transformers\n-\n-    from .modeling_tf_utils import load_tf_weights\n-\n-    logger.info(f\"Loading TensorFlow weights from {tf_checkpoint_path}\")\n-\n-    # Instantiate and load the associated TF 2.0 model\n-    tf_model_class_name = \"TF\" + pt_model.__class__.__name__  # Add \"TF\" at the beginning\n-    tf_model_class = getattr(transformers, tf_model_class_name)\n-    tf_model = tf_model_class(pt_model.config)\n-\n-    if tf_inputs is None:\n-        tf_inputs = tf_model.dummy_inputs\n-\n-    if tf_inputs is not None:\n-        tf_model(tf_inputs, training=False)  # Make sure model is built\n-\n-    load_tf_weights(tf_model, tf_checkpoint_path)\n-\n-    return load_tf2_model_in_pytorch_model(\n-        pt_model, tf_model, allow_missing_keys=allow_missing_keys, output_loading_info=output_loading_info\n-    )\n-\n-\n-def load_tf2_model_in_pytorch_model(pt_model, tf_model, allow_missing_keys=False, output_loading_info=False):\n-    \"\"\"Load TF 2.0 model in a pytorch model\"\"\"\n-    weights = tf_model.weights\n-\n-    return load_tf2_weights_in_pytorch_model(\n-        pt_model, weights, allow_missing_keys=allow_missing_keys, output_loading_info=output_loading_info\n-    )\n-\n-\n-def load_tf2_weights_in_pytorch_model(pt_model, tf_weights, allow_missing_keys=False, output_loading_info=False):\n-    \"\"\"Load TF2.0 symbolic weights in a PyTorch model\"\"\"\n-    try:\n-        import tensorflow as tf  # noqa: F401\n-        import torch  # noqa: F401\n-    except ImportError:\n-        logger.error(\n-            \"Loading a TensorFlow model in PyTorch, requires both PyTorch and TensorFlow to be installed. Please see \"\n-            \"https://pytorch.org/ and https://www.tensorflow.org/install/ for installation instructions.\"\n-        )\n-        raise\n-\n-    tf_state_dict = {tf_weight.name: tf_weight.numpy() for tf_weight in tf_weights}\n-    return load_tf2_state_dict_in_pytorch_model(\n-        pt_model, tf_state_dict, allow_missing_keys=allow_missing_keys, output_loading_info=output_loading_info\n-    )\n-\n-\n-def load_tf2_state_dict_in_pytorch_model(pt_model, tf_state_dict, allow_missing_keys=False, output_loading_info=False):\n-    import torch\n-\n-    new_pt_params_dict = {}\n-    current_pt_params_dict = dict(pt_model.named_parameters())\n-\n-    # Make sure we are able to load PyTorch base models as well as derived models (with heads)\n-    # TF models always have a prefix, some of PyTorch models (base ones) don't\n-    start_prefix_to_remove = \"\"\n-    if not any(s.startswith(pt_model.base_model_prefix) for s in current_pt_params_dict):\n-        start_prefix_to_remove = pt_model.base_model_prefix + \".\"\n-\n-    # Build a map from potential PyTorch weight names to TF 2.0 Variables\n-    tf_weights_map = {}\n-    for name, tf_weight in tf_state_dict.items():\n-        pt_name, transpose = convert_tf_weight_name_to_pt_weight_name(\n-            name, start_prefix_to_remove=start_prefix_to_remove, tf_weight_shape=tf_weight.shape\n-        )\n-        tf_weights_map[pt_name] = (tf_weight, transpose)\n-\n-    all_tf_weights = set(tf_weights_map.keys())\n-    loaded_pt_weights_data_ptr = {}\n-    missing_keys_pt = []\n-    for pt_weight_name, pt_weight in current_pt_params_dict.items():\n-        # Handle PyTorch shared weight not duplicated in TF 2.0\n-        if pt_weight.data_ptr() in loaded_pt_weights_data_ptr and pt_weight.data_ptr() != 0:\n-            new_pt_params_dict[pt_weight_name] = loaded_pt_weights_data_ptr[pt_weight.data_ptr()]\n-            continue\n-\n-        pt_weight_name_to_check = pt_weight_name\n-        # New `weight_norm` from https://github.com/huggingface/transformers/pull/24030\n-        key_components = pt_weight_name.split(\".\")\n-        name = None\n-        if key_components[-3::2] == [\"parametrizations\", \"original0\"]:\n-            name = key_components[-2] + \"_g\"\n-        elif key_components[-3::2] == [\"parametrizations\", \"original1\"]:\n-            name = key_components[-2] + \"_v\"\n-        if name is not None:\n-            key_components = key_components[:-3] + [name]\n-            pt_weight_name_to_check = \".\".join(key_components)\n-\n-        # Find associated numpy array in pytorch model state dict\n-        if pt_weight_name_to_check not in tf_weights_map:\n-            if allow_missing_keys:\n-                missing_keys_pt.append(pt_weight_name)\n-                continue\n-\n-            raise AttributeError(f\"{pt_weight_name} not found in TF 2.0 model\")\n-\n-        array, transpose = tf_weights_map[pt_weight_name_to_check]\n-\n-        array = apply_transpose(transpose, array, pt_weight.shape, pt_to_tf=False)\n-\n-        if numpy.isscalar(array):\n-            array = numpy.array(array)\n-        if not is_torch_tensor(array) and not is_numpy_array(array):\n-            array = array.numpy()\n-        if is_numpy_array(array):\n-            # Convert to torch tensor\n-            array = torch.from_numpy(array)\n-\n-        new_pt_params_dict[pt_weight_name] = array\n-        loaded_pt_weights_data_ptr[pt_weight.data_ptr()] = array\n-        all_tf_weights.discard(pt_weight_name)\n-\n-    missing_keys, unexpected_keys = pt_model.load_state_dict(new_pt_params_dict, strict=False)\n-    missing_keys += missing_keys_pt\n-\n-    # Some models may have keys that are not in the state by design, removing them before needlessly warning\n-    # the user.\n-    if pt_model._keys_to_ignore_on_load_missing is not None:\n-        for pat in pt_model._keys_to_ignore_on_load_missing:\n-            missing_keys = [k for k in missing_keys if re.search(pat, k) is None]\n-\n-    if pt_model._keys_to_ignore_on_load_unexpected is not None:\n-        for pat in pt_model._keys_to_ignore_on_load_unexpected:\n-            unexpected_keys = [k for k in unexpected_keys if re.search(pat, k) is None]\n-\n-    if len(unexpected_keys) > 0:\n-        logger.warning(\n-            \"Some weights of the TF 2.0 model were not used when initializing the PyTorch model\"\n-            f\" {pt_model.__class__.__name__}: {unexpected_keys}\\n- This IS expected if you are initializing\"\n-            f\" {pt_model.__class__.__name__} from a TF 2.0 model trained on another task or with another architecture\"\n-            \" (e.g. initializing a BertForSequenceClassification model from a TFBertForPreTraining model).\\n- This IS\"\n-            f\" NOT expected if you are initializing {pt_model.__class__.__name__} from a TF 2.0 model that you expect\"\n-            \" to be exactly identical (e.g. initializing a BertForSequenceClassification model from a\"\n-            \" TFBertForSequenceClassification model).\"\n-        )\n-    else:\n-        logger.warning(f\"All TF 2.0 model weights were used when initializing {pt_model.__class__.__name__}.\\n\")\n-    if len(missing_keys) > 0:\n-        logger.warning(\n-            f\"Some weights of {pt_model.__class__.__name__} were not initialized from the TF 2.0 model and are newly\"\n-            f\" initialized: {missing_keys}\\nYou should probably TRAIN this model on a down-stream task to be able to\"\n-            \" use it for predictions and inference.\"\n-        )\n-    else:\n-        logger.warning(\n-            f\"All the weights of {pt_model.__class__.__name__} were initialized from the TF 2.0 model.\\n\"\n-            \"If your task is similar to the task the model of the checkpoint was trained on, \"\n-            f\"you can already use {pt_model.__class__.__name__} for predictions without further training.\"\n-        )\n-\n-    logger.info(f\"Weights or buffers not loaded from TF 2.0 model: {all_tf_weights}\")\n-\n-    if output_loading_info:\n-        loading_info = {\"missing_keys\": missing_keys, \"unexpected_keys\": unexpected_keys}\n-        return pt_model, loading_info\n-\n-    return pt_model"
        },
        {
            "sha": "c7bb80656d1b8596e9f3313b6f768c6d2251099d",
            "filename": "src/transformers/modeling_tf_utils.py",
            "status": "removed",
            "additions": 0,
            "deletions": 3529,
            "changes": 3529,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodeling_tf_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodeling_tf_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_tf_utils.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "31783d041fe44ecc90bb994b900dbd28a2e6927f",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 69,
            "deletions": 257,
            "changes": 326,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -79,11 +79,8 @@\n     ADAPTER_WEIGHTS_NAME,\n     CONFIG_NAME,\n     DUMMY_INPUTS,\n-    FLAX_WEIGHTS_NAME,\n     SAFE_WEIGHTS_INDEX_NAME,\n     SAFE_WEIGHTS_NAME,\n-    TF2_WEIGHTS_NAME,\n-    TF_WEIGHTS_NAME,\n     WEIGHTS_INDEX_NAME,\n     WEIGHTS_NAME,\n     ContextManagers,\n@@ -506,13 +503,6 @@ def load_state_dict(\n     # Use safetensors if possible\n     if checkpoint_file.endswith(\".safetensors\") and is_safetensors_available():\n         with safe_open(checkpoint_file, framework=\"pt\") as f:\n-            metadata = f.metadata()\n-\n-            if metadata is not None and metadata.get(\"format\") not in [\"pt\", \"tf\", \"flax\", \"mlx\"]:\n-                raise OSError(\n-                    f\"The safetensors archive passed at {checkpoint_file} does not contain the valid metadata. Make sure \"\n-                    \"you save your model with the `save_pretrained` method.\"\n-                )\n             state_dict = {}\n             for k in f.keys():\n                 if map_location == \"meta\":\n@@ -568,11 +558,7 @@ def load_state_dict(\n                         \"model. Make sure you have saved the model properly.\"\n                     ) from e\n         except (UnicodeDecodeError, ValueError):\n-            raise OSError(\n-                f\"Unable to load weights from pytorch checkpoint file for '{checkpoint_file}' \"\n-                f\"at '{checkpoint_file}'. \"\n-                \"If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.\"\n-            )\n+            raise OSError(f\"Unable to load weights from pytorch checkpoint file '{checkpoint_file}'.\")\n \n \n def set_initialized_submodules(model, state_dict_keys):\n@@ -1004,9 +990,7 @@ def _get_resolved_checkpoint_files(\n     subfolder: str,\n     variant: Optional[str],\n     gguf_file: Optional[str],\n-    from_tf: bool,\n-    from_flax: bool,\n-    use_safetensors: bool,\n+    use_safetensors: Optional[bool],\n     cache_dir: str,\n     force_download: bool,\n     proxies: Optional[dict[str, str]],\n@@ -1032,19 +1016,6 @@ def _get_resolved_checkpoint_files(\n                 # If the filename is explicitly defined, load this by default.\n                 archive_file = os.path.join(pretrained_model_name_or_path, subfolder, transformers_explicit_filename)\n                 is_sharded = transformers_explicit_filename.endswith(\".safetensors.index.json\")\n-            elif from_tf and os.path.isfile(\n-                os.path.join(pretrained_model_name_or_path, subfolder, TF_WEIGHTS_NAME + \".index\")\n-            ):\n-                # Load from a TF 1.0 checkpoint in priority if from_tf\n-                archive_file = os.path.join(pretrained_model_name_or_path, subfolder, TF_WEIGHTS_NAME + \".index\")\n-            elif from_tf and os.path.isfile(os.path.join(pretrained_model_name_or_path, subfolder, TF2_WEIGHTS_NAME)):\n-                # Load from a TF 2.0 checkpoint in priority if from_tf\n-                archive_file = os.path.join(pretrained_model_name_or_path, subfolder, TF2_WEIGHTS_NAME)\n-            elif from_flax and os.path.isfile(\n-                os.path.join(pretrained_model_name_or_path, subfolder, FLAX_WEIGHTS_NAME)\n-            ):\n-                # Load from a Flax checkpoint in priority if from_flax\n-                archive_file = os.path.join(pretrained_model_name_or_path, subfolder, FLAX_WEIGHTS_NAME)\n             elif use_safetensors is not False and os.path.isfile(\n                 os.path.join(pretrained_model_name_or_path, subfolder, _add_variant(SAFE_WEIGHTS_NAME, variant))\n             ):\n@@ -1075,46 +1046,19 @@ def _get_resolved_checkpoint_files(\n                     pretrained_model_name_or_path, subfolder, _add_variant(WEIGHTS_INDEX_NAME, variant)\n                 )\n                 is_sharded = True\n-            # At this stage we don't have a weight file so we will raise an error.\n-            elif not use_safetensors and (\n-                os.path.isfile(os.path.join(pretrained_model_name_or_path, subfolder, TF_WEIGHTS_NAME + \".index\"))\n-                or os.path.isfile(os.path.join(pretrained_model_name_or_path, subfolder, TF2_WEIGHTS_NAME))\n-            ):\n-                raise OSError(\n-                    f\"Error no file named {_add_variant(WEIGHTS_NAME, variant)} found in directory\"\n-                    f\" {pretrained_model_name_or_path} but there is a file for TensorFlow weights. Use\"\n-                    \" `from_tf=True` to load this model from those weights.\"\n-                )\n-            elif not use_safetensors and os.path.isfile(\n-                os.path.join(pretrained_model_name_or_path, subfolder, FLAX_WEIGHTS_NAME)\n-            ):\n-                raise OSError(\n-                    f\"Error no file named {_add_variant(WEIGHTS_NAME, variant)} found in directory\"\n-                    f\" {pretrained_model_name_or_path} but there is a file for Flax weights. Use `from_flax=True`\"\n-                    \" to load this model from those weights.\"\n-                )\n             elif use_safetensors:\n                 raise OSError(\n                     f\"Error no file named {_add_variant(SAFE_WEIGHTS_NAME, variant)} found in directory\"\n                     f\" {pretrained_model_name_or_path}.\"\n                 )\n             else:\n                 raise OSError(\n-                    f\"Error no file named {_add_variant(WEIGHTS_NAME, variant)}, {_add_variant(SAFE_WEIGHTS_NAME, variant)},\"\n-                    f\" {TF2_WEIGHTS_NAME}, {TF_WEIGHTS_NAME + '.index'} or {FLAX_WEIGHTS_NAME} found in directory\"\n-                    f\" {pretrained_model_name_or_path}.\"\n+                    f\"Error no file named {_add_variant(SAFE_WEIGHTS_NAME, variant)}, or {_add_variant(WEIGHTS_NAME, variant)},\"\n+                    f\" found in directory {pretrained_model_name_or_path}.\"\n                 )\n         elif os.path.isfile(os.path.join(subfolder, pretrained_model_name_or_path)):\n             archive_file = pretrained_model_name_or_path\n             is_local = True\n-        elif os.path.isfile(os.path.join(subfolder, pretrained_model_name_or_path + \".index\")):\n-            if not from_tf:\n-                raise ValueError(\n-                    f\"We found a TensorFlow checkpoint at {pretrained_model_name_or_path + '.index'}, please set \"\n-                    \"from_tf to True to load from this checkpoint.\"\n-                )\n-            archive_file = os.path.join(subfolder, pretrained_model_name_or_path + \".index\")\n-            is_local = True\n         elif is_remote_url(pretrained_model_name_or_path):\n             filename = pretrained_model_name_or_path\n             resolved_archive_file = download_url(pretrained_model_name_or_path)\n@@ -1123,10 +1067,6 @@ def _get_resolved_checkpoint_files(\n             if transformers_explicit_filename is not None:\n                 filename = transformers_explicit_filename\n                 is_sharded = transformers_explicit_filename.endswith(\".safetensors.index.json\")\n-            elif from_tf:\n-                filename = TF2_WEIGHTS_NAME\n-            elif from_flax:\n-                filename = FLAX_WEIGHTS_NAME\n             elif use_safetensors is not False:\n                 filename = _add_variant(SAFE_WEIGHTS_NAME, variant)\n             else:\n@@ -1223,28 +1163,15 @@ def _get_resolved_checkpoint_files(\n                                     name=\"Thread-auto_conversion\",\n                                 ).start()\n                     else:\n-                        # Otherwise, no PyTorch file was found, maybe there is a TF or Flax model file.\n-                        # We try those to give a helpful error message.\n+                        # Otherwise, no PyTorch file was found\n                         has_file_kwargs = {\n                             \"revision\": revision,\n                             \"proxies\": proxies,\n                             \"token\": token,\n                             \"cache_dir\": cache_dir,\n                             \"local_files_only\": local_files_only,\n                         }\n-                        if has_file(pretrained_model_name_or_path, TF2_WEIGHTS_NAME, **has_file_kwargs):\n-                            raise OSError(\n-                                f\"{pretrained_model_name_or_path} does not appear to have a file named\"\n-                                f\" {_add_variant(WEIGHTS_NAME, variant)} but there is a file for TensorFlow weights.\"\n-                                \" Use `from_tf=True` to load this model from those weights.\"\n-                            )\n-                        elif has_file(pretrained_model_name_or_path, FLAX_WEIGHTS_NAME, **has_file_kwargs):\n-                            raise OSError(\n-                                f\"{pretrained_model_name_or_path} does not appear to have a file named\"\n-                                f\" {_add_variant(WEIGHTS_NAME, variant)} but there is a file for Flax weights. Use\"\n-                                \" `from_flax=True` to load this model from those weights.\"\n-                            )\n-                        elif variant is not None and has_file(\n+                        if variant is not None and has_file(\n                             pretrained_model_name_or_path, WEIGHTS_NAME, **has_file_kwargs\n                         ):\n                             raise OSError(\n@@ -1255,8 +1182,7 @@ def _get_resolved_checkpoint_files(\n                         else:\n                             raise OSError(\n                                 f\"{pretrained_model_name_or_path} does not appear to have a file named\"\n-                                f\" {_add_variant(WEIGHTS_NAME, variant)}, {_add_variant(SAFE_WEIGHTS_NAME, variant)},\"\n-                                f\" {TF2_WEIGHTS_NAME}, {TF_WEIGHTS_NAME} or {FLAX_WEIGHTS_NAME}.\"\n+                                f\" {_add_variant(WEIGHTS_NAME, variant)} or {_add_variant(SAFE_WEIGHTS_NAME, variant)}.\"\n                             )\n \n             except OSError:\n@@ -1269,8 +1195,7 @@ def _get_resolved_checkpoint_files(\n                     f\"Can't load the model for '{pretrained_model_name_or_path}'. If you were trying to load it\"\n                     \" from 'https://huggingface.co/models', make sure you don't have a local directory with the\"\n                     f\" same name. Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a\"\n-                    f\" directory containing a file named {_add_variant(WEIGHTS_NAME, variant)},\"\n-                    f\" {TF2_WEIGHTS_NAME}, {TF_WEIGHTS_NAME} or {FLAX_WEIGHTS_NAME}.\"\n+                    f\" directory containing a file named {_add_variant(WEIGHTS_NAME, variant)}.\"\n                 ) from e\n \n         if is_local:\n@@ -1682,8 +1607,6 @@ def invert_attention_mask(self, encoder_attention_mask: Tensor) -> Tensor:\n         if encoder_attention_mask.dim() == 2:\n             encoder_extended_attention_mask = encoder_attention_mask[:, None, None, :]\n         # T5 has a mask that can compare sequence ids, we can simulate this here with this transposition\n-        # Cf. https://github.com/tensorflow/mesh/blob/8d2465e9bc93129b913b5ccc6a59aa97abd96ec6/mesh_tensorflow\n-        # /transformer/transformer_layers.py#L270\n         # encoder_extended_attention_mask = (encoder_extended_attention_mask ==\n         # encoder_extended_attention_mask.transpose(-1, -2))\n         encoder_extended_attention_mask = encoder_extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n@@ -2018,19 +1941,13 @@ class PreTrainedModel(nn.Module, EmbeddingAccessMixin, ModuleUtilsMixin, PushToH\n \n         - **config_class** ([`PretrainedConfig`]) -- A subclass of [`PretrainedConfig`] to use as configuration class\n           for this model architecture.\n-        - **load_tf_weights** (`Callable`) -- A python *method* for loading a TensorFlow checkpoint in a PyTorch model,\n-          taking as arguments:\n-\n-            - **model** ([`PreTrainedModel`]) -- An instance of the model on which to load the TensorFlow checkpoint.\n-            - **config** ([`PreTrainedConfig`]) -- An instance of the configuration associated to the model.\n-            - **path** (`str`) -- A path to the TensorFlow checkpoint.\n-\n         - **base_model_prefix** (`str`) -- A string indicating the attribute associated to the base model in derived\n           classes of the same architecture adding modules on top of the base model.\n         - **is_parallelizable** (`bool`) -- A flag indicating whether this model supports model parallelization.\n         - **main_input_name** (`str`) -- The name of the principal input to the model (often `input_ids` for NLP\n           models, `pixel_values` for vision models and `input_values` for speech models).\n-        - **can_record_outputs** (dict):\"\"\"\n+        - **can_record_outputs** (dict):\n+    \"\"\"\n \n     config_class = None\n     base_model_prefix = \"\"\n@@ -2155,13 +2072,6 @@ def dummy_inputs(self) -> dict[str, torch.Tensor]:\n         \"\"\"\n         return {\"input_ids\": torch.tensor(DUMMY_INPUTS)}\n \n-    @property\n-    def framework(self) -> str:\n-        \"\"\"\n-        :str: Identifies that this is a PyTorch model.\n-        \"\"\"\n-        return \"pt\"\n-\n     def __init_subclass__(cls, **kwargs):\n         super().__init_subclass__(**kwargs)\n         # For BC we keep the original `config_class` definition in case\n@@ -3800,9 +3710,6 @@ def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):\n         \"\"\"\n         Activates gradient checkpointing for the current model.\n \n-        Note that in other frameworks this feature can be referred to as \"activation checkpointing\" or \"checkpoint\n-        activations\".\n-\n         We pass the `__call__` method of the modules instead of `forward` because `__call__` attaches all the hooks of\n         the module. https://discuss.pytorch.org/t/any-different-between-model-input-and-model-forward-input/3690/2\n \n@@ -3863,9 +3770,6 @@ def _set_gradient_checkpointing(self, enable: bool = True, gradient_checkpointin\n     def gradient_checkpointing_disable(self):\n         \"\"\"\n         Deactivates gradient checkpointing for the current model.\n-\n-        Note that in other frameworks this feature can be referred to as \"activation checkpointing\" or \"checkpoint\n-        activations\".\n         \"\"\"\n         if self.supports_gradient_checkpointing:\n             # For old GC format (transformers < 4.35.0) for models that live on the Hub\n@@ -3887,9 +3791,6 @@ def gradient_checkpointing_disable(self):\n     def is_gradient_checkpointing(self) -> bool:\n         \"\"\"\n         Whether gradient checkpointing is activated for this model or not.\n-\n-        Note that in other frameworks this feature can be referred to as \"activation checkpointing\" or \"checkpoint\n-        activations\".\n         \"\"\"\n         return any(hasattr(m, \"gradient_checkpointing\") and m.gradient_checkpointing for m in self.modules())\n \n@@ -4543,13 +4444,6 @@ def from_pretrained(\n                     - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\n                     - A path to a *directory* containing model weights saved using\n                       [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\n-                    - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In\n-                      this case, `from_tf` should be set to `True` and a configuration object should be provided as\n-                      `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a\n-                      PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n-                    - A path or url to a model folder containing a *flax checkpoint file* in *.msgpack* format (e.g,\n-                      `./flax_model/` containing `flax_model.msgpack`). In this case, `from_flax` should be set to\n-                      `True`.\n                     - `None` if you are both providing the configuration and state dictionary (resp. with keyword\n                       arguments `config` and `state_dict`).\n             model_args (sequence of positional arguments, *optional*):\n@@ -4578,12 +4472,6 @@ def from_pretrained(\n             cache_dir (`Union[str, os.PathLike]`, *optional*):\n                 Path to a directory in which a downloaded pretrained model configuration should be cached if the\n                 standard cache should not be used.\n-            from_tf (`bool`, *optional*, defaults to `False`):\n-                Load the model weights from a TensorFlow checkpoint save file (see docstring of\n-                `pretrained_model_name_or_path` argument).\n-            from_flax (`bool`, *optional*, defaults to `False`):\n-                Load the model weights from a Flax checkpoint save file (see docstring of\n-                `pretrained_model_name_or_path` argument).\n             ignore_mismatched_sizes (`bool`, *optional*, defaults to `False`):\n                 Whether or not to raise an error if some of the weights from the checkpoint do not have the same size\n                 as the weights of the model (if for instance, you are instantiating a model with 10 labels from a\n@@ -4699,8 +4587,7 @@ def from_pretrained(\n                 In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can\n                 specify the folder name here.\n             variant (`str`, *optional*):\n-                If specified load weights from `variant` filename, *e.g.* pytorch_model.<variant>.bin. `variant` is\n-                ignored when using `from_tf` or `from_flax`.\n+                If specified load weights from `variant` filename, *e.g.* pytorch_model.<variant>.bin.\n             use_safetensors (`bool`, *optional*, defaults to `None`):\n                 Whether or not to use `safetensors` checkpoints. Defaults to `None`. If not specified and `safetensors`\n                 is not installed, it will be set to `False`.\n@@ -4744,16 +4631,9 @@ def from_pretrained(\n         >>> # Update configuration during loading.\n         >>> model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\", output_attentions=True)\n         >>> assert model.config.output_attentions == True\n-        >>> # Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).\n-        >>> config = BertConfig.from_json_file(\"./tf_model/my_tf_model_config.json\")\n-        >>> model = BertModel.from_pretrained(\"./tf_model/my_tf_checkpoint.ckpt.index\", from_tf=True, config=config)\n-        >>> # Loading from a Flax checkpoint file instead of a PyTorch model (slower)\n-        >>> model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\", from_flax=True)\n         ```\n         \"\"\"\n         state_dict = kwargs.pop(\"state_dict\", None)\n-        from_tf = kwargs.pop(\"from_tf\", False)\n-        from_flax = kwargs.pop(\"from_flax\", False)\n         proxies = kwargs.pop(\"proxies\", None)\n         output_loading_info = kwargs.pop(\"output_loading_info\", False)\n         use_auth_token = kwargs.pop(\"use_auth_token\", None)\n@@ -4796,8 +4676,10 @@ def from_pretrained(\n         # Not used anymore -- remove them from the kwargs\n         _ = kwargs.pop(\"resume_download\", None)\n         _ = kwargs.pop(\"mirror\", None)\n-        _ = kwargs.pop(\"_fast_init\", True)\n+        _ = kwargs.pop(\"_fast_init\", None)\n         _ = kwargs.pop(\"low_cpu_mem_usage\", None)\n+        _ = kwargs.pop(\"from_tf\", None)\n+        _ = kwargs.pop(\"from_flax\", None)\n \n         # For BC on torch_dtype argument\n         if torch_dtype is not None:\n@@ -4964,8 +4846,6 @@ def from_pretrained(\n                 \"Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\"\n             )\n \n-        from_pt = not (from_tf | from_flax)\n-\n         user_agent = {\"file_type\": \"model\", \"framework\": \"pytorch\", \"from_auto_class\": from_auto_class}\n         if from_pipeline is not None:\n             user_agent[\"using_pipeline\"] = from_pipeline\n@@ -5016,7 +4896,7 @@ def from_pretrained(\n                 )\n \n         hf_quantizer, config, dtype, device_map = get_hf_quantizer(\n-            config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent\n+            config, quantization_config, dtype, device_map, weights_only, user_agent\n         )\n \n         if gguf_file is not None and hf_quantizer is not None:\n@@ -5039,8 +4919,6 @@ def from_pretrained(\n             subfolder=subfolder,\n             variant=variant,\n             gguf_file=gguf_file,\n-            from_tf=from_tf,\n-            from_flax=from_flax,\n             use_safetensors=use_safetensors,\n             cache_dir=cache_dir,\n             force_download=force_download,\n@@ -5054,56 +4932,35 @@ def from_pretrained(\n             transformers_explicit_filename=transformers_explicit_filename,\n         )\n \n-        is_sharded = sharded_metadata is not None\n         is_quantized = hf_quantizer is not None\n         is_from_file = pretrained_model_name_or_path is not None or gguf_file is not None\n \n-        if (\n-            is_safetensors_available()\n-            and is_from_file\n-            and not is_sharded\n-            and checkpoint_files[0].endswith(\".safetensors\")\n-        ):\n+        # Just a helpful message in case we try to load safetensors files coming from old Transformers tf/flax classes\n+        if is_safetensors_available() and is_from_file and checkpoint_files[0].endswith(\".safetensors\"):\n             with safe_open(checkpoint_files[0], framework=\"pt\") as f:\n                 metadata = f.metadata()\n-\n-            if metadata is None:\n-                # Assume it's a pytorch checkpoint (introduced for timm checkpoints)\n-                pass\n-            elif metadata.get(\"format\") == \"pt\":\n-                pass\n-            elif metadata.get(\"format\") == \"tf\":\n-                from_tf = True\n-                logger.info(\"A TensorFlow safetensors file is being loaded in a PyTorch model.\")\n-            elif metadata.get(\"format\") == \"flax\":\n-                from_flax = True\n-                logger.info(\"A Flax safetensors file is being loaded in a PyTorch model.\")\n-            elif metadata.get(\"format\") == \"mlx\":\n-                # This is a mlx file, we assume weights are compatible with pt\n-                pass\n-            else:\n-                raise ValueError(\n-                    f\"Incompatible safetensors file. File metadata is not ['pt', 'tf', 'flax', 'mlx'] but {metadata.get('format')}\"\n+            if metadata is not None and metadata.get(\"format\") in [\"tf\", \"flax\"]:\n+                logger.warning(\n+                    \"The safetensors checkpoint found has format `tf` or `flax`. This mean that the keys will very\"\n+                    \"likely not match to the model you are trying to load, and will be newly initialized. If it's the case \"\n+                    \"another warning will be raised later. Consider converting your checkpoint to the correct format.\"\n                 )\n \n-        from_pt = not (from_tf | from_flax)\n+        if gguf_file:\n+            from .modeling_gguf_pytorch_utils import load_gguf_checkpoint\n \n-        if from_pt:\n-            if gguf_file:\n-                from .modeling_gguf_pytorch_utils import load_gguf_checkpoint\n-\n-                # we need a dummy model to get the state_dict - for this reason, we keep the state_dict as if it was\n-                # passed directly as a kwarg from now on\n-                with torch.device(\"meta\"):\n-                    dummy_model = cls(config)\n-                state_dict = load_gguf_checkpoint(checkpoint_files[0], return_tensors=True, model_to_load=dummy_model)[\n-                    \"tensors\"\n-                ]\n+            # we need a dummy model to get the state_dict - for this reason, we keep the state_dict as if it was\n+            # passed directly as a kwarg from now on\n+            with torch.device(\"meta\"):\n+                dummy_model = cls(config)\n+            state_dict = load_gguf_checkpoint(checkpoint_files[0], return_tensors=True, model_to_load=dummy_model)[\n+                \"tensors\"\n+            ]\n \n-            # Find the correct dtype based on current state\n-            config, dtype, dtype_orig = _get_dtype(\n-                cls, dtype, checkpoint_files, config, sharded_metadata, state_dict, weights_only\n-            )\n+        # Find the correct dtype based on current state\n+        config, dtype, dtype_orig = _get_dtype(\n+            cls, dtype, checkpoint_files, config, sharded_metadata, state_dict, weights_only\n+        )\n \n         config.name_or_path = pretrained_model_name_or_path\n         model_init_context = cls.get_init_context(is_quantized, _is_ds_init_called)\n@@ -5166,40 +5023,36 @@ def _assign_original_dtype(module):\n         if device_map is not None:\n             device_map = _get_device_map(model, device_map, max_memory, hf_quantizer, dtype, keep_in_fp32_regex)\n \n+        # restore default dtype\n+        if dtype_orig is not None:\n+            torch.set_default_dtype(dtype_orig)\n+\n         # Finalize model weight initialization\n-        if from_tf:\n-            model, loading_info = cls._load_from_tf(model, config, checkpoint_files)\n-        elif from_flax:\n-            model = cls._load_from_flax(model, checkpoint_files)\n-        elif from_pt:\n-            # restore default dtype\n-            if dtype_orig is not None:\n-                torch.set_default_dtype(dtype_orig)\n+        (\n+            model,\n+            missing_keys,\n+            unexpected_keys,\n+            mismatched_keys,\n+            offload_index,\n+            error_msgs,\n+        ) = cls._load_pretrained_model(\n+            model,\n+            state_dict,\n+            checkpoint_files,\n+            pretrained_model_name_or_path,\n+            ignore_mismatched_sizes=ignore_mismatched_sizes,\n+            sharded_metadata=sharded_metadata,\n+            device_map=device_map,\n+            disk_offload_folder=offload_folder,\n+            offload_state_dict=offload_state_dict,\n+            dtype=dtype,\n+            hf_quantizer=hf_quantizer,\n+            keep_in_fp32_regex=keep_in_fp32_regex,\n+            device_mesh=device_mesh,\n+            key_mapping=key_mapping,\n+            weights_only=weights_only,\n+        )\n \n-            (\n-                model,\n-                missing_keys,\n-                unexpected_keys,\n-                mismatched_keys,\n-                offload_index,\n-                error_msgs,\n-            ) = cls._load_pretrained_model(\n-                model,\n-                state_dict,\n-                checkpoint_files,\n-                pretrained_model_name_or_path,\n-                ignore_mismatched_sizes=ignore_mismatched_sizes,\n-                sharded_metadata=sharded_metadata,\n-                device_map=device_map,\n-                disk_offload_folder=offload_folder,\n-                offload_state_dict=offload_state_dict,\n-                dtype=dtype,\n-                hf_quantizer=hf_quantizer,\n-                keep_in_fp32_regex=keep_in_fp32_regex,\n-                device_mesh=device_mesh,\n-                key_mapping=key_mapping,\n-                weights_only=weights_only,\n-            )\n         # make sure token embedding weights are still tied if needed\n         model.tie_weights()\n \n@@ -5292,15 +5145,12 @@ def _assign_original_dtype(module):\n             )\n \n         if output_loading_info:\n-            if from_pt:\n-                loading_info = {\n-                    \"missing_keys\": missing_keys,\n-                    \"unexpected_keys\": unexpected_keys,\n-                    \"mismatched_keys\": mismatched_keys,\n-                    \"error_msgs\": error_msgs,\n-                }\n-            elif from_flax:\n-                loading_info = None\n+            loading_info = {\n+                \"missing_keys\": missing_keys,\n+                \"unexpected_keys\": unexpected_keys,\n+                \"mismatched_keys\": mismatched_keys,\n+                \"error_msgs\": error_msgs,\n+            }\n             return model, loading_info\n         return model\n \n@@ -5751,44 +5601,6 @@ def _load_pretrained_model(\n \n         return model, missing_keys, unexpected_keys, mismatched_keys, disk_offload_index, error_msgs\n \n-    @classmethod\n-    def _load_from_tf(cls, model, config, checkpoint_files):\n-        if checkpoint_files[0].endswith(\".index\"):\n-            # Load from a TensorFlow 1.X checkpoint - provided by original authors\n-            model = cls.load_tf_weights(model, config, checkpoint_files[0][:-6])  # Remove the '.index'\n-            loading_info = None\n-        else:\n-            # Load from our TensorFlow 2.0 checkpoints\n-            try:\n-                from .modeling_tf_pytorch_utils import load_tf2_checkpoint_in_pytorch_model\n-\n-                model, loading_info = load_tf2_checkpoint_in_pytorch_model(\n-                    model, checkpoint_files[0], allow_missing_keys=True, output_loading_info=True\n-                )\n-            except ImportError:\n-                logger.error(\n-                    \"Loading a TensorFlow model in PyTorch, requires both PyTorch and TensorFlow to be installed.\"\n-                    \" Please see https://pytorch.org/ and https://www.tensorflow.org/install/ for installation\"\n-                    \" instructions.\"\n-                )\n-                raise\n-        return model, loading_info\n-\n-    @classmethod\n-    def _load_from_flax(cls, model, checkpoint_files):\n-        try:\n-            from .modeling_flax_pytorch_utils import load_flax_checkpoint_in_pytorch_model\n-\n-            model = load_flax_checkpoint_in_pytorch_model(model, checkpoint_files[0])\n-        except ImportError:\n-            logger.error(\n-                \"Loading a Flax model in PyTorch, requires both PyTorch and Flax to be installed. Please see\"\n-                \" https://pytorch.org/ and https://flax.readthedocs.io/en/latest/installation.html for\"\n-                \" installation instructions.\"\n-            )\n-            raise\n-        return model\n-\n     def retrieve_modules_from_names(self, names, add_prefix=False, remove_prefix=False):\n         module_keys = {\".\".join(key.split(\".\")[:-1]) for key in names}\n "
        },
        {
            "sha": "ac2cf362ebf21825b099d3a1ac9f94a43f790dcf",
            "filename": "src/transformers/models/albert/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Falbert%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Falbert%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2F__init__.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -20,8 +20,6 @@\n if TYPE_CHECKING:\n     from .configuration_albert import *\n     from .modeling_albert import *\n-    from .modeling_flax_albert import *\n-    from .modeling_tf_albert import *\n     from .tokenization_albert import *\n     from .tokenization_albert_fast import *\n else:"
        },
        {
            "sha": "c3d1dc54022390e8141ac57165ec08568d21fc85",
            "filename": "src/transformers/models/albert/modeling_albert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 133,
            "changes": 133,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -15,7 +15,6 @@\n \"\"\"PyTorch ALBERT model.\"\"\"\n \n import math\n-import os\n from dataclasses import dataclass\n from typing import Optional, Union\n \n@@ -47,132 +46,6 @@\n logger = logging.get_logger(__name__)\n \n \n-def load_tf_weights_in_albert(model, config, tf_checkpoint_path):\n-    \"\"\"Load tf checkpoints in a pytorch model.\"\"\"\n-    try:\n-        import re\n-\n-        import numpy as np\n-        import tensorflow as tf\n-    except ImportError:\n-        logger.error(\n-            \"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"\n-            \"https://www.tensorflow.org/install/ for installation instructions.\"\n-        )\n-        raise\n-    tf_path = os.path.abspath(tf_checkpoint_path)\n-    logger.info(f\"Converting TensorFlow checkpoint from {tf_path}\")\n-    # Load weights from TF model\n-    init_vars = tf.train.list_variables(tf_path)\n-    names = []\n-    arrays = []\n-    for name, shape in init_vars:\n-        logger.info(f\"Loading TF weight {name} with shape {shape}\")\n-        array = tf.train.load_variable(tf_path, name)\n-        names.append(name)\n-        arrays.append(array)\n-\n-    for name, array in zip(names, arrays):\n-        print(name)\n-\n-    for name, array in zip(names, arrays):\n-        original_name = name\n-\n-        # If saved from the TF HUB module\n-        name = name.replace(\"module/\", \"\")\n-\n-        # Renaming and simplifying\n-        name = name.replace(\"ffn_1\", \"ffn\")\n-        name = name.replace(\"bert/\", \"albert/\")\n-        name = name.replace(\"attention_1\", \"attention\")\n-        name = name.replace(\"transform/\", \"\")\n-        name = name.replace(\"LayerNorm_1\", \"full_layer_layer_norm\")\n-        name = name.replace(\"LayerNorm\", \"attention/LayerNorm\")\n-        name = name.replace(\"transformer/\", \"\")\n-\n-        # The feed forward layer had an 'intermediate' step which has been abstracted away\n-        name = name.replace(\"intermediate/dense/\", \"\")\n-        name = name.replace(\"ffn/intermediate/output/dense/\", \"ffn_output/\")\n-\n-        # ALBERT attention was split between self and output which have been abstracted away\n-        name = name.replace(\"/output/\", \"/\")\n-        name = name.replace(\"/self/\", \"/\")\n-\n-        # The pooler is a linear layer\n-        name = name.replace(\"pooler/dense\", \"pooler\")\n-\n-        # The classifier was simplified to predictions from cls/predictions\n-        name = name.replace(\"cls/predictions\", \"predictions\")\n-        name = name.replace(\"predictions/attention\", \"predictions\")\n-\n-        # Naming was changed to be more explicit\n-        name = name.replace(\"embeddings/attention\", \"embeddings\")\n-        name = name.replace(\"inner_group_\", \"albert_layers/\")\n-        name = name.replace(\"group_\", \"albert_layer_groups/\")\n-\n-        # Classifier\n-        if len(name.split(\"/\")) == 1 and (\"output_bias\" in name or \"output_weights\" in name):\n-            name = \"classifier/\" + name\n-\n-        # No ALBERT model currently handles the next sentence prediction task\n-        if \"seq_relationship\" in name:\n-            name = name.replace(\"seq_relationship/output_\", \"sop_classifier/classifier/\")\n-            name = name.replace(\"weights\", \"weight\")\n-\n-        name = name.split(\"/\")\n-\n-        # Ignore the gradients applied by the LAMB/ADAM optimizers.\n-        if (\n-            \"adam_m\" in name\n-            or \"adam_v\" in name\n-            or \"AdamWeightDecayOptimizer\" in name\n-            or \"AdamWeightDecayOptimizer_1\" in name\n-            or \"global_step\" in name\n-        ):\n-            logger.info(f\"Skipping {'/'.join(name)}\")\n-            continue\n-\n-        pointer = model\n-        for m_name in name:\n-            if re.fullmatch(r\"[A-Za-z]+_\\d+\", m_name):\n-                scope_names = re.split(r\"_(\\d+)\", m_name)\n-            else:\n-                scope_names = [m_name]\n-\n-            if scope_names[0] == \"kernel\" or scope_names[0] == \"gamma\":\n-                pointer = getattr(pointer, \"weight\")\n-            elif scope_names[0] == \"output_bias\" or scope_names[0] == \"beta\":\n-                pointer = getattr(pointer, \"bias\")\n-            elif scope_names[0] == \"output_weights\":\n-                pointer = getattr(pointer, \"weight\")\n-            elif scope_names[0] == \"squad\":\n-                pointer = getattr(pointer, \"classifier\")\n-            else:\n-                try:\n-                    pointer = getattr(pointer, scope_names[0])\n-                except AttributeError:\n-                    logger.info(f\"Skipping {'/'.join(name)}\")\n-                    continue\n-            if len(scope_names) >= 2:\n-                num = int(scope_names[1])\n-                pointer = pointer[num]\n-\n-        if m_name[-11:] == \"_embeddings\":\n-            pointer = getattr(pointer, \"weight\")\n-        elif m_name == \"kernel\":\n-            array = np.transpose(array)\n-        try:\n-            if pointer.shape != array.shape:\n-                raise ValueError(f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\")\n-        except ValueError as e:\n-            e.args += (pointer.shape, array.shape)\n-            raise\n-        print(f\"Initialize PyTorch weight {name} from {original_name}\")\n-        pointer.data = torch.from_numpy(array)\n-\n-    return model\n-\n-\n class AlbertEmbeddings(nn.Module):\n     \"\"\"\n     Construct the embeddings from word, position and token_type embeddings.\n@@ -184,8 +57,6 @@ def __init__(self, config: AlbertConfig):\n         self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.embedding_size)\n         self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.embedding_size)\n \n-        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n-        # any TensorFlow checkpoint file\n         self.LayerNorm = nn.LayerNorm(config.embedding_size, eps=config.layer_norm_eps)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n \n@@ -546,15 +417,12 @@ def forward(\n @auto_docstring\n class AlbertPreTrainedModel(PreTrainedModel):\n     config: AlbertConfig\n-    load_tf_weights = load_tf_weights_in_albert\n     base_model_prefix = \"albert\"\n     _supports_sdpa = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n         if isinstance(module, nn.Linear):\n-            # Slightly different from the TF version which uses truncated_normal for initialization\n-            # cf https://github.com/pytorch/pytorch/pull/5617\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n@@ -1337,7 +1205,6 @@ def forward(\n \n \n __all__ = [\n-    \"load_tf_weights_in_albert\",\n     \"AlbertPreTrainedModel\",\n     \"AlbertModel\",\n     \"AlbertForPreTraining\","
        },
        {
            "sha": "f2f19cb27716fb3f8846ef88e870e3eb1188a4bf",
            "filename": "src/transformers/models/albert/modeling_flax_albert.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1132,
            "changes": 1132,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_flax_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_flax_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_flax_albert.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,1132 +0,0 @@\n-# coding=utf-8\n-# Copyright 2021 Google AI, Google Brain and the HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-from typing import Callable, Optional\n-\n-import flax\n-import flax.linen as nn\n-import jax\n-import jax.numpy as jnp\n-import numpy as np\n-from flax.core.frozen_dict import FrozenDict, freeze, unfreeze\n-from flax.linen.attention import dot_product_attention_weights\n-from flax.traverse_util import flatten_dict, unflatten_dict\n-from jax import lax\n-\n-from ...modeling_flax_outputs import (\n-    FlaxBaseModelOutput,\n-    FlaxBaseModelOutputWithPooling,\n-    FlaxMaskedLMOutput,\n-    FlaxMultipleChoiceModelOutput,\n-    FlaxQuestionAnsweringModelOutput,\n-    FlaxSequenceClassifierOutput,\n-    FlaxTokenClassifierOutput,\n-)\n-from ...modeling_flax_utils import (\n-    ACT2FN,\n-    FlaxPreTrainedModel,\n-    append_call_sample_docstring,\n-    append_replace_return_docstrings,\n-    overwrite_call_docstring,\n-)\n-from ...utils import ModelOutput, add_start_docstrings, add_start_docstrings_to_model_forward, logging\n-from .configuration_albert import AlbertConfig\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-_CHECKPOINT_FOR_DOC = \"albert/albert-base-v2\"\n-_CONFIG_FOR_DOC = \"AlbertConfig\"\n-\n-\n-@flax.struct.dataclass\n-class FlaxAlbertForPreTrainingOutput(ModelOutput):\n-    \"\"\"\n-    Output type of [`FlaxAlbertForPreTraining`].\n-\n-    Args:\n-        prediction_logits (`jnp.ndarray` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        sop_logits (`jnp.ndarray` of shape `(batch_size, 2)`):\n-            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n-            before SoftMax).\n-        hidden_states (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-    \"\"\"\n-\n-    prediction_logits: jnp.ndarray = None\n-    sop_logits: jnp.ndarray = None\n-    hidden_states: Optional[tuple[jnp.ndarray]] = None\n-    attentions: Optional[tuple[jnp.ndarray]] = None\n-\n-\n-ALBERT_START_DOCSTRING = r\"\"\"\n-\n-    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading, saving and converting weights from PyTorch models)\n-\n-    This model is also a\n-    [flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html) subclass. Use it as\n-    a regular Flax linen Module and refer to the Flax documentation for all matter related to general usage and\n-    behavior.\n-\n-    Finally, this model supports inherent JAX features such as:\n-\n-    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)\n-    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)\n-    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)\n-    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)\n-\n-    Parameters:\n-        config ([`AlbertConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.\n-        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):\n-            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and\n-            `jax.numpy.bfloat16` (on TPUs).\n-\n-            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If\n-            specified all the computation will be performed with the given `dtype`.\n-\n-            **Note that this only specifies the dtype of the computation and does not influence the dtype of model\n-            parameters.**\n-\n-            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and\n-            [`~FlaxPreTrainedModel.to_bf16`].\n-\"\"\"\n-\n-ALBERT_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`numpy.ndarray` of shape `({0})`):\n-            Indices of input sequence tokens in the vocabulary.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`numpy.ndarray` of shape `({0})`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        token_type_ids (`numpy.ndarray` of shape `({0})`, *optional*):\n-            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n-            1]`:\n-\n-            - 0 corresponds to a *sentence A* token,\n-            - 1 corresponds to a *sentence B* token.\n-\n-            [What are token type IDs?](../glossary#token-type-ids)\n-        position_ids (`numpy.ndarray` of shape `({0})`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\n-\"\"\"\n-\n-\n-class FlaxAlbertEmbeddings(nn.Module):\n-    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n-\n-    config: AlbertConfig\n-    dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n-\n-    def setup(self):\n-        self.word_embeddings = nn.Embed(\n-            self.config.vocab_size,\n-            self.config.embedding_size,\n-            embedding_init=jax.nn.initializers.normal(stddev=self.config.initializer_range),\n-        )\n-        self.position_embeddings = nn.Embed(\n-            self.config.max_position_embeddings,\n-            self.config.embedding_size,\n-            embedding_init=jax.nn.initializers.normal(stddev=self.config.initializer_range),\n-        )\n-        self.token_type_embeddings = nn.Embed(\n-            self.config.type_vocab_size,\n-            self.config.embedding_size,\n-            embedding_init=jax.nn.initializers.normal(stddev=self.config.initializer_range),\n-        )\n-        self.LayerNorm = nn.LayerNorm(epsilon=self.config.layer_norm_eps, dtype=self.dtype)\n-        self.dropout = nn.Dropout(rate=self.config.hidden_dropout_prob)\n-\n-    def __call__(self, input_ids, token_type_ids, position_ids, deterministic: bool = True):\n-        # Embed\n-        inputs_embeds = self.word_embeddings(input_ids.astype(\"i4\"))\n-        position_embeds = self.position_embeddings(position_ids.astype(\"i4\"))\n-        token_type_embeddings = self.token_type_embeddings(token_type_ids.astype(\"i4\"))\n-\n-        # Sum all embeddings\n-        hidden_states = inputs_embeds + token_type_embeddings + position_embeds\n-\n-        # Layer Norm\n-        hidden_states = self.LayerNorm(hidden_states)\n-        hidden_states = self.dropout(hidden_states, deterministic=deterministic)\n-        return hidden_states\n-\n-\n-class FlaxAlbertSelfAttention(nn.Module):\n-    config: AlbertConfig\n-    dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n-\n-    def setup(self):\n-        if self.config.hidden_size % self.config.num_attention_heads != 0:\n-            raise ValueError(\n-                \"`config.hidden_size`: {self.config.hidden_size} has to be a multiple of `config.num_attention_heads` \"\n-                \"                   : {self.config.num_attention_heads}\"\n-            )\n-\n-        self.query = nn.Dense(\n-            self.config.hidden_size,\n-            dtype=self.dtype,\n-            kernel_init=jax.nn.initializers.normal(self.config.initializer_range),\n-        )\n-        self.key = nn.Dense(\n-            self.config.hidden_size,\n-            dtype=self.dtype,\n-            kernel_init=jax.nn.initializers.normal(self.config.initializer_range),\n-        )\n-        self.value = nn.Dense(\n-            self.config.hidden_size,\n-            dtype=self.dtype,\n-            kernel_init=jax.nn.initializers.normal(self.config.initializer_range),\n-        )\n-        self.dense = nn.Dense(\n-            self.config.hidden_size,\n-            kernel_init=jax.nn.initializers.normal(self.config.initializer_range),\n-            dtype=self.dtype,\n-        )\n-        self.LayerNorm = nn.LayerNorm(epsilon=self.config.layer_norm_eps, dtype=self.dtype)\n-        self.dropout = nn.Dropout(rate=self.config.hidden_dropout_prob)\n-\n-    def __call__(self, hidden_states, attention_mask, deterministic=True, output_attentions: bool = False):\n-        head_dim = self.config.hidden_size // self.config.num_attention_heads\n-\n-        query_states = self.query(hidden_states).reshape(\n-            hidden_states.shape[:2] + (self.config.num_attention_heads, head_dim)\n-        )\n-        value_states = self.value(hidden_states).reshape(\n-            hidden_states.shape[:2] + (self.config.num_attention_heads, head_dim)\n-        )\n-        key_states = self.key(hidden_states).reshape(\n-            hidden_states.shape[:2] + (self.config.num_attention_heads, head_dim)\n-        )\n-\n-        # Convert the boolean attention mask to an attention bias.\n-        if attention_mask is not None:\n-            # attention mask in the form of attention bias\n-            attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n-            attention_bias = lax.select(\n-                attention_mask > 0,\n-                jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n-                jnp.full(attention_mask.shape, jnp.finfo(self.dtype).min).astype(self.dtype),\n-            )\n-        else:\n-            attention_bias = None\n-\n-        dropout_rng = None\n-        if not deterministic and self.config.attention_probs_dropout_prob > 0.0:\n-            dropout_rng = self.make_rng(\"dropout\")\n-\n-        attn_weights = dot_product_attention_weights(\n-            query_states,\n-            key_states,\n-            bias=attention_bias,\n-            dropout_rng=dropout_rng,\n-            dropout_rate=self.config.attention_probs_dropout_prob,\n-            broadcast_dropout=True,\n-            deterministic=deterministic,\n-            dtype=self.dtype,\n-            precision=None,\n-        )\n-\n-        attn_output = jnp.einsum(\"...hqk,...khd->...qhd\", attn_weights, value_states)\n-        attn_output = attn_output.reshape(attn_output.shape[:2] + (-1,))\n-\n-        projected_attn_output = self.dense(attn_output)\n-        projected_attn_output = self.dropout(projected_attn_output, deterministic=deterministic)\n-        layernormed_attn_output = self.LayerNorm(projected_attn_output + hidden_states)\n-        outputs = (layernormed_attn_output, attn_weights) if output_attentions else (layernormed_attn_output,)\n-        return outputs\n-\n-\n-class FlaxAlbertLayer(nn.Module):\n-    config: AlbertConfig\n-    dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n-\n-    def setup(self):\n-        self.attention = FlaxAlbertSelfAttention(self.config, dtype=self.dtype)\n-        self.ffn = nn.Dense(\n-            self.config.intermediate_size,\n-            kernel_init=jax.nn.initializers.normal(self.config.initializer_range),\n-            dtype=self.dtype,\n-        )\n-        self.activation = ACT2FN[self.config.hidden_act]\n-        self.ffn_output = nn.Dense(\n-            self.config.hidden_size,\n-            kernel_init=jax.nn.initializers.normal(self.config.initializer_range),\n-            dtype=self.dtype,\n-        )\n-        self.full_layer_layer_norm = nn.LayerNorm(epsilon=self.config.layer_norm_eps, dtype=self.dtype)\n-        self.dropout = nn.Dropout(rate=self.config.hidden_dropout_prob)\n-\n-    def __call__(\n-        self,\n-        hidden_states,\n-        attention_mask,\n-        deterministic: bool = True,\n-        output_attentions: bool = False,\n-    ):\n-        attention_outputs = self.attention(\n-            hidden_states, attention_mask, deterministic=deterministic, output_attentions=output_attentions\n-        )\n-        attention_output = attention_outputs[0]\n-        ffn_output = self.ffn(attention_output)\n-        ffn_output = self.activation(ffn_output)\n-        ffn_output = self.ffn_output(ffn_output)\n-        ffn_output = self.dropout(ffn_output, deterministic=deterministic)\n-        hidden_states = self.full_layer_layer_norm(ffn_output + attention_output)\n-\n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attention_outputs[1],)\n-        return outputs\n-\n-\n-class FlaxAlbertLayerCollection(nn.Module):\n-    config: AlbertConfig\n-    dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n-\n-    def setup(self):\n-        self.layers = [\n-            FlaxAlbertLayer(self.config, name=str(i), dtype=self.dtype) for i in range(self.config.inner_group_num)\n-        ]\n-\n-    def __call__(\n-        self,\n-        hidden_states,\n-        attention_mask,\n-        deterministic: bool = True,\n-        output_attentions: bool = False,\n-        output_hidden_states: bool = False,\n-    ):\n-        layer_hidden_states = ()\n-        layer_attentions = ()\n-\n-        for layer_index, albert_layer in enumerate(self.layers):\n-            layer_output = albert_layer(\n-                hidden_states,\n-                attention_mask,\n-                deterministic=deterministic,\n-                output_attentions=output_attentions,\n-            )\n-            hidden_states = layer_output[0]\n-\n-            if output_attentions:\n-                layer_attentions = layer_attentions + (layer_output[1],)\n-\n-            if output_hidden_states:\n-                layer_hidden_states = layer_hidden_states + (hidden_states,)\n-\n-        outputs = (hidden_states,)\n-        if output_hidden_states:\n-            outputs = outputs + (layer_hidden_states,)\n-        if output_attentions:\n-            outputs = outputs + (layer_attentions,)\n-        return outputs  # last-layer hidden state, (layer hidden states), (layer attentions)\n-\n-\n-class FlaxAlbertLayerCollections(nn.Module):\n-    config: AlbertConfig\n-    dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n-    layer_index: Optional[str] = None\n-\n-    def setup(self):\n-        self.albert_layers = FlaxAlbertLayerCollection(self.config, dtype=self.dtype)\n-\n-    def __call__(\n-        self,\n-        hidden_states,\n-        attention_mask,\n-        deterministic: bool = True,\n-        output_attentions: bool = False,\n-        output_hidden_states: bool = False,\n-    ):\n-        outputs = self.albert_layers(\n-            hidden_states,\n-            attention_mask,\n-            deterministic=deterministic,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-        )\n-        return outputs\n-\n-\n-class FlaxAlbertLayerGroups(nn.Module):\n-    config: AlbertConfig\n-    dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n-\n-    def setup(self):\n-        self.layers = [\n-            FlaxAlbertLayerCollections(self.config, name=str(i), layer_index=str(i), dtype=self.dtype)\n-            for i in range(self.config.num_hidden_groups)\n-        ]\n-\n-    def __call__(\n-        self,\n-        hidden_states,\n-        attention_mask,\n-        deterministic: bool = True,\n-        output_attentions: bool = False,\n-        output_hidden_states: bool = False,\n-        return_dict: bool = True,\n-    ):\n-        all_attentions = () if output_attentions else None\n-        all_hidden_states = (hidden_states,) if output_hidden_states else None\n-\n-        for i in range(self.config.num_hidden_layers):\n-            # Index of the hidden group\n-            group_idx = int(i / (self.config.num_hidden_layers / self.config.num_hidden_groups))\n-            layer_group_output = self.layers[group_idx](\n-                hidden_states,\n-                attention_mask,\n-                deterministic=deterministic,\n-                output_attentions=output_attentions,\n-                output_hidden_states=output_hidden_states,\n-            )\n-            hidden_states = layer_group_output[0]\n-\n-            if output_attentions:\n-                all_attentions = all_attentions + layer_group_output[-1]\n-\n-            if output_hidden_states:\n-                all_hidden_states = all_hidden_states + (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, all_hidden_states, all_attentions] if v is not None)\n-        return FlaxBaseModelOutput(\n-            last_hidden_state=hidden_states, hidden_states=all_hidden_states, attentions=all_attentions\n-        )\n-\n-\n-class FlaxAlbertEncoder(nn.Module):\n-    config: AlbertConfig\n-    dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n-\n-    def setup(self):\n-        self.embedding_hidden_mapping_in = nn.Dense(\n-            self.config.hidden_size,\n-            kernel_init=jax.nn.initializers.normal(self.config.initializer_range),\n-            dtype=self.dtype,\n-        )\n-        self.albert_layer_groups = FlaxAlbertLayerGroups(self.config, dtype=self.dtype)\n-\n-    def __call__(\n-        self,\n-        hidden_states,\n-        attention_mask,\n-        deterministic: bool = True,\n-        output_attentions: bool = False,\n-        output_hidden_states: bool = False,\n-        return_dict: bool = True,\n-    ):\n-        hidden_states = self.embedding_hidden_mapping_in(hidden_states)\n-        return self.albert_layer_groups(\n-            hidden_states,\n-            attention_mask,\n-            deterministic=deterministic,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-        )\n-\n-\n-class FlaxAlbertOnlyMLMHead(nn.Module):\n-    config: AlbertConfig\n-    dtype: jnp.dtype = jnp.float32\n-    bias_init: Callable[..., np.ndarray] = jax.nn.initializers.zeros\n-\n-    def setup(self):\n-        self.dense = nn.Dense(self.config.embedding_size, dtype=self.dtype)\n-        self.activation = ACT2FN[self.config.hidden_act]\n-        self.LayerNorm = nn.LayerNorm(epsilon=self.config.layer_norm_eps, dtype=self.dtype)\n-        self.decoder = nn.Dense(self.config.vocab_size, dtype=self.dtype, use_bias=False)\n-        self.bias = self.param(\"bias\", self.bias_init, (self.config.vocab_size,))\n-\n-    def __call__(self, hidden_states, shared_embedding=None):\n-        hidden_states = self.dense(hidden_states)\n-        hidden_states = self.activation(hidden_states)\n-        hidden_states = self.LayerNorm(hidden_states)\n-\n-        if shared_embedding is not None:\n-            hidden_states = self.decoder.apply({\"params\": {\"kernel\": shared_embedding.T}}, hidden_states)\n-        else:\n-            hidden_states = self.decoder(hidden_states)\n-\n-        hidden_states += self.bias\n-        return hidden_states\n-\n-\n-class FlaxAlbertSOPHead(nn.Module):\n-    config: AlbertConfig\n-    dtype: jnp.dtype = jnp.float32\n-\n-    def setup(self):\n-        self.dropout = nn.Dropout(self.config.classifier_dropout_prob)\n-        self.classifier = nn.Dense(2, dtype=self.dtype)\n-\n-    def __call__(self, pooled_output, deterministic=True):\n-        pooled_output = self.dropout(pooled_output, deterministic=deterministic)\n-        logits = self.classifier(pooled_output)\n-        return logits\n-\n-\n-class FlaxAlbertPreTrainedModel(FlaxPreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n-    config_class = AlbertConfig\n-    base_model_prefix = \"albert\"\n-    module_class: nn.Module = None\n-\n-    def __init__(\n-        self,\n-        config: AlbertConfig,\n-        input_shape: tuple = (1, 1),\n-        seed: int = 0,\n-        dtype: jnp.dtype = jnp.float32,\n-        _do_init: bool = True,\n-        **kwargs,\n-    ):\n-        module = self.module_class(config=config, dtype=dtype, **kwargs)\n-        super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n-\n-    def init_weights(self, rng: jax.random.PRNGKey, input_shape: tuple, params: FrozenDict = None) -> FrozenDict:\n-        # init input tensors\n-        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n-        token_type_ids = jnp.zeros_like(input_ids)\n-        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_shape)\n-        attention_mask = jnp.ones_like(input_ids)\n-\n-        params_rng, dropout_rng = jax.random.split(rng)\n-        rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n-\n-        random_params = self.module.init(\n-            rngs, input_ids, attention_mask, token_type_ids, position_ids, return_dict=False\n-        )[\"params\"]\n-\n-        if params is not None:\n-            random_params = flatten_dict(unfreeze(random_params))\n-            params = flatten_dict(unfreeze(params))\n-            for missing_key in self._missing_keys:\n-                params[missing_key] = random_params[missing_key]\n-            self._missing_keys = set()\n-            return freeze(unflatten_dict(params))\n-        else:\n-            return random_params\n-\n-    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    def __call__(\n-        self,\n-        input_ids,\n-        attention_mask=None,\n-        token_type_ids=None,\n-        position_ids=None,\n-        params: Optional[dict] = None,\n-        dropout_rng: jax.random.PRNGKey = None,\n-        train: bool = False,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ):\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.return_dict\n-\n-        # init input tensors if not passed\n-        if token_type_ids is None:\n-            token_type_ids = jnp.zeros_like(input_ids)\n-\n-        if position_ids is None:\n-            position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n-\n-        if attention_mask is None:\n-            attention_mask = jnp.ones_like(input_ids)\n-\n-        # Handle any PRNG if needed\n-        rngs = {}\n-        if dropout_rng is not None:\n-            rngs[\"dropout\"] = dropout_rng\n-\n-        return self.module.apply(\n-            {\"params\": params or self.params},\n-            jnp.array(input_ids, dtype=\"i4\"),\n-            jnp.array(attention_mask, dtype=\"i4\"),\n-            jnp.array(token_type_ids, dtype=\"i4\"),\n-            jnp.array(position_ids, dtype=\"i4\"),\n-            not train,\n-            output_attentions,\n-            output_hidden_states,\n-            return_dict,\n-            rngs=rngs,\n-        )\n-\n-\n-class FlaxAlbertModule(nn.Module):\n-    config: AlbertConfig\n-    dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n-    add_pooling_layer: bool = True\n-\n-    def setup(self):\n-        self.embeddings = FlaxAlbertEmbeddings(self.config, dtype=self.dtype)\n-        self.encoder = FlaxAlbertEncoder(self.config, dtype=self.dtype)\n-        if self.add_pooling_layer:\n-            self.pooler = nn.Dense(\n-                self.config.hidden_size,\n-                kernel_init=jax.nn.initializers.normal(self.config.initializer_range),\n-                dtype=self.dtype,\n-                name=\"pooler\",\n-            )\n-            self.pooler_activation = nn.tanh\n-        else:\n-            self.pooler = None\n-            self.pooler_activation = None\n-\n-    def __call__(\n-        self,\n-        input_ids,\n-        attention_mask,\n-        token_type_ids: Optional[np.ndarray] = None,\n-        position_ids: Optional[np.ndarray] = None,\n-        deterministic: bool = True,\n-        output_attentions: bool = False,\n-        output_hidden_states: bool = False,\n-        return_dict: bool = True,\n-    ):\n-        # make sure `token_type_ids` is correctly initialized when not passed\n-        if token_type_ids is None:\n-            token_type_ids = jnp.zeros_like(input_ids)\n-\n-        # make sure `position_ids` is correctly initialized when not passed\n-        if position_ids is None:\n-            position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n-\n-        hidden_states = self.embeddings(input_ids, token_type_ids, position_ids, deterministic=deterministic)\n-\n-        outputs = self.encoder(\n-            hidden_states,\n-            attention_mask,\n-            deterministic=deterministic,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-        hidden_states = outputs[0]\n-        if self.add_pooling_layer:\n-            pooled = self.pooler(hidden_states[:, 0])\n-            pooled = self.pooler_activation(pooled)\n-        else:\n-            pooled = None\n-\n-        if not return_dict:\n-            # if pooled is None, don't return it\n-            if pooled is None:\n-                return (hidden_states,) + outputs[1:]\n-            return (hidden_states, pooled) + outputs[1:]\n-\n-        return FlaxBaseModelOutputWithPooling(\n-            last_hidden_state=hidden_states,\n-            pooler_output=pooled,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-\n-@add_start_docstrings(\n-    \"The bare Albert Model transformer outputting raw hidden-states without any specific head on top.\",\n-    ALBERT_START_DOCSTRING,\n-)\n-class FlaxAlbertModel(FlaxAlbertPreTrainedModel):\n-    module_class = FlaxAlbertModule\n-\n-\n-append_call_sample_docstring(FlaxAlbertModel, _CHECKPOINT_FOR_DOC, FlaxBaseModelOutputWithPooling, _CONFIG_FOR_DOC)\n-\n-\n-class FlaxAlbertForPreTrainingModule(nn.Module):\n-    config: AlbertConfig\n-    dtype: jnp.dtype = jnp.float32\n-\n-    def setup(self):\n-        self.albert = FlaxAlbertModule(config=self.config, dtype=self.dtype)\n-        self.predictions = FlaxAlbertOnlyMLMHead(config=self.config, dtype=self.dtype)\n-        self.sop_classifier = FlaxAlbertSOPHead(config=self.config, dtype=self.dtype)\n-\n-    def __call__(\n-        self,\n-        input_ids,\n-        attention_mask,\n-        token_type_ids,\n-        position_ids,\n-        deterministic: bool = True,\n-        output_attentions: bool = False,\n-        output_hidden_states: bool = False,\n-        return_dict: bool = True,\n-    ):\n-        # Model\n-        outputs = self.albert(\n-            input_ids,\n-            attention_mask,\n-            token_type_ids,\n-            position_ids,\n-            deterministic=deterministic,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        if self.config.tie_word_embeddings:\n-            shared_embedding = self.albert.variables[\"params\"][\"embeddings\"][\"word_embeddings\"][\"embedding\"]\n-        else:\n-            shared_embedding = None\n-\n-        hidden_states = outputs[0]\n-        pooled_output = outputs[1]\n-\n-        prediction_scores = self.predictions(hidden_states, shared_embedding=shared_embedding)\n-        sop_scores = self.sop_classifier(pooled_output, deterministic=deterministic)\n-\n-        if not return_dict:\n-            return (prediction_scores, sop_scores) + outputs[2:]\n-\n-        return FlaxAlbertForPreTrainingOutput(\n-            prediction_logits=prediction_scores,\n-            sop_logits=sop_scores,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-\n-@add_start_docstrings(\n-    \"\"\"\n-    Albert Model with two heads on top as done during the pretraining: a `masked language modeling` head and a\n-    `sentence order prediction (classification)` head.\n-    \"\"\",\n-    ALBERT_START_DOCSTRING,\n-)\n-class FlaxAlbertForPreTraining(FlaxAlbertPreTrainedModel):\n-    module_class = FlaxAlbertForPreTrainingModule\n-\n-\n-FLAX_ALBERT_FOR_PRETRAINING_DOCSTRING = \"\"\"\n-    Returns:\n-\n-    Example:\n-\n-    ```python\n-    >>> from transformers import AutoTokenizer, FlaxAlbertForPreTraining\n-\n-    >>> tokenizer = AutoTokenizer.from_pretrained(\"albert/albert-base-v2\")\n-    >>> model = FlaxAlbertForPreTraining.from_pretrained(\"albert/albert-base-v2\")\n-\n-    >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"np\")\n-    >>> outputs = model(**inputs)\n-\n-    >>> prediction_logits = outputs.prediction_logits\n-    >>> seq_relationship_logits = outputs.sop_logits\n-    ```\n-\"\"\"\n-\n-overwrite_call_docstring(\n-    FlaxAlbertForPreTraining,\n-    ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\") + FLAX_ALBERT_FOR_PRETRAINING_DOCSTRING,\n-)\n-append_replace_return_docstrings(\n-    FlaxAlbertForPreTraining, output_type=FlaxAlbertForPreTrainingOutput, config_class=_CONFIG_FOR_DOC\n-)\n-\n-\n-class FlaxAlbertForMaskedLMModule(nn.Module):\n-    config: AlbertConfig\n-    dtype: jnp.dtype = jnp.float32\n-\n-    def setup(self):\n-        self.albert = FlaxAlbertModule(config=self.config, add_pooling_layer=False, dtype=self.dtype)\n-        self.predictions = FlaxAlbertOnlyMLMHead(config=self.config, dtype=self.dtype)\n-\n-    def __call__(\n-        self,\n-        input_ids,\n-        attention_mask,\n-        token_type_ids,\n-        position_ids,\n-        deterministic: bool = True,\n-        output_attentions: bool = False,\n-        output_hidden_states: bool = False,\n-        return_dict: bool = True,\n-    ):\n-        # Model\n-        outputs = self.albert(\n-            input_ids,\n-            attention_mask,\n-            token_type_ids,\n-            position_ids,\n-            deterministic=deterministic,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        hidden_states = outputs[0]\n-        if self.config.tie_word_embeddings:\n-            shared_embedding = self.albert.variables[\"params\"][\"embeddings\"][\"word_embeddings\"][\"embedding\"]\n-        else:\n-            shared_embedding = None\n-\n-        # Compute the prediction scores\n-        logits = self.predictions(hidden_states, shared_embedding=shared_embedding)\n-\n-        if not return_dict:\n-            return (logits,) + outputs[1:]\n-\n-        return FlaxMaskedLMOutput(\n-            logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-\n-@add_start_docstrings(\"\"\"Albert Model with a `language modeling` head on top.\"\"\", ALBERT_START_DOCSTRING)\n-class FlaxAlbertForMaskedLM(FlaxAlbertPreTrainedModel):\n-    module_class = FlaxAlbertForMaskedLMModule\n-\n-\n-append_call_sample_docstring(\n-    FlaxAlbertForMaskedLM, _CHECKPOINT_FOR_DOC, FlaxMaskedLMOutput, _CONFIG_FOR_DOC, revision=\"refs/pr/11\"\n-)\n-\n-\n-class FlaxAlbertForSequenceClassificationModule(nn.Module):\n-    config: AlbertConfig\n-    dtype: jnp.dtype = jnp.float32\n-\n-    def setup(self):\n-        self.albert = FlaxAlbertModule(config=self.config, dtype=self.dtype)\n-        classifier_dropout = (\n-            self.config.classifier_dropout_prob\n-            if self.config.classifier_dropout_prob is not None\n-            else self.config.hidden_dropout_prob\n-        )\n-        self.dropout = nn.Dropout(rate=classifier_dropout)\n-        self.classifier = nn.Dense(\n-            self.config.num_labels,\n-            dtype=self.dtype,\n-        )\n-\n-    def __call__(\n-        self,\n-        input_ids,\n-        attention_mask,\n-        token_type_ids,\n-        position_ids,\n-        deterministic: bool = True,\n-        output_attentions: bool = False,\n-        output_hidden_states: bool = False,\n-        return_dict: bool = True,\n-    ):\n-        # Model\n-        outputs = self.albert(\n-            input_ids,\n-            attention_mask,\n-            token_type_ids,\n-            position_ids,\n-            deterministic=deterministic,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        pooled_output = outputs[1]\n-        pooled_output = self.dropout(pooled_output, deterministic=deterministic)\n-        logits = self.classifier(pooled_output)\n-\n-        if not return_dict:\n-            return (logits,) + outputs[2:]\n-\n-        return FlaxSequenceClassifierOutput(\n-            logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-\n-@add_start_docstrings(\n-    \"\"\"\n-    Albert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled\n-    output) e.g. for GLUE tasks.\n-    \"\"\",\n-    ALBERT_START_DOCSTRING,\n-)\n-class FlaxAlbertForSequenceClassification(FlaxAlbertPreTrainedModel):\n-    module_class = FlaxAlbertForSequenceClassificationModule\n-\n-\n-append_call_sample_docstring(\n-    FlaxAlbertForSequenceClassification,\n-    _CHECKPOINT_FOR_DOC,\n-    FlaxSequenceClassifierOutput,\n-    _CONFIG_FOR_DOC,\n-)\n-\n-\n-class FlaxAlbertForMultipleChoiceModule(nn.Module):\n-    config: AlbertConfig\n-    dtype: jnp.dtype = jnp.float32\n-\n-    def setup(self):\n-        self.albert = FlaxAlbertModule(config=self.config, dtype=self.dtype)\n-        self.dropout = nn.Dropout(rate=self.config.hidden_dropout_prob)\n-        self.classifier = nn.Dense(1, dtype=self.dtype)\n-\n-    def __call__(\n-        self,\n-        input_ids,\n-        attention_mask,\n-        token_type_ids,\n-        position_ids,\n-        deterministic: bool = True,\n-        output_attentions: bool = False,\n-        output_hidden_states: bool = False,\n-        return_dict: bool = True,\n-    ):\n-        num_choices = input_ids.shape[1]\n-        input_ids = input_ids.reshape(-1, input_ids.shape[-1]) if input_ids is not None else None\n-        attention_mask = attention_mask.reshape(-1, attention_mask.shape[-1]) if attention_mask is not None else None\n-        token_type_ids = token_type_ids.reshape(-1, token_type_ids.shape[-1]) if token_type_ids is not None else None\n-        position_ids = position_ids.reshape(-1, position_ids.shape[-1]) if position_ids is not None else None\n-\n-        # Model\n-        outputs = self.albert(\n-            input_ids,\n-            attention_mask,\n-            token_type_ids,\n-            position_ids,\n-            deterministic=deterministic,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        pooled_output = outputs[1]\n-        pooled_output = self.dropout(pooled_output, deterministic=deterministic)\n-        logits = self.classifier(pooled_output)\n-\n-        reshaped_logits = logits.reshape(-1, num_choices)\n-\n-        if not return_dict:\n-            return (reshaped_logits,) + outputs[2:]\n-\n-        return FlaxMultipleChoiceModelOutput(\n-            logits=reshaped_logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-\n-@add_start_docstrings(\n-    \"\"\"\n-    Albert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a\n-    softmax) e.g. for RocStories/SWAG tasks.\n-    \"\"\",\n-    ALBERT_START_DOCSTRING,\n-)\n-class FlaxAlbertForMultipleChoice(FlaxAlbertPreTrainedModel):\n-    module_class = FlaxAlbertForMultipleChoiceModule\n-\n-\n-overwrite_call_docstring(\n-    FlaxAlbertForMultipleChoice, ALBERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\")\n-)\n-append_call_sample_docstring(\n-    FlaxAlbertForMultipleChoice,\n-    _CHECKPOINT_FOR_DOC,\n-    FlaxMultipleChoiceModelOutput,\n-    _CONFIG_FOR_DOC,\n-)\n-\n-\n-class FlaxAlbertForTokenClassificationModule(nn.Module):\n-    config: AlbertConfig\n-    dtype: jnp.dtype = jnp.float32\n-\n-    def setup(self):\n-        self.albert = FlaxAlbertModule(config=self.config, dtype=self.dtype, add_pooling_layer=False)\n-        classifier_dropout = (\n-            self.config.classifier_dropout_prob\n-            if self.config.classifier_dropout_prob is not None\n-            else self.config.hidden_dropout_prob\n-        )\n-        self.dropout = nn.Dropout(rate=classifier_dropout)\n-        self.classifier = nn.Dense(self.config.num_labels, dtype=self.dtype)\n-\n-    def __call__(\n-        self,\n-        input_ids,\n-        attention_mask,\n-        token_type_ids,\n-        position_ids,\n-        deterministic: bool = True,\n-        output_attentions: bool = False,\n-        output_hidden_states: bool = False,\n-        return_dict: bool = True,\n-    ):\n-        # Model\n-        outputs = self.albert(\n-            input_ids,\n-            attention_mask,\n-            token_type_ids,\n-            position_ids,\n-            deterministic=deterministic,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        hidden_states = outputs[0]\n-        hidden_states = self.dropout(hidden_states, deterministic=deterministic)\n-        logits = self.classifier(hidden_states)\n-\n-        if not return_dict:\n-            return (logits,) + outputs[1:]\n-\n-        return FlaxTokenClassifierOutput(\n-            logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-\n-@add_start_docstrings(\n-    \"\"\"\n-    Albert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\n-    Named-Entity-Recognition (NER) tasks.\n-    \"\"\",\n-    ALBERT_START_DOCSTRING,\n-)\n-class FlaxAlbertForTokenClassification(FlaxAlbertPreTrainedModel):\n-    module_class = FlaxAlbertForTokenClassificationModule\n-\n-\n-append_call_sample_docstring(\n-    FlaxAlbertForTokenClassification,\n-    _CHECKPOINT_FOR_DOC,\n-    FlaxTokenClassifierOutput,\n-    _CONFIG_FOR_DOC,\n-)\n-\n-\n-class FlaxAlbertForQuestionAnsweringModule(nn.Module):\n-    config: AlbertConfig\n-    dtype: jnp.dtype = jnp.float32\n-\n-    def setup(self):\n-        self.albert = FlaxAlbertModule(config=self.config, dtype=self.dtype, add_pooling_layer=False)\n-        self.qa_outputs = nn.Dense(self.config.num_labels, dtype=self.dtype)\n-\n-    def __call__(\n-        self,\n-        input_ids,\n-        attention_mask,\n-        token_type_ids,\n-        position_ids,\n-        deterministic: bool = True,\n-        output_attentions: bool = False,\n-        output_hidden_states: bool = False,\n-        return_dict: bool = True,\n-    ):\n-        # Model\n-        outputs = self.albert(\n-            input_ids,\n-            attention_mask,\n-            token_type_ids,\n-            position_ids,\n-            deterministic=deterministic,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        hidden_states = outputs[0]\n-\n-        logits = self.qa_outputs(hidden_states)\n-        start_logits, end_logits = jnp.split(logits, self.config.num_labels, axis=-1)\n-        start_logits = start_logits.squeeze(-1)\n-        end_logits = end_logits.squeeze(-1)\n-\n-        if not return_dict:\n-            return (start_logits, end_logits) + outputs[1:]\n-\n-        return FlaxQuestionAnsweringModelOutput(\n-            start_logits=start_logits,\n-            end_logits=end_logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-\n-@add_start_docstrings(\n-    \"\"\"\n-    Albert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n-    layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n-    \"\"\",\n-    ALBERT_START_DOCSTRING,\n-)\n-class FlaxAlbertForQuestionAnswering(FlaxAlbertPreTrainedModel):\n-    module_class = FlaxAlbertForQuestionAnsweringModule\n-\n-\n-append_call_sample_docstring(\n-    FlaxAlbertForQuestionAnswering,\n-    _CHECKPOINT_FOR_DOC,\n-    FlaxQuestionAnsweringModelOutput,\n-    _CONFIG_FOR_DOC,\n-)\n-\n-__all__ = [\n-    \"FlaxAlbertPreTrainedModel\",\n-    \"FlaxAlbertModel\",\n-    \"FlaxAlbertForPreTraining\",\n-    \"FlaxAlbertForMaskedLM\",\n-    \"FlaxAlbertForSequenceClassification\",\n-    \"FlaxAlbertForMultipleChoice\",\n-    \"FlaxAlbertForTokenClassification\",\n-    \"FlaxAlbertForQuestionAnswering\",\n-]"
        },
        {
            "sha": "101ab63dc0545992fe68a53205f4ad81c607d9ca",
            "filename": "src/transformers/models/albert/modeling_tf_albert.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1572,
            "changes": 1572,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_tf_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_tf_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_tf_albert.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,1572 +0,0 @@\n-# coding=utf-8\n-# Copyright 2018 The OpenAI Team Authors and HuggingFace Inc. team.\n-# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"TF 2.0 ALBERT model.\"\"\"\n-\n-from __future__ import annotations\n-\n-import math\n-from dataclasses import dataclass\n-\n-import numpy as np\n-import tensorflow as tf\n-\n-from ...activations_tf import get_tf_activation\n-from ...modeling_tf_outputs import (\n-    TFBaseModelOutput,\n-    TFBaseModelOutputWithPooling,\n-    TFMaskedLMOutput,\n-    TFMultipleChoiceModelOutput,\n-    TFQuestionAnsweringModelOutput,\n-    TFSequenceClassifierOutput,\n-    TFTokenClassifierOutput,\n-)\n-from ...modeling_tf_utils import (\n-    TFMaskedLanguageModelingLoss,\n-    TFModelInputType,\n-    TFMultipleChoiceLoss,\n-    TFPreTrainedModel,\n-    TFQuestionAnsweringLoss,\n-    TFSequenceClassificationLoss,\n-    TFTokenClassificationLoss,\n-    get_initializer,\n-    keras,\n-    keras_serializable,\n-    unpack_inputs,\n-)\n-from ...tf_utils import check_embeddings_within_bounds, shape_list, stable_softmax\n-from ...utils import (\n-    ModelOutput,\n-    add_code_sample_docstrings,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    logging,\n-    replace_return_docstrings,\n-)\n-from .configuration_albert import AlbertConfig\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-_CHECKPOINT_FOR_DOC = \"albert/albert-base-v2\"\n-_CONFIG_FOR_DOC = \"AlbertConfig\"\n-\n-\n-class TFAlbertPreTrainingLoss:\n-    \"\"\"\n-    Loss function suitable for ALBERT pretraining, that is, the task of pretraining a language model by combining SOP +\n-    MLM. .. note:: Any label of -100 will be ignored (along with the corresponding logits) in the loss computation.\n-    \"\"\"\n-\n-    def hf_compute_loss(self, labels: tf.Tensor, logits: tf.Tensor) -> tf.Tensor:\n-        loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=keras.losses.Reduction.NONE)\n-        if self.config.tf_legacy_loss:\n-            # make sure only labels that are not equal to -100\n-            # are taken into account as loss\n-            masked_lm_active_loss = tf.not_equal(tf.reshape(tensor=labels[\"labels\"], shape=(-1,)), -100)\n-            masked_lm_reduced_logits = tf.boolean_mask(\n-                tensor=tf.reshape(tensor=logits[0], shape=(-1, shape_list(logits[0])[2])),\n-                mask=masked_lm_active_loss,\n-            )\n-            masked_lm_labels = tf.boolean_mask(\n-                tensor=tf.reshape(tensor=labels[\"labels\"], shape=(-1,)), mask=masked_lm_active_loss\n-            )\n-            sentence_order_active_loss = tf.not_equal(\n-                tf.reshape(tensor=labels[\"sentence_order_label\"], shape=(-1,)), -100\n-            )\n-            sentence_order_reduced_logits = tf.boolean_mask(\n-                tensor=tf.reshape(tensor=logits[1], shape=(-1, 2)), mask=sentence_order_active_loss\n-            )\n-            sentence_order_label = tf.boolean_mask(\n-                tensor=tf.reshape(tensor=labels[\"sentence_order_label\"], shape=(-1,)), mask=sentence_order_active_loss\n-            )\n-            masked_lm_loss = loss_fn(y_true=masked_lm_labels, y_pred=masked_lm_reduced_logits)\n-            sentence_order_loss = loss_fn(y_true=sentence_order_label, y_pred=sentence_order_reduced_logits)\n-            masked_lm_loss = tf.reshape(tensor=masked_lm_loss, shape=(-1, shape_list(sentence_order_loss)[0]))\n-            masked_lm_loss = tf.reduce_mean(input_tensor=masked_lm_loss, axis=0)\n-\n-            return masked_lm_loss + sentence_order_loss\n-\n-        # Clip negative labels to zero here to avoid NaNs and errors - those positions will get masked later anyway\n-        unmasked_lm_losses = loss_fn(y_true=tf.nn.relu(labels[\"labels\"]), y_pred=logits[0])\n-        # make sure only labels that are not equal to -100\n-        # are taken into account for the loss computation\n-        lm_loss_mask = tf.cast(labels[\"labels\"] != -100, dtype=unmasked_lm_losses.dtype)\n-        masked_lm_losses = unmasked_lm_losses * lm_loss_mask\n-        reduced_masked_lm_loss = tf.reduce_sum(masked_lm_losses) / tf.reduce_sum(lm_loss_mask)\n-\n-        sop_logits = tf.reshape(logits[1], (-1, 2))\n-        # Clip negative labels to zero here to avoid NaNs and errors - those positions will get masked later anyway\n-        unmasked_sop_loss = loss_fn(y_true=tf.nn.relu(labels[\"sentence_order_label\"]), y_pred=sop_logits)\n-        sop_loss_mask = tf.cast(labels[\"sentence_order_label\"] != -100, dtype=unmasked_sop_loss.dtype)\n-\n-        masked_sop_loss = unmasked_sop_loss * sop_loss_mask\n-        reduced_masked_sop_loss = tf.reduce_sum(masked_sop_loss) / tf.reduce_sum(sop_loss_mask)\n-\n-        return tf.reshape(reduced_masked_lm_loss + reduced_masked_sop_loss, (1,))\n-\n-\n-class TFAlbertEmbeddings(keras.layers.Layer):\n-    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n-\n-    def __init__(self, config: AlbertConfig, **kwargs):\n-        super().__init__(**kwargs)\n-\n-        self.config = config\n-        self.embedding_size = config.embedding_size\n-        self.max_position_embeddings = config.max_position_embeddings\n-        self.initializer_range = config.initializer_range\n-        self.LayerNorm = keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n-        self.dropout = keras.layers.Dropout(rate=config.hidden_dropout_prob)\n-\n-    def build(self, input_shape=None):\n-        with tf.name_scope(\"word_embeddings\"):\n-            self.weight = self.add_weight(\n-                name=\"weight\",\n-                shape=[self.config.vocab_size, self.embedding_size],\n-                initializer=get_initializer(self.initializer_range),\n-            )\n-\n-        with tf.name_scope(\"token_type_embeddings\"):\n-            self.token_type_embeddings = self.add_weight(\n-                name=\"embeddings\",\n-                shape=[self.config.type_vocab_size, self.embedding_size],\n-                initializer=get_initializer(self.initializer_range),\n-            )\n-\n-        with tf.name_scope(\"position_embeddings\"):\n-            self.position_embeddings = self.add_weight(\n-                name=\"embeddings\",\n-                shape=[self.max_position_embeddings, self.embedding_size],\n-                initializer=get_initializer(self.initializer_range),\n-            )\n-\n-        if self.built:\n-            return\n-        self.built = True\n-        if getattr(self, \"LayerNorm\", None) is not None:\n-            with tf.name_scope(self.LayerNorm.name):\n-                self.LayerNorm.build([None, None, self.config.embedding_size])\n-\n-    # Copied from transformers.models.bert.modeling_tf_bert.TFBertEmbeddings.call\n-    def call(\n-        self,\n-        input_ids: tf.Tensor | None = None,\n-        position_ids: tf.Tensor | None = None,\n-        token_type_ids: tf.Tensor | None = None,\n-        inputs_embeds: tf.Tensor | None = None,\n-        past_key_values_length=0,\n-        training: bool = False,\n-    ) -> tf.Tensor:\n-        \"\"\"\n-        Applies embedding based on inputs tensor.\n-\n-        Returns:\n-            final_embeddings (`tf.Tensor`): output embedding tensor.\n-        \"\"\"\n-        if input_ids is None and inputs_embeds is None:\n-            raise ValueError(\"Need to provide either `input_ids` or `input_embeds`.\")\n-\n-        if input_ids is not None:\n-            check_embeddings_within_bounds(input_ids, self.config.vocab_size)\n-            inputs_embeds = tf.gather(params=self.weight, indices=input_ids)\n-\n-        input_shape = shape_list(inputs_embeds)[:-1]\n-\n-        if token_type_ids is None:\n-            token_type_ids = tf.fill(dims=input_shape, value=0)\n-\n-        if position_ids is None:\n-            position_ids = tf.expand_dims(\n-                tf.range(start=past_key_values_length, limit=input_shape[1] + past_key_values_length), axis=0\n-            )\n-\n-        position_embeds = tf.gather(params=self.position_embeddings, indices=position_ids)\n-        token_type_embeds = tf.gather(params=self.token_type_embeddings, indices=token_type_ids)\n-        final_embeddings = inputs_embeds + position_embeds + token_type_embeds\n-        final_embeddings = self.LayerNorm(inputs=final_embeddings)\n-        final_embeddings = self.dropout(inputs=final_embeddings, training=training)\n-\n-        return final_embeddings\n-\n-\n-class TFAlbertAttention(keras.layers.Layer):\n-    \"\"\"Contains the complete attention sublayer, including both dropouts and layer norm.\"\"\"\n-\n-    def __init__(self, config: AlbertConfig, **kwargs):\n-        super().__init__(**kwargs)\n-\n-        if config.hidden_size % config.num_attention_heads != 0:\n-            raise ValueError(\n-                f\"The hidden size ({config.hidden_size}) is not a multiple of the number \"\n-                f\"of attention heads ({config.num_attention_heads})\"\n-            )\n-\n-        self.num_attention_heads = config.num_attention_heads\n-        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n-        self.all_head_size = self.num_attention_heads * self.attention_head_size\n-        self.sqrt_att_head_size = math.sqrt(self.attention_head_size)\n-        self.output_attentions = config.output_attentions\n-\n-        self.query = keras.layers.Dense(\n-            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"query\"\n-        )\n-        self.key = keras.layers.Dense(\n-            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"key\"\n-        )\n-        self.value = keras.layers.Dense(\n-            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"value\"\n-        )\n-        self.dense = keras.layers.Dense(\n-            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n-        )\n-        self.LayerNorm = keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n-        # Two different dropout probabilities; see https://github.com/google-research/albert/blob/master/modeling.py#L971-L993\n-        self.attention_dropout = keras.layers.Dropout(rate=config.attention_probs_dropout_prob)\n-        self.output_dropout = keras.layers.Dropout(rate=config.hidden_dropout_prob)\n-        self.config = config\n-\n-    def transpose_for_scores(self, tensor: tf.Tensor, batch_size: int) -> tf.Tensor:\n-        # Reshape from [batch_size, seq_length, all_head_size] to [batch_size, seq_length, num_attention_heads, attention_head_size]\n-        tensor = tf.reshape(tensor=tensor, shape=(batch_size, -1, self.num_attention_heads, self.attention_head_size))\n-\n-        # Transpose the tensor from [batch_size, seq_length, num_attention_heads, attention_head_size] to [batch_size, num_attention_heads, seq_length, attention_head_size]\n-        return tf.transpose(tensor, perm=[0, 2, 1, 3])\n-\n-    def call(\n-        self,\n-        input_tensor: tf.Tensor,\n-        attention_mask: tf.Tensor,\n-        head_mask: tf.Tensor,\n-        output_attentions: bool,\n-        training: bool = False,\n-    ) -> tuple[tf.Tensor]:\n-        batch_size = shape_list(input_tensor)[0]\n-        mixed_query_layer = self.query(inputs=input_tensor)\n-        mixed_key_layer = self.key(inputs=input_tensor)\n-        mixed_value_layer = self.value(inputs=input_tensor)\n-        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n-        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)\n-        value_layer = self.transpose_for_scores(mixed_value_layer, batch_size)\n-\n-        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-        # (batch size, num_heads, seq_len_q, seq_len_k)\n-        attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n-        dk = tf.cast(self.sqrt_att_head_size, dtype=attention_scores.dtype)\n-        attention_scores = tf.divide(attention_scores, dk)\n-\n-        if attention_mask is not None:\n-            # Apply the attention mask is (precomputed for all layers in TFAlbertModel call() function)\n-            attention_scores = tf.add(attention_scores, attention_mask)\n-\n-        # Normalize the attention scores to probabilities.\n-        attention_probs = stable_softmax(logits=attention_scores, axis=-1)\n-\n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.attention_dropout(inputs=attention_probs, training=training)\n-\n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = tf.multiply(attention_probs, head_mask)\n-\n-        context_layer = tf.matmul(attention_probs, value_layer)\n-        context_layer = tf.transpose(context_layer, perm=[0, 2, 1, 3])\n-\n-        # (batch_size, seq_len_q, all_head_size)\n-        context_layer = tf.reshape(tensor=context_layer, shape=(batch_size, -1, self.all_head_size))\n-        self_outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-        hidden_states = self_outputs[0]\n-        hidden_states = self.dense(inputs=hidden_states)\n-        hidden_states = self.output_dropout(inputs=hidden_states, training=training)\n-        attention_output = self.LayerNorm(inputs=hidden_states + input_tensor)\n-\n-        # add attentions if we output them\n-        outputs = (attention_output,) + self_outputs[1:]\n-\n-        return outputs\n-\n-    def build(self, input_shape=None):\n-        if self.built:\n-            return\n-        self.built = True\n-        if getattr(self, \"query\", None) is not None:\n-            with tf.name_scope(self.query.name):\n-                self.query.build([None, None, self.config.hidden_size])\n-        if getattr(self, \"key\", None) is not None:\n-            with tf.name_scope(self.key.name):\n-                self.key.build([None, None, self.config.hidden_size])\n-        if getattr(self, \"value\", None) is not None:\n-            with tf.name_scope(self.value.name):\n-                self.value.build([None, None, self.config.hidden_size])\n-        if getattr(self, \"dense\", None) is not None:\n-            with tf.name_scope(self.dense.name):\n-                self.dense.build([None, None, self.config.hidden_size])\n-        if getattr(self, \"LayerNorm\", None) is not None:\n-            with tf.name_scope(self.LayerNorm.name):\n-                self.LayerNorm.build([None, None, self.config.hidden_size])\n-\n-\n-class TFAlbertLayer(keras.layers.Layer):\n-    def __init__(self, config: AlbertConfig, **kwargs):\n-        super().__init__(**kwargs)\n-\n-        self.attention = TFAlbertAttention(config, name=\"attention\")\n-        self.ffn = keras.layers.Dense(\n-            units=config.intermediate_size, kernel_initializer=get_initializer(config.initializer_range), name=\"ffn\"\n-        )\n-\n-        if isinstance(config.hidden_act, str):\n-            self.activation = get_tf_activation(config.hidden_act)\n-        else:\n-            self.activation = config.hidden_act\n-\n-        self.ffn_output = keras.layers.Dense(\n-            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"ffn_output\"\n-        )\n-        self.full_layer_layer_norm = keras.layers.LayerNormalization(\n-            epsilon=config.layer_norm_eps, name=\"full_layer_layer_norm\"\n-        )\n-        self.dropout = keras.layers.Dropout(rate=config.hidden_dropout_prob)\n-        self.config = config\n-\n-    def call(\n-        self,\n-        hidden_states: tf.Tensor,\n-        attention_mask: tf.Tensor,\n-        head_mask: tf.Tensor,\n-        output_attentions: bool,\n-        training: bool = False,\n-    ) -> tuple[tf.Tensor]:\n-        attention_outputs = self.attention(\n-            input_tensor=hidden_states,\n-            attention_mask=attention_mask,\n-            head_mask=head_mask,\n-            output_attentions=output_attentions,\n-            training=training,\n-        )\n-        ffn_output = self.ffn(inputs=attention_outputs[0])\n-        ffn_output = self.activation(ffn_output)\n-        ffn_output = self.ffn_output(inputs=ffn_output)\n-        ffn_output = self.dropout(inputs=ffn_output, training=training)\n-        hidden_states = self.full_layer_layer_norm(inputs=ffn_output + attention_outputs[0])\n-\n-        # add attentions if we output them\n-        outputs = (hidden_states,) + attention_outputs[1:]\n-\n-        return outputs\n-\n-    def build(self, input_shape=None):\n-        if self.built:\n-            return\n-        self.built = True\n-        if getattr(self, \"attention\", None) is not None:\n-            with tf.name_scope(self.attention.name):\n-                self.attention.build(None)\n-        if getattr(self, \"ffn\", None) is not None:\n-            with tf.name_scope(self.ffn.name):\n-                self.ffn.build([None, None, self.config.hidden_size])\n-        if getattr(self, \"ffn_output\", None) is not None:\n-            with tf.name_scope(self.ffn_output.name):\n-                self.ffn_output.build([None, None, self.config.intermediate_size])\n-        if getattr(self, \"full_layer_layer_norm\", None) is not None:\n-            with tf.name_scope(self.full_layer_layer_norm.name):\n-                self.full_layer_layer_norm.build([None, None, self.config.hidden_size])\n-\n-\n-class TFAlbertLayerGroup(keras.layers.Layer):\n-    def __init__(self, config: AlbertConfig, **kwargs):\n-        super().__init__(**kwargs)\n-\n-        self.albert_layers = [\n-            TFAlbertLayer(config, name=f\"albert_layers_._{i}\") for i in range(config.inner_group_num)\n-        ]\n-\n-    def call(\n-        self,\n-        hidden_states: tf.Tensor,\n-        attention_mask: tf.Tensor,\n-        head_mask: tf.Tensor,\n-        output_attentions: bool,\n-        output_hidden_states: bool,\n-        training: bool = False,\n-    ) -> TFBaseModelOutput | tuple[tf.Tensor]:\n-        layer_hidden_states = () if output_hidden_states else None\n-        layer_attentions = () if output_attentions else None\n-\n-        for layer_index, albert_layer in enumerate(self.albert_layers):\n-            if output_hidden_states:\n-                layer_hidden_states = layer_hidden_states + (hidden_states,)\n-\n-            layer_output = albert_layer(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                head_mask=head_mask[layer_index],\n-                output_attentions=output_attentions,\n-                training=training,\n-            )\n-            hidden_states = layer_output[0]\n-\n-            if output_attentions:\n-                layer_attentions = layer_attentions + (layer_output[1],)\n-\n-        # Add last layer\n-        if output_hidden_states:\n-            layer_hidden_states = layer_hidden_states + (hidden_states,)\n-\n-        return tuple(v for v in [hidden_states, layer_hidden_states, layer_attentions] if v is not None)\n-\n-    def build(self, input_shape=None):\n-        if self.built:\n-            return\n-        self.built = True\n-        if getattr(self, \"albert_layers\", None) is not None:\n-            for layer in self.albert_layers:\n-                with tf.name_scope(layer.name):\n-                    layer.build(None)\n-\n-\n-class TFAlbertTransformer(keras.layers.Layer):\n-    def __init__(self, config: AlbertConfig, **kwargs):\n-        super().__init__(**kwargs)\n-\n-        self.num_hidden_layers = config.num_hidden_layers\n-        self.num_hidden_groups = config.num_hidden_groups\n-        # Number of layers in a hidden group\n-        self.layers_per_group = int(config.num_hidden_layers / config.num_hidden_groups)\n-        self.embedding_hidden_mapping_in = keras.layers.Dense(\n-            units=config.hidden_size,\n-            kernel_initializer=get_initializer(config.initializer_range),\n-            name=\"embedding_hidden_mapping_in\",\n-        )\n-        self.albert_layer_groups = [\n-            TFAlbertLayerGroup(config, name=f\"albert_layer_groups_._{i}\") for i in range(config.num_hidden_groups)\n-        ]\n-        self.config = config\n-\n-    def call(\n-        self,\n-        hidden_states: tf.Tensor,\n-        attention_mask: tf.Tensor,\n-        head_mask: tf.Tensor,\n-        output_attentions: bool,\n-        output_hidden_states: bool,\n-        return_dict: bool,\n-        training: bool = False,\n-    ) -> TFBaseModelOutput | tuple[tf.Tensor]:\n-        hidden_states = self.embedding_hidden_mapping_in(inputs=hidden_states)\n-        all_attentions = () if output_attentions else None\n-        all_hidden_states = (hidden_states,) if output_hidden_states else None\n-\n-        for i in range(self.num_hidden_layers):\n-            # Index of the hidden group\n-            group_idx = int(i / (self.num_hidden_layers / self.num_hidden_groups))\n-            layer_group_output = self.albert_layer_groups[group_idx](\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                head_mask=head_mask[group_idx * self.layers_per_group : (group_idx + 1) * self.layers_per_group],\n-                output_attentions=output_attentions,\n-                output_hidden_states=output_hidden_states,\n-                training=training,\n-            )\n-            hidden_states = layer_group_output[0]\n-\n-            if output_attentions:\n-                all_attentions = all_attentions + layer_group_output[-1]\n-\n-            if output_hidden_states:\n-                all_hidden_states = all_hidden_states + (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, all_hidden_states, all_attentions] if v is not None)\n-\n-        return TFBaseModelOutput(\n-            last_hidden_state=hidden_states, hidden_states=all_hidden_states, attentions=all_attentions\n-        )\n-\n-    def build(self, input_shape=None):\n-        if self.built:\n-            return\n-        self.built = True\n-        if getattr(self, \"embedding_hidden_mapping_in\", None) is not None:\n-            with tf.name_scope(self.embedding_hidden_mapping_in.name):\n-                self.embedding_hidden_mapping_in.build([None, None, self.config.embedding_size])\n-        if getattr(self, \"albert_layer_groups\", None) is not None:\n-            for layer in self.albert_layer_groups:\n-                with tf.name_scope(layer.name):\n-                    layer.build(None)\n-\n-\n-class TFAlbertPreTrainedModel(TFPreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n-    config_class = AlbertConfig\n-    base_model_prefix = \"albert\"\n-\n-\n-class TFAlbertMLMHead(keras.layers.Layer):\n-    def __init__(self, config: AlbertConfig, input_embeddings: keras.layers.Layer, **kwargs):\n-        super().__init__(**kwargs)\n-\n-        self.config = config\n-        self.embedding_size = config.embedding_size\n-        self.dense = keras.layers.Dense(\n-            config.embedding_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n-        )\n-        if isinstance(config.hidden_act, str):\n-            self.activation = get_tf_activation(config.hidden_act)\n-        else:\n-            self.activation = config.hidden_act\n-\n-        self.LayerNorm = keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n-\n-        # The output weights are the same as the input embeddings, but there is\n-        # an output-only bias for each token.\n-        self.decoder = input_embeddings\n-\n-    def build(self, input_shape=None):\n-        self.bias = self.add_weight(shape=(self.config.vocab_size,), initializer=\"zeros\", trainable=True, name=\"bias\")\n-        self.decoder_bias = self.add_weight(\n-            shape=(self.config.vocab_size,), initializer=\"zeros\", trainable=True, name=\"decoder/bias\"\n-        )\n-\n-        if self.built:\n-            return\n-        self.built = True\n-        if getattr(self, \"dense\", None) is not None:\n-            with tf.name_scope(self.dense.name):\n-                self.dense.build([None, None, self.config.hidden_size])\n-        if getattr(self, \"LayerNorm\", None) is not None:\n-            with tf.name_scope(self.LayerNorm.name):\n-                self.LayerNorm.build([None, None, self.config.embedding_size])\n-\n-    def get_output_embeddings(self) -> keras.layers.Layer:\n-        return self.decoder\n-\n-    def set_output_embeddings(self, value: tf.Variable):\n-        self.decoder.weight = value\n-        self.decoder.vocab_size = shape_list(value)[0]\n-\n-    def get_bias(self) -> dict[str, tf.Variable]:\n-        return {\"bias\": self.bias, \"decoder_bias\": self.decoder_bias}\n-\n-    def set_bias(self, value: tf.Variable):\n-        self.bias = value[\"bias\"]\n-        self.decoder_bias = value[\"decoder_bias\"]\n-        self.config.vocab_size = shape_list(value[\"bias\"])[0]\n-\n-    def call(self, hidden_states: tf.Tensor) -> tf.Tensor:\n-        hidden_states = self.dense(inputs=hidden_states)\n-        hidden_states = self.activation(hidden_states)\n-        hidden_states = self.LayerNorm(inputs=hidden_states)\n-        seq_length = shape_list(tensor=hidden_states)[1]\n-        hidden_states = tf.reshape(tensor=hidden_states, shape=[-1, self.embedding_size])\n-        hidden_states = tf.matmul(a=hidden_states, b=self.decoder.weight, transpose_b=True)\n-        hidden_states = tf.reshape(tensor=hidden_states, shape=[-1, seq_length, self.config.vocab_size])\n-        hidden_states = tf.nn.bias_add(value=hidden_states, bias=self.decoder_bias)\n-\n-        return hidden_states\n-\n-\n-@keras_serializable\n-class TFAlbertMainLayer(keras.layers.Layer):\n-    config_class = AlbertConfig\n-\n-    def __init__(self, config: AlbertConfig, add_pooling_layer: bool = True, **kwargs):\n-        super().__init__(**kwargs)\n-\n-        self.config = config\n-\n-        self.embeddings = TFAlbertEmbeddings(config, name=\"embeddings\")\n-        self.encoder = TFAlbertTransformer(config, name=\"encoder\")\n-        self.pooler = (\n-            keras.layers.Dense(\n-                units=config.hidden_size,\n-                kernel_initializer=get_initializer(config.initializer_range),\n-                activation=\"tanh\",\n-                name=\"pooler\",\n-            )\n-            if add_pooling_layer\n-            else None\n-        )\n-\n-    def get_input_embeddings(self) -> keras.layers.Layer:\n-        return self.embeddings\n-\n-    def set_input_embeddings(self, value: tf.Variable):\n-        self.embeddings.weight = value\n-        self.embeddings.vocab_size = shape_list(value)[0]\n-\n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        raise NotImplementedError\n-\n-    @unpack_inputs\n-    def call(\n-        self,\n-        input_ids: TFModelInputType | None = None,\n-        attention_mask: np.ndarray | tf.Tensor | None = None,\n-        token_type_ids: np.ndarray | tf.Tensor | None = None,\n-        position_ids: np.ndarray | tf.Tensor | None = None,\n-        head_mask: np.ndarray | tf.Tensor | None = None,\n-        inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: bool | None = None,\n-        output_hidden_states: bool | None = None,\n-        return_dict: bool | None = None,\n-        training: bool = False,\n-    ) -> TFBaseModelOutputWithPooling | tuple[tf.Tensor]:\n-        if input_ids is not None and inputs_embeds is not None:\n-            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n-        elif input_ids is not None:\n-            input_shape = shape_list(input_ids)\n-        elif inputs_embeds is not None:\n-            input_shape = shape_list(inputs_embeds)[:-1]\n-        else:\n-            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n-\n-        if attention_mask is None:\n-            attention_mask = tf.fill(dims=input_shape, value=1)\n-\n-        if token_type_ids is None:\n-            token_type_ids = tf.fill(dims=input_shape, value=0)\n-\n-        embedding_output = self.embeddings(\n-            input_ids=input_ids,\n-            position_ids=position_ids,\n-            token_type_ids=token_type_ids,\n-            inputs_embeds=inputs_embeds,\n-            training=training,\n-        )\n-\n-        # We create a 3D attention mask from a 2D tensor mask.\n-        # Sizes are [batch_size, 1, 1, to_seq_length]\n-        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n-        # this attention mask is more simple than the triangular masking of causal attention\n-        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n-        extended_attention_mask = tf.reshape(attention_mask, (input_shape[0], 1, 1, input_shape[1]))\n-\n-        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n-        # masked positions, this operation will create a tensor which is 0.0 for\n-        # positions we want to attend and -10000.0 for masked positions.\n-        # Since we are adding it to the raw scores before the softmax, this is\n-        # effectively the same as removing these entirely.\n-        extended_attention_mask = tf.cast(extended_attention_mask, dtype=embedding_output.dtype)\n-        one_cst = tf.constant(1.0, dtype=embedding_output.dtype)\n-        ten_thousand_cst = tf.constant(-10000.0, dtype=embedding_output.dtype)\n-        extended_attention_mask = tf.multiply(tf.subtract(one_cst, extended_attention_mask), ten_thousand_cst)\n-\n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        if head_mask is not None:\n-            raise NotImplementedError\n-        else:\n-            head_mask = [None] * self.config.num_hidden_layers\n-\n-        encoder_outputs = self.encoder(\n-            hidden_states=embedding_output,\n-            attention_mask=extended_attention_mask,\n-            head_mask=head_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-            training=training,\n-        )\n-\n-        sequence_output = encoder_outputs[0]\n-        pooled_output = self.pooler(inputs=sequence_output[:, 0]) if self.pooler is not None else None\n-\n-        if not return_dict:\n-            return (\n-                sequence_output,\n-                pooled_output,\n-            ) + encoder_outputs[1:]\n-\n-        return TFBaseModelOutputWithPooling(\n-            last_hidden_state=sequence_output,\n-            pooler_output=pooled_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n-        )\n-\n-    def build(self, input_shape=None):\n-        if self.built:\n-            return\n-        self.built = True\n-        if getattr(self, \"embeddings\", None) is not None:\n-            with tf.name_scope(self.embeddings.name):\n-                self.embeddings.build(None)\n-        if getattr(self, \"encoder\", None) is not None:\n-            with tf.name_scope(self.encoder.name):\n-                self.encoder.build(None)\n-        if getattr(self, \"pooler\", None) is not None:\n-            with tf.name_scope(self.pooler.name):\n-                self.pooler.build([None, None, self.config.hidden_size])\n-\n-\n-@dataclass\n-class TFAlbertForPreTrainingOutput(ModelOutput):\n-    \"\"\"\n-    Output type of [`TFAlbertForPreTraining`].\n-\n-    Args:\n-        prediction_logits (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        sop_logits (`tf.Tensor` of shape `(batch_size, 2)`):\n-            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n-            before SoftMax).\n-        hidden_states (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-    \"\"\"\n-\n-    loss: tf.Tensor | None = None\n-    prediction_logits: tf.Tensor | None = None\n-    sop_logits: tf.Tensor | None = None\n-    hidden_states: tuple[tf.Tensor] | None = None\n-    attentions: tuple[tf.Tensor] | None = None\n-\n-\n-ALBERT_START_DOCSTRING = r\"\"\"\n-\n-    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it\n-    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and\n-    behavior.\n-\n-    <Tip>\n-\n-    TensorFlow models and layers in `transformers` accept two formats as input:\n-\n-    - having all inputs as keyword arguments (like PyTorch models), or\n-    - having all inputs as a list, tuple or dict in the first positional argument.\n-\n-    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models\n-    and layers. Because of this support, when using methods like `model.fit()` things should \"just work\" for you - just\n-    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second\n-    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with\n-    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first\n-    positional argument:\n-\n-    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`\n-    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:\n-    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`\n-    - a dictionary with one or several input Tensors associated to the input names given in the docstring:\n-    `model({\"input_ids\": input_ids, \"token_type_ids\": token_type_ids})`\n-\n-    Note that when creating models and layers with\n-    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry\n-    about any of this, as you can just pass inputs like you would to any other Python function!\n-\n-    </Tip>\n-\n-    Args:\n-        config ([`AlbertConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-ALBERT_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`Numpy array` or `tf.Tensor` of shape `({0})`):\n-            Indices of input sequence tokens in the vocabulary.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.__call__`] and\n-            [`PreTrainedTokenizer.encode`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`Numpy array` or `tf.Tensor` of shape `({0})`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        token_type_ids (`Numpy array` or `tf.Tensor` of shape `({0})`, *optional*):\n-            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n-            1]`:\n-\n-            - 0 corresponds to a *sentence A* token,\n-            - 1 corresponds to a *sentence B* token.\n-\n-            [What are token type IDs?](../glossary#token-type-ids)\n-        position_ids (`Numpy array` or `tf.Tensor` of shape `({0})`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        head_mask (`Numpy array` or `tf.Tensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        inputs_embeds (`tf.Tensor` of shape `({0}, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the\n-            config will be used instead.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail. This argument can be used only in eager mode, in graph mode the value in the config will be\n-            used instead.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple. This argument can be used in\n-            eager mode, in graph mode the value will always be set to True.\n-        training (`bool`, *optional*, defaults to `False`):\n-            Whether or not to use the model in training mode (some modules like dropout modules have different\n-            behaviors between training and evaluation).\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare Albert Model transformer outputting raw hidden-states without any specific head on top.\",\n-    ALBERT_START_DOCSTRING,\n-)\n-class TFAlbertModel(TFAlbertPreTrainedModel):\n-    def __init__(self, config: AlbertConfig, *inputs, **kwargs):\n-        super().__init__(config, *inputs, **kwargs)\n-\n-        self.albert = TFAlbertMainLayer(config, name=\"albert\")\n-\n-    @unpack_inputs\n-    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=TFBaseModelOutputWithPooling,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n-    def call(\n-        self,\n-        input_ids: TFModelInputType | None = None,\n-        attention_mask: np.ndarray | tf.Tensor | None = None,\n-        token_type_ids: np.ndarray | tf.Tensor | None = None,\n-        position_ids: np.ndarray | tf.Tensor | None = None,\n-        head_mask: np.ndarray | tf.Tensor | None = None,\n-        inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: bool | None = None,\n-        output_hidden_states: bool | None = None,\n-        return_dict: bool | None = None,\n-        training: bool | None = False,\n-    ) -> TFBaseModelOutputWithPooling | tuple[tf.Tensor]:\n-        outputs = self.albert(\n-            input_ids=input_ids,\n-            attention_mask=attention_mask,\n-            token_type_ids=token_type_ids,\n-            position_ids=position_ids,\n-            head_mask=head_mask,\n-            inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-            training=training,\n-        )\n-\n-        return outputs\n-\n-    def build(self, input_shape=None):\n-        if self.built:\n-            return\n-        self.built = True\n-        if getattr(self, \"albert\", None) is not None:\n-            with tf.name_scope(self.albert.name):\n-                self.albert.build(None)\n-\n-\n-@add_start_docstrings(\n-    \"\"\"\n-    Albert Model with two heads on top for pretraining: a `masked language modeling` head and a `sentence order\n-    prediction` (classification) head.\n-    \"\"\",\n-    ALBERT_START_DOCSTRING,\n-)\n-class TFAlbertForPreTraining(TFAlbertPreTrainedModel, TFAlbertPreTrainingLoss):\n-    # names with a '.' represents the authorized unexpected/missing layers when a TF model is loaded from a PT model\n-    _keys_to_ignore_on_load_unexpected = [r\"predictions.decoder.weight\"]\n-\n-    def __init__(self, config: AlbertConfig, *inputs, **kwargs):\n-        super().__init__(config, *inputs, **kwargs)\n-\n-        self.num_labels = config.num_labels\n-\n-        self.albert = TFAlbertMainLayer(config, name=\"albert\")\n-        self.predictions = TFAlbertMLMHead(config, input_embeddings=self.albert.embeddings, name=\"predictions\")\n-        self.sop_classifier = TFAlbertSOPHead(config, name=\"sop_classifier\")\n-\n-    def get_lm_head(self) -> keras.layers.Layer:\n-        return self.predictions\n-\n-    @unpack_inputs\n-    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @replace_return_docstrings(output_type=TFAlbertForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)\n-    def call(\n-        self,\n-        input_ids: TFModelInputType | None = None,\n-        attention_mask: np.ndarray | tf.Tensor | None = None,\n-        token_type_ids: np.ndarray | tf.Tensor | None = None,\n-        position_ids: np.ndarray | tf.Tensor | None = None,\n-        head_mask: np.ndarray | tf.Tensor | None = None,\n-        inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: bool | None = None,\n-        output_hidden_states: bool | None = None,\n-        return_dict: bool | None = None,\n-        labels: np.ndarray | tf.Tensor | None = None,\n-        sentence_order_label: np.ndarray | tf.Tensor | None = None,\n-        training: bool | None = False,\n-    ) -> TFAlbertForPreTrainingOutput | tuple[tf.Tensor]:\n-        r\"\"\"\n-        Return:\n-\n-        Example:\n-\n-        ```python\n-        >>> import tensorflow as tf\n-        >>> from transformers import AutoTokenizer, TFAlbertForPreTraining\n-\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"albert/albert-base-v2\")\n-        >>> model = TFAlbertForPreTraining.from_pretrained(\"albert/albert-base-v2\")\n-\n-        >>> input_ids = tf.constant(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True))[None, :]\n-        >>> # Batch size 1\n-        >>> outputs = model(input_ids)\n-\n-        >>> prediction_logits = outputs.prediction_logits\n-        >>> sop_logits = outputs.sop_logits\n-        ```\"\"\"\n-\n-        outputs = self.albert(\n-            input_ids=input_ids,\n-            attention_mask=attention_mask,\n-            token_type_ids=token_type_ids,\n-            position_ids=position_ids,\n-            head_mask=head_mask,\n-            inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-            training=training,\n-        )\n-        sequence_output, pooled_output = outputs[:2]\n-        prediction_scores = self.predictions(hidden_states=sequence_output)\n-        sop_scores = self.sop_classifier(pooled_output=pooled_output, training=training)\n-        total_loss = None\n-\n-        if labels is not None and sentence_order_label is not None:\n-            d_labels = {\"labels\": labels}\n-            d_labels[\"sentence_order_label\"] = sentence_order_label\n-            total_loss = self.hf_compute_loss(labels=d_labels, logits=(prediction_scores, sop_scores))\n-\n-        if not return_dict:\n-            output = (prediction_scores, sop_scores) + outputs[2:]\n-            return ((total_loss,) + output) if total_loss is not None else output\n-\n-        return TFAlbertForPreTrainingOutput(\n-            loss=total_loss,\n-            prediction_logits=prediction_scores,\n-            sop_logits=sop_scores,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-    def build(self, input_shape=None):\n-        if self.built:\n-            return\n-        self.built = True\n-        if getattr(self, \"albert\", None) is not None:\n-            with tf.name_scope(self.albert.name):\n-                self.albert.build(None)\n-        if getattr(self, \"predictions\", None) is not None:\n-            with tf.name_scope(self.predictions.name):\n-                self.predictions.build(None)\n-        if getattr(self, \"sop_classifier\", None) is not None:\n-            with tf.name_scope(self.sop_classifier.name):\n-                self.sop_classifier.build(None)\n-\n-\n-class TFAlbertSOPHead(keras.layers.Layer):\n-    def __init__(self, config: AlbertConfig, **kwargs):\n-        super().__init__(**kwargs)\n-\n-        self.dropout = keras.layers.Dropout(rate=config.classifier_dropout_prob)\n-        self.classifier = keras.layers.Dense(\n-            units=config.num_labels,\n-            kernel_initializer=get_initializer(config.initializer_range),\n-            name=\"classifier\",\n-        )\n-        self.config = config\n-\n-    def call(self, pooled_output: tf.Tensor, training: bool) -> tf.Tensor:\n-        dropout_pooled_output = self.dropout(inputs=pooled_output, training=training)\n-        logits = self.classifier(inputs=dropout_pooled_output)\n-\n-        return logits\n-\n-    def build(self, input_shape=None):\n-        if self.built:\n-            return\n-        self.built = True\n-        if getattr(self, \"classifier\", None) is not None:\n-            with tf.name_scope(self.classifier.name):\n-                self.classifier.build([None, None, self.config.hidden_size])\n-\n-\n-@add_start_docstrings(\"\"\"Albert Model with a `language modeling` head on top.\"\"\", ALBERT_START_DOCSTRING)\n-class TFAlbertForMaskedLM(TFAlbertPreTrainedModel, TFMaskedLanguageModelingLoss):\n-    # names with a '.' represents the authorized unexpected/missing layers when a TF model is loaded from a PT model\n-    _keys_to_ignore_on_load_unexpected = [r\"pooler\", r\"predictions.decoder.weight\"]\n-\n-    def __init__(self, config: AlbertConfig, *inputs, **kwargs):\n-        super().__init__(config, *inputs, **kwargs)\n-\n-        self.albert = TFAlbertMainLayer(config, add_pooling_layer=False, name=\"albert\")\n-        self.predictions = TFAlbertMLMHead(config, input_embeddings=self.albert.embeddings, name=\"predictions\")\n-\n-    def get_lm_head(self) -> keras.layers.Layer:\n-        return self.predictions\n-\n-    @unpack_inputs\n-    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @replace_return_docstrings(output_type=TFMaskedLMOutput, config_class=_CONFIG_FOR_DOC)\n-    def call(\n-        self,\n-        input_ids: TFModelInputType | None = None,\n-        attention_mask: np.ndarray | tf.Tensor | None = None,\n-        token_type_ids: np.ndarray | tf.Tensor | None = None,\n-        position_ids: np.ndarray | tf.Tensor | None = None,\n-        head_mask: np.ndarray | tf.Tensor | None = None,\n-        inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: bool | None = None,\n-        output_hidden_states: bool | None = None,\n-        return_dict: bool | None = None,\n-        labels: np.ndarray | tf.Tensor | None = None,\n-        training: bool | None = False,\n-    ) -> TFMaskedLMOutput | tuple[tf.Tensor]:\n-        r\"\"\"\n-        labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n-            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n-            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n-\n-        Returns:\n-\n-        Example:\n-\n-        ```python\n-        >>> import tensorflow as tf\n-        >>> from transformers import AutoTokenizer, TFAlbertForMaskedLM\n-\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"albert/albert-base-v2\")\n-        >>> model = TFAlbertForMaskedLM.from_pretrained(\"albert/albert-base-v2\")\n-\n-        >>> # add mask_token\n-        >>> inputs = tokenizer(f\"The capital of [MASK] is Paris.\", return_tensors=\"tf\")\n-        >>> logits = model(**inputs).logits\n-\n-        >>> # retrieve index of [MASK]\n-        >>> mask_token_index = tf.where(inputs.input_ids == tokenizer.mask_token_id)[0][1]\n-        >>> predicted_token_id = tf.math.argmax(logits[0, mask_token_index], axis=-1)\n-        >>> tokenizer.decode(predicted_token_id)\n-        'france'\n-        ```\n-\n-        ```python\n-        >>> labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"tf\")[\"input_ids\"]\n-        >>> labels = tf.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n-        >>> outputs = model(**inputs, labels=labels)\n-        >>> round(float(outputs.loss), 2)\n-        0.81\n-        ```\n-        \"\"\"\n-        outputs = self.albert(\n-            input_ids=input_ids,\n-            attention_mask=attention_mask,\n-            token_type_ids=token_type_ids,\n-            position_ids=position_ids,\n-            head_mask=head_mask,\n-            inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-            training=training,\n-        )\n-        sequence_output = outputs[0]\n-        prediction_scores = self.predictions(hidden_states=sequence_output, training=training)\n-        loss = None if labels is None else self.hf_compute_loss(labels=labels, logits=prediction_scores)\n-\n-        if not return_dict:\n-            output = (prediction_scores,) + outputs[2:]\n-\n-            return ((loss,) + output) if loss is not None else output\n-\n-        return TFMaskedLMOutput(\n-            loss=loss,\n-            logits=prediction_scores,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-    def build(self, input_shape=None):\n-        if self.built:\n-            return\n-        self.built = True\n-        if getattr(self, \"albert\", None) is not None:\n-            with tf.name_scope(self.albert.name):\n-                self.albert.build(None)\n-        if getattr(self, \"predictions\", None) is not None:\n-            with tf.name_scope(self.predictions.name):\n-                self.predictions.build(None)\n-\n-\n-@add_start_docstrings(\n-    \"\"\"\n-    Albert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled\n-    output) e.g. for GLUE tasks.\n-    \"\"\",\n-    ALBERT_START_DOCSTRING,\n-)\n-class TFAlbertForSequenceClassification(TFAlbertPreTrainedModel, TFSequenceClassificationLoss):\n-    # names with a '.' represents the authorized unexpected/missing layers when a TF model is loaded from a PT model\n-    _keys_to_ignore_on_load_unexpected = [r\"predictions\"]\n-    _keys_to_ignore_on_load_missing = [r\"dropout\"]\n-\n-    def __init__(self, config: AlbertConfig, *inputs, **kwargs):\n-        super().__init__(config, *inputs, **kwargs)\n-\n-        self.num_labels = config.num_labels\n-\n-        self.albert = TFAlbertMainLayer(config, name=\"albert\")\n-        self.dropout = keras.layers.Dropout(rate=config.classifier_dropout_prob)\n-        self.classifier = keras.layers.Dense(\n-            units=config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"classifier\"\n-        )\n-        self.config = config\n-\n-    @unpack_inputs\n-    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=\"vumichien/albert-base-v2-imdb\",\n-        output_type=TFSequenceClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        expected_output=\"'LABEL_1'\",\n-        expected_loss=0.12,\n-    )\n-    def call(\n-        self,\n-        input_ids: TFModelInputType | None = None,\n-        attention_mask: np.ndarray | tf.Tensor | None = None,\n-        token_type_ids: np.ndarray | tf.Tensor | None = None,\n-        position_ids: np.ndarray | tf.Tensor | None = None,\n-        head_mask: np.ndarray | tf.Tensor | None = None,\n-        inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: bool | None = None,\n-        output_hidden_states: bool | None = None,\n-        return_dict: bool | None = None,\n-        labels: np.ndarray | tf.Tensor | None = None,\n-        training: bool | None = False,\n-    ) -> TFSequenceClassifierOutput | tuple[tf.Tensor]:\n-        r\"\"\"\n-        labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-        outputs = self.albert(\n-            input_ids=input_ids,\n-            attention_mask=attention_mask,\n-            token_type_ids=token_type_ids,\n-            position_ids=position_ids,\n-            head_mask=head_mask,\n-            inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-            training=training,\n-        )\n-        pooled_output = outputs[1]\n-        pooled_output = self.dropout(inputs=pooled_output, training=training)\n-        logits = self.classifier(inputs=pooled_output)\n-        loss = None if labels is None else self.hf_compute_loss(labels=labels, logits=logits)\n-\n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-\n-            return ((loss,) + output) if loss is not None else output\n-\n-        return TFSequenceClassifierOutput(\n-            loss=loss,\n-            logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-    def build(self, input_shape=None):\n-        if self.built:\n-            return\n-        self.built = True\n-        if getattr(self, \"albert\", None) is not None:\n-            with tf.name_scope(self.albert.name):\n-                self.albert.build(None)\n-        if getattr(self, \"classifier\", None) is not None:\n-            with tf.name_scope(self.classifier.name):\n-                self.classifier.build([None, None, self.config.hidden_size])\n-\n-\n-@add_start_docstrings(\n-    \"\"\"\n-    Albert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\n-    Named-Entity-Recognition (NER) tasks.\n-    \"\"\",\n-    ALBERT_START_DOCSTRING,\n-)\n-class TFAlbertForTokenClassification(TFAlbertPreTrainedModel, TFTokenClassificationLoss):\n-    # names with a '.' represents the authorized unexpected/missing layers when a TF model is loaded from a PT model\n-    _keys_to_ignore_on_load_unexpected = [r\"pooler\", r\"predictions\"]\n-    _keys_to_ignore_on_load_missing = [r\"dropout\"]\n-\n-    def __init__(self, config: AlbertConfig, *inputs, **kwargs):\n-        super().__init__(config, *inputs, **kwargs)\n-\n-        self.num_labels = config.num_labels\n-\n-        self.albert = TFAlbertMainLayer(config, add_pooling_layer=False, name=\"albert\")\n-        classifier_dropout_prob = (\n-            config.classifier_dropout_prob\n-            if config.classifier_dropout_prob is not None\n-            else config.hidden_dropout_prob\n-        )\n-        self.dropout = keras.layers.Dropout(rate=classifier_dropout_prob)\n-        self.classifier = keras.layers.Dense(\n-            units=config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"classifier\"\n-        )\n-        self.config = config\n-\n-    @unpack_inputs\n-    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=TFTokenClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n-    def call(\n-        self,\n-        input_ids: TFModelInputType | None = None,\n-        attention_mask: np.ndarray | tf.Tensor | None = None,\n-        token_type_ids: np.ndarray | tf.Tensor | None = None,\n-        position_ids: np.ndarray | tf.Tensor | None = None,\n-        head_mask: np.ndarray | tf.Tensor | None = None,\n-        inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: bool | None = None,\n-        output_hidden_states: bool | None = None,\n-        return_dict: bool | None = None,\n-        labels: np.ndarray | tf.Tensor | None = None,\n-        training: bool | None = False,\n-    ) -> TFTokenClassifierOutput | tuple[tf.Tensor]:\n-        r\"\"\"\n-        labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n-        \"\"\"\n-        outputs = self.albert(\n-            input_ids=input_ids,\n-            attention_mask=attention_mask,\n-            token_type_ids=token_type_ids,\n-            position_ids=position_ids,\n-            head_mask=head_mask,\n-            inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-            training=training,\n-        )\n-        sequence_output = outputs[0]\n-        sequence_output = self.dropout(inputs=sequence_output, training=training)\n-        logits = self.classifier(inputs=sequence_output)\n-        loss = None if labels is None else self.hf_compute_loss(labels=labels, logits=logits)\n-\n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-\n-            return ((loss,) + output) if loss is not None else output\n-\n-        return TFTokenClassifierOutput(\n-            loss=loss,\n-            logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-    def build(self, input_shape=None):\n-        if self.built:\n-            return\n-        self.built = True\n-        if getattr(self, \"albert\", None) is not None:\n-            with tf.name_scope(self.albert.name):\n-                self.albert.build(None)\n-        if getattr(self, \"classifier\", None) is not None:\n-            with tf.name_scope(self.classifier.name):\n-                self.classifier.build([None, None, self.config.hidden_size])\n-\n-\n-@add_start_docstrings(\n-    \"\"\"\n-    Albert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n-    layer on top of the hidden-states output to compute `span start logits` and `span end logits`).\n-    \"\"\",\n-    ALBERT_START_DOCSTRING,\n-)\n-class TFAlbertForQuestionAnswering(TFAlbertPreTrainedModel, TFQuestionAnsweringLoss):\n-    # names with a '.' represents the authorized unexpected/missing layers when a TF model is loaded from a PT model\n-    _keys_to_ignore_on_load_unexpected = [r\"pooler\", r\"predictions\"]\n-\n-    def __init__(self, config: AlbertConfig, *inputs, **kwargs):\n-        super().__init__(config, *inputs, **kwargs)\n-\n-        self.num_labels = config.num_labels\n-\n-        self.albert = TFAlbertMainLayer(config, add_pooling_layer=False, name=\"albert\")\n-        self.qa_outputs = keras.layers.Dense(\n-            units=config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"qa_outputs\"\n-        )\n-        self.config = config\n-\n-    @unpack_inputs\n-    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=\"vumichien/albert-base-v2-squad2\",\n-        output_type=TFQuestionAnsweringModelOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        qa_target_start_index=12,\n-        qa_target_end_index=13,\n-        expected_output=\"'a nice puppet'\",\n-        expected_loss=7.36,\n-    )\n-    def call(\n-        self,\n-        input_ids: TFModelInputType | None = None,\n-        attention_mask: np.ndarray | tf.Tensor | None = None,\n-        token_type_ids: np.ndarray | tf.Tensor | None = None,\n-        position_ids: np.ndarray | tf.Tensor | None = None,\n-        head_mask: np.ndarray | tf.Tensor | None = None,\n-        inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: bool | None = None,\n-        output_hidden_states: bool | None = None,\n-        return_dict: bool | None = None,\n-        start_positions: np.ndarray | tf.Tensor | None = None,\n-        end_positions: np.ndarray | tf.Tensor | None = None,\n-        training: bool | None = False,\n-    ) -> TFQuestionAnsweringModelOutput | tuple[tf.Tensor]:\n-        r\"\"\"\n-        start_positions (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        end_positions (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        \"\"\"\n-        outputs = self.albert(\n-            input_ids=input_ids,\n-            attention_mask=attention_mask,\n-            token_type_ids=token_type_ids,\n-            position_ids=position_ids,\n-            head_mask=head_mask,\n-            inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-            training=training,\n-        )\n-        sequence_output = outputs[0]\n-        logits = self.qa_outputs(inputs=sequence_output)\n-        start_logits, end_logits = tf.split(value=logits, num_or_size_splits=2, axis=-1)\n-        start_logits = tf.squeeze(input=start_logits, axis=-1)\n-        end_logits = tf.squeeze(input=end_logits, axis=-1)\n-        loss = None\n-\n-        if start_positions is not None and end_positions is not None:\n-            labels = {\"start_position\": start_positions}\n-            labels[\"end_position\"] = end_positions\n-            loss = self.hf_compute_loss(labels=labels, logits=(start_logits, end_logits))\n-\n-        if not return_dict:\n-            output = (start_logits, end_logits) + outputs[2:]\n-\n-            return ((loss,) + output) if loss is not None else output\n-\n-        return TFQuestionAnsweringModelOutput(\n-            loss=loss,\n-            start_logits=start_logits,\n-            end_logits=end_logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-    def build(self, input_shape=None):\n-        if self.built:\n-            return\n-        self.built = True\n-        if getattr(self, \"albert\", None) is not None:\n-            with tf.name_scope(self.albert.name):\n-                self.albert.build(None)\n-        if getattr(self, \"qa_outputs\", None) is not None:\n-            with tf.name_scope(self.qa_outputs.name):\n-                self.qa_outputs.build([None, None, self.config.hidden_size])\n-\n-\n-@add_start_docstrings(\n-    \"\"\"\n-    Albert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a\n-    softmax) e.g. for RocStories/SWAG tasks.\n-    \"\"\",\n-    ALBERT_START_DOCSTRING,\n-)\n-class TFAlbertForMultipleChoice(TFAlbertPreTrainedModel, TFMultipleChoiceLoss):\n-    # names with a '.' represents the authorized unexpected/missing layers when a TF model is loaded from a PT model\n-    _keys_to_ignore_on_load_unexpected = [r\"pooler\", r\"predictions\"]\n-    _keys_to_ignore_on_load_missing = [r\"dropout\"]\n-\n-    def __init__(self, config: AlbertConfig, *inputs, **kwargs):\n-        super().__init__(config, *inputs, **kwargs)\n-\n-        self.albert = TFAlbertMainLayer(config, name=\"albert\")\n-        self.dropout = keras.layers.Dropout(rate=config.hidden_dropout_prob)\n-        self.classifier = keras.layers.Dense(\n-            units=1, kernel_initializer=get_initializer(config.initializer_range), name=\"classifier\"\n-        )\n-        self.config = config\n-\n-    @unpack_inputs\n-    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=TFMultipleChoiceModelOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n-    def call(\n-        self,\n-        input_ids: TFModelInputType | None = None,\n-        attention_mask: np.ndarray | tf.Tensor | None = None,\n-        token_type_ids: np.ndarray | tf.Tensor | None = None,\n-        position_ids: np.ndarray | tf.Tensor | None = None,\n-        head_mask: np.ndarray | tf.Tensor | None = None,\n-        inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: bool | None = None,\n-        output_hidden_states: bool | None = None,\n-        return_dict: bool | None = None,\n-        labels: np.ndarray | tf.Tensor | None = None,\n-        training: bool | None = False,\n-    ) -> TFMultipleChoiceModelOutput | tuple[tf.Tensor]:\n-        r\"\"\"\n-        labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the multiple choice classification loss. Indices should be in `[0, ..., num_choices]`\n-            where `num_choices` is the size of the second dimension of the input tensors. (See `input_ids` above)\n-        \"\"\"\n-\n-        if input_ids is not None:\n-            num_choices = shape_list(input_ids)[1]\n-            seq_length = shape_list(input_ids)[2]\n-        else:\n-            num_choices = shape_list(inputs_embeds)[1]\n-            seq_length = shape_list(inputs_embeds)[2]\n-\n-        flat_input_ids = tf.reshape(input_ids, (-1, seq_length)) if input_ids is not None else None\n-        flat_attention_mask = (\n-            tf.reshape(tensor=attention_mask, shape=(-1, seq_length)) if attention_mask is not None else None\n-        )\n-        flat_token_type_ids = (\n-            tf.reshape(tensor=token_type_ids, shape=(-1, seq_length)) if token_type_ids is not None else None\n-        )\n-        flat_position_ids = (\n-            tf.reshape(tensor=position_ids, shape=(-1, seq_length)) if position_ids is not None else None\n-        )\n-        flat_inputs_embeds = (\n-            tf.reshape(tensor=inputs_embeds, shape=(-1, seq_length, shape_list(inputs_embeds)[3]))\n-            if inputs_embeds is not None\n-            else None\n-        )\n-        outputs = self.albert(\n-            input_ids=flat_input_ids,\n-            attention_mask=flat_attention_mask,\n-            token_type_ids=flat_token_type_ids,\n-            position_ids=flat_position_ids,\n-            head_mask=head_mask,\n-            inputs_embeds=flat_inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-            training=training,\n-        )\n-        pooled_output = outputs[1]\n-        pooled_output = self.dropout(inputs=pooled_output, training=training)\n-        logits = self.classifier(inputs=pooled_output)\n-        reshaped_logits = tf.reshape(tensor=logits, shape=(-1, num_choices))\n-        loss = None if labels is None else self.hf_compute_loss(labels=labels, logits=reshaped_logits)\n-\n-        if not return_dict:\n-            output = (reshaped_logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n-        return TFMultipleChoiceModelOutput(\n-            loss=loss,\n-            logits=reshaped_logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-    def build(self, input_shape=None):\n-        if self.built:\n-            return\n-        self.built = True\n-        if getattr(self, \"albert\", None) is not None:\n-            with tf.name_scope(self.albert.name):\n-                self.albert.build(None)\n-        if getattr(self, \"classifier\", None) is not None:\n-            with tf.name_scope(self.classifier.name):\n-                self.classifier.build([None, None, self.config.hidden_size])\n-\n-\n-__all__ = [\n-    \"TFAlbertPreTrainedModel\",\n-    \"TFAlbertModel\",\n-    \"TFAlbertForPreTraining\",\n-    \"TFAlbertForMaskedLM\",\n-    \"TFAlbertForSequenceClassification\",\n-    \"TFAlbertForTokenClassification\",\n-    \"TFAlbertForQuestionAnswering\",\n-    \"TFAlbertForMultipleChoice\",\n-    \"TFAlbertMainLayer\",\n-]"
        },
        {
            "sha": "839856b921191a8727c01229773a08fefed96b21",
            "filename": "src/transformers/models/align/modeling_align.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -516,8 +516,6 @@ def __init__(self, config):\n         self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n         self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n \n-        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n-        # any TensorFlow checkpoint file\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n         # position_ids (1, len position emb) is contiguous in memory and exported when serialized"
        },
        {
            "sha": "5cae61f12d7ff4e863ebb68b3b3a81bbfe7dd9ff",
            "filename": "src/transformers/models/altclip/modeling_altclip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -101,8 +101,6 @@ def __init__(self, config):\n         self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n         self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n \n-        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n-        # any TensorFlow checkpoint file\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n         # position_ids (1, len position emb) is contiguous in memory and exported when serialized"
        },
        {
            "sha": "f3f57b3d53c25b8d6a58d46249c2bb9d45174837",
            "filename": "src/transformers/models/aria/image_processing_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -232,10 +232,7 @@ def preprocess(\n         images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n-            raise ValueError(\n-                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n-                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n-            )\n+            raise ValueError(\"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, or torch.Tensor\")\n \n         validate_preprocess_arguments(\n             do_normalize=do_normalize,"
        },
        {
            "sha": "405c3d21dadbde9884f1e393a4907a4209145393",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -615,10 +615,7 @@ def preprocess(\n         images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n-            raise ValueError(\n-                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n-                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n-            )\n+            raise ValueError(\"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, or torch.Tensor\")\n \n         validate_preprocess_arguments(\n             do_normalize=do_normalize,"
        },
        {
            "sha": "b7ff6fa08e2fbbcf04453180df5799771966b4b0",
            "filename": "src/transformers/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Ffeature_extraction_audio_spectrogram_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Ffeature_extraction_audio_spectrogram_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Ffeature_extraction_audio_spectrogram_transformer.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -179,7 +179,6 @@ def __call__(\n             return_tensors (`str` or [`~utils.TensorType`], *optional*):\n                 If set, will return tensors instead of list of python integers. Acceptable values are:\n \n-                - `'tf'`: Return TensorFlow `tf.constant` objects.\n                 - `'pt'`: Return PyTorch `torch.Tensor` objects.\n                 - `'np'`: Return Numpy `np.ndarray` objects.\n         \"\"\""
        },
        {
            "sha": "6b86884b3b7b0819cc58157a6593aa5d53c883b9",
            "filename": "src/transformers/models/auto/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fauto%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fauto%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2F__init__.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -23,8 +23,6 @@\n     from .feature_extraction_auto import *\n     from .image_processing_auto import *\n     from .modeling_auto import *\n-    from .modeling_flax_auto import *\n-    from .modeling_tf_auto import *\n     from .processing_auto import *\n     from .tokenization_auto import *\n     from .video_processing_auto import *"
        },
        {
            "sha": "75c053643f666c60a3ef661e85ffd867c63c6989",
            "filename": "src/transformers/models/auto/auto_factory.py",
            "status": "modified",
            "additions": 1,
            "deletions": 221,
            "changes": 222,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -102,10 +102,6 @@\n                     - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\n                     - A path to a *directory* containing model weights saved using\n                       [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\n-                    - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In\n-                      this case, `from_tf` should be set to `True` and a configuration object should be provided as\n-                      `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a\n-                      PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n             model_args (additional positional arguments, *optional*):\n                 Will be passed along to the underlying model `__init__()` method.\n             config ([`PretrainedConfig`], *optional*):\n@@ -127,9 +123,6 @@\n             cache_dir (`str` or `os.PathLike`, *optional*):\n                 Path to a directory in which a downloaded pretrained model configuration should be cached if the\n                 standard cache should not be used.\n-            from_tf (`bool`, *optional*, defaults to `False`):\n-                Load the model weights from a TensorFlow checkpoint save file (see docstring of\n-                `pretrained_model_name_or_path` argument).\n             force_download (`bool`, *optional*, defaults to `False`):\n                 Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n                 cached versions if they exist.\n@@ -182,210 +175,6 @@\n         >>> model = BaseAutoModelClass.from_pretrained(\"checkpoint_placeholder\", output_attentions=True)\n         >>> model.config.output_attentions\n         True\n-\n-        >>> # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n-        >>> config = AutoConfig.from_pretrained(\"./tf_model/shortcut_placeholder_tf_model_config.json\")\n-        >>> model = BaseAutoModelClass.from_pretrained(\n-        ...     \"./tf_model/shortcut_placeholder_tf_checkpoint.ckpt.index\", from_tf=True, config=config\n-        ... )\n-        ```\n-\"\"\"\n-\n-FROM_PRETRAINED_TF_DOCSTRING = \"\"\"\n-        Instantiate one of the model classes of the library from a pretrained model.\n-\n-        The model class to instantiate is selected based on the `model_type` property of the config object (either\n-        passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by\n-        falling back to using pattern matching on `pretrained_model_name_or_path`:\n-\n-        List options\n-\n-        Args:\n-            pretrained_model_name_or_path (`str` or `os.PathLike`):\n-                Can be either:\n-\n-                    - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\n-                    - A path to a *directory* containing model weights saved using\n-                      [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\n-                    - A path or url to a *PyTorch state_dict save file* (e.g, `./pt_model/pytorch_model.bin`). In this\n-                      case, `from_pt` should be set to `True` and a configuration object should be provided as `config`\n-                      argument. This loading path is slower than converting the PyTorch model in a TensorFlow model\n-                      using the provided conversion scripts and loading the TensorFlow model afterwards.\n-            model_args (additional positional arguments, *optional*):\n-                Will be passed along to the underlying model `__init__()` method.\n-            config ([`PretrainedConfig`], *optional*):\n-                Configuration for the model to use instead of an automatically loaded configuration. Configuration can\n-                be automatically loaded when:\n-\n-                    - The model is a model provided by the library (loaded with the *model id* string of a pretrained\n-                      model).\n-                    - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the\n-                      save directory.\n-                    - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a\n-                      configuration JSON file named *config.json* is found in the directory.\n-            cache_dir (`str` or `os.PathLike`, *optional*):\n-                Path to a directory in which a downloaded pretrained model configuration should be cached if the\n-                standard cache should not be used.\n-            from_pt (`bool`, *optional*, defaults to `False`):\n-                Load the model weights from a PyTorch checkpoint save file (see docstring of\n-                `pretrained_model_name_or_path` argument).\n-            force_download (`bool`, *optional*, defaults to `False`):\n-                Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n-                cached versions if they exist.\n-            resume_download:\n-                Deprecated and ignored. All downloads are now resumed by default when possible.\n-                Will be removed in v5 of Transformers.\n-            proxies (`dict[str, str]`, *optional*):\n-                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n-                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n-            output_loading_info(`bool`, *optional*, defaults to `False`):\n-                Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.\n-            local_files_only(`bool`, *optional*, defaults to `False`):\n-                Whether or not to only look at local files (e.g., not try downloading the model).\n-            revision (`str`, *optional*, defaults to `\"main\"`):\n-                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n-                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n-                identifier allowed by git.\n-            trust_remote_code (`bool`, *optional*, defaults to `False`):\n-                Whether or not to allow for custom models defined on the Hub in their own modeling files. This option\n-                should only be set to `True` for repositories you trust and in which you have read the code, as it will\n-                execute code present on the Hub on your local machine.\n-            code_revision (`str`, *optional*, defaults to `\"main\"`):\n-                The specific revision to use for the code on the Hub, if the code leaves in a different repository than\n-                the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based\n-                system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier\n-                allowed by git.\n-            kwargs (additional keyword arguments, *optional*):\n-                Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,\n-                `output_attentions=True`). Behaves differently depending on whether a `config` is provided or\n-                automatically loaded:\n-\n-                    - If a configuration is provided with `config`, `**kwargs` will be directly passed to the\n-                      underlying model's `__init__` method (we assume all relevant updates to the configuration have\n-                      already been done)\n-                    - If a configuration is not provided, `kwargs` will be first passed to the configuration class\n-                      initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that\n-                      corresponds to a configuration attribute will be used to override said attribute with the\n-                      supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute\n-                      will be passed to the underlying model's `__init__` function.\n-\n-        Examples:\n-\n-        ```python\n-        >>> from transformers import AutoConfig, BaseAutoModelClass\n-\n-        >>> # Download model and configuration from huggingface.co and cache.\n-        >>> model = BaseAutoModelClass.from_pretrained(\"checkpoint_placeholder\")\n-\n-        >>> # Update configuration during loading\n-        >>> model = BaseAutoModelClass.from_pretrained(\"checkpoint_placeholder\", output_attentions=True)\n-        >>> model.config.output_attentions\n-        True\n-\n-        >>> # Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)\n-        >>> config = AutoConfig.from_pretrained(\"./pt_model/shortcut_placeholder_pt_model_config.json\")\n-        >>> model = BaseAutoModelClass.from_pretrained(\n-        ...     \"./pt_model/shortcut_placeholder_pytorch_model.bin\", from_pt=True, config=config\n-        ... )\n-        ```\n-\"\"\"\n-\n-FROM_PRETRAINED_FLAX_DOCSTRING = \"\"\"\n-        Instantiate one of the model classes of the library from a pretrained model.\n-\n-        The model class to instantiate is selected based on the `model_type` property of the config object (either\n-        passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by\n-        falling back to using pattern matching on `pretrained_model_name_or_path`:\n-\n-        List options\n-\n-        Args:\n-            pretrained_model_name_or_path (`str` or `os.PathLike`):\n-                Can be either:\n-\n-                    - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\n-                    - A path to a *directory* containing model weights saved using\n-                      [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\n-                    - A path or url to a *PyTorch state_dict save file* (e.g, `./pt_model/pytorch_model.bin`). In this\n-                      case, `from_pt` should be set to `True` and a configuration object should be provided as `config`\n-                      argument. This loading path is slower than converting the PyTorch model in a TensorFlow model\n-                      using the provided conversion scripts and loading the TensorFlow model afterwards.\n-            model_args (additional positional arguments, *optional*):\n-                Will be passed along to the underlying model `__init__()` method.\n-            config ([`PretrainedConfig`], *optional*):\n-                Configuration for the model to use instead of an automatically loaded configuration. Configuration can\n-                be automatically loaded when:\n-\n-                    - The model is a model provided by the library (loaded with the *model id* string of a pretrained\n-                      model).\n-                    - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the\n-                      save directory.\n-                    - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a\n-                      configuration JSON file named *config.json* is found in the directory.\n-            cache_dir (`str` or `os.PathLike`, *optional*):\n-                Path to a directory in which a downloaded pretrained model configuration should be cached if the\n-                standard cache should not be used.\n-            from_pt (`bool`, *optional*, defaults to `False`):\n-                Load the model weights from a PyTorch checkpoint save file (see docstring of\n-                `pretrained_model_name_or_path` argument).\n-            force_download (`bool`, *optional*, defaults to `False`):\n-                Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n-                cached versions if they exist.\n-            resume_download:\n-                Deprecated and ignored. All downloads are now resumed by default when possible.\n-                Will be removed in v5 of Transformers.\n-            proxies (`dict[str, str]`, *optional*):\n-                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n-                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n-            output_loading_info(`bool`, *optional*, defaults to `False`):\n-                Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.\n-            local_files_only(`bool`, *optional*, defaults to `False`):\n-                Whether or not to only look at local files (e.g., not try downloading the model).\n-            revision (`str`, *optional*, defaults to `\"main\"`):\n-                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n-                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n-                identifier allowed by git.\n-            trust_remote_code (`bool`, *optional*, defaults to `False`):\n-                Whether or not to allow for custom models defined on the Hub in their own modeling files. This option\n-                should only be set to `True` for repositories you trust and in which you have read the code, as it will\n-                execute code present on the Hub on your local machine.\n-            code_revision (`str`, *optional*, defaults to `\"main\"`):\n-                The specific revision to use for the code on the Hub, if the code leaves in a different repository than\n-                the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based\n-                system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier\n-                allowed by git.\n-            kwargs (additional keyword arguments, *optional*):\n-                Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,\n-                `output_attentions=True`). Behaves differently depending on whether a `config` is provided or\n-                automatically loaded:\n-\n-                    - If a configuration is provided with `config`, `**kwargs` will be directly passed to the\n-                      underlying model's `__init__` method (we assume all relevant updates to the configuration have\n-                      already been done)\n-                    - If a configuration is not provided, `kwargs` will be first passed to the configuration class\n-                      initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that\n-                      corresponds to a configuration attribute will be used to override said attribute with the\n-                      supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute\n-                      will be passed to the underlying model's `__init__` function.\n-\n-        Examples:\n-\n-        ```python\n-        >>> from transformers import AutoConfig, BaseAutoModelClass\n-\n-        >>> # Download model and configuration from huggingface.co and cache.\n-        >>> model = BaseAutoModelClass.from_pretrained(\"checkpoint_placeholder\")\n-\n-        >>> # Update configuration during loading\n-        >>> model = BaseAutoModelClass.from_pretrained(\"checkpoint_placeholder\", output_attentions=True)\n-        >>> model.config.output_attentions\n-        True\n-\n-        >>> # Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)\n-        >>> config = AutoConfig.from_pretrained(\"./pt_model/shortcut_placeholder_pt_model_config.json\")\n-        >>> model = BaseAutoModelClass.from_pretrained(\n-        ...     \"./pt_model/shortcut_placeholder_pytorch_model.bin\", from_pt=True, config=config\n-        ... )\n         ```\n \"\"\"\n \n@@ -400,10 +189,6 @@ def _get_model_class(config, model_mapping):\n     for arch in architectures:\n         if arch in name_to_model:\n             return name_to_model[arch]\n-        elif f\"TF{arch}\" in name_to_model:\n-            return name_to_model[f\"TF{arch}\"]\n-        elif f\"Flax{arch}\" in name_to_model:\n-            return name_to_model[f\"Flax{arch}\"]\n \n     # If not architecture is set in the config or match the supported models, the first element of the tuple is the\n     # defaults.\n@@ -696,12 +481,7 @@ def auto_class_update(cls, checkpoint_for_example: str = \"google-bert/bert-base-\n     from_config = replace_list_option_in_docstrings(model_mapping._model_mapping, use_model_types=False)(from_config)\n     cls.from_config = classmethod(from_config)\n \n-    if name.startswith(\"TF\"):\n-        from_pretrained_docstring = FROM_PRETRAINED_TF_DOCSTRING\n-    elif name.startswith(\"Flax\"):\n-        from_pretrained_docstring = FROM_PRETRAINED_FLAX_DOCSTRING\n-    else:\n-        from_pretrained_docstring = FROM_PRETRAINED_TORCH_DOCSTRING\n+    from_pretrained_docstring = FROM_PRETRAINED_TORCH_DOCSTRING\n     from_pretrained = copy_func(_BaseAutoModelClass.from_pretrained)\n     from_pretrained_docstring = insert_head_doc(from_pretrained_docstring, head_doc=head_doc)\n     from_pretrained_docstring = from_pretrained_docstring.replace(\"BaseAutoModelClass\", name)"
        },
        {
            "sha": "0588d03cb6cdb43b94cc3fcd73b1791d1a5ee809",
            "filename": "src/transformers/models/auto/modeling_flax_auto.py",
            "status": "removed",
            "additions": 0,
            "deletions": 413,
            "changes": 413,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_flax_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_flax_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_flax_auto.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,413 +0,0 @@\n-# coding=utf-8\n-# Copyright 2018 The Google Flax Team Authors and The HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Auto Model class.\"\"\"\n-\n-from collections import OrderedDict\n-\n-from ...utils import logging\n-from .auto_factory import _BaseAutoModelClass, _LazyAutoMapping, auto_class_update\n-from .configuration_auto import CONFIG_MAPPING_NAMES\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-FLAX_MODEL_MAPPING_NAMES = OrderedDict(\n-    [\n-        # Base model mapping\n-        (\"albert\", \"FlaxAlbertModel\"),\n-        (\"bart\", \"FlaxBartModel\"),\n-        (\"beit\", \"FlaxBeitModel\"),\n-        (\"bert\", \"FlaxBertModel\"),\n-        (\"big_bird\", \"FlaxBigBirdModel\"),\n-        (\"blenderbot\", \"FlaxBlenderbotModel\"),\n-        (\"blenderbot-small\", \"FlaxBlenderbotSmallModel\"),\n-        (\"bloom\", \"FlaxBloomModel\"),\n-        (\"clip\", \"FlaxCLIPModel\"),\n-        (\"dinov2\", \"FlaxDinov2Model\"),\n-        (\"distilbert\", \"FlaxDistilBertModel\"),\n-        (\"electra\", \"FlaxElectraModel\"),\n-        (\"gemma\", \"FlaxGemmaModel\"),\n-        (\"gpt-sw3\", \"FlaxGPT2Model\"),\n-        (\"gpt2\", \"FlaxGPT2Model\"),\n-        (\"gpt_neo\", \"FlaxGPTNeoModel\"),\n-        (\"gptj\", \"FlaxGPTJModel\"),\n-        (\"llama\", \"FlaxLlamaModel\"),\n-        (\"longt5\", \"FlaxLongT5Model\"),\n-        (\"marian\", \"FlaxMarianModel\"),\n-        (\"mbart\", \"FlaxMBartModel\"),\n-        (\"mistral\", \"FlaxMistralModel\"),\n-        (\"mt5\", \"FlaxMT5Model\"),\n-        (\"opt\", \"FlaxOPTModel\"),\n-        (\"pegasus\", \"FlaxPegasusModel\"),\n-        (\"regnet\", \"FlaxRegNetModel\"),\n-        (\"resnet\", \"FlaxResNetModel\"),\n-        (\"roberta\", \"FlaxRobertaModel\"),\n-        (\"roberta-prelayernorm\", \"FlaxRobertaPreLayerNormModel\"),\n-        (\"roformer\", \"FlaxRoFormerModel\"),\n-        (\"t5\", \"FlaxT5Model\"),\n-        (\"vision-text-dual-encoder\", \"FlaxVisionTextDualEncoderModel\"),\n-        (\"vit\", \"FlaxViTModel\"),\n-        (\"wav2vec2\", \"FlaxWav2Vec2Model\"),\n-        (\"whisper\", \"FlaxWhisperModel\"),\n-        (\"xglm\", \"FlaxXGLMModel\"),\n-        (\"xlm-roberta\", \"FlaxXLMRobertaModel\"),\n-    ]\n-)\n-\n-FLAX_MODEL_FOR_PRETRAINING_MAPPING_NAMES = OrderedDict(\n-    [\n-        # Model for pre-training mapping\n-        (\"albert\", \"FlaxAlbertForPreTraining\"),\n-        (\"bart\", \"FlaxBartForConditionalGeneration\"),\n-        (\"bert\", \"FlaxBertForPreTraining\"),\n-        (\"big_bird\", \"FlaxBigBirdForPreTraining\"),\n-        (\"electra\", \"FlaxElectraForPreTraining\"),\n-        (\"longt5\", \"FlaxLongT5ForConditionalGeneration\"),\n-        (\"mbart\", \"FlaxMBartForConditionalGeneration\"),\n-        (\"mt5\", \"FlaxMT5ForConditionalGeneration\"),\n-        (\"roberta\", \"FlaxRobertaForMaskedLM\"),\n-        (\"roberta-prelayernorm\", \"FlaxRobertaPreLayerNormForMaskedLM\"),\n-        (\"roformer\", \"FlaxRoFormerForMaskedLM\"),\n-        (\"t5\", \"FlaxT5ForConditionalGeneration\"),\n-        (\"wav2vec2\", \"FlaxWav2Vec2ForPreTraining\"),\n-        (\"whisper\", \"FlaxWhisperForConditionalGeneration\"),\n-        (\"xlm-roberta\", \"FlaxXLMRobertaForMaskedLM\"),\n-    ]\n-)\n-\n-FLAX_MODEL_FOR_MASKED_LM_MAPPING_NAMES = OrderedDict(\n-    [\n-        # Model for Masked LM mapping\n-        (\"albert\", \"FlaxAlbertForMaskedLM\"),\n-        (\"bart\", \"FlaxBartForConditionalGeneration\"),\n-        (\"bert\", \"FlaxBertForMaskedLM\"),\n-        (\"big_bird\", \"FlaxBigBirdForMaskedLM\"),\n-        (\"distilbert\", \"FlaxDistilBertForMaskedLM\"),\n-        (\"electra\", \"FlaxElectraForMaskedLM\"),\n-        (\"mbart\", \"FlaxMBartForConditionalGeneration\"),\n-        (\"roberta\", \"FlaxRobertaForMaskedLM\"),\n-        (\"roberta-prelayernorm\", \"FlaxRobertaPreLayerNormForMaskedLM\"),\n-        (\"roformer\", \"FlaxRoFormerForMaskedLM\"),\n-        (\"xlm-roberta\", \"FlaxXLMRobertaForMaskedLM\"),\n-    ]\n-)\n-\n-FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES = OrderedDict(\n-    [\n-        # Model for Seq2Seq Causal LM mapping\n-        (\"bart\", \"FlaxBartForConditionalGeneration\"),\n-        (\"blenderbot\", \"FlaxBlenderbotForConditionalGeneration\"),\n-        (\"blenderbot-small\", \"FlaxBlenderbotSmallForConditionalGeneration\"),\n-        (\"encoder-decoder\", \"FlaxEncoderDecoderModel\"),\n-        (\"longt5\", \"FlaxLongT5ForConditionalGeneration\"),\n-        (\"marian\", \"FlaxMarianMTModel\"),\n-        (\"mbart\", \"FlaxMBartForConditionalGeneration\"),\n-        (\"mt5\", \"FlaxMT5ForConditionalGeneration\"),\n-        (\"pegasus\", \"FlaxPegasusForConditionalGeneration\"),\n-        (\"t5\", \"FlaxT5ForConditionalGeneration\"),\n-    ]\n-)\n-\n-FLAX_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES = OrderedDict(\n-    [\n-        # Model for Image-classification\n-        (\"beit\", \"FlaxBeitForImageClassification\"),\n-        (\"dinov2\", \"FlaxDinov2ForImageClassification\"),\n-        (\"regnet\", \"FlaxRegNetForImageClassification\"),\n-        (\"resnet\", \"FlaxResNetForImageClassification\"),\n-        (\"vit\", \"FlaxViTForImageClassification\"),\n-    ]\n-)\n-\n-FLAX_MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES = OrderedDict(\n-    [\n-        (\"vision-encoder-decoder\", \"FlaxVisionEncoderDecoderModel\"),\n-    ]\n-)\n-\n-FLAX_MODEL_FOR_CAUSAL_LM_MAPPING_NAMES = OrderedDict(\n-    [\n-        # Model for Causal LM mapping\n-        (\"bart\", \"FlaxBartForCausalLM\"),\n-        (\"bert\", \"FlaxBertForCausalLM\"),\n-        (\"big_bird\", \"FlaxBigBirdForCausalLM\"),\n-        (\"bloom\", \"FlaxBloomForCausalLM\"),\n-        (\"electra\", \"FlaxElectraForCausalLM\"),\n-        (\"gemma\", \"FlaxGemmaForCausalLM\"),\n-        (\"gpt-sw3\", \"FlaxGPT2LMHeadModel\"),\n-        (\"gpt2\", \"FlaxGPT2LMHeadModel\"),\n-        (\"gpt_neo\", \"FlaxGPTNeoForCausalLM\"),\n-        (\"gptj\", \"FlaxGPTJForCausalLM\"),\n-        (\"llama\", \"FlaxLlamaForCausalLM\"),\n-        (\"mistral\", \"FlaxMistralForCausalLM\"),\n-        (\"opt\", \"FlaxOPTForCausalLM\"),\n-        (\"roberta\", \"FlaxRobertaForCausalLM\"),\n-        (\"roberta-prelayernorm\", \"FlaxRobertaPreLayerNormForCausalLM\"),\n-        (\"xglm\", \"FlaxXGLMForCausalLM\"),\n-        (\"xlm-roberta\", \"FlaxXLMRobertaForCausalLM\"),\n-    ]\n-)\n-\n-FLAX_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES = OrderedDict(\n-    [\n-        # Model for Sequence Classification mapping\n-        (\"albert\", \"FlaxAlbertForSequenceClassification\"),\n-        (\"bart\", \"FlaxBartForSequenceClassification\"),\n-        (\"bert\", \"FlaxBertForSequenceClassification\"),\n-        (\"big_bird\", \"FlaxBigBirdForSequenceClassification\"),\n-        (\"distilbert\", \"FlaxDistilBertForSequenceClassification\"),\n-        (\"electra\", \"FlaxElectraForSequenceClassification\"),\n-        (\"mbart\", \"FlaxMBartForSequenceClassification\"),\n-        (\"roberta\", \"FlaxRobertaForSequenceClassification\"),\n-        (\"roberta-prelayernorm\", \"FlaxRobertaPreLayerNormForSequenceClassification\"),\n-        (\"roformer\", \"FlaxRoFormerForSequenceClassification\"),\n-        (\"xlm-roberta\", \"FlaxXLMRobertaForSequenceClassification\"),\n-    ]\n-)\n-\n-FLAX_MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES = OrderedDict(\n-    [\n-        # Model for Question Answering mapping\n-        (\"albert\", \"FlaxAlbertForQuestionAnswering\"),\n-        (\"bart\", \"FlaxBartForQuestionAnswering\"),\n-        (\"bert\", \"FlaxBertForQuestionAnswering\"),\n-        (\"big_bird\", \"FlaxBigBirdForQuestionAnswering\"),\n-        (\"distilbert\", \"FlaxDistilBertForQuestionAnswering\"),\n-        (\"electra\", \"FlaxElectraForQuestionAnswering\"),\n-        (\"mbart\", \"FlaxMBartForQuestionAnswering\"),\n-        (\"roberta\", \"FlaxRobertaForQuestionAnswering\"),\n-        (\"roberta-prelayernorm\", \"FlaxRobertaPreLayerNormForQuestionAnswering\"),\n-        (\"roformer\", \"FlaxRoFormerForQuestionAnswering\"),\n-        (\"xlm-roberta\", \"FlaxXLMRobertaForQuestionAnswering\"),\n-    ]\n-)\n-\n-FLAX_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES = OrderedDict(\n-    [\n-        # Model for Token Classification mapping\n-        (\"albert\", \"FlaxAlbertForTokenClassification\"),\n-        (\"bert\", \"FlaxBertForTokenClassification\"),\n-        (\"big_bird\", \"FlaxBigBirdForTokenClassification\"),\n-        (\"distilbert\", \"FlaxDistilBertForTokenClassification\"),\n-        (\"electra\", \"FlaxElectraForTokenClassification\"),\n-        (\"roberta\", \"FlaxRobertaForTokenClassification\"),\n-        (\"roberta-prelayernorm\", \"FlaxRobertaPreLayerNormForTokenClassification\"),\n-        (\"roformer\", \"FlaxRoFormerForTokenClassification\"),\n-        (\"xlm-roberta\", \"FlaxXLMRobertaForTokenClassification\"),\n-    ]\n-)\n-\n-FLAX_MODEL_FOR_MULTIPLE_CHOICE_MAPPING_NAMES = OrderedDict(\n-    [\n-        # Model for Multiple Choice mapping\n-        (\"albert\", \"FlaxAlbertForMultipleChoice\"),\n-        (\"bert\", \"FlaxBertForMultipleChoice\"),\n-        (\"big_bird\", \"FlaxBigBirdForMultipleChoice\"),\n-        (\"distilbert\", \"FlaxDistilBertForMultipleChoice\"),\n-        (\"electra\", \"FlaxElectraForMultipleChoice\"),\n-        (\"roberta\", \"FlaxRobertaForMultipleChoice\"),\n-        (\"roberta-prelayernorm\", \"FlaxRobertaPreLayerNormForMultipleChoice\"),\n-        (\"roformer\", \"FlaxRoFormerForMultipleChoice\"),\n-        (\"xlm-roberta\", \"FlaxXLMRobertaForMultipleChoice\"),\n-    ]\n-)\n-\n-FLAX_MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING_NAMES = OrderedDict(\n-    [\n-        (\"bert\", \"FlaxBertForNextSentencePrediction\"),\n-    ]\n-)\n-\n-FLAX_MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES = OrderedDict(\n-    [\n-        (\"speech-encoder-decoder\", \"FlaxSpeechEncoderDecoderModel\"),\n-        (\"whisper\", \"FlaxWhisperForConditionalGeneration\"),\n-    ]\n-)\n-\n-FLAX_MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES = OrderedDict(\n-    [\n-        (\"whisper\", \"FlaxWhisperForAudioClassification\"),\n-    ]\n-)\n-\n-FLAX_MODEL_MAPPING = _LazyAutoMapping(CONFIG_MAPPING_NAMES, FLAX_MODEL_MAPPING_NAMES)\n-FLAX_MODEL_FOR_PRETRAINING_MAPPING = _LazyAutoMapping(CONFIG_MAPPING_NAMES, FLAX_MODEL_FOR_PRETRAINING_MAPPING_NAMES)\n-FLAX_MODEL_FOR_MASKED_LM_MAPPING = _LazyAutoMapping(CONFIG_MAPPING_NAMES, FLAX_MODEL_FOR_MASKED_LM_MAPPING_NAMES)\n-FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING = _LazyAutoMapping(\n-    CONFIG_MAPPING_NAMES, FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES\n-)\n-FLAX_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING = _LazyAutoMapping(\n-    CONFIG_MAPPING_NAMES, FLAX_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES\n-)\n-FLAX_MODEL_FOR_VISION_2_SEQ_MAPPING = _LazyAutoMapping(CONFIG_MAPPING_NAMES, FLAX_MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES)\n-FLAX_MODEL_FOR_CAUSAL_LM_MAPPING = _LazyAutoMapping(CONFIG_MAPPING_NAMES, FLAX_MODEL_FOR_CAUSAL_LM_MAPPING_NAMES)\n-FLAX_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING = _LazyAutoMapping(\n-    CONFIG_MAPPING_NAMES, FLAX_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES\n-)\n-FLAX_MODEL_FOR_QUESTION_ANSWERING_MAPPING = _LazyAutoMapping(\n-    CONFIG_MAPPING_NAMES, FLAX_MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES\n-)\n-FLAX_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING = _LazyAutoMapping(\n-    CONFIG_MAPPING_NAMES, FLAX_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES\n-)\n-FLAX_MODEL_FOR_MULTIPLE_CHOICE_MAPPING = _LazyAutoMapping(\n-    CONFIG_MAPPING_NAMES, FLAX_MODEL_FOR_MULTIPLE_CHOICE_MAPPING_NAMES\n-)\n-FLAX_MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING = _LazyAutoMapping(\n-    CONFIG_MAPPING_NAMES, FLAX_MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING_NAMES\n-)\n-FLAX_MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING = _LazyAutoMapping(\n-    CONFIG_MAPPING_NAMES, FLAX_MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES\n-)\n-FLAX_MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING = _LazyAutoMapping(\n-    CONFIG_MAPPING_NAMES, FLAX_MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES\n-)\n-\n-\n-class FlaxAutoModel(_BaseAutoModelClass):\n-    _model_mapping = FLAX_MODEL_MAPPING\n-\n-\n-FlaxAutoModel = auto_class_update(FlaxAutoModel)\n-\n-\n-class FlaxAutoModelForPreTraining(_BaseAutoModelClass):\n-    _model_mapping = FLAX_MODEL_FOR_PRETRAINING_MAPPING\n-\n-\n-FlaxAutoModelForPreTraining = auto_class_update(FlaxAutoModelForPreTraining, head_doc=\"pretraining\")\n-\n-\n-class FlaxAutoModelForCausalLM(_BaseAutoModelClass):\n-    _model_mapping = FLAX_MODEL_FOR_CAUSAL_LM_MAPPING\n-\n-\n-FlaxAutoModelForCausalLM = auto_class_update(FlaxAutoModelForCausalLM, head_doc=\"causal language modeling\")\n-\n-\n-class FlaxAutoModelForMaskedLM(_BaseAutoModelClass):\n-    _model_mapping = FLAX_MODEL_FOR_MASKED_LM_MAPPING\n-\n-\n-FlaxAutoModelForMaskedLM = auto_class_update(FlaxAutoModelForMaskedLM, head_doc=\"masked language modeling\")\n-\n-\n-class FlaxAutoModelForSeq2SeqLM(_BaseAutoModelClass):\n-    _model_mapping = FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING\n-\n-\n-FlaxAutoModelForSeq2SeqLM = auto_class_update(\n-    FlaxAutoModelForSeq2SeqLM,\n-    head_doc=\"sequence-to-sequence language modeling\",\n-    checkpoint_for_example=\"google-t5/t5-base\",\n-)\n-\n-\n-class FlaxAutoModelForSequenceClassification(_BaseAutoModelClass):\n-    _model_mapping = FLAX_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING\n-\n-\n-FlaxAutoModelForSequenceClassification = auto_class_update(\n-    FlaxAutoModelForSequenceClassification, head_doc=\"sequence classification\"\n-)\n-\n-\n-class FlaxAutoModelForQuestionAnswering(_BaseAutoModelClass):\n-    _model_mapping = FLAX_MODEL_FOR_QUESTION_ANSWERING_MAPPING\n-\n-\n-FlaxAutoModelForQuestionAnswering = auto_class_update(FlaxAutoModelForQuestionAnswering, head_doc=\"question answering\")\n-\n-\n-class FlaxAutoModelForTokenClassification(_BaseAutoModelClass):\n-    _model_mapping = FLAX_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING\n-\n-\n-FlaxAutoModelForTokenClassification = auto_class_update(\n-    FlaxAutoModelForTokenClassification, head_doc=\"token classification\"\n-)\n-\n-\n-class FlaxAutoModelForMultipleChoice(_BaseAutoModelClass):\n-    _model_mapping = FLAX_MODEL_FOR_MULTIPLE_CHOICE_MAPPING\n-\n-\n-FlaxAutoModelForMultipleChoice = auto_class_update(FlaxAutoModelForMultipleChoice, head_doc=\"multiple choice\")\n-\n-\n-class FlaxAutoModelForNextSentencePrediction(_BaseAutoModelClass):\n-    _model_mapping = FLAX_MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING\n-\n-\n-FlaxAutoModelForNextSentencePrediction = auto_class_update(\n-    FlaxAutoModelForNextSentencePrediction, head_doc=\"next sentence prediction\"\n-)\n-\n-\n-class FlaxAutoModelForImageClassification(_BaseAutoModelClass):\n-    _model_mapping = FLAX_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING\n-\n-\n-FlaxAutoModelForImageClassification = auto_class_update(\n-    FlaxAutoModelForImageClassification, head_doc=\"image classification\"\n-)\n-\n-\n-class FlaxAutoModelForVision2Seq(_BaseAutoModelClass):\n-    _model_mapping = FLAX_MODEL_FOR_VISION_2_SEQ_MAPPING\n-\n-\n-FlaxAutoModelForVision2Seq = auto_class_update(FlaxAutoModelForVision2Seq, head_doc=\"vision-to-text modeling\")\n-\n-\n-class FlaxAutoModelForSpeechSeq2Seq(_BaseAutoModelClass):\n-    _model_mapping = FLAX_MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING\n-\n-\n-FlaxAutoModelForSpeechSeq2Seq = auto_class_update(\n-    FlaxAutoModelForSpeechSeq2Seq, head_doc=\"sequence-to-sequence speech-to-text modeling\"\n-)\n-\n-__all__ = [\n-    \"FLAX_MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING\",\n-    \"FLAX_MODEL_FOR_CAUSAL_LM_MAPPING\",\n-    \"FLAX_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING\",\n-    \"FLAX_MODEL_FOR_MASKED_LM_MAPPING\",\n-    \"FLAX_MODEL_FOR_MULTIPLE_CHOICE_MAPPING\",\n-    \"FLAX_MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING\",\n-    \"FLAX_MODEL_FOR_PRETRAINING_MAPPING\",\n-    \"FLAX_MODEL_FOR_QUESTION_ANSWERING_MAPPING\",\n-    \"FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING\",\n-    \"FLAX_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING\",\n-    \"FLAX_MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING\",\n-    \"FLAX_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING\",\n-    \"FLAX_MODEL_FOR_VISION_2_SEQ_MAPPING\",\n-    \"FLAX_MODEL_MAPPING\",\n-    \"FlaxAutoModel\",\n-    \"FlaxAutoModelForCausalLM\",\n-    \"FlaxAutoModelForImageClassification\",\n-    \"FlaxAutoModelForMaskedLM\",\n-    \"FlaxAutoModelForMultipleChoice\",\n-    \"FlaxAutoModelForNextSentencePrediction\",\n-    \"FlaxAutoModelForPreTraining\",\n-    \"FlaxAutoModelForQuestionAnswering\",\n-    \"FlaxAutoModelForSeq2SeqLM\",\n-    \"FlaxAutoModelForSequenceClassification\",\n-    \"FlaxAutoModelForSpeechSeq2Seq\",\n-    \"FlaxAutoModelForTokenClassification\",\n-    \"FlaxAutoModelForVision2Seq\",\n-]"
        },
        {
            "sha": "cf39f4d7c9c40bd87a8e4c5e3037e2cbe3574a29",
            "filename": "src/transformers/models/auto/modeling_tf_auto.py",
            "status": "removed",
            "additions": 0,
            "deletions": 776,
            "changes": 776,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_tf_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_tf_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_tf_auto.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,776 +0,0 @@\n-# coding=utf-8\n-# Copyright 2018 The HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Auto Model class.\"\"\"\n-\n-import warnings\n-from collections import OrderedDict\n-\n-from ...utils import logging\n-from .auto_factory import _BaseAutoModelClass, _LazyAutoMapping, auto_class_update\n-from .configuration_auto import CONFIG_MAPPING_NAMES\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-TF_MODEL_MAPPING_NAMES = OrderedDict(\n-    [\n-        # Base model mapping\n-        (\"albert\", \"TFAlbertModel\"),\n-        (\"bart\", \"TFBartModel\"),\n-        (\"bert\", \"TFBertModel\"),\n-        (\"blenderbot\", \"TFBlenderbotModel\"),\n-        (\"blenderbot-small\", \"TFBlenderbotSmallModel\"),\n-        (\"blip\", \"TFBlipModel\"),\n-        (\"camembert\", \"TFCamembertModel\"),\n-        (\"clip\", \"TFCLIPModel\"),\n-        (\"convbert\", \"TFConvBertModel\"),\n-        (\"convnext\", \"TFConvNextModel\"),\n-        (\"convnextv2\", \"TFConvNextV2Model\"),\n-        (\"ctrl\", \"TFCTRLModel\"),\n-        (\"cvt\", \"TFCvtModel\"),\n-        (\"data2vec-vision\", \"TFData2VecVisionModel\"),\n-        (\"deberta\", \"TFDebertaModel\"),\n-        (\"deberta-v2\", \"TFDebertaV2Model\"),\n-        (\"deit\", \"TFDeiTModel\"),\n-        (\"distilbert\", \"TFDistilBertModel\"),\n-        (\"dpr\", \"TFDPRQuestionEncoder\"),\n-        (\"efficientformer\", \"TFEfficientFormerModel\"),\n-        (\"electra\", \"TFElectraModel\"),\n-        (\"esm\", \"TFEsmModel\"),\n-        (\"flaubert\", \"TFFlaubertModel\"),\n-        (\"funnel\", (\"TFFunnelModel\", \"TFFunnelBaseModel\")),\n-        (\"gpt-sw3\", \"TFGPT2Model\"),\n-        (\"gpt2\", \"TFGPT2Model\"),\n-        (\"gptj\", \"TFGPTJModel\"),\n-        (\"groupvit\", \"TFGroupViTModel\"),\n-        (\"hubert\", \"TFHubertModel\"),\n-        (\"idefics\", \"TFIdeficsModel\"),\n-        (\"layoutlm\", \"TFLayoutLMModel\"),\n-        (\"layoutlmv3\", \"TFLayoutLMv3Model\"),\n-        (\"led\", \"TFLEDModel\"),\n-        (\"longformer\", \"TFLongformerModel\"),\n-        (\"lxmert\", \"TFLxmertModel\"),\n-        (\"marian\", \"TFMarianModel\"),\n-        (\"mbart\", \"TFMBartModel\"),\n-        (\"mistral\", \"TFMistralModel\"),\n-        (\"mobilebert\", \"TFMobileBertModel\"),\n-        (\"mobilevit\", \"TFMobileViTModel\"),\n-        (\"mpnet\", \"TFMPNetModel\"),\n-        (\"mt5\", \"TFMT5Model\"),\n-        (\"openai-gpt\", \"TFOpenAIGPTModel\"),\n-        (\"opt\", \"TFOPTModel\"),\n-        (\"pegasus\", \"TFPegasusModel\"),\n-        (\"regnet\", \"TFRegNetModel\"),\n-        (\"rembert\", \"TFRemBertModel\"),\n-        (\"resnet\", \"TFResNetModel\"),\n-        (\"roberta\", \"TFRobertaModel\"),\n-        (\"roberta-prelayernorm\", \"TFRobertaPreLayerNormModel\"),\n-        (\"roformer\", \"TFRoFormerModel\"),\n-        (\"sam\", \"TFSamModel\"),\n-        (\"sam_vision_model\", \"TFSamVisionModel\"),\n-        (\"segformer\", \"TFSegformerModel\"),\n-        (\"speech_to_text\", \"TFSpeech2TextModel\"),\n-        (\"swiftformer\", \"TFSwiftFormerModel\"),\n-        (\"swin\", \"TFSwinModel\"),\n-        (\"t5\", \"TFT5Model\"),\n-        (\"tapas\", \"TFTapasModel\"),\n-        (\"transfo-xl\", \"TFTransfoXLModel\"),\n-        (\"vision-text-dual-encoder\", \"TFVisionTextDualEncoderModel\"),\n-        (\"vit\", \"TFViTModel\"),\n-        (\"vit_mae\", \"TFViTMAEModel\"),\n-        (\"wav2vec2\", \"TFWav2Vec2Model\"),\n-        (\"whisper\", \"TFWhisperModel\"),\n-        (\"xglm\", \"TFXGLMModel\"),\n-        (\"xlm\", \"TFXLMModel\"),\n-        (\"xlm-roberta\", \"TFXLMRobertaModel\"),\n-        (\"xlnet\", \"TFXLNetModel\"),\n-    ]\n-)\n-\n-TF_MODEL_FOR_PRETRAINING_MAPPING_NAMES = OrderedDict(\n-    [\n-        # Model for pre-training mapping\n-        (\"albert\", \"TFAlbertForPreTraining\"),\n-        (\"bart\", \"TFBartForConditionalGeneration\"),\n-        (\"bert\", \"TFBertForPreTraining\"),\n-        (\"camembert\", \"TFCamembertForMaskedLM\"),\n-        (\"ctrl\", \"TFCTRLLMHeadModel\"),\n-        (\"distilbert\", \"TFDistilBertForMaskedLM\"),\n-        (\"electra\", \"TFElectraForPreTraining\"),\n-        (\"flaubert\", \"TFFlaubertWithLMHeadModel\"),\n-        (\"funnel\", \"TFFunnelForPreTraining\"),\n-        (\"gpt-sw3\", \"TFGPT2LMHeadModel\"),\n-        (\"gpt2\", \"TFGPT2LMHeadModel\"),\n-        (\"idefics\", \"TFIdeficsForVisionText2Text\"),\n-        (\"layoutlm\", \"TFLayoutLMForMaskedLM\"),\n-        (\"lxmert\", \"TFLxmertForPreTraining\"),\n-        (\"mobilebert\", \"TFMobileBertForPreTraining\"),\n-        (\"mpnet\", \"TFMPNetForMaskedLM\"),\n-        (\"openai-gpt\", \"TFOpenAIGPTLMHeadModel\"),\n-        (\"roberta\", \"TFRobertaForMaskedLM\"),\n-        (\"roberta-prelayernorm\", \"TFRobertaPreLayerNormForMaskedLM\"),\n-        (\"t5\", \"TFT5ForConditionalGeneration\"),\n-        (\"tapas\", \"TFTapasForMaskedLM\"),\n-        (\"transfo-xl\", \"TFTransfoXLLMHeadModel\"),\n-        (\"vit_mae\", \"TFViTMAEForPreTraining\"),\n-        (\"xlm\", \"TFXLMWithLMHeadModel\"),\n-        (\"xlm-roberta\", \"TFXLMRobertaForMaskedLM\"),\n-        (\"xlnet\", \"TFXLNetLMHeadModel\"),\n-    ]\n-)\n-\n-TF_MODEL_WITH_LM_HEAD_MAPPING_NAMES = OrderedDict(\n-    [\n-        # Model with LM heads mapping\n-        (\"albert\", \"TFAlbertForMaskedLM\"),\n-        (\"bart\", \"TFBartForConditionalGeneration\"),\n-        (\"bert\", \"TFBertForMaskedLM\"),\n-        (\"camembert\", \"TFCamembertForMaskedLM\"),\n-        (\"convbert\", \"TFConvBertForMaskedLM\"),\n-        (\"ctrl\", \"TFCTRLLMHeadModel\"),\n-        (\"distilbert\", \"TFDistilBertForMaskedLM\"),\n-        (\"electra\", \"TFElectraForMaskedLM\"),\n-        (\"esm\", \"TFEsmForMaskedLM\"),\n-        (\"flaubert\", \"TFFlaubertWithLMHeadModel\"),\n-        (\"funnel\", \"TFFunnelForMaskedLM\"),\n-        (\"gpt-sw3\", \"TFGPT2LMHeadModel\"),\n-        (\"gpt2\", \"TFGPT2LMHeadModel\"),\n-        (\"gptj\", \"TFGPTJForCausalLM\"),\n-        (\"layoutlm\", \"TFLayoutLMForMaskedLM\"),\n-        (\"led\", \"TFLEDForConditionalGeneration\"),\n-        (\"longformer\", \"TFLongformerForMaskedLM\"),\n-        (\"marian\", \"TFMarianMTModel\"),\n-        (\"mobilebert\", \"TFMobileBertForMaskedLM\"),\n-        (\"mpnet\", \"TFMPNetForMaskedLM\"),\n-        (\"openai-gpt\", \"TFOpenAIGPTLMHeadModel\"),\n-        (\"rembert\", \"TFRemBertForMaskedLM\"),\n-        (\"roberta\", \"TFRobertaForMaskedLM\"),\n-        (\"roberta-prelayernorm\", \"TFRobertaPreLayerNormForMaskedLM\"),\n-        (\"roformer\", \"TFRoFormerForMaskedLM\"),\n-        (\"speech_to_text\", \"TFSpeech2TextForConditionalGeneration\"),\n-        (\"t5\", \"TFT5ForConditionalGeneration\"),\n-        (\"tapas\", \"TFTapasForMaskedLM\"),\n-        (\"transfo-xl\", \"TFTransfoXLLMHeadModel\"),\n-        (\"whisper\", \"TFWhisperForConditionalGeneration\"),\n-        (\"xlm\", \"TFXLMWithLMHeadModel\"),\n-        (\"xlm-roberta\", \"TFXLMRobertaForMaskedLM\"),\n-        (\"xlnet\", \"TFXLNetLMHeadModel\"),\n-    ]\n-)\n-\n-TF_MODEL_FOR_CAUSAL_LM_MAPPING_NAMES = OrderedDict(\n-    [\n-        # Model for Causal LM mapping\n-        (\"bert\", \"TFBertLMHeadModel\"),\n-        (\"camembert\", \"TFCamembertForCausalLM\"),\n-        (\"ctrl\", \"TFCTRLLMHeadModel\"),\n-        (\"gpt-sw3\", \"TFGPT2LMHeadModel\"),\n-        (\"gpt2\", \"TFGPT2LMHeadModel\"),\n-        (\"gptj\", \"TFGPTJForCausalLM\"),\n-        (\"mistral\", \"TFMistralForCausalLM\"),\n-        (\"openai-gpt\", \"TFOpenAIGPTLMHeadModel\"),\n-        (\"opt\", \"TFOPTForCausalLM\"),\n-        (\"rembert\", \"TFRemBertForCausalLM\"),\n-        (\"roberta\", \"TFRobertaForCausalLM\"),\n-        (\"roberta-prelayernorm\", \"TFRobertaPreLayerNormForCausalLM\"),\n-        (\"roformer\", \"TFRoFormerForCausalLM\"),\n-        (\"transfo-xl\", \"TFTransfoXLLMHeadModel\"),\n-        (\"xglm\", \"TFXGLMForCausalLM\"),\n-        (\"xlm\", \"TFXLMWithLMHeadModel\"),\n-        (\"xlm-roberta\", \"TFXLMRobertaForCausalLM\"),\n-        (\"xlnet\", \"TFXLNetLMHeadModel\"),\n-    ]\n-)\n-\n-TF_MODEL_FOR_MASKED_IMAGE_MODELING_MAPPING_NAMES = OrderedDict(\n-    [\n-        (\"deit\", \"TFDeiTForMaskedImageModeling\"),\n-        (\"swin\", \"TFSwinForMaskedImageModeling\"),\n-    ]\n-)\n-\n-TF_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES = OrderedDict(\n-    [\n-        # Model for Image-classsification\n-        (\"convnext\", \"TFConvNextForImageClassification\"),\n-        (\"convnextv2\", \"TFConvNextV2ForImageClassification\"),\n-        (\"cvt\", \"TFCvtForImageClassification\"),\n-        (\"data2vec-vision\", \"TFData2VecVisionForImageClassification\"),\n-        (\"deit\", (\"TFDeiTForImageClassification\", \"TFDeiTForImageClassificationWithTeacher\")),\n-        (\n-            \"efficientformer\",\n-            (\"TFEfficientFormerForImageClassification\", \"TFEfficientFormerForImageClassificationWithTeacher\"),\n-        ),\n-        (\"mobilevit\", \"TFMobileViTForImageClassification\"),\n-        (\"regnet\", \"TFRegNetForImageClassification\"),\n-        (\"resnet\", \"TFResNetForImageClassification\"),\n-        (\"segformer\", \"TFSegformerForImageClassification\"),\n-        (\"swiftformer\", \"TFSwiftFormerForImageClassification\"),\n-        (\"swin\", \"TFSwinForImageClassification\"),\n-        (\"vit\", \"TFViTForImageClassification\"),\n-    ]\n-)\n-\n-\n-TF_MODEL_FOR_ZERO_SHOT_IMAGE_CLASSIFICATION_MAPPING_NAMES = OrderedDict(\n-    [\n-        # Model for Zero Shot Image Classification mapping\n-        (\"blip\", \"TFBlipModel\"),\n-        (\"clip\", \"TFCLIPModel\"),\n-    ]\n-)\n-\n-\n-TF_MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING_NAMES = OrderedDict(\n-    [\n-        # Model for Semantic Segmentation mapping\n-        (\"data2vec-vision\", \"TFData2VecVisionForSemanticSegmentation\"),\n-        (\"mobilevit\", \"TFMobileViTForSemanticSegmentation\"),\n-        (\"segformer\", \"TFSegformerForSemanticSegmentation\"),\n-    ]\n-)\n-\n-TF_MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES = OrderedDict(\n-    [\n-        (\"blip\", \"TFBlipForConditionalGeneration\"),\n-        (\"vision-encoder-decoder\", \"TFVisionEncoderDecoderModel\"),\n-    ]\n-)\n-\n-TF_MODEL_FOR_MASKED_LM_MAPPING_NAMES = OrderedDict(\n-    [\n-        # Model for Masked LM mapping\n-        (\"albert\", \"TFAlbertForMaskedLM\"),\n-        (\"bert\", \"TFBertForMaskedLM\"),\n-        (\"camembert\", \"TFCamembertForMaskedLM\"),\n-        (\"convbert\", \"TFConvBertForMaskedLM\"),\n-        (\"deberta\", \"TFDebertaForMaskedLM\"),\n-        (\"deberta-v2\", \"TFDebertaV2ForMaskedLM\"),\n-        (\"distilbert\", \"TFDistilBertForMaskedLM\"),\n-        (\"electra\", \"TFElectraForMaskedLM\"),\n-        (\"esm\", \"TFEsmForMaskedLM\"),\n-        (\"flaubert\", \"TFFlaubertWithLMHeadModel\"),\n-        (\"funnel\", \"TFFunnelForMaskedLM\"),\n-        (\"layoutlm\", \"TFLayoutLMForMaskedLM\"),\n-        (\"longformer\", \"TFLongformerForMaskedLM\"),\n-        (\"mobilebert\", \"TFMobileBertForMaskedLM\"),\n-        (\"mpnet\", \"TFMPNetForMaskedLM\"),\n-        (\"rembert\", \"TFRemBertForMaskedLM\"),\n-        (\"roberta\", \"TFRobertaForMaskedLM\"),\n-        (\"roberta-prelayernorm\", \"TFRobertaPreLayerNormForMaskedLM\"),\n-        (\"roformer\", \"TFRoFormerForMaskedLM\"),\n-        (\"tapas\", \"TFTapasForMaskedLM\"),\n-        (\"xlm\", \"TFXLMWithLMHeadModel\"),\n-        (\"xlm-roberta\", \"TFXLMRobertaForMaskedLM\"),\n-    ]\n-)\n-\n-TF_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES = OrderedDict(\n-    [\n-        # Model for Seq2Seq Causal LM mapping\n-        (\"bart\", \"TFBartForConditionalGeneration\"),\n-        (\"blenderbot\", \"TFBlenderbotForConditionalGeneration\"),\n-        (\"blenderbot-small\", \"TFBlenderbotSmallForConditionalGeneration\"),\n-        (\"encoder-decoder\", \"TFEncoderDecoderModel\"),\n-        (\"led\", \"TFLEDForConditionalGeneration\"),\n-        (\"marian\", \"TFMarianMTModel\"),\n-        (\"mbart\", \"TFMBartForConditionalGeneration\"),\n-        (\"mt5\", \"TFMT5ForConditionalGeneration\"),\n-        (\"pegasus\", \"TFPegasusForConditionalGeneration\"),\n-        (\"t5\", \"TFT5ForConditionalGeneration\"),\n-    ]\n-)\n-\n-TF_MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES = OrderedDict(\n-    [\n-        (\"speech_to_text\", \"TFSpeech2TextForConditionalGeneration\"),\n-        (\"whisper\", \"TFWhisperForConditionalGeneration\"),\n-    ]\n-)\n-\n-TF_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES = OrderedDict(\n-    [\n-        # Model for Sequence Classification mapping\n-        (\"albert\", \"TFAlbertForSequenceClassification\"),\n-        (\"bart\", \"TFBartForSequenceClassification\"),\n-        (\"bert\", \"TFBertForSequenceClassification\"),\n-        (\"camembert\", \"TFCamembertForSequenceClassification\"),\n-        (\"convbert\", \"TFConvBertForSequenceClassification\"),\n-        (\"ctrl\", \"TFCTRLForSequenceClassification\"),\n-        (\"deberta\", \"TFDebertaForSequenceClassification\"),\n-        (\"deberta-v2\", \"TFDebertaV2ForSequenceClassification\"),\n-        (\"distilbert\", \"TFDistilBertForSequenceClassification\"),\n-        (\"electra\", \"TFElectraForSequenceClassification\"),\n-        (\"esm\", \"TFEsmForSequenceClassification\"),\n-        (\"flaubert\", \"TFFlaubertForSequenceClassification\"),\n-        (\"funnel\", \"TFFunnelForSequenceClassification\"),\n-        (\"gpt-sw3\", \"TFGPT2ForSequenceClassification\"),\n-        (\"gpt2\", \"TFGPT2ForSequenceClassification\"),\n-        (\"gptj\", \"TFGPTJForSequenceClassification\"),\n-        (\"layoutlm\", \"TFLayoutLMForSequenceClassification\"),\n-        (\"layoutlmv3\", \"TFLayoutLMv3ForSequenceClassification\"),\n-        (\"longformer\", \"TFLongformerForSequenceClassification\"),\n-        (\"mistral\", \"TFMistralForSequenceClassification\"),\n-        (\"mobilebert\", \"TFMobileBertForSequenceClassification\"),\n-        (\"mpnet\", \"TFMPNetForSequenceClassification\"),\n-        (\"openai-gpt\", \"TFOpenAIGPTForSequenceClassification\"),\n-        (\"rembert\", \"TFRemBertForSequenceClassification\"),\n-        (\"roberta\", \"TFRobertaForSequenceClassification\"),\n-        (\"roberta-prelayernorm\", \"TFRobertaPreLayerNormForSequenceClassification\"),\n-        (\"roformer\", \"TFRoFormerForSequenceClassification\"),\n-        (\"tapas\", \"TFTapasForSequenceClassification\"),\n-        (\"transfo-xl\", \"TFTransfoXLForSequenceClassification\"),\n-        (\"xlm\", \"TFXLMForSequenceClassification\"),\n-        (\"xlm-roberta\", \"TFXLMRobertaForSequenceClassification\"),\n-        (\"xlnet\", \"TFXLNetForSequenceClassification\"),\n-    ]\n-)\n-\n-TF_MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES = OrderedDict(\n-    [\n-        # Model for Question Answering mapping\n-        (\"albert\", \"TFAlbertForQuestionAnswering\"),\n-        (\"bert\", \"TFBertForQuestionAnswering\"),\n-        (\"camembert\", \"TFCamembertForQuestionAnswering\"),\n-        (\"convbert\", \"TFConvBertForQuestionAnswering\"),\n-        (\"deberta\", \"TFDebertaForQuestionAnswering\"),\n-        (\"deberta-v2\", \"TFDebertaV2ForQuestionAnswering\"),\n-        (\"distilbert\", \"TFDistilBertForQuestionAnswering\"),\n-        (\"electra\", \"TFElectraForQuestionAnswering\"),\n-        (\"flaubert\", \"TFFlaubertForQuestionAnsweringSimple\"),\n-        (\"funnel\", \"TFFunnelForQuestionAnswering\"),\n-        (\"gptj\", \"TFGPTJForQuestionAnswering\"),\n-        (\"layoutlmv3\", \"TFLayoutLMv3ForQuestionAnswering\"),\n-        (\"longformer\", \"TFLongformerForQuestionAnswering\"),\n-        (\"mobilebert\", \"TFMobileBertForQuestionAnswering\"),\n-        (\"mpnet\", \"TFMPNetForQuestionAnswering\"),\n-        (\"rembert\", \"TFRemBertForQuestionAnswering\"),\n-        (\"roberta\", \"TFRobertaForQuestionAnswering\"),\n-        (\"roberta-prelayernorm\", \"TFRobertaPreLayerNormForQuestionAnswering\"),\n-        (\"roformer\", \"TFRoFormerForQuestionAnswering\"),\n-        (\"xlm\", \"TFXLMForQuestionAnsweringSimple\"),\n-        (\"xlm-roberta\", \"TFXLMRobertaForQuestionAnswering\"),\n-        (\"xlnet\", \"TFXLNetForQuestionAnsweringSimple\"),\n-    ]\n-)\n-TF_MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES = OrderedDict([(\"wav2vec2\", \"TFWav2Vec2ForSequenceClassification\")])\n-\n-TF_MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING_NAMES = OrderedDict(\n-    [\n-        (\"layoutlm\", \"TFLayoutLMForQuestionAnswering\"),\n-        (\"layoutlmv3\", \"TFLayoutLMv3ForQuestionAnswering\"),\n-    ]\n-)\n-\n-\n-TF_MODEL_FOR_TABLE_QUESTION_ANSWERING_MAPPING_NAMES = OrderedDict(\n-    [\n-        # Model for Table Question Answering mapping\n-        (\"tapas\", \"TFTapasForQuestionAnswering\"),\n-    ]\n-)\n-\n-TF_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES = OrderedDict(\n-    [\n-        # Model for Token Classification mapping\n-        (\"albert\", \"TFAlbertForTokenClassification\"),\n-        (\"bert\", \"TFBertForTokenClassification\"),\n-        (\"camembert\", \"TFCamembertForTokenClassification\"),\n-        (\"convbert\", \"TFConvBertForTokenClassification\"),\n-        (\"deberta\", \"TFDebertaForTokenClassification\"),\n-        (\"deberta-v2\", \"TFDebertaV2ForTokenClassification\"),\n-        (\"distilbert\", \"TFDistilBertForTokenClassification\"),\n-        (\"electra\", \"TFElectraForTokenClassification\"),\n-        (\"esm\", \"TFEsmForTokenClassification\"),\n-        (\"flaubert\", \"TFFlaubertForTokenClassification\"),\n-        (\"funnel\", \"TFFunnelForTokenClassification\"),\n-        (\"layoutlm\", \"TFLayoutLMForTokenClassification\"),\n-        (\"layoutlmv3\", \"TFLayoutLMv3ForTokenClassification\"),\n-        (\"longformer\", \"TFLongformerForTokenClassification\"),\n-        (\"mobilebert\", \"TFMobileBertForTokenClassification\"),\n-        (\"mpnet\", \"TFMPNetForTokenClassification\"),\n-        (\"rembert\", \"TFRemBertForTokenClassification\"),\n-        (\"roberta\", \"TFRobertaForTokenClassification\"),\n-        (\"roberta-prelayernorm\", \"TFRobertaPreLayerNormForTokenClassification\"),\n-        (\"roformer\", \"TFRoFormerForTokenClassification\"),\n-        (\"xlm\", \"TFXLMForTokenClassification\"),\n-        (\"xlm-roberta\", \"TFXLMRobertaForTokenClassification\"),\n-        (\"xlnet\", \"TFXLNetForTokenClassification\"),\n-    ]\n-)\n-\n-TF_MODEL_FOR_MULTIPLE_CHOICE_MAPPING_NAMES = OrderedDict(\n-    [\n-        # Model for Multiple Choice mapping\n-        (\"albert\", \"TFAlbertForMultipleChoice\"),\n-        (\"bert\", \"TFBertForMultipleChoice\"),\n-        (\"camembert\", \"TFCamembertForMultipleChoice\"),\n-        (\"convbert\", \"TFConvBertForMultipleChoice\"),\n-        (\"deberta-v2\", \"TFDebertaV2ForMultipleChoice\"),\n-        (\"distilbert\", \"TFDistilBertForMultipleChoice\"),\n-        (\"electra\", \"TFElectraForMultipleChoice\"),\n-        (\"flaubert\", \"TFFlaubertForMultipleChoice\"),\n-        (\"funnel\", \"TFFunnelForMultipleChoice\"),\n-        (\"longformer\", \"TFLongformerForMultipleChoice\"),\n-        (\"mobilebert\", \"TFMobileBertForMultipleChoice\"),\n-        (\"mpnet\", \"TFMPNetForMultipleChoice\"),\n-        (\"rembert\", \"TFRemBertForMultipleChoice\"),\n-        (\"roberta\", \"TFRobertaForMultipleChoice\"),\n-        (\"roberta-prelayernorm\", \"TFRobertaPreLayerNormForMultipleChoice\"),\n-        (\"roformer\", \"TFRoFormerForMultipleChoice\"),\n-        (\"xlm\", \"TFXLMForMultipleChoice\"),\n-        (\"xlm-roberta\", \"TFXLMRobertaForMultipleChoice\"),\n-        (\"xlnet\", \"TFXLNetForMultipleChoice\"),\n-    ]\n-)\n-\n-TF_MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING_NAMES = OrderedDict(\n-    [\n-        (\"bert\", \"TFBertForNextSentencePrediction\"),\n-        (\"mobilebert\", \"TFMobileBertForNextSentencePrediction\"),\n-    ]\n-)\n-TF_MODEL_FOR_MASK_GENERATION_MAPPING_NAMES = OrderedDict(\n-    [\n-        (\"sam\", \"TFSamModel\"),\n-    ]\n-)\n-TF_MODEL_FOR_TEXT_ENCODING_MAPPING_NAMES = OrderedDict(\n-    [\n-        (\"albert\", \"TFAlbertModel\"),\n-        (\"bert\", \"TFBertModel\"),\n-        (\"convbert\", \"TFConvBertModel\"),\n-        (\"deberta\", \"TFDebertaModel\"),\n-        (\"deberta-v2\", \"TFDebertaV2Model\"),\n-        (\"distilbert\", \"TFDistilBertModel\"),\n-        (\"electra\", \"TFElectraModel\"),\n-        (\"flaubert\", \"TFFlaubertModel\"),\n-        (\"longformer\", \"TFLongformerModel\"),\n-        (\"mobilebert\", \"TFMobileBertModel\"),\n-        (\"mt5\", \"TFMT5EncoderModel\"),\n-        (\"rembert\", \"TFRemBertModel\"),\n-        (\"roberta\", \"TFRobertaModel\"),\n-        (\"roberta-prelayernorm\", \"TFRobertaPreLayerNormModel\"),\n-        (\"roformer\", \"TFRoFormerModel\"),\n-        (\"t5\", \"TFT5EncoderModel\"),\n-        (\"xlm\", \"TFXLMModel\"),\n-        (\"xlm-roberta\", \"TFXLMRobertaModel\"),\n-    ]\n-)\n-\n-TF_MODEL_MAPPING = _LazyAutoMapping(CONFIG_MAPPING_NAMES, TF_MODEL_MAPPING_NAMES)\n-TF_MODEL_FOR_PRETRAINING_MAPPING = _LazyAutoMapping(CONFIG_MAPPING_NAMES, TF_MODEL_FOR_PRETRAINING_MAPPING_NAMES)\n-TF_MODEL_WITH_LM_HEAD_MAPPING = _LazyAutoMapping(CONFIG_MAPPING_NAMES, TF_MODEL_WITH_LM_HEAD_MAPPING_NAMES)\n-TF_MODEL_FOR_CAUSAL_LM_MAPPING = _LazyAutoMapping(CONFIG_MAPPING_NAMES, TF_MODEL_FOR_CAUSAL_LM_MAPPING_NAMES)\n-TF_MODEL_FOR_MASKED_IMAGE_MODELING_MAPPING = _LazyAutoMapping(\n-    CONFIG_MAPPING_NAMES, TF_MODEL_FOR_MASKED_IMAGE_MODELING_MAPPING_NAMES\n-)\n-TF_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING = _LazyAutoMapping(\n-    CONFIG_MAPPING_NAMES, TF_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES\n-)\n-TF_MODEL_FOR_ZERO_SHOT_IMAGE_CLASSIFICATION_MAPPING = _LazyAutoMapping(\n-    CONFIG_MAPPING_NAMES, TF_MODEL_FOR_ZERO_SHOT_IMAGE_CLASSIFICATION_MAPPING_NAMES\n-)\n-TF_MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING = _LazyAutoMapping(\n-    CONFIG_MAPPING_NAMES, TF_MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING_NAMES\n-)\n-TF_MODEL_FOR_VISION_2_SEQ_MAPPING = _LazyAutoMapping(CONFIG_MAPPING_NAMES, TF_MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES)\n-TF_MODEL_FOR_MASKED_LM_MAPPING = _LazyAutoMapping(CONFIG_MAPPING_NAMES, TF_MODEL_FOR_MASKED_LM_MAPPING_NAMES)\n-TF_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING = _LazyAutoMapping(\n-    CONFIG_MAPPING_NAMES, TF_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES\n-)\n-TF_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING = _LazyAutoMapping(\n-    CONFIG_MAPPING_NAMES, TF_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES\n-)\n-TF_MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING = _LazyAutoMapping(\n-    CONFIG_MAPPING_NAMES, TF_MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES\n-)\n-TF_MODEL_FOR_QUESTION_ANSWERING_MAPPING = _LazyAutoMapping(\n-    CONFIG_MAPPING_NAMES, TF_MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES\n-)\n-TF_MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING = _LazyAutoMapping(\n-    CONFIG_MAPPING_NAMES, TF_MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING_NAMES\n-)\n-TF_MODEL_FOR_TABLE_QUESTION_ANSWERING_MAPPING = _LazyAutoMapping(\n-    CONFIG_MAPPING_NAMES, TF_MODEL_FOR_TABLE_QUESTION_ANSWERING_MAPPING_NAMES\n-)\n-TF_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING = _LazyAutoMapping(\n-    CONFIG_MAPPING_NAMES, TF_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES\n-)\n-TF_MODEL_FOR_MULTIPLE_CHOICE_MAPPING = _LazyAutoMapping(\n-    CONFIG_MAPPING_NAMES, TF_MODEL_FOR_MULTIPLE_CHOICE_MAPPING_NAMES\n-)\n-TF_MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING = _LazyAutoMapping(\n-    CONFIG_MAPPING_NAMES, TF_MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING_NAMES\n-)\n-TF_MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING = _LazyAutoMapping(\n-    CONFIG_MAPPING_NAMES, TF_MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES\n-)\n-\n-TF_MODEL_FOR_MASK_GENERATION_MAPPING = _LazyAutoMapping(\n-    CONFIG_MAPPING_NAMES, TF_MODEL_FOR_MASK_GENERATION_MAPPING_NAMES\n-)\n-\n-TF_MODEL_FOR_TEXT_ENCODING_MAPPING = _LazyAutoMapping(CONFIG_MAPPING_NAMES, TF_MODEL_FOR_TEXT_ENCODING_MAPPING_NAMES)\n-\n-\n-class TFAutoModelForMaskGeneration(_BaseAutoModelClass):\n-    _model_mapping = TF_MODEL_FOR_MASK_GENERATION_MAPPING\n-\n-\n-class TFAutoModelForTextEncoding(_BaseAutoModelClass):\n-    _model_mapping = TF_MODEL_FOR_TEXT_ENCODING_MAPPING\n-\n-\n-class TFAutoModel(_BaseAutoModelClass):\n-    _model_mapping = TF_MODEL_MAPPING\n-\n-\n-TFAutoModel = auto_class_update(TFAutoModel)\n-\n-\n-class TFAutoModelForAudioClassification(_BaseAutoModelClass):\n-    _model_mapping = TF_MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING\n-\n-\n-TFAutoModelForAudioClassification = auto_class_update(\n-    TFAutoModelForAudioClassification, head_doc=\"audio classification\"\n-)\n-\n-\n-class TFAutoModelForPreTraining(_BaseAutoModelClass):\n-    _model_mapping = TF_MODEL_FOR_PRETRAINING_MAPPING\n-\n-\n-TFAutoModelForPreTraining = auto_class_update(TFAutoModelForPreTraining, head_doc=\"pretraining\")\n-\n-\n-# Private on purpose, the public class will add the deprecation warnings.\n-class _TFAutoModelWithLMHead(_BaseAutoModelClass):\n-    _model_mapping = TF_MODEL_WITH_LM_HEAD_MAPPING\n-\n-\n-_TFAutoModelWithLMHead = auto_class_update(_TFAutoModelWithLMHead, head_doc=\"language modeling\")\n-\n-\n-class TFAutoModelForCausalLM(_BaseAutoModelClass):\n-    _model_mapping = TF_MODEL_FOR_CAUSAL_LM_MAPPING\n-\n-\n-TFAutoModelForCausalLM = auto_class_update(TFAutoModelForCausalLM, head_doc=\"causal language modeling\")\n-\n-\n-class TFAutoModelForMaskedImageModeling(_BaseAutoModelClass):\n-    _model_mapping = TF_MODEL_FOR_MASKED_IMAGE_MODELING_MAPPING\n-\n-\n-TFAutoModelForMaskedImageModeling = auto_class_update(\n-    TFAutoModelForMaskedImageModeling, head_doc=\"masked image modeling\"\n-)\n-\n-\n-class TFAutoModelForImageClassification(_BaseAutoModelClass):\n-    _model_mapping = TF_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING\n-\n-\n-TFAutoModelForImageClassification = auto_class_update(\n-    TFAutoModelForImageClassification, head_doc=\"image classification\"\n-)\n-\n-\n-class TFAutoModelForZeroShotImageClassification(_BaseAutoModelClass):\n-    _model_mapping = TF_MODEL_FOR_ZERO_SHOT_IMAGE_CLASSIFICATION_MAPPING\n-\n-\n-TFAutoModelForZeroShotImageClassification = auto_class_update(\n-    TFAutoModelForZeroShotImageClassification, head_doc=\"zero-shot image classification\"\n-)\n-\n-\n-class TFAutoModelForSemanticSegmentation(_BaseAutoModelClass):\n-    _model_mapping = TF_MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING\n-\n-\n-TFAutoModelForSemanticSegmentation = auto_class_update(\n-    TFAutoModelForSemanticSegmentation, head_doc=\"semantic segmentation\"\n-)\n-\n-\n-class TFAutoModelForVision2Seq(_BaseAutoModelClass):\n-    _model_mapping = TF_MODEL_FOR_VISION_2_SEQ_MAPPING\n-\n-\n-TFAutoModelForVision2Seq = auto_class_update(TFAutoModelForVision2Seq, head_doc=\"vision-to-text modeling\")\n-\n-\n-class TFAutoModelForMaskedLM(_BaseAutoModelClass):\n-    _model_mapping = TF_MODEL_FOR_MASKED_LM_MAPPING\n-\n-\n-TFAutoModelForMaskedLM = auto_class_update(TFAutoModelForMaskedLM, head_doc=\"masked language modeling\")\n-\n-\n-class TFAutoModelForSeq2SeqLM(_BaseAutoModelClass):\n-    _model_mapping = TF_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING\n-\n-\n-TFAutoModelForSeq2SeqLM = auto_class_update(\n-    TFAutoModelForSeq2SeqLM,\n-    head_doc=\"sequence-to-sequence language modeling\",\n-    checkpoint_for_example=\"google-t5/t5-base\",\n-)\n-\n-\n-class TFAutoModelForSequenceClassification(_BaseAutoModelClass):\n-    _model_mapping = TF_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING\n-\n-\n-TFAutoModelForSequenceClassification = auto_class_update(\n-    TFAutoModelForSequenceClassification, head_doc=\"sequence classification\"\n-)\n-\n-\n-class TFAutoModelForQuestionAnswering(_BaseAutoModelClass):\n-    _model_mapping = TF_MODEL_FOR_QUESTION_ANSWERING_MAPPING\n-\n-\n-TFAutoModelForQuestionAnswering = auto_class_update(TFAutoModelForQuestionAnswering, head_doc=\"question answering\")\n-\n-\n-class TFAutoModelForDocumentQuestionAnswering(_BaseAutoModelClass):\n-    _model_mapping = TF_MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING\n-\n-\n-TFAutoModelForDocumentQuestionAnswering = auto_class_update(\n-    TFAutoModelForDocumentQuestionAnswering,\n-    head_doc=\"document question answering\",\n-    checkpoint_for_example='impira/layoutlm-document-qa\", revision=\"52e01b3',\n-)\n-\n-\n-class TFAutoModelForTableQuestionAnswering(_BaseAutoModelClass):\n-    _model_mapping = TF_MODEL_FOR_TABLE_QUESTION_ANSWERING_MAPPING\n-\n-\n-TFAutoModelForTableQuestionAnswering = auto_class_update(\n-    TFAutoModelForTableQuestionAnswering,\n-    head_doc=\"table question answering\",\n-    checkpoint_for_example=\"google/tapas-base-finetuned-wtq\",\n-)\n-\n-\n-class TFAutoModelForTokenClassification(_BaseAutoModelClass):\n-    _model_mapping = TF_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING\n-\n-\n-TFAutoModelForTokenClassification = auto_class_update(\n-    TFAutoModelForTokenClassification, head_doc=\"token classification\"\n-)\n-\n-\n-class TFAutoModelForMultipleChoice(_BaseAutoModelClass):\n-    _model_mapping = TF_MODEL_FOR_MULTIPLE_CHOICE_MAPPING\n-\n-\n-TFAutoModelForMultipleChoice = auto_class_update(TFAutoModelForMultipleChoice, head_doc=\"multiple choice\")\n-\n-\n-class TFAutoModelForNextSentencePrediction(_BaseAutoModelClass):\n-    _model_mapping = TF_MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING\n-\n-\n-TFAutoModelForNextSentencePrediction = auto_class_update(\n-    TFAutoModelForNextSentencePrediction, head_doc=\"next sentence prediction\"\n-)\n-\n-\n-class TFAutoModelForSpeechSeq2Seq(_BaseAutoModelClass):\n-    _model_mapping = TF_MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING\n-\n-\n-TFAutoModelForSpeechSeq2Seq = auto_class_update(\n-    TFAutoModelForSpeechSeq2Seq, head_doc=\"sequence-to-sequence speech-to-text modeling\"\n-)\n-\n-\n-class TFAutoModelWithLMHead(_TFAutoModelWithLMHead):\n-    @classmethod\n-    def from_config(cls, config):\n-        warnings.warn(\n-            \"The class `TFAutoModelWithLMHead` is deprecated and will be removed in a future version. Please use\"\n-            \" `TFAutoModelForCausalLM` for causal language models, `TFAutoModelForMaskedLM` for masked language models\"\n-            \" and `TFAutoModelForSeq2SeqLM` for encoder-decoder models.\",\n-            FutureWarning,\n-        )\n-        return super().from_config(config)\n-\n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n-        warnings.warn(\n-            \"The class `TFAutoModelWithLMHead` is deprecated and will be removed in a future version. Please use\"\n-            \" `TFAutoModelForCausalLM` for causal language models, `TFAutoModelForMaskedLM` for masked language models\"\n-            \" and `TFAutoModelForSeq2SeqLM` for encoder-decoder models.\",\n-            FutureWarning,\n-        )\n-        return super().from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n-\n-\n-__all__ = [\n-    \"TF_MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING\",\n-    \"TF_MODEL_FOR_CAUSAL_LM_MAPPING\",\n-    \"TF_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING\",\n-    \"TF_MODEL_FOR_MASK_GENERATION_MAPPING\",\n-    \"TF_MODEL_FOR_MASKED_IMAGE_MODELING_MAPPING\",\n-    \"TF_MODEL_FOR_MASKED_LM_MAPPING\",\n-    \"TF_MODEL_FOR_MULTIPLE_CHOICE_MAPPING\",\n-    \"TF_MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING\",\n-    \"TF_MODEL_FOR_PRETRAINING_MAPPING\",\n-    \"TF_MODEL_FOR_QUESTION_ANSWERING_MAPPING\",\n-    \"TF_MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING\",\n-    \"TF_MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING\",\n-    \"TF_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING\",\n-    \"TF_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING\",\n-    \"TF_MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING\",\n-    \"TF_MODEL_FOR_TABLE_QUESTION_ANSWERING_MAPPING\",\n-    \"TF_MODEL_FOR_TEXT_ENCODING_MAPPING\",\n-    \"TF_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING\",\n-    \"TF_MODEL_FOR_VISION_2_SEQ_MAPPING\",\n-    \"TF_MODEL_FOR_ZERO_SHOT_IMAGE_CLASSIFICATION_MAPPING\",\n-    \"TF_MODEL_MAPPING\",\n-    \"TF_MODEL_WITH_LM_HEAD_MAPPING\",\n-    \"TFAutoModel\",\n-    \"TFAutoModelForAudioClassification\",\n-    \"TFAutoModelForCausalLM\",\n-    \"TFAutoModelForImageClassification\",\n-    \"TFAutoModelForMaskedImageModeling\",\n-    \"TFAutoModelForMaskedLM\",\n-    \"TFAutoModelForMaskGeneration\",\n-    \"TFAutoModelForMultipleChoice\",\n-    \"TFAutoModelForNextSentencePrediction\",\n-    \"TFAutoModelForPreTraining\",\n-    \"TFAutoModelForDocumentQuestionAnswering\",\n-    \"TFAutoModelForQuestionAnswering\",\n-    \"TFAutoModelForSemanticSegmentation\",\n-    \"TFAutoModelForSeq2SeqLM\",\n-    \"TFAutoModelForSequenceClassification\",\n-    \"TFAutoModelForSpeechSeq2Seq\",\n-    \"TFAutoModelForTableQuestionAnswering\",\n-    \"TFAutoModelForTextEncoding\",\n-    \"TFAutoModelForTokenClassification\",\n-    \"TFAutoModelForVision2Seq\",\n-    \"TFAutoModelForZeroShotImageClassification\",\n-    \"TFAutoModelWithLMHead\",\n-]"
        },
        {
            "sha": "aaede4e8e80ed0b12e15d34bf5d3e79422605003",
            "filename": "src/transformers/models/aya_vision/processing_aya_vision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Faya_vision%2Fprocessing_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Faya_vision%2Fprocessing_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fprocessing_aya_vision.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -160,10 +160,8 @@ def __call__(\n                 `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n             return_tensors (`str` or [`~utils.TensorType`], *optional*):\n                 If set, will return tensors of a particular framework. Acceptable values are:\n-                - `'tf'`: Return TensorFlow `tf.constant` objects.\n                 - `'pt'`: Return PyTorch `torch.Tensor` objects.\n                 - `'np'`: Return NumPy `np.ndarray` objects.\n-                - `'jax'`: Return JAX `jnp.ndarray` objects.\n \n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:"
        },
        {
            "sha": "475b85cf7e8e9bed9f79e1c8167b7142eef5c415",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -335,8 +335,6 @@ class BarkPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n         if isinstance(module, (nn.Linear,)):\n-            # Slightly different from the TF version which uses truncated_normal for initialization\n-            # cf https://github.com/pytorch/pytorch/pull/5617\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n                 module.bias.data.zero_()"
        },
        {
            "sha": "d268fb7d2b863c65107bacdd415aeb904d8de963",
            "filename": "src/transformers/models/bart/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbart%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbart%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2F__init__.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -20,8 +20,6 @@\n if TYPE_CHECKING:\n     from .configuration_bart import *\n     from .modeling_bart import *\n-    from .modeling_flax_bart import *\n-    from .modeling_tf_bart import *\n     from .tokenization_bart import *\n     from .tokenization_bart_fast import *\n else:"
        },
        {
            "sha": "e560bfa7d4a279ad2ab224baa7c0f4479696ca48",
            "filename": "src/transformers/models/bart/configuration_bart.py",
            "status": "modified",
            "additions": 9,
            "deletions": 13,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbart%2Fconfiguration_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbart%2Fconfiguration_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fconfiguration_bart.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e",
            "patch": "@@ -17,13 +17,13 @@\n import warnings\n from collections import OrderedDict\n from collections.abc import Mapping\n-from typing import Any, Optional\n+from typing import Any\n \n from ... import PreTrainedTokenizer\n from ...configuration_utils import PretrainedConfig\n from ...onnx import OnnxConfig, OnnxConfigWithPast, OnnxSeq2SeqConfigWithPast\n from ...onnx.utils import compute_effective_axis_dimension\n-from ...utils import TensorType, is_torch_available, logging\n+from ...utils import is_torch_available, logging\n \n \n logger = logging.get_logger(__name__)\n@@ -244,16 +244,15 @@ def _generate_dummy_inputs_for_default_and_seq2seq_lm(\n         batch_size: int = -1,\n         seq_length: int = -1,\n         is_pair: bool = False,\n-        framework: Optional[TensorType] = None,\n     ) -> Mapping[str, Any]:\n         encoder_inputs = self._generate_dummy_inputs_for_sequence_classification_and_question_answering(\n-            tokenizer, batch_size, seq_length, is_pair, framework\n+            tokenizer, batch_size, seq_length, is_pair\n         )\n \n         # Generate decoder inputs\n         decoder_seq_length = seq_length if not self.use_past else 1\n         decoder_inputs = self._generate_dummy_inputs_for_sequence_classification_and_question_answering(\n-            tokenizer, batch_size, decoder_seq_length, is_pair, framework\n+            tokenizer, batch_size, decoder_seq_length, is_pair\n         )\n         decoder_inputs = {f\"decoder_{name}\": tensor for name, tensor in decoder_inputs.items()}\n         common_inputs = dict(**encoder_inputs, **decoder_inputs)\n@@ -312,10 +311,9 @@ def _generate_dummy_inputs_for_causal_lm(\n         batch_size: int = -1,\n         seq_length: int = -1,\n         is_pair: bool = False,\n-        framework: Optional[TensorType] = None,\n     ) -> Mapping[str, Any]:\n         common_inputs = self._generate_dummy_inputs_for_sequence_classification_and_question_answering(\n-            tokenizer, batch_size, seq_length, is_pair, framework\n+            tokenizer, batch_size, seq_length, is_pair\n         )\n \n         if self.use_past:\n@@ -350,7 +348,6 @@ def _generate_dummy_inputs_for_sequence_classification_and_question_answering(\n         batch_size: int = -1,\n         seq_length: int = -1,\n         is_pair: bool = False,\n-        framework: Optional[TensorType] = None,\n     ) -> Mapping[str, Any]:\n         # Copied from OnnxConfig.generate_dummy_inputs\n         # Did not use super(OnnxConfigWithPast, self).generate_dummy_inputs for code clarity.\n@@ -367,7 +364,7 @@ def _generate_dummy_inputs_for_sequence_classification_and_question_answering(\n \n         # Generate dummy inputs according to compute batch and sequence\n         dummy_input = [\" \".join([tokenizer.unk_token]) * seq_length] * batch_size\n-        common_inputs = dict(tokenizer(dummy_input, return_tensors=framework))\n+        common_inputs = dict(tokenizer(dummy_input, return_tensors=\"pt\"))\n         return common_inputs\n \n     def generate_dummy_inputs(\n@@ -376,20 +373,19 @@ def generate_dummy_inputs(\n         batch_size: int = -1,\n         seq_length: int = -1,\n         is_pair: bool = False,\n-        framework: Optional[TensorType] = None,\n     ) -> Mapping[str, Any]:\n         if self.task in [\"default\", \"seq2seq-lm\"]:\n             common_inputs = self._generate_dummy_inputs_for_default_and_seq2seq_lm(\n-                tokenizer, batch_size=batch_size, seq_length=seq_length, is_pair=is_pair, framework=framework\n+                tokenizer, batch_size=batch_size, seq_length=seq_length, is_pair=is_pair\n             )\n \n         elif self.task == \"causal-lm\":\n             common_inputs = self._generate_dummy_inputs_for_causal_lm(\n-                tokenizer, batch_size=batch_size, seq_length=seq_length, is_pair=is_pair, framework=framework\n+                tokenizer, batch_size=batch_size, seq_length=seq_length, is_pair=is_pair\n             )\n         else:\n             common_inputs = self._generate_dummy_inputs_for_sequence_classification_and_question_answering(\n-                tokenizer, batch_size=batch_size, seq_length=seq_length, is_pair=is_pair, framework=framework\n+                tokenizer, batch_size=batch_size, seq_length=seq_length, is_pair=is_pair\n             )\n \n         return common_inputs"
        },
        {
            "sha": "818254f3bfa1e95685fdcde8af8f074edb69761f",
            "filename": "src/transformers/models/bart/modeling_flax_bart.py",
            "status": "removed",
            "additions": 0,
            "deletions": 2006,
            "changes": 2006,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_flax_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_flax_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_flax_bart.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc",
            "patch": "@@ -1,2006 +0,0 @@\n-# coding=utf-8\n-# Copyright 2021 The Fairseq Authors and The Google Flax Team Authors And The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Flax Bart model.\"\"\"\n-\n-import math\n-import random\n-from functools import partial\n-from typing import Callable, Optional\n-\n-import flax.linen as nn\n-import jax\n-import jax.numpy as jnp\n-from flax.core.frozen_dict import FrozenDict, freeze, unfreeze\n-from flax.linen import combine_masks, make_causal_mask\n-from flax.linen.attention import dot_product_attention_weights\n-from flax.traverse_util import flatten_dict, unflatten_dict\n-from jax import lax\n-from jax.random import PRNGKey\n-\n-from ...modeling_flax_outputs import (\n-    FlaxBaseModelOutput,\n-    FlaxBaseModelOutputWithPastAndCrossAttentions,\n-    FlaxCausalLMOutputWithCrossAttentions,\n-    FlaxSeq2SeqLMOutput,\n-    FlaxSeq2SeqModelOutput,\n-    FlaxSeq2SeqQuestionAnsweringModelOutput,\n-    FlaxSeq2SeqSequenceClassifierOutput,\n-)\n-from ...modeling_flax_utils import (\n-    ACT2FN,\n-    FlaxPreTrainedModel,\n-    append_call_sample_docstring,\n-    append_replace_return_docstrings,\n-    overwrite_call_docstring,\n-)\n-from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n-from .configuration_bart import BartConfig\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-_CHECKPOINT_FOR_DOC = \"facebook/bart-base\"\n-_CONFIG_FOR_DOC = \"BartConfig\"\n-\n-\n-BART_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a Flax Linen\n-    [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html) subclass. Use it as a\n-    regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.\n-\n-    Finally, this model supports inherent JAX features such as:\n-\n-    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)\n-    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)\n-    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)\n-    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)\n-\n-    Parameters:\n-        config ([`BartConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.\n-        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):\n-            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and\n-            `jax.numpy.bfloat16` (on TPUs).\n-\n-            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If\n-            specified all the computation will be performed with the given `dtype`.\n-\n-            **Note that this only specifies the dtype of the computation and does not influence the dtype of model\n-            parameters.**\n-\n-            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and\n-            [`~FlaxPreTrainedModel.to_bf16`].\n-\"\"\"\n-\n-BART_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`jnp.ndarray` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        decoder_input_ids (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*):\n-            Indices of decoder input sequence tokens in the vocabulary.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are decoder input IDs?](../glossary#decoder-input-ids)\n-\n-            For translation and summarization training, `decoder_input_ids` should be provided. If no\n-            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right\n-            for denoising pre-training following the paper.\n-        decoder_attention_mask (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*):\n-            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n-            be used by default.\n-\n-            If you want to change padding behavior, you should modify to your needs. See diagram 1 in [the\n-            paper](https://huggingface.co/papers/1910.13461) for more information on the default strategy.\n-        position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-        decoder_position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the\n-            range `[0, config.max_position_embeddings - 1]`.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n-BART_ENCODE_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`jnp.ndarray` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-BART_DECODE_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        decoder_input_ids (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`):\n-            Indices of decoder input sequence tokens in the vocabulary.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are decoder input IDs?](../glossary#decoder-input-ids)\n-\n-            For translation and summarization training, `decoder_input_ids` should be provided. If no\n-            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right\n-            for denoising pre-training following the paper.\n-        encoder_outputs (`tuple(tuple(jnp.ndarray)`):\n-            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)\n-            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of\n-            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n-        encoder_attention_mask (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        decoder_attention_mask (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*):\n-            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n-            be used by default.\n-\n-            If you want to change padding behavior, you should modify to your needs. See diagram 1 in [the\n-            paper](https://huggingface.co/papers/1910.13461) for more information on the default strategy.\n-        decoder_position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the\n-            range `[0, config.max_position_embeddings - 1]`.\n-        past_key_values (`dict[str, np.ndarray]`, *optional*, returned by `init_cache` or when passing previous `past_key_values`):\n-            Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast\n-            auto-regressive decoding. Pre-computed key and value hidden-states are of shape *[batch_size, max_length]*.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n-def shift_tokens_right(input_ids: jnp.ndarray, pad_token_id: int, decoder_start_token_id: int) -> jnp.ndarray:\n-    \"\"\"\n-    Shift input ids one token to the right.\n-    \"\"\"\n-    shifted_input_ids = jnp.zeros_like(input_ids)\n-    shifted_input_ids = shifted_input_ids.at[:, 1:].set(input_ids[:, :-1])\n-    shifted_input_ids = shifted_input_ids.at[:, 0].set(decoder_start_token_id)\n-\n-    shifted_input_ids = jnp.where(shifted_input_ids == -100, pad_token_id, shifted_input_ids)\n-    return shifted_input_ids\n-\n-\n-class FlaxBartAttention(nn.Module):\n-    config: BartConfig\n-    embed_dim: int\n-    num_heads: int\n-    dropout: float = 0.0\n-    causal: bool = False\n-    bias: bool = True\n-    dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n-\n-    def setup(self) -> None:\n-        self.head_dim = self.embed_dim // self.num_heads\n-        if self.head_dim * self.num_heads != self.embed_dim:\n-            raise ValueError(\n-                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}\"\n-                f\" and `num_heads`: {self.num_heads}).\"\n-            )\n-\n-        dense = partial(\n-            nn.Dense,\n-            self.embed_dim,\n-            use_bias=self.bias,\n-            dtype=self.dtype,\n-            kernel_init=jax.nn.initializers.normal(self.config.init_std),\n-        )\n-\n-        self.q_proj, self.k_proj, self.v_proj = dense(), dense(), dense()\n-        self.out_proj = dense()\n-\n-        self.dropout_layer = nn.Dropout(rate=self.dropout)\n-\n-        if self.causal:\n-            self.causal_mask = make_causal_mask(\n-                jnp.ones((1, self.config.max_position_embeddings), dtype=\"bool\"), dtype=\"bool\"\n-            )\n-\n-    def _split_heads(self, hidden_states):\n-        return hidden_states.reshape(hidden_states.shape[:2] + (self.num_heads, self.head_dim))\n-\n-    def _merge_heads(self, hidden_states):\n-        return hidden_states.reshape(hidden_states.shape[:2] + (self.embed_dim,))\n-\n-    @nn.compact\n-    def _concatenate_to_cache(self, key, value, query, attention_mask):\n-        \"\"\"\n-        This function takes projected key, value states from a single input token and concatenates the states to cached\n-        states from previous steps. This function is slightly adapted from the official Flax repository:\n-        https://github.com/google/flax/blob/491ce18759622506588784b4fca0e4bf05f8c8cd/flax/linen/attention.py#L252\n-        \"\"\"\n-        # detect if we're initializing by absence of existing cache data.\n-        is_initialized = self.has_variable(\"cache\", \"cached_key\")\n-        cached_key = self.variable(\"cache\", \"cached_key\", jnp.zeros, key.shape, key.dtype)\n-        cached_value = self.variable(\"cache\", \"cached_value\", jnp.zeros, value.shape, value.dtype)\n-        cache_index = self.variable(\"cache\", \"cache_index\", lambda: jnp.array(0, dtype=jnp.int32))\n-\n-        if is_initialized:\n-            *batch_dims, max_length, num_heads, depth_per_head = cached_key.value.shape\n-            # update key, value caches with our new 1d spatial slices\n-            cur_index = cache_index.value\n-            indices = (0,) * len(batch_dims) + (cur_index, 0, 0)\n-            key = lax.dynamic_update_slice(cached_key.value, key, indices)\n-            value = lax.dynamic_update_slice(cached_value.value, value, indices)\n-            cached_key.value = key\n-            cached_value.value = value\n-            num_updated_cache_vectors = query.shape[1]\n-            cache_index.value = cache_index.value + num_updated_cache_vectors\n-            # causal mask for cached decoder self-attention: our single query position should only attend to those key positions that have already been generated and cached, not the remaining zero elements.\n-            pad_mask = jnp.broadcast_to(\n-                jnp.arange(max_length) < cur_index + num_updated_cache_vectors,\n-                tuple(batch_dims) + (1, num_updated_cache_vectors, max_length),\n-            )\n-            attention_mask = combine_masks(pad_mask, attention_mask)\n-        return key, value, attention_mask\n-\n-    def __call__(\n-        self,\n-        hidden_states: jnp.ndarray,\n-        key_value_states: Optional[jnp.ndarray] = None,\n-        attention_mask: Optional[jnp.ndarray] = None,\n-        init_cache: bool = False,\n-        deterministic: bool = True,\n-    ) -> tuple[jnp.ndarray]:\n-        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-\n-        # if key_value_states are provided this layer is used as a cross-attention layer\n-        # for the decoder\n-        is_cross_attention = key_value_states is not None\n-        batch_size = hidden_states.shape[0]\n-\n-        # get query proj\n-        query_states = self.q_proj(hidden_states)\n-        # get key, value proj\n-        if is_cross_attention:\n-            # cross_attentions\n-            key_states = self.k_proj(key_value_states)\n-            value_states = self.v_proj(key_value_states)\n-        else:\n-            # self_attention\n-            key_states = self.k_proj(hidden_states)\n-            value_states = self.v_proj(hidden_states)\n-\n-        query_states = self._split_heads(query_states)\n-        key_states = self._split_heads(key_states)\n-        value_states = self._split_heads(value_states)\n-\n-        # handle cache prepare causal attention mask\n-        if self.causal:\n-            query_length, key_length = query_states.shape[1], key_states.shape[1]\n-            if self.has_variable(\"cache\", \"cached_key\"):\n-                mask_shift = self.variables[\"cache\"][\"cache_index\"]\n-                max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n-                causal_mask = lax.dynamic_slice(\n-                    self.causal_mask, (0, 0, mask_shift, 0), (1, 1, query_length, max_decoder_length)\n-                )\n-            else:\n-                causal_mask = self.causal_mask[:, :, :query_length, :key_length]\n-            causal_mask = jnp.broadcast_to(causal_mask, (batch_size,) + causal_mask.shape[1:])\n-\n-        # combine masks if needed\n-        if attention_mask is not None and self.causal:\n-            attention_mask = jnp.broadcast_to(jnp.expand_dims(attention_mask, axis=(-3, -2)), causal_mask.shape)\n-            attention_mask = combine_masks(attention_mask, causal_mask)\n-        elif self.causal:\n-            attention_mask = causal_mask\n-        elif attention_mask is not None:\n-            attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n-\n-        # During fast autoregressive decoding, we feed one position at a time,\n-        # and cache the keys and values step by step.\n-        if self.causal and (self.has_variable(\"cache\", \"cached_key\") or init_cache):\n-            key_states, value_states, attention_mask = self._concatenate_to_cache(\n-                key_states, value_states, query_states, attention_mask\n-            )\n-\n-        # Convert the boolean attention mask to an attention bias.\n-        if attention_mask is not None:\n-            # attention mask in the form of attention bias\n-            attention_bias = lax.select(\n-                attention_mask > 0,\n-                jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n-                jnp.full(attention_mask.shape, jnp.finfo(self.dtype).min).astype(self.dtype),\n-            )\n-        else:\n-            attention_bias = None\n-\n-        dropout_rng = None\n-        if not deterministic and self.dropout > 0.0:\n-            dropout_rng = self.make_rng(\"dropout\")\n-\n-        attn_weights = dot_product_attention_weights(\n-            query_states,\n-            key_states,\n-            bias=attention_bias,\n-            dropout_rng=dropout_rng,\n-            dropout_rate=self.dropout,\n-            broadcast_dropout=True,\n-            deterministic=deterministic,\n-            dtype=self.dtype,\n-            precision=None,\n-        )\n-\n-        attn_output = jnp.einsum(\"...hqk,...khd->...qhd\", attn_weights, value_states)\n-        attn_output = self._merge_heads(attn_output)\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, attn_weights\n-\n-\n-class FlaxBartEncoderLayer(nn.Module):\n-    config: BartConfig\n-    dtype: jnp.dtype = jnp.float32\n-\n-    def setup(self) -> None:\n-        self.embed_dim = self.config.d_model\n-        self.self_attn = FlaxBartAttention(\n-            config=self.config,\n-            embed_dim=self.embed_dim,\n-            num_heads=self.config.encoder_attention_heads,\n-            dropout=self.config.attention_dropout,\n-            dtype=self.dtype,\n-        )\n-        self.self_attn_layer_norm = nn.LayerNorm(dtype=self.dtype, epsilon=1e-05)\n-        self.dropout_layer = nn.Dropout(rate=self.config.dropout)\n-        self.activation_fn = ACT2FN[self.config.activation_function]\n-        self.activation_dropout_layer = nn.Dropout(rate=self.config.activation_dropout)\n-        self.fc1 = nn.Dense(\n-            self.config.encoder_ffn_dim,\n-            dtype=self.dtype,\n-            kernel_init=jax.nn.initializers.normal(self.config.init_std),\n-        )\n-        self.fc2 = nn.Dense(\n-            self.embed_dim, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(self.config.init_std)\n-        )\n-        self.final_layer_norm = nn.LayerNorm(dtype=self.dtype, epsilon=1e-05)\n-\n-    def __call__(\n-        self,\n-        hidden_states: jnp.ndarray,\n-        attention_mask: jnp.ndarray,\n-        output_attentions: bool = True,\n-        deterministic: bool = True,\n-    ) -> tuple[jnp.ndarray]:\n-        residual = hidden_states\n-        hidden_states, attn_weights = self.self_attn(hidden_states=hidden_states, attention_mask=attention_mask)\n-\n-        hidden_states = self.dropout_layer(hidden_states, deterministic=deterministic)\n-        hidden_states = residual + hidden_states\n-        hidden_states = self.self_attn_layer_norm(hidden_states)\n-\n-        residual = hidden_states\n-        hidden_states = self.activation_fn(self.fc1(hidden_states))\n-        hidden_states = self.activation_dropout_layer(hidden_states, deterministic=deterministic)\n-        hidden_states = self.fc2(hidden_states)\n-        hidden_states = self.dropout_layer(hidden_states, deterministic=deterministic)\n-        hidden_states = residual + hidden_states\n-        hidden_states = self.final_layer_norm(hidden_states)\n-\n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n-\n-\n-class FlaxBartEncoderLayerCollection(nn.Module):\n-    config: BartConfig\n-    dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n-\n-    def setup(self):\n-        self.layers = [\n-            FlaxBartEncoderLayer(self.config, name=str(i), dtype=self.dtype) for i in range(self.config.encoder_layers)\n-        ]\n-        self.layerdrop = self.config.encoder_layerdrop\n-\n-    def __call__(\n-        self,\n-        hidden_states,\n-        attention_mask,\n-        deterministic: bool = True,\n-        output_attentions: bool = False,\n-        output_hidden_states: bool = False,\n-        return_dict: bool = True,\n-    ):\n-        all_attentions = () if output_attentions else None\n-        all_hidden_states = () if output_hidden_states else None\n-\n-        for encoder_layer in self.layers:\n-            if output_hidden_states:\n-                all_hidden_states = all_hidden_states + (hidden_states,)\n-            # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n-            dropout_probability = random.uniform(0, 1)\n-            if not deterministic and (dropout_probability < self.layerdrop):  # skip the layer\n-                layer_outputs = (None, None)\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    attention_mask,\n-                    output_attentions,\n-                    deterministic,\n-                )\n-            hidden_states = layer_outputs[0]\n-            if output_attentions:\n-                all_attentions = all_attentions + (layer_outputs[1],)\n-\n-        if output_hidden_states:\n-            all_hidden_states += (hidden_states,)\n-\n-        outputs = (hidden_states, all_hidden_states, all_attentions)\n-\n-        if not return_dict:\n-            return tuple(v for v in outputs if v is not None)\n-\n-        return FlaxBaseModelOutput(\n-            last_hidden_state=hidden_states, hidden_states=all_hidden_states, attentions=all_attentions\n-        )\n-\n-\n-class FlaxBartDecoderLayer(nn.Module):\n-    config: BartConfig\n-    dtype: jnp.dtype = jnp.float32\n-\n-    def setup(self) -> None:\n-        self.embed_dim = self.config.d_model\n-        self.self_attn = FlaxBartAttention(\n-            config=self.config,\n-            embed_dim=self.embed_dim,\n-            num_heads=self.config.decoder_attention_heads,\n-            dropout=self.config.attention_dropout,\n-            causal=True,\n-            dtype=self.dtype,\n-        )\n-        self.dropout_layer = nn.Dropout(rate=self.config.dropout)\n-        self.activation_fn = ACT2FN[self.config.activation_function]\n-        self.activation_dropout_layer = nn.Dropout(rate=self.config.activation_dropout)\n-\n-        self.self_attn_layer_norm = nn.LayerNorm(dtype=self.dtype, epsilon=1e-05)\n-        self.encoder_attn = FlaxBartAttention(\n-            config=self.config,\n-            embed_dim=self.embed_dim,\n-            num_heads=self.config.decoder_attention_heads,\n-            dropout=self.config.attention_dropout,\n-            dtype=self.dtype,\n-        )\n-        self.encoder_attn_layer_norm = nn.LayerNorm(dtype=self.dtype, epsilon=1e-05)\n-        self.fc1 = nn.Dense(\n-            self.config.decoder_ffn_dim,\n-            dtype=self.dtype,\n-            kernel_init=jax.nn.initializers.normal(self.config.init_std),\n-        )\n-        self.fc2 = nn.Dense(\n-            self.embed_dim, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(self.config.init_std)\n-        )\n-        self.final_layer_norm = nn.LayerNorm(dtype=self.dtype, epsilon=1e-05)\n-\n-    def __call__(\n-        self,\n-        hidden_states: jnp.ndarray,\n-        attention_mask: jnp.ndarray,\n-        encoder_hidden_states: Optional[jnp.ndarray] = None,\n-        encoder_attention_mask: Optional[jnp.ndarray] = None,\n-        init_cache: bool = False,\n-        output_attentions: bool = True,\n-        deterministic: bool = True,\n-    ) -> tuple[jnp.ndarray]:\n-        residual = hidden_states\n-\n-        # Self Attention\n-        hidden_states, self_attn_weights = self.self_attn(\n-            hidden_states=hidden_states, attention_mask=attention_mask, init_cache=init_cache\n-        )\n-        hidden_states = self.dropout_layer(hidden_states, deterministic=deterministic)\n-        hidden_states = residual + hidden_states\n-        hidden_states = self.self_attn_layer_norm(hidden_states)\n-\n-        # Cross-Attention Block\n-        cross_attn_weights = None\n-        if encoder_hidden_states is not None:\n-            residual = hidden_states\n-\n-            hidden_states, cross_attn_weights = self.encoder_attn(\n-                hidden_states=hidden_states,\n-                key_value_states=encoder_hidden_states,\n-                attention_mask=encoder_attention_mask,\n-            )\n-            hidden_states = self.dropout_layer(hidden_states, deterministic=deterministic)\n-            hidden_states = residual + hidden_states\n-            hidden_states = self.encoder_attn_layer_norm(hidden_states)\n-\n-        # Fully Connected\n-        residual = hidden_states\n-        hidden_states = self.activation_fn(self.fc1(hidden_states))\n-        hidden_states = self.activation_dropout_layer(hidden_states, deterministic=deterministic)\n-        hidden_states = self.fc2(hidden_states)\n-        hidden_states = self.dropout_layer(hidden_states, deterministic=deterministic)\n-        hidden_states = residual + hidden_states\n-        hidden_states = self.final_layer_norm(hidden_states)\n-\n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights, cross_attn_weights)\n-\n-        return outputs\n-\n-\n-class FlaxBartDecoderLayerCollection(nn.Module):\n-    config: BartConfig\n-    dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n-\n-    def setup(self):\n-        self.layers = [\n-            FlaxBartDecoderLayer(self.config, name=str(i), dtype=self.dtype) for i in range(self.config.decoder_layers)\n-        ]\n-        self.layerdrop = self.config.decoder_layerdrop\n-\n-    def __call__(\n-        self,\n-        hidden_states,\n-        attention_mask,\n-        encoder_hidden_states: Optional[jnp.ndarray] = None,\n-        encoder_attention_mask: Optional[jnp.ndarray] = None,\n-        deterministic: bool = True,\n-        init_cache: bool = False,\n-        output_attentions: bool = False,\n-        output_hidden_states: bool = False,\n-        return_dict: bool = True,\n-    ):\n-        # decoder layers\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attns = () if output_attentions else None\n-        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n-\n-        for decoder_layer in self.layers:\n-            if output_hidden_states:\n-                all_hidden_states += (hidden_states,)\n-                # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n-            dropout_probability = random.uniform(0, 1)\n-            if not deterministic and (dropout_probability < self.layerdrop):\n-                layer_outputs = (None, None, None)\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=attention_mask,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    init_cache=init_cache,\n-                    output_attentions=output_attentions,\n-                    deterministic=deterministic,\n-                )\n-\n-            hidden_states = layer_outputs[0]\n-            if output_attentions:\n-                all_self_attns += (layer_outputs[1],)\n-\n-                if encoder_hidden_states is not None:\n-                    all_cross_attentions += (layer_outputs[2],)\n-\n-        # add hidden states from the last decoder layer\n-        if output_hidden_states:\n-            all_hidden_states += (hidden_states,)\n-\n-        outputs = [hidden_states, all_hidden_states, all_self_attns, all_cross_attentions]\n-\n-        if not return_dict:\n-            return tuple(v for v in outputs if v is not None)\n-\n-        return FlaxBaseModelOutputWithPastAndCrossAttentions(\n-            last_hidden_state=hidden_states,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attns,\n-            cross_attentions=all_cross_attentions,\n-        )\n-\n-\n-class FlaxBartClassificationHead(nn.Module):\n-    \"\"\"Head for sentence-level classification tasks.\"\"\"\n-\n-    config: BartConfig\n-    inner_dim: int\n-    num_classes: int\n-    pooler_dropout: float\n-    dtype: jnp.dtype = jnp.float32\n-\n-    def setup(self):\n-        self.dense = nn.Dense(\n-            self.inner_dim, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(self.config.init_std)\n-        )\n-        self.dropout = nn.Dropout(rate=self.pooler_dropout)\n-        self.out_proj = nn.Dense(\n-            self.num_classes,\n-            dtype=self.dtype,\n-            kernel_init=jax.nn.initializers.normal(self.config.init_std),\n-        )\n-\n-    def __call__(self, hidden_states: jnp.ndarray, deterministic: bool):\n-        hidden_states = self.dropout(hidden_states, deterministic=deterministic)\n-        hidden_states = self.dense(hidden_states)\n-        hidden_states = jnp.tanh(hidden_states)\n-        hidden_states = self.dropout(hidden_states, deterministic=deterministic)\n-        hidden_states = self.out_proj(hidden_states)\n-        return hidden_states\n-\n-\n-class FlaxBartEncoder(nn.Module):\n-    config: BartConfig\n-    embed_tokens: nn.Embed\n-    dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n-\n-    def setup(self):\n-        self.dropout_layer = nn.Dropout(rate=self.config.dropout)\n-\n-        embed_dim = self.config.d_model\n-        self.padding_idx = self.config.pad_token_id\n-        self.max_source_positions = self.config.max_position_embeddings\n-        self.embed_scale = math.sqrt(embed_dim) if self.config.scale_embedding else 1.0\n-\n-        # Bart is set up so that if padding_idx is specified then offset the embedding ids by 2\n-        # and adjust num_embeddings appropriately. Other models don't have this hack\n-        self.offset = 2\n-        self.embed_positions = nn.Embed(\n-            self.config.max_position_embeddings + self.offset,\n-            embed_dim,\n-            embedding_init=jax.nn.initializers.normal(self.config.init_std),\n-            dtype=self.dtype,\n-        )\n-        self.layers = FlaxBartEncoderLayerCollection(self.config, self.dtype)\n-        self.layernorm_embedding = nn.LayerNorm(dtype=self.dtype, epsilon=1e-05)\n-\n-    def __call__(\n-        self,\n-        input_ids,\n-        attention_mask,\n-        position_ids,\n-        output_attentions: bool = False,\n-        output_hidden_states: bool = False,\n-        return_dict: bool = True,\n-        deterministic: bool = True,\n-    ):\n-        input_shape = input_ids.shape\n-        input_ids = input_ids.reshape(-1, input_shape[-1])\n-\n-        inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n-\n-        embed_pos = self.embed_positions(position_ids + self.offset)\n-\n-        hidden_states = inputs_embeds + embed_pos\n-        hidden_states = self.layernorm_embedding(hidden_states)\n-        hidden_states = self.dropout_layer(hidden_states, deterministic=deterministic)\n-\n-        outputs = self.layers(\n-            hidden_states,\n-            attention_mask,\n-            deterministic=deterministic,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        if not return_dict:\n-            return outputs\n-\n-        return FlaxBaseModelOutput(\n-            last_hidden_state=outputs.last_hidden_state,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-\n-class FlaxBartDecoder(nn.Module):\n-    config: BartConfig\n-    embed_tokens: nn.Embed\n-    dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n-\n-    def setup(self):\n-        self.dropout_layer = nn.Dropout(rate=self.config.dropout)\n-\n-        embed_dim = self.config.d_model\n-        self.padding_idx = self.config.pad_token_id\n-        self.max_target_positions = self.config.max_position_embeddings\n-        self.embed_scale = math.sqrt(self.config.d_model) if self.config.scale_embedding else 1.0\n-\n-        # Bart is set up so that if padding_idx is specified then offset the embedding ids by 2\n-        # and adjust num_embeddings appropriately. Other models don't have this hack\n-        self.offset = 2\n-        self.embed_positions = nn.Embed(\n-            self.config.max_position_embeddings + self.offset,\n-            embed_dim,\n-            embedding_init=jax.nn.initializers.normal(self.config.init_std),\n-            dtype=self.dtype,\n-        )\n-\n-        self.layers = FlaxBartDecoderLayerCollection(self.config, self.dtype)\n-        self.layernorm_embedding = nn.LayerNorm(dtype=self.dtype, epsilon=1e-05)\n-\n-    def __call__(\n-        self,\n-        input_ids,\n-        attention_mask,\n-        position_ids,\n-        encoder_hidden_states: Optional[jnp.ndarray] = None,\n-        encoder_attention_mask: Optional[jnp.ndarray] = None,\n-        init_cache: bool = False,\n-        output_attentions: bool = False,\n-        output_hidden_states: bool = False,\n-        return_dict: bool = True,\n-        deterministic: bool = True,\n-    ):\n-        input_shape = input_ids.shape\n-        input_ids = input_ids.reshape(-1, input_shape[-1])\n-\n-        inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n-\n-        # embed positions\n-        positions = self.embed_positions(position_ids + self.offset)\n-\n-        hidden_states = inputs_embeds + positions\n-        hidden_states = self.layernorm_embedding(hidden_states)\n-\n-        hidden_states = self.dropout_layer(hidden_states, deterministic=deterministic)\n-\n-        outputs = self.layers(\n-            hidden_states,\n-            attention_mask,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            deterministic=deterministic,\n-            init_cache=init_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        if not return_dict:\n-            return outputs\n-\n-        return FlaxBaseModelOutputWithPastAndCrossAttentions(\n-            last_hidden_state=outputs.last_hidden_state,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-            cross_attentions=outputs.cross_attentions,\n-        )\n-\n-\n-class FlaxBartModule(nn.Module):\n-    config: BartConfig\n-    dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n-\n-    def setup(self):\n-        self.shared = nn.Embed(\n-            self.config.vocab_size,\n-            self.config.d_model,\n-            embedding_init=jax.nn.initializers.normal(self.config.init_std),\n-            dtype=self.dtype,\n-        )\n-\n-        self.encoder = FlaxBartEncoder(self.config, dtype=self.dtype, embed_tokens=self.shared)\n-        self.decoder = FlaxBartDecoder(self.config, dtype=self.dtype, embed_tokens=self.shared)\n-\n-    def _get_encoder_module(self):\n-        return self.encoder\n-\n-    def _get_decoder_module(self):\n-        return self.decoder\n-\n-    def __call__(\n-        self,\n-        input_ids,\n-        attention_mask,\n-        decoder_input_ids,\n-        decoder_attention_mask,\n-        position_ids,\n-        decoder_position_ids,\n-        output_attentions: bool = False,\n-        output_hidden_states: bool = False,\n-        return_dict: bool = True,\n-        deterministic: bool = True,\n-    ):\n-        encoder_outputs = self.encoder(\n-            input_ids=input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-            deterministic=deterministic,\n-        )\n-\n-        decoder_outputs = self.decoder(\n-            input_ids=decoder_input_ids,\n-            attention_mask=decoder_attention_mask,\n-            position_ids=decoder_position_ids,\n-            encoder_hidden_states=encoder_outputs[0],\n-            encoder_attention_mask=attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-            deterministic=deterministic,\n-        )\n-\n-        if not return_dict:\n-            return decoder_outputs + encoder_outputs\n-\n-        return FlaxSeq2SeqModelOutput(\n-            last_hidden_state=decoder_outputs.last_hidden_state,\n-            decoder_hidden_states=decoder_outputs.hidden_states,\n-            decoder_attentions=decoder_outputs.attentions,\n-            cross_attentions=decoder_outputs.cross_attentions,\n-            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n-            encoder_hidden_states=encoder_outputs.hidden_states,\n-            encoder_attentions=encoder_outputs.attentions,\n-        )\n-\n-\n-class FlaxBartPreTrainedModel(FlaxPreTrainedModel):\n-    config_class = BartConfig\n-    base_model_prefix: str = \"model\"\n-    module_class: nn.Module = None\n-\n-    def __init__(\n-        self,\n-        config: BartConfig,\n-        input_shape: tuple[int] = (1, 1),\n-        seed: int = 0,\n-        dtype: jnp.dtype = jnp.float32,\n-        _do_init: bool = True,\n-        **kwargs,\n-    ):\n-        module = self.module_class(config=config, dtype=dtype, **kwargs)\n-        super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n-\n-    def init_weights(self, rng: jax.random.PRNGKey, input_shape: tuple, params: FrozenDict = None) -> FrozenDict:\n-        # init input tensors\n-        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n-        # make sure initialization pass will work for FlaxBartForSequenceClassificationModule\n-        input_ids = input_ids.at[(..., -1)].set(self.config.eos_token_id)\n-        attention_mask = jnp.ones_like(input_ids)\n-        decoder_input_ids = input_ids\n-        decoder_attention_mask = jnp.ones_like(input_ids)\n-\n-        batch_size, sequence_length = input_ids.shape\n-        position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[None, :], (batch_size, sequence_length))\n-        decoder_position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[None, :], (batch_size, sequence_length))\n-\n-        params_rng, dropout_rng = jax.random.split(rng)\n-        rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n-\n-        random_params = self.module.init(\n-            rngs,\n-            input_ids,\n-            attention_mask,\n-            decoder_input_ids,\n-            decoder_attention_mask,\n-            position_ids,\n-            decoder_position_ids,\n-        )[\"params\"]\n-\n-        if params is not None:\n-            random_params = flatten_dict(unfreeze(random_params))\n-            params = flatten_dict(unfreeze(params))\n-            for missing_key in self._missing_keys:\n-                params[missing_key] = random_params[missing_key]\n-            self._missing_keys = set()\n-            return freeze(unflatten_dict(params))\n-        else:\n-            return random_params\n-\n-    def init_cache(self, batch_size, max_length, encoder_outputs):\n-        r\"\"\"\n-        Args:\n-            batch_size (`int`):\n-                batch_size used for fast auto-regressive decoding. Defines the batch size of the initialized cache.\n-            max_length (`int`):\n-                maximum possible length for auto-regressive decoding. Defines the sequence length of the initialized\n-                cache.\n-            encoder_outputs (`Union[FlaxBaseModelOutput, tuple(tuple(jnp.ndarray)]`):\n-                `encoder_outputs` consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*:\n-                `attentions`). `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*)\n-                is a sequence of hidden-states at the output of the last layer of the encoder. Used in the\n-                cross-attention of the decoder.\n-        \"\"\"\n-        # init input variables to retrieve cache\n-        decoder_input_ids = jnp.ones((batch_size, max_length), dtype=\"i4\")\n-        decoder_attention_mask = jnp.ones_like(decoder_input_ids)\n-        decoder_position_ids = jnp.broadcast_to(\n-            jnp.arange(jnp.atleast_2d(decoder_input_ids).shape[-1]), decoder_input_ids.shape\n-        )\n-\n-        def _decoder_forward(module, decoder_input_ids, decoder_attention_mask, decoder_position_ids, **kwargs):\n-            decoder_module = module._get_decoder_module()\n-            return decoder_module(\n-                decoder_input_ids,\n-                decoder_attention_mask,\n-                decoder_position_ids,\n-                **kwargs,\n-            )\n-\n-        init_variables = self.module.init(\n-            jax.random.PRNGKey(0),\n-            decoder_input_ids=decoder_input_ids,\n-            decoder_attention_mask=decoder_attention_mask,\n-            decoder_position_ids=decoder_position_ids,\n-            encoder_hidden_states=encoder_outputs[0],\n-            init_cache=True,\n-            method=_decoder_forward,  # we only need to call the decoder to init the cache\n-        )\n-        return unfreeze(init_variables[\"cache\"])\n-\n-    @add_start_docstrings(BART_ENCODE_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=FlaxBaseModelOutput, config_class=BartConfig)\n-    def encode(\n-        self,\n-        input_ids: jnp.ndarray,\n-        attention_mask: Optional[jnp.ndarray] = None,\n-        position_ids: Optional[jnp.ndarray] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        train: bool = False,\n-        params: Optional[dict] = None,\n-        dropout_rng: PRNGKey = None,\n-    ):\n-        r\"\"\"\n-        Returns:\n-\n-        Example:\n-\n-        ```python\n-        >>> from transformers import AutoTokenizer, FlaxBartForConditionalGeneration\n-\n-        >>> model = FlaxBartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n-\n-        >>> text = \"My friends are cool but they eat too many carbs.\"\n-        >>> inputs = tokenizer(text, max_length=1024, return_tensors=\"jax\")\n-        >>> encoder_outputs = model.encode(**inputs)\n-        ```\"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.return_dict\n-\n-        if attention_mask is None:\n-            attention_mask = jnp.ones_like(input_ids)\n-        if position_ids is None:\n-            batch_size, sequence_length = input_ids.shape\n-            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[None, :], (batch_size, sequence_length))\n-\n-        # Handle any PRNG if needed\n-        rngs = {}\n-        if dropout_rng is not None:\n-            rngs[\"dropout\"] = dropout_rng\n-\n-        def _encoder_forward(module, input_ids, attention_mask, position_ids, **kwargs):\n-            encode_module = module._get_encoder_module()\n-            return encode_module(input_ids, attention_mask, position_ids, **kwargs)\n-\n-        return self.module.apply(\n-            {\"params\": params or self.params},\n-            input_ids=jnp.array(input_ids, dtype=\"i4\"),\n-            attention_mask=jnp.array(attention_mask, dtype=\"i4\"),\n-            position_ids=jnp.array(position_ids, dtype=\"i4\"),\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-            deterministic=not train,\n-            rngs=rngs,\n-            method=_encoder_forward,\n-        )\n-\n-    @add_start_docstrings(BART_DECODE_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=FlaxBaseModelOutputWithPastAndCrossAttentions, config_class=BartConfig)\n-    def decode(\n-        self,\n-        decoder_input_ids,\n-        encoder_outputs,\n-        encoder_attention_mask: Optional[jnp.ndarray] = None,\n-        decoder_attention_mask: Optional[jnp.ndarray] = None,\n-        decoder_position_ids: Optional[jnp.ndarray] = None,\n-        past_key_values: Optional[dict] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        train: bool = False,\n-        params: Optional[dict] = None,\n-        dropout_rng: PRNGKey = None,\n-    ):\n-        r\"\"\"\n-        Returns:\n-\n-        Example:\n-\n-        ```python\n-        >>> import jax.numpy as jnp\n-        >>> from transformers import AutoTokenizer, FlaxBartForConditionalGeneration\n-\n-        >>> model = FlaxBartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n-\n-        >>> text = \"My friends are cool but they eat too many carbs.\"\n-        >>> inputs = tokenizer(text, max_length=1024, return_tensors=\"jax\")\n-        >>> encoder_outputs = model.encode(**inputs)\n-\n-        >>> decoder_start_token_id = model.config.decoder_start_token_id\n-        >>> decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype=\"i4\") * decoder_start_token_id\n-\n-        >>> outputs = model.decode(decoder_input_ids, encoder_outputs)\n-        >>> last_decoder_hidden_states = outputs.last_hidden_state\n-        ```\"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.return_dict\n-\n-        encoder_hidden_states = encoder_outputs[0]\n-        if encoder_attention_mask is None:\n-            batch_size, sequence_length = encoder_hidden_states.shape[:2]\n-            encoder_attention_mask = jnp.ones((batch_size, sequence_length))\n-\n-        batch_size, sequence_length = decoder_input_ids.shape\n-        if decoder_attention_mask is None:\n-            decoder_attention_mask = jnp.ones((batch_size, sequence_length))\n-\n-        if decoder_position_ids is None:\n-            if past_key_values is not None:\n-                raise ValueError(\"Make sure to provide `decoder_position_ids` when passing `past_key_values`.\")\n-\n-            decoder_position_ids = jnp.broadcast_to(\n-                jnp.arange(sequence_length)[None, :], (batch_size, sequence_length)\n-            )\n-\n-        # Handle any PRNG if needed\n-        rngs = {}\n-        if dropout_rng is not None:\n-            rngs[\"dropout\"] = dropout_rng\n-\n-        inputs = {\"params\": params or self.params}\n-\n-        # if past_key_values are passed then cache is already initialized a private flag init_cache has to be\n-        # passed down to ensure cache is used. It has to be made sure that cache is marked as mutable so that\n-        # it can be changed by FlaxBartAttention module\n-        if past_key_values:\n-            inputs[\"cache\"] = past_key_values\n-            mutable = [\"cache\"]\n-        else:\n-            mutable = False\n-\n-        def _decoder_forward(module, decoder_input_ids, decoder_attention_mask, decoder_position_ids, **kwargs):\n-            decoder_module = module._get_decoder_module()\n-            return decoder_module(\n-                decoder_input_ids,\n-                decoder_attention_mask,\n-                decoder_position_ids,\n-                **kwargs,\n-            )\n-\n-        outputs = self.module.apply(\n-            inputs,\n-            decoder_input_ids=jnp.array(decoder_input_ids, dtype=\"i4\"),\n-            decoder_attention_mask=jnp.array(decoder_attention_mask, dtype=\"i4\"),\n-            decoder_position_ids=jnp.array(decoder_position_ids, dtype=\"i4\"),\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=jnp.array(encoder_attention_mask, dtype=\"i4\"),\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-            deterministic=not train,\n-            rngs=rngs,\n-            mutable=mutable,\n-            method=_decoder_forward,\n-        )\n-\n-        # add updated cache to model output\n-        if past_key_values is not None and return_dict:\n-            outputs, past = outputs\n-            outputs[\"past_key_values\"] = unfreeze(past[\"cache\"])\n-            return outputs\n-        elif past_key_values is not None and not return_dict:\n-            outputs, past = outputs\n-            outputs = outputs[:1] + (unfreeze(past[\"cache\"]),) + outputs[1:]\n-\n-        return outputs\n-\n-    @add_start_docstrings_to_model_forward(BART_INPUTS_DOCSTRING)\n-    def __call__(\n-        self,\n-        input_ids: jnp.ndarray,\n-        attention_mask: Optional[jnp.ndarray] = None,\n-        decoder_input_ids: Optional[jnp.ndarray] = None,\n-        decoder_attention_mask: Optional[jnp.ndarray] = None,\n-        position_ids: Optional[jnp.ndarray] = None,\n-        decoder_position_ids: Optional[jnp.ndarray] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        train: bool = False,\n-        params: Optional[dict] = None,\n-        dropout_rng: PRNGKey = None,\n-    ):\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.return_dict\n-\n-        # prepare encoder inputs\n-        if attention_mask is None:\n-            attention_mask = jnp.ones_like(input_ids)\n-        if position_ids is None:\n-            batch_size, sequence_length = input_ids.shape\n-            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[None, :], (batch_size, sequence_length))\n-\n-        # prepare decoder inputs\n-        if decoder_input_ids is None:\n-            decoder_input_ids = shift_tokens_right(\n-                input_ids, self.config.pad_token_id, decoder_start_token_id=self.config.decoder_start_token_id\n-            )\n-        if decoder_attention_mask is None:\n-            decoder_attention_mask = jnp.ones_like(decoder_input_ids)\n-        if decoder_position_ids is None:\n-            batch_size, sequence_length = decoder_input_ids.shape\n-            decoder_position_ids = jnp.broadcast_to(\n-                jnp.arange(sequence_length)[None, :], (batch_size, sequence_length)\n-            )\n-\n-        # Handle any PRNG if needed\n-        rngs = {\"dropout\": dropout_rng} if dropout_rng is not None else {}\n-\n-        return self.module.apply(\n-            {\"params\": params or self.params},\n-            input_ids=jnp.array(input_ids, dtype=\"i4\"),\n-            attention_mask=jnp.array(attention_mask, dtype=\"i4\"),\n-            position_ids=jnp.array(position_ids, dtype=\"i4\"),\n-            decoder_input_ids=jnp.array(decoder_input_ids, dtype=\"i4\"),\n-            decoder_attention_mask=jnp.array(decoder_attention_mask, dtype=\"i4\"),\n-            decoder_position_ids=jnp.array(decoder_position_ids, dtype=\"i4\"),\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-            deterministic=not train,\n-            rngs=rngs,\n-        )\n-\n-\n-@add_start_docstrings(\n-    \"The bare Bart Model transformer outputting raw hidden-states without any specific head on top.\",\n-    BART_START_DOCSTRING,\n-)\n-class FlaxBartModel(FlaxBartPreTrainedModel):\n-    config: BartConfig\n-    dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n-    module_class = FlaxBartModule\n-\n-\n-append_call_sample_docstring(FlaxBartModel, _CHECKPOINT_FOR_DOC, FlaxSeq2SeqModelOutput, _CONFIG_FOR_DOC)\n-\n-\n-class FlaxBartForConditionalGenerationModule(nn.Module):\n-    config: BartConfig\n-    dtype: jnp.dtype = jnp.float32\n-    bias_init: Callable[..., jnp.ndarray] = jax.nn.initializers.zeros\n-\n-    def setup(self):\n-        self.model = FlaxBartModule(config=self.config, dtype=self.dtype)\n-        self.lm_head = nn.Dense(\n-            self.model.shared.num_embeddings,\n-            use_bias=False,\n-            dtype=self.dtype,\n-            kernel_init=jax.nn.initializers.normal(self.config.init_std),\n-        )\n-        self.final_logits_bias = self.param(\"final_logits_bias\", self.bias_init, (1, self.model.shared.num_embeddings))\n-\n-    def _get_encoder_module(self):\n-        return self.model.encoder\n-\n-    def _get_decoder_module(self):\n-        return self.model.decoder\n-\n-    def __call__(\n-        self,\n-        input_ids,\n-        attention_mask,\n-        decoder_input_ids,\n-        decoder_attention_mask,\n-        position_ids,\n-        decoder_position_ids,\n-        output_attentions: bool = False,\n-        output_hidden_states: bool = False,\n-        return_dict: bool = True,\n-        deterministic: bool = True,\n-    ):\n-        outputs = self.model(\n-            input_ids=input_ids,\n-            attention_mask=attention_mask,\n-            decoder_input_ids=decoder_input_ids,\n-            decoder_attention_mask=decoder_attention_mask,\n-            position_ids=position_ids,\n-            decoder_position_ids=decoder_position_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-            deterministic=deterministic,\n-        )\n-\n-        hidden_states = outputs[0]\n-\n-        if self.config.tie_word_embeddings:\n-            shared_embedding = self.model.variables[\"params\"][\"shared\"][\"embedding\"]\n-            lm_logits = self.lm_head.apply({\"params\": {\"kernel\": shared_embedding.T}}, hidden_states)\n-        else:\n-            lm_logits = self.lm_head(hidden_states)\n-\n-        lm_logits += jax.lax.stop_gradient(self.final_logits_bias.astype(self.dtype))\n-\n-        if not return_dict:\n-            output = (lm_logits,) + outputs[1:]\n-            return output\n-\n-        return FlaxSeq2SeqLMOutput(\n-            logits=lm_logits,\n-            decoder_hidden_states=outputs.decoder_hidden_states,\n-            decoder_attentions=outputs.decoder_attentions,\n-            cross_attentions=outputs.cross_attentions,\n-            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n-            encoder_hidden_states=outputs.encoder_hidden_states,\n-            encoder_attentions=outputs.encoder_attentions,\n-        )\n-\n-\n-@add_start_docstrings(\n-    \"The BART Model with a language modeling head. Can be used for summarization.\", BART_START_DOCSTRING\n-)\n-class FlaxBartForConditionalGeneration(FlaxBartPreTrainedModel):\n-    module_class = FlaxBartForConditionalGenerationModule\n-    dtype: jnp.dtype = jnp.float32\n-\n-    @add_start_docstrings(BART_DECODE_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=FlaxCausalLMOutputWithCrossAttentions, config_class=BartConfig)\n-    def decode(\n-        self,\n-        decoder_input_ids,\n-        encoder_outputs,\n-        encoder_attention_mask: Optional[jnp.ndarray] = None,\n-        decoder_attention_mask: Optional[jnp.ndarray] = None,\n-        decoder_position_ids: Optional[jnp.ndarray] = None,\n-        past_key_values: Optional[dict] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        train: bool = False,\n-        params: Optional[dict] = None,\n-        dropout_rng: PRNGKey = None,\n-    ):\n-        r\"\"\"\n-        Returns:\n-\n-        Example:\n-\n-        ```python\n-        >>> import jax.numpy as jnp\n-        >>> from transformers import AutoTokenizer, FlaxBartForConditionalGeneration\n-\n-        >>> model = FlaxBartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n-\n-        >>> text = \"My friends are cool but they eat too many carbs.\"\n-        >>> inputs = tokenizer(text, max_length=1024, return_tensors=\"jax\")\n-        >>> encoder_outputs = model.encode(**inputs)\n-\n-        >>> decoder_start_token_id = model.config.decoder_start_token_id\n-        >>> decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype=\"i4\") * decoder_start_token_id\n-\n-        >>> outputs = model.decode(decoder_input_ids, encoder_outputs)\n-        >>> logits = outputs.logits\n-        ```\"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.return_dict\n-\n-        encoder_hidden_states = encoder_outputs[0]\n-        if encoder_attention_mask is None:\n-            batch_size, sequence_length = encoder_hidden_states.shape[:2]\n-            encoder_attention_mask = jnp.ones((batch_size, sequence_length))\n-\n-        batch_size, sequence_length = decoder_input_ids.shape\n-        if decoder_attention_mask is None:\n-            decoder_attention_mask = jnp.ones((batch_size, sequence_length))\n-\n-        if decoder_position_ids is None:\n-            if past_key_values is not None:\n-                raise ValueError(\"Make sure to provide `decoder_position_ids` when passing `past_key_values`.\")\n-\n-            decoder_position_ids = jnp.broadcast_to(\n-                jnp.arange(sequence_length)[None, :], (batch_size, sequence_length)\n-            )\n-\n-        # Handle any PRNG if needed\n-        rngs = {}\n-        if dropout_rng is not None:\n-            rngs[\"dropout\"] = dropout_rng\n-\n-        inputs = {\"params\": params or self.params}\n-\n-        # if past_key_values are passed then cache is already initialized a private flag init_cache has to be\n-        # passed down to ensure cache is used. It has to be made sure that cache is marked as mutable so that\n-        # it can be changed by FlaxBartAttention module\n-        if past_key_values:\n-            inputs[\"cache\"] = past_key_values\n-            mutable = [\"cache\"]\n-        else:\n-            mutable = False\n-\n-        def _decoder_forward(module, decoder_input_ids, decoder_attention_mask, decoder_position_ids, **kwargs):\n-            decoder_module = module._get_decoder_module()\n-            outputs = decoder_module(\n-                decoder_input_ids,\n-                decoder_attention_mask,\n-                decoder_position_ids,\n-                **kwargs,\n-            )\n-            hidden_states = outputs[0]\n-\n-            if self.config.tie_word_embeddings:\n-                shared_embedding = module.model.variables[\"params\"][\"shared\"][\"embedding\"]\n-                lm_logits = module.lm_head.apply({\"params\": {\"kernel\": shared_embedding.T}}, hidden_states)\n-            else:\n-                lm_logits = module.lm_head(hidden_states)\n-\n-            lm_logits += module.final_logits_bias.astype(self.dtype)\n-            return lm_logits, outputs\n-\n-        outputs = self.module.apply(\n-            inputs,\n-            decoder_input_ids=jnp.array(decoder_input_ids, dtype=\"i4\"),\n-            decoder_attention_mask=jnp.array(decoder_attention_mask, dtype=\"i4\"),\n-            decoder_position_ids=jnp.array(decoder_position_ids, dtype=\"i4\"),\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=jnp.array(encoder_attention_mask, dtype=\"i4\"),\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-            deterministic=not train,\n-            rngs=rngs,\n-            mutable=mutable,\n-            method=_decoder_forward,\n-        )\n-\n-        if past_key_values is None:\n-            lm_logits, decoder_outputs = outputs\n-        else:\n-            (lm_logits, decoder_outputs), past = outputs\n-\n-        if return_dict:\n-            outputs = FlaxCausalLMOutputWithCrossAttentions(\n-                logits=lm_logits,\n-                hidden_states=decoder_outputs.hidden_states,\n-                attentions=decoder_outputs.attentions,\n-                cross_attentions=decoder_outputs.cross_attentions,\n-            )\n-        else:\n-            outputs = (lm_logits,) + decoder_outputs[1:]\n-\n-        # add updated cache to model output\n-        if past_key_values is not None and return_dict:\n-            outputs[\"past_key_values\"] = unfreeze(past[\"cache\"])\n-            return outputs\n-        elif past_key_values is not None and not return_dict:\n-            outputs = outputs[:1] + (unfreeze(past[\"cache\"]),) + outputs[1:]\n-\n-        return outputs\n-\n-    def prepare_inputs_for_generation(\n-        self,\n-        decoder_input_ids,\n-        max_length,\n-        attention_mask: Optional[jax.Array] = None,\n-        decoder_attention_mask: Optional[jax.Array] = None,\n-        encoder_outputs=None,\n-        **kwargs,\n-    ):\n-        # initializing the cache\n-        batch_size, seq_length = decoder_input_ids.shape\n-\n-        past_key_values = self.init_cache(batch_size, max_length, encoder_outputs)\n-        # Note that usually one would have to put 0's in the attention_mask for x > input_ids.shape[-1] and x < cache_length.\n-        # But since the decoder uses a causal mask, those positions are masked anyways.\n-        # Thus we can create a single static attention_mask here, which is more efficient for compilation\n-        extended_attention_mask = jnp.ones((batch_size, max_length), dtype=\"i4\")\n-        if decoder_attention_mask is not None:\n-            position_ids = decoder_attention_mask.cumsum(axis=-1) - 1\n-            extended_attention_mask = lax.dynamic_update_slice(extended_attention_mask, decoder_attention_mask, (0, 0))\n-        else:\n-            position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[None, :], (batch_size, seq_length))\n-\n-        return {\n-            \"past_key_values\": past_key_values,\n-            \"encoder_outputs\": encoder_outputs,\n-            \"encoder_attention_mask\": attention_mask,\n-            \"decoder_attention_mask\": extended_attention_mask,\n-            \"decoder_position_ids\": position_ids,\n-        }\n-\n-    def update_inputs_for_generation(self, model_outputs, model_kwargs):\n-        model_kwargs[\"past_key_values\"] = model_outputs.past_key_values\n-        model_kwargs[\"decoder_position_ids\"] = model_kwargs[\"decoder_position_ids\"][:, -1:] + 1\n-        return model_kwargs\n-\n-\n-FLAX_BART_CONDITIONAL_GENERATION_DOCSTRING = \"\"\"\n-    Returns:\n-\n-    Summarization example:\n-\n-    ```python\n-    >>> from transformers import AutoTokenizer, FlaxBartForConditionalGeneration\n-\n-    >>> model = FlaxBartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n-    >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n-\n-    >>> ARTICLE_TO_SUMMARIZE = \"My friends are cool but they eat too many carbs.\"\n-    >>> inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors=\"np\")\n-\n-    >>> # Generate Summary\n-    >>> summary_ids = model.generate(inputs[\"input_ids\"]).sequences\n-    >>> print(tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False))\n-    ```\n-\n-    Mask filling example:\n-\n-    ```python\n-    >>> import jax\n-    >>> from transformers import AutoTokenizer, FlaxBartForConditionalGeneration\n-\n-    >>> model = FlaxBartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n-    >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\n-\n-    >>> TXT = \"My friends are <mask> but they eat too many carbs.\"\n-    >>> input_ids = tokenizer([TXT], return_tensors=\"jax\")[\"input_ids\"]\n-\n-    >>> logits = model(input_ids).logits\n-    >>> masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero()[0].item()\n-    >>> probs = jax.nn.softmax(logits[0, masked_index], axis=0)\n-    >>> values, predictions = jax.lax.top_k(probs, k=1)\n-\n-    >>> tokenizer.decode(predictions).split()\n-    ```\n-\"\"\"\n-\n-overwrite_call_docstring(\n-    FlaxBartForConditionalGeneration, BART_INPUTS_DOCSTRING + FLAX_BART_CONDITIONAL_GENERATION_DOCSTRING\n-)\n-append_replace_return_docstrings(\n-    FlaxBartForConditionalGeneration, output_type=FlaxSeq2SeqLMOutput, config_class=_CONFIG_FOR_DOC\n-)\n-\n-\n-class FlaxBartForSequenceClassificationModule(nn.Module):\n-    config: BartConfig\n-    dtype: jnp.dtype = jnp.float32\n-    num_labels: Optional[int] = None\n-\n-    def setup(self):\n-        self.model = FlaxBartModule(config=self.config, dtype=self.dtype)\n-        self.classification_head = FlaxBartClassificationHead(\n-            config=self.config,\n-            inner_dim=self.config.d_model,\n-            num_classes=self.num_labels if self.num_labels is not None else self.config.num_labels,\n-            pooler_dropout=self.config.classifier_dropout,\n-        )\n-\n-    def _get_encoder_module(self):\n-        return self.model.encoder\n-\n-    def _get_decoder_module(self):\n-        return self.model.decoder\n-\n-    def __call__(\n-        self,\n-        input_ids,\n-        attention_mask,\n-        decoder_input_ids,\n-        decoder_attention_mask,\n-        position_ids,\n-        decoder_position_ids,\n-        output_attentions: bool = False,\n-        output_hidden_states: bool = False,\n-        return_dict: bool = True,\n-        deterministic: bool = True,\n-    ):\n-        outputs = self.model(\n-            input_ids=input_ids,\n-            attention_mask=attention_mask,\n-            decoder_input_ids=decoder_input_ids,\n-            decoder_attention_mask=decoder_attention_mask,\n-            position_ids=position_ids,\n-            decoder_position_ids=decoder_position_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-            deterministic=deterministic,\n-        )\n-\n-        hidden_states = outputs[0]  # last hidden state\n-\n-        eos_mask = jnp.where(input_ids == self.config.eos_token_id, 1, 0)\n-\n-        # The first condition is necessary to overcome jax._src.errors.ConcretizationTypeError during JIT compilation\n-        if not isinstance(eos_mask, jax.interpreters.partial_eval.DynamicJaxprTracer):\n-            if len(jnp.unique(eos_mask.sum(1))) > 1:\n-                raise ValueError(\"All examples must have the same number of <eos> tokens.\")\n-\n-            if any(eos_mask.sum(1) == 0):\n-                raise ValueError(\"There are missing <eos> tokens in input_ids\")\n-\n-            # Ensure to keep 1 only for the last <eos> token for each example\n-            eos_mask_noised = eos_mask + jnp.arange(eos_mask.shape[1]) * 1e-6\n-            eos_mask = jnp.where(eos_mask_noised == eos_mask_noised.max(1).reshape(-1, 1), 1, 0)\n-\n-        sentence_representation = jnp.einsum(\"ijk, ij -> ijk\", hidden_states, eos_mask).sum(1)\n-        logits = self.classification_head(sentence_representation, deterministic=deterministic)\n-\n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return output\n-\n-        return FlaxSeq2SeqSequenceClassifierOutput(\n-            logits=logits,\n-            decoder_hidden_states=outputs.decoder_hidden_states,\n-            decoder_attentions=outputs.decoder_attentions,\n-            cross_attentions=outputs.cross_attentions,\n-            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n-            encoder_hidden_states=outputs.encoder_hidden_states,\n-            encoder_attentions=outputs.encoder_attentions,\n-        )\n-\n-\n-@add_start_docstrings(\n-    \"\"\"\n-    Bart model with a sequence classification/head on top (a linear layer on top of the pooled output) e.g. for GLUE\n-    tasks.\n-    \"\"\",\n-    BART_START_DOCSTRING,\n-)\n-class FlaxBartForSequenceClassification(FlaxBartPreTrainedModel):\n-    module_class = FlaxBartForSequenceClassificationModule\n-    dtype = jnp.float32\n-\n-\n-append_call_sample_docstring(\n-    FlaxBartForSequenceClassification,\n-    _CHECKPOINT_FOR_DOC,\n-    FlaxSeq2SeqSequenceClassifierOutput,\n-    _CONFIG_FOR_DOC,\n-)\n-\n-\n-class FlaxBartForQuestionAnsweringModule(nn.Module):\n-    config: BartConfig\n-    dtype: jnp.dtype = jnp.float32\n-    num_labels = 2\n-\n-    def setup(self):\n-        self.model = FlaxBartModule(config=self.config, dtype=self.dtype)\n-        self.qa_outputs = nn.Dense(\n-            self.num_labels, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(self.config.init_std)\n-        )\n-\n-    def _get_encoder_module(self):\n-        return self.model.encoder\n-\n-    def _get_decoder_module(self):\n-        return self.model.decoder\n-\n-    def __call__(\n-        self,\n-        input_ids,\n-        attention_mask,\n-        decoder_input_ids,\n-        decoder_attention_mask,\n-        position_ids,\n-        decoder_position_ids,\n-        output_attentions: bool = False,\n-        output_hidden_states: bool = False,\n-        return_dict: bool = True,\n-        deterministic: bool = True,\n-    ):\n-        outputs = self.model(\n-            input_ids=input_ids,\n-            attention_mask=attention_mask,\n-            decoder_input_ids=decoder_input_ids,\n-            decoder_attention_mask=decoder_attention_mask,\n-            position_ids=position_ids,\n-            decoder_position_ids=decoder_position_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-            deterministic=deterministic,\n-        )\n-\n-        sequence_output = outputs[0]\n-\n-        logits = self.qa_outputs(sequence_output)\n-        start_logits, end_logits = jnp.split(logits, logits.shape[-1], axis=-1)\n-        start_logits = start_logits.squeeze(-1)\n-        end_logits = end_logits.squeeze(-1)\n-\n-        if not return_dict:\n-            output = (start_logits, end_logits) + outputs[1:]\n-            return output\n-\n-        return FlaxSeq2SeqQuestionAnsweringModelOutput(\n-            start_logits=start_logits,\n-            end_logits=end_logits,\n-            decoder_hidden_states=outputs.decoder_hidden_states,\n-            decoder_attentions=outputs.decoder_attentions,\n-            cross_attentions=outputs.cross_attentions,\n-            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n-            encoder_hidden_states=outputs.encoder_hidden_states,\n-            encoder_attentions=outputs.encoder_attentions,\n-        )\n-\n-\n-@add_start_docstrings(\n-    \"\"\"\n-    BART Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n-    layer on top of the hidden-states output to compute `span start logits` and `span end logits`).\n-    \"\"\",\n-    BART_START_DOCSTRING,\n-)\n-class FlaxBartForQuestionAnswering(FlaxBartPreTrainedModel):\n-    module_class = FlaxBartForQuestionAnsweringModule\n-    dtype = jnp.float32\n-\n-\n-append_call_sample_docstring(\n-    FlaxBartForQuestionAnswering,\n-    _CHECKPOINT_FOR_DOC,\n-    FlaxSeq2SeqQuestionAnsweringModelOutput,\n-    _CONFIG_FOR_DOC,\n-)\n-\n-\n-class FlaxBartDecoderPreTrainedModel(FlaxPreTrainedModel):\n-    config_class = BartConfig\n-    base_model_prefix: str = \"model\"\n-    module_class: nn.Module = None\n-\n-    def __init__(\n-        self,\n-        config: BartConfig,\n-        input_shape: tuple[int] = (1, 1),\n-        seed: int = 0,\n-        dtype: jnp.dtype = jnp.float32,\n-        _do_init: bool = True,\n-        **kwargs,\n-    ):\n-        config.is_decoder = True\n-        config.is_encoder_decoder = False\n-        module = self.module_class(config=config, dtype=dtype, **kwargs)\n-        super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n-\n-    def init_weights(self, rng: jax.random.PRNGKey, input_shape: tuple, params: FrozenDict = None) -> FrozenDict:\n-        # init input tensors\n-        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n-        attention_mask = jnp.ones_like(input_ids)\n-\n-        batch_size, sequence_length = input_ids.shape\n-        position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[None, :], (batch_size, sequence_length))\n-\n-        params_rng, dropout_rng = jax.random.split(rng)\n-        rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n-        encoder_hidden_states = jnp.zeros(input_shape + (self.config.d_model,))\n-        encoder_attention_mask = attention_mask\n-        module_init_outputs = self.module.init(\n-            rngs,\n-            input_ids,\n-            attention_mask,\n-            position_ids,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            return_dict=False,\n-        )\n-        return module_init_outputs[\"params\"]\n-\n-    def init_cache(self, batch_size, max_length):\n-        r\"\"\"\n-        Args:\n-            batch_size (`int`):\n-                batch_size used for fast auto-regressive decoding. Defines the batch size of the initialized cache.\n-            max_length (`int`):\n-                maximum possible length for auto-regressive decoding. Defines the sequence length of the initialized\n-                cache.\n-        \"\"\"\n-        # init input variables to retrieve cache\n-        input_ids = jnp.ones((batch_size, max_length), dtype=\"i4\")\n-        attention_mask = jnp.ones_like(input_ids, dtype=\"i4\")\n-        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n-\n-        init_variables = self.module.init(\n-            jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n-        )\n-        return unfreeze(init_variables[\"cache\"])\n-\n-    @add_start_docstrings_to_model_forward(BART_DECODE_INPUTS_DOCSTRING)\n-    def __call__(\n-        self,\n-        input_ids: jnp.ndarray,\n-        attention_mask: Optional[jnp.ndarray] = None,\n-        position_ids: Optional[jnp.ndarray] = None,\n-        encoder_hidden_states: Optional[jnp.ndarray] = None,\n-        encoder_attention_mask: Optional[jnp.ndarray] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        train: bool = False,\n-        params: Optional[dict] = None,\n-        past_key_values: Optional[dict] = None,\n-        dropout_rng: PRNGKey = None,\n-    ):\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.return_dict\n-\n-        if encoder_hidden_states is not None and encoder_attention_mask is None:\n-            batch_size, sequence_length = encoder_hidden_states.shape[:2]\n-            encoder_attention_mask = jnp.ones((batch_size, sequence_length))\n-\n-        # prepare decoder inputs\n-        if attention_mask is None:\n-            attention_mask = jnp.ones_like(input_ids)\n-        if position_ids is None:\n-            batch_size, sequence_length = input_ids.shape\n-            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[None, :], (batch_size, sequence_length))\n-\n-        # Handle any PRNG if needed\n-        rngs = {\"dropout\": dropout_rng} if dropout_rng is not None else {}\n-\n-        inputs = {\"params\": params or self.params}\n-\n-        # if past_key_values are passed then cache is already initialized a private flag init_cache has to be passed\n-        # down to ensure cache is used. It has to be made sure that cache is marked as mutable so that it can be\n-        # changed by FlaxBartAttention module\n-        if past_key_values:\n-            inputs[\"cache\"] = past_key_values\n-            mutable = [\"cache\"]\n-        else:\n-            mutable = False\n-\n-        outputs = self.module.apply(\n-            inputs,\n-            input_ids=jnp.array(input_ids, dtype=\"i4\"),\n-            attention_mask=jnp.array(attention_mask, dtype=\"i4\"),\n-            position_ids=jnp.array(position_ids, dtype=\"i4\"),\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-            deterministic=not train,\n-            rngs=rngs,\n-            mutable=mutable,\n-        )\n-\n-        # add updated cache to model output\n-        if past_key_values is not None and return_dict:\n-            outputs, past_key_values = outputs\n-            outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n-            return outputs\n-        elif past_key_values is not None and not return_dict:\n-            outputs, past_key_values = outputs\n-            outputs = outputs[:1] + (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n-\n-        return outputs\n-\n-\n-class FlaxBartDecoderWrapper(nn.Module):\n-    \"\"\"\n-    This wrapper class is a helper class to correctly load pretrained checkpoints when the causal language model is\n-    used in combination with the [`EncoderDecoderModel`] framework.\n-    \"\"\"\n-\n-    config: BartConfig\n-    dtype: jnp.dtype = jnp.float32\n-\n-    def setup(self):\n-        embed_dim = self.config.d_model\n-        embed_tokens = nn.Embed(\n-            self.config.vocab_size,\n-            embed_dim,\n-            embedding_init=jax.nn.initializers.normal(self.config.init_std),\n-            dtype=self.dtype,\n-        )\n-        self.decoder = FlaxBartDecoder(config=self.config, embed_tokens=embed_tokens, dtype=self.dtype)\n-\n-    def __call__(self, *args, **kwargs):\n-        return self.decoder(*args, **kwargs)\n-\n-\n-class FlaxBartForCausalLMModule(nn.Module):\n-    config: BartConfig\n-    dtype: jnp.dtype = jnp.float32\n-\n-    def setup(self):\n-        self.model = FlaxBartDecoderWrapper(config=self.config, dtype=self.dtype)\n-        self.lm_head = nn.Dense(\n-            self.config.vocab_size,\n-            use_bias=False,\n-            dtype=self.dtype,\n-            kernel_init=jax.nn.initializers.normal(self.config.init_std),\n-        )\n-\n-    def __call__(\n-        self,\n-        input_ids,\n-        attention_mask,\n-        position_ids,\n-        encoder_hidden_states: Optional[jnp.ndarray] = None,\n-        encoder_attention_mask: Optional[jnp.ndarray] = None,\n-        init_cache: bool = False,\n-        output_attentions: bool = False,\n-        output_hidden_states: bool = False,\n-        return_dict: bool = True,\n-        deterministic: bool = True,\n-    ):\n-        outputs = self.model(\n-            input_ids,\n-            attention_mask,\n-            position_ids,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            deterministic=deterministic,\n-            init_cache=init_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        hidden_states = outputs[0]\n-\n-        if self.config.tie_word_embeddings:\n-            shared_embedding = self.model.variables[\"params\"][\"decoder\"][\"embed_tokens\"][\"embedding\"]\n-            lm_logits = self.lm_head.apply({\"params\": {\"kernel\": shared_embedding.T}}, hidden_states)\n-        else:\n-            lm_logits = self.lm_head(hidden_states)\n-\n-        if not return_dict:\n-            return (lm_logits,) + outputs[1:]\n-\n-        return FlaxCausalLMOutputWithCrossAttentions(\n-            logits=lm_logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-            cross_attentions=outputs.cross_attentions,\n-        )\n-\n-\n-@add_start_docstrings(\n-    \"\"\"\n-    Bart Decoder Model with a language modeling head on top (linear layer with weights tied to the input embeddings)\n-    e.g for autoregressive tasks.\n-    \"\"\",\n-    BART_START_DOCSTRING,\n-)\n-class FlaxBartForCausalLM(FlaxBartDecoderPreTrainedModel):\n-    module_class = FlaxBartForCausalLMModule\n-\n-    def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[jax.Array] = None):\n-        # initializing the cache\n-        batch_size, seq_length = input_ids.shape\n-\n-        past_key_values = self.init_cache(batch_size, max_length)\n-        # Note that usually one would have to put 0's in the attention_mask for x > input_ids.shape[-1] and x < cache_length.\n-        # But since the decoder uses a causal mask, those positions are masked anyway.\n-        # Thus, we can create a single static attention_mask here, which is more efficient for compilation\n-        extended_attention_mask = jnp.ones((batch_size, max_length), dtype=\"i4\")\n-        if attention_mask is not None:\n-            position_ids = attention_mask.cumsum(axis=-1) - 1\n-            extended_attention_mask = lax.dynamic_update_slice(extended_attention_mask, attention_mask, (0, 0))\n-        else:\n-            position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[None, :], (batch_size, seq_length))\n-\n-        return {\n-            \"past_key_values\": past_key_values,\n-            \"attention_mask\": extended_attention_mask,\n-            \"position_ids\": position_ids,\n-        }\n-\n-    def update_inputs_for_generation(self, model_outputs, model_kwargs):\n-        model_kwargs[\"past_key_values\"] = model_outputs.past_key_values\n-        model_kwargs[\"position_ids\"] = model_kwargs[\"position_ids\"][:, -1:] + 1\n-        return model_kwargs\n-\n-\n-append_call_sample_docstring(\n-    FlaxBartForCausalLM,\n-    _CHECKPOINT_FOR_DOC,\n-    FlaxCausalLMOutputWithCrossAttentions,\n-    _CONFIG_FOR_DOC,\n-)\n-\n-\n-__all__ = [\n-    \"FlaxBartDecoderPreTrainedModel\",\n-    \"FlaxBartForCausalLM\",\n-    \"FlaxBartForConditionalGeneration\",\n-    \"FlaxBartForQuestionAnswering\",\n-    \"FlaxBartForSequenceClassification\",\n-    \"FlaxBartModel\",\n-    \"FlaxBartPreTrainedModel\",\n-]"
        },
        {
            "sha": "0a6d2317d696f9a9d1edce17f256ceccc8134e49",
            "filename": "src/transformers/models/bart/modeling_tf_bart.py",
            "status": "removed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_tf_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_tf_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_tf_bart.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "66dcfe1e56f757be33462438a4896726c89ebbf5",
            "filename": "src/transformers/models/beit/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbeit%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbeit%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2F__init__.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "984eac3bf67ec7a65e3ef3c5926e51e14697e60b",
            "filename": "src/transformers/models/beit/image_processing_beit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "7a55543dee6260fb9028b9b5bdd6d2b4178928e5",
            "filename": "src/transformers/models/beit/image_processing_beit_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "9b6e7f1cd1a6483de83c8c505445bfeb810540b7",
            "filename": "src/transformers/models/beit/modeling_beit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "c80deace6b39dc79968eff060a15573b0e2ea5ad",
            "filename": "src/transformers/models/beit/modeling_flax_beit.py",
            "status": "removed",
            "additions": 0,
            "deletions": 956,
            "changes": 956,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_flax_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_flax_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_flax_beit.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "b78228a591aaf6a969adbd7cb678303508ee6f11",
            "filename": "src/transformers/models/bert/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbert%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbert%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2F__init__.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "8e1e85d5c04e51041822bbfc1843e14a820a1f0b",
            "filename": "src/transformers/models/bert/convert_bert_pytorch_checkpoint_to_original_tf.py",
            "status": "removed",
            "additions": 0,
            "deletions": 112,
            "changes": 112,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fbert%2Fconvert_bert_pytorch_checkpoint_to_original_tf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fbert%2Fconvert_bert_pytorch_checkpoint_to_original_tf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fconvert_bert_pytorch_checkpoint_to_original_tf.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "186d13bb7541f485dc8f705ae7bd4ffd90b2c4f3",
            "filename": "src/transformers/models/bert/modeling_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 80,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "37828642eb4eb025973952ce0750a66e7e7693ea",
            "filename": "src/transformers/models/bert/modeling_flax_bert.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1727,
            "changes": 1727,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_flax_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_flax_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_flax_bert.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "1ca82f9f18200fafc165d851da0f24591b259aa8",
            "filename": "src/transformers/models/bert/modeling_tf_bert.py",
            "status": "removed",
            "additions": 0,
            "deletions": 2125,
            "changes": 2125,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_tf_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_tf_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_tf_bert.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "c8fca52c4cbf39d033296429e2034d7b9dd7904e",
            "filename": "src/transformers/models/bert/tokenization_bert_tf.py",
            "status": "removed",
            "additions": 0,
            "deletions": 259,
            "changes": 259,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert_tf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert_tf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert_tf.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "c1b09041cd74b9ebb13e5e19919e8b59f725ce2c",
            "filename": "src/transformers/models/bert_generation/modeling_bert_generation.py",
            "status": "modified",
            "additions": 0,
            "deletions": 89,
            "changes": 89,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "e9bc0f08af3ec74cad26a129987a948c88ec9c44",
            "filename": "src/transformers/models/big_bird/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbig_bird%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbig_bird%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2F__init__.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "9064c7cbdc0878d08c26ccc0fdf7b7139cf275ef",
            "filename": "src/transformers/models/big_bird/convert_bigbird_original_tf_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 184,
            "deletions": 1,
            "changes": 185,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fconvert_bigbird_original_tf_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fconvert_bigbird_original_tf_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fconvert_bigbird_original_tf_checkpoint_to_pytorch.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "69dc11a7cb6950139a34df09f0324205307e5491",
            "filename": "src/transformers/models/big_bird/modeling_big_bird.py",
            "status": "modified",
            "additions": 0,
            "deletions": 168,
            "changes": 168,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "11dcb30f3d47e3ecf6d90d7a7305682d687e5828",
            "filename": "src/transformers/models/big_bird/modeling_flax_big_bird.py",
            "status": "removed",
            "additions": 0,
            "deletions": 2648,
            "changes": 2648,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_flax_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_flax_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_flax_big_bird.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "dc32c34e0d25c3286533720b6d801ec8db17f24b",
            "filename": "src/transformers/models/bigbird_pegasus/configuration_bigbird_pegasus.py",
            "status": "modified",
            "additions": 10,
            "deletions": 14,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fconfiguration_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fconfiguration_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fconfiguration_bigbird_pegasus.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "70644c8d3df28b3077c4667bf4d2e8e454821c04",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "983149fea57401b287874da1e17c96133245e7cd",
            "filename": "src/transformers/models/bit/image_processing_bit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbit%2Fimage_processing_bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbit%2Fimage_processing_bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbit%2Fimage_processing_bit.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "616c6d31d339a90631c9624a0b65aa9ebb3eb5d9",
            "filename": "src/transformers/models/bit/modeling_bit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "6e728fd0914ad3cdc914f0780fe76b25e5782204",
            "filename": "src/transformers/models/blenderbot/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fblenderbot%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fblenderbot%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2F__init__.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "8e4e4812aafd33ff77e1eb08ac6b14ea2eec6c60",
            "filename": "src/transformers/models/blenderbot/configuration_blenderbot.py",
            "status": "modified",
            "additions": 9,
            "deletions": 13,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fconfiguration_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fconfiguration_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fconfiguration_blenderbot.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "8b147211881b778d60236615ecc240812b2865b1",
            "filename": "src/transformers/models/blenderbot/modeling_flax_blenderbot.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1508,
            "changes": 1508,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_flax_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_flax_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_flax_blenderbot.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "78f4f6a6761e71087b5d232a42cb137595af9702",
            "filename": "src/transformers/models/blenderbot/modeling_tf_blenderbot.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1557,
            "changes": 1557,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_tf_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_tf_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_tf_blenderbot.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "7f08df82e757b33886ee65d8b6ad050ea663fe78",
            "filename": "src/transformers/models/blenderbot_small/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2F__init__.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "6cd7f7275c173bf5c6bd5b325ba404da8fc07ae7",
            "filename": "src/transformers/models/blenderbot_small/configuration_blenderbot_small.py",
            "status": "modified",
            "additions": 10,
            "deletions": 14,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fconfiguration_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fconfiguration_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fconfiguration_blenderbot_small.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "ac30320bbdb4535d04b36b19cd97cea97a24d673",
            "filename": "src/transformers/models/blenderbot_small/modeling_flax_blenderbot_small.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1528,
            "changes": 1528,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_flax_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_flax_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_flax_blenderbot_small.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "be7711801ed238e9e5b06c665e38befdb3d86a75",
            "filename": "src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1527,
            "changes": 1527,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_tf_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_tf_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_tf_blenderbot_small.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "c16593d7ce17f230b8c5126f0cb9fde21239f8ba",
            "filename": "src/transformers/models/blip/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fblip%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fblip%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2F__init__.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "0efc3c5d1eb34f8b61ba36d669f226a33391ca00",
            "filename": "src/transformers/models/blip/image_processing_blip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fblip%2Fimage_processing_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fblip%2Fimage_processing_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fimage_processing_blip.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "99026a2b4fd0b90bffc6f28d4629b2704a3851c5",
            "filename": "src/transformers/models/blip/modeling_blip_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "a1a1f7928273c69de30b4ba1c190435d91dfd509",
            "filename": "src/transformers/models/blip/modeling_tf_blip.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1709,
            "changes": 1709,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_tf_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_tf_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_tf_blip.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "7dae1126e03b12b39b17b8fdc00b6050f9681fcd",
            "filename": "src/transformers/models/blip/modeling_tf_blip_text.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1122,
            "changes": 1122,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_tf_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_tf_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_tf_blip_text.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "4ac741f84f465984dd0535814b40e539d77dab00",
            "filename": "src/transformers/models/blip/processing_blip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fblip%2Fprocessing_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fblip%2Fprocessing_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fprocessing_blip.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "71f79583c77ebcc77b1d273f88c07a96068313f0",
            "filename": "src/transformers/models/blip_2/processing_blip_2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "4a938fd80b2503005baca95875df32139ac3666c",
            "filename": "src/transformers/models/bloom/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbloom%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbloom%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2F__init__.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "8d5fa7656a73486a94224c28b3c33190ff779267",
            "filename": "src/transformers/models/bloom/configuration_bloom.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbloom%2Fconfiguration_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbloom%2Fconfiguration_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fconfiguration_bloom.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "84e31fddfb2e6f099bac06821c71a4f8f2c9ef63",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "c7bb1cc9c9a5b098f9c03d6313818e744e61951e",
            "filename": "src/transformers/models/bloom/modeling_flax_bloom.py",
            "status": "removed",
            "additions": 0,
            "deletions": 737,
            "changes": 737,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_flax_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_flax_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_flax_bloom.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "75b4e2b4238c7793d7146a714310d435742fe124",
            "filename": "src/transformers/models/bridgetower/image_processing_bridgetower.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "97fcc469a4c6596120c5cc5638f8c6b604b531bc",
            "filename": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "5f5dd05ff82dcf116a1f380b8e699d6a222a15e9",
            "filename": "src/transformers/models/bros/modeling_bros.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "a53efce63544c16efbecc9a7f2b46e8f10adda8f",
            "filename": "src/transformers/models/byt5/convert_byt5_original_tf_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 110,
            "deletions": 1,
            "changes": 111,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbyt5%2Fconvert_byt5_original_tf_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fbyt5%2Fconvert_byt5_original_tf_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbyt5%2Fconvert_byt5_original_tf_checkpoint_to_pytorch.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "a3a9c395eb5bf130a3a3a3f75697b34211af13ed",
            "filename": "src/transformers/models/camembert/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fcamembert%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fcamembert%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2F__init__.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "aa86eb18d652d38388ea264265f3c44ecba13bde",
            "filename": "src/transformers/models/camembert/modeling_camembert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "0869902aa96245ead1bb22f2e818517288a35534",
            "filename": "src/transformers/models/camembert/modeling_tf_camembert.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1800,
            "changes": 1800,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_tf_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_tf_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_tf_camembert.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "5c18f64dba1c853be92f80cda26e00ff46f3617f",
            "filename": "src/transformers/models/canine/convert_canine_original_tf_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 102,
            "deletions": 1,
            "changes": 103,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fcanine%2Fconvert_canine_original_tf_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fcanine%2Fconvert_canine_original_tf_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcanine%2Fconvert_canine_original_tf_checkpoint_to_pytorch.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "545919dc7b779baea342f61addafffdfe24b7be1",
            "filename": "src/transformers/models/canine/modeling_canine.py",
            "status": "modified",
            "additions": 0,
            "deletions": 108,
            "changes": 108,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "484ce53e729c4e1d7d21d3a1c8a54b0376657530",
            "filename": "src/transformers/models/chameleon/image_processing_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "bf4441c00a2e6e123636dd07e22101a07685abd2",
            "filename": "src/transformers/models/chameleon/processing_chameleon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "5b9c31965585a963d8437660031dc6e52a8800a2",
            "filename": "src/transformers/models/chinese_clip/configuration_chinese_clip.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fconfiguration_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fconfiguration_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fconfiguration_chinese_clip.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "1ada2c7156697f964a014c3cad3e45ee9c2008eb",
            "filename": "src/transformers/models/chinese_clip/image_processing_chinese_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "a689886abc3788e18bbad5699943ba64a1627d89",
            "filename": "src/transformers/models/chinese_clip/modeling_chinese_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "75c79c4e38349d4d2010bb170054d05e3aa14668",
            "filename": "src/transformers/models/clap/feature_extraction_clap.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fclap%2Ffeature_extraction_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fclap%2Ffeature_extraction_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Ffeature_extraction_clap.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "b8983eecf035497376fafa74e52cd6e78f44bc36",
            "filename": "src/transformers/models/clap/modeling_clap.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "36fb3521a93e5aa315a1ee8b6a51ea8c7e27fb24",
            "filename": "src/transformers/models/clip/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fclip%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fclip%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2F__init__.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "22c245485a0df117ca1e1293fe5384e595b93074",
            "filename": "src/transformers/models/clip/configuration_clip.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fclip%2Fconfiguration_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fclip%2Fconfiguration_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fconfiguration_clip.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "ca5e00579f68ed1f3119a59d7f1a2e02bb2b9198",
            "filename": "src/transformers/models/clip/image_processing_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "0394974d06477064c2a151e499a847002106fe45",
            "filename": "src/transformers/models/clip/modeling_flax_clip.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1306,
            "changes": 1306,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_flax_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_flax_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_flax_clip.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "ab2e38827998ad4c67059b42387dbe9c192c6a18",
            "filename": "src/transformers/models/clip/modeling_tf_clip.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1460,
            "changes": 1460,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_tf_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_tf_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_tf_clip.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "e8cd47b0aa54ebe804c38e9ee211c22feeb8fe4c",
            "filename": "src/transformers/models/clipseg/processing_clipseg.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fclipseg%2Fprocessing_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fclipseg%2Fprocessing_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fprocessing_clipseg.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "077e70af67b182325fadfab318dd71cce66b003c",
            "filename": "src/transformers/models/clvp/feature_extraction_clvp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fclvp%2Ffeature_extraction_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fclvp%2Ffeature_extraction_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Ffeature_extraction_clvp.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "658f3cfca1acca3960c4fa374ea8689222c4269c",
            "filename": "src/transformers/models/codegen/configuration_codegen.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fcodegen%2Fconfiguration_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fcodegen%2Fconfiguration_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fconfiguration_codegen.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "aae404d1e7f3b7c6c720a1f5ffd91a8492a14e03",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "d8a5a2745ae74273917cfcd28e84841a3fcb35bf",
            "filename": "src/transformers/models/codegen/tokenization_codegen.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "4cbeff06ad89f6363b313fae125f3d27dabde72e",
            "filename": "src/transformers/models/codegen/tokenization_codegen_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen_fast.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "8072cbe7c17cfd33d5052327bc51d3302f0d5e65",
            "filename": "src/transformers/models/cohere/tokenization_cohere_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fcohere%2Ftokenization_cohere_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fcohere%2Ftokenization_cohere_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Ftokenization_cohere_fast.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "cde77af658bca052f584750fb287ee584a7cab71",
            "filename": "src/transformers/models/cohere2_vision/processing_cohere2_vision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fprocessing_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fprocessing_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fprocessing_cohere2_vision.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "0c932a732258b3c581f0099feae8da4af4ba82e7",
            "filename": "src/transformers/models/colpali/modular_colpali.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodular_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodular_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodular_colpali.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "5d77eced20d9a8f779f96f7dcfbc111ad03a4a9f",
            "filename": "src/transformers/models/colpali/processing_colpali.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "a9a1f8ce3e1e861be1d773ce1b67b9f612a4170e",
            "filename": "src/transformers/models/colqwen2/modular_colqwen2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "372ce542d580a34227a76c6b290fcc6e31cb0531",
            "filename": "src/transformers/models/colqwen2/processing_colqwen2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "cf506b834918c26e30ac5a755c9bff9f667b5676",
            "filename": "src/transformers/models/conditional_detr/image_processing_conditional_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 38,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "2ee35cc19a3f79b062a2f1edebdf11956726d490",
            "filename": "src/transformers/models/conditional_detr/modeling_conditional_detr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "20999ba510da0b352b71b75fca6246382fd4727a",
            "filename": "src/transformers/models/convbert/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fconvbert%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fconvbert%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2F__init__.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "350e5a7f3f90ec83eeeb25e17dc8a6bfc54a6e63",
            "filename": "src/transformers/models/convbert/convert_convbert_original_tf1_checkpoint_to_pytorch.py",
            "status": "added",
            "additions": 183,
            "deletions": 0,
            "changes": 183,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fconvbert%2Fconvert_convbert_original_tf1_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fconvbert%2Fconvert_convbert_original_tf1_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2Fconvert_convbert_original_tf1_checkpoint_to_pytorch.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "3d4ff779874b30b0c094c596cedaca597e03ed36",
            "filename": "src/transformers/models/convbert/convert_convbert_original_tf1_checkpoint_to_pytorch_and_tf2.py",
            "status": "removed",
            "additions": 0,
            "deletions": 57,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fconvbert%2Fconvert_convbert_original_tf1_checkpoint_to_pytorch_and_tf2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fconvbert%2Fconvert_convbert_original_tf1_checkpoint_to_pytorch_and_tf2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2Fconvert_convbert_original_tf1_checkpoint_to_pytorch_and_tf2.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "5f4dd419b4fc1606bc0243ef4796f1b703489086",
            "filename": "src/transformers/models/convbert/modeling_convbert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 132,
            "changes": 132,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "47c720f5c12c36a67ed3441d4dcccea329b69f70",
            "filename": "src/transformers/models/convbert/modeling_tf_convbert.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1474,
            "changes": 1474,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_tf_convbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_tf_convbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_tf_convbert.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "92f60410082221a755376270a52485c7b165858e",
            "filename": "src/transformers/models/convnext/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fconvnext%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fconvnext%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2F__init__.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "ae0be69a56216e5c211cca1f14a58acfcd5d8b1a",
            "filename": "src/transformers/models/convnext/image_processing_convnext.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "e3224c29405fed1d58a29d7eb013bdddcd507013",
            "filename": "src/transformers/models/convnext/modeling_convnext.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_convnext.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "7306877466d9b793682d01d90645f08420930b59",
            "filename": "src/transformers/models/convnext/modeling_tf_convnext.py",
            "status": "removed",
            "additions": 0,
            "deletions": 667,
            "changes": 667,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_tf_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_tf_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_tf_convnext.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "9e02170eceae73027828d4f184be8e830c02cb8f",
            "filename": "src/transformers/models/convnextv2/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fconvnextv2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fconvnextv2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnextv2%2F__init__.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "3bf6130824ed5dbac7ea0f98a8ae33ef6cd050df",
            "filename": "src/transformers/models/convnextv2/modeling_convnextv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_convnextv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_convnextv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_convnextv2.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "d370c3008d4701cdd4c3c78572a29218cc55693b",
            "filename": "src/transformers/models/convnextv2/modeling_tf_convnextv2.py",
            "status": "removed",
            "additions": 0,
            "deletions": 681,
            "changes": 681,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_tf_convnextv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_tf_convnextv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_tf_convnextv2.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "7e16ecbb60012f53ceee884b0f677072f12079f0",
            "filename": "src/transformers/models/csm/processing_csm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fcsm%2Fprocessing_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fcsm%2Fprocessing_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fprocessing_csm.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "93f27ba0710eecb71d1a4ba6b7c306c7ef7d13e8",
            "filename": "src/transformers/models/ctrl/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fctrl%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fctrl%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2F__init__.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "03da5b51c907261b0c822e40f232aacd32dde412",
            "filename": "src/transformers/models/ctrl/modeling_ctrl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "1dce90147bd854b5072410eb6d6e74f73c0a9fd0",
            "filename": "src/transformers/models/ctrl/modeling_tf_ctrl.py",
            "status": "removed",
            "additions": 0,
            "deletions": 920,
            "changes": 920,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_tf_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_tf_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_tf_ctrl.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "08a67f82b4114debab23ec9b3fbf4b3aa280961f",
            "filename": "src/transformers/models/cvt/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fcvt%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fcvt%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcvt%2F__init__.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "bd27b2db7f2462abedaa39cbcf24a062df8f5043",
            "filename": "src/transformers/models/cvt/modeling_cvt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "9239e1918eeca7588e0a978af4ef524380cd3f5f",
            "filename": "src/transformers/models/cvt/modeling_tf_cvt.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1095,
            "changes": 1095,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_tf_cvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_tf_cvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_tf_cvt.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "cbb7450c7f0b84d3ab55c407a6f55526134f05e9",
            "filename": "src/transformers/models/dab_detr/modeling_dab_detr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "21af67e2233ac3b371a60441f7a6a1cb495d367b",
            "filename": "src/transformers/models/dac/feature_extraction_dac.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdac%2Ffeature_extraction_dac.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdac%2Ffeature_extraction_dac.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdac%2Ffeature_extraction_dac.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "4fcf78dd606f56c7b0def6d4ff1a3fb8f49a8fa0",
            "filename": "src/transformers/models/data2vec/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdata2vec%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdata2vec%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2F__init__.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "1d901908f81895804222d4d66e004ed58f67dce6",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "e5925862521064fcf8b90045166125eb1dc08095",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_vision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "0fa0fe1f811ee1748956c2481d28b0ad1ef516e0",
            "filename": "src/transformers/models/data2vec/modeling_tf_data2vec_vision.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1723,
            "changes": 1723,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_tf_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_tf_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_tf_data2vec_vision.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "ac2dbc3af259ffa4f7a353641b79673184b9a060",
            "filename": "src/transformers/models/deberta/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeberta%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeberta%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta%2F__init__.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "49015eb7cc5b68d87ebc04ba6720aee875c42412",
            "filename": "src/transformers/models/deberta/configuration_deberta.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeberta%2Fconfiguration_deberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeberta%2Fconfiguration_deberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta%2Fconfiguration_deberta.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "3074db6ca00a49fae6e448a113aaa78bf98f3969",
            "filename": "src/transformers/models/deberta/modeling_deberta.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_deberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_deberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_deberta.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "40d23fc28b9475623dc8a94d5539c5a530b3f376",
            "filename": "src/transformers/models/deberta/modeling_tf_deberta.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1652,
            "changes": 1652,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_tf_deberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_tf_deberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_tf_deberta.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "929b26e60ae0b79073925fa82b1d0af16ebbab07",
            "filename": "src/transformers/models/deberta_v2/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2F__init__.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "43576e815d074845360010d61f81bf525b6853d0",
            "filename": "src/transformers/models/deberta_v2/configuration_deberta_v2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fconfiguration_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fconfiguration_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fconfiguration_deberta_v2.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "71bf04b955425d59d3e6b18d0cf8a3f74438dba2",
            "filename": "src/transformers/models/deberta_v2/modeling_deberta_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "d71891ac19c00a8bc3ddb63e547b404570059a7a",
            "filename": "src/transformers/models/deberta_v2/modeling_tf_deberta_v2.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1879,
            "changes": 1879,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_tf_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_tf_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_tf_deberta_v2.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "a16c9ab71075c91c2e2ff502cd78f3937cae9a0a",
            "filename": "src/transformers/models/decision_transformer/modeling_decision_transformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 63,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "9d3d9a408a0038769fdd51d5b6ac47bc471960d8",
            "filename": "src/transformers/models/deepseek_vl/image_processing_deepseek_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "ce2f9be16ae6deab1f478029fa6444cd117f0bb8",
            "filename": "src/transformers/models/deepseek_vl/modular_deepseek_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "ddeb4f799ee1be600f1d56eb463baf98df942452",
            "filename": "src/transformers/models/deepseek_vl/processing_deepseek_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fprocessing_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fprocessing_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fprocessing_deepseek_vl.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "d3d5a7e3e54211509318b34d1253641ce24f6433",
            "filename": "src/transformers/models/deepseek_vl_hybrid/image_processing_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "e9808b02ce345db12c07441a401362ecceafbca9",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modular_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "d20fa495f9b8ed3a0400650048424746e1c4c596",
            "filename": "src/transformers/models/deepseek_vl_hybrid/processing_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fprocessing_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fprocessing_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fprocessing_deepseek_vl_hybrid.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "ef028eda1ed18fadf135f065a0ada8d185465b77",
            "filename": "src/transformers/models/deformable_detr/image_processing_deformable_detr.py",
            "status": "modified",
            "additions": 3,
            "deletions": 38,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "657e78be87ef53b3ab4943013df6f993ef9169ee",
            "filename": "src/transformers/models/deformable_detr/modeling_deformable_detr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "ef3e5149fe55173c24aac914e137c334c2770105",
            "filename": "src/transformers/models/deit/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeit%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeit%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2F__init__.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "8909fe5aff665d1e05dbcd08fad3c638174ac498",
            "filename": "src/transformers/models/deit/configuration_deit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeit%2Fconfiguration_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeit%2Fconfiguration_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fconfiguration_deit.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "795c872c62e582f022880c3e5d193578a74b60ad",
            "filename": "src/transformers/models/deit/image_processing_deit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeit%2Fimage_processing_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeit%2Fimage_processing_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fimage_processing_deit.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "3c56eee87911edc445641e0bbc14f094e1c5efa7",
            "filename": "src/transformers/models/deit/modeling_tf_deit.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1232,
            "changes": 1232,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_tf_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_tf_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_tf_deit.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "b54e07d240eaae93b52f4ba11830686b0639478d",
            "filename": "src/transformers/models/deprecated/deta/image_processing_deta.py",
            "status": "modified",
            "additions": 2,
            "deletions": 36,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fimage_processing_deta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fimage_processing_deta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fimage_processing_deta.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "1e233608e3e45183ccd6bbcdc83a62d528b42430",
            "filename": "src/transformers/models/deprecated/deta/modeling_deta.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "e43cb0e61df1c0242153bebd417a3bebcafc78ec",
            "filename": "src/transformers/models/deprecated/efficientformer/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2F__init__.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "a8dcedea620afd9e43dbbd5e39fb3fa2b0a58ea2",
            "filename": "src/transformers/models/deprecated/efficientformer/image_processing_efficientformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fimage_processing_efficientformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fimage_processing_efficientformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fimage_processing_efficientformer.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "2167df912d8711ed0225c8d6c8a05eb7c4d4ce8c",
            "filename": "src/transformers/models/deprecated/efficientformer/modeling_efficientformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fmodeling_efficientformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fmodeling_efficientformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fmodeling_efficientformer.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "643097e79c3eab47b62f7e7c4781b2b276418176",
            "filename": "src/transformers/models/deprecated/efficientformer/modeling_tf_efficientformer.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1198,
            "changes": 1198,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fmodeling_tf_efficientformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fmodeling_tf_efficientformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fmodeling_tf_efficientformer.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "4cecdf5728a3cf9ace0a452ebecd0f63cf85f4df",
            "filename": "src/transformers/models/deprecated/ernie_m/modeling_ernie_m.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fmodeling_ernie_m.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fmodeling_ernie_m.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fmodeling_ernie_m.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "eb5b030bbf8296d1149e488d406e34f1603da5ad",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/modeling_gptsan_japanese.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "daaa4b2ee4899b4ef970933616ae8b0902619f71",
            "filename": "src/transformers/models/deprecated/jukebox/modeling_jukebox.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "473d23d495650d5d3c59da0c71ca9ae88800fec4",
            "filename": "src/transformers/models/deprecated/jukebox/tokenization_jukebox.py",
            "status": "modified",
            "additions": 3,
            "deletions": 20,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Ftokenization_jukebox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Ftokenization_jukebox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Ftokenization_jukebox.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "0ce7c1da31a2a239d5c52ccddf78879ca63b69fa",
            "filename": "src/transformers/models/deprecated/mctct/feature_extraction_mctct.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Ffeature_extraction_mctct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Ffeature_extraction_mctct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Ffeature_extraction_mctct.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "357b8b2c368198ed1365279d85872561257d43c9",
            "filename": "src/transformers/models/deprecated/mctct/modeling_mctct.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "02e41e91b24cea44a32cbcc6eefb3aa7bc8ae001",
            "filename": "src/transformers/models/deprecated/mega/modeling_mega.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "1667c98297fa40e9501472dbd00cb8291c9a8c16",
            "filename": "src/transformers/models/deprecated/nat/modeling_nat.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fmodeling_nat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fmodeling_nat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fmodeling_nat.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "eaf1cedfed32d4a3ec5e16526ade08fda505f745",
            "filename": "src/transformers/models/deprecated/nezha/modeling_nezha.py",
            "status": "modified",
            "additions": 0,
            "deletions": 79,
            "changes": 79,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "f522a1d721549f7eadccd8c8fa800f34454ec053",
            "filename": "src/transformers/models/deprecated/qdqbert/modeling_qdqbert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 80,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "69bab60f6803ec985dd9c66531eeea62dc8175f3",
            "filename": "src/transformers/models/deprecated/realm/modeling_realm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 114,
            "changes": 114,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "354ca2aba63ad853cf7cb6962c75eb8b50551294",
            "filename": "src/transformers/models/deprecated/realm/retrieval_realm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fretrieval_realm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fretrieval_realm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fretrieval_realm.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "926d7551e51bb7eff3e1768086b139a168df3e82",
            "filename": "src/transformers/models/deprecated/retribert/modeling_retribert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Fmodeling_retribert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Fmodeling_retribert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Fmodeling_retribert.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "fa74d8aa3b557c48cd9561c9f1b28aa2008b1b7f",
            "filename": "src/transformers/models/deprecated/tapex/tokenization_tapex.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftapex%2Ftokenization_tapex.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftapex%2Ftokenization_tapex.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftapex%2Ftokenization_tapex.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "1b4126f9ef2002bbd5c449930fbcc8f824985d17",
            "filename": "src/transformers/models/deprecated/trajectory_transformer/modeling_trajectory_transformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 76,
            "changes": 76,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2Fmodeling_trajectory_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2Fmodeling_trajectory_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2Fmodeling_trajectory_transformer.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "9bd3dd7b8838be6c8ad766aff370d675c3ad15ff",
            "filename": "src/transformers/models/deprecated/transfo_xl/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2F__init__.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "989a70ef71bd9156b353c0b4b9080a264aff8692",
            "filename": "src/transformers/models/deprecated/transfo_xl/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 129,
            "deletions": 1,
            "changes": 130,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fconvert_transfo_xl_original_tf_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fconvert_transfo_xl_original_tf_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fconvert_transfo_xl_original_tf_checkpoint_to_pytorch.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "3c7830d633448bde3b8a420f0d84cdd070498882",
            "filename": "src/transformers/models/deprecated/transfo_xl/modeling_tf_transfo_xl.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1128,
            "changes": 1128,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_tf_transfo_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_tf_transfo_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_tf_transfo_xl.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "48205e06fb20a473959544db4971dff0d3e58cbf",
            "filename": "src/transformers/models/deprecated/transfo_xl/modeling_tf_transfo_xl_utilities.py",
            "status": "removed",
            "additions": 0,
            "deletions": 178,
            "changes": 178,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_tf_transfo_xl_utilities.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_tf_transfo_xl_utilities.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_tf_transfo_xl_utilities.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "a7b4825e5fcd5db472b8c4c83127822712157440",
            "filename": "src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 129,
            "changes": 129,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "e7081cd46d423f86ae797d8c52079844197b89f8",
            "filename": "src/transformers/models/deprecated/transfo_xl/tokenization_transfo_xl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Ftokenization_transfo_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Ftokenization_transfo_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Ftokenization_transfo_xl.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "19b5cddb246bdf698361e10a3cc99f85d63875b8",
            "filename": "src/transformers/models/deprecated/tvlt/image_processing_tvlt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fimage_processing_tvlt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fimage_processing_tvlt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fimage_processing_tvlt.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "82aa12ada9e9b85652c52cba0a110daa800f5afb",
            "filename": "src/transformers/models/deprecated/tvlt/modeling_tvlt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fmodeling_tvlt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fmodeling_tvlt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fmodeling_tvlt.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "6ee0e881e55883603d914be6f9b2f70307d21fb3",
            "filename": "src/transformers/models/deprecated/van/modeling_van.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fmodeling_van.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fmodeling_van.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fmodeling_van.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "662382be43dff424c857254d1b7a3e17c2791917",
            "filename": "src/transformers/models/deprecated/vit_hybrid/image_processing_vit_hybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fimage_processing_vit_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fimage_processing_vit_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fimage_processing_vit_hybrid.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "5710016bd51361dede8c73cef68687c6234148f6",
            "filename": "src/transformers/models/depth_anything/modeling_depth_anything.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fmodeling_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fmodeling_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fmodeling_depth_anything.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "5b76d8cbc8e6edf87bbb5bffb6f033d5bc7bd5f3",
            "filename": "src/transformers/models/depth_pro/image_processing_depth_pro.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "9fb4c35b23e597849af609d6e40e1008c0cf2b84",
            "filename": "src/transformers/models/depth_pro/modeling_depth_pro.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "7a2e67f83de655be9e6d6462d2cd2de7f7c29a7d",
            "filename": "src/transformers/models/detr/image_processing_detr.py",
            "status": "modified",
            "additions": 3,
            "deletions": 39,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "f30ebfa418590f3f74ee58d901889fa309f6f91c",
            "filename": "src/transformers/models/detr/image_processing_detr_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "89441a8b1246f964234a4b3dc26bcaa6457f3e3f",
            "filename": "src/transformers/models/detr/modeling_detr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "dcb32d2be6f4f16d5f914eb51dfedda62720f3f6",
            "filename": "src/transformers/models/dia/feature_extraction_dia.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdia%2Ffeature_extraction_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdia%2Ffeature_extraction_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Ffeature_extraction_dia.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "a65b4862c473d2329b5c64af63d1c233a51a79b0",
            "filename": "src/transformers/models/dinat/modeling_dinat.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "002634ed4b490fdc845c911b4a5515fe9833db5d",
            "filename": "src/transformers/models/dinov2/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdinov2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdinov2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2%2F__init__.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "f84d442a3efca278b3d53c656d57991539fae154",
            "filename": "src/transformers/models/dinov2/modeling_dinov2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "b9ea2eaa3ebc5ac5db192dde220dfe114ef38235",
            "filename": "src/transformers/models/dinov2/modeling_flax_dinov2.py",
            "status": "removed",
            "additions": 0,
            "deletions": 801,
            "changes": 801,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_flax_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_flax_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_flax_dinov2.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "042c21babd19cbbbc6ca9666e74d41c2663c8c85",
            "filename": "src/transformers/models/dinov2_with_registers/modeling_dinov2_with_registers.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "8eef42c03d17a23d9737ded760315487bb06b01d",
            "filename": "src/transformers/models/dinov3_convnext/modeling_dinov3_convnext.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2Fmodeling_dinov3_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2Fmodeling_dinov3_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2Fmodeling_dinov3_convnext.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "76e365903082950479a84bb8874d193d8ab11691",
            "filename": "src/transformers/models/dinov3_vit/modeling_dinov3_vit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodeling_dinov3_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodeling_dinov3_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodeling_dinov3_vit.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "094524ab267f37e2fd16424da0ad09804abb7f7a",
            "filename": "src/transformers/models/distilbert/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdistilbert%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdistilbert%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2F__init__.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "8f0cdcd76898b4921e89243e49f4129ef1704614",
            "filename": "src/transformers/models/distilbert/modeling_distilbert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "fba3dfd9d332d5f34dfea23a20cec7a7871de9e4",
            "filename": "src/transformers/models/distilbert/modeling_flax_distilbert.py",
            "status": "removed",
            "additions": 0,
            "deletions": 906,
            "changes": 906,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_flax_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_flax_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_flax_distilbert.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "a2efa1105c1cde9236e9c32627a5773cb38cc862",
            "filename": "src/transformers/models/distilbert/modeling_tf_distilbert.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1146,
            "changes": 1146,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_tf_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_tf_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_tf_distilbert.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "f49cc964080d1df1cc6daacbe4d1943c56c03d2c",
            "filename": "src/transformers/models/donut/image_processing_donut.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "c541b960fd2ed9a2cb252d59ec609245574ece09",
            "filename": "src/transformers/models/donut/modeling_donut_swin.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "386b47bb2ecd07ed85a243e7c1920c8d2bfb4441",
            "filename": "src/transformers/models/dpr/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdpr%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdpr%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpr%2F__init__.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "7ee4dcaf52e1b9afddcb7c488beaeb89445b59bd",
            "filename": "src/transformers/models/dpr/modeling_dpr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdpr%2Fmodeling_dpr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdpr%2Fmodeling_dpr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpr%2Fmodeling_dpr.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "aef83e6c55fbe27ea57e48bf2baca515999010cb",
            "filename": "src/transformers/models/dpr/modeling_tf_dpr.py",
            "status": "removed",
            "additions": 0,
            "deletions": 799,
            "changes": 799,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fdpr%2Fmodeling_tf_dpr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fdpr%2Fmodeling_tf_dpr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpr%2Fmodeling_tf_dpr.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "1a87ef9fd9157aff7b00f669a464844cd38d5c7b",
            "filename": "src/transformers/models/dpr/tokenization_dpr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdpr%2Ftokenization_dpr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdpr%2Ftokenization_dpr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpr%2Ftokenization_dpr.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "5f501dbdd4f08f2e26cb378f47f98bc85237f217",
            "filename": "src/transformers/models/dpr/tokenization_dpr_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdpr%2Ftokenization_dpr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdpr%2Ftokenization_dpr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpr%2Ftokenization_dpr_fast.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "d0263630b075c34385a80faa0f28841e1ffa0ab5",
            "filename": "src/transformers/models/dpt/configuration_dpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdpt%2Fconfiguration_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdpt%2Fconfiguration_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fconfiguration_dpt.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "0ec3eaed1c4306c1547eb7738f8bff00f9ba763c",
            "filename": "src/transformers/models/dpt/image_processing_dpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "3e80ad7943db6fdc92e018d6793a8ea75acd5058",
            "filename": "src/transformers/models/dpt/image_processing_dpt_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt_fast.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "7be71fd3ceb4371e69663402cdd0a84afb640acb",
            "filename": "src/transformers/models/dpt/modeling_dpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "5b87278683acb33bfde2d332b7da647cc2bc8386",
            "filename": "src/transformers/models/efficientloftr/image_processing_efficientloftr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "eaad420b31f8df0c7482da664aff587bb2b4a2e7",
            "filename": "src/transformers/models/efficientnet/image_processing_efficientnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "e368fefa0e79e41c12f56bf73ac172d9b435b84f",
            "filename": "src/transformers/models/efficientnet/modeling_efficientnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "506212b561e1b666bc45c80285a6884f3eb5371e",
            "filename": "src/transformers/models/electra/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Felectra%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Felectra%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2F__init__.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "00d6fecc21b04a93e4acb8d7143d132be1f9fb43",
            "filename": "src/transformers/models/electra/convert_electra_original_tf_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 85,
            "deletions": 1,
            "changes": 86,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Felectra%2Fconvert_electra_original_tf_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Felectra%2Fconvert_electra_original_tf_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fconvert_electra_original_tf_checkpoint_to_pytorch.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "d3b47ea55b790af4f3c167278ca6286b32b0afdf",
            "filename": "src/transformers/models/electra/modeling_electra.py",
            "status": "modified",
            "additions": 0,
            "deletions": 89,
            "changes": 89,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "14d845476d62f9defb2de4392742037762fb959f",
            "filename": "src/transformers/models/electra/modeling_flax_electra.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1614,
            "changes": 1614,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_flax_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_flax_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_flax_electra.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "3a5c33e503d7386df5c2be0fc10a079ee4fe014a",
            "filename": "src/transformers/models/electra/modeling_tf_electra.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1775,
            "changes": 1775,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_tf_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_tf_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_tf_electra.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "c46dce41f529055cfaae752f80ed93a6f65f9562",
            "filename": "src/transformers/models/emu3/image_processing_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "ef2681d2385b7c021fdaa36d78c9931154b5db82",
            "filename": "src/transformers/models/emu3/processing_emu3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "1086bdfb228e8affc28278dd70230c5fab226789",
            "filename": "src/transformers/models/encodec/feature_extraction_encodec.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fencodec%2Ffeature_extraction_encodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fencodec%2Ffeature_extraction_encodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencodec%2Ffeature_extraction_encodec.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "b1cde1442a13b526aa55a919fbc1462fd52459b3",
            "filename": "src/transformers/models/encoder_decoder/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2F__init__.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "55a736fd9034d7b36681fab2a302da6c91155eeb",
            "filename": "src/transformers/models/encoder_decoder/modeling_encoder_decoder.py",
            "status": "modified",
            "additions": 0,
            "deletions": 104,
            "changes": 104,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df2529d79d75f44e70396df5888a32ffa02d61e/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py?ref=4df2529d79d75f44e70396df5888a32ffa02d61e"
        },
        {
            "sha": "4a27c23c3c69ae928c73273c9397d5f5aad2b1c0",
            "filename": "src/transformers/models/encoder_decoder/modeling_flax_encoder_decoder.py",
            "status": "removed",
            "additions": 0,
            "deletions": 901,
            "changes": 901,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_flax_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_flax_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_flax_encoder_decoder.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        },
        {
            "sha": "7e5343d200499e1f3b8ba26f8d70924c2999a2fc",
            "filename": "src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py",
            "status": "removed",
            "additions": 0,
            "deletions": 661,
            "changes": 661,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_tf_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ac3c5171a6b01708e574eaa11fa2a60086d26bc/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_tf_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_tf_encoder_decoder.py?ref=5ac3c5171a6b01708e574eaa11fa2a60086d26bc"
        }
    ],
    "stats": {
        "total": 185061,
        "additions": 3792,
        "deletions": 181269
    }
}