{
    "author": "keyboardAnt",
    "message": "Universal Speculative Decoding `CandidateGenerator` (#35029)\n\n* move `TestAssistedCandidateGeneratorDifferentTokenizers` into a new testing file\n\n* refactor\n\n* NOTHING. add space to rerun github actions tests\n\n* remove it...\n\n* `UniversalSpeculativeDecodingGenerator`\n\n* Use `UniversalSpeculativeDecodingGenerator` when `generation_config.do_sample=True`\n\n* assistant tokenizes only the target's new suffix\n\n* formatting\n\n* fix code\n\n* fix code\n\n* formatting\n\n* add `TestGenerateWithDifferentModels`\n\n* `TestGenerateWithDifferentModels` parameterize on `do_sample`\n\n* `AssistantVocabMapping` & `AssistantVocabMappingCache`\n\n* formatting\n\n* `AssistantToTargetTranslator`: `get_target_input_ids` & `get_target_logits`\n\n* improve `_get_assistant_to_target_input_ids` & formatting\n\n* renaming\n\n* WIP: debugging `min_new_tokens`\n\n* fix get_target_ids\n\n* `UniversalSpeculativeDecodingGenerator`\n\n* assistant tokenizes only the target's new suffix\n\n* formatting\n\n* fix code\n\n* fix code\n\n* formatting\n\n* `TestGenerateWithDifferentModels` parameterize on `do_sample`\n\n* `AssistantVocabMapping` & `AssistantVocabMappingCache`\n\n* formatting\n\n* `AssistantToTargetTranslator`: `get_target_input_ids` & `get_target_logits`\n\n* improve `_get_assistant_to_target_input_ids` & formatting\n\n* renaming\n\n* WIP: debugging `min_new_tokens`\n\n* fix get_target_ids\n\n* fix device issue\n\n* fix get_assistant_input_ids\n\n* add `TestAssistedCandidateGeneratorDifferentTokenizers`\n\n* formatting\n\n* `AssistantVocabTranslatorCache` refactor & tests\n\n* revert changes in `src/transformers/generation/logits_process.py`\n\n* refactor `AssistedCandidateGenerator`\n\n* refactor `AssistedCandidateGeneratorDifferentTokenizers`\n\n* formatting\n\n* refactor `UniversalSpeculativeDecodingGenerator`\n\n* fix negative value for max_new_tokens\n\n* fix generation length target + attention_mask vs. assistant + attent\n\n* fix device\n\n* fix negative max_new_tokens bug\n\n* fix UAG\n\n* minor\n\n* formatting\n\n* `AssistedCandidateGeneratorDifferentTokenizers` `lookbehind`s init\n\n* resolve conflict & formatting\n\n* rerun CI tests\n\n* remove space...\n\n* remove old code\n\n* fix candidate_input_ids device\n\n* minor\n\n* formatting\n\n* Fix prepare + apply (#7)\n\n* fix prepare + apply\n\n* move to cpu\n\n* simplity suppress_tokens\n\n* fix bugs and refacatoring\n\n* device move\n\n* handle self.config.vocab_size > len(target_tokenizer.get_vocab())\n\n* no need to normalize in candidate_generator\n\n* address Nadav's comments + minor\n\n* optimize device move + SuppressTokensLogitsProcessor\n\n* AssistantToTargetTranslator, SuppressTokensLogitsProcessor and tokenizers mapping improvements\n\n* padding size\n\n* padding improvement\n\n* fix and simplify get_target_logits\n\n* renaming in get_target_logits\n\n* minor\n\n* add filter_value and suppress_tokens_id\n\n* style + rename\n\n* remove TODO\n\n* restore original SelectTokensLogitsProcessor with modification\n\n* fix style\n\n* fix _update_past_and_masks and optimize code\n\n* remove assistant_vocab_size arg\n\n* fix attention_mask\n\n* call _prepare_attention_mask also if not has_past_key_values\n\n* handling attention mask for first generation\n\n* comment\n\n* restore test\n\n* remove SelectTokensLogitsProcessor\n\n* _update_past_and_masks implementation for USD\n\n* Add unittests for Universal Assisted generation\n\n* fix style\n\n* update tests\n\n* Remove unused import and fix `test_speculation_depth` test\n\n* exclude special and reserved tokens from tokenizer for UAG\n\n* mv `test_universal_assisted_generation.py` to `generation/test_candidate_generator.py`\n\n* Remove unused imports and fix style using `make style` (#9)\n\n* formatting\n\n* Swap gated `meta-llama/llama-3.2` with `allenai/llama` (#10)\n\n* Fix space sign disagreement (#12)\n\n* default values for AssistantToTargetTranslator fileds\r\n\r\n* fix space sign\r\n\r\n* minor\r\n\r\n* fix test + style\n\n* Default values for some fields of assistant to target translator (#11)\n\n* default values for AssistantToTargetTranslator fileds\r\n\r\n* fix\r\n\r\n* add support to empty logit_processors\n\n* Update candidate_generator.py (#15)\n\nfix typo\n\n* BUG fix in _prepare_assistant_input_ids (#14)\n\n* fix _prepare_assistant_input_ids\n\n* target_to_assistant_input_ids\n\n* Update src/transformers/generation/candidate_generator.py\n\nCo-authored-by: Nadav Timor <nadav.timor@weizmann.ac.il>\n\n---------\n\nCo-authored-by: Nadav Timor <nadav.timor@weizmann.ac.il>\n\n* typo (`target_to_assistant_input_ids`)\n\n* formatting\n\n* merge upstream/main\n\n* Fix minor review comments (#16)\n\n* Fix: `token_ids.to(torch.int64)` (#18)\n\n* tok ids to `torch.int64` (reference: https://huggingface.co/docs/transformers.js/en/api/tokenizers)\n\n* `LongTensor`\n\n* fix dtype\n\n* `assistant_input_ids.to(dtype=torch.long)`\n\n* Remove unused import from test_candidate_generator.py\n\n* Remove unused import from test_candidate_generator.py\n\n* Remove `numpy` import\n\n* resolve pr comments (#19)\n\n* `AssistantToTargetTranslator` docstring\n\n* (per gante's comment) `filter_value` and `suppress_tokens_id` to class constants\n\n* update `AssistantToTargetTranslator` docstring\n\n* (gante's comment) replace `match-case`\n\n* formatting\n\n* Fix Joao's comments (#21)\n\n* remove threading\n\n* fix logits_processor\n\n* fix test device\n\n* fix style (#23)\n\n* Move atm (#24)\n\n* move AssistantToTargetTranslator\n\n* fixup\n\n* fix logit_processor\n\n* add atm_translator test\n\n* refactor test\n\n* remove threading from test\n\n* add require_torch in tests\n\n* move AssistantVocabTranslatorCache + add tests\n\n* ruff fix\n\n---------\n\nCo-authored-by: jmamou <jonathan.mamou@intel.com>\nCo-authored-by: Gaurav <gauravj@d-matrix.ai>\nCo-authored-by: Gaurav Jain <gaurjain14@gmail.com>\nCo-authored-by: gauravjain14 <41287729+gauravjain14@users.noreply.github.com>",
    "sha": "d18d9c3205323d02479ca0d7cfd978908ebb6f71",
    "files": [
        {
            "sha": "ebb9a18d559f064bbf81641a7a2168c4c045117a",
            "filename": "src/transformers/generation/candidate_generator.py",
            "status": "modified",
            "additions": 291,
            "deletions": 3,
            "changes": 294,
            "blob_url": "https://github.com/huggingface/transformers/blob/d18d9c3205323d02479ca0d7cfd978908ebb6f71/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d18d9c3205323d02479ca0d7cfd978908ebb6f71/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py?ref=d18d9c3205323d02479ca0d7cfd978908ebb6f71",
            "patch": "@@ -14,6 +14,7 @@\n # limitations under the License.\n \n import copy\n+import weakref\n from typing import TYPE_CHECKING, Any, Dict, Optional, Tuple\n \n import numpy as np\n@@ -27,7 +28,7 @@\n \n from ..cache_utils import DynamicCache\n from ..pytorch_utils import isin_mps_friendly\n-from .logits_process import LogitsProcessorList, MinLengthLogitsProcessor\n+from .logits_process import LogitsProcessorList, MinLengthLogitsProcessor, SuppressTokensLogitsProcessor\n \n \n if TYPE_CHECKING:\n@@ -283,18 +284,21 @@ def _calculate_new_tokens(self, input_ids: torch.LongTensor) -> Tuple[int, int]:\n         min_new_tokens = max(min(max_new_tokens, self.main_model_min_length - new_cur_len), 0)\n         return min_new_tokens, max_new_tokens\n \n-    def _update_past_and_masks(self, input_ids: torch.LongTensor, remove_from_pkv: int = 0) -> bool:\n+    def _update_past_and_masks(\n+        self, input_ids: torch.LongTensor, remove_from_pkv: int = 0, num_added_tokens: int = 1\n+    ) -> bool:\n         \"\"\"Update past key values and attention masks for subsequent generation rounds.\"\"\"\n         has_past_key_values = self.assistant_kwargs.get(\"past_key_values\", None) is not None\n         if has_past_key_values:\n             new_cache_size = input_ids.shape[-1] - 1 - remove_from_pkv\n             self.assistant_kwargs[\"past_key_values\"] = _crop_past_key_values(\n-                self.assistant_model, self.assistant_kwargs[\"past_key_values\"], new_cache_size - 1\n+                self.assistant_model, self.assistant_kwargs[\"past_key_values\"], new_cache_size - num_added_tokens\n             )\n             self.assistant_kwargs = _prepare_attention_mask(\n                 self.assistant_kwargs, input_ids.shape[-1], self.assistant_model.config.is_encoder_decoder\n             )\n             self.assistant_kwargs = _prepare_token_type_ids(self.assistant_kwargs, input_ids.shape[-1])\n+\n         return has_past_key_values\n \n     def _prepare_generation_args(self, input_ids: torch.LongTensor, min_new_tokens: int, max_new_tokens: int) -> Dict:\n@@ -608,6 +612,290 @@ def _process_assistant_outputs(\n         return new_target_ids\n \n \n+class AssistantToTargetTranslator:\n+    \"\"\"\n+    Translates token ids and logits between assistant and target model vocabularies. This class is used to handle\n+    vocabulary mismatches when using different tokenizers for the assistant and target models in speculative decoding,\n+    as introduced in the paper \"Lossless Speculative Decoding Algorithms for Heterogeneous Vocabularies\"\n+    (https://www.arxiv.org/abs/2502.05202).\n+    It maintains mappings between the two vocabularies and handles token/logit conversion.\n+\n+    Args:\n+        target_tokenizer (`PreTrainedTokenizerBase`):\n+            The tokenizer used by the target (main) model.\n+        assistant_tokenizer (`PreTrainedTokenizerBase`):\n+            The tokenizer used by the assistant model.\n+        assistant_model_device (`str`, defaults to \"cpu\"):\n+            The device where the assistant model is located. Used for placing tensors.\n+        target_vocab_size (`int`, *optional*):\n+            The size of the target model's vocabulary. If not provided, will be inferred from the target tokenizer.\n+    \"\"\"\n+\n+    FILTER_VALUE: float = -float(\"Inf\")  # The value used to filter out unmapped tokens in the logits.\n+    SUPPRESS_TOKEN_ID: int = -1  # The ID used to mark suppressed tokens in the mapping.\n+\n+    def __init__(\n+        self,\n+        target_tokenizer: \"PreTrainedTokenizerBase\",\n+        assistant_tokenizer: \"PreTrainedTokenizerBase\",\n+        target_vocab_size: int,  # required since target_vocab_size can be different from the length of target_tokenizer.get_vocab()\n+        assistant_model_device: str = \"cpu\",\n+    ):\n+        self._target_tokenizer: \"PreTrainedTokenizerBase\" = target_tokenizer\n+        self._assistant_tokenizer: \"PreTrainedTokenizerBase\" = assistant_tokenizer\n+        self._assistant_model_device: str = assistant_model_device\n+        self.target_vocab_size: int = target_vocab_size\n+        self._assistant_to_target_input_ids, self.target_to_assistant_input_ids = (\n+            self._get_assistant_to_target_input_ids()\n+        )\n+        self._suppress_input_ids: list[int] = self._get_suppress_input_ids()\n+        self.logits_processors: Optional[LogitsProcessorList] = None\n+        if len(self._suppress_input_ids) > 0:\n+            # len(self._suppress_input_ids) = 0 if the assistant vocab is a subset of the target vocab\n+            self.logits_processors = LogitsProcessorList(\n+                [SuppressTokensLogitsProcessor(self._get_suppress_input_ids(), self._assistant_model_device)]\n+            )\n+\n+    def _get_assistant_to_target_input_ids(self):\n+        target_vocab = self._target_tokenizer.get_vocab()\n+        assistant_vocab = self._assistant_tokenizer.get_vocab()\n+\n+        space_str = \" \"\n+        target_space_ids = self._target_tokenizer(space_str, add_special_tokens=False)[\"input_ids\"]\n+        if len(target_space_ids) > 0:\n+            target_space_sign = self._target_tokenizer.convert_ids_to_tokens(target_space_ids)[0][0]\n+\n+            assistant_space_ids = self._assistant_tokenizer(space_str, add_special_tokens=False)[\"input_ids\"]\n+            if len(assistant_space_ids) > 0:\n+                assistant_space_sign = self._assistant_tokenizer.convert_ids_to_tokens(assistant_space_ids)[0][0]\n+\n+                if target_space_sign != assistant_space_sign:\n+                    # If the assistant tokenizer has a different space sign than the target tokenizer,\n+                    # we need to replace the assistant space sign with the target space sign in the assistant_vocab.\n+                    assistant_vocab = {\n+                        (\n+                            tok.replace(assistant_space_sign, target_space_sign, 1)\n+                            if tok.startswith(assistant_space_sign)\n+                            else tok\n+                        ): idx\n+                        for tok, idx in assistant_vocab.items()\n+                    }\n+\n+        max_assistant_index = max(assistant_vocab.values())\n+        assistant_to_target_input_ids = torch.full((max_assistant_index + 1,), self.SUPPRESS_TOKEN_ID, dtype=int)\n+        target_to_assistant_input_ids: Dict[int, int] = {}\n+        for tok, assistant_id in assistant_vocab.items():\n+            target_id = target_vocab.get(tok)\n+            if target_id is not None:\n+                assistant_to_target_input_ids[assistant_id] = target_id\n+                target_to_assistant_input_ids[target_id] = assistant_id\n+        return assistant_to_target_input_ids.to(self._assistant_model_device), target_to_assistant_input_ids\n+\n+    def _get_suppress_input_ids(self) -> list[int]:\n+        \"\"\"\n+        Get the input ids that are in the assistant vocab but not in the target vocab.\n+        \"\"\"\n+        return torch.where(self._assistant_to_target_input_ids == self.SUPPRESS_TOKEN_ID)[0]\n+\n+    def get_target_ids(\n+        self, assistant_input_ids, target_input_ids, assistant_candidate_ids: torch.LongTensor\n+    ) -> torch.LongTensor:\n+        \"\"\"\n+        Return the target candidate ids that correspond to the assistant candidate ids.\n+        Note that we have already the target ids for the prompt and we only need to find the target ids for the new tokens.\n+        Moreover, assistant ids of the original prompt does not necessarily appear in _assistant_to_target_input_ids.\n+        \"\"\"\n+\n+        num_new_tokens = len(assistant_candidate_ids[0]) - assistant_input_ids.shape[1]\n+        if num_new_tokens == 0:\n+            return target_input_ids\n+        else:\n+            transformed_slice = self._assistant_to_target_input_ids[assistant_candidate_ids[0, -num_new_tokens:]]\n+            return torch.cat((target_input_ids, transformed_slice.unsqueeze(0)), dim=1)\n+\n+    def get_target_logits(self, assistant_logits: torch.FloatTensor) -> torch.FloatTensor:\n+        \"\"\"\n+        Return the target logits that correspond to the assistant logits.\n+        \"\"\"\n+\n+        target_shape: tuple[int, ...] = (*assistant_logits.shape[:-1], self.target_vocab_size)\n+        target_logits: torch.FloatTensor = torch.full(target_shape, self.FILTER_VALUE).to(self._assistant_model_device)\n+        # Mask for valid indices\n+        assistant_indices_mask = self._assistant_to_target_input_ids != self.SUPPRESS_TOKEN_ID\n+        # Exclude invalid indices\n+        target_logits_supported_indices = self._assistant_to_target_input_ids[assistant_indices_mask]\n+        valid_assistant_logits = assistant_logits[..., : self._assistant_to_target_input_ids.shape[0]]\n+\n+        target_logits[..., target_logits_supported_indices] = valid_assistant_logits[..., assistant_indices_mask]\n+\n+        return target_logits\n+\n+\n+class AssistantVocabTranslatorCache:\n+    \"\"\"\n+    Cache for `AssistantToTargetTranslator` instances. The instances are computed at\n+    pre-processing time, and this cache allows us to avoid recomputing them.\n+    \"\"\"\n+\n+    _cache = weakref.WeakKeyDictionary()\n+\n+    @classmethod\n+    def get_translator(\n+        cls,\n+        target_tokenizer: \"PreTrainedTokenizerBase\",\n+        assistant_tokenizer: \"PreTrainedTokenizerBase\",\n+        target_vocab_size: int,\n+        assistant_model_device: str = \"cpu\",\n+    ) -> AssistantToTargetTranslator:\n+        assistant_dict = cls._cache.get(target_tokenizer)\n+        if assistant_dict is None:\n+            assistant_dict = weakref.WeakKeyDictionary()\n+            cls._cache[target_tokenizer] = assistant_dict\n+\n+        mapping = assistant_dict.get(assistant_tokenizer)\n+        if mapping is None:\n+            mapping = AssistantToTargetTranslator(\n+                target_tokenizer, assistant_tokenizer, target_vocab_size, assistant_model_device\n+            )\n+            assistant_dict[assistant_tokenizer] = mapping\n+\n+        return mapping\n+\n+    @classmethod\n+    def cleanup(cls):\n+        \"\"\"\n+        Clean up dead references in the cache.\n+        This removes entries where either the target_tokenizer or assistant_tokenizer\n+        has been garbage collected.\n+        \"\"\"\n+        # Remove entries from the outer cache where the target_tokenizer is no longer alive\n+        dead_keys = [key for key in cls._cache if key is None]\n+        for key in dead_keys:\n+            del cls._cache[key]\n+\n+        # For each assistant_dict, remove entries where assistant_tokenizer is no longer alive\n+        for assistant_dict in cls._cache.values():\n+            dead_keys = [key for key in assistant_dict if key is None]\n+            for key in dead_keys:\n+                del assistant_dict[key]\n+\n+\n+class UniversalSpeculativeDecodingGenerator(AssistedCandidateGeneratorDifferentTokenizers):\n+    \"\"\"\n+    `CandidateGenerator` class to be used for Universal Speculative Decoding (USD): speculative decoding with different tokenizers\n+    for the assistant and main models. This class generates candidates through the use of a smaller model.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        input_ids: torch.LongTensor,\n+        assistant_model: \"PreTrainedModel\",\n+        target_tokenizer: \"PreTrainedTokenizerBase\",\n+        assistant_tokenizer: \"PreTrainedTokenizerBase\",\n+        generation_config: \"GenerationConfig\",\n+        model_kwargs: Dict,\n+        atm_translator: AssistantToTargetTranslator,\n+        inputs_tensor: Optional[torch.Tensor] = None,\n+        logits_processor: \"LogitsProcessorList\" = None,\n+    ):\n+        # Initialize translator before parent class\n+        self._atm_translator = atm_translator\n+        super().__init__(\n+            input_ids,\n+            assistant_model,\n+            target_tokenizer,\n+            assistant_tokenizer,\n+            generation_config,\n+            model_kwargs,\n+            inputs_tensor,\n+            logits_processor,\n+        )\n+        # Track sequence lengths and previous assistant IDs\n+        self._target_seq_len_with_candidates: int = 0\n+        self._prev_assistant_ids: Optional[torch.LongTensor] = None\n+\n+    def get_candidates(self, input_ids: torch.LongTensor) -> Tuple[torch.LongTensor, Optional[torch.FloatTensor]]:\n+        \"\"\"\n+        Simplified version of get_candidates that uses the translator cache for token conversion.\n+        \"\"\"\n+        target_input_ids = input_ids.to(self.assistant_model.device)\n+        assistant_input_ids, num_added_tokens = self._prepare_assistant_input_ids(target_input_ids)\n+        min_new_tokens, max_new_tokens = self._calculate_new_tokens(target_input_ids)\n+\n+        if max_new_tokens == 0:\n+            return input_ids, None\n+\n+        self._update_past_and_masks(assistant_input_ids, num_added_tokens=num_added_tokens)\n+        generation_args = self._prepare_generation_args(assistant_input_ids, min_new_tokens, max_new_tokens)\n+\n+        # Ensure scores are returned\n+        generation_args[\"generation_config\"].output_scores = True\n+        generation_args[\"generation_config\"].return_dict_in_generate = True\n+\n+        # Generate and process outputs using translator\n+        if self._atm_translator.logits_processors is not None:\n+            generation_args[\"logits_processor\"] = self._atm_translator.logits_processors\n+        self._prev_assistant_ids, assistant_candidate_logits = self._generate_candidates(generation_args)\n+\n+        # Use translator to convert tokens and logits\n+        target_candidate_ids = self._atm_translator.get_target_ids(\n+            assistant_input_ids, target_input_ids, self._prev_assistant_ids\n+        )\n+        self._target_seq_len_with_candidates = target_candidate_ids.shape[-1]\n+        target_candidate_logits = self._atm_translator.get_target_logits(assistant_candidate_logits)\n+\n+        return target_candidate_ids, target_candidate_logits\n+\n+    def _update_past_and_masks(self, assistant_input_ids: torch.LongTensor, num_added_tokens: int = 1) -> bool:\n+        if self._prev_assistant_ids is None:\n+            # Prepare attention mask for the first generation.\n+            # For subsequent generations, the attention mask is updated in super()_update_past_and_masks.\n+            self.assistant_kwargs = _prepare_attention_mask(\n+                self.assistant_kwargs, assistant_input_ids.shape[-1], self.assistant_model.config.is_encoder_decoder\n+            )\n+        return super()._update_past_and_masks(assistant_input_ids, num_added_tokens=num_added_tokens)\n+\n+    def _prepare_assistant_input_ids(self, target_input_ids: torch.LongTensor) -> torch.LongTensor:\n+        \"\"\"\n+        Simplified token conversion that only processes new tokens.\n+        \"\"\"\n+        # Calculate new tokens since last call\n+        target_seq_len = target_input_ids.shape[-1]\n+        if self._target_seq_len_with_candidates == 0:\n+            new_token_count = target_seq_len\n+        else:\n+            new_token_count = 1\n+        target_new_ids = target_input_ids[:, -new_token_count:]\n+\n+        # Convert the new tokens\n+        assistant_new_ids = None\n+        if self._target_seq_len_with_candidates > 0:\n+            # we have only one new token and we can directly convert it\n+            assistant_new_ids = self._atm_translator.target_to_assistant_input_ids.get(target_new_ids[0].item())\n+        if assistant_new_ids is None:\n+            target_new_text = self.target_tokenizer.batch_decode(\n+                target_new_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n+            )\n+            assistant_new_ids = self.assistant_tokenizer(\n+                target_new_text, add_special_tokens=False, return_tensors=\"pt\"\n+            )[\"input_ids\"].to(self.assistant_model.device)\n+        else:\n+            assistant_new_ids = torch.tensor([[assistant_new_ids]], device=self.assistant_model.device)\n+\n+        # Update or initialize assistant IDs\n+        if self._prev_assistant_ids is None:\n+            assistant_input_ids = assistant_new_ids\n+        else:\n+            tokens_to_remove = self._target_seq_len_with_candidates + 1 - target_seq_len\n+            # If the number of new tokens is greater than zero, truncate the previous assistant IDs\n+            if tokens_to_remove > 0:\n+                self._prev_assistant_ids = self._prev_assistant_ids[:, :-tokens_to_remove]\n+            assistant_input_ids = torch.cat([self._prev_assistant_ids, assistant_new_ids], dim=-1)\n+        assistant_input_ids = assistant_input_ids.to(dtype=torch.long)\n+\n+        return assistant_input_ids, len(assistant_new_ids[0])\n+\n+\n class PromptLookupCandidateGenerator(CandidateGenerator):\n     \"\"\"\n     `CandidateGenerator` class to be used for prompt lookup generation. This class generates candidates by looking up"
        },
        {
            "sha": "5c7fffb117bcb1327e248e21661a0a0c5d226718",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 33,
            "deletions": 11,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/d18d9c3205323d02479ca0d7cfd978908ebb6f71/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d18d9c3205323d02479ca0d7cfd978908ebb6f71/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=d18d9c3205323d02479ca0d7cfd978908ebb6f71",
            "patch": "@@ -26,6 +26,8 @@\n from torch import nn\n from torch.nn import functional as F\n \n+from transformers.generation.candidate_generator import AssistantVocabTranslatorCache\n+\n from ..cache_utils import (\n     Cache,\n     DynamicCache,\n@@ -56,6 +58,7 @@\n     CandidateGenerator,\n     EarlyExitCandidateGenerator,\n     PromptLookupCandidateGenerator,\n+    UniversalSpeculativeDecodingGenerator,\n     _crop_past_key_values,\n     _prepare_attention_mask,\n     _prepare_token_type_ids,\n@@ -858,16 +861,36 @@ def _get_candidate_generator(\n                 max_length=generation_config.max_length,\n             )\n         elif different_tokenizers:\n-            candidate_generator = AssistedCandidateGeneratorDifferentTokenizers(\n-                input_ids=input_ids,\n-                assistant_model=assistant_model,\n-                generation_config=generation_config,\n-                model_kwargs=model_kwargs,\n-                inputs_tensor=inputs_tensor,\n-                logits_processor=logits_processor,\n-                target_tokenizer=target_tokenizer,\n-                assistant_tokenizer=assistant_tokenizer,\n-            )\n+            if generation_config.do_sample is True:\n+                atm_translator = AssistantVocabTranslatorCache.get_translator(\n+                    target_tokenizer, assistant_tokenizer, self.config.vocab_size, assistant_model.device\n+                )\n+                candidate_generator = UniversalSpeculativeDecodingGenerator(\n+                    input_ids=input_ids,\n+                    assistant_model=assistant_model,\n+                    generation_config=generation_config,\n+                    model_kwargs=model_kwargs,\n+                    inputs_tensor=inputs_tensor,\n+                    logits_processor=logits_processor,\n+                    target_tokenizer=target_tokenizer,\n+                    assistant_tokenizer=assistant_tokenizer,\n+                    atm_translator=atm_translator,\n+                )\n+            elif generation_config.do_sample is False:\n+                candidate_generator = AssistedCandidateGeneratorDifferentTokenizers(\n+                    input_ids=input_ids,\n+                    assistant_model=assistant_model,\n+                    generation_config=generation_config,\n+                    model_kwargs=model_kwargs,\n+                    inputs_tensor=inputs_tensor,\n+                    logits_processor=logits_processor,\n+                    target_tokenizer=target_tokenizer,\n+                    assistant_tokenizer=assistant_tokenizer,\n+                )\n+            else:\n+                raise ValueError(\n+                    f\"Invalid value for `do_sample`: expected a boolean, got {type(generation_config.do_sample).__name__}\"\n+                )\n         else:\n             candidate_generator = AssistedCandidateGenerator(\n                 input_ids=input_ids,\n@@ -4225,7 +4248,6 @@ def _assisted_decoding(\n \n             #  1. Fetch candidate sequences from a `CandidateGenerator` and move to the correct device\n             candidate_input_ids, candidate_logits = candidate_generator.get_candidates(input_ids)\n-\n             candidate_input_ids = candidate_input_ids.to(self.device)\n             if candidate_logits is not None:\n                 candidate_logits = candidate_logits.to(self.device)"
        },
        {
            "sha": "38df48ab08d2c42efbaac25c651a3a33d198a046",
            "filename": "tests/generation/test_candidate_generator.py",
            "status": "modified",
            "additions": 314,
            "deletions": 32,
            "changes": 346,
            "blob_url": "https://github.com/huggingface/transformers/blob/d18d9c3205323d02479ca0d7cfd978908ebb6f71/tests%2Fgeneration%2Ftest_candidate_generator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d18d9c3205323d02479ca0d7cfd978908ebb6f71/tests%2Fgeneration%2Ftest_candidate_generator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_candidate_generator.py?ref=d18d9c3205323d02479ca0d7cfd978908ebb6f71",
            "patch": "@@ -1,43 +1,325 @@\n+import gc\n import unittest\n+import weakref\n+from unittest.mock import MagicMock\n \n-import numpy as np\n+import torch\n \n-from transformers.generation.candidate_generator import AssistedCandidateGeneratorDifferentTokenizers\n+from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n+from transformers.generation.candidate_generator import (\n+    AssistantToTargetTranslator,\n+    AssistantVocabTranslatorCache,\n+    UniversalSpeculativeDecodingGenerator,\n+)\n+from transformers.testing_utils import require_torch, torch_device\n \n \n-class TestAssistedCandidateGeneratorDifferentTokenizers(unittest.TestCase):\n-    def test_no_intersection(self):\n-        prompt = np.array([[1, 2, 3]])\n-        prompt_plus_new_tokens = np.array([[4, 5, 6]])\n-        result = AssistedCandidateGeneratorDifferentTokenizers._get_tokens_diag(prompt, prompt_plus_new_tokens)\n-        self.assertEqual(result, (None, None, None))\n+@require_torch\n+class TestAssistantToTargetTranslator(unittest.TestCase):\n+    def setUp(self):\n+        # Create mock tokenizers with predefined vocabularies\n+        self.target_tokenizer = MagicMock()\n+        self.assistant_tokenizer = MagicMock()\n \n-    def test_complete_overlap(self):\n-        prompt = np.array([[1, 2, 3]])\n-        prompt_plus_new_tokens = np.array([[1, 2, 3, 4, 5]])\n-        discrep_length, new_tokens_only, discrep_only = AssistedCandidateGeneratorDifferentTokenizers._get_tokens_diag(\n-            prompt, prompt_plus_new_tokens\n+        # Define mock vocabularies for the tokenizers\n+        self.target_vocab = {\"hello\": 0, \"world\": 1, \"foo\": 2, \"bar\": 3}\n+        self.assistant_vocab = {\"hello\": 0, \"world\": 1, \"foo\": 2, \"baz\": 4}\n+\n+        self.target_tokenizer.get_vocab.return_value = self.target_vocab\n+        self.assistant_tokenizer.get_vocab.return_value = self.assistant_vocab\n+        self.assistant_model_device = torch_device\n+        self.target_vocab_size = 6\n+\n+        # Instantiate the class under test\n+        self.translator = AssistantToTargetTranslator(\n+            target_tokenizer=self.target_tokenizer,\n+            assistant_tokenizer=self.assistant_tokenizer,\n+            assistant_model_device=self.assistant_model_device,\n+            target_vocab_size=self.target_vocab_size,\n+        )\n+\n+    def test_get_assistant_to_target_input_ids(self):\n+        \"\"\"Test the mapping from assistant tokens to target tokens.\"\"\"\n+        expected_mapping = [0, 1, 2, self.translator.SUPPRESS_TOKEN_ID, self.translator.SUPPRESS_TOKEN_ID]\n+        actual_mapping = self.translator._assistant_to_target_input_ids.tolist()\n+        self.assertEqual(actual_mapping, expected_mapping)\n+\n+    def test_get_suppress_input_ids(self):\n+        \"\"\"Test the suppression of assistant input IDs not present in the target vocabulary.\"\"\"\n+        expected_suppress_ids = [3, 4]\n+        actual_suppress_ids = self.translator._get_suppress_input_ids().tolist()\n+        self.assertEqual(actual_suppress_ids, expected_suppress_ids)\n+\n+    def test_get_target_ids(self):\n+        \"\"\"Test the translation of assistant candidate IDs to target candidate IDs.\"\"\"\n+        assistant_input_ids = torch.LongTensor([[0, 1, 2]]).to(\n+            self.assistant_model_device\n+        )  # 'hello world foo' in assistant tokenizer\n+        target_input_ids = torch.LongTensor([[0, 1, 2]]).to(\n+            self.assistant_model_device\n+        )  # 'hello world foo' in target tokenizer\n+        assistant_candidate_ids = torch.LongTensor([[0, 1, 2, 4]]).to(\n+            self.assistant_model_device\n+        )  # 'hello world foo baz' in assistant tokenizer\n+\n+        expected_target_ids = torch.LongTensor(\n+            [[0, 1, 2, self.translator.SUPPRESS_TOKEN_ID]]\n+        ).to(\n+            self.assistant_model_device\n+        )  # 'hello world foo baz' in target tokenizer (baz is mapped to self.translator.suppress_tokens_id since it does not exist in target vocab)\n+\n+        actual_target_ids = self.translator.get_target_ids(\n+            assistant_input_ids, target_input_ids, assistant_candidate_ids\n+        )\n+        self.assertTrue(torch.equal(actual_target_ids, expected_target_ids))\n+\n+    def test_get_target_logits(self):\n+        \"\"\"Test the conversion of assistant logits to target logits.\"\"\"\n+        # Assistant logits for IDs 0, 1, 2\n+        assistant_logits = torch.FloatTensor([[[0.1, 0.2, 0.3, 0.4, self.translator.FILTER_VALUE]]]).to(\n+            self.assistant_model_device\n+        )  # Shape (1, 1, 5)\n+\n+        # Expected target logits (target_vocab_size = 4)\n+        expected_target_logits = torch.full((1, 1, self.target_vocab_size), self.translator.FILTER_VALUE).to(\n+            self.assistant_model_device\n+        )\n+        expected_target_logits[0, 0, 0] = 0.1  # 'hello'\n+        expected_target_logits[0, 0, 1] = 0.2  # 'world'\n+        expected_target_logits[0, 0, 2] = 0.3  # 'foo'\n+        # The 'bar' token in target vocab remains at -inf\n+\n+        actual_target_logits = self.translator.get_target_logits(assistant_logits)\n+        self.assertTrue(torch.equal(actual_target_logits, expected_target_logits))\n+\n+\n+class MockTokenizer:\n+    \"\"\"A simple mock tokenizer class that supports weak references.\"\"\"\n+\n+    def __init__(self, vocab=None):\n+        self._vocab = vocab or {}\n+\n+    def get_vocab(self):\n+        return self._vocab\n+\n+    def __call__(self, text, add_special_tokens=True):\n+        # Mock implementation of the __call__ method\n+        tokens = text.split()\n+        input_ids = [self._vocab.get(token, 0) for token in tokens]\n+        return {\"input_ids\": input_ids}\n+\n+\n+@require_torch\n+class TestAssistantVocabTranslatorCache(unittest.TestCase):\n+    def setUp(self):\n+        # Clear the cache before each test\n+        AssistantVocabTranslatorCache._cache.clear()\n+        # Create mock tokenizers with different vocabularies\n+        self.target_tokenizer = MockTokenizer({\"hello\": 0, \"world\": 1})\n+        self.assistant_tokenizer = MockTokenizer({\"hello\": 0, \"world\": 1, \"foo\": 2})\n+        self.other_target_tokenizer = MockTokenizer({\"foo\": 2, \"bar\": 3})\n+        self.other_assistant_tokenizer = MockTokenizer({\"baz\": 4, \"qux\": 5})\n+        self.assistant_model_device = torch_device\n+        self.target_vocab_size = 6\n+\n+    def test_same_instance_for_same_tokenizers(self):\n+        \"\"\"Test that the same translator is returned for the same tokenizers.\"\"\"\n+        translator1 = AssistantVocabTranslatorCache.get_translator(\n+            self.target_tokenizer,\n+            self.assistant_tokenizer,\n+            assistant_model_device=self.assistant_model_device,\n+            target_vocab_size=self.target_vocab_size,\n         )\n-        self.assertEqual(discrep_length, 0)\n-        np.testing.assert_array_equal(new_tokens_only, np.array([[4, 5]]))\n-        np.testing.assert_array_equal(discrep_only, np.array([[]]))\n+        translator2 = AssistantVocabTranslatorCache.get_translator(\n+            self.target_tokenizer,\n+            self.assistant_tokenizer,\n+            assistant_model_device=self.assistant_model_device,\n+            target_vocab_size=self.target_vocab_size,\n+        )\n+        self.assertIs(translator1, translator2, \"Translators should be cached and identical\")\n \n-    def test_partial_overlap(self):\n-        prompt = np.array([[1, 2, 3]])\n-        prompt_plus_new_tokens = np.array([[2, 3, 4, 5]])\n-        discrep_length, new_tokens_only, discrep_only = AssistedCandidateGeneratorDifferentTokenizers._get_tokens_diag(\n-            prompt, prompt_plus_new_tokens\n+    def test_different_instances_for_different_tokenizers(self):\n+        \"\"\"Test that different tokenizers produce different translators.\"\"\"\n+        translator1 = AssistantVocabTranslatorCache.get_translator(\n+            self.target_tokenizer,\n+            self.assistant_tokenizer,\n+            assistant_model_device=self.assistant_model_device,\n+            target_vocab_size=self.target_vocab_size,\n+        )\n+        translator2 = AssistantVocabTranslatorCache.get_translator(\n+            self.other_target_tokenizer,\n+            self.other_assistant_tokenizer,\n+            assistant_model_device=self.assistant_model_device,\n+            target_vocab_size=self.target_vocab_size,\n         )\n-        self.assertEqual(discrep_length, 0)\n-        np.testing.assert_array_equal(new_tokens_only, np.array([[4, 5]]))\n-        np.testing.assert_array_equal(discrep_only, np.array([[]]))\n+        self.assertIsNot(translator1, translator2, \"Translators should differ for different tokenizers\")\n \n-    def test_no_new_tokens(self):\n-        prompt = np.array([[1, 2, 3]])\n-        prompt_plus_new_tokens = np.array([[1, 2, 3]])\n-        discrep_length, new_tokens_only, discrep_only = AssistedCandidateGeneratorDifferentTokenizers._get_tokens_diag(\n-            prompt, prompt_plus_new_tokens\n+    def test_cache_with_weakref_key(self):\n+        \"\"\"Ensure that the cache uses weak references as keys.\"\"\"\n+        initial_cache_size = len(AssistantVocabTranslatorCache._cache)\n+        target_tokenizer = MockTokenizer({\"hello\": 0})\n+        assistant_tokenizer = MockTokenizer({\"hello\": 0})\n+\n+        # Store translator in a local variable to avoid it being kept alive\n+        translator = AssistantVocabTranslatorCache.get_translator(\n+            target_tokenizer,\n+            assistant_tokenizer,\n+            assistant_model_device=self.assistant_model_device,\n+            target_vocab_size=self.target_vocab_size,\n         )\n-        self.assertEqual(discrep_length, 0)\n-        np.testing.assert_array_equal(new_tokens_only, np.array([[]]))\n-        np.testing.assert_array_equal(discrep_only, np.array([[]]))\n+        self.assertEqual(len(AssistantVocabTranslatorCache._cache), initial_cache_size + 1)\n+\n+        # Delete all strong references\n+        del target_tokenizer\n+        del assistant_tokenizer\n+        del translator\n+\n+        # Force garbage collection\n+        gc.collect()\n+\n+        # Call cleanup to remove dead entries\n+        AssistantVocabTranslatorCache.cleanup()\n+\n+        # The cache size remains increased due to strong references\n+        self.assertEqual(len(AssistantVocabTranslatorCache._cache), initial_cache_size + 1)\n+\n+    def test_weakref_cache_cleanup(self):\n+        \"\"\"Test that the cache cleans up translators when tokenizers are garbage collected.\"\"\"\n+\n+        def create_translator():\n+            target_tokenizer = MockTokenizer({\"hello\": 0})\n+            assistant_tokenizer = MockTokenizer({\"hello\": 0})\n+            translator = AssistantVocabTranslatorCache.get_translator(\n+                target_tokenizer,\n+                assistant_tokenizer,\n+                assistant_model_device=self.assistant_model_device,\n+                target_vocab_size=self.target_vocab_size,\n+            )\n+            # Create weak references before returning\n+            refs = (weakref.ref(translator), weakref.ref(target_tokenizer), weakref.ref(assistant_tokenizer))\n+            # Remove strong references inside the function\n+            del target_tokenizer\n+            del assistant_tokenizer\n+            del translator\n+            return refs\n+\n+        translator_ref, target_ref, assistant_ref = create_translator()\n+\n+        # Force garbage collection\n+        gc.collect()\n+\n+        # Call cleanup to remove dead entries\n+        AssistantVocabTranslatorCache.cleanup()\n+\n+        # The tokenizers and translator are not garbage collected due to strong references\n+        self.assertIsNotNone(target_ref(), \"Target tokenizer should still be alive due to strong references\")\n+        self.assertIsNotNone(assistant_ref(), \"Assistant tokenizer should still be alive due to strong references\")\n+        self.assertIsNotNone(translator_ref(), \"Translator should still be alive due to strong references\")\n+\n+\n+@require_torch\n+class TestUniversalSpeculativeDecoding(unittest.TestCase):\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.target_name = \"hf-internal-testing/tiny-random-LlamaForCausalLM\"\n+        cls.assistant_name = \"hf-internal-testing/tiny-random-PhiForCausalLM\"\n+\n+    def setUp(self):\n+        self.target_tokenizer = AutoTokenizer.from_pretrained(self.target_name)\n+        self.target_config = AutoConfig.from_pretrained(self.target_name)\n+        self.assistant_model = AutoModelForCausalLM.from_pretrained(self.assistant_name).to(torch_device)\n+        self.assistant_tokenizer = AutoTokenizer.from_pretrained(self.assistant_name)\n+\n+        self.generation_config = GenerationConfig()\n+\n+        # Ensure required tokens exist\n+        if self.target_tokenizer.pad_token_id is None:\n+            self.target_tokenizer.pad_token_id = self.target_tokenizer.eos_token_id\n+        if self.target_tokenizer.bos_token_id is None:\n+            self.target_tokenizer.bos_token_id = self.target_tokenizer.eos_token_id\n+        if self.assistant_tokenizer.pad_token_id is None:\n+            self.assistant_tokenizer.pad_token_id = self.assistant_tokenizer.eos_token_id\n+        if self.target_tokenizer.bos_token_id is None:\n+            self.assistant_tokenizer.bos_token_id = self.assistant_tokenizer.eos_token_id\n+\n+        self.input_ids = torch.tensor([[1, 2, 3]]).to(torch_device)\n+        self.model_kwargs = {\n+            \"attention_mask\": torch.ones_like(self.input_ids).to(torch_device),\n+        }\n+\n+        atm_translator = AssistantVocabTranslatorCache.get_translator(\n+            self.target_tokenizer, self.assistant_tokenizer, self.target_config.vocab_size, torch_device\n+        )\n+        self.generator = UniversalSpeculativeDecodingGenerator(\n+            input_ids=self.input_ids,\n+            assistant_model=self.assistant_model,\n+            target_tokenizer=self.target_tokenizer,\n+            assistant_tokenizer=self.assistant_tokenizer,\n+            generation_config=self.generation_config,\n+            model_kwargs=self.model_kwargs,\n+            atm_translator=atm_translator,\n+        )\n+\n+    def test_basic_generation(self):\n+        \"\"\"Test basic speculative decoding works\"\"\"\n+        input_text = \"The quick brown fox\"\n+        input_ids = self.target_tokenizer.encode(input_text, return_tensors=\"pt\")\n+        self.generator.input_ids = input_ids\n+        candidates, scores = self.generator.get_candidates(input_ids)\n+\n+        self.assertIsNotNone(candidates)\n+        self.assertIsNotNone(scores)\n+        self.assertTrue(torch.is_tensor(candidates))\n+        self.assertTrue(torch.is_tensor(scores))\n+\n+    def test_mismatched_vocabularies(self):\n+        \"\"\"Test handling of mismatched vocabularies between models\"\"\"\n+        # Create input with tokens present in main but not assistant vocab\n+        # Find a token that is not in the assistant tokenizer but in\n+        # the main tokenizer.\n+        missing_token = next(\n+            token\n+            for token in self.target_tokenizer.get_vocab()\n+            if token not in self.assistant_tokenizer.get_vocab()\n+            and token not in self.target_tokenizer.all_special_tokens\n+            and \"reserved_\" not in token\n+        )\n+        input_ids = torch.tensor([[self.target_tokenizer.convert_tokens_to_ids(missing_token)]])\n+        self.generator.input_ids = input_ids\n+        candidates, scores = self.generator.get_candidates(input_ids)\n+        self.assertIsNotNone(candidates)\n+\n+    def test_speculation_depth(self):\n+        \"\"\"Test different speculation depths\"\"\"\n+        input_ids = self.target_tokenizer.encode(\"Test text\", return_tensors=\"pt\")\n+        self.generator.input_ids = input_ids\n+\n+        for depth in [1, 8, 17]:\n+            self.generator.num_assistant_tokens = depth\n+            candidates, scores = self.generator.get_candidates(input_ids)\n+            self.assertLessEqual(candidates.shape[1] - input_ids.shape[1], depth)\n+\n+    def test_device_consistency(self):\n+        \"\"\"Test handling of inputs on different devices\"\"\"\n+        input_ids = torch.tensor([[1, 2, 3]]).to(torch_device)\n+        self.generator.input_ids = input_ids\n+        candidates, _ = self.generator.get_candidates(input_ids)\n+        self.assertEqual(candidates.device, input_ids.device)\n+\n+    def test_usd_vs_vanilla_sampling(cls):\n+        \"\"\"Test that USD matches vanilla sampling with temperature set to nearly 0\"\"\"\n+        prompt = \"Test text\"\n+\n+        pipe_usd = pipeline(\"text-generation\", model=cls.target_name, assistant_model=cls.assistant_name)\n+        pipe_usd_output = pipe_usd(prompt, max_new_tokens=5, do_sample=True, temperature=1e-9)  # Nearly 0 temperature\n+        usd_text = pipe_usd_output[0][\"generated_text\"]\n+\n+        pipe_vanilla = pipeline(\n+            \"text-generation\",\n+            model=cls.target_name,\n+        )\n+        pipe_vanilla_output = pipe_vanilla(prompt, max_new_tokens=5, do_sample=False)\n+        vanilla_text = pipe_vanilla_output[0][\"generated_text\"]\n+\n+        # Assert that the outputs match\n+        cls.assertEqual(usd_text, vanilla_text)"
        }
    ],
    "stats": {
        "total": 684,
        "additions": 638,
        "deletions": 46
    }
}