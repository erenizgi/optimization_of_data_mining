{
    "author": "Cyrilvallez",
    "message": "One cache class to rule them all (#40276)\n\n* remove all classes\n\n* fix generate\n\n* start replacing everywhere\n\n* finish removing everywhere\n\n* typo\n\n* typo\n\n* fix\n\n* typo\n\n* remove num_layers=1\n\n* CI\n\n* fix all docstrings\n\n* review\n\n* style",
    "sha": "242bb2cafccec9f90479f5f688bca9d240b1031f",
    "files": [
        {
            "sha": "c78e8e4e6166f8d847911ff4670a3b8fc3569e32",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 189,
            "deletions": 285,
            "changes": 474,
            "blob_url": "https://github.com/huggingface/transformers/blob/242bb2cafccec9f90479f5f688bca9d240b1031f/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242bb2cafccec9f90479f5f688bca9d240b1031f/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=242bb2cafccec9f90479f5f688bca9d240b1031f",
            "patch": "@@ -248,17 +248,16 @@ class StaticLayer(CacheLayerMixin):\n     \"\"\"\n     A static cache layer that stores the key and value states as static tensors of shape `[batch_size, num_heads, max_cache_len), head_dim]`.\n     It lazily allocates its full backing tensors, and then mutates them in-place. Built for `torch.compile` support.\n+\n+    Args:\n+        max_cache_len (`int`):\n+            Maximum number of tokens that can be stored, used for tensor preallocation.\n     \"\"\"\n \n     is_compileable = True\n     is_sliding = False\n \n     def __init__(self, max_cache_len: int):\n-        \"\"\"\n-        Args:\n-            max_cache_len (`int`):\n-                Maximum number of tokens that can be stored, used for tensor preallocation.\n-        \"\"\"\n         super().__init__()\n         self.max_cache_len = max_cache_len\n \n@@ -357,18 +356,17 @@ class SlidingWindowLayer(StaticLayer):\n     A static cache layer that stores the key and value states as static tensors of shape\n     `[batch_size, num_heads, min(max_cache_len, sliding_window), head_dim]`. It lazily allocates its full backing\n     tensors, and then mutates them in-place. Built for `torch.compile` support.\n+\n+    Args:\n+        max_cache_len (`int`):\n+            Maximum number of tokens that can be stored, used for tensor preallocation.\n+        sliding_window (`int`):\n+            The size of the sliding window.\n     \"\"\"\n \n     is_sliding = True\n \n     def __init__(self, max_cache_len: int, sliding_window: int):\n-        \"\"\"\n-        Args:\n-            max_cache_len (`int`):\n-                Maximum number of tokens that can be stored, used for tensor preallocation.\n-            sliding_window (`int`):\n-                The size of the sliding window.\n-        \"\"\"\n         effective_max_cache_len = min(sliding_window, max_cache_len)\n         super().__init__(max_cache_len=effective_max_cache_len)\n         self.cumulative_length = 0\n@@ -717,19 +715,12 @@ def _dequantize(self, qtensor):\n         return tensor\n \n \n-STATIC_LAYER_CLASS_MAPPING: dict[str, type[CacheLayerMixin]] = {\n-    \"full_attention\": StaticLayer,\n-    \"sliding_attention\": SlidingWindowLayer,\n-    \"chunked_attention\": ChunkedSlidingLayer,\n-}\n-\n-\n class Cache:\n     \"\"\"\n     A `Cache` is mostly a list of `CacheLayerMixin` objects, one per model layer. It serves as a container for\n     the Cache of each layer.\n \n-    Parameters:\n+    Args:\n         layers (`Optional`, *optional*):\n             A list of pre-created `CacheLayerMixin`. If omitted (`None`), then `layer_class_to_replicate` will\n             be used.\n@@ -973,6 +964,22 @@ class DynamicCache(Cache):\n \n     See `Cache` for details on common methods that are implemented by all cache classes.\n \n+    Args:\n+        ddp_cache_data (`Iterable[tuple[torch.Tensor, torch.Tensor]]`, *optional*):\n+            It was originally added for compatibility with `torch.distributed` (DDP). In a nutshell, it is\n+            `map(gather_map, zip(*caches))`, i.e. each item in the iterable contains the key and value states\n+            for a layer gathered across replicas by torch.distributed (shape=[global batch size, num_heads, seq_len, head_dim]).\n+            Note: it needs to be the 1st arg as well to work correctly\n+        config (`PretrainedConfig`, *optional*):\n+            The config of the model for which this Cache will be used. If passed, it will be used to check for sliding\n+            or hybrid layer structure, greatly reducing the memory requirement of the cached tensors to\n+            `[batch_size, num_heads, min(seq_len, sliding_window), head_dim]`.\n+        offloading (`bool`, *optional*, defaults to `False`):\n+            Whether to perform offloading of the layers to `cpu`, to save GPU memory.\n+        offload_only_non_sliding (`bool`, *optional*, defaults to `False`):\n+            If `offloading` is `True`, this further decides if only the non-sliding layers will be offloaded (because\n+            usually the sliding layers are small in size, so there is no need to offload them, and skipping it is faster).\n+\n     Example:\n \n     ```python\n@@ -995,21 +1002,9 @@ def __init__(\n         self,\n         ddp_cache_data: Optional[Iterable[tuple[torch.Tensor, torch.Tensor]]] = None,\n         config: Optional[PretrainedConfig] = None,\n+        offloading: bool = False,\n+        offload_only_non_sliding: bool = False,\n     ):\n-        \"\"\"\n-        Create a `DynamicCache`. Specialized constructor for DDP cache data, needed for BC.\n-\n-        Args:\n-            ddp_cache_data (`Iterable[tuple[torch.Tensor, torch.Tensor]]`, *optional*):\n-                It was originally added for compatibility with `torch.distributed` (DDP). In a nutshell, it is\n-                `map(gather_map, zip(*caches))`, i.e. each item in the iterable contains the key and value states\n-                for a layer gathered across replicas by torch.distributed (shape=[global batch size, num_heads, seq_len, head_dim]).\n-                Note: it needs to be the 1st arg as well to work correctly\n-            config (`PretrainedConfig`, *optional*):\n-                The config of the model for which this Cache will be used. If passed, it will be used to check for sliding\n-                or hybrid layer structure, greatly reducing the memory requirement of the cached tensors to\n-                `[batch_size, num_heads, min(seq_len, sliding_window), head_dim]`.\n-        \"\"\"\n         layers = []\n         # If a config is passed, use it to infer the layer types and initialize accordingly\n         if config is not None:\n@@ -1040,9 +1035,13 @@ def __init__(\n \n         # If neither of config nor ddp_data was passed, then simply lazy init a full cache of DynamicLayer\n         if len(layers) == 0:\n-            super().__init__(layer_class_to_replicate=DynamicLayer)\n+            super().__init__(\n+                layer_class_to_replicate=DynamicLayer,\n+                offloading=offloading,\n+                offload_only_non_sliding=offload_only_non_sliding,\n+            )\n         else:\n-            super().__init__(layers=layers)\n+            super().__init__(layers=layers, offloading=offloading, offload_only_non_sliding=offload_only_non_sliding)\n \n     def to_legacy_cache(self) -> tuple[tuple[torch.Tensor, torch.Tensor]]:\n         \"\"\"\n@@ -1070,24 +1069,25 @@ def from_legacy_cache(cls, past_key_values: tuple[tuple[torch.Tensor, torch.Tens\n         return cache\n \n \n-class OffloadedCache(Cache):\n-    \"\"\"\n-    A drop-in replacement for DynamicCache that conserves accelerator (GPU, XPU) memory at the expense of more CPU memory.\n-    Useful for generating from models with very long context.\n-\n-    See `Cache` for details on common methods that are implemented by all cache classes.\n-    \"\"\"\n-\n-    def __init__(self) -> None:\n-        super().__init__(layer_class_to_replicate=DynamicLayer, offloading=True)\n-\n-\n class StaticCache(Cache):\n     \"\"\"\n-    Static Cache class to be used with `torch.compile(model)` and `torch.export()`.\n+    Static Cache class to be used with `torch.compile(model)` and `torch.export()`. It will check the `config`\n+    for potential hybrid cache structure, and initialize each layer accordingly.\n \n     See `Cache` for details on common methods that are implemented by all cache classes.\n \n+    Args:\n+        config (`PretrainedConfig`):\n+            The config of the model for which this Cache will be used. It will be used to check for sliding\n+            or hybrid layer structure, and initialize each layer accordingly.\n+        max_cache_len (`int`):\n+            The maximum number of tokens that this Cache should hold.\n+        offloading (`bool`, *optional*, defaults to `False`):\n+            Whether to perform offloading of the layers to `cpu`, to save GPU memory.\n+        offload_only_non_sliding (`bool`, *optional*, defaults to `True`):\n+            If `offloading` is `True`, this further decides if only the non-sliding layers will be offloaded (because\n+            usually the sliding layers are small in size, so there is no need to offload them, and skipping it is faster).\n+\n     Example:\n \n     ```python\n@@ -1108,154 +1108,37 @@ class StaticCache(Cache):\n     ```\n     \"\"\"\n \n-    # Pass-in args and kwargs as well to avoid crashing for BC (it used more arguments before)\n-    def __init__(self, config: PretrainedConfig, max_cache_len: int, *args, **kwargs):\n-        layers = [StaticLayer(max_cache_len) for _ in range(config.num_hidden_layers)]\n-        super().__init__(layers=layers)\n-\n-\n-class OffloadedStaticCache(Cache):\n-    \"\"\"\n-    A drop-in replacement for StaticCache that conserves accelerator memory by offloading\n-    cache tensors to CPU when not actively being used.\n-\n-    This cache maintains the compilation-friendly properties of StaticCache while enabling\n-    much longer sequences by offloading inactive layers to CPU memory.\n-\n-    See `Cache` for details on common methods that are implemented by all cache classes.\n-\n-    Example:\n-\n-    ```python\n-    >>> from transformers import AutoTokenizer, AutoModelForCausalLM, OffloadedStaticCache\n-\n-    >>> model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n-    >>> tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n-\n-    >>> inputs = tokenizer(text=\"My name is GPT2\", return_tensors=\"pt\")\n-\n-    >>> # Prepare a cache class with offloading\n-    >>> max_generated_length = inputs.input_ids.shape[1] + 10\n-    >>> past_key_values = OffloadedStaticCache(config=model.config, max_cache_len=max_generated_length)\n-    >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n-    >>> outputs.past_key_values # access cache with offloaded layers\n-    OffloadedStaticCache()\n-    ```\n-    \"\"\"\n-\n-    # Pass-in args and kwargs as well to avoid crashing for BC (it used more arguments before)\n-    def __init__(self, config: PretrainedConfig, max_cache_len: int, *args, **kwargs):\n-        layers = [StaticLayer(max_cache_len) for _ in range(config.num_hidden_layers)]\n-        super().__init__(layers=layers, offloading=True)\n-\n-\n-class SlidingWindowCache(Cache):\n-    \"\"\"\n-    Sliding Window Cache class to be used with `torch.compile` for models like Mistral that support sliding window attention.\n-    See `Cache` for details on common methods that are implemented by all cache classes.\n-\n-    Example:\n-\n-    ```python\n-    >>> from transformers import AutoTokenizer, AutoModelForCausalLM, SlidingWindowCache\n-\n-    >>> model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n-    >>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n-\n-    >>> inputs = tokenizer(text=\"My name is Mistral\", return_tensors=\"pt\")\n-\n-    >>> # Prepare a cache class and pass it to model's forward\n-    >>> # Leave empty space for 10 new tokens, which can be used when calling forward iteratively 10 times to generate\n-    >>> max_generated_length = inputs.input_ids.shape[1] + 10\n-    >>> past_key_values = SlidingWindowCache(config=model.config, max_cache_len=max_generated_length)\n-    >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n-    >>> outputs.past_key_values # access cache filled with key/values from generation\n-    SlidingWindowCache()\n-    ```\n-    \"\"\"\n-\n-    # Pass-in args and kwargs as well to avoid crashing for BC (it used more arguments before)\n-    def __init__(self, config: PretrainedConfig, max_cache_len: int, *args, **kwargs):\n-        layers = [SlidingWindowLayer(max_cache_len, config.sliding_window) for _ in range(config.num_hidden_layers)]\n-        super().__init__(layers=layers)\n-\n-\n-class HybridCache(Cache):\n-    \"\"\"\n-    Hybrid Cache class to be used with `torch.compile` for models that alternate between a local sliding window\n-    attention and global attention in every other layer (originally implemented for Gemma2).\n-    Under the hood, Hybrid Cache leverages [\"SlidingWindowLayer\"] for sliding window attention and [\"StaticLayer\"]\n-    for global attention. For more information, see the documentation of those layer types.\n-\n-    See `Cache` for details on common methods that are implemented by all cache classes.\n-\n-    Example:\n-\n-    ```python\n-    >>> from transformers import AutoTokenizer, AutoModelForCausalLM, HybridCache\n-\n-    >>> model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b\")\n-    >>> tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n-\n-    >>> inputs = tokenizer(text=\"My name is Gemma\", return_tensors=\"pt\")\n-\n-    >>> # Prepare a cache class and pass it to model's forward\n-    >>> # Leave empty space for 10 new tokens, which can be used when calling forward iteratively 10 times to generate\n-    >>> max_generated_length = inputs.input_ids.shape[1] + 10\n-    >>> past_key_values = HybridCache(config=model.config, max_cache_len=max_generated_length)\n-    >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n-    >>> outputs.past_key_values # access cache filled with key/values from generation\n-    HybridCache()\n-    ```\n-    \"\"\"\n-\n-    # Pass-in args and kwargs as well to avoid crashing for BC (it used more arguments before)\n-    def __init__(self, config: PretrainedConfig, max_cache_len: int, *args, **kwargs):\n-        if hasattr(config, \"layer_types\"):\n-            layers = []\n-            for layer_type in config.layer_types:\n-                init_kwargs = {\"max_cache_len\": max_cache_len}\n-                if layer_type == \"sliding_attention\":\n-                    init_kwargs[\"sliding_window\"] = config.sliding_window\n-                elif layer_type == \"chunked_attention\":\n-                    init_kwargs[\"sliding_window\"] = config.attention_chunk_size\n-                layers.append(STATIC_LAYER_CLASS_MAPPING[layer_type](**init_kwargs))\n-        else:\n-            # In this case, fall back to StaticCache\n-            layers = [StaticLayer(max_cache_len) for _ in range(config.num_hidden_layers)]\n-        super().__init__(layers=layers)\n-\n-\n-# The mapping already handles dispatching the correct layers in Hybrid, this is only used for BC\n-class HybridChunkedCache(HybridCache): ...\n-\n-\n-class OffloadedHybridCache(Cache):\n-    \"\"\"\n-    A drop-in replacement for HybridChunkedCache that conserves accelerator memory by offloading\n-    cache tensors to CPU when not actively being used.\n-\n-    This cache maintains the compilation-friendly properties of HybridChunkedCache while enabling\n-    much longer sequences by offloading inactive layers to CPU memory.\n+    # Pass-in kwargs as well to avoid crashing for BC (it used more arguments before)\n+    def __init__(\n+        self,\n+        config: PretrainedConfig,\n+        max_cache_len: int,\n+        offloading: bool = False,\n+        offload_only_non_sliding: bool = True,\n+        **kwargs,\n+    ):\n+        config = config.get_text_config()\n+        layer_types = getattr(config, \"layer_types\", None)\n+        # If `layer_types` is not explicitly provided, infer if the model is fully sliding\n+        if layer_types is None:\n+            if getattr(config, \"sliding_window\", None) is not None:\n+                layer_types = [\"sliding_attention\" for _ in range(config.num_hidden_layers)]\n+            elif getattr(config, \"attention_chunk_size\", None) is not None:\n+                layer_types = [\"chunked_attention\" for _ in range(config.num_hidden_layers)]\n+            else:\n+                layer_types = [\"full_attention\" for _ in range(config.num_hidden_layers)]\n \n-    See `Cache` for details on common methods that are implemented by all cache classes.\n-    \"\"\"\n+        layers = []\n+        for layer_type in layer_types:\n+            if layer_type == \"sliding_attention\":\n+                layer = SlidingWindowLayer(max_cache_len=max_cache_len, sliding_window=config.sliding_window)\n+            elif layer_type == \"chunked_attention\":\n+                layer = ChunkedSlidingLayer(max_cache_len=max_cache_len, sliding_window=config.attention_chunk_size)\n+            else:\n+                layer = StaticLayer(max_cache_len=max_cache_len)\n+            layers.append(layer)\n \n-    # Pass-in args and kwargs as well to avoid crashing for BC (it used more arguments before)\n-    def __init__(self, config: PretrainedConfig, max_cache_len: int, *args, **kwargs):\n-        if hasattr(config, \"layer_types\"):\n-            layers = []\n-            for layer_type in config.layer_types:\n-                init_kwargs = {\"max_cache_len\": max_cache_len}\n-                if layer_type == \"sliding_attention\":\n-                    init_kwargs[\"sliding_window\"] = config.sliding_window\n-                elif layer_type == \"chunked_attention\":\n-                    init_kwargs[\"sliding_window\"] = config.attention_chunk_size\n-                layers.append(STATIC_LAYER_CLASS_MAPPING[layer_type](**init_kwargs))\n-        else:\n-            # In this case, fall back to StaticCache\n-            layers = [StaticLayer(max_cache_len) for _ in range(config.num_hidden_layers)]\n-        super().__init__(layers=layers, offloading=True)\n+        super().__init__(layers=layers, offloading=offloading, offload_only_non_sliding=offload_only_non_sliding)\n \n \n class QuantizedCache(Cache):\n@@ -1271,6 +1154,22 @@ class QuantizedCache(Cache):\n     described in the paper.\n \n     See `Cache` for details on common methods that are implemented by all cache classes.\n+\n+    Args:\n+        backend (`str`):\n+            The quantization backend to use. One of `(\"quanto\", \"hqq\").\n+        config (`PretrainedConfig`):\n+            The config of the model for which this Cache will be used.\n+        nbits (`int`, *optional*, defaults to 4):\n+            The number of bits for quantization.\n+        axis_key (`int`, *optional*, defaults to 0):\n+            The axis on which to quantize the keys.\n+        axis_value (`int`, *optional*, defaults to 0):\n+            The axis on which to quantize the values.\n+        q_group_size (`int`, *optional*, defaults to 64):\n+            Quantization is done per-channel according to a set `q_group_size` for both keys and values.\n+        residual_length (`int`, *optional*, defaults to 128):\n+            Maximum capacity for the original precision cache\n     \"\"\"\n \n     def __init__(\n@@ -1290,110 +1189,27 @@ def __init__(\n         else:\n             raise ValueError(f\"Unknown quantization backend `{backend}`\")\n \n+        config = config.get_text_config(decoder=True)\n         layers = [\n             layer_class(nbits, axis_key, axis_value, q_group_size, residual_length)\n             for _ in range(config.num_hidden_layers)\n         ]\n         super().__init__(layers=layers)\n \n \n-class QuantoQuantizedCache(QuantizedCache):\n-    \"\"\"\n-    A quantizer cache similar to what is described in the\n-    [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache paper](https://arxiv.org/abs/2402.02750).\n-    It allows the model to generate longer sequence length without allocating too much memory for keys and values\n-    by applying quantization.\n-    The cache has two types of storage, one for original precision and one for the\n-    quantized cache. A `residual length` is set as a maximum capacity for the original precision cache. When the\n-    length goes beyond maximum capacity, the original precision cache is discarded and moved into the quantized cache.\n-    The quantization is done per-channel with a set `q_group_size` for both keys and values, in contrast to what was\n-    described in the paper.\n-\n-    See `Cache` for details on common methods that are implemented by all cache classes.\n-\n-    Example:\n-\n-    ```python\n-    >>> # Run pip install quanto first if you don't have it yet\n-    >>> from transformers import AutoTokenizer, AutoModelForCausalLM, QuantoQuantizedCache\n-\n-    >>> model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n-    >>> tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n-\n-    >>> inputs = tokenizer(text=\"My name is Qwen2\", return_tensors=\"pt\")\n-\n-    >>> # Prepare a cache class and pass it to model's forward\n-    >>> past_key_values = QuantoQuantizedCache(config=model.config, nbits=4)\n-    >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n-    >>> outputs.past_key_values # access cache filled with key/values from generation\n-    QuantoQuantizedCache()\n-    ```\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        config: PretrainedConfig,\n-        nbits: int = 4,\n-        axis_key: int = 0,\n-        axis_value: int = 0,\n-        q_group_size: int = 64,\n-        residual_length: int = 128,\n-    ):\n-        super().__init__(\"quanto\", config, nbits, axis_key, axis_value, q_group_size, residual_length)\n-\n-\n-class HQQQuantizedCache(QuantizedCache):\n-    \"\"\"\n-    A quantizer cache similar to what is described in the\n-    [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache paper](https://arxiv.org/abs/2402.02750).\n-    It allows the model to generate longer sequence length without allocating too much memory for keys and values\n-    by applying quantization.\n-    The cache has two types of storage, one for original precision and one for the\n-    quantized cache. A `residual length` is set as a maximum capacity for the original precision cache. When the\n-    length goes beyond maximum capacity, the original precision cache is discarded and moved into the quantized cache.\n-    The quantization is done per-channel with a set `q_group_size` for both keys and values, in contrast to what was\n-    described in the paper.\n-\n-    See `Cache` for details on common methods that are implemented by all cache classes.\n-\n-    Example:\n-\n-    ```python\n-    >>> # Run pip install hqq first if you don't have it yet\n-    >>> from transformers import AutoTokenizer, AutoModelForCausalLM, HQQQuantizedCache\n-\n-    >>> model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n-    >>> tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n-\n-    >>> inputs = tokenizer(text=\"My name is Qwen2\", return_tensors=\"pt\")\n-\n-    >>> # Prepare a cache class and pass it to model's forward\n-    >>> past_key_values = HQQQuantizedCache(config=model.config, nbits=4, axis_key=1, axis_value=1)\n-    >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n-    >>> outputs.past_key_values # access cache filled with key/values from generation\n-    HQQQuantizedCache()\n-    ```\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        config: PretrainedConfig,\n-        nbits: int = 4,\n-        axis_key: int = 0,\n-        axis_value: int = 0,\n-        q_group_size: int = 64,\n-        residual_length: int = 128,\n-    ):\n-        super().__init__(\"hqq\", config, nbits, axis_key, axis_value, q_group_size, residual_length)\n-\n-\n class EncoderDecoderCache(Cache):\n     \"\"\"\n     Base, abstract class for all encoder-decoder caches. Can be used to hold combinations of self-attention and\n     cross-attention caches.\n \n     See `Cache` for details on common methods that are implemented by all cache classes.\n \n+    Args:\n+        caches (`Iterable`):\n+            Usually an iterable of length 2, containing 2 `Cache` objects, the first one for self-attention, the\n+            second one for cross-attention. Can optionally also be an iterable of length 1, containing a\n+            `tuple[tuple[torch.Tensor]]` (usually used for compatibility with torch dp and ddp).\n+\n     Example:\n \n     ```python\n@@ -1590,6 +1406,94 @@ def is_compileable(self) -> bool:\n ### Deprecated classes\n \n \n+class OffloadedCache(DynamicCache):\n+    def __init__(self) -> None:\n+        logger.warning_once(\n+            \"`OffloadedCache` is deprecated and will be removed in version v4.60 \"\n+            \"Use `DynamicCache(offloading=True)` instead\"\n+        )\n+        super().__init__(offloading=True)\n+\n+\n+class OffloadedStaticCache(StaticCache):\n+    def __init__(self, config: PretrainedConfig, max_cache_len: int, *args, **kwargs):\n+        logger.warning_once(\n+            \"`OffloadedStaticCache` is deprecated and will be removed in version v4.59 \"\n+            \"Use `StaticCache(..., offloading=True)` instead\"\n+        )\n+        super().__init__(config=config, max_cache_len=max_cache_len, offloading=True)\n+\n+\n+class SlidingWindowCache(StaticCache):\n+    def __init__(self, config: PretrainedConfig, max_cache_len: int, *args, **kwargs):\n+        logger.warning_once(\n+            \"`SlidingWindowCache` is deprecated and will be removed in version v4.59 \"\n+            \"Use `StaticCache(...)` instead which will correctly infer the type of each layer.\"\n+        )\n+        super().__init__(config=config, max_cache_len=max_cache_len)\n+\n+\n+class HybridCache(StaticCache):\n+    def __init__(self, config: PretrainedConfig, max_cache_len: int, *args, **kwargs):\n+        logger.warning_once(\n+            \"`HybridCache` is deprecated and will be removed in version v4.59 \"\n+            \"Use `StaticCache(...)` instead which will correctly infer the type of each layer.\"\n+        )\n+        super().__init__(config=config, max_cache_len=max_cache_len)\n+\n+\n+class HybridChunkedCache(StaticCache):\n+    def __init__(self, config: PretrainedConfig, max_cache_len: int, *args, **kwargs):\n+        logger.warning_once(\n+            \"`HybridChunkedCache` is deprecated and will be removed in version v4.59 \"\n+            \"Use `StaticCache(...)` instead which will correctly infer the type of each layer.\"\n+        )\n+        super().__init__(config=config, max_cache_len=max_cache_len)\n+\n+\n+class OffloadedHybridCache(StaticCache):\n+    def __init__(self, config: PretrainedConfig, max_cache_len: int, *args, **kwargs):\n+        logger.warning_once(\n+            \"`OffloadedHybridCache` is deprecated and will be removed in version v4.59 \"\n+            \"Use `StaticCache(..., offload=True)` instead which will correctly infer the type of each layer.\"\n+        )\n+        super().__init__(config=config, max_cache_len=max_cache_len, offloading=True)\n+\n+\n+class QuantoQuantizedCache(QuantizedCache):\n+    def __init__(\n+        self,\n+        config: PretrainedConfig,\n+        nbits: int = 4,\n+        axis_key: int = 0,\n+        axis_value: int = 0,\n+        q_group_size: int = 64,\n+        residual_length: int = 128,\n+    ):\n+        logger.warning_once(\n+            \"`QuantoQuantizedCache` is deprecated and will be removed in version v4.59 \"\n+            \"Use `QuantizedCache(backend='quanto', ...)` instead.\"\n+        )\n+        super().__init__(\"quanto\", config, nbits, axis_key, axis_value, q_group_size, residual_length)\n+\n+\n+class HQQQuantizedCache(QuantizedCache):\n+    def __init__(\n+        self,\n+        config: PretrainedConfig,\n+        nbits: int = 4,\n+        axis_key: int = 0,\n+        axis_value: int = 0,\n+        q_group_size: int = 64,\n+        residual_length: int = 128,\n+    ):\n+        logger.warning_once(\n+            \"`HQQQuantizedCache` is deprecated and will be removed in version v4.59 \"\n+            \"Use `QuantizedCache(backend='hqq', ...)` instead.\"\n+        )\n+        super().__init__(\"hqq\", config, nbits, axis_key, axis_value, q_group_size, residual_length)\n+\n+\n class SinkCache(Cache):\n     \"\"\"\n     It is now a `custom_generate` repository on the Hub: https://huggingface.co/transformers-community/sink_cache."
        },
        {
            "sha": "0ffd33c2d64953c602ae50b9985b7fc96d6c136b",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 15,
            "deletions": 28,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/242bb2cafccec9f90479f5f688bca9d240b1031f/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242bb2cafccec9f90479f5f688bca9d240b1031f/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=242bb2cafccec9f90479f5f688bca9d240b1031f",
            "patch": "@@ -43,35 +43,23 @@\n \n logger = logging.get_logger(__name__)\n METADATA_FIELDS = (\"_from_model_config\", \"_commit_hash\", \"_original_object_hash\", \"transformers_version\")\n-STATIC_CACHE_CLASSES_MAPPING = {}\n-QUANT_BACKEND_CLASSES_MAPPING = {}\n-ALL_CACHE_IMPLEMENTATIONS = []\n+STATIC_CACHE_IMPLEMENTATIONS = (\"static\", \"offloaded_static\")\n+DYNAMIC_CACHE_IMPLEMENTATIONS = (\"dynamic\", \"offloaded\", \"quantized\")\n+# All the following are redundant and deprecated, but kept for BC\n+DEPRECATED_STATIC_CACHE_IMPLEMENTATIONS = (\n+    \"sliding_window\",\n+    \"hybrid\",\n+    \"hybrid_chunked\",\n+    \"offloaded_hybrid\",\n+    \"offloaded_hybrid_chunked\",\n+)\n+ALL_STATIC_CACHE_IMPLEMENTATIONS = STATIC_CACHE_IMPLEMENTATIONS + DEPRECATED_STATIC_CACHE_IMPLEMENTATIONS\n+ALL_CACHE_IMPLEMENTATIONS = ALL_STATIC_CACHE_IMPLEMENTATIONS + DYNAMIC_CACHE_IMPLEMENTATIONS\n+\n \n if is_torch_available():\n-    from ..cache_utils import (\n-        HQQQuantizedCache,\n-        HybridCache,\n-        HybridChunkedCache,\n-        OffloadedHybridCache,\n-        OffloadedStaticCache,\n-        QuantoQuantizedCache,\n-        SlidingWindowCache,\n-        StaticCache,\n-    )\n     from .logits_process import SynthIDTextWatermarkLogitsProcessor, WatermarkLogitsProcessor\n \n-    STATIC_CACHE_CLASSES_MAPPING = {\n-        \"static\": StaticCache,\n-        \"offloaded_static\": OffloadedStaticCache,\n-        \"sliding_window\": SlidingWindowCache,\n-        \"hybrid\": HybridCache,\n-        \"hybrid_chunked\": HybridChunkedCache,\n-        \"offloaded_hybrid\": OffloadedHybridCache,\n-        \"offloaded_hybrid_chunked\": OffloadedHybridCache,\n-    }\n-    QUANT_BACKEND_CLASSES_MAPPING = {\"quanto\": QuantoQuantizedCache, \"HQQ\": HQQQuantizedCache}\n-    ALL_CACHE_IMPLEMENTATIONS = list(STATIC_CACHE_CLASSES_MAPPING.keys()) + [\"offloaded\", \"dynamic\", \"quantized\"]\n-\n \n class GenerationMode(ExplicitEnum):\n     \"\"\"\n@@ -173,9 +161,8 @@ class GenerationConfig(PushToHubMixin):\n \n             - `\"dynamic\"`: [`DynamicCache`]\n             - `\"static\"`: [`StaticCache`]\n-            - `\"offloaded_static\"`: [`OffloadedStaticCache`]\n-            - `\"sliding_window\"`: [`SlidingWindowCache`]\n-            - `\"hybrid\"`: [`HybridCache`]\n+            - `\"offloaded\"`: [`DynamicCache(offloaded=True)`]\n+            - `\"offloaded_static\"`: [`StaticCache(offloaded=True)`]\n             - `\"quantized\"`: [`QuantizedCache`]\n \n             If none is specified, we will use the default cache for the model (which is often [`DynamicCache`]). See"
        },
        {
            "sha": "9ba3e2a6d2775559b5ee25f8749e03d7f39badd9",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 18,
            "deletions": 24,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/242bb2cafccec9f90479f5f688bca9d240b1031f/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242bb2cafccec9f90479f5f688bca9d240b1031f/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=242bb2cafccec9f90479f5f688bca9d240b1031f",
            "patch": "@@ -32,9 +32,8 @@\n     Cache,\n     DynamicCache,\n     EncoderDecoderCache,\n-    HybridChunkedCache,\n-    OffloadedCache,\n-    OffloadedHybridCache,\n+    QuantizedCache,\n+    StaticCache,\n )\n from ..configuration_utils import PretrainedConfig\n from ..dynamic_module_utils import (\n@@ -71,8 +70,9 @@\n     _prepare_token_type_ids,\n )\n from .configuration_utils import (\n-    QUANT_BACKEND_CLASSES_MAPPING,\n-    STATIC_CACHE_CLASSES_MAPPING,\n+    ALL_STATIC_CACHE_IMPLEMENTATIONS,\n+    DEPRECATED_STATIC_CACHE_IMPLEMENTATIONS,\n+    STATIC_CACHE_IMPLEMENTATIONS,\n     GenerationConfig,\n     GenerationMode,\n )\n@@ -1822,27 +1822,18 @@ def _get_cache(self, cache_implementation: str, batch_size: int, max_cache_len:\n \n         Returns the resulting cache object.\n         \"\"\"\n-        if cache_implementation == \"hybrid\" and \"llama4\" in getattr(self.config, \"model_type\", \"\"):\n-            cache_implementation = \"hybrid_chunked\"\n-\n-        cache_cls: Cache = STATIC_CACHE_CLASSES_MAPPING[cache_implementation]\n         requires_cross_attention_cache = (\n             self.config.is_encoder_decoder or model_kwargs.get(\"encoder_outputs\") is not None\n         )\n+        offload_cache = \"offloaded\" in cache_implementation\n \n         if hasattr(self, \"_cache\"):\n             cache_to_check = self._cache.self_attention_cache if requires_cross_attention_cache else self._cache\n \n-        if cache_implementation == \"sliding_window\":\n-            max_cache_len = min(self.config.sliding_window, max_cache_len)\n-\n         need_new_cache = (\n             not hasattr(self, \"_cache\")\n-            or (not isinstance(cache_to_check, cache_cls))\n+            or cache_to_check.offloading != offload_cache\n             or cache_to_check.max_batch_size != batch_size\n-            or isinstance(\n-                cache_to_check, (HybridChunkedCache, OffloadedHybridCache)\n-            )  # due to internal slicing, we always re-init\n             or cache_to_check.max_cache_len < max_cache_len\n         )\n \n@@ -1853,12 +1844,12 @@ def _get_cache(self, cache_implementation: str, batch_size: int, max_cache_len:\n             )\n \n         if need_new_cache:\n-            cache_kwargs = {\"config\": self.config.get_text_config(), \"max_cache_len\": max_cache_len}\n-            self._cache = cache_cls(**cache_kwargs)\n+            cache_kwargs = {\"config\": self.config, \"max_cache_len\": max_cache_len, \"offloading\": offload_cache}\n+            self._cache = StaticCache(**cache_kwargs)\n             if requires_cross_attention_cache:\n                 encoder_kwargs = cache_kwargs.copy()\n                 encoder_kwargs[\"max_cache_len\"] = model_kwargs[\"encoder_outputs\"][0].shape[1]\n-                self._cache = EncoderDecoderCache(self._cache, cache_cls(**encoder_kwargs))\n+                self._cache = EncoderDecoderCache(self._cache, StaticCache(**encoder_kwargs))\n         else:\n             self._cache.reset()\n         return self._cache\n@@ -1957,7 +1948,12 @@ def _prepare_cache_for_generation(\n             else {}\n         )\n         if generation_config.cache_implementation is not None:\n-            if generation_config.cache_implementation in STATIC_CACHE_CLASSES_MAPPING:\n+            if generation_config.cache_implementation in ALL_STATIC_CACHE_IMPLEMENTATIONS:\n+                if generation_config.cache_implementation in DEPRECATED_STATIC_CACHE_IMPLEMENTATIONS:\n+                    logger.warning_once(\n+                        f\"Using `cache_implementation='{generation_config.cache_implementation}' is deprecated. Please only \"\n+                        f\"use one of {STATIC_CACHE_IMPLEMENTATIONS}, and the layer structure will be inferred automatically.\"\n+                    )\n                 model_kwargs[cache_name] = self._get_cache(\n                     cache_implementation=generation_config.cache_implementation,\n                     batch_size=max(generation_config.num_beams, generation_config.num_return_sequences) * batch_size,\n@@ -1977,7 +1973,6 @@ def _prepare_cache_for_generation(\n                     cache_config[\"config\"] = self.config.get_text_config()\n                 # Pop the backend from the config (defaults to quanto if not defined)\n                 backend = cache_config.pop(\"backend\", \"quanto\")\n-                cache_class = QUANT_BACKEND_CLASSES_MAPPING[backend]\n \n                 if backend == \"quanto\" and not is_optimum_quanto_available():\n                     raise ImportError(\n@@ -1989,10 +1984,9 @@ def _prepare_cache_for_generation(\n                         \"You need to install `HQQ` in order to use KV cache quantization with HQQ backend. \"\n                         \"Please install it via  with `pip install hqq`\"\n                     )\n-\n-                model_kwargs[cache_name] = cache_class(**cache_config)\n+                model_kwargs[cache_name] = QuantizedCache(backend=backend, **cache_config)\n             elif generation_config.cache_implementation == \"offloaded\":\n-                model_kwargs[cache_name] = OffloadedCache()\n+                model_kwargs[cache_name] = DynamicCache(**dynamic_cache_kwargs, offloading=True)\n             elif generation_config.cache_implementation == \"dynamic\":\n                 model_kwargs[cache_name] = DynamicCache(**dynamic_cache_kwargs)\n "
        },
        {
            "sha": "c5455587e98e9b27870ef81b92101f32ec58201a",
            "filename": "src/transformers/integrations/executorch.py",
            "status": "modified",
            "additions": 6,
            "deletions": 10,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/242bb2cafccec9f90479f5f688bca9d240b1031f/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242bb2cafccec9f90479f5f688bca9d240b1031f/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fexecutorch.py?ref=242bb2cafccec9f90479f5f688bca9d240b1031f",
            "patch": "@@ -20,7 +20,6 @@\n     DynamicLayer,\n     DynamicSlidingWindowLayer,\n     EncoderDecoderCache,\n-    HybridCache,\n     StaticCache,\n )\n from ..generation.configuration_utils import GenerationConfig\n@@ -38,9 +37,6 @@\n )\n \n \n-# Add this to src/transformers/integrations/executorch.py\n-\n-\n class TorchExportableModuleForVLM:\n     \"\"\"\n     A wrapper class for exporting Vision-Language Models (VLMs) like SmolVLM2 for ExecuTorch.\n@@ -207,7 +203,7 @@ def __init__(\n         model: PreTrainedModel,\n     ):\n         \"\"\"\n-        Initializes the exportable module with `HybridCache`.\n+        Initializes the exportable module.\n \n         Args:\n             model (`PreTrainedModel`): The pretrained model to wrap.\n@@ -636,7 +632,7 @@ def generate(\n class TorchExportableModuleWithHybridCache(torch.nn.Module):\n     \"\"\"\n     A recipe module designed to make a `PreTrainedModel` exportable with `torch.export`,\n-    specifically for decoder-only LM to `HybridCache`. This module ensures that the\n+    specifically for decoder-only LM to hybrid `StaticCache`. This module ensures that the\n     exported model is compatible with further lowering and execution in `ExecuTorch`.\n     \"\"\"\n \n@@ -645,13 +641,13 @@ def __init__(\n         model: PreTrainedModel,\n     ):\n         \"\"\"\n-        Initializes the exportable module with `HybridCache`.\n+        Initializes the exportable module.\n \n         Args:\n             model (`PreTrainedModel`): The pretrained model to wrap.\n \n         Raises:\n-            AssertionError: If the model doesn't have the expected configuration for HybridCache.\n+            AssertionError: If the model doesn't have the expected configuration for an hybrid StaticCache.\n         \"\"\"\n         super().__init__()\n         self.model = model\n@@ -676,8 +672,8 @@ def __init__(\n         if not config.use_cache:\n             raise AssertionError(\"Model must have caching enabled.\")\n \n-        # Initialize the HybridCache\n-        self.cache = HybridCache(config=config, max_cache_len=generation_config.cache_config.get(\"max_cache_len\"))\n+        # Initialize the cache\n+        self.cache = StaticCache(config=config, max_cache_len=generation_config.cache_config.get(\"max_cache_len\"))\n         head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n         num_heads = getattr(config, \"num_key_value_heads\", config.num_attention_heads)\n         max_batch_size = generation_config.cache_config.get(\"batch_size\")"
        },
        {
            "sha": "3dccd648c9835c8b477ffe742a1d465877007a01",
            "filename": "src/transformers/masking_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/242bb2cafccec9f90479f5f688bca9d240b1031f/src%2Ftransformers%2Fmasking_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242bb2cafccec9f90479f5f688bca9d240b1031f/src%2Ftransformers%2Fmasking_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmasking_utils.py?ref=242bb2cafccec9f90479f5f688bca9d240b1031f",
            "patch": "@@ -736,7 +736,7 @@ def create_causal_mask(\n ) -> Optional[Union[torch.Tensor, BlockMask]]:\n     \"\"\"\n     Create a standard causal mask based on the attention implementation used (stored in the config). If `past_key_values`\n-    has an HybridCache structure, this function will return the mask corresponding to one of the \"full_attention\" layers (to align\n+    has an hybrid cache structure, this function will return the mask corresponding to one of the \"full_attention\" layers (to align\n     to what is needed in the `modeling_xxx.py` files).\n \n     Args:\n@@ -761,7 +761,7 @@ def create_causal_mask(\n             An optional mask function to combine with the causal mask function (by doing the intersection of both). This is\n             useful to easily overlay another mask on top of the causal one, for example for image tokens handling.\n     \"\"\"\n-    # If we have an HybridCache structure, here we want to create the mask for the full layers\n+    # If we have an hybrid cache structure, here we want to create the mask for the full layers\n     if hasattr(past_key_values, \"is_sliding\") and False in past_key_values.is_sliding:\n         layer_idx = past_key_values.is_sliding.index(False)\n     else:\n@@ -828,7 +828,7 @@ def create_sliding_window_causal_mask(\n ) -> Optional[Union[torch.Tensor, BlockMask]]:\n     \"\"\"\n     Create a sliding window causal mask based on the attention implementation used (stored in the config). This type\n-    of attention pattern was mostly democratized by Mistral. If `past_key_values` has an HybridCache structure, this\n+    of attention pattern was mostly democratized by Mistral. If `past_key_values` has an hybrid cache structure, this\n     function will return the mask corresponding to one of the \"sliding_attention\" layers (to align to what is needed in the\n     `modeling_xxx.py` files).\n \n@@ -854,7 +854,7 @@ def create_sliding_window_causal_mask(\n             An optional mask function to combine with the sliding causal mask function (by doing the intersection of both). This is\n             useful to easily overlay another mask on top of the sliding causal one, for example for image tokens handling.\n     \"\"\"\n-    # If we have an HybridCache structure, here we want to create the mask for the sliding layers\n+    # If we have an hybrid cache structure, here we want to create the mask for the sliding layers\n     if hasattr(past_key_values, \"is_sliding\") and True in past_key_values.is_sliding:\n         layer_idx = past_key_values.is_sliding.index(True)\n     else:\n@@ -923,7 +923,7 @@ def create_chunked_causal_mask(\n ) -> Optional[Union[torch.Tensor, BlockMask]]:\n     \"\"\"\n     Create a chunked attention causal mask based on the attention implementation used (stored in the config). This type\n-    of attention pattern was mostly democratized by Llama4. If `past_key_values` has an HybridCache structure, this\n+    of attention pattern was mostly democratized by Llama4. If `past_key_values` has an hybrid cache structure, this\n     function will return the mask corresponding to one of the \"chunked_attention\" layers (to align to what is needed in the\n     `modeling_xxx.py` files).\n \n@@ -949,7 +949,7 @@ def create_chunked_causal_mask(\n             An optional mask function to combine with the chunked causal mask function (by doing the intersection of both). This is\n             useful to easily overlay another mask on top of the chunked causal one, for example for image tokens handling.\n     \"\"\"\n-    # If we have an HybridCache structure, here we want to create the mask for the sliding layers\n+    # If we have an hybrid cache structure, here we want to create the mask for the sliding layers\n     if hasattr(past_key_values, \"is_sliding\") and True in past_key_values.is_sliding:\n         layer_idx = past_key_values.is_sliding.index(True)\n     else:"
        },
        {
            "sha": "0befaacb7295238a4bc25ae1e3296d5d2ec9f758",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242bb2cafccec9f90479f5f688bca9d240b1031f/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242bb2cafccec9f90479f5f688bca9d240b1031f/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=242bb2cafccec9f90479f5f688bca9d240b1031f",
            "patch": "@@ -467,7 +467,7 @@ class BambaMixer(nn.Module):\n     and is why Mamba is called **selective** state spaces)\n \n     The are a few differences between this and Mamba2Mixer:\n-    - The variable use_precomputed_states is slightly different due to the HybridCache structure\n+    - The variable use_precomputed_states is slightly different due to the hybrid cache structure\n     - There's a few non-obvious bugs fixed with batching in the slow path that exist in main\n     - Some extra variables that our layer doesn't need have been removed\n     - We ported most of the refactors in https://github.com/huggingface/transformers/pull/35154, which is (as of Dec 18, 2024) unmerged"
        },
        {
            "sha": "03fa58825c507bfbcdacd9033b1401804379dc9e",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242bb2cafccec9f90479f5f688bca9d240b1031f/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242bb2cafccec9f90479f5f688bca9d240b1031f/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=242bb2cafccec9f90479f5f688bca9d240b1031f",
            "patch": "@@ -225,7 +225,7 @@ class BambaMixer(nn.Module):\n     and is why Mamba is called **selective** state spaces)\n \n     The are a few differences between this and Mamba2Mixer:\n-    - The variable use_precomputed_states is slightly different due to the HybridCache structure\n+    - The variable use_precomputed_states is slightly different due to the hybrid cache structure\n     - There's a few non-obvious bugs fixed with batching in the slow path that exist in main\n     - Some extra variables that our layer doesn't need have been removed\n     - We ported most of the refactors in https://github.com/huggingface/transformers/pull/35154, which is (as of Dec 18, 2024) unmerged"
        },
        {
            "sha": "458cea4054b01ddcfa5ceb60b6598ff4d56cefe3",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242bb2cafccec9f90479f5f688bca9d240b1031f/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242bb2cafccec9f90479f5f688bca9d240b1031f/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=242bb2cafccec9f90479f5f688bca9d240b1031f",
            "patch": "@@ -477,7 +477,7 @@ class GPTNeoPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"GPTNeoBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn = True\n-    _can_compile_fullgraph = False  # TODO: needs a HybridCache\n+    _can_compile_fullgraph = False  # TODO: needs a hybrid cache\n \n     def __init__(self, *inputs, **kwargs):\n         super().__init__(*inputs, **kwargs)"
        },
        {
            "sha": "0a0e8299a9334f62d3eb89a4c231055639ad0c0f",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242bb2cafccec9f90479f5f688bca9d240b1031f/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242bb2cafccec9f90479f5f688bca9d240b1031f/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=242bb2cafccec9f90479f5f688bca9d240b1031f",
            "patch": "@@ -394,7 +394,7 @@ class GraniteMoeHybridMambaLayer(nn.Module):\n     and is why Mamba is called **selective** state spaces)\n \n     The are a few differences between this and Mamba2Mixer:\n-    - The variable use_precomputed_states is slightly different due to the HybridCache structure\n+    - The variable use_precomputed_states is slightly different due to the hybrid cache structure\n     - There's a few non-obvious bugs fixed with batching in the slow path that exist in main\n     - Some extra variables that our layer doesn't need have been removed\n     - We ported most of the refactors in https://github.com/huggingface/transformers/pull/35154, which is (as of Dec 18, 2024) unmerged"
        },
        {
            "sha": "397bb3209adfae45312a2ab6fe3ce996cf3ad459",
            "filename": "src/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 6,
            "deletions": 10,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/242bb2cafccec9f90479f5f688bca9d240b1031f/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242bb2cafccec9f90479f5f688bca9d240b1031f/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py?ref=242bb2cafccec9f90479f5f688bca9d240b1031f",
            "patch": "@@ -27,7 +27,7 @@\n import torch.nn as nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationConfig, GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n@@ -945,14 +945,9 @@ def _update_causal_mask(\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n         using_static_cache = isinstance(past_key_values, StaticCache)\n-        using_sliding_window_cache = isinstance(past_key_values, SlidingWindowCache)\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and not (using_static_cache or using_sliding_window_cache)\n-            and not output_attentions\n-        ):\n+        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -965,8 +960,8 @@ def _update_causal_mask(\n         dtype = input_tensor.dtype\n         min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n-        # SlidingWindowCache or StaticCache\n-        if using_sliding_window_cache or using_static_cache:\n+        # StaticCache\n+        if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         # DynamicCache or no cache\n         else:\n@@ -1049,7 +1044,8 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n-                if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n+                is_static_sliding_cache = isinstance(past_key_values, StaticCache) and all(past_key_values.is_sliding)\n+                if not is_static_sliding_cache or sequence_length > target_length:\n                     sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n                         cache_position.reshape(-1, 1) - text_config.sliding_window\n                     )"
        },
        {
            "sha": "4eb9385d3efe9505675ccfc665cb27a5937df81a",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 12,
            "deletions": 20,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/242bb2cafccec9f90479f5f688bca9d240b1031f/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242bb2cafccec9f90479f5f688bca9d240b1031f/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=242bb2cafccec9f90479f5f688bca9d240b1031f",
            "patch": "@@ -24,7 +24,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationConfig, GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n@@ -1084,14 +1084,9 @@ def _update_causal_mask(\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n         using_static_cache = isinstance(past_key_values, StaticCache)\n-        using_sliding_window_cache = isinstance(past_key_values, SlidingWindowCache)\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and not (using_static_cache or using_sliding_window_cache)\n-            and not output_attentions\n-        ):\n+        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -1104,8 +1099,8 @@ def _update_causal_mask(\n         dtype = input_tensor.dtype\n         min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n-        # SlidingWindowCache or StaticCache\n-        if using_sliding_window_cache or using_static_cache:\n+        # StaticCache\n+        if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         # DynamicCache or no cache\n         else:\n@@ -1189,7 +1184,8 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n-                if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n+                is_static_sliding_cache = isinstance(past_key_values, StaticCache) and all(past_key_values.is_sliding)\n+                if not is_static_sliding_cache or sequence_length > target_length:\n                     sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n                         cache_position.reshape(-1, 1) - text_config.sliding_window\n                     )\n@@ -1357,14 +1353,9 @@ def _update_causal_mask(\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n         using_static_cache = isinstance(past_key_values, StaticCache)\n-        using_sliding_window_cache = isinstance(past_key_values, SlidingWindowCache)\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and not (using_static_cache or using_sliding_window_cache)\n-            and not output_attentions\n-        ):\n+        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -1377,8 +1368,8 @@ def _update_causal_mask(\n         dtype = input_tensor.dtype\n         min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n-        # SlidingWindowCache or StaticCache\n-        if using_sliding_window_cache or using_static_cache:\n+        # StaticCache\n+        if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         # DynamicCache or no cache\n         else:\n@@ -1462,7 +1453,8 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n-                if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n+                is_static_sliding_cache = isinstance(past_key_values, StaticCache) and all(past_key_values.is_sliding)\n+                if not is_static_sliding_cache or sequence_length > target_length:\n                     sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n                         cache_position.reshape(-1, 1) - text_config.sliding_window\n                     )\n@@ -2055,7 +2047,7 @@ def generate(\n             depth_decoder_generation_config = {\n                 \"min_length\": self.num_codebooks + 1,\n                 \"max_length\": self.num_codebooks + 1,\n-                \"cache_implementation\": \"sliding_window\",\n+                \"cache_implementation\": \"static\",\n             }\n         # update kwargs_depth_decoder: kwargs_depth_decoder have priority over depth_decoder_generation_config\n         depth_decoder_generation_config.update(kwargs_depth_decoder)"
        },
        {
            "sha": "6d42d62f25cff19bf2ded98d89e12cf0153cb9e5",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/242bb2cafccec9f90479f5f688bca9d240b1031f/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242bb2cafccec9f90479f5f688bca9d240b1031f/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=242bb2cafccec9f90479f5f688bca9d240b1031f",
            "patch": "@@ -21,7 +21,7 @@\n import torch.utils.checkpoint\n from torch import nn\n \n-from ...cache_utils import Cache, HybridCache, StaticCache\n+from ...cache_utils import Cache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast\n@@ -188,8 +188,6 @@ def _update_causal_mask(\n         inputs_lead_dim, sequence_length = input_tensor.shape[:2]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n-        elif isinstance(past_key_values, HybridCache):\n-            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]\n@@ -561,7 +559,8 @@ def prepare_inputs_for_generation(\n         if cache_position[0] == 0:\n             model_inputs[\"pixel_values\"] = pixel_values\n         is_training = token_type_ids is not None and labels is not None\n-        if cache_position[0] == 0 and isinstance(past_key_values, HybridCache):\n+        is_static_hybrid_cache = isinstance(past_key_values, StaticCache) and any(past_key_values.is_sliding)\n+        if cache_position[0] == 0 and is_static_hybrid_cache:\n             input_tensor = inputs_embeds if inputs_embeds is not None else input_ids\n             causal_mask = self.model._update_causal_mask(\n                 attention_mask, token_type_ids, past_key_values, cache_position, input_tensor, is_training"
        },
        {
            "sha": "ba1a617a3a9de36a8bf502279f70dc090d6160a0",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 6,
            "deletions": 10,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/242bb2cafccec9f90479f5f688bca9d240b1031f/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242bb2cafccec9f90479f5f688bca9d240b1031f/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=242bb2cafccec9f90479f5f688bca9d240b1031f",
            "patch": "@@ -23,7 +23,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter, _prepare_4d_causal_attention_mask\n from ...modeling_flash_attention_utils import is_flash_attn_available\n@@ -1073,14 +1073,9 @@ def _update_causal_mask(\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n         using_static_cache = isinstance(past_key_values, StaticCache)\n-        using_sliding_window_cache = isinstance(past_key_values, SlidingWindowCache)\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and not (using_static_cache or using_sliding_window_cache)\n-            and not output_attentions\n-        ):\n+        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -1093,8 +1088,8 @@ def _update_causal_mask(\n         dtype = input_tensor.dtype\n         min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n-        # SlidingWindowCache or StaticCache\n-        if using_sliding_window_cache or using_static_cache:\n+        # StaticCache\n+        if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         # DynamicCache or no cache\n         else:\n@@ -1177,7 +1172,8 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n-                if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n+                is_static_sliding_cache = isinstance(past_key_values, StaticCache) and all(past_key_values.is_sliding)\n+                if not is_static_sliding_cache or sequence_length > target_length:\n                     sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n                         cache_position.reshape(-1, 1) - text_config.sliding_window\n                     )"
        },
        {
            "sha": "18569b580350674386cbae433d0bd886b783f1db",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 6,
            "deletions": 10,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/242bb2cafccec9f90479f5f688bca9d240b1031f/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242bb2cafccec9f90479f5f688bca9d240b1031f/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=242bb2cafccec9f90479f5f688bca9d240b1031f",
            "patch": "@@ -28,7 +28,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n@@ -924,14 +924,9 @@ def _update_causal_mask(\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n         using_static_cache = isinstance(past_key_values, StaticCache)\n-        using_sliding_window_cache = isinstance(past_key_values, SlidingWindowCache)\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and not (using_static_cache or using_sliding_window_cache)\n-            and not output_attentions\n-        ):\n+        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -944,8 +939,8 @@ def _update_causal_mask(\n         dtype = input_tensor.dtype\n         min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n-        # SlidingWindowCache or StaticCache\n-        if using_sliding_window_cache or using_static_cache:\n+        # StaticCache\n+        if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         # DynamicCache or no cache\n         else:\n@@ -1029,7 +1024,8 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n                 # the check is needed to verify is current checkpoint was trained with sliding window or not\n-                if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n+                is_static_sliding_cache = isinstance(past_key_values, StaticCache) and all(past_key_values.is_sliding)\n+                if not is_static_sliding_cache or sequence_length > target_length:\n                     sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n                         cache_position.reshape(-1, 1) - text_config.sliding_window\n                     )"
        },
        {
            "sha": "b37b469f41f8e8a71b35c165356f2c373f38fdd0",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 11,
            "deletions": 9,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/242bb2cafccec9f90479f5f688bca9d240b1031f/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242bb2cafccec9f90479f5f688bca9d240b1031f/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=242bb2cafccec9f90479f5f688bca9d240b1031f",
            "patch": "@@ -82,8 +82,7 @@\n         Cache,\n         DynamicCache,\n         EncoderDecoderCache,\n-        HybridCache,\n-        QuantoQuantizedCache,\n+        QuantoQuantizedLayer,\n         StaticCache,\n     )\n     from transformers.generation import (\n@@ -1956,7 +1955,6 @@ def test_generate_with_static_cache(self):\n         Tests that generating with static cache give almost same results as with dynamic cache, and the output cache\n         has the expected shapes\n         \"\"\"\n-        set_model_tester_for_less_flaky_test(self)\n         for model_class in self.all_generative_model_classes:\n             # Here, we should ideally not skip any model, and test them all. However, some old models cannot correctly\n             # use a static cache because they don't create the causal masks correctly.\n@@ -2041,7 +2039,7 @@ def test_generate_with_quant_cache(self):\n             }\n \n             results = model.generate(**generation_kwargs, **inputs_dict)\n-            self.assertTrue(isinstance(results.past_key_values, QuantoQuantizedCache))\n+            self.assertTrue(all(isinstance(layer, QuantoQuantizedLayer) for layer in results.past_key_values.layers))\n \n             # passing past key values of different type should raise Error\n             with self.assertRaises(ValueError):\n@@ -2062,7 +2060,6 @@ def test_generate_compile_model_forward_fullgraph(self):\n \n          Runs two sequential generations to ensure the cache doesn't get stuck after the first compiled run! \n         \"\"\"\n-        set_model_tester_for_less_flaky_test(self)\n         for model_class in self.all_generative_model_classes:\n             # 1. Test exclusion criteria\n             if not model_class._can_compile_fullgraph:\n@@ -2163,7 +2160,12 @@ def test_generate_compile_model_forward_fullgraph(self):\n             finally:\n                 torch._logging.set_logs()\n \n-            if \"Recompiling\" in cl.out or (\"guard\" in cl.out and \"failure\" in cl.out):\n+            # Compilation of sliding layers necessarily has recompiles with `dynamic=False` - however this test\n+            # still checks that `fullgraph=True` is supported in this case, as compilation with `dynamic=None`\n+            # is the default and does not actually lead to too many recompiles\n+            has_sliding_layers = any(decoder_cache.is_sliding)\n+            has_recompilation = \"Recompiling\" in cl.out or (\"guard\" in cl.out and \"failure\" in cl.out)\n+            if not has_sliding_layers and has_recompilation:\n                 raise RuntimeError(\n                     f\"`torch.compile` recompiled part of the forward pass in {model.__class__.__name__}. \"\n                     \"See the test logs for more details.\"\n@@ -2530,11 +2532,11 @@ def _check_attentions_for_generate(\n         self.assertEqual(len(attentions), (output_length - prompt_length))\n \n         use_cache = decoder_past_key_values is not None\n-        has_static_cache = isinstance(decoder_past_key_values, (StaticCache, HybridCache))\n+        has_static_cache = isinstance(decoder_past_key_values, StaticCache)\n \n         # When `output_attentions=True`, each iteration of generate appends the attentions corresponding to the new\n         # token(s)\n-        # NOTE: `HybridCache` may have different lengths on different layers, if this test starts failing add more\n+        # NOTE: `StaticCache` may have different lengths on different layers, if this test starts failing add more\n         # elaborate checks\n         for generated_length, iter_attentions in enumerate(attentions):\n             # regardless of using cache, the first forward pass will have the full prompt as input\n@@ -2579,7 +2581,7 @@ def _check_hidden_states_for_generate(\n \n         # When `output_hidden_states=True`, each iteration of generate appends the hidden states corresponding to the\n         # new token(s)\n-        # NOTE: `HybridCache` may have different lengths on different layers, if this test starts failing add more\n+        # NOTE: `StaticCache` may have different lengths on different layers, if this test starts failing add more\n         # elaborate checks\n         for generated_length, iter_hidden_states in enumerate(hidden_states):\n             # regardless of using cache, the first forward pass will have the full prompt as input"
        },
        {
            "sha": "450e8d117f7ee9b9be8f4e794b7eea296200b0d7",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 18,
            "deletions": 157,
            "changes": 175,
            "blob_url": "https://github.com/huggingface/transformers/blob/242bb2cafccec9f90479f5f688bca9d240b1031f/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242bb2cafccec9f90479f5f688bca9d240b1031f/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=242bb2cafccec9f90479f5f688bca9d240b1031f",
            "patch": "@@ -50,11 +50,8 @@\n         DynamicCache,\n         Gemma2Config,\n         GenerationConfig,\n-        HybridCache,\n-        HybridChunkedCache,\n         LlamaConfig,\n         QuantizedCache,\n-        SlidingWindowCache,\n         StaticCache,\n         convert_and_export_with_cache,\n         pipeline,\n@@ -424,7 +421,7 @@ def test_static_cache_greedy_decoding_pad_left(self, attn_implementation):\n     @require_torch_accelerator\n     @slow\n     def test_offloaded_cache_uses_less_memory_than_dynamic_cache(self):\n-        \"\"\"Tests that OffloadedCache uses less memory than the default DynamicCache\"\"\"\n+        \"\"\"Tests that offloading uses less memory than the default DynamicCache\"\"\"\n         model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n         tokenizer = AutoTokenizer.from_pretrained(model_name)\n         model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n@@ -827,9 +824,8 @@ def test_hybrid_cache_exportability(self):\n         model = AutoModelForCausalLM.from_pretrained(model_id)\n         model.eval()\n         self.assertEqual(model.config.use_cache, True)\n-        self.assertEqual(model.config.cache_implementation, \"hybrid\")\n \n-        # Export + HybridCache\n+        # Export + hybrid StaticCache\n         model.eval()\n         max_batch_size = 1\n         max_cache_len = 23\n@@ -838,7 +834,7 @@ def test_hybrid_cache_exportability(self):\n \n         model.generation_config = GenerationConfig(\n             use_cache=True,\n-            cache_implementation=\"hybrid\",\n+            cache_implementation=\"static\",\n             max_length=max_cache_len,\n             cache_config={\n                 \"batch_size\": max_batch_size,\n@@ -944,7 +940,7 @@ def test_static_cache(self):\n         )\n \n     def test_sliding_window_cache(self):\n-        \"\"\"Test SlidingWindowCache with manually prefilled states and hardcoded assertions.\n+        \"\"\"Test fully sliding StaticCache with manually prefilled states and hardcoded assertions.\n \n         Scenario 1: Update within window, no slide yet\n         prefill:       [1.0, 2.0, 0.0, 0.0]\n@@ -961,7 +957,7 @@ def test_sliding_window_cache(self):\n         # Scenario 1: Update within window, no slide yet\n         config = copy.deepcopy(self.config)\n         config.layer_types = [\"sliding_attention\"] * config.num_hidden_layers\n-        sliding_cache = SlidingWindowCache(config=config, max_cache_len=self.max_cache_len)\n+        sliding_cache = StaticCache(config=config, max_cache_len=self.max_cache_len)\n         prefill = torch.tensor([1.0, 2.0])[None, None, :, None]\n         sliding_cache.update(\n             key_states=prefill,\n@@ -978,11 +974,11 @@ def test_sliding_window_cache(self):\n         self.assertEqual(\n             sliding_cache.layers[0].keys[0, 0, :, 0].tolist(),\n             [1.0, 2.0, 3.0, 0.0],\n-            \"SlidingWindowCache Scenario 1 failed\",\n+            \"Fully sliding StaticCache Scenario 1 failed\",\n         )\n \n         # Scenario 2: Update causing slide\n-        sliding_cache = SlidingWindowCache(config=config, max_cache_len=self.max_cache_len)\n+        sliding_cache = StaticCache(config=config, max_cache_len=self.max_cache_len)\n         prefill = torch.tensor([1.0, 2.0, 3.0, 4.0])[None, None, :, None]\n         sliding_cache.update(\n             key_states=prefill,\n@@ -999,11 +995,11 @@ def test_sliding_window_cache(self):\n         self.assertEqual(\n             sliding_cache.layers[0].keys[0, 0, :, 0].tolist(),\n             [2.0, 3.0, 4.0, 5.0],\n-            \"SlidingWindowCache Scenario 2 failed\",\n+            \"Fully sliding StaticCache Scenario 2 failed\",\n         )\n \n         # Scenario 3: Long prompt handling\n-        sliding_cache = SlidingWindowCache(config=config, max_cache_len=self.max_cache_len)\n+        sliding_cache = StaticCache(config=config, max_cache_len=self.max_cache_len)\n         long_prefill = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])[None, None, :, None]\n         sliding_cache.update(\n             key_states=long_prefill,\n@@ -1014,144 +1010,7 @@ def test_sliding_window_cache(self):\n         self.assertEqual(\n             sliding_cache.layers[0].keys[0, 0, :, 0].tolist(),\n             [3.0, 4.0, 5.0, 6.0],\n-            \"SlidingWindowCache Scenario 3 failed\",\n-        )\n-\n-    def test_hybrid_cache_static_mode(self):\n-        \"\"\"Test HybridCache with only 1 static layer.\n-\n-        Scenario 1: Static layer behavior\n-        prefill:       [1.0, 2.0, 0.0, 0.0]\n-        update pos 2:  [1.0, 2.0, 3.0, 0.0]\n-\n-        Scenario 2: Fill to capacity\n-        update pos 3:  [1.0, 2.0, 3.0, 4.0]\n-        \"\"\"\n-        config = copy.deepcopy(self.config)\n-        config.layer_types = [\"full_attention\"] * config.num_hidden_layers\n-\n-        # Scenario 1\n-        hybrid_cache_static_mode = HybridCache(config=config, max_cache_len=self.max_cache_len)\n-        prefill = torch.tensor([1.0, 2.0])[None, None, :, None]\n-        hybrid_cache_static_mode.update(\n-            key_states=prefill,\n-            value_states=prefill,\n-            layer_idx=0,\n-            cache_kwargs={\"cache_position\": torch.arange(2)},\n-        )\n-        hybrid_cache_static_mode.update(\n-            key_states=torch.tensor(3.0)[None, None, None, None],\n-            value_states=torch.tensor(3.0)[None, None, None, None],\n-            layer_idx=0,\n-            cache_kwargs={\"cache_position\": torch.tensor([2])},\n-        )\n-        self.assertEqual(\n-            hybrid_cache_static_mode.layers[0].keys[0, 0, :, 0].tolist(),\n-            [1.0, 2.0, 3.0, 0.0],\n-            \"HybridCache Static Scenario 1 failed\",\n-        )\n-\n-        # Scenario 2\n-        hybrid_cache_static_mode.update(\n-            key_states=torch.tensor(4.0)[None, None, None, None],\n-            value_states=torch.tensor(4.0)[None, None, None, None],\n-            layer_idx=0,\n-            cache_kwargs={\"cache_position\": torch.tensor([3])},\n-        )\n-        self.assertEqual(\n-            hybrid_cache_static_mode.layers[0].keys[0, 0, :, 0].tolist(),\n-            [1.0, 2.0, 3.0, 4.0],\n-            \"HybridCache Static Scenario 2 failed\",\n-        )\n-\n-    def test_hybrid_cache_sliding_mode(self):\n-        \"\"\"Test HybridCache in sliding mode with hardcoded assertions.\n-\n-        Scenario 1: Update within window, no slide yet\n-        prefill:       [1.0, 2.0, 0.0, 0.0]\n-        update pos 2:  [1.0, 2.0, 3.0, 0.0]\n-\n-        Scenario 2: Update causing first slide\n-        prefill:       [1.0, 2.0, 3.0, 4.0]\n-        update pos 4:  [2.0, 3.0, 4.0, 5.0] (shift happens as pos > window_size-1)\n-\n-        Scenario 3: Update causing subsequent slide\n-        update pos 5:  [3.0, 4.0, 5.0, 6.0] (shift continues)\n-\n-        Scenario 4: Long prompt handling (prompt_len > window_size)\n-        input:         [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n-        result:        [3.0, 4.0, 5.0, 6.0] (keeps last window_size tokens)\n-        \"\"\"\n-        config = copy.deepcopy(self.config)\n-        config.layer_types = [\"sliding_attention\"] * config.num_hidden_layers\n-        # Scenario 1: Update within window, no slide yet\n-        hybrid_cache = HybridCache(config=config, max_cache_len=self.max_cache_len)\n-        prefill = torch.tensor([1.0, 2.0])[None, None, :, None]\n-        hybrid_cache.update(\n-            key_states=prefill,\n-            value_states=prefill,\n-            layer_idx=0,\n-            cache_kwargs={\"cache_position\": torch.arange(2)},\n-        )\n-        hybrid_cache.update(\n-            key_states=torch.tensor(3.0)[None, None, None, None],\n-            value_states=torch.tensor(3.0)[None, None, None, None],\n-            layer_idx=0,\n-            cache_kwargs={\"cache_position\": torch.tensor([2])},\n-        )\n-        self.assertEqual(\n-            hybrid_cache.layers[0].keys[0, 0, :, 0].tolist(),\n-            [1.0, 2.0, 3.0, 0.0],\n-            \"HybridCache Sliding Scenario 1 failed\",\n-        )\n-\n-        # Scenario 2: Update causing first slide\n-        hybrid_cache = HybridCache(config=config, max_cache_len=self.max_cache_len)\n-        prefill = torch.tensor([1.0, 2.0, 3.0, 4.0])[None, None, :, None]\n-        hybrid_cache.update(\n-            key_states=prefill,\n-            value_states=prefill,\n-            layer_idx=0,\n-            cache_kwargs={\"cache_position\": torch.arange(4)},\n-        )\n-        hybrid_cache.update(\n-            key_states=torch.tensor(5.0)[None, None, None, None],\n-            value_states=torch.tensor(5.0)[None, None, None, None],\n-            layer_idx=0,\n-            cache_kwargs={\"cache_position\": torch.tensor([4])},\n-        )\n-        self.assertEqual(\n-            hybrid_cache.layers[0].keys[0, 0, :, 0].tolist(),\n-            [2.0, 3.0, 4.0, 5.0],\n-            \"HybridCache Sliding Scenario 2 failed\",\n-        )\n-\n-        # Scenario 3: Update causing subsequent slide\n-        hybrid_cache.update(\n-            key_states=torch.tensor(6.0)[None, None, None, None],\n-            value_states=torch.tensor(6.0)[None, None, None, None],\n-            layer_idx=0,\n-            cache_kwargs={\"cache_position\": torch.tensor([5])},\n-        )\n-        self.assertEqual(\n-            hybrid_cache.layers[0].keys[0, 0, :, 0].tolist(),\n-            [3.0, 4.0, 5.0, 6.0],\n-            \"HybridCache Sliding Scenario 3 failed\",\n-        )\n-\n-        # Scenario 4: Long prompt handling\n-        hybrid_cache = HybridCache(config=config, max_cache_len=self.max_cache_len)\n-        long_prefill = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])[None, None, :, None]\n-        hybrid_cache.update(\n-            key_states=long_prefill,\n-            value_states=long_prefill,\n-            layer_idx=0,\n-            cache_kwargs={\"cache_position\": torch.arange(6)},\n-        )\n-        self.assertEqual(\n-            hybrid_cache.layers[0].keys[0, 0, :, 0].tolist(),\n-            [3.0, 4.0, 5.0, 6.0],\n-            \"HybridCache Sliding Scenario 4 failed\",\n+            \"Fully sliding StaticCache Scenario 3 failed\",\n         )\n \n     def test_dynamic_cache(self):\n@@ -1221,7 +1080,7 @@ def test_dynamic_cache_batch_select_indices(self):\n \n     def test_hybrid_cache(self):\n         \"\"\"\n-        Test HybridCache with a mix of static and sliding layers,\n+        Test hybrid StaticCache with a mix of static and sliding layers,\n         with prefill size bigger than sliding window.\n \n         prefill:\n@@ -1237,7 +1096,7 @@ def test_hybrid_cache(self):\n         config.num_hidden_layers = 2\n         config.layer_types = [\"full_attention\", \"sliding_attention\"]\n         config.sliding_window = 2\n-        hybrid_cache = HybridCache(config=config, max_cache_len=self.max_cache_len)\n+        hybrid_cache = StaticCache(config=config, max_cache_len=self.max_cache_len)\n \n         # Prefill both layers up to cache capacity\n         prefill_static = torch.tensor([1.0, 2.0, 3.0])[None, None, :, None]\n@@ -1318,7 +1177,7 @@ def test_hybrid_cache(self):\n \n     def test_hybrid_chunked_cache(self):\n         \"\"\"\n-        Test HybridChunkedCache with both static and sliding layers and special cases:\n+        Test hybrid chunked StaticCache with both static and sliding layers and special cases:\n             1. a pre-fill longer than the sliding window\n             2. a single-token decoding step (normal generation)\n             3. a multi-token decoding step after the window is already full\n@@ -1340,8 +1199,9 @@ def test_hybrid_chunked_cache(self):\n         config.num_hidden_layers = 2\n         config.layer_types = [\"full_attention\", \"chunked_attention\"]\n         config.attention_chunk_size = 2\n+        config.sliding_window = None\n         max_cache_len = 4\n-        chunked_cache = HybridChunkedCache(config=config, max_cache_len=max_cache_len)\n+        chunked_cache = StaticCache(config=config, max_cache_len=max_cache_len)\n \n         # 1) PREFILL (3 tokens > sliding_window)\n         prefill_static = torch.tensor([1.0, 2.0, 3.0])[None, None, :, None]\n@@ -1419,8 +1279,9 @@ def test_hybrid_chunked_cache_extra_cases(self):\n         config = copy.deepcopy(self.config)\n         config.num_hidden_layers = 1\n         config.layer_types = [\"chunked_attention\"]\n-        config.sliding_window = 3\n-        cache = HybridChunkedCache(config=config, max_cache_len=3)\n+        config.sliding_window = None\n+        config.attention_chunk_size = 3\n+        cache = StaticCache(config=config, max_cache_len=3)\n \n         # Step 0 : multi-token prefill\n         first_chunk = torch.tensor([10.0, 20.0])[None, None, :, None]  # L = 2"
        }
    ],
    "stats": {
        "total": 877,
        "additions": 300,
        "deletions": 577
    }
}