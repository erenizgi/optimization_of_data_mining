{
    "author": "eustlb",
    "message": "[ASR pipline] fix with datasets 4.0 (#39504)\n\n* fix\n\n* handle edge case\n\n* make",
    "sha": "01d5f94695aac6b72f83225a16b69cf979e86df7",
    "files": [
        {
            "sha": "b29f4160786dc6b22dc7cecb553f24b76b55c7b7",
            "filename": "src/transformers/pipelines/automatic_speech_recognition.py",
            "status": "modified",
            "additions": 10,
            "deletions": 3,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/01d5f94695aac6b72f83225a16b69cf979e86df7/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01d5f94695aac6b72f83225a16b69cf979e86df7/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py?ref=01d5f94695aac6b72f83225a16b69cf979e86df7",
            "patch": "@@ -380,7 +380,11 @@ def preprocess(self, inputs, chunk_length_s=0, stride_length_s=None):\n \n             if isinstance(inputs, torchcodec.decoders.AudioDecoder):\n                 _audio_samples = inputs.get_all_samples()\n+\n+                # torchcodec always returns (num_channels, num_samples)\n+                # while before (datasets < 4.0) we had (2, num_samples) if stereo, (num_samples,) if mono\n                 _array = _audio_samples.data\n+                _array = _array[0] if _array.ndim == 2 and _array.shape[0] == 1 else _array\n                 inputs = {\"array\": _array, \"sampling_rate\": _audio_samples.sample_rate}\n \n         if isinstance(inputs, dict):\n@@ -429,10 +433,13 @@ def preprocess(self, inputs, chunk_length_s=0, stride_length_s=None):\n                 # can add extra data in the inputs, so we need to keep track\n                 # of the original length in the stride so we can cut properly.\n                 stride = (inputs.shape[0], int(round(stride[0] * ratio)), int(round(stride[1] * ratio)))\n-        if not isinstance(inputs, np.ndarray):\n+        if not isinstance(inputs, (np.ndarray, torch.Tensor)):\n             raise TypeError(f\"We expect a numpy ndarray or torch tensor as input, got `{type(inputs)}`\")\n-        if len(inputs.shape) != 1:\n-            raise ValueError(\"We expect a single channel audio input for AutomaticSpeechRecognitionPipeline\")\n+        if inputs.ndim != 1:\n+            logger.warning(\n+                f\"We expect a single channel audio input for AutomaticSpeechRecognitionPipeline, got {inputs.ndim}. Taking the mean of the channels for mono conversion.\"\n+            )\n+            inputs = inputs.mean(axis=0)\n \n         if chunk_length_s:\n             if stride_length_s is None:"
        }
    ],
    "stats": {
        "total": 13,
        "additions": 10,
        "deletions": 3
    }
}