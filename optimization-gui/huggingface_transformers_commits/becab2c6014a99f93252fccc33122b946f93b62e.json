{
    "author": "Cyrilvallez",
    "message": "Use the config for DynamicCache initialization in all modelings (#40420)\n\n* update all\n\n* remove the most horrible old code\n\n* style",
    "sha": "becab2c6014a99f93252fccc33122b946f93b62e",
    "files": [
        {
            "sha": "04272b7ff8958693d9fe595c53884e36d924599e",
            "filename": "docs/source/en/cache_explanation.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/docs%2Fsource%2Fen%2Fcache_explanation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/docs%2Fsource%2Fen%2Fcache_explanation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fcache_explanation.md?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -15,6 +15,7 @@ rendered properly in your Markdown viewer.\n -->\n \n # Caching\n+\n Imagine you're having a conversation with someone, and instead of remembering what they previously said, they have to start from scratch every time you respond. This would be slow and inefficient, right?\n \n You can extend this analogy to transformer models. Autoregressive model generation can be slow because it makes a prediction one token at a time. Each new prediction is dependent on all the previous context.\n@@ -107,7 +108,7 @@ model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16, device_map=device)\n tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n-past_key_values = DynamicCache()\n+past_key_values = DynamicCache(config=model.config)\n messages = [{\"role\": \"user\", \"content\": \"Hello, what's your name.\"}]\n inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True).to(model.device)\n \n@@ -138,7 +139,7 @@ The cache position tracks where to insert new tokens in the attention cache. It\n Cache position is used internally for two purposes:\n \n 1. Selecting new tokens to process in the input sequence and ensuring only tokens that havenâ€™t been cached yet are passed to the model's `forward`.\n-2. Storing key/value pairs at the correct positions in the cache. This is especially important for fixed-size caches, like [`StaticCache`], that pre-allocates a specific cache length.\n+2. Storing key/value pairs at the correct positions in the cache. This is especially important for fixed-size caches, that pre-allocates a specific cache length.\n \n The generation loop usually takes care of the cache position, but if you're writing a custom generation method, it is important that cache positions are accurate since they are used to write and read key/value states into fixed slots.\n "
        },
        {
            "sha": "02e7bf8862cb4753e8d684e993842429521843a3",
            "filename": "docs/source/en/kv_cache.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/docs%2Fsource%2Fen%2Fkv_cache.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/docs%2Fsource%2Fen%2Fkv_cache.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fkv_cache.md?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -227,7 +227,7 @@ tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n user_prompts = [\"Hello, what's your name?\", \"Btw, yesterday I was on a rock concert.\"]\n \n-past_key_values = DynamicCache()\n+past_key_values = DynamicCache(config=model.config)\n \n messages = []\n for prompt in user_prompts:"
        },
        {
            "sha": "d22d28d41c4bae1f576978e2f419b4020d4c998f",
            "filename": "docs/source/en/model_doc/gemma.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma.md?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -150,7 +150,7 @@ visualizer(\"LLMs generate text through a process known as\")\n    )\n    input_text = \"LLMs generate text through a process known as\"\n    input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n-   past_key_values = DynamicCache()\n+   past_key_values = DynamicCache(config=model.config)\n    outputs = model.generate(**input_ids, max_new_tokens=50, past_key_values=past_key_values)\n    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n    ```"
        },
        {
            "sha": "a2390449738a869c2fd6beb7818cead792396428",
            "filename": "docs/source/ko/cache_explanation.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/docs%2Fsource%2Fko%2Fcache_explanation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/docs%2Fsource%2Fko%2Fcache_explanation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fcache_explanation.md?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -107,7 +107,7 @@ model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16, device_map=device)\n tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n-past_key_values = DynamicCache()\n+past_key_values = DynamicCache(config=model.config)\n messages = [{\"role\": \"user\", \"content\": \"Hello, what's your name.\"}]\n inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True).to(model.device)\n "
        },
        {
            "sha": "7ce101a29b27a13d29903fa78505ea8073292f84",
            "filename": "examples/modular-transformers/modeling_dummy_bert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -541,7 +541,7 @@ def forward(\n                 use_cache = False\n \n         if use_cache and self.config.is_decoder and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n         if use_cache and self.config.is_decoder and isinstance(past_key_values, tuple):\n             logger.warning_once("
        },
        {
            "sha": "304b88227f01da058ee1ac3f9d31c32d64fa490e",
            "filename": "examples/modular-transformers/modeling_roberta.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/examples%2Fmodular-transformers%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/examples%2Fmodular-transformers%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_roberta.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -544,7 +544,7 @@ def forward(\n                 use_cache = False\n \n         if use_cache and self.config.is_decoder and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n         if use_cache and self.config.is_decoder and isinstance(past_key_values, tuple):\n             logger.warning_once("
        },
        {
            "sha": "4d5f6071a67d3ccbee174ba5aeb1a604b06a456d",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -996,7 +996,6 @@ class DynamicCache(Cache):\n     >>> past_key_values = DynamicCache(config=model.config)\n     >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n     >>> outputs.past_key_values # access cache filled with key/values from generation\n-    DynamicCache()\n     ```\n     \"\"\"\n \n@@ -1223,8 +1222,8 @@ class EncoderDecoderCache(Cache):\n     >>> inputs = processor(audio=YOUR-AUDIO, return_tensors=\"pt\")\n \n     >>> # Prepare cache classes for encoder and decoder and pass it to model's forward\n-    >>> self_attention_cache = DynamicCache()\n-    >>> cross_attention_cache = DynamicCache()\n+    >>> self_attention_cache = DynamicCache(config=self.config)\n+    >>> cross_attention_cache = DynamicCache(config=self.config)\n     >>> past_key_values = EncoderDecoderCache(self_attention_cache, cross_attention_cache)\n     >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n     >>> outputs.past_key_values # access cache filled with key/values from generation"
        },
        {
            "sha": "e376565c2ac0e4ecd06949a5c1e1a4d9a9fc04e0",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -1998,7 +1998,7 @@ def _prepare_cache_for_generation(\n             elif \"dynamic\" in generation_config.cache_implementation:\n                 model_kwargs[cache_name] = DynamicCache(**dynamic_cache_kwargs)\n \n-        # Use DynamicCache() instance by default. This will avoid back and forth from legacy format that\n+        # Use DynamicCache instance by default. This will avoid back and forth from legacy format that\n         # keeps copying the cache thus using much more memory\n         else:\n             model_kwargs[cache_name] = ("
        },
        {
            "sha": "85343bd02fe3bd032b459ce6a02b953ed20cd6c5",
            "filename": "src/transformers/integrations/executorch.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fexecutorch.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -854,7 +854,7 @@ def __init__(self, model, max_static_cache_length, batch_size):\n         head_dim = getattr(self.config, \"head_dim\", self.config.hidden_size // self.config.num_attention_heads)\n         num_heads = getattr(self.config, \"num_key_value_heads\", self.config.num_attention_heads)\n         self.static_cache.early_initialization(batch_size, num_heads, head_dim, torch.float32, model_device)\n-        self.cache = EncoderDecoderCache(self.static_cache, DynamicCache())\n+        self.cache = EncoderDecoderCache(self.static_cache, DynamicCache(config=self.config))\n \n         register_dynamic_cache_export_support()\n \n@@ -1051,7 +1051,7 @@ def export_with_dynamic_cache(\n             {\n                 \"input_ids\": example_input_ids,\n                 \"attention_mask\": example_attention_mask,\n-                \"past_key_values\": DynamicCache(),\n+                \"past_key_values\": DynamicCache(config=model.config),\n                 \"use_cache\": True,\n             },\n             strict=False,"
        },
        {
            "sha": "76768305ed652e6a343f5515755748c2b41c9b18",
            "filename": "src/transformers/models/autoformer/modeling_autoformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -1155,7 +1155,7 @@ def forward(\n             use_cache = False\n \n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \""
        },
        {
            "sha": "17782e561f45565ff66342f61bc0e598edb28f99",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -499,7 +499,7 @@ def forward(\n                 use_cache = False\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \""
        },
        {
            "sha": "6f88b664e82b0a5fa8158f3c598094bdbfbdfcce",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -1044,9 +1044,9 @@ def forward(\n         # initialize `past_key_values`\n         if use_cache and past_key_values is None:\n             past_key_values = (\n-                EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n                 if encoder_hidden_states is not None\n-                else DynamicCache()\n+                else DynamicCache(config=self.config)\n             )\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once("
        },
        {
            "sha": "9d2ce111f0bbc00d334d84b8af0f1e3d61566bdf",
            "filename": "src/transformers/models/bert/modeling_bert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -630,7 +630,7 @@ def forward(\n                 use_cache = False\n \n         if use_cache and self.config.is_decoder and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n         if use_cache and self.config.is_decoder and isinstance(past_key_values, tuple):\n             logger.warning_once("
        },
        {
            "sha": "b795b9fcb01cc33fdb40ac68ff140f1e09c000bb",
            "filename": "src/transformers/models/bert_generation/modeling_bert_generation.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -381,7 +381,7 @@ def forward(\n                 use_cache = False\n \n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \""
        },
        {
            "sha": "70d166a9c0085fd52f6987ff18253eb3bba5f6dd",
            "filename": "src/transformers/models/big_bird/modeling_big_bird.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -1569,7 +1569,7 @@ def forward(\n                 use_cache = False\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \""
        },
        {
            "sha": "70943bdc3cac6a58a11cd45e6995c9eb668783d8",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -2202,9 +2202,9 @@ def forward(\n         # initialize `past_key_values`\n         if use_cache and past_key_values is None:\n             past_key_values = (\n-                EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n                 if encoder_hidden_states is not None\n-                else DynamicCache()\n+                else DynamicCache(config=self.config)\n             )\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once("
        },
        {
            "sha": "07ab5e150db3ebf63e264b24915b9d1c3068faa5",
            "filename": "src/transformers/models/biogpt/modeling_biogpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -557,7 +557,7 @@ def forward(\n \n         # initialize past_key_values\n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \""
        },
        {
            "sha": "e37fbce26a76bba1831cd6118d6804fad6a6df94",
            "filename": "src/transformers/models/biogpt/modular_biogpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -381,7 +381,7 @@ def forward(\n \n         # initialize past_key_values\n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \""
        },
        {
            "sha": "4dce61c1c4e0b8338ca6ffbf6a6e1c66d92a9d68",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -1000,9 +1000,9 @@ def forward(\n         # initialize `past_key_values`\n         if use_cache and past_key_values is None:\n             past_key_values = (\n-                EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n                 if encoder_hidden_states is not None\n-                else DynamicCache()\n+                else DynamicCache(config=self.config)\n             )\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once("
        },
        {
            "sha": "65d18827b4d2d834182a8722b020dba1414d45ff",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -986,9 +986,9 @@ def forward(\n         # initialize `past_key_values`\n         if use_cache and past_key_values is None:\n             past_key_values = (\n-                EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n                 if encoder_hidden_states is not None\n-                else DynamicCache()\n+                else DynamicCache(config=self.config)\n             )\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once("
        },
        {
            "sha": "3eff5ac785e2218be6689c4544c1006710b97986",
            "filename": "src/transformers/models/blip/modeling_blip_text.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -444,9 +444,11 @@ def forward(\n             # The model acts as encoder decoder but is not an encoder decoder. So we cast all cache objects to\n             # `EncoderDecoderCache` type assuming that the incoming cache is from `self_attention`\n             elif isinstance(past_key_values, DynamicCache):\n-                past_key_values = EncoderDecoderCache(past_key_values, DynamicCache())\n+                past_key_values = EncoderDecoderCache(past_key_values, DynamicCache(config=self.config))\n             elif past_key_values is None:\n-                past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                past_key_values = EncoderDecoderCache(\n+                    DynamicCache(config=self.config), DynamicCache(config=self.config)\n+                )\n \n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None"
        },
        {
            "sha": "64576b683952a36b3980350e8e7a2529d6ec6749",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -550,7 +550,7 @@ def forward(\n             raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         batch_size, seq_length, _ = inputs_embeds.shape\n         past_length = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "385f6b224825db628eaa965a07d60bdea8f9e69c",
            "filename": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -768,7 +768,7 @@ def forward(\n                 use_cache = False\n \n         if use_cache and self.config.is_decoder and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n         if use_cache and self.config.is_decoder and isinstance(past_key_values, tuple):\n             logger.warning_once("
        },
        {
            "sha": "a4074a6fd381af9c7ba00e0f811f75050b7143ce",
            "filename": "src/transformers/models/camembert/modeling_camembert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -587,7 +587,7 @@ def forward(\n                 use_cache = False\n \n         if use_cache and self.config.is_decoder and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n         if use_cache and self.config.is_decoder and isinstance(past_key_values, tuple):\n             logger.warning_once("
        },
        {
            "sha": "9586eb6d3e850cd4cb0642b361d6fa5e608a3cf7",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -961,7 +961,7 @@ def forward(\n \n         # torch.jit.trace() doesn't support cache objects in the output\n         if use_cache and past_key_values is None and not torch.jit.is_tracing():\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "e847c3e4208eef9923c6de8b2e3b51f54d7b56bf",
            "filename": "src/transformers/models/clvp/modeling_clvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -1070,7 +1070,7 @@ def forward(\n                 use_cache = False\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \""
        },
        {
            "sha": "6c4f16ed1cb582b37d8cdf06a37b92a701556af4",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -381,7 +381,7 @@ def forward(\n             raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         seq_length = inputs_embeds.shape[1]\n         if cache_position is None:"
        },
        {
            "sha": "9fe0afeb72a331dffa0c50e03175ef36a85e0b95",
            "filename": "src/transformers/models/cpmant/modeling_cpmant.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -637,7 +637,7 @@ def forward(\n         span = torch.full((batch, seq_length), 0, dtype=dtype, device=device)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \""
        },
        {
            "sha": "0f3ce02d1af90471f3c90b611b6d3d46cb30e64f",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -440,7 +440,7 @@ def forward(\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds.\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "895b313291ad3cefadbc2040c3dc04a27ba944a7",
            "filename": "src/transformers/models/csm/modular_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -187,7 +187,7 @@ def forward(\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds.\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "ece001f9ce1faf891267f4fd23f226356e4ac86b",
            "filename": "src/transformers/models/ctrl/modeling_ctrl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -339,7 +339,7 @@ def forward(\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \""
        },
        {
            "sha": "7d65843d821a98fdf35a728a0641e44316113b46",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -485,7 +485,7 @@ def forward(\n                 use_cache = False\n \n         if use_cache and self.config.is_decoder and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n         if use_cache and self.config.is_decoder and isinstance(past_key_values, tuple):\n             logger.warning_once("
        },
        {
            "sha": "8517a235389db7ac119ac6f20f4b78c0469302f9",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -915,7 +915,7 @@ def forward(\n             raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "b13127a6f86fb033dc773abd671494e884a3b980",
            "filename": "src/transformers/models/decision_transformer/modeling_decision_transformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -569,7 +569,7 @@ def forward(\n         # based on pattern from src/transformers/models/whisper/modeling_whisper.py::WhisperDecoder and similar addition in GPT2Model\n         if use_cache:\n             if past_key_values is None:\n-                past_key_values = DynamicCache()\n+                past_key_values = DynamicCache(config=self.config)\n             elif isinstance(past_key_values, tuple):\n                 logger.warning_once(\n                     \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.53.0. \"\n@@ -578,10 +578,10 @@ def forward(\n                 )\n                 past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n             elif past_key_values is None:\n-                past_key_values = DynamicCache()\n+                past_key_values = DynamicCache(config=self.config)\n \n             if self.config.add_cross_attention and not isinstance(past_key_values, EncoderDecoderCache):\n-                past_key_values = EncoderDecoderCache(past_key_values, DynamicCache())\n+                past_key_values = EncoderDecoderCache(past_key_values, DynamicCache(config=self.config))\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "cf662b224aabd884521d7e14b8a167886377f4b5",
            "filename": "src/transformers/models/dia/modeling_dia.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -785,7 +785,7 @@ def forward(\n                 use_cache = False\n \n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n         if encoder_outputs is None:\n             encoder_outputs = self.encoder("
        },
        {
            "sha": "f99d32a01d9cffab79bd72852d776447e084681b",
            "filename": "src/transformers/models/dia/modular_dia.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -600,7 +600,7 @@ def forward(\n                 use_cache = False\n \n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n         if encoder_outputs is None:\n             encoder_outputs = self.encoder("
        },
        {
            "sha": "565cb67e05f4201365b9fe6d6ff672f99656c9b1",
            "filename": "src/transformers/models/electra/modeling_electra.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -540,7 +540,7 @@ def forward(\n                 use_cache = False\n \n         if use_cache and self.config.is_decoder and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n         if use_cache and self.config.is_decoder and isinstance(past_key_values, tuple):\n             logger.warning_once("
        },
        {
            "sha": "67ca76c97256801a89ca43ec561cfb931a485ece",
            "filename": "src/transformers/models/ernie/modeling_ernie.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -469,7 +469,7 @@ def forward(\n                 use_cache = False\n \n         if use_cache and self.config.is_decoder and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n         if use_cache and self.config.is_decoder and isinstance(past_key_values, tuple):\n             logger.warning_once("
        },
        {
            "sha": "36f1f25c32be12663f8c386be5a30527b4f40f02",
            "filename": "src/transformers/models/ernie4_5_moe/modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -527,7 +527,7 @@ def forward(\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)"
        },
        {
            "sha": "7bddb745df007f85fbbd6f72eacc723035d021c0",
            "filename": "src/transformers/models/ernie4_5_moe/modular_ernie4_5_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -265,7 +265,7 @@ def forward(\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)"
        },
        {
            "sha": "e74fadc049b1f170f6edfa5bbcafb109298fb116",
            "filename": "src/transformers/models/evolla/modeling_evolla.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -1439,7 +1439,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "c2790a2ca93f61c8e8d11865b1bb974dbc7f219d",
            "filename": "src/transformers/models/evolla/modular_evolla.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -872,7 +872,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "3f315bc464b0ff0a96bfd51129e2705e24dee1fa",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -764,7 +764,7 @@ def forward(\n             raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         # Compute alibi tensor: check build_alibi_tensor documentation\n         alibi = None"
        },
        {
            "sha": "1dadc6f5377b4007c78f44d81d7a39acf2dedbdb",
            "filename": "src/transformers/models/flaubert/modeling_flaubert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 8,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"PyTorch Flaubert model, based on XLM.\"\"\"\n \n-import itertools\n import math\n from dataclasses import dataclass\n from typing import Callable, Optional, Union\n@@ -81,11 +80,9 @@ def get_masks(slen, lengths, causal, padding_mask=None):\n \n # Copied from transformers.models.xlm.modeling_xlm.MultiHeadAttention\n class MultiHeadAttention(nn.Module):\n-    NEW_ID = itertools.count()\n-\n-    def __init__(self, n_heads, dim, config):\n+    def __init__(self, n_heads, dim, config, layer_idx: int = 0):\n         super().__init__()\n-        self.layer_id = next(MultiHeadAttention.NEW_ID)\n+        self.layer_id = layer_idx\n         self.dim = dim\n         self.n_heads = n_heads\n         self.head_dim = dim // n_heads\n@@ -766,8 +763,8 @@ def __init__(self, config):  # , dico, is_encoder, with_output):\n         #     self.layer_norm15 = nn.ModuleList()\n         #     self.encoder_attn = nn.ModuleList()\n \n-        for _ in range(self.n_layers):\n-            self.attentions.append(MultiHeadAttention(self.n_heads, self.dim, config=config))\n+        for i in range(self.n_layers):\n+            self.attentions.append(MultiHeadAttention(self.n_heads, self.dim, config=config, layer_idx=i))\n             self.layer_norm1.append(nn.LayerNorm(self.dim, eps=config.layer_norm_eps))\n             # if self.is_decoder:\n             #     self.layer_norm15.append(nn.LayerNorm(self.dim, eps=config.layer_norm_eps))\n@@ -859,7 +856,7 @@ def forward(\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n         if cache is None:\n-            cache = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            cache = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n         if isinstance(cache, tuple):\n             cache = EncoderDecoderCache.from_legacy_cache(cache)"
        },
        {
            "sha": "bb7e1e15be338038fba799c0b03217d51c5c86a6",
            "filename": "src/transformers/models/fsmt/modeling_fsmt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -650,7 +650,7 @@ def forward(\n \n         # initialize `past_key_values`\n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \""
        },
        {
            "sha": "5f72f27d9382aee69bc12d4c7b5b82e93d616dcd",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -361,7 +361,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "281fcd54fb7d526f1772ea81a425a154612dd824",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -385,7 +385,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "3c21143a32056b2b00fb3e47f7a599232e1349c8",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -1608,7 +1608,7 @@ def forward(\n         per_layer_inputs = self.project_per_layer_inputs(inputs_embeds, per_layer_inputs)\n \n         if use_cache and past_key_values is None and not self.training:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "b6629053f118da65fd0ecee84d274d972d311a45",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -2057,7 +2057,7 @@ def forward(\n         per_layer_inputs = self.project_per_layer_inputs(inputs_embeds, per_layer_inputs)\n \n         if use_cache and past_key_values is None and not self.training:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "2b69cf07a0467adbdbd016aa14a094dc0ed046a1",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -410,7 +410,7 @@ def forward(\n             raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None"
        },
        {
            "sha": "a8510d9dcb2588654baf153e4dbbb4746df658ab",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -838,7 +838,7 @@ def forward(\n \n         # torch.jit.trace() doesn't support cache objects in the output\n         if use_cache and past_key_values is None and not torch.jit.is_tracing():\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)"
        },
        {
            "sha": "3b144a1cdefa46eb1b4c0edddd6604315fd37b89",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -873,7 +873,7 @@ def forward(\n \n         # torch.jit.trace() doesn't support cache objects in the output\n         if use_cache and past_key_values is None and not torch.jit.is_tracing():\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)"
        },
        {
            "sha": "286d3d30cbe050ef6e5e83a18c67956f9e8a6cbb",
            "filename": "src/transformers/models/glm4v_moe/modeling_glm4v_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -954,7 +954,7 @@ def forward(\n \n         # torch.jit.trace() doesn't support cache objects in the output\n         if use_cache and past_key_values is None and not torch.jit.is_tracing():\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)"
        },
        {
            "sha": "375479f19780a9cef59eb6f0cb9e764c4ded1814",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -841,7 +841,7 @@ def forward(\n         # based on pattern from src/transformers/models/whisper/modeling_whisper.py::WhisperDecoder\n         if use_cache:\n             if past_key_values is None:\n-                past_key_values = DynamicCache()\n+                past_key_values = DynamicCache(config=self.config)\n             elif isinstance(past_key_values, tuple):\n                 logger.warning_once(\n                     \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.53.0. \"\n@@ -851,7 +851,7 @@ def forward(\n                 past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n \n             if self.config.add_cross_attention and not isinstance(past_key_values, EncoderDecoderCache):\n-                past_key_values = EncoderDecoderCache(past_key_values, DynamicCache())\n+                past_key_values = EncoderDecoderCache(past_key_values, DynamicCache(config=self.config))\n \n         if inputs_embeds is None:\n             inputs_embeds = self.wte(input_ids)"
        },
        {
            "sha": "3f901354a8636f1d48ad42e6e0192f0cc51b74c9",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -488,7 +488,7 @@ def forward(\n             raise ValueError(\"batch_size has to be defined and > 0\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \""
        },
        {
            "sha": "9622620ca37635f2d646f730da5a3adda8bd0f9f",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -576,7 +576,7 @@ def forward(\n             raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         seq_length = inputs_embeds.shape[1]\n         if cache_position is None:"
        },
        {
            "sha": "4125946e120663239c80a24e694ab1277a08b146",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -433,7 +433,7 @@ def forward(\n             raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "08e8f182f9d019271e6d5d932ad750ead6e029a1",
            "filename": "src/transformers/models/gpt_neox/modular_gpt_neox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -311,7 +311,7 @@ def forward(\n             raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "108a68ef70d1d0dfe4de0bdcd95d8807d4f6f27d",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -455,7 +455,7 @@ def forward(\n             raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         seq_length = inputs_embeds.shape[1]\n         if cache_position is None:"
        },
        {
            "sha": "73666daed0ad45619df301f190720b3545f4e4d8",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -661,7 +661,7 @@ def forward(\n             raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         seq_length = inputs_embeds.shape[1]\n         if cache_position is None:"
        },
        {
            "sha": "dc458b619def11d7b90cb8bdcacb87183f49b768",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -411,7 +411,7 @@ def forward(\n         inputs_embeds = inputs_embeds * self.embedding_multiplier  # main diff with Llama\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "a0141bd91245cb2c3681eee258b1270390ee9b69",
            "filename": "src/transformers/models/granite/modular_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -165,7 +165,7 @@ def forward(\n         inputs_embeds = inputs_embeds * self.embedding_multiplier  # main diff with Llama\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "7f864395ccb67a005e17c1670bd66bd97697d76d",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -678,7 +678,7 @@ def forward(\n             raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "9bbb7b768a101ab2aabab40e9ee3ccb1321a15d9",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -654,7 +654,7 @@ def forward(\n             raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "00d449b7aaa124f334afac049420138549ee598e",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -1053,7 +1053,7 @@ def forward(\n             raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         batch_size, seq_length, _ = inputs_embeds.shape\n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "293c0c8df6c72c64d125188e48795b684734511c",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -1063,7 +1063,7 @@ def forward(\n             raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if inputs_embeds is None:\n             inputs_embeds = self.text_model.get_input_embeddings()(input_ids)"
        },
        {
            "sha": "5b9b751865fffd074ade0083747b2ba1c38230cc",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -783,7 +783,7 @@ def forward(\n             raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if inputs_embeds is None:\n             inputs_embeds = self.text_model.get_input_embeddings()(input_ids).to(self.device)"
        },
        {
            "sha": "23fe9b1e194d8bbcbea0324fb0e4a53310849d87",
            "filename": "src/transformers/models/imagegpt/modeling_imagegpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -647,7 +647,7 @@ def forward(\n                 use_cache = False\n \n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \""
        },
        {
            "sha": "7e94a5305fbb4631aca03ffd7532dd6fcb3eb94e",
            "filename": "src/transformers/models/informer/modeling_informer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -1217,7 +1217,7 @@ def forward(\n         input_shape = inputs_embeds.size()[:-1]\n         # initialize `past_key_values`\n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \""
        },
        {
            "sha": "64a93b71c9cda3711b9f7190fcab98fbd18c8dcd",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -926,7 +926,7 @@ def forward(\n             raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "6739022a3977cea7f4d12f30f50f97357f7df85a",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -1052,9 +1052,9 @@ def forward(\n \n         if use_cache and past_key_values is None:\n             past_key_values = (\n-                EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n                 if encoder_hidden_states is not None\n-                else DynamicCache()\n+                else DynamicCache(config=self.config)\n             )\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once("
        },
        {
            "sha": "27e692273c7168a81da3a9b0ef008513c56700f6",
            "filename": "src/transformers/models/kosmos2_5/modeling_kosmos2_5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -1131,7 +1131,7 @@ def forward(\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "641eec0634d8644ccb6d616de101a30ba48a05af",
            "filename": "src/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -873,7 +873,7 @@ def forward(\n             raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None"
        },
        {
            "sha": "d9944e53f5d2c340e1a3b3f2b5283b2d4ddb9e3c",
            "filename": "src/transformers/models/led/modeling_led.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -1761,7 +1761,7 @@ def forward(\n                 use_cache = False\n \n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \""
        },
        {
            "sha": "fa4f854f3c9bc78721a131cbd4b3e506ceb5e79d",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -1429,9 +1429,11 @@ def forward(\n         if self.is_decoder:\n             if use_cache and past_key_values is None:\n                 if self.config.is_encoder_decoder:\n-                    past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                    past_key_values = EncoderDecoderCache(\n+                        DynamicCache(config=self.config), DynamicCache(config=self.config)\n+                    )\n                 else:\n-                    past_key_values = DynamicCache()\n+                    past_key_values = DynamicCache(config=self.config)\n         elif not self.is_decoder:\n             # do not pass cache object down the line for encoder stack\n             # it messes indexing later in decoder-stack because cache object is modified in-place"
        },
        {
            "sha": "23b3e826b006f0c70515ac8c92098953f784a875",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -1056,7 +1056,7 @@ def forward(\n \n         # initialize `past_key_values`\n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \""
        },
        {
            "sha": "beedb078c3144e6c1331cf9aefdee267299ab14c",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -997,9 +997,9 @@ def forward(\n         # initialize `past_key_values`\n         if use_cache and past_key_values is None:\n             past_key_values = (\n-                EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n                 if encoder_hidden_states is not None\n-                else DynamicCache()\n+                else DynamicCache(config=self.config)\n             )\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once("
        },
        {
            "sha": "aa79d950d20005521268c563f9d39dc5afb468ad",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -1040,9 +1040,9 @@ def forward(\n         # initialize `past_key_values`\n         if use_cache and past_key_values is None:\n             past_key_values = (\n-                EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n                 if encoder_hidden_states is not None\n-                else DynamicCache()\n+                else DynamicCache(config=self.config)\n             )\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once("
        },
        {
            "sha": "92ed01e73ebd4fad41db40356ce85dcfada6c1b4",
            "filename": "src/transformers/models/megatron_bert/modeling_megatron_bert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -510,7 +510,7 @@ def forward(\n                 )\n                 use_cache = False\n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \""
        },
        {
            "sha": "d4b4ea4d0c4d90ae4326228da049eb5ecdf35fde",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -1103,7 +1103,7 @@ def forward(\n             raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "2125af6c12d22204f456264bbf6c4f5e2cf1238f",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -1229,7 +1229,7 @@ def forward(\n         hidden_states = inputs_embeds\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "f2225663315db286ecd6d16a21a773387ea5804e",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -636,7 +636,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "1428c6f044e7be6790757da200e43c9256152aeb",
            "filename": "src/transformers/models/moonshine/modular_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -658,7 +658,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "df254675450d60d7a9663650068aeae2ba207cc8",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -1280,7 +1280,7 @@ def forward(\n             raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None"
        },
        {
            "sha": "8a9dafa564fcc0a46b5c0588c3393d956166464a",
            "filename": "src/transformers/models/mpt/modeling_mpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -355,7 +355,7 @@ def forward(\n             inputs_embeds = self.wte(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \""
        },
        {
            "sha": "0febc1685b9250ee44babb5f305485b20dec1ecc",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -997,9 +997,11 @@ def forward(\n         if self.is_decoder:\n             if use_cache and past_key_values is None:\n                 if self.config.is_encoder_decoder:\n-                    past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                    past_key_values = EncoderDecoderCache(\n+                        DynamicCache(config=self.config), DynamicCache(config=self.config)\n+                    )\n                 else:\n-                    past_key_values = DynamicCache()\n+                    past_key_values = DynamicCache(config=self.config)\n         elif not self.is_decoder:\n             # do not pass cache object down the line for encoder stack\n             # it messes indexing later in decoder-stack because cache object is modified in-place"
        },
        {
            "sha": "07b25984d5b0fc90585c5d8feb3970e50fb01e6a",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -560,7 +560,7 @@ def forward(\n                 use_cache = False\n \n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \""
        },
        {
            "sha": "1b007a4e1c03f877dad23e693707c400a723b18f",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -522,7 +522,7 @@ def forward(\n                 use_cache = False\n \n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \""
        },
        {
            "sha": "9659dda6ae31c553c0f4adcf2f83f7c91d71e29e",
            "filename": "src/transformers/models/mvp/modeling_mvp.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -857,9 +857,9 @@ def forward(\n \n         if use_cache and past_key_values is None:\n             past_key_values = (\n-                EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n                 if encoder_hidden_states is not None\n-                else DynamicCache()\n+                else DynamicCache(config=self.config)\n             )\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once("
        },
        {
            "sha": "55a33f9ffba5931dff1ede00319e50b0d4797a1c",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -666,13 +666,13 @@ def forward(\n             use_cache = False\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "7a20e3142cbbb26b419a98fdd6ba7ef395c43164",
            "filename": "src/transformers/models/nllb_moe/modeling_nllb_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -1234,7 +1234,7 @@ def forward(\n \n         # initialize `past_key_values`\n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \""
        },
        {
            "sha": "0b427f9bef19e2e5b8c382774209f3533ae79c13",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -793,7 +793,7 @@ def forward(\n             raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "92a395ec5438ed649760292dae0d77ed9e9e80e9",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -599,7 +599,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n         if cache_position is None:"
        },
        {
            "sha": "c51b856611202a6e0e8d2b084d467c485daafc56",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -1049,9 +1049,9 @@ def forward(\n         # initialize `past_key_values`\n         if use_cache and past_key_values is None:\n             past_key_values = (\n-                EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n                 if encoder_hidden_states is not None\n-                else DynamicCache()\n+                else DynamicCache(config=self.config)\n             )\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once("
        },
        {
            "sha": "c2049f8317f7eaef18b933ff6b502a49e0932c05",
            "filename": "src/transformers/models/pegasus_x/modeling_pegasus_x.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -1306,7 +1306,7 @@ def forward(\n \n         # initialize `past_key_values`\n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \""
        },
        {
            "sha": "e6ac1d8311fcfa9d7a4b61874706c65564392361",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -477,7 +477,7 @@ def forward(\n             raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)"
        },
        {
            "sha": "b9b719425229b4273083f5d173fc9b1543adb889",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -364,7 +364,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "d17501ed24ddcbb3e83045c0a45393e7a743657d",
            "filename": "src/transformers/models/phi/modular_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -213,7 +213,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "d780dc5348798db78574502544c647821e6c1827",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -1127,9 +1127,11 @@ def forward(\n \n         if use_cache and past_key_values is None:\n             if self.config.is_encoder_decoder:\n-                past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                past_key_values = EncoderDecoderCache(\n+                    DynamicCache(config=self.config), DynamicCache(config=self.config)\n+                )\n             else:\n-                past_key_values = DynamicCache()\n+                past_key_values = DynamicCache(config=self.config)\n \n         past_key_values_length = 0\n         if cache_position is not None:"
        },
        {
            "sha": "66c31ca3905b4b337ef2490945f3c77f14edb0fb",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -960,9 +960,9 @@ def forward(\n         # initialize `past_key_values`\n         if use_cache and past_key_values is None:\n             past_key_values = (\n-                EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n                 if encoder_hidden_states is not None\n-                else DynamicCache()\n+                else DynamicCache(config=self.config)\n             )\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once("
        },
        {
            "sha": "90fa7275e8343bbf7240f1b0d3f7908d1d0e80f9",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -745,9 +745,11 @@ def forward(\n         if self.is_decoder:\n             if use_cache and past_key_values is None:\n                 if self.config.is_encoder_decoder:\n-                    past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                    past_key_values = EncoderDecoderCache(\n+                        DynamicCache(config=self.config), DynamicCache(config=self.config)\n+                    )\n                 else:\n-                    past_key_values = DynamicCache()\n+                    past_key_values = DynamicCache(config=self.config)\n         elif not self.is_decoder:\n             # do not pass cache object down the line for encoder stack\n             # it messes indexing later in decoder-stack because cache object is modified in-place"
        },
        {
            "sha": "8515343101ffd432e0fed16bb3886222ff70ed0e",
            "filename": "src/transformers/models/prophetnet/modeling_prophetnet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -1240,9 +1240,9 @@ def forward(\n \n         if use_cache and past_key_values is None:\n             past_key_values = (\n-                EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n                 if encoder_hidden_states is not None\n-                else DynamicCache()\n+                else DynamicCache(config=self.config)\n             )\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once("
        },
        {
            "sha": "899b527a7e08cb1704c45061f12e721dfcb95069",
            "filename": "src/transformers/models/rembert/modeling_rembert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -506,7 +506,7 @@ def forward(\n                 use_cache = False\n \n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \""
        },
        {
            "sha": "2df2c6bc72b7f4ca19d9c3b0cf0b4f9f3095b8e6",
            "filename": "src/transformers/models/roberta/modeling_roberta.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -586,7 +586,7 @@ def forward(\n                 use_cache = False\n \n         if use_cache and self.config.is_decoder and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n         if use_cache and self.config.is_decoder and isinstance(past_key_values, tuple):\n             logger.warning_once("
        },
        {
            "sha": "33aba51f217719f01e577ef85707be7ec7ba576f",
            "filename": "src/transformers/models/roberta_prelayernorm/modeling_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -477,7 +477,7 @@ def forward(\n                 use_cache = False\n \n         if use_cache and self.config.is_decoder and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n         if use_cache and self.config.is_decoder and isinstance(past_key_values, tuple):\n             logger.warning_once("
        },
        {
            "sha": "970057ed10f250729d7c034d39ca53f41e91bf72",
            "filename": "src/transformers/models/roc_bert/modeling_roc_bert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -596,7 +596,7 @@ def forward(\n                 use_cache = False\n \n         if use_cache and self.config.is_decoder and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n         if use_cache and self.config.is_decoder and isinstance(past_key_values, tuple):\n             logger.warning_once("
        },
        {
            "sha": "d878d89058f345aa60788d5986753f9424b1787c",
            "filename": "src/transformers/models/roformer/modeling_roformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -541,7 +541,7 @@ def forward(\n                 use_cache = False\n \n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \""
        },
        {
            "sha": "718ce3837485dfa9b4e8ba6ba7c35cea586022ae",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -1801,7 +1801,7 @@ def forward(\n \n         # initialize `past_key_values`\n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \""
        },
        {
            "sha": "859b91f688fa269a9267c216775becbee59ae5e1",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -1844,7 +1844,7 @@ def forward(\n \n         # initialize `past_key_values`\n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \""
        },
        {
            "sha": "cde441cc672fdb64cd46e91ba014264f770db2da",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -754,7 +754,7 @@ def forward(\n             raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if inputs_embeds is None:\n             inputs_embeds = self.text_model.get_input_embeddings()(input_ids).to(input_ids.device)"
        },
        {
            "sha": "67bea742532a2827f40e699182a284cd055a005d",
            "filename": "src/transformers/models/smolvlm/modular_smolvlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -293,7 +293,7 @@ def forward(\n             raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if inputs_embeds is None:\n             inputs_embeds = self.text_model.get_input_embeddings()(input_ids).to(input_ids.device)"
        },
        {
            "sha": "cb801b9f4f24866b76912a90f7d0726d6b00d01d",
            "filename": "src/transformers/models/speech_to_text/modeling_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -874,7 +874,7 @@ def forward(\n                 use_cache = False\n \n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \""
        },
        {
            "sha": "b9502dd6ea0713bd2f7aa7866f0078156f78e952",
            "filename": "src/transformers/models/speecht5/modeling_speecht5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -1583,7 +1583,7 @@ def forward(\n                 use_cache = False\n \n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \""
        },
        {
            "sha": "69cf41bcfdd8e9f0990c48e58df4cf1f89d59157",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -705,7 +705,7 @@ def forward(\n             raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n \n         if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache()\n+            past_key_values = DynamicCache(config=self.config)\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)"
        },
        {
            "sha": "2bf97ba8317ad34da33945f1a14db4959584a7e5",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -953,9 +953,11 @@ def forward(\n         if self.is_decoder:\n             if use_cache and past_key_values is None:\n                 if self.config.is_encoder_decoder:\n-                    past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                    past_key_values = EncoderDecoderCache(\n+                        DynamicCache(config=self.config), DynamicCache(config=self.config)\n+                    )\n                 else:\n-                    past_key_values = DynamicCache()\n+                    past_key_values = DynamicCache(config=self.config)\n         elif not self.is_decoder:\n             # do not pass cache object down the line for encoder stack\n             # it messes indexing later in decoder-stack because cache object is modified in-place"
        },
        {
            "sha": "35290bad16c639306131d6a75fe323633d9fd49b",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -1010,9 +1010,11 @@ def forward(\n         if self.is_decoder:\n             if use_cache and past_key_values is None:\n                 if self.config.is_encoder_decoder:\n-                    past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                    past_key_values = EncoderDecoderCache(\n+                        DynamicCache(config=self.config), DynamicCache(config=self.config)\n+                    )\n                 else:\n-                    past_key_values = DynamicCache()\n+                    past_key_values = DynamicCache(config=self.config)\n         elif not self.is_decoder:\n             # do not pass cache object down the line for encoder stack\n             # it messes indexing later in decoder-stack because cache object is modified in-place"
        },
        {
            "sha": "903a2c56b1babd361783f0f1ff70f18267ce3186",
            "filename": "src/transformers/models/tapas/modeling_tapas.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -578,7 +578,7 @@ def forward(\n         cache_position=None,\n     ):\n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \""
        },
        {
            "sha": "f46b2611d1b8b11dcce7ec024729c179aebe3f87",
            "filename": "src/transformers/models/time_series_transformer/modeling_time_series_transformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -994,7 +994,7 @@ def forward(\n         input_shape = inputs_embeds.size()[:-1]\n         # initialize `past_key_values`\n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \""
        },
        {
            "sha": "298c3fd2cb62433122e6515a09fe9cd8a15c2277",
            "filename": "src/transformers/models/trocr/modeling_trocr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -584,9 +584,9 @@ def forward(\n \n         if use_cache and past_key_values is None:\n             past_key_values = (\n-                EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n                 if encoder_hidden_states is not None\n-                else DynamicCache()\n+                else DynamicCache(config=self.config)\n             )\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once("
        },
        {
            "sha": "d5c9904c3db357a5657b7b8995a0b6aa6cb756f4",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -1230,9 +1230,11 @@ def forward(\n         if self.is_decoder:\n             if use_cache and past_key_values is None:\n                 if self.config.is_encoder_decoder:\n-                    past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                    past_key_values = EncoderDecoderCache(\n+                        DynamicCache(config=self.config), DynamicCache(config=self.config)\n+                    )\n                 else:\n-                    past_key_values = DynamicCache()\n+                    past_key_values = DynamicCache(config=self.config)\n         elif not self.is_decoder:\n             # do not pass cache object down the line for encoder stack\n             # it messes indexing later in decoder-stack because cache object is modified in-place"
        },
        {
            "sha": "73bedc363ed84f3b36172f0e7a6e445b352ec6d0",
            "filename": "src/transformers/models/umt5/modeling_umt5.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -697,9 +697,11 @@ def forward(\n         if self.is_decoder:\n             if use_cache and past_key_values is None:\n                 if self.config.is_encoder_decoder:\n-                    past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                    past_key_values = EncoderDecoderCache(\n+                        DynamicCache(config=self.config), DynamicCache(config=self.config)\n+                    )\n                 else:\n-                    past_key_values = DynamicCache()\n+                    past_key_values = DynamicCache(config=self.config)\n         elif not self.is_decoder:\n             # do not pass cache object down the line for encoder stack\n             # it messes indexing later in decoder-stack because cache object is modified in-place"
        },
        {
            "sha": "10dd77e3c415c4fa55b1d5c74bbda633a43d0d71",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -869,9 +869,11 @@ def forward(\n \n         if use_cache and past_key_values is None:\n             if self.config.is_encoder_decoder:\n-                past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                past_key_values = EncoderDecoderCache(\n+                    DynamicCache(config=self.config), DynamicCache(config=self.config)\n+                )\n             else:\n-                past_key_values = DynamicCache()\n+                past_key_values = DynamicCache(config=self.config)\n \n         past_key_values_length = 0\n         if cache_position is not None:"
        },
        {
            "sha": "244135b7d2e50565b4ab84b05a30bcd0fbc8da66",
            "filename": "src/transformers/models/xglm/modeling_xglm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -496,9 +496,9 @@ def forward(\n         # initialize `past_key_values`\n         if use_cache and past_key_values is None:\n             past_key_values = (\n-                EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n                 if encoder_hidden_states is not None\n-                else DynamicCache()\n+                else DynamicCache(config=self.config)\n             )\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once("
        },
        {
            "sha": "4e7316fb781bfdf5b93a06c97c3577719e5da54d",
            "filename": "src/transformers/models/xlm/modeling_xlm.py",
            "status": "modified",
            "additions": 5,
            "deletions": 8,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -16,7 +16,6 @@\n PyTorch XLM model.\n \"\"\"\n \n-import itertools\n import math\n from dataclasses import dataclass\n from typing import Callable, Optional, Union\n@@ -495,11 +494,9 @@ def forward(\n \n \n class MultiHeadAttention(nn.Module):\n-    NEW_ID = itertools.count()\n-\n-    def __init__(self, n_heads, dim, config):\n+    def __init__(self, n_heads, dim, config, layer_idx: int = 0):\n         super().__init__()\n-        self.layer_id = next(MultiHeadAttention.NEW_ID)\n+        self.layer_id = layer_idx\n         self.dim = dim\n         self.n_heads = n_heads\n         self.head_dim = dim // n_heads\n@@ -743,8 +740,8 @@ def __init__(self, config):\n         #     self.layer_norm15 = nn.ModuleList()\n         #     self.encoder_attn = nn.ModuleList()\n \n-        for _ in range(self.n_layers):\n-            self.attentions.append(MultiHeadAttention(self.n_heads, self.dim, config=config))\n+        for i in range(self.n_layers):\n+            self.attentions.append(MultiHeadAttention(self.n_heads, self.dim, config=config, layer_idx=i))\n             self.layer_norm1.append(nn.LayerNorm(self.dim, eps=config.layer_norm_eps))\n             # if self.is_decoder:\n             #     self.layer_norm15.append(nn.LayerNorm(self.dim, eps=config.layer_norm_eps))\n@@ -828,7 +825,7 @@ def forward(\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n         if cache is None:\n-            cache = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            cache = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n         if isinstance(cache, tuple):\n             cache = EncoderDecoderCache.from_legacy_cache(cache)"
        },
        {
            "sha": "facfd3a6003914d2cef9a4606067131631392871",
            "filename": "src/transformers/models/xlm_roberta/modeling_xlm_roberta.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -587,7 +587,7 @@ def forward(\n                 use_cache = False\n \n         if use_cache and self.config.is_decoder and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n         if use_cache and self.config.is_decoder and isinstance(past_key_values, tuple):\n             logger.warning_once("
        },
        {
            "sha": "4bbe7591f779293eafba4d650778f72037d06ee6",
            "filename": "src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -576,7 +576,7 @@ def forward(\n                 use_cache = False\n \n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \""
        },
        {
            "sha": "7ddf15bdb5963a3e5631bd02256704e7d48cba3e",
            "filename": "src/transformers/models/xmod/modeling_xmod.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -535,7 +535,7 @@ def forward(\n                 use_cache = False\n \n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \""
        },
        {
            "sha": "13d4df9035b80fb11de2da1651f9f326598c56d9",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -1850,7 +1850,7 @@ def test_generate_with_quant_cache(self):\n \n             # passing past key values of different type should raise Error\n             with self.assertRaises(ValueError):\n-                model.generate(past_key_valyes=DynamicCache(), **generation_kwargs, **inputs_dict)\n+                model.generate(past_key_valyes=DynamicCache(config=model.config), **generation_kwargs, **inputs_dict)\n \n             # setting incorrect cache_config args should raise an Error, i.e. nbits=60 does not make sense\n             generation_kwargs[\"cache_config\"] = {\"nbits\": 60, \"q_group_size\": 8, \"residual_length\": 128}\n@@ -3918,7 +3918,7 @@ def test_prepare_inputs_for_generation_decoder_llm(self):\n         # 5. When we pass a cache, we discard data related to already seen tokens in some tensors. We are now also\n         # forced to pass a correctly prepared `cache_positions` to slice the data accordingly.\n         init_input_ids = input_ids[:, :2]\n-        dynamic_cache = DynamicCache()\n+        dynamic_cache = DynamicCache(config=config)\n         dynamic_cache = model(init_input_ids, past_key_values=dynamic_cache).past_key_values\n         with self.assertRaises(AttributeError):  # past_key_values + no cache_position -> exception\n             model_inputs = model.prepare_inputs_for_generation(input_ids, past_key_values=dynamic_cache)\n@@ -3951,7 +3951,7 @@ def test_prepare_inputs_for_generation_decoder_llm(self):\n         # a) must use the cache b) must expect `input_ids` after the prompt is processed\n         init_inputs_embeds = model.get_input_embeddings()(init_input_ids)\n         init_cache_positions = torch.arange(init_input_ids.shape[-1], dtype=torch.long).to(torch_device)\n-        empty_cache = DynamicCache()\n+        empty_cache = DynamicCache(config=config)\n \n         # Prompt processing\n         model_inputs = model.prepare_inputs_for_generation("
        },
        {
            "sha": "ae37e2432ddb3cb756bef152b90fc9cf65e6d269",
            "filename": "tests/models/gpt2/test_modeling_gpt2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -443,7 +443,7 @@ def create_and_check_cached_forward_with_and_without_attention_mask(self, config\n         # Cached forward once with the attention mask provided and the other time without it (which should assume full attention)\n         cache_outputs = model(**cache_inputs)\n         # Caches are mutable (unlike legacy tuples), so we need to copy them before using multiple times\n-        pkv_copy = DynamicCache()\n+        pkv_copy = DynamicCache(config=config)\n         pkv_copy.update(\n             cache_outputs.past_key_values.layers[0].keys, cache_outputs.past_key_values.layers[0].values, 0\n         )"
        },
        {
            "sha": "bacb98308b60d32bf50c611540234470611ee435",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -1585,7 +1585,7 @@ def _create_and_check_torch_fx_tracing(self, config, inputs_dict, output_loss=Fa\n                         head_dim = model.config.hidden_size // model.config.num_attention_heads\n \n                         cache_shape = (batch_size, num_heads, 0, head_dim)\n-                        empty_pkv = DynamicCache()\n+                        empty_pkv = DynamicCache(config=model.config)\n \n                         cache_length = 9\n                         cache_shape = (batch_size, num_heads, cache_length, head_dim)"
        },
        {
            "sha": "50cec84151bde899d8d4641985919ebeb3440eec",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/becab2c6014a99f93252fccc33122b946f93b62e/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/becab2c6014a99f93252fccc33122b946f93b62e/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=becab2c6014a99f93252fccc33122b946f93b62e",
            "patch": "@@ -606,7 +606,7 @@ def test_dynamic_cache_exportability(self):\n         res = ep.module()(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n-            past_key_values=DynamicCache(),\n+            past_key_values=DynamicCache(config=model.config),\n             use_cache=True,\n         )\n         self.assertTrue(len(res.past_key_values) == model.config.num_hidden_layers)\n@@ -622,7 +622,7 @@ def test_dynamic_cache_exportability(self):\n             ),\n         )\n \n-        past_key_values_eager = DynamicCache()\n+        past_key_values_eager = DynamicCache(config=model.config)\n         res_eager = model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n@@ -654,7 +654,7 @@ def test_dynamic_cache_exportability_multiple_run(self):\n         res = ep.module()(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n-            past_key_values=DynamicCache(),\n+            past_key_values=DynamicCache(config=model.config),\n             use_cache=True,\n         )\n         self.assertTrue(len(res.past_key_values) == model.config.num_hidden_layers)\n@@ -673,7 +673,7 @@ def test_dynamic_cache_exportability_multiple_run(self):\n         res_eager = model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n-            past_key_values=DynamicCache(),\n+            past_key_values=DynamicCache(config=model.config),\n             use_cache=True,\n         )\n         past_key_values_eager = res_eager.past_key_values"
        }
    ],
    "stats": {
        "total": 376,
        "additions": 195,
        "deletions": 181
    }
}