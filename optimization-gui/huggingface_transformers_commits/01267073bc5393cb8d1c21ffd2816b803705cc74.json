{
    "author": "SunMarc",
    "message": "small cleaning of quantization class  (#42633)\n\n* small cleaning\n\n* fix\n\n* Apply suggestions from code review\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "01267073bc5393cb8d1c21ffd2816b803705cc74",
    "files": [
        {
            "sha": "4481bc7b62250a2b7a370c1c5aaefee1e25f052a",
            "filename": "docs/source/en/quantization/contribute.md",
            "status": "modified",
            "additions": 11,
            "deletions": 7,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/01267073bc5393cb8d1c21ffd2816b803705cc74/docs%2Fsource%2Fen%2Fquantization%2Fcontribute.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/01267073bc5393cb8d1c21ffd2816b803705cc74/docs%2Fsource%2Fen%2Fquantization%2Fcontribute.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fcontribute.md?ref=01267073bc5393cb8d1c21ffd2816b803705cc74",
            "patch": "@@ -46,26 +46,30 @@ Some quantization methods may require \"pre-quantizing\" the model through data ca\n \n ## Create new HFQuantizer class\n \n+0. The best starting point would be to have a look at another quantization method such as Finegrained Fp8. You will have to update or create three files in total: the [config file](https://github.com/huggingface/transformers/blob/main/src/transformers/utils/quantization_config.py), the [integration file](https://github.com/huggingface/transformers/blob/main/src/transformers/integrations/finegrained_fp8.py) and the [quantizer file](https://github.com/huggingface/transformers/blob/main/src/transformers/quantizers/quantizer_finegrained_fp8.py).\n+\n 1. Create a new quantization config class inside [src/transformers/utils/quantization_config.py](https://github.com/huggingface/transformers/blob/abbffc4525566a48a9733639797c812301218b83/src/transformers/utils/quantization_config.py). Add the new quantization config to the [_import_structure](https://github.com/huggingface/transformers/blob/abbffc4525566a48a9733639797c812301218b83/src/transformers/__init__.py#L1088) inside Transformers' [src/transformers/__init__.py](https://github.com/huggingface/transformers/blob/abbffc4525566a48a9733639797c812301218b83/src/transformers/__init__.py) file.\n \n 2. Create a new file inside [src/transformers/quantizers/](https://github.com/huggingface/transformers/tree/abbffc4525566a48a9733639797c812301218b83/src/transformers/quantizers) named `quantizer_your_method.py`, and make it inherit from [`~quantizers.HfQuantizer]. Make sure to add the new quantizer and quantization config in the quantization auto-mapping in [src/transformers/quantizers/auto.py](https://github.com/huggingface/transformers/blob/abbffc4525566a48a9733639797c812301218b83/src/transformers/quantizers/auto.py).\n \n-3. Define the following class attributes and property methods for your quantization method.\n+3. Define the following class attributes and property methods for your quantization method:\n \n     - `requires_calibration`: Whether the quantization method requires a data calibration process. If set to `True`, you can only support inference (with quantized weights) and not inference and quantization.\n-    - `required_packages`: A list of strings of the required packages to use the quantized weights. You might need to define some new utility methods such as `is_auto_awq_available` in [transformers/src/utils/import_utils.py](https://github.com/huggingface/transformers/blob/abbffc4525566a48a9733639797c812301218b83/src/transformers/utils/import_utils.py).\n-    - `requires_parameters_quantization`: Only required if your quantization method requires extra attention to the underlying [nn.Parameter](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html) object. For example, bitsandbytes uses [`~bitsandbytes.nn.Params4bit`] and [`~bitsandbytes.nn.Int8Params`], which requires some extra attention when quantizing the model. Most of the recent quantization method packs int2 and int4 weights inside [torch.uint8](https://pytorch.org/docs/stable/tensors.html) weights, so this flag should not be really required (set to `False` by default).\n     - `is_serializable`: A property method to determine whether the method is serializable or not.\n     - `is_trainable`:  A property method to determine whether you can fine-tune models on top of the quantization method (with or without PEFT approaches).\n \n 4. Write the `validate_environment` and `update_dtype` methods. These methods are called before creating the quantized model to ensure users use the right configuration. Refer to other quantizers for an example of it is implemented.\n \n 5. Write the `_process_model_before_weight_loading` method. In Transformers, the quantized models are initialized first on the `\"meta\"` device before loading the weights. This means the `_process_model_before_weight_loading` method takes care of manipulating the model skeleton to replace some modules ([nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)) with the target modules (quantization modules).\n \n-    You can define module replacement logic or any other utility method by creating a new file in [transformers/src/integrations/](https://github.com/huggingface/transformers/tree/abbffc4525566a48a9733639797c812301218b83/src/transformers/integrations) and exposing the relevant methods in that folder's `__init__.py` file. The best starting point would be to have a look at another quantization method such as [quantizer_awq.py](https://github.com/huggingface/transformers/blob/abbffc4525566a48a9733639797c812301218b83/src/transformers/quantizers/quantizer_awq.py).\n+You can define module replacement logic or any other utility method by creating a new file in [transformers/src/integrations/](https://github.com/huggingface/transformers/tree/abbffc4525566a48a9733639797c812301218b83/src/transformers/integrations) and exposing the relevant methods in that folder's `__init__.py` file. \n+\n+6. Add the `get_quantize_ops` method to the quantizer class if the quantization supports quantizing on the fly. In transformers, we materialize each tensor and apply a sequence of different operations on it. In our case, the quantization operation happens at the end. You need to create a `XXXQuantize`,  a subclass of `ConversionOps`, and add a `convert` method. In the `convert` method, you need to quantize the weights and return a dictionary of quantized params.\n+\n+7. Add the `get_weight_conversions` method to the quantizer class if the quantization supports loading pre-quantized weights. In transformers, we can collect multiple tensors and apply operations on them. This is particularly useful when we have tensors in the checkpoint that require to be regrouped to re-create the quantized tensors.\n \n-6. Write the `_process_model_after_weight_loading` method. This method enables implementing additional features that require manipulating the model after loading the weights.\n+8. Write the `_process_model_after_weight_loading` method if needed. This method enables implementing additional features that require manipulating the model after loading the weights.\n \n-7. Document everything! Make sure your quantization method is documented by adding a new file under `docs/source/en/quantization`.\n+9. Document everything! Make sure your quantization method is documented by adding a new file under `docs/source/en/quantization`.\n \n-8. You should add tests by adding the package in our nightly Dockerfile inside `docker/transformers-quantization-latest-gpu` and then adding a new test file in `tests/quantization/xxx`. Feel free to check out existing quantization methods to see how it is implemented.\n+10. You should add tests by adding the package in our nightly Dockerfile inside `docker/transformers-quantization-latest-gpu` and then adding a new test file in `tests/quantization/xxx`. Feel free to check out existing quantization methods to see how it is implemented."
        },
        {
            "sha": "5a04a906b19919b61f5c3a844d23991a7ed6f4ad",
            "filename": "src/transformers/integrations/bitsandbytes.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py?ref=01267073bc5393cb8d1c21ffd2816b803705cc74",
            "patch": "@@ -44,7 +44,7 @@ def convert(\n         we need to store some parameters to create the quantized weight. For example, bnb requires 6 values that are stored in the checkpoint to recover the quantized weight. So we store them in a dict that it stored in hf_quantizer for now as we can't save it in the op since we create an op per tensor.\n         \"\"\"\n         value = list(input_dict.values())[0]\n-        value = value[0] if isinstance(value, list) else value\n+        value = value[0]\n \n         # update param name to get the weights instead of the quantized stats\n         module, _ = get_module_from_name(model, full_layer_name)"
        },
        {
            "sha": "4d7b20cca7dba8374a175c3979cee5dee305647d",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 7,
            "deletions": 61,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=01267073bc5393cb8d1c21ffd2816b803705cc74",
            "patch": "@@ -75,26 +75,14 @@ class HfQuantizer(ABC):\n     Attributes\n         quantization_config (`transformers.utils.quantization_config.QuantizationConfigMixin`):\n             The quantization config that defines the quantization parameters of your model that you want to quantize.\n-        modules_to_not_convert (`list[str]`, *optional*):\n-            The list of module names to not convert when quantizing the model.\n-        required_packages (`list[str]`, *optional*):\n-            The list of required pip packages to install prior to using the quantizer\n         requires_calibration (`bool`):\n             Whether the quantization method requires to calibrate the model before using it.\n-        requires_parameters_quantization (`bool`):\n-            Whether the quantization method requires to create a new Parameter. For example, for bitsandbytes, it is\n-            required to create a new xxxParameter in order to properly quantize the model.\n     \"\"\"\n \n     requires_calibration = False\n-    required_packages = None\n-    requires_parameters_quantization = False\n \n     def __init__(self, quantization_config: QuantizationConfigMixin, **kwargs):\n         self.quantization_config = quantization_config\n-\n-        # -- Handle extra kwargs below --\n-        self.modules_to_not_convert = kwargs.pop(\"modules_to_not_convert\", [])\n         self.pre_quantized = kwargs.pop(\"pre_quantized\", True)\n \n         if not self.pre_quantized and self.requires_calibration:\n@@ -157,53 +145,16 @@ def param_element_size(self, model: \"PreTrainedModel\", param_name: str, param: \"\n                 return mapping[custom_dtype]\n         return param.element_size()\n \n-    def update_missing_keys(self, model, missing_keys: list[str], prefix: str) -> list[str]:\n-        \"\"\"\n-        Override this method if you want to adjust the `missing_keys`.\n-\n-        Args:\n-            missing_keys (`list[str]`, *optional*):\n-                The list of missing keys in the checkpoint compared to the state dict of the model\n-        \"\"\"\n-        return missing_keys\n-\n-    def update_expected_keys(self, model, expected_keys: list[str], loaded_keys: list[str]) -> list[str]:\n-        \"\"\"\n-        Override this method if you want to adjust the `update_expected_keys`.\n-\n-        Args:\n-            expected_keys (`list[str]`, *optional*):\n-                The list of the expected keys in the initialized model.\n-            loaded_keys (`list[str]`, *optional*):\n-                The list of the loaded keys in the checkpoint.\n-        \"\"\"\n-        return expected_keys\n-\n-    def update_unexpected_keys(self, model, unexpected_keys: list[str]) -> list[str]:\n-        return unexpected_keys\n-\n     def adjust_max_memory(self, max_memory: dict[str, int | str]) -> dict[str, int | str]:\n         \"\"\"adjust max_memory argument for infer_auto_device_map() if extra memory is needed for quantization\"\"\"\n         return max_memory\n \n     def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n         \"\"\"\n-        Check whether a given param needs quantization as defined by `create_quantized_param`.\n+        Check whether a given param needs to be quantized.\n         \"\"\"\n         return False\n \n-    def create_quantized_param(self, *args, **kwargs):\n-        \"\"\"\n-        Take needed components from state_dict (those from which `param_needs_quantization` is True) and create\n-        quantized param.\n-        It usually also load the new param directly in the `model`.\n-        Note: only applicable if requires_parameters_quantization == True.\n-        \"\"\"\n-        if not self.requires_parameters_quantization:\n-            raise AttributeError(\n-                f\"`.create_quantized_param()` method is not supported by quantizer class {self.__class__.__name__}.\"\n-            )\n-\n     def validate_environment(self, *args, **kwargs):\n         \"\"\"\n         This method is used to potentially check for potential conflicts with arguments that are\n@@ -263,6 +214,11 @@ def postprocess_model(self, model: \"PreTrainedModel\", **kwargs):\n             kwargs (`dict`, *optional*):\n                 The keyword arguments that are passed along `_process_model_after_weight_loading`.\n         \"\"\"\n+        model.config.quantization_config = self.quantization_config\n+\n+        if self.pre_quantized and getattr(self.quantization_config, \"dequantize\", False):\n+            self.remove_quantization_config(model)\n+\n         return self._process_model_after_weight_loading(model, **kwargs)\n \n     def remove_quantization_config(self, model):\n@@ -285,13 +241,7 @@ def dequantize(self, model):\n         Note not all quantization schemes support this.\n         \"\"\"\n         model = self._dequantize(model)\n-\n-        # Delete quantizer and quantization config\n-        del model.hf_quantizer\n-        del model.config.quantization_config\n-        del model.config._pre_quantization_dtype\n-        del model.quantization_method\n-        model.is_quantized = False\n+        self.remove_quantization_config(model)\n \n         return model\n \n@@ -353,10 +303,6 @@ def get_state_dict_and_metadata(self, model, safe_serialization=False):\n         \"\"\"Get state dict and metadata. Useful when we need to modify a bit the state dict due to quantization\"\"\"\n         return None, {}\n \n-    def update_state_dict_with_metadata(self, state_dict, metadata):\n-        \"\"\"Update state dict with metadata. Default behaviour returns state_dict\"\"\"\n-        return state_dict\n-\n     @abstractmethod\n     def is_serializable(self, safe_serialization=None): ...\n "
        },
        {
            "sha": "11f80af80717b37e950c7a23aa9832cbe494228f",
            "filename": "src/transformers/quantizers/quantizer_aqlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_aqlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_aqlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_aqlm.py?ref=01267073bc5393cb8d1c21ffd2816b803705cc74",
            "patch": "@@ -39,12 +39,9 @@ class AqlmHfQuantizer(HfQuantizer):\n     \"\"\"\n \n     requires_calibration = True\n-    required_packages = [\"aqlm\"]\n-    optimum_quantizer = None\n \n     def __init__(self, quantization_config: QuantizationConfigMixin, **kwargs):\n         super().__init__(quantization_config, **kwargs)\n-        self.quantization_config = quantization_config\n \n     def validate_environment(self, *args, **kwargs):\n         if not is_accelerate_available():\n@@ -77,7 +74,6 @@ def _process_model_before_weight_loading(\n             quantization_config=self.quantization_config,\n             linear_weights_not_to_quantize=self.quantization_config.linear_weights_not_to_quantize,\n         )\n-        model.config.quantization_config = self.quantization_config\n \n     @property\n     def is_trainable(self) -> bool:\n@@ -90,5 +86,5 @@ def is_trainable(self) -> bool:\n             )\n             return False\n \n-    def is_serializable(self, safe_serialization=None):\n+    def is_serializable(self, **kwargs):\n         return True"
        },
        {
            "sha": "9b575ce35aeee54968d0de59ce63d0e0e7cbcd23",
            "filename": "src/transformers/quantizers/quantizer_auto_round.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_auto_round.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_auto_round.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_auto_round.py?ref=01267073bc5393cb8d1c21ffd2816b803705cc74",
            "patch": "@@ -36,7 +36,6 @@ class AutoRoundQuantizer(HfQuantizer):\n \n     # AutoRound requires data calibration - we support only inference\n     requires_calibration = True\n-    required_packages = [\"auto_round\"]\n \n     def __init__(self, quantization_config: QuantizationConfigMixin, **kwargs):\n         super().__init__(quantization_config, **kwargs)"
        },
        {
            "sha": "a6affed5ea4ddad8666c148891d9808738f7340e",
            "filename": "src/transformers/quantizers/quantizer_awq.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py?ref=01267073bc5393cb8d1c21ffd2816b803705cc74",
            "patch": "@@ -40,8 +40,6 @@ class AwqQuantizer(HfQuantizer):\n     # AWQ requires data calibration - we support only inference\n     requires_calibration = True\n \n-    required_packages = [\"awq\", \"accelerate\"]\n-\n     def __init__(self, quantization_config, **kwargs):\n         super().__init__(quantization_config, **kwargs)\n "
        },
        {
            "sha": "5761d8fc3c7bb47fa7752bbc53ee38b46411125d",
            "filename": "src/transformers/quantizers/quantizer_bitnet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bitnet.py?ref=01267073bc5393cb8d1c21ffd2816b803705cc74",
            "patch": "@@ -37,14 +37,10 @@ class BitNetHfQuantizer(HfQuantizer):\n     Check out the paper introducing this method: https://huggingface.co/papers/2402.17764\n     \"\"\"\n \n-    requires_parameters_quantization = False\n     requires_calibration = True\n \n-    required_packages = [\"accelerate\"]\n-\n     def __init__(self, quantization_config, **kwargs):\n         super().__init__(quantization_config, **kwargs)\n-        self.quantization_config = quantization_config\n \n     def validate_environment(self, *args, **kwargs):\n         if not is_accelerate_available():\n@@ -62,8 +58,8 @@ def validate_environment(self, *args, **kwargs):\n                 \"You have loaded a BitNet model on CPU and have a CUDA device available, make sure to set \"\n                 \"your model on a GPU device in order to run your model.\"\n             )\n-        elif device_map is not None:\n-            if isinstance(device_map, dict) and (\"cpu\" in device_map.values() or \"disk\" in device_map.values()):\n+        elif isinstance(device_map, dict):\n+            if len(device_map) > 1 and \"cpu\" in device_map.values() or \"disk\" in device_map.values():\n                 raise ValueError(\n                     \"You are attempting to load a BitNet model with a device_map that contains a CPU or disk device.\"\n                     \"This is not supported. Please remove the CPU or disk device from the device_map.\""
        },
        {
            "sha": "dd4a06b8719ff56c33579f7c028a0e819d3dedf7",
            "filename": "src/transformers/quantizers/quantizer_bnb_4bit.py",
            "status": "modified",
            "additions": 12,
            "deletions": 107,
            "changes": 119,
            "blob_url": "https://github.com/huggingface/transformers/blob/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py?ref=01267073bc5393cb8d1c21ffd2816b803705cc74",
            "patch": "@@ -11,7 +11,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from collections import defaultdict\n from typing import TYPE_CHECKING\n \n from .base import HfQuantizer\n@@ -38,34 +37,20 @@\n     import torch\n \n     from ..core_model_loading import WeightConverter\n-    from ..pytorch_utils import Conv1D\n \n logger = logging.get_logger(__name__)\n \n \n class Bnb4BitHfQuantizer(HfQuantizer):\n     \"\"\"\n-    4-bit quantization from bitsandbytes quantization method:\n-        before loading: converts transformer layers into Linear4bit during loading: load 16bit weight and pass to the\n-        layer object after: quantizes individual weights in Linear4bit into 4bit at the first .cuda() call\n-        saving:\n-            from state dict, as usual; saves weights and `quant_state` components\n-        loading:\n-            need to locate `quant_state` components and pass to Param4bit constructor\n+    4-bit quantization from bitsandbytes quantization method\n     \"\"\"\n \n-    use_keep_in_fp32_modules = True\n-    requires_parameters_quantization = True\n     requires_calibration = False\n \n-    required_packages = [\"bitsandbytes\", \"accelerate\"]\n-\n     def __init__(self, quantization_config, **kwargs):\n         super().__init__(quantization_config, **kwargs)\n \n-        if self.quantization_config.llm_int8_skip_modules is not None:\n-            self.modules_to_not_convert = self.quantization_config.llm_int8_skip_modules\n-\n         # This describes the additional items that are saved on the state dict (on the params themselves)\n         self.bnb_keys = [\n             f\"quant_state.bitsandbytes__{self.quantization_config.bnb_4bit_quant_type}\",\n@@ -90,17 +75,9 @@ def validate_environment(self, *args, **kwargs):\n         validate_bnb_backend_availability(raise_exception=True)\n \n         device_map = kwargs.get(\"device_map\")\n-        if (\n-            device_map is not None\n-            and isinstance(device_map, dict)\n-            and not self.quantization_config.llm_int8_enable_fp32_cpu_offload\n-        ):\n-            device_map_without_lm_head = {\n-                key: device_map[key] for key in device_map if key not in self.modules_to_not_convert\n-            }\n-            if set(device_map.values()) == {\"cpu\"}:\n-                pass\n-            elif \"cpu\" in device_map_without_lm_head.values() or \"disk\" in device_map_without_lm_head.values():\n+        if not self.quantization_config.llm_int8_enable_fp32_cpu_offload and isinstance(device_map, dict):\n+            values = set(device_map.values())\n+            if values != {\"cpu\"} and (\"cpu\" in values or \"disk\" in values):\n                 raise ValueError(\n                     \"Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \"\n                     \"quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \"\n@@ -117,13 +94,11 @@ def adjust_target_dtype(self, target_dtype: \"torch.dtype\") -> \"torch.dtype\":\n             logger.info(\"target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\")\n         return CustomDtype.INT4\n \n-    def update_unexpected_keys(self, model, unexpected_keys: list[str]) -> list[str]:\n-        return [k for k in unexpected_keys if not any(k.endswith(x) for x in self.bnb_keys)]\n-\n     def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n         import bitsandbytes as bnb\n \n-        # They are on the params themselves, so we cannot easily extract the module from the name\n+        # TODO: maybe remove\n+        # # They are on the params themselves, so we cannot easily extract the module from the name\n         if any(param_name.endswith(x) for x in self.bnb_keys):\n             return True\n         module, name = get_module_from_name(model, param_name)\n@@ -142,71 +117,13 @@ def get_param_name(self, param_name: str) -> str:\n                 )\n         return param_name\n \n-    def create_quantized_param(\n-        self,\n-        model: \"PreTrainedModel\",\n-        param_value: \"torch.Tensor\",\n-        param_name: str,\n-        target_device: \"torch.device\",\n-        **kwargs,\n-    ):\n-        import bitsandbytes as bnb\n-\n-        full_name = param_name\n-\n-        # update param name to get the weights instead of the quantized stats\n-        param_name = self.get_param_name(param_name)\n-        module, tensor_name = get_module_from_name(model, param_name)\n-\n-        # `torch.Tensor.to(<int num>)` is not supported by `torch_npu` (see this [issue](https://github.com/Ascend/pytorch/issues/16)).\n-        if isinstance(target_device, int) and is_torch_npu_available():\n-            target_device = f\"npu:{target_device}\"\n-\n-        # construct `new_value` for the module._parameters[tensor_name]\n-        if self.pre_quantized:\n-            module_name = param_name.rsplit(\".\", 1)[0]\n-            # Save the states for later quantization when they are all gathered\n-            if not hasattr(self, \"param_quant_stats\"):\n-                self.param_quant_stats = defaultdict(dict)\n-            self.param_quant_stats[module_name].update({full_name: param_value})\n-\n-            # We are ready for quantization in this case (note, the +1 is for the weight itself)\n-            if len(self.param_quant_stats[module_name]) == len(self.bnb_keys) + 1:\n-                weight = self.param_quant_stats[module_name].pop(f\"{module_name}.weight\")\n-                new_value = bnb.nn.Params4bit.from_prequantized(\n-                    data=weight,\n-                    quantized_stats=self.param_quant_stats[module_name],\n-                    requires_grad=False,\n-                    device=target_device,\n-                    module=module,\n-                )\n-                # Set it\n-                module._parameters[tensor_name] = new_value\n-                # Delete the states\n-                del self.param_quant_stats[module_name]\n-        else:\n-            new_value = param_value.to(\"cpu\")\n-            old_value = getattr(module, tensor_name)\n-\n-            # Support models using `Conv1D` in place of `nn.Linear` (e.g. openai-community/gpt2) by transposing the weight matrix prior to quantization.\n-            # Since weights are saved in the correct \"orientation\", we skip transposing when loading.\n-            if issubclass(module.source_cls, Conv1D):\n-                new_value = new_value.T\n-\n-            kwargs = old_value.__dict__\n-            kwargs.pop(\"_is_hf_initialized\", None)\n-            new_value = bnb.nn.Params4bit(new_value, requires_grad=False, **kwargs).to(target_device)\n-\n-            module._parameters[tensor_name] = new_value\n-\n-    # Copied from transformers.quantizers.quantizer_bnb_8bit.Bnb8BitHfQuantizer.adjust_max_memory\n     def adjust_max_memory(self, max_memory: dict[str, int | str]) -> dict[str, int | str]:\n         # need more space for buffers that are created during quantization\n         max_memory = {key: val * 0.90 for key, val in max_memory.items()}\n         return max_memory\n \n-    # Copied from transformers.quantizers.quantizer_bnb_8bit.Bnb8BitHfQuantizer.update_dtype\n     def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n+        # TODO: remove ? is it still true ? we will move to dtype = \"auto\" so it will likely be either fp16 or bf16\n         if dtype is None:\n             # We force the `dtype` to be float16, this is a requirement from `bitsandbytes`\n             logger.info(\n@@ -238,7 +155,6 @@ def update_device_map(self, device_map):\n             )\n         return device_map\n \n-    # Copied from transformers.quantizers.quantizer_bnb_8bit.Bnb8BitHfQuantizer._process_model_before_weight_loading\n     def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n@@ -248,39 +164,28 @@ def _process_model_before_weight_loading(\n     ):\n         from ..integrations import replace_with_bnb_linear\n \n-        llm_int8_enable_fp32_cpu_offload = self.quantization_config.llm_int8_enable_fp32_cpu_offload\n-\n         self.modules_to_not_convert = self.get_modules_to_not_convert(\n             model, self.quantization_config.llm_int8_skip_modules, keep_in_fp32_modules\n         )\n \n-        # Extend `self.modules_to_not_convert` to keys that are supposed to be offloaded to `cpu` or `disk`\n-        if isinstance(device_map, dict) and len(device_map.keys()) > 1:\n-            keys_on_cpu = [key for key, value in device_map.items() if value in [\"disk\", \"cpu\"]]\n+        if self.quantization_config.llm_int8_enable_fp32_cpu_offload:\n+            if isinstance(device_map, dict):\n+                keys_on_cpu = [key for key, value in device_map.items() if value in [\"disk\", \"cpu\"]]\n+                self.modules_to_not_convert.extend(keys_on_cpu)\n \n-            if len(keys_on_cpu) > 0 and not llm_int8_enable_fp32_cpu_offload:\n-                raise ValueError(\n-                    \"If you want to offload some keys to `cpu` or `disk`, you need to set \"\n-                    \"`llm_int8_enable_fp32_cpu_offload=True`. Note that these modules will not be \"\n-                    \" converted to 8-bit but kept in 32-bit.\"\n-                )\n-            self.modules_to_not_convert.extend(keys_on_cpu)\n         model = replace_with_bnb_linear(\n             model,\n             modules_to_not_convert=self.modules_to_not_convert,\n             quantization_config=self.quantization_config,\n             pre_quantized=self.pre_quantized,\n         )\n \n-        model.config.quantization_config = self.quantization_config\n-\n-    # Copied from transformers.quantizers.quantizer_bnb_8bit.Bnb8BitHfQuantizer._process_model_after_weight_loading with 8bit->4bit\n     def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n         model.is_loaded_in_4bit = True\n         model.is_4bit_serializable = self.is_serializable()\n         return model\n \n-    def is_serializable(self, safe_serialization=None):\n+    def is_serializable(self, **kwargs):\n         return True\n \n     @property"
        },
        {
            "sha": "582144e922203bde293e2a987bf146810aa16065",
            "filename": "src/transformers/quantizers/quantizer_bnb_8bit.py",
            "status": "modified",
            "additions": 15,
            "deletions": 89,
            "changes": 104,
            "blob_url": "https://github.com/huggingface/transformers/blob/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py?ref=01267073bc5393cb8d1c21ffd2816b803705cc74",
            "patch": "@@ -25,6 +25,8 @@\n     is_accelerate_available,\n     is_bitsandbytes_available,\n     is_torch_available,\n+    is_torch_hpu_available,\n+    is_torch_npu_available,\n     is_torch_xpu_available,\n     logging,\n )\n@@ -35,34 +37,20 @@\n     import torch\n \n     from ..core_model_loading import WeightConverter\n-    from ..pytorch_utils import Conv1D\n \n logger = logging.get_logger(__name__)\n \n \n class Bnb8BitHfQuantizer(HfQuantizer):\n     \"\"\"\n-    8-bit quantization from bitsandbytes quantization method:\n-        before loading: converts transformer layers into Linear8bitLt during loading: load 16bit weight and pass to the\n-        layer object after: quantizes individual weights in Linear8bitLt into 8bit at fitst .cuda() call\n-    saving:\n-        from state dict, as usual; saves weights and 'SCB' component\n-    loading:\n-        need to locate SCB component and pass to the Linear8bitLt object\n+    8-bit quantization from bitsandbytes quantization method\n     \"\"\"\n \n-    use_keep_in_fp32_modules = True\n-    requires_parameters_quantization = True\n     requires_calibration = False\n \n-    required_packages = [\"bitsandbytes\", \"accelerate\"]\n-\n     def __init__(self, quantization_config, **kwargs):\n         super().__init__(quantization_config, **kwargs)\n \n-        if self.quantization_config.llm_int8_skip_modules is not None:\n-            self.modules_to_not_convert = self.quantization_config.llm_int8_skip_modules\n-\n     def validate_environment(self, *args, **kwargs):\n         if not is_accelerate_available():\n             raise ImportError(\n@@ -78,17 +66,9 @@ def validate_environment(self, *args, **kwargs):\n         validate_bnb_backend_availability(raise_exception=True)\n \n         device_map = kwargs.get(\"device_map\")\n-        if (\n-            device_map is not None\n-            and isinstance(device_map, dict)\n-            and not self.quantization_config.llm_int8_enable_fp32_cpu_offload\n-        ):\n-            device_map_without_lm_head = {\n-                key: device_map[key] for key in device_map if key not in self.modules_to_not_convert\n-            }\n-            if set(device_map.values()) == {\"cpu\"}:\n-                pass\n-            elif \"cpu\" in device_map_without_lm_head.values() or \"disk\" in device_map_without_lm_head.values():\n+        if not self.quantization_config.llm_int8_enable_fp32_cpu_offload and isinstance(device_map, dict):\n+            values = set(device_map.values())\n+            if values != {\"cpu\"} and (\"cpu\" in values or \"disk\" in values):\n                 raise ValueError(\n                     \"Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \"\n                     \"quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \"\n@@ -120,6 +100,10 @@ def update_device_map(self, device_map):\n         if device_map is None:\n             if torch.cuda.is_available():\n                 device_map = {\"\": torch.cuda.current_device()}\n+            elif is_torch_npu_available():\n+                device_map = {\"\": f\"npu:{torch.npu.current_device()}\"}\n+            elif is_torch_hpu_available():\n+                device_map = {\"\": f\"hpu:{torch.hpu.current_device()}\"}\n             elif is_torch_xpu_available():\n                 device_map = {\"\": torch.xpu.current_device()}\n             else:\n@@ -132,61 +116,14 @@ def update_device_map(self, device_map):\n         return device_map\n \n     def adjust_target_dtype(self, target_dtype: \"torch.dtype\") -> \"torch.dtype\":\n-        if target_dtype != torch.int8:\n-            logger.info(\"target_dtype {target_dtype} is replaced by `torch.int8` for 8-bit BnB quantization\")\n         return torch.int8\n \n-    def update_unexpected_keys(self, model, unexpected_keys: list[str]) -> list[str]:\n-        bnb_keys = [\"SCB\", \"weight_format\"]\n-        return [k for k in unexpected_keys if not any(k.endswith(x) for x in bnb_keys)]\n-\n     def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n         import bitsandbytes as bnb\n \n         module, name = get_module_from_name(model, param_name)\n         return isinstance(module, bnb.nn.Linear8bitLt) and name != \"bias\"\n \n-    def create_quantized_param(\n-        self,\n-        model: \"PreTrainedModel\",\n-        param_value: \"torch.Tensor\",\n-        param_name: str,\n-        target_device: \"torch.device\",\n-        **kwargs,\n-    ):\n-        import bitsandbytes as bnb\n-\n-        module, tensor_name = get_module_from_name(model, param_name)\n-\n-        if self.pre_quantized and not self.is_serializable():\n-            raise ValueError(\n-                \"Detected int8 weights but the version of bitsandbytes is not compatible with int8 serialization. \"\n-                \"Make sure to download the latest `bitsandbytes` version. `pip install --upgrade bitsandbytes`.\"\n-            )\n-        # Those 2 can only happen when self.pre_quantized == True\n-        if tensor_name == \"SCB\":\n-            setattr(module.weight, \"SCB\", param_value.to(target_device))\n-            return\n-        # It's not used, but it's getting serialized for BC reason...\n-        elif tensor_name == \"weight_format\":\n-            return\n-\n-        # Support models using `Conv1D` in place of `nn.Linear` (e.g. openai-community/gpt2) by transposing the weight matrix prior to quantization.\n-        # Since weights are saved in the correct \"orientation\", we skip transposing when loading.\n-        if issubclass(module.source_cls, Conv1D) and not self.pre_quantized:\n-            param_value = param_value.T\n-\n-        old_value = getattr(module, tensor_name)\n-        kwargs = old_value.__dict__\n-        kwargs.pop(\"_is_hf_initialized\", None)\n-        # Need to pop SCB and reset it because of bnb internals that modifies its value when switching devices ...\n-        SCB = kwargs.pop(\"SCB\", None)\n-        new_value = bnb.nn.Int8Params(param_value.to(\"cpu\"), requires_grad=False, **kwargs).to(target_device)\n-        if SCB is not None:\n-            setattr(new_value, \"SCB\", SCB)\n-        # Set it to the module\n-        module._parameters[tensor_name] = new_value\n-\n     def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n         model.is_loaded_in_8bit = True\n         model.is_8bit_serializable = self.is_serializable()\n@@ -201,23 +138,14 @@ def _process_model_before_weight_loading(\n     ):\n         from ..integrations import replace_with_bnb_linear\n \n-        llm_int8_enable_fp32_cpu_offload = self.quantization_config.llm_int8_enable_fp32_cpu_offload\n-\n         self.modules_to_not_convert = self.get_modules_to_not_convert(\n             model, self.quantization_config.llm_int8_skip_modules, keep_in_fp32_modules\n         )\n \n-        # Extend `self.modules_to_not_convert` to keys that are supposed to be offloaded to `cpu` or `disk`\n-        if isinstance(device_map, dict) and len(device_map.keys()) > 1:\n-            keys_on_cpu = [key for key, value in device_map.items() if value in [\"disk\", \"cpu\"]]\n-\n-            if len(keys_on_cpu) > 0 and not llm_int8_enable_fp32_cpu_offload:\n-                raise ValueError(\n-                    \"If you want to offload some keys to `cpu` or `disk`, you need to set \"\n-                    \"`llm_int8_enable_fp32_cpu_offload=True`. Note that these modules will not be \"\n-                    \" converted to 8-bit but kept in 32-bit.\"\n-                )\n-            self.modules_to_not_convert.extend(keys_on_cpu)\n+        if self.quantization_config.llm_int8_enable_fp32_cpu_offload:\n+            if isinstance(device_map, dict):\n+                keys_on_cpu = [key for key, value in device_map.items() if value in [\"disk\", \"cpu\"]]\n+                self.modules_to_not_convert.extend(keys_on_cpu)\n \n         model = replace_with_bnb_linear(\n             model,\n@@ -226,9 +154,7 @@ def _process_model_before_weight_loading(\n             pre_quantized=self.pre_quantized,\n         )\n \n-        model.config.quantization_config = self.quantization_config\n-\n-    def is_serializable(self, safe_serialization=None):\n+    def is_serializable(self, **kwargs):\n         return True\n \n     @property"
        },
        {
            "sha": "3f70ca96379e0872ba052fff63ceb9dd4aa427bd",
            "filename": "src/transformers/quantizers/quantizer_compressed_tensors.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py?ref=01267073bc5393cb8d1c21ffd2816b803705cc74",
            "patch": "@@ -31,7 +31,6 @@ class CompressedTensorsHfQuantizer(HfQuantizer):\n     \"\"\"\n \n     requires_calibration = True\n-    required_packages = [\"compressed_tensors\"]\n \n     def __init__(self, quantization_config: CompressedTensorsConfig, **kwargs):\n         super().__init__(quantization_config, **kwargs)\n@@ -58,9 +57,6 @@ def validate_environment(self, *args, **kwargs):\n                 \"Using `compressed_tensors` quantized models requires the compressed-tensors library: \"\n                 \"`pip install compressed-tensors`\"\n             )\n-        if not is_torch_available():\n-            # torch already should be installed as part of compressed tensors\n-            raise ImportError(\"torch is required for using compressed-tensors quantization\")\n \n     def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n         if dtype is None:\n@@ -113,6 +109,6 @@ def is_qat_trainable(self) -> bool:\n         # models need to be decompressed carry out qat\n         return not self.run_compressed or not self.quantization_config.is_quantization_compressed\n \n-    def is_serializable(self, safe_serialization=None) -> bool:\n+    def is_serializable(self, **kwargs) -> bool:\n         \"\"\"Models quantized using compressed tensors can be saved to disk\"\"\"\n         return True"
        },
        {
            "sha": "235abb89fd5d71f66d1ddd75931709e489f445b6",
            "filename": "src/transformers/quantizers/quantizer_eetq.py",
            "status": "modified",
            "additions": 4,
            "deletions": 12,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_eetq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_eetq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_eetq.py?ref=01267073bc5393cb8d1c21ffd2816b803705cc74",
            "patch": "@@ -32,19 +32,13 @@\n \n class EetqHfQuantizer(HfQuantizer):\n     \"\"\"\n-    8-bit quantization from EETQ quantization method:\n-        before loading: converts transformer layers into W8A16Linear during loading: load 16bit weight and pass to the\n-        layer object after: quantizes individual weights in Linear8bitLt into 8bit at first .cuda() call\n+    8-bit quantization from EETQ quantization method\n     \"\"\"\n \n-    requires_parameters_quantization = True\n     requires_calibration = False\n \n-    required_packages = [\"eetq\", \"accelerate\"]\n-\n     def __init__(self, quantization_config, **kwargs):\n         super().__init__(quantization_config, **kwargs)\n-        self.quantization_config = quantization_config\n \n     def validate_environment(self, *args, **kwargs):\n         if not is_kernels_available():\n@@ -62,8 +56,8 @@ def validate_environment(self, *args, **kwargs):\n                 \"You have loaded an EETQ model on CPU and have a CUDA device available, make sure to set \"\n                 \"your model on a GPU device in order to run your model.\"\n             )\n-        elif device_map is not None:\n-            if isinstance(device_map, dict) and (\"cpu\" in device_map.values() or \"disk\" in device_map.values()):\n+        elif isinstance(device_map, dict):\n+            if len(device_map) > 1 and \"cpu\" in device_map.values() or \"disk\" in device_map.values():\n                 raise ValueError(\n                     \"You are attempting to load an EETQ model with a device_map that contains a CPU or disk device.\"\n                     \" This is not supported. Please remove the CPU or disk device from the device_map.\"\n@@ -111,9 +105,7 @@ def _process_model_before_weight_loading(\n             model, modules_to_not_convert=self.modules_to_not_convert, pre_quantized=self.pre_quantized\n         )\n \n-        model.config.quantization_config = self.quantization_config\n-\n-    def is_serializable(self, safe_serialization=None):\n+    def is_serializable(self, **kwargs):\n         return True\n \n     @property"
        },
        {
            "sha": "25e6e83e6cca478799e95c8da694cf22bf008835",
            "filename": "src/transformers/quantizers/quantizer_fbgemm_fp8.py",
            "status": "modified",
            "additions": 7,
            "deletions": 114,
            "changes": 121,
            "blob_url": "https://github.com/huggingface/transformers/blob/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py?ref=01267073bc5393cb8d1c21ffd2816b803705cc74",
            "patch": "@@ -35,37 +35,23 @@ class FbgemmFp8HfQuantizer(HfQuantizer):\n     FP8 quantization using fbgemm kernels\n     \"\"\"\n \n-    requires_parameters_quantization = True\n     requires_calibration = False\n \n-    required_packages = [\"fbgemm-gpu\", \"accelerate\"]\n-\n     def __init__(self, quantization_config, **kwargs):\n         super().__init__(quantization_config, **kwargs)\n-        self.quantization_config = quantization_config\n \n     def validate_environment(self, *args, **kwargs):\n-        if not is_torch_available():\n-            raise ImportError(\n-                \"Using fbgemm fp8 quantization requires torch >= 2.1.0\"\n-                \"Please install the latest version of torch ( pip install --upgrade torch )\"\n-            )\n         if not is_fbgemm_gpu_available():\n             raise ImportError(\n                 \"Using fbgemm fp8 quantization requires fbgemm-gpu library\"\n                 \"Please install the latest version of fbgemm-gpu library by following : https://pytorch.org/FBGEMM/fbgemm_gpu-development/InstallationInstructions.html#fbgemm-gpu-install-libraries\"\n             )\n-\n         if not is_accelerate_available():\n             raise ImportError(\n                 \"Loading an FP8 quantized model requires accelerate (`pip install --upgrade accelerate`)\"\n             )\n-\n-        if not torch.cuda.is_available():\n-            raise RuntimeError(\"Using FP8 quantized models with fbgemm kernels requires a GPU\")\n-\n         compute_capability = torch.cuda.get_device_capability()\n-        major, minor = compute_capability\n+        major, _ = compute_capability\n         if major < 9:\n             raise ValueError(\n                 \"FP8 quantized models is only supported on GPUs with compute capability >= 9.0 (e.g H100)\"\n@@ -77,12 +63,8 @@ def validate_environment(self, *args, **kwargs):\n                 \"You have loaded an FP8 model on CPU and have a CUDA device available, make sure to set \"\n                 \"your model on a GPU device in order to run your model. To remove this warning, pass device_map = 'cuda'. \"\n             )\n-        elif device_map is not None:\n-            if (\n-                not self.pre_quantized\n-                and isinstance(device_map, dict)\n-                and (\"cpu\" in device_map.values() or \"disk\" in device_map.values())\n-            ):\n+        elif isinstance(device_map, dict):\n+            if not self.pre_quantized and (\"cpu\" in device_map.values() or \"disk\" in device_map.values()):\n                 raise ValueError(\n                     \"You are attempting to load an FP8 model with a device_map that contains a CPU or disk device.\"\n                     \"This is not supported when the model is quantized on the fly. \"\n@@ -101,7 +83,7 @@ def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n             )\n         elif dtype == torch.float16:\n             raise ValueError(\n-                \"You cannot use FP8 with dtype=torch.float16.We recommend you passing dtype=torch.bfloat16\"\n+                \"You cannot use FP8 with dtype=torch.float16. We recommend you passing dtype=torch.bfloat16\"\n             )\n         return dtype\n \n@@ -122,76 +104,6 @@ def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **\n                 return True\n         return False\n \n-    def create_quantized_param(\n-        self,\n-        model: \"PreTrainedModel\",\n-        param_value: \"torch.Tensor\",\n-        param_name: str,\n-        target_device: \"torch.device\",\n-        **kwargs,\n-    ):\n-        from ..integrations import FbgemmFp8Linear, FbgemmFp8Llama4TextExperts\n-\n-        module, tensor_name = get_module_from_name(model, param_name)\n-\n-        # Sanity checks\n-        if isinstance(module, FbgemmFp8Linear):\n-            if self.pre_quantized or tensor_name == \"bias\":\n-                if tensor_name == \"weight\" and param_value.dtype != torch.float8_e4m3fn:\n-                    raise ValueError(\"Expect quantized weights but got an unquantized weight\")\n-            else:\n-                if tensor_name == \"weight_scale\":\n-                    raise ValueError(\"Expect unquantized weights but got a quantized weight_scale\")\n-        if isinstance(module, FbgemmFp8Llama4TextExperts):\n-            if not (self.pre_quantized or tensor_name == \"bias\"):\n-                if tensor_name == \"gate_up_proj_scale\" or tensor_name == \"down_proj_scale\":\n-                    raise ValueError(\"Expect unquantized weights but got a quantized weight_scale\")\n-\n-        if isinstance(module, FbgemmFp8Llama4TextExperts):\n-            if tensor_name == \"gate_up_proj\":\n-                # Process each expert separately\n-                # Transpose the second and third dimension\n-                transposed_param = param_value.transpose(1, 2)\n-\n-                # Reshape to 2D for quantization\n-                original_shape = transposed_param.shape\n-                flattened_param = transposed_param.reshape(-1, original_shape[-1])\n-\n-                # Quantize using per row instead of per column\n-                new_value_flat, weight_scale_flat = torch.ops.fbgemm.quantize_fp8_per_row(flattened_param)\n-\n-                # Reshape back to original dimensions\n-                new_value = new_value_flat.reshape(original_shape)\n-                new_value = new_value.transpose(1, 2)\n-                weight_scale = weight_scale_flat.reshape(original_shape[0], 1, original_shape[1])\n-            elif tensor_name == \"down_proj\":\n-                # Process each expert separately\n-                # Transpose the weights for proper quantization\n-                transposed_param = param_value.transpose(1, 2)\n-\n-                # Reshape to 2D for quantization\n-                original_shape = transposed_param.shape\n-                flattened_param = transposed_param.reshape(-1, original_shape[-1])\n-\n-                # Quantize using per column\n-                new_value_flat, weight_scale_flat = torch.ops.fbgemm.quantize_fp8_per_row(flattened_param)\n-\n-                # Reshape back to original dimensions\n-                new_value = new_value_flat.reshape(original_shape)\n-                new_value = new_value.transpose(1, 2)\n-                weight_scale = weight_scale_flat.reshape(original_shape[0], original_shape[1], 1)\n-\n-            module._parameters[f\"{tensor_name}_scale\"] = torch.nn.Parameter(weight_scale.to(target_device))\n-        else:\n-            new_value, weight_scale = torch.ops.fbgemm.quantize_fp8_per_row(param_value)\n-            module._parameters[f\"{tensor_name}_scale\"] = torch.nn.Parameter(\n-                weight_scale.view(weight_scale.shape[0], 1).to(target_device)\n-            )\n-\n-        module._parameters[tensor_name] = torch.nn.Parameter(new_value.to(target_device))\n-\n-        del param_name\n-\n     def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n@@ -200,38 +112,19 @@ def _process_model_before_weight_loading(\n     ):\n         from ..integrations import replace_with_fbgemm_fp8_linear\n \n-        tp_plan = model._tp_plan\n         self.modules_to_not_convert = self.get_modules_to_not_convert(\n             model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n         )\n \n-        config = model.config\n         model = replace_with_fbgemm_fp8_linear(\n             model,\n             modules_to_not_convert=self.modules_to_not_convert,\n             quantization_config=self.quantization_config,\n             pre_quantized=self.pre_quantized,\n-            config=config,\n-            tp_plan=tp_plan,\n+            config=model.config,\n+            tp_plan=model._tp_plan,\n         )\n \n-        model.config.quantization_config = self.quantization_config\n-\n-    def update_missing_keys(self, model, missing_keys: list[str], prefix: str) -> list[str]:\n-        from ..integrations import FbgemmFp8Linear, FbgemmFp8Llama4TextExperts\n-\n-        not_missing_keys = []\n-        for name, module in model.named_modules():\n-            if isinstance(module, (FbgemmFp8Linear, FbgemmFp8Llama4TextExperts)):\n-                for missing in missing_keys:\n-                    if (\n-                        (name in missing or name in f\"{prefix}.{missing}\")\n-                        and not missing.endswith(\".weight\")\n-                        and not missing.endswith(\".bias\")\n-                    ):\n-                        not_missing_keys.append(missing)\n-        return [k for k in missing_keys if k not in not_missing_keys]\n-\n     def update_tp_plan(self, config):\n         if \"Llama4\" in config.__class__.__name__:\n             text_plan = {\n@@ -279,7 +172,7 @@ def update_tp_plan(self, config):\n \n         return config\n \n-    def is_serializable(self, safe_serialization=None):\n+    def is_serializable(self, **kwargs):\n         return True\n \n     @property"
        },
        {
            "sha": "eb0c38f5e137d24d236e2fb68715f3ccfb9b5d3b",
            "filename": "src/transformers/quantizers/quantizer_finegrained_fp8.py",
            "status": "modified",
            "additions": 10,
            "deletions": 106,
            "changes": 116,
            "blob_url": "https://github.com/huggingface/transformers/blob/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py?ref=01267073bc5393cb8d1c21ffd2816b803705cc74",
            "patch": "@@ -20,26 +20,20 @@ class FineGrainedFP8HfQuantizer(HfQuantizer):\n     Supports both e4m3fn formats based on platform.\n     \"\"\"\n \n-    requires_parameters_quantization = True\n     requires_calibration = False\n-    required_packages = [\"accelerate\"]\n \n     def __init__(self, quantization_config, **kwargs):\n         super().__init__(quantization_config, **kwargs)\n-        self.quantization_config = quantization_config\n \n     def validate_environment(self, *args, **kwargs):\n-        if not is_torch_available():\n-            raise ImportError(\n-                \"Using fp8 quantization requires torch >= 2.1.0\"\n-                \"Please install the latest version of torch ( pip install --upgrade torch )\"\n-            )\n-\n         if not is_accelerate_available():\n             raise ImportError(\"Loading an FP8 quantized model requires accelerate (`pip install accelerate`)\")\n \n-        if (not (torch.cuda.is_available() or is_torch_xpu_available())) and not self.quantization_config.dequantize:\n-            if self.pre_quantized:\n+        if self.quantization_config.dequantize:\n+            return\n+\n+        if not torch.cuda.is_available() and not is_torch_xpu_available():\n+            if self.pre_quantized and not self.quantization_config.dequantize:\n                 logger.warning_once(\n                     \"Using FP8 quantized models requires a GPU or XPU, we will default to dequantizing the model to bf16 since no GPU or XPU is available\"\n                 )\n@@ -64,88 +58,19 @@ def validate_environment(self, *args, **kwargs):\n                 \"your model on a GPU or XPU device in order to run your model. To remove this warning, \"\n                 \"pass device_map = 'cuda' or 'xpu'. \"\n             )\n-        elif device_map is not None:\n+        elif isinstance(device_map, dict):\n             if (\n                 not self.pre_quantized\n-                and isinstance(device_map, dict)\n-                and (\"cpu\" in device_map.values() or \"disk\" in device_map.values())\n+                and len(device_map) > 1\n+                and \"cpu\" in device_map.values()\n+                or \"disk\" in device_map.values()\n             ):\n                 raise ValueError(\n                     \"You are attempting to load an FP8 model with a device_map that contains a cpu/disk device.\"\n                     \"This is not supported when the model is quantized on the fly. \"\n                     \"Please use a quantized checkpoint or remove the cpu/disk device from the device_map.\"\n                 )\n \n-    def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n-        if dtype is None:\n-            logger.info(\"Setting dtype to torch.float32 as no dtype was specified in from_pretrained\")\n-            dtype = torch.float32\n-        return dtype\n-\n-    # TODO: make this into a `ConversionType` ops -> potentially requires all weights on all ranks\n-    # depending on the layer type (moe -> no if ep)\n-    def create_quantized_param(\n-        self,\n-        model: \"PreTrainedModel\",\n-        param_value: \"torch.Tensor\",\n-        param_name: str,\n-        target_device: \"torch.device\",\n-        **kwargs,\n-    ):\n-        from ..integrations.finegrained_fp8 import FP8Linear\n-        from ..modeling_utils import _load_parameter_into_model\n-\n-        # Sanity checks\n-        module, tensor_name = get_module_from_name(model, param_name)\n-        if isinstance(module, FP8Linear):\n-            if self.pre_quantized or tensor_name == \"bias\":\n-                if tensor_name == \"weight\" and param_value.dtype != torch.float8_e4m3fn:\n-                    raise ValueError(\"Expect quantized weights but got an unquantized weight\")\n-            else:\n-                return\n-                # if tensor_name == \"weight_scale_inv\":\n-                #     raise ValueError(\"Expect unquantized weights but got a quantized weight_scale\")\n-\n-        param_value = param_value.to(target_device)\n-\n-        # Get FP8 min/max values\n-        fp8_min = torch.finfo(torch.float8_e4m3fn).min\n-        fp8_max = torch.finfo(torch.float8_e4m3fn).max\n-\n-        block_size_m, block_size_n = self.quantization_config.weight_block_size\n-\n-        rows, cols = param_value.shape[-2:]\n-\n-        if rows % block_size_m != 0 or cols % block_size_n != 0:\n-            raise ValueError(\n-                f\"Matrix dimensions ({rows}, {cols}) must be divisible by block sizes ({block_size_m}, {block_size_n})\"\n-            )\n-        param_value_orig_shape = param_value.shape\n-\n-        param_value = param_value.reshape(\n-            -1, rows // block_size_m, block_size_m, cols // block_size_n, block_size_n\n-        ).permute(0, 1, 3, 2, 4)\n-\n-        # Calculate scaling factor for each block\n-        max_abs = torch.amax(torch.abs(param_value), dim=(-1, -2))\n-        scale = fp8_max / max_abs\n-        scale_orig_shape = scale.shape\n-        scale = scale.unsqueeze(-1).unsqueeze(-1)\n-\n-        # Quantize the weights\n-        quantized_param = torch.clamp(param_value * scale, min=fp8_min, max=fp8_max).to(torch.float8_e4m3fn)\n-\n-        quantized_param = quantized_param.permute(0, 1, 3, 2, 4)\n-        # Reshape back to matrix shape\n-        quantized_param = quantized_param.reshape(param_value_orig_shape)\n-\n-        # Reshape scale to match the number of blocks\n-        scale = scale.reshape(scale_orig_shape).squeeze().reciprocal()\n-\n-        # Load into the model\n-        _load_parameter_into_model(model, param_name, quantized_param)\n-        _load_parameter_into_model(model, param_name.rsplit(\".\", 1)[0] + \".weight_scale_inv\", scale)\n-\n     def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n         from ..integrations.finegrained_fp8 import FP8Expert, FP8Linear\n \n@@ -176,27 +101,6 @@ def _process_model_before_weight_loading(\n             pre_quantized=self.pre_quantized,\n         )\n \n-        model.config.quantization_config = self.quantization_config\n-\n-    def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n-        if self.pre_quantized and self.quantization_config.dequantize:\n-            self.remove_quantization_config(model)\n-\n-    def update_missing_keys(self, model, missing_keys: list[str], prefix: str) -> list[str]:\n-        from ..integrations import FP8Linear\n-\n-        not_missing_keys = []\n-        for name, module in model.named_modules():\n-            if isinstance(module, FP8Linear):\n-                for missing in missing_keys:\n-                    if (\n-                        (name in missing or name in f\"{prefix}.{missing}\")\n-                        and not missing.endswith(\".weight\")\n-                        and not missing.endswith(\".bias\")\n-                    ):\n-                        not_missing_keys.append(missing)\n-        return [k for k in missing_keys if k not in not_missing_keys]\n-\n     # NOTE: TP is applied before quantization so this is only to add hooks.\n     # Quantization is incompatible with DTensors, so we have to anyway have\n     # gathers! But it should be model independant -> figure out where to put\n@@ -226,7 +130,7 @@ def update_tp_plan(self, config):\n \n         return config\n \n-    def is_serializable(self, safe_serialization=None):\n+    def is_serializable(self, **kwargs):\n         return True\n \n     @property"
        },
        {
            "sha": "f9d66986a2b48424d8a5b1b1028dceeb154093ef",
            "filename": "src/transformers/quantizers/quantizer_fp_quant.py",
            "status": "modified",
            "additions": 21,
            "deletions": 79,
            "changes": 100,
            "blob_url": "https://github.com/huggingface/transformers/blob/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_fp_quant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_fp_quant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_fp_quant.py?ref=01267073bc5393cb8d1c21ffd2816b803705cc74",
            "patch": "@@ -36,13 +36,10 @@ class FPQuantHfQuantizer(HfQuantizer):\n     \"\"\"\n \n     requires_calibration = False\n-    requires_parameters_quantization = True\n     is_qat_trainable = True\n-    required_packages = [\"fp_quant\"]\n \n     def __init__(self, quantization_config: QuantizationConfigMixin, **kwargs):\n         super().__init__(quantization_config, **kwargs)\n-        self.quantization_config = quantization_config\n \n     def validate_environment(self, device_map, **kwargs):\n         if not torch.cuda.is_available() and not is_torch_xpu_available():\n@@ -68,66 +65,35 @@ def validate_environment(self, device_map, **kwargs):\n                 \"You are attempting to load a FPQuant model without setting device_map.\"\n                 \" Please set device_map comprised of 'cuda' devices.\"\n             )\n-        elif (\n-            isinstance(device_map, dict)\n-            and (\"cpu\" in device_map.values() or \"disk\" in device_map.values())\n-            and not self.quantization_config.pseudoquantization\n-        ):\n-            raise ValueError(\n-                \"You are attempting to load a FPQuant model with a device_map that contains a CPU or disk device.\"\n-                \" This is not supported. Please remove the CPU or disk device from the device_map.\"\n-            )\n+        elif isinstance(device_map, dict):\n+            if (\n+                not self.quantization_config.pseudoquantization\n+                and len(device_map) > 1\n+                and \"cpu\" in device_map.values()\n+                or \"disk\" in device_map.values()\n+            ):\n+                raise ValueError(\n+                    \"You are attempting to load a FPQuant model with a device_map that contains a CPU or disk device.\"\n+                    \" This is not supported. Please remove the CPU or disk device from the device_map.\"\n+                )\n \n     def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n         if dtype is None:\n             logger.info(\"`dtype` is None. Setting `dtype=torch.bfloat16` for qutlass compatibility.\")\n             dtype = torch.bfloat16\n         elif dtype != torch.bfloat16:\n             raise ValueError(f\"Invalid `dtype` {dtype}. fp_quant quantization only supports `dtype=torch.bfloat16`.\")\n-\n         return dtype\n \n-    def create_quantized_param(\n-        self,\n-        model: \"PreTrainedModel\",\n-        param_value: \"torch.Tensor\",\n-        param_name: str,\n-        target_device: \"torch.device\",\n-        **kwargs,\n-    ):\n-        module, _ = get_module_from_name(model, param_name)\n-\n-        if target_device == \"cpu\" and param_name.endswith(\"weight\"):\n-            # Works agains hard-coded missing key dispatch to CPU\n-            return\n-\n-        # The module holds either:\n-        #  * `weight` when `store_master_weights=True`\n-        #  * `qweight` and `scales` when `store_master_weights=False` and `pseudoquantization=False`\n-        #  * `dqweight` when `store_master_weights=False` and `pseudoquantization=True`\n-\n-        if param_name.endswith(\".qweight\"):\n-            # Loading a real quantized checkpoint without master weights\n-            module.qweight = torch.nn.Parameter(\n-                param_value.to(target_device),\n-                requires_grad=False,\n-            )\n-            module.weight = None\n-            module.dqweight = None\n-            return\n-\n-        if param_name.endswith(\".dqweight\"):\n-            # Loading a pseudo-quantized checkpoint without master weights\n-            module.dqweight = torch.nn.Parameter(param_value.to(target_device))\n-            module.weight = None\n-            module.qweight = None\n-            module.scales = None\n-            return\n-\n-        # Loading master weights or an unquantized checkpoint\n-        module.weight = torch.nn.Parameter(param_value.to(target_device))\n-        # Let pre-forward handle the quantization and set None where necessary\n-        module.pre_forward()\n+    def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n+        from fp_quant import FPQuantLinear\n+\n+        module, tensor_name = get_module_from_name(model, param_name)\n+        if isinstance(module, FPQuantLinear) and tensor_name in [\"weight\", \"qweight\", \"dqweight\"]:\n+            # Only quantize weights of FPQuantLinear modules that are not already quantized\n+            return True\n+        else:\n+            return False\n \n     def _process_model_before_weight_loading(\n         self,\n@@ -142,20 +108,6 @@ def _process_model_before_weight_loading(\n             model,\n             fp_quant_linear_config=adapt_fp_quant_config(self.quantization_config),\n         )\n-        model.config.quantization_config = self.quantization_config\n-\n-    def update_missing_keys(self, model, missing_keys: list[str], prefix: str) -> list[str]:\n-        from fp_quant import FPQuantLinear\n-\n-        fp_quant_names = {name for name, module in model.named_modules() if isinstance(module, FPQuantLinear)}\n-\n-        def should_exclude(key: str) -> bool:\n-            if key.endswith(\".weight\") or key.endswith(\".bias\"):\n-                return False\n-            full_key = f\"{prefix}.{key}\"\n-            return any(name in key or name in full_key for name in fp_quant_names)\n-\n-        return [key for key in missing_keys if not should_exclude(key)]\n \n     @property\n     def is_trainable(self, model: Optional[\"PreTrainedModel\"] = None):\n@@ -166,15 +118,5 @@ def is_trainable(self, model: Optional[\"PreTrainedModel\"] = None):\n             )\n         return trainable\n \n-    def is_serializable(self, safe_serialization=None):\n+    def is_serializable(self, **kwargs):\n         return True\n-\n-    def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n-        from fp_quant import FPQuantLinear\n-\n-        module, tensor_name = get_module_from_name(model, param_name)\n-        if isinstance(module, FPQuantLinear) and tensor_name in [\"weight\", \"qweight\", \"dqweight\"]:\n-            # Only quantize weights of FPQuantLinear modules that are not already quantized\n-            return True\n-        else:\n-            return False"
        },
        {
            "sha": "8b828bc7ce86e52e79a50e86b971c026081d893f",
            "filename": "src/transformers/quantizers/quantizer_gptq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_gptq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_gptq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_gptq.py?ref=01267073bc5393cb8d1c21ffd2816b803705cc74",
            "patch": "@@ -39,8 +39,6 @@ class GptqHfQuantizer(HfQuantizer):\n     \"\"\"\n \n     requires_calibration = False\n-    required_packages = [\"optimum\", \"auto_gptq\", \"gptqmodel\"]\n-    optimum_quantizer = None\n \n     def __init__(self, quantization_config: QuantizationConfigMixin, **kwargs):\n         super().__init__(quantization_config, **kwargs)\n@@ -120,5 +118,5 @@ def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs\n     def is_trainable(self) -> bool:\n         return True\n \n-    def is_serializable(self, safe_serialization=None):\n+    def is_serializable(self, **kwargs):\n         return True"
        },
        {
            "sha": "d706e2c9f1cee6325ea3fd2f132195fb0d1c8446",
            "filename": "src/transformers/quantizers/quantizer_higgs.py",
            "status": "modified",
            "additions": 39,
            "deletions": 53,
            "changes": 92,
            "blob_url": "https://github.com/huggingface/transformers/blob/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_higgs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_higgs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_higgs.py?ref=01267073bc5393cb8d1c21ffd2816b803705cc74",
            "patch": "@@ -37,12 +37,9 @@ class HiggsHfQuantizer(HfQuantizer):\n     \"\"\"\n \n     requires_calibration = False\n-    requires_parameters_quantization = True\n-    required_packages = [\"flute-kernel\", \"fast_hadamard_transform\"]\n \n     def __init__(self, quantization_config: QuantizationConfigMixin, **kwargs):\n         super().__init__(quantization_config, **kwargs)\n-        self.quantization_config = quantization_config\n \n     def validate_environment(self, device_map, **kwargs):\n         if not torch.cuda.is_available():\n@@ -64,11 +61,12 @@ def validate_environment(self, device_map, **kwargs):\n                 \"You are attempting to load a HIGGS model without setting device_map.\"\n                 \" Please set device_map comprised of 'cuda' devices.\"\n             )\n-        elif isinstance(device_map, dict) and (\"cpu\" in device_map.values() or \"disk\" in device_map.values()):\n-            raise ValueError(\n-                \"You are attempting to load a HIGGS model with a device_map that contains a CPU or disk device.\"\n-                \" This is not supported. Please remove the CPU or disk device from the device_map.\"\n-            )\n+        elif isinstance(device_map, dict):\n+            if \"cpu\" in device_map.values() or \"disk\" in device_map.values():\n+                raise ValueError(\n+                    \"You are attempting to load a HIGGS model with a device_map that contains a CPU or disk device.\"\n+                    \" This is not supported. Please remove the CPU or disk device from the device_map.\"\n+                )\n \n     def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n         if dtype is None:\n@@ -81,37 +79,39 @@ def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n \n         return dtype\n \n-    def create_quantized_param(\n-        self,\n-        model: \"PreTrainedModel\",\n-        param_value: \"torch.Tensor\",\n-        param_name: str,\n-        target_device: \"torch.device\",\n-        **kwargs,\n-    ):\n-        from ..integrations import quantize_with_higgs\n-\n-        flute_dict = quantize_with_higgs(\n-            param_value.to(target_device),\n-            self.quantization_config.bits,\n-            self.quantization_config.p,\n-            self.quantization_config.group_size,\n-            self.quantization_config.hadamard_size,\n-        )\n-        del param_value\n-\n-        module, _ = get_module_from_name(model, param_name)\n-        module_name = \".\".join(param_name.split(\".\")[:-1])\n-        for key, value in flute_dict.items():\n-            if key in module._parameters:\n-                module._parameters[key] = torch.nn.Parameter(value, requires_grad=False)\n-            elif key in module._buffers:\n-                module._buffers[key] = torch.nn.Buffer(value)\n-            elif key == \"tune_metadata\":\n-                module.tune_metadata = value\n-                self.quantization_config.tune_metadata[module_name] = value.to_dict()\n-            else:\n-                raise ValueError(f\"Unexpected key {key} in module {module}\")\n+    # TODO: to remove\n+    # Kept here in case we see some interest in adding support for it\n+    # def create_quantized_param(\n+    #     self,\n+    #     model: \"PreTrainedModel\",\n+    #     param_value: \"torch.Tensor\",\n+    #     param_name: str,\n+    #     target_device: \"torch.device\",\n+    #     **kwargs,\n+    # ):\n+    #     from ..integrations import quantize_with_higgs\n+\n+    #     flute_dict = quantize_with_higgs(\n+    #         param_value.to(target_device),\n+    #         self.quantization_config.bits,\n+    #         self.quantization_config.p,\n+    #         self.quantization_config.group_size,\n+    #         self.quantization_config.hadamard_size,\n+    #     )\n+    #     del param_value\n+\n+    #     module, _ = get_module_from_name(model, param_name)\n+    #     module_name = \".\".join(param_name.split(\".\")[:-1])\n+    #     for key, value in flute_dict.items():\n+    #         if key in module._parameters:\n+    #             module._parameters[key] = torch.nn.Parameter(value, requires_grad=False)\n+    #         elif key in module._buffers:\n+    #             module._buffers[key] = torch.nn.Buffer(value)\n+    #         elif key == \"tune_metadata\":\n+    #             module.tune_metadata = value\n+    #             self.quantization_config.tune_metadata[module_name] = value.to_dict()\n+    #         else:\n+    #             raise ValueError(f\"Unexpected key {key} in module {module}\")\n \n     def _process_model_before_weight_loading(\n         self,\n@@ -130,7 +130,6 @@ def _process_model_before_weight_loading(\n             quantization_config=self.quantization_config,\n             modules_to_not_convert=self.modules_to_not_convert,\n         )\n-        model.config.quantization_config = self.quantization_config\n \n     def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n         from flute.tune import TuneMetaData, maybe_tune_and_repack\n@@ -157,19 +156,6 @@ def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs\n             )\n             self.quantization_config.tune_metadata[name] = module.tune_metadata.to_dict()\n \n-    def update_missing_keys(self, model, missing_keys: list[str], prefix: str) -> list[str]:\n-        from ..integrations import HiggsLinear\n-\n-        higgs_names = {name for name, module in model.named_modules() if isinstance(module, HiggsLinear)}\n-\n-        def should_update(key: str) -> bool:\n-            if key.endswith(\".weight\") or key.endswith(\".bias\"):\n-                return False\n-            full_key = f\"{prefix}.{key}\"\n-            return any(name in key or name in full_key for name in higgs_names)\n-\n-        return [key for key in missing_keys if not should_update(key)]\n-\n     @property\n     def is_trainable(self) -> bool:\n         return False"
        },
        {
            "sha": "2dfbe20b35a40d3256580c2a7de36531daa90836",
            "filename": "src/transformers/quantizers/quantizer_hqq.py",
            "status": "modified",
            "additions": 143,
            "deletions": 152,
            "changes": 295,
            "blob_url": "https://github.com/huggingface/transformers/blob/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py?ref=01267073bc5393cb8d1c21ffd2816b803705cc74",
            "patch": "@@ -12,7 +12,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from collections import defaultdict\n from typing import TYPE_CHECKING\n \n from ..integrations import prepare_for_hqq_linear\n@@ -49,10 +48,7 @@ class HqqHfQuantizer(HfQuantizer):\n     nn.Linear modules are first tagged with quant_config in _process_model_before_weight_loading().\n     \"\"\"\n \n-    use_keep_in_fp32_modules = False\n-    requires_parameters_quantization = True\n     requires_calibration = False\n-    required_packages = [\"hqq\"]\n \n     def __init__(self, quantization_config, **kwargs):\n         if not is_hqq_available():\n@@ -83,161 +79,156 @@ def validate_environment(self, *args, **kwargs):\n             else:\n                 self.using_multi_gpu = len(set(device_map.values())) > 1\n \n-    def update_missing_keys(\n-        self, model: \"PreTrainedModel\", missing_keys: list[str], prefix: str, **kwargs\n-    ) -> list[str]:\n-        if self.pre_quantized:\n-            return [key for key in missing_keys if (\"weight\" not in key)]\n-        else:\n-            return missing_keys\n-\n-    # Adds missing keys for HQQLinear modules that are loaded but the model with initialized with torch.nn.Linear\n-    def update_expected_keys(\n-        self, model: \"PreTrainedModel\", expected_keys: list[str], loaded_keys: list[str]\n-    ) -> list[str]:\n-        if not self.pre_quantized:\n-            return expected_keys\n-\n-        # Collects all quantizable (linear) layers\n-        def _find_hqq_quantizable_layers(model, layers):\n-            for name, module in model.named_children():\n-                if isinstance(module, (torch.nn.Linear)):\n-                    layers.add(module.name)\n-                _find_hqq_quantizable_layers(module, layers)\n-\n-        new_keys = set(expected_keys)\n-\n-        # Name modules\n-        for name, module in model.named_modules():\n-            module.name = name\n-\n-        # valid modules are Linear layers that have HQQLinear state_dict. We ignore skip_modules and any layers with Linear state_dict() params\n-        _valid_modules = set()\n-        _find_hqq_quantizable_layers(model, _valid_modules)\n-\n-        # Remove skipped modules\n-        _skipped_modules = set()\n-        for _module in _valid_modules:\n-            for _skip_module in model.config.quantization_config[\"skip_modules\"]:\n-                if _skip_module in _module:\n-                    _skipped_modules.add(_module)\n-        _valid_modules -= _skipped_modules\n-\n-        # Append new expected layers based on _ref_keys\n-        _ref_keys = HQQLinear(\n-            linear_layer=None,\n-            quant_config=None,\n-            compute_dtype=torch.float16,\n-            device=\"cpu\",\n-            del_orig=False,\n-        ).state_dict_keys() - {\"bias\"}\n-\n-        # Clean-up\n-        _rm_keys = set()\n-        for key in new_keys:\n-            if any(_module in key for _module in _valid_modules):\n-                _rm_keys.add(key)\n-        new_keys -= _rm_keys\n-        # At this point, new_keys contains all the keys of the layers that are NOT HQQLinear or torch.nn.Linear\n-\n-        # Re-populate Linear/HQQLinear\n-        for _module in _valid_modules:\n-            if _module + \".weight\" in loaded_keys:\n-                new_keys.add(_module + \".weight\")\n-            else:\n-                new_keys.update({_module + \".\" + _ref_key for _ref_key in _ref_keys})\n-            if _module + \".bias\" in loaded_keys:\n-                new_keys.add(_module + \".bias\")\n-\n-        return list(new_keys)\n+    # TODO: to remove\n+    # Kept here in case we see some interest in adding support for it\n+    # # Adds missing keys for HQQLinear modules that are loaded but the model with initialized with torch.nn.Linear\n+    # def update_expected_keys(\n+    #     self, model: \"PreTrainedModel\", expected_keys: list[str], loaded_keys: list[str]\n+    # ) -> list[str]:\n+    #     if not self.pre_quantized:\n+    #         return expected_keys\n+\n+    #     # Collects all quantizable (linear) layers\n+    #     def _find_hqq_quantizable_layers(model, layers):\n+    #         for name, module in model.named_children():\n+    #             if isinstance(module, (torch.nn.Linear)):\n+    #                 layers.add(module.name)\n+    #             _find_hqq_quantizable_layers(module, layers)\n+\n+    #     new_keys = set(expected_keys)\n+\n+    #     # Name modules\n+    #     for name, module in model.named_modules():\n+    #         module.name = name\n+\n+    #     # valid modules are Linear layers that have HQQLinear state_dict. We ignore skip_modules and any layers with Linear state_dict() params\n+    #     _valid_modules = set()\n+    #     _find_hqq_quantizable_layers(model, _valid_modules)\n+\n+    #     # Remove skipped modules\n+    #     _skipped_modules = set()\n+    #     for _module in _valid_modules:\n+    #         for _skip_module in model.config.quantization_config[\"skip_modules\"]:\n+    #             if _skip_module in _module:\n+    #                 _skipped_modules.add(_module)\n+    #     _valid_modules -= _skipped_modules\n+\n+    #     # Append new expected layers based on _ref_keys\n+    #     _ref_keys = HQQLinear(\n+    #         linear_layer=None,\n+    #         quant_config=None,\n+    #         compute_dtype=torch.float16,\n+    #         device=\"cpu\",\n+    #         del_orig=False,\n+    #     ).state_dict_keys() - {\"bias\"}\n+\n+    #     # Clean-up\n+    #     _rm_keys = set()\n+    #     for key in new_keys:\n+    #         if any(_module in key for _module in _valid_modules):\n+    #             _rm_keys.add(key)\n+    #     new_keys -= _rm_keys\n+    #     # At this point, new_keys contains all the keys of the layers that are NOT HQQLinear or torch.nn.Linear\n+\n+    #     # Re-populate Linear/HQQLinear\n+    #     for _module in _valid_modules:\n+    #         if _module + \".weight\" in loaded_keys:\n+    #             new_keys.add(_module + \".weight\")\n+    #         else:\n+    #             new_keys.update({_module + \".\" + _ref_key for _ref_key in _ref_keys})\n+    #         if _module + \".bias\" in loaded_keys:\n+    #             new_keys.add(_module + \".bias\")\n+\n+    #     return list(new_keys)\n \n     def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n         module, _ = get_module_from_name(model, param_name)\n         # Since we do not prepare the modules in advance, we need every param of the Linear layer to go through\n         # `create_quantized_param`, even when `self.is_quantized == True`\n         return isinstance(module, torch.nn.Linear)\n \n-    def create_quantized_param(\n-        self,\n-        model: \"PreTrainedModel\",\n-        param_value: \"torch.Tensor\",\n-        param_name: str,\n-        target_device: \"torch.device\",\n-        **kwargs,\n-    ):\n-        module, tensor_name = get_module_from_name(model, param_name)\n-        module_name = param_name.rsplit(\".\", 1)[0]\n-        parent_module, node = get_module_from_name(model, module_name)\n-\n-        quant_config = model.config.quantization_config[\"quant_config\"]\n-        skip_modules = model.config.quantization_config[\"skip_modules\"]\n-\n-        # In this case we do not quantize this layer (it's explicitly skipped) -> simply load param\n-        if any(skip_module in module.name for skip_module in skip_modules):\n-            module.load_state_dict(\n-                {tensor_name: param_value.to(device=target_device, dtype=self.dtype)}, strict=False, assign=True\n-            )\n-            return\n-\n-        # We need this hack as the model is not pre-prepared as an empty skeleton on meta device\n-        if self.pre_quantized:\n-            # Save them for later\n-            if not hasattr(self, \"hqq_params\"):\n-                self.hqq_params = defaultdict(dict)\n-            self.hqq_params[module_name].update({tensor_name: param_value})\n-            hqq_params = self.hqq_params[module_name]\n-\n-            # If they are all present and saved, make it a HQQLinear layer! (we cannot do it param after param because\n-            # hqq does not support it...)\n-            if all(k in hqq_params for k in self.hqq_keys) and (\"bias\" in hqq_params or module.bias is None):\n-                hqq_layer = HQQLinear(\n-                    linear_layer=None,\n-                    quant_config=None,\n-                    compute_dtype=self.dtype,\n-                    device=target_device,\n-                    del_orig=False,\n-                )\n-                hqq_layer.load_state_dict(hqq_params)\n-\n-                if hqq_layer.bias is not None and isinstance(hqq_layer.bias, torch.Tensor):\n-                    hqq_layer.bias = torch.nn.Parameter(hqq_layer.bias)\n-                if self.using_multi_gpu:\n-                    hqq_layer = self._patch_layer_for_multigpu(hqq_layer)\n-\n-                setattr(parent_module, node, hqq_layer)\n-                del self.hqq_params[module_name], module\n-            return\n-\n-        # Load param in the module (without caring about device or dtype, it will be changed later)\n-        module.load_state_dict({tensor_name: param_value}, strict=False, assign=True)\n-\n-        # If both the weight and bias have already been loaded, time to quantize!\n-        module_is_ready = module.weight.device.type != \"meta\" and (\n-            module.bias is None or module.bias.device.type != \"meta\"\n-        )\n-\n-        if module_is_ready:\n-            module_tag = \".\".join(module.name.split(\".\")[-2:])\n-            if \"weight_quant_params\" in quant_config:\n-                module_quant_config = quant_config\n-            elif module_tag in quant_config:\n-                module_quant_config = quant_config[module_tag]\n-\n-            hqq_layer = HQQLinear(\n-                module,\n-                quant_config=module_quant_config,\n-                compute_dtype=self.dtype,\n-                device=target_device,\n-                del_orig=True,\n-            )\n-\n-            if hqq_layer.bias is not None and isinstance(hqq_layer.bias, torch.Tensor):\n-                hqq_layer.bias = torch.nn.Parameter(hqq_layer.bias)\n-\n-            if self.using_multi_gpu:\n-                hqq_layer = self._patch_layer_for_multigpu(hqq_layer)\n-\n-            setattr(parent_module, node, hqq_layer)\n+    # TODO: to remove\n+    # def create_quantized_param(\n+    #     self,\n+    #     model: \"PreTrainedModel\",\n+    #     param_value: \"torch.Tensor\",\n+    #     param_name: str,\n+    #     target_device: \"torch.device\",\n+    #     **kwargs,\n+    # ):\n+    #     module, tensor_name = get_module_from_name(model, param_name)\n+    #     module_name = param_name.rsplit(\".\", 1)[0]\n+    #     parent_module, node = get_module_from_name(model, module_name)\n+\n+    #     quant_config = model.config.quantization_config[\"quant_config\"]\n+    #     skip_modules = model.config.quantization_config[\"skip_modules\"]\n+\n+    #     # In this case we do not quantize this layer (it's explicitly skipped) -> simply load param\n+    #     if any(skip_module in module.name for skip_module in skip_modules):\n+    #         module.load_state_dict(\n+    #             {tensor_name: param_value.to(device=target_device, dtype=self.dtype)}, strict=False, assign=True\n+    #         )\n+    #         return\n+\n+    #     # We need this hack as the model is not pre-prepared as an empty skeleton on meta device\n+    #     if self.pre_quantized:\n+    #         # Save them for later\n+    #         if not hasattr(self, \"hqq_params\"):\n+    #             self.hqq_params = defaultdict(dict)\n+    #         self.hqq_params[module_name].update({tensor_name: param_value})\n+    #         hqq_params = self.hqq_params[module_name]\n+\n+    #         # If they are all present and saved, make it a HQQLinear layer! (we cannot do it param after param because\n+    #         # hqq does not support it...)\n+    #         if all(k in hqq_params for k in self.hqq_keys) and (\"bias\" in hqq_params or module.bias is None):\n+    #             hqq_layer = HQQLinear(\n+    #                 linear_layer=None,\n+    #                 quant_config=None,\n+    #                 compute_dtype=self.dtype,\n+    #                 device=target_device,\n+    #                 del_orig=False,\n+    #             )\n+    #             hqq_layer.load_state_dict(hqq_params)\n+\n+    #             if hqq_layer.bias is not None and isinstance(hqq_layer.bias, torch.Tensor):\n+    #                 hqq_layer.bias = torch.nn.Parameter(hqq_layer.bias)\n+    #             if self.using_multi_gpu:\n+    #                 hqq_layer = self._patch_layer_for_multigpu(hqq_layer)\n+\n+    #             setattr(parent_module, node, hqq_layer)\n+    #             del self.hqq_params[module_name], module\n+    #         return\n+\n+    #     # Load param in the module (without caring about device or dtype, it will be changed later)\n+    #     module.load_state_dict({tensor_name: param_value}, strict=False, assign=True)\n+\n+    #     # If both the weight and bias have already been loaded, time to quantize!\n+    #     module_is_ready = module.weight.device.type != \"meta\" and (\n+    #         module.bias is None or module.bias.device.type != \"meta\"\n+    #     )\n+\n+    #     if module_is_ready:\n+    #         module_tag = \".\".join(module.name.split(\".\")[-2:])\n+    #         if \"weight_quant_params\" in quant_config:\n+    #             module_quant_config = quant_config\n+    #         elif module_tag in quant_config:\n+    #             module_quant_config = quant_config[module_tag]\n+\n+    #         hqq_layer = HQQLinear(\n+    #             module,\n+    #             quant_config=module_quant_config,\n+    #             compute_dtype=self.dtype,\n+    #             device=target_device,\n+    #             del_orig=True,\n+    #         )\n+\n+    #         if hqq_layer.bias is not None and isinstance(hqq_layer.bias, torch.Tensor):\n+    #             hqq_layer.bias = torch.nn.Parameter(hqq_layer.bias)\n+\n+    #         if self.using_multi_gpu:\n+    #             hqq_layer = self._patch_layer_for_multigpu(hqq_layer)\n+\n+    #         setattr(parent_module, node, hqq_layer)\n \n     def _patch_layer_for_multigpu(self, hqq_layer):\n         def forward_with_device(self, x):"
        },
        {
            "sha": "08b8538b82309e9251654905406aeeda9f6371db",
            "filename": "src/transformers/quantizers/quantizer_mxfp4.py",
            "status": "modified",
            "additions": 11,
            "deletions": 161,
            "changes": 172,
            "blob_url": "https://github.com/huggingface/transformers/blob/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py?ref=01267073bc5393cb8d1c21ffd2816b803705cc74",
            "patch": "@@ -43,14 +43,10 @@ class Mxfp4HfQuantizer(HfQuantizer):\n     FP4 quantization using fbgemm kernels\n     \"\"\"\n \n-    requires_parameters_quantization = True\n     requires_calibration = False\n \n-    required_packages = [\"accelerate\"]\n-\n     def __init__(self, quantization_config, **kwargs):\n         super().__init__(quantization_config, **kwargs)\n-        self.quantization_config = quantization_config\n         self.triton_kernels_hub = None\n \n     def _lazy_import_kernels(self):\n@@ -74,7 +70,7 @@ def validate_environment(self, *args, **kwargs):\n         if self.quantization_config.dequantize:\n             return\n \n-        if not (torch.cuda.is_available() or torch.xpu.is_available()):\n+        if not torch.cuda.is_available() and not torch.xpu.is_available():\n             if self.pre_quantized:\n                 logger.warning_once(\n                     \"Using MXFP4 quantized models requires a GPU, we will default to dequantizing the model to bf16\"\n@@ -131,12 +127,8 @@ def validate_environment(self, *args, **kwargs):\n                 \"You have loaded an FP4 model on CPU and have a CUDA/XPU device available, make sure to set \"\n                 \"your model on a GPU/XPU device in order to run your model. To remove this warning, pass device_map = 'cuda' or device_map = 'xpu'. \"\n             )\n-        elif device_map is not None:\n-            if (\n-                not self.pre_quantized\n-                and isinstance(device_map, dict)\n-                and (\"cpu\" in device_map.values() or \"disk\" in device_map.values())\n-            ):\n+        elif isinstance(device_map, dict):\n+            if not self.pre_quantized and (\"cpu\" in device_map.values() or \"disk\" in device_map.values()):\n                 raise ValueError(\n                     \"You are attempting to load an FP4 model with a device_map that contains a CPU or disk device.\"\n                     \"This is not supported when the model is quantized on the fly. \"\n@@ -157,146 +149,21 @@ def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n \n     def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n         from ..integrations import Mxfp4GptOssExperts\n-        from ..models.gpt_oss.modeling_gpt_oss import GptOssExperts\n \n-        if self.pre_quantized:\n-            return False\n-        # if we are dequantizing, the model doesn't have scales, and blocks only params like gate_up_proj and down_proj so we need to handle this case differently\n-        if self.quantization_config.dequantize and (\"blocks\" in param_name or \"scales\" in param_name):\n-            module, tensor_name = get_module_from_name(model, param_name[: -len(\"_blocks\")])\n-        else:\n-            module, tensor_name = get_module_from_name(model, param_name)\n-        if isinstance(module, Mxfp4GptOssExperts) or (\n-            isinstance(module, GptOssExperts) and self.quantization_config.dequantize\n-        ):\n+        module, tensor_name = get_module_from_name(model, param_name)\n+        if isinstance(module, Mxfp4GptOssExperts):\n             if tensor_name in [\"down_proj_bias\", \"gate_up_proj_bias\"]:\n                 return False\n             return True\n         return False\n \n-    def create_quantized_param(\n-        self,\n-        model: \"PreTrainedModel\",\n-        param_value: \"torch.Tensor\",\n-        param_name: str,\n-        target_device: \"torch.device\",\n-        **kwargs,\n-    ):\n-        from ..integrations import (\n-            Mxfp4GptOssExperts,\n-            dequantize,\n-            load_and_swizzle_mxfp4,\n-            quantize_to_mxfp4,\n-            swizzle_mxfp4,\n-        )\n-        from ..models.gpt_oss.modeling_gpt_oss import GptOssExperts\n-\n-        if not self.pre_quantized:\n-            triton_kernels_hub = self._lazy_import_kernels()\n-            module, _ = get_module_from_name(model, param_name)\n-            with torch.device(target_device):\n-                if isinstance(module, Mxfp4GptOssExperts):\n-                    triton_weight_tensor, weight_scale = quantize_to_mxfp4(param_value, triton_kernels_hub)\n-                    PrecisionConfig, FlexCtx, InFlexData = (\n-                        triton_kernels_hub.matmul_ogs.PrecisionConfig,\n-                        triton_kernels_hub.matmul_ogs.FlexCtx,\n-                        triton_kernels_hub.matmul_ogs.InFlexData,\n-                    )\n-                    triton_weight_tensor, weight_scale = swizzle_mxfp4(\n-                        triton_weight_tensor, weight_scale, triton_kernels_hub\n-                    )\n-\n-                    proj = \"gate_up_proj\" if \"gate_up_proj\" in param_name else \"down_proj\"\n-                    setattr(module, proj, triton_weight_tensor)\n-                    setattr(\n-                        module,\n-                        f\"{proj}_precision_config\",\n-                        PrecisionConfig(weight_scale=weight_scale, flex_ctx=FlexCtx(rhs_data=InFlexData())),\n-                    )\n-\n-                    delattr(module, f\"{proj}_blocks\")\n-                    delattr(module, f\"{proj}_scales\")\n-\n-        # The params going here are either gate_up_proj_blocks, or down_proj_blocks, or gate_up_proj_scales, or down_proj_scales\n-        else:\n-            #  This is when loading a quantized model (blocks and scales exist)\n-            empty_param = kwargs.get(\"empty_param\")\n-            casting_dtype = kwargs.get(\"casting_dtype\")\n-            to_contiguous = kwargs.get(\"to_contiguous\")\n-            rank = kwargs.get(\"rank\")\n-            device_mesh = kwargs.get(\"device_mesh\")\n-            if (\"blocks\" in param_name or \"scales\" in param_name) and self.quantization_config.dequantize:\n-                # blocks and scales have the same length that's why this works for both\n-                module, _ = get_module_from_name(model, param_name[: -len(\"_blocks\")])\n-            else:\n-                module, _ = get_module_from_name(model, param_name)\n-\n-            shard_kwargs = {\n-                \"empty_param\": empty_param,\n-                \"casting_dtype\": casting_dtype,\n-                \"to_contiguous\": to_contiguous,\n-                \"rank\": rank,\n-                \"device_mesh\": device_mesh,\n-                \"model\": model,\n-            }\n-\n-            if isinstance(module, Mxfp4GptOssExperts) or (\n-                isinstance(module, GptOssExperts) and self.quantization_config.dequantize\n-            ):\n-                if self.quantization_config.dequantize:\n-                    # dq_param_name is the name of the parameter without the blocks or scales suffix, it's used in this case since we don't switch linears\n-                    # so we only have the original param name\n-                    dq_param_name = param_name[: -len(\"_blocks\")]\n-                    dequantize(module, param_name, param_value, target_device, dq_param_name, **shard_kwargs)\n-                else:\n-                    load_and_swizzle_mxfp4(\n-                        module,\n-                        param_name,\n-                        param_value,\n-                        target_device,\n-                        self._lazy_import_kernels(),\n-                        **shard_kwargs,\n-                    )\n-\n     def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n-        # we are not really dequantizing, we are just removing everything related to quantization here\n-        if self.quantization_config.dequantize:\n-            self.remove_quantization_config(model)\n         # clean cache due to triton ops\n         if torch.cuda.is_available():\n             torch.cuda.empty_cache()\n         elif torch.xpu.is_available():\n             torch.xpu.empty_cache()\n \n-    def update_expected_keys(self, model: \"PreTrainedModel\", expected_keys: list[str], checkpoint_keys: list[str]):\n-        # Replace expected_keys for experts' gate_up_proj and down_proj with their _blocks and _scales variants\n-        new_expected_keys = []\n-        for key in expected_keys:\n-            if key.endswith(\".mlp.experts.gate_up_proj\"):\n-                base = key[: -len(\"gate_up_proj\")]\n-                new_expected_keys.append(base + \"gate_up_proj_blocks\")\n-                new_expected_keys.append(base + \"gate_up_proj_scales\")\n-            elif key.endswith(\".mlp.experts.down_proj\"):\n-                base = key[: -len(\"down_proj\")]\n-                new_expected_keys.append(base + \"down_proj_blocks\")\n-                new_expected_keys.append(base + \"down_proj_scales\")\n-            elif not self.pre_quantized:\n-                # in this case, we are quantizing the model so we need to update the keys as we changed the layers\n-                if key.endswith(\".mlp.experts.down_proj_blocks\"):\n-                    base = key[: -len(\"down_proj_blocks\")]\n-                    new_expected_keys.append(base + \"down_proj\")\n-                elif key.endswith(\".mlp.experts.gate_up_proj_blocks\"):\n-                    base = key[: -len(\"gate_up_proj_blocks\")]\n-                    new_expected_keys.append(base + \"gate_up_proj\")\n-                elif key.endswith(\"scales\"):\n-                    # we remove it the scales as the checkpoint don't contain them\n-                    continue\n-                else:\n-                    new_expected_keys.append(key)\n-            else:\n-                new_expected_keys.append(key)\n-        return new_expected_keys\n-\n     def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n@@ -306,10 +173,6 @@ def _process_model_before_weight_loading(\n     ):\n         from ..integrations import replace_with_mxfp4_linear\n \n-        self.modules_to_not_convert = self.get_modules_to_not_convert(\n-            model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n-        )\n-\n         # if we are using kernels, we can't use the quantized model, since the forward pass is different and needs special handling\n         if use_kernels:\n             logger.warning_once(\n@@ -318,27 +181,14 @@ def _process_model_before_weight_loading(\n             )\n             self.quantization_config.dequantize = True\n \n+        self.modules_to_not_convert = self.get_modules_to_not_convert(\n+            model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n+        )\n+\n         model = replace_with_mxfp4_linear(\n             model, modules_to_not_convert=self.modules_to_not_convert, quantization_config=self.quantization_config\n         )\n \n-        model.config.quantization_config = self.quantization_config\n-\n-    def update_missing_keys(self, model, missing_keys: list[str], prefix: str) -> list[str]:\n-        from ..integrations import Mxfp4GptOssExperts\n-\n-        not_missing_keys = []\n-        for name, module in model.named_modules():\n-            if isinstance(module, Mxfp4GptOssExperts):\n-                for missing in missing_keys:\n-                    if (\n-                        (name in missing or name in f\"{prefix}.{missing}\")\n-                        and not missing.endswith(\".weight\")\n-                        and not missing.endswith(\".bias\")\n-                    ):\n-                        not_missing_keys.append(missing)\n-        return [k for k in missing_keys if k not in not_missing_keys]\n-\n     def update_tp_plan(self, config):\n         if \"GptOssConfig\" in config.__class__.__name__:\n             if getattr(config, \"base_model_tp_plan\", None) is not None:\n@@ -378,7 +228,7 @@ def get_param_name(self, param_name: str) -> str:\n                 return param_name.replace(\"down_proj\", \"down_proj_blocks\")\n         return param_name\n \n-    def get_state_dict_and_metadata(self, model, safe_serialization: bool = False):\n+    def get_state_dict_and_metadata(self, model, **kwargs):\n         from ..integrations import Mxfp4GptOssExperts\n \n         state_dict = model.state_dict()\n@@ -417,7 +267,7 @@ def get_state_dict_and_metadata(self, model, safe_serialization: bool = False):\n         metadata = {}\n         return state_dict, metadata\n \n-    def is_serializable(self, safe_serialization=None):\n+    def is_serializable(self, **kwargs):\n         return True\n \n     @property"
        },
        {
            "sha": "6525efa0ff5252ca4a6a56243eb65a841ca0d695",
            "filename": "src/transformers/quantizers/quantizer_quanto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py?ref=01267073bc5393cb8d1c21ffd2816b803705cc74",
            "patch": "@@ -40,8 +40,6 @@ class QuantoHfQuantizer(HfQuantizer):\n     Quantizer for the quanto library\n     \"\"\"\n \n-    required_packages = [\"quanto\", \"accelerate\"]\n-    requires_parameters_quantization = True\n     requires_calibration = False\n \n     def __init__(self, quantization_config: QuantoConfig, **kwargs):\n@@ -57,12 +55,8 @@ def validate_environment(self, *args, **kwargs):\n                 \"Loading an optimum-quanto quantized model requires accelerate library (`pip install accelerate`)\"\n             )\n         device_map = kwargs.get(\"device_map\")\n-        if device_map is not None:\n-            if (\n-                isinstance(device_map, dict)\n-                and len(device_map) >= 2\n-                and (\"cpu\" in device_map.values() or \"disk\" in device_map.values())\n-            ):\n+        if isinstance(device_map, dict):\n+            if len(device_map) > 1 and \"cpu\" in device_map.values() or \"disk\" in device_map.values():\n                 raise ValueError(\n                     \"You are attempting to load an model with a device_map that contains a CPU or disk device.\"\n                     \"This is not supported with quanto when the model is quantized on the fly. \"\n@@ -113,7 +107,6 @@ def _process_model_before_weight_loading(\n         model = replace_with_quanto_layers(\n             model, modules_to_not_convert=self.modules_to_not_convert, quantization_config=self.quantization_config\n         )\n-        model.config.quantization_config = self.quantization_config\n \n     @property\n     def is_trainable(self) -> bool:"
        },
        {
            "sha": "c8c8eb6c7f26d6c0649a41b2ddcba753aeb6743d",
            "filename": "src/transformers/quantizers/quantizer_quark.py",
            "status": "modified",
            "additions": 1,
            "deletions": 17,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_quark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_quark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_quark.py?ref=01267073bc5393cb8d1c21ffd2816b803705cc74",
            "patch": "@@ -45,12 +45,6 @@ class QuarkHfQuantizer(HfQuantizer):\n     \"\"\"\n \n     requires_calibration = True  # On-the-fly quantization with quark is not supported for now.\n-    required_packages = [\"quark\"]\n-\n-    # Checkpoints are expected to be already quantized when loading a quark model. However, as some keys from\n-    # the checkpoint might mismatch the model parameters keys, we use the `create_quantized_param` method\n-    # to load the checkpoints, remapping the keys.\n-    requires_parameters_quantization = True\n \n     def __init__(self, quantization_config, **kwargs):\n         super().__init__(quantization_config, **kwargs)\n@@ -78,17 +72,7 @@ def _process_model_before_weight_loading(self, model: \"PreTrainedModel\", **kwarg\n     def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n         return True\n \n-    def create_quantized_param(self, model, param, param_name, param_device, **kwargs):\n-        from ..modeling_utils import _load_parameter_into_model\n-\n-        postfix = param_name.split(\".\")[-1]\n-\n-        if postfix in CHECKPOINT_KEYS:\n-            param_name = param_name.replace(postfix, CHECKPOINT_KEYS[postfix])\n-\n-        _load_parameter_into_model(model, param_name, param.to(param_device))\n-\n-    def is_serializable(self, safe_serialization=None):\n+    def is_serializable(self, **kwargs):\n         return False\n \n     @property"
        },
        {
            "sha": "7107141607726bbfcecb16ab2365d78b76b8c38d",
            "filename": "src/transformers/quantizers/quantizer_spqr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_spqr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_spqr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_spqr.py?ref=01267073bc5393cb8d1c21ffd2816b803705cc74",
            "patch": "@@ -39,7 +39,6 @@ class SpQRHfQuantizer(HfQuantizer):\n \n     def __init__(self, quantization_config: QuantizationConfigMixin, **kwargs):\n         super().__init__(quantization_config, **kwargs)\n-        self.quantization_config = quantization_config\n \n     def validate_environment(self, *args, **kwargs):\n         if not torch.cuda.is_available():\n@@ -71,17 +70,15 @@ def _process_model_before_weight_loading(\n         self.modules_to_not_convert = self.get_modules_to_not_convert(\n             model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n         )\n-\n         replace_with_spqr_linear(\n             model,\n             quantization_config=self.quantization_config,\n             modules_to_not_convert=self.modules_to_not_convert,\n         )\n-        model.config.quantization_config = self.quantization_config\n \n     @property\n     def is_trainable(self):\n         return False\n \n-    def is_serializable(self, safe_serialization=None):\n+    def is_serializable(self, **kwargs):\n         return True"
        },
        {
            "sha": "e5a6de6478c5df696b615b084092b886ed65185d",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 1,
            "deletions": 158,
            "changes": 159,
            "blob_url": "https://github.com/huggingface/transformers/blob/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=01267073bc5393cb8d1c21ffd2816b803705cc74",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n import importlib\n import re\n-import types\n-from collections import defaultdict\n from typing import TYPE_CHECKING\n \n from packaging import version\n@@ -37,15 +35,12 @@\n \n if is_torch_available():\n     import torch\n-    import torch.nn as nn\n \n if is_torchao_available():\n     if version.parse(importlib.metadata.version(\"torchao\")) >= version.parse(\"0.15.0\"):\n         from torchao.prototype.safetensors.safetensors_support import (\n             flatten_tensor_state_dict,\n-            unflatten_tensor_state_dict,\n         )\n-        from torchao.prototype.safetensors.safetensors_utils import is_metadata_torchao\n \n \n logger = logging.get_logger(__name__)\n@@ -94,9 +89,7 @@ class TorchAoHfQuantizer(HfQuantizer):\n     Quantizer for torchao: https://github.com/pytorch/ao/\n     \"\"\"\n \n-    requires_parameters_quantization = True\n     requires_calibration = False\n-    required_packages = [\"torchao\"]\n \n     def __init__(self, quantization_config, **kwargs):\n         super().__init__(quantization_config, **kwargs)\n@@ -230,9 +223,6 @@ def _process_model_before_weight_loading(\n             ]\n         return\n \n-    def update_unexpected_keys(self, model, unexpected_keys: list[str]) -> list[str]:\n-        return [k for k in unexpected_keys if not any(k.endswith(x) for x in self.full_ao_keys)]\n-\n     def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n         if self.pre_quantized:\n             return False\n@@ -267,148 +257,6 @@ def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **\n \n         return isinstance(module, tuple(_QUANTIZABLE)) and tensor_name == \"weight\"\n \n-    def create_quantized_param(\n-        self,\n-        model: \"PreTrainedModel\",\n-        param_value: \"torch.Tensor\",\n-        param_name: str,\n-        target_device: \"torch.device\",\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Each nn.Linear layer that needs to be quantized is processed here.\n-        First, we set the value the weight tensor, then we move it to the target device. Finally, we quantize the module.\n-        \"\"\"\n-        from torchao.quantization import quantize_\n-\n-        full_name = param_name\n-        # Those are the pre quantized weights\n-        if \":\" in param_name:\n-            param_name = param_name.rsplit(\":\", 1)[0]\n-        module, tensor_name = get_module_from_name(model, param_name)\n-\n-        if self.pre_quantized:\n-            # If it's a bias, no need to do anything special (except removing the \":_data\" part of the key, but was\n-            # already done) - if it's unsafe-serialized (i.e. not safetensors), not need for anything either\n-            is_unsafe_serialization = \":\" not in full_name\n-            if tensor_name == \"bias\" or is_unsafe_serialization:\n-                module._parameters[tensor_name] = torch.nn.Parameter(\n-                    param_value.to(target_device), requires_grad=param_value.requires_grad\n-                )\n-                return\n-            # Sanity check for the new serialization format\n-            elif not (TORCHAO_VERSION >= version.parse(\"0.15.0\") and is_metadata_torchao(self.metadata)):\n-                raise ValueError(\"To use `safetensors` serialization, you should have `torchao>=0.15.0` installed\")\n-\n-            # Save the states for later quantization when they are all gathered\n-            if not hasattr(self, \"ao_params\"):\n-                self.ao_params = defaultdict(dict)\n-            self.ao_params[param_name].update({full_name: param_value})\n-\n-            # We are ready for quantization in this case (we retrieved all the needed keys)\n-            if len(self.ao_params[param_name]) == len(self.weight_ao_keys):\n-                new_param = unflatten_tensor_state_dict(self.ao_params[param_name], self.metadata)[param_name]\n-                # Set it\n-                module._parameters[tensor_name] = torch.nn.Parameter(\n-                    new_param.to(target_device), requires_grad=new_param.requires_grad\n-                )\n-\n-                # Free memory\n-                del self.ao_params[param_name]\n-\n-            # Add repr to the module\n-            if isinstance(module, nn.Linear):\n-                module.extra_repr = types.MethodType(_linear_extra_repr, module)\n-        else:\n-            module._parameters[tensor_name] = torch.nn.Parameter(\n-                param_value, requires_grad=param_value.requires_grad\n-            ).to(target_device)\n-            # if we are quantizing tied parameters, to avoid tying the quantized weights\n-            # the correct order to do it is\n-            # 1. load the weight to model\n-            # 2. run tie_weights to populate the weights\n-            # 3. quantize\n-            input_embed = model.get_input_embeddings()\n-            if self.quantization_config.untie_embedding_weights and id(module) == id(input_embed):\n-                model.tie_weights()\n-                setattr(model.config.get_text_config(decoder=True), \"tie_word_embeddings\", False)\n-\n-            # handle FqnToConfig, introduced in torchao 0.15.0+\n-            if self.quantization_config._get_ao_version() >= version.Version(\"0.15.0\"):\n-                from torchao.quantization import FqnToConfig\n-\n-                config = self.quantization_config.get_apply_tensor_subclass()\n-                if isinstance(config, FqnToConfig):\n-                    module_fqn, top_level_param_name = param_name.rsplit(\".\", 1)\n-                    c = None\n-                    if param_name in config.fqn_to_config:\n-                        assert not module_fqn.startswith(\"re:\"), (\n-                            \"param fqn should not start with`re:`, which is used for specifying regex\"\n-                        )\n-                        c = config.module_fqn_to_config[param_name]\n-                    elif module_fqn in config.fqn_to_config:\n-                        assert not module_fqn.startswith(\"re:\"), (\n-                            \"module fqn should not start with`re:`, which is used for specifying regex\"\n-                        )\n-                        c = config.module_fqn_to_config[module_fqn]\n-                    # regex match module and param\n-                    else:\n-                        for maybe_module_fqn_pattern in config.fqn_to_config:\n-                            # if key doesn't start with re, it is an exact fqn key, so we don't regex match\n-                            if not maybe_module_fqn_pattern.startswith(\"re:\"):\n-                                continue\n-                            # see if param matches first\n-                            elif re.fullmatch(maybe_module_fqn_pattern[3:], param_name):\n-                                c = config.module_fqn_to_config[maybe_module_fqn_pattern]\n-                                break\n-                            elif re.fullmatch(maybe_module_fqn_pattern[3:], module_fqn):\n-                                # we'll apply the config for first fully matched pattern\n-                                c = config.module_fqn_to_config[maybe_module_fqn_pattern]\n-                                break\n-                        else:\n-                            c = config.module_fqn_to_config.get(\"_default\", None)\n-\n-                    if c is not None:\n-                        if top_level_param_name == \"weight\":\n-                            # we can apply the module config directly\n-                            quantize_(module, c, (lambda x, fqn: True))\n-                        else:\n-                            # need to apply to custom param name\n-                            custom_param_fqn_config = FqnToConfig({top_level_param_name: c})\n-                            quantize_(module, custom_param_fqn_config, filter_fn=None)\n-                    return\n-\n-            # handle ModuleFqnToConfig, introduced in torchao 0.12.0+\n-            # TODO deprecate this when we deprecate ModuleFqnToConfig\n-            elif self.quantization_config._get_ao_version() >= version.Version(\"0.12.0\"):\n-                from torchao.quantization import ModuleFqnToConfig\n-\n-                config = self.quantization_config.get_apply_tensor_subclass()\n-                if isinstance(config, ModuleFqnToConfig):\n-                    module_fqn, _ = param_name.rsplit(\".\", 1)\n-                    c = None\n-                    if module_fqn in config.module_fqn_to_config:\n-                        assert not module_fqn.startswith(\"re:\"), (\n-                            \"module fqn should not start with`re:`, which is used for specifying regex\"\n-                        )\n-                        c = config.module_fqn_to_config[module_fqn]\n-                    else:\n-                        for maybe_module_fqn_pattern in config.module_fqn_to_config:\n-                            if not maybe_module_fqn_pattern.startswith(\"re:\"):\n-                                continue\n-                            elif re.fullmatch(maybe_module_fqn_pattern[3:], module_fqn):\n-                                # we'll apply the config for first fully matched pattern\n-                                c = config.module_fqn_to_config[maybe_module_fqn_pattern]\n-                                break\n-                        else:\n-                            c = config.module_fqn_to_config.get(\"_default\", None)\n-                    if c is not None:\n-                        # filter_fn: not filtering out any modules\n-                        quantize_(module, c, filter_fn=lambda x, fqn: True)\n-                    return\n-\n-            quantize_(module, self.quantization_config.get_apply_tensor_subclass())\n-\n     def preprocess_model(self, model: \"PreTrainedModel\", config, dtype=None, checkpoint_files=None, **kwargs):\n         \"\"\"\n         Setting model attributes and/or converting model before weights loading. At this point\n@@ -451,18 +299,13 @@ def is_serializable(self, safe_serialization=None) -> bool:\n                 )\n             return _is_torchao_serializable\n \n-        _is_torchao_serializable = version.parse(importlib.metadata.version(\"huggingface_hub\")) >= version.parse(\n-            \"0.25.0\"\n-        )\n-        if not _is_torchao_serializable:\n-            logger.warning(\"torchao quantized model is only serializable after huggingface_hub >= 0.25.0 \")\n         if self.offload and self.quantization_config.modules_to_not_convert is None:\n             logger.warning(\n                 \"The model contains offloaded modules and these modules are not quantized. We don't recommend saving the model as we won't be able to reload them.\"\n                 \"If you want to specify modules to not quantize, please specify modules_to_not_convert in the quantization_config.\"\n             )\n             return False\n-        return _is_torchao_serializable\n+        return True\n \n     def get_accelerator_warm_up_factor(self):\n         \"\"\""
        },
        {
            "sha": "09808b97cf65b8cc20ef3af7c6f65f3b4e22f9dd",
            "filename": "src/transformers/quantizers/quantizer_vptq.py",
            "status": "modified",
            "additions": 7,
            "deletions": 21,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_vptq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01267073bc5393cb8d1c21ffd2816b803705cc74/src%2Ftransformers%2Fquantizers%2Fquantizer_vptq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_vptq.py?ref=01267073bc5393cb8d1c21ffd2816b803705cc74",
            "patch": "@@ -35,11 +35,9 @@ class VptqHfQuantizer(HfQuantizer):\n     \"\"\"\n \n     requires_calibration = True\n-    required_packages = [\"vptq\"]\n \n     def __init__(self, quantization_config: QuantizationConfigMixin, **kwargs):\n         super().__init__(quantization_config, **kwargs)\n-        self.quantization_config = quantization_config\n \n     def validate_environment(self, *args, **kwargs):\n         if not is_accelerate_available():\n@@ -48,21 +46,15 @@ def validate_environment(self, *args, **kwargs):\n         if not is_vptq_available():\n             raise ImportError(\"Using `vptq` quantization requires VPTQ>=0.0.4: `pip install -U vptq`\")\n \n+        if not torch.cuda.is_available():\n+            raise RuntimeError(\"GPU is required to run VTPQ quantized model.\")\n+\n     def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n         if dtype is None:\n-            if torch.cuda.is_available():\n-                dtype = torch.float16\n-                logger.info(\n-                    \"CUDA available. Assuming VPTQ inference on GPU and loading the model in `torch.float16`. To overwrite it, set `dtype` manually.\"\n-                )\n-            else:\n-                import vptq\n-\n-                device_availability = getattr(vptq, \"device_availability\", lambda device: False)\n-                if device_availability(\"cpu\") is True:\n-                    raise RuntimeError(\"No GPU found. Please wait for the next release of VPTQ to use CPU inference\")\n-                dtype = torch.float32\n-                logger.info(\"No GPU found. Assuming VPTQ inference on CPU and loading the model in `torch.float32`.\")\n+            dtype = torch.float16\n+            logger.info(\n+                \"Assuming VPTQ inference on GPU and loading the model in `torch.float16`. To overwrite it, set `dtype` manually.\"\n+            )\n         return dtype\n \n     def _process_model_before_weight_loading(\n@@ -71,22 +63,16 @@ def _process_model_before_weight_loading(\n         keep_in_fp32_modules: list[str] | None = None,\n         **kwargs,\n     ):\n-        \"\"\"\n-        we don't have param like modules_to_not_convert to indicate which layers should not be quantized\n-        because `quantization_config` include the layers that should be quantized\n-        \"\"\"\n         from ..integrations import replace_with_vptq_linear\n \n         self.modules_to_not_convert = self.get_modules_to_not_convert(\n             model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n         )\n-\n         replace_with_vptq_linear(\n             model,\n             quantization_config=self.quantization_config,\n             modules_to_not_convert=self.modules_to_not_convert,\n         )\n-        model.config.quantization_config = self.quantization_config\n \n     @property\n     def is_trainable(self) -> bool:"
        }
    ],
    "stats": {
        "total": 1471,
        "additions": 298,
        "deletions": 1173
    }
}