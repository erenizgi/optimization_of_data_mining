{
    "author": "stevhliu",
    "message": "[docs] top_p, top_k, temperature docstrings (#35065)\n\nclarify",
    "sha": "98e8062df3cf82b367e8a243963e4afb0d9d3407",
    "files": [
        {
            "sha": "18cf26b8b73415411e20e75c2e953258677e5159",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/98e8062df3cf82b367e8a243963e4afb0d9d3407/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/98e8062df3cf82b367e8a243963e4afb0d9d3407/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=98e8062df3cf82b367e8a243963e4afb0d9d3407",
            "patch": "@@ -195,12 +195,12 @@ class GenerationConfig(PushToHubMixin):\n         > Parameters for manipulation of the model output logits\n \n         temperature (`float`, *optional*, defaults to 1.0):\n-            The value used to modulate the next token probabilities.\n+            The value used to module the next token probabilities. This value is set in a model's `generation_config.json` file. If it isn't set, the default value is 1.0\n         top_k (`int`, *optional*, defaults to 50):\n-            The number of highest probability vocabulary tokens to keep for top-k-filtering.\n+            The number of highest probability vocabulary tokens to keep for top-k-filtering. This value is set in a model's `generation_config.json` file. If it isn't set, the default value is 50.\n         top_p (`float`, *optional*, defaults to 1.0):\n             If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to\n-            `top_p` or higher are kept for generation.\n+            `top_p` or higher are kept for generation. This value is set in a model's `generation_config.json` file. If it isn't set, the default value is 1.0\n         min_p (`float`, *optional*):\n             Minimum token probability, which will be scaled by the probability of the most likely token. It must be a\n             value between 0 and 1. Typical values are in the 0.01-0.2 range, comparably selective as setting `top_p` in"
        }
    ],
    "stats": {
        "total": 6,
        "additions": 3,
        "deletions": 3
    }
}