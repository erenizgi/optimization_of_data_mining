{
    "author": "vasqu",
    "message": "[`GPTQ`, `CompressedTensors`] Fix unsafe imports and metada check (#34815)\n\n* fix gptq creation when optimum is not installed + fix metadata checking\r\n\r\n* fix compressed tensors as well\r\n\r\n* style\r\n\r\n* pray for ci luck on flaky tests :prayge:\r\n\r\n* trigger ci\r\n\r\n---------\r\n\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\r\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "24c91f095fec4d90fa6901ef17146b4f4c21d0a3",
    "files": [
        {
            "sha": "7d208087bbbfece0a4bb47238a773b23e3dbcd77",
            "filename": "src/transformers/quantizers/quantizer_compressed_tensors.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/24c91f095fec4d90fa6901ef17146b4f4c21d0a3/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24c91f095fec4d90fa6901ef17146b4f4c21d0a3/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py?ref=24c91f095fec4d90fa6901ef17146b4f4c21d0a3",
            "patch": "@@ -37,6 +37,13 @@ class CompressedTensorsHfQuantizer(HfQuantizer):\n \n     def __init__(self, quantization_config: CompressedTensorsConfig, **kwargs):\n         super().__init__(quantization_config, **kwargs)\n+\n+        if not is_compressed_tensors_available():\n+            raise ImportError(\n+                \"Using `compressed_tensors` quantized models requires the compressed-tensors library: \"\n+                \"`pip install compressed-tensors`\"\n+            )\n+\n         from compressed_tensors.compressors import ModelCompressor\n \n         self.compressor = ModelCompressor.from_compression_config(quantization_config)"
        },
        {
            "sha": "d47a2ba79cb60da847000f13a1de0527703ce0e9",
            "filename": "src/transformers/quantizers/quantizer_gptq.py",
            "status": "modified",
            "additions": 11,
            "deletions": 4,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/24c91f095fec4d90fa6901ef17146b4f4c21d0a3/src%2Ftransformers%2Fquantizers%2Fquantizer_gptq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24c91f095fec4d90fa6901ef17146b4f4c21d0a3/src%2Ftransformers%2Fquantizers%2Fquantizer_gptq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_gptq.py?ref=24c91f095fec4d90fa6901ef17146b4f4c21d0a3",
            "patch": "@@ -44,18 +44,25 @@ class GptqHfQuantizer(HfQuantizer):\n \n     def __init__(self, quantization_config: QuantizationConfigMixin, **kwargs):\n         super().__init__(quantization_config, **kwargs)\n+\n+        if not is_optimum_available():\n+            raise ImportError(\"Loading a GPTQ quantized model requires optimum (`pip install optimum`)\")\n         from optimum.gptq import GPTQQuantizer\n \n         self.optimum_quantizer = GPTQQuantizer.from_dict(self.quantization_config.to_dict_optimum())\n \n     def validate_environment(self, *args, **kwargs):\n+        if not is_optimum_available():\n+            raise ImportError(\"Loading a GPTQ quantized model requires optimum (`pip install optimum`)\")\n+\n+        if not is_auto_gptq_available():\n+            raise ImportError(\n+                \"Loading a GPTQ quantized model requires the auto-gptq library (`pip install auto-gptq`)\"\n+            )\n+\n         gptq_supports_cpu = version.parse(importlib.metadata.version(\"auto-gptq\")) > version.parse(\"0.4.2\")\n         if not gptq_supports_cpu and not torch.cuda.is_available():\n             raise RuntimeError(\"GPU is required to quantize or run quantize model.\")\n-        elif not (is_optimum_available() and is_auto_gptq_available()):\n-            raise ImportError(\n-                \"Loading a GPTQ quantized model requires optimum (`pip install optimum`) and auto-gptq library (`pip install auto-gptq`)\"\n-            )\n         elif version.parse(importlib.metadata.version(\"auto_gptq\")) < version.parse(\"0.4.2\"):\n             raise ImportError(\n                 \"You need a version of auto_gptq >= 0.4.2 to use GPTQ: `pip install --upgrade auto-gptq`\""
        }
    ],
    "stats": {
        "total": 22,
        "additions": 18,
        "deletions": 4
    }
}