{
    "author": "Cyrilvallez",
    "message": "âš ï¸âš ï¸ Use `dtype` instead of `torch_dtype` everywhere! (#39782)\n\n* update everywhere\n\n* style\n\n* pipelines\n\n* switch it everywhere in tests\n\n* switch it everywhere in docs\n\n* switch in converters everywhere\n\n* update in examples\n\n* update in model docstrings\n\n* style\n\n* warnings\n\n* style\n\n* Update configuration_utils.py\n\n* fix\n\n* Update configuration_utils.py\n\n* fixes and add first test\n\n* add pipeline tests\n\n* Update test_pipelines_common.py\n\n* add config test\n\n* Update test_modeling_common.py\n\n* add new ones\n\n* post rebase\n\n* add new\n\n* post rebase adds",
    "sha": "d8f6d3790acdd7033e199875b5b57691adf10735",
    "files": [
        {
            "sha": "5d782bcea78e58a96b5947a5085d8d5c72e5d276",
            "filename": "README.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/README.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/README.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/README.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -147,7 +147,7 @@ chat = [\n     {\"role\": \"user\", \"content\": \"Hey, can you tell me any fun things to do in New York?\"}\n ]\n \n-pipeline = pipeline(task=\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n+pipeline = pipeline(task=\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\", dtype=torch.bfloat16, device_map=\"auto\")\n response = pipeline(chat, max_new_tokens=512)\n print(response[0][\"generated_text\"][-1][\"content\"])\n ```"
        },
        {
            "sha": "b2dcf370455bb5f90251afc5f08b9656a0cadddc",
            "filename": "benchmark/benches/llama.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/benchmark%2Fbenches%2Fllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/benchmark%2Fbenches%2Fllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark%2Fbenches%2Fllama.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -106,12 +106,12 @@ def run_benchmark(\n \n         logger.info(\"downloading weights\")\n         # This is to avoid counting download in model load time measurement\n-        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)\n+        model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.float16)\n         gen_config = GenerationConfig(do_sample=False, top_p=1, temperature=1)\n         logger.info(\"loading model\")\n         start = perf_counter()\n         model = AutoModelForCausalLM.from_pretrained(\n-            model_id, torch_dtype=torch.float16, generation_config=gen_config\n+            model_id, dtype=torch.float16, generation_config=gen_config\n         ).eval()\n         model.to(device)\n         torch.cuda.synchronize()\n@@ -252,7 +252,7 @@ def sample(logits, temperature: float = 1.0, top_k: Optional[int] = None):\n \n         logger.info(\"compiling model\")\n \n-        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, generation_config=gen_config)\n+        model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.float16, generation_config=gen_config)\n         model.to(device)\n         model = torch.compile(model, mode=\"max-autotune\", fullgraph=True)\n "
        },
        {
            "sha": "2e197e5b321e0c2ffa83e0b3adabc22a414ed1c5",
            "filename": "benchmark/config/generation.yaml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/benchmark%2Fconfig%2Fgeneration.yaml",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/benchmark%2Fconfig%2Fgeneration.yaml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark%2Fconfig%2Fgeneration.yaml?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -19,7 +19,7 @@ backend:\n   model: meta-llama/Llama-2-7b-hf\n   cache_implementation: static\n   torch_compile: true\n-  torch_dtype: float16\n+  dtype: float16\n   torch_compile_config:\n     backend: inductor\n     mode: reduce-overhead"
        },
        {
            "sha": "0e05e1fedcbc6df769abdcc552f153904f336dc4",
            "filename": "docs/source/ar/chat_templating.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Far%2Fchat_templating.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Far%2Fchat_templating.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Fchat_templating.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -304,7 +304,7 @@ from transformers import AutoModelForCausalLM, AutoTokenizer\n checkpoint = \"NousResearch/Hermes-2-Pro-Llama-3-8B\"\n \n tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n-model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=torch.bfloat16, device_map=\"auto\")\n+model = AutoModelForCausalLM.from_pretrained(checkpoint, dtype=torch.bfloat16, device_map=\"auto\")\n \n ```python\n messages = ["
        },
        {
            "sha": "c3e320375dcdace7e20aa3da862e8da0bfbb11ee",
            "filename": "docs/source/ar/conversations.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Far%2Fconversations.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Far%2Fconversations.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Fconversations.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -25,7 +25,7 @@ chat = [\n import torch\n from transformers import pipeline\n \n-pipe = pipeline(\"text-generation\", \"meta-llama/Meta-Llama-3-8B-Instruct\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n+pipe = pipeline(\"text-generation\", \"meta-llama/Meta-Llama-3-8B-Instruct\", dtype=torch.bfloat16, device_map=\"auto\")\n response = pipe(chat, max_new_tokens=512)\n print(response[0]['generated_text'][-1]['content'])\n ```\n@@ -126,7 +126,7 @@ chat = [\n ]\n \n # 1: ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù…Ø­Ù„Ù„\n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", device_map=\"auto\", dtype=torch.bfloat16)\n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n \n # 2: ØªØ·Ø¨ÙŠÙ‚ Ù‚Ø§Ù„Ø¨ Ø§Ù„Ø¯Ø±Ø¯Ø´Ø©\n@@ -164,7 +164,7 @@ print(\"Decoded output:\\n\", decoded_output)\n \n ### Ø§Ø¹ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø©\n \n-Ø¨Ø´ÙƒÙ„ Ø§ÙØªØ±Ø§Ø¶ÙŠØŒ ØªÙ‚ÙˆÙ… ÙØ¦Ø§Øª Hugging Face Ù…Ø«Ù„ [`TextGenerationPipeline`] Ø£Ùˆ [`AutoModelForCausalLM`] Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ Ø¯Ù‚Ø© \"float32\". ÙˆÙ‡Ø°Ø§ ÙŠØ¹Ù†ÙŠ Ø£Ù†Ù‡ ÙŠØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ 4 Ø¨Ø§ÙŠØªØ§Øª (32 Ø¨Øª) Ù„ÙƒÙ„ Ù…Ø¹Ù„Ù…Ø©ØŒ Ù„Ø°Ø§ ÙØ¥Ù† Ù†Ù…ÙˆØ°Ø¬ \"8B\" Ø¨Ø­Ø¬Ù… 8 Ù…Ù„ÙŠØ§Ø± Ù…Ø¹Ù„Ù…Ø© Ø³ÙŠØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ~32 Ø¬ÙŠØ¬Ø§Ø¨Ø§ÙŠØª Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©. ÙˆÙ…Ø¹ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙƒÙˆÙ† Ù‡Ø°Ø§ Ù…Ø¶ÙŠØ¹Ø© Ù„Ù„Ù…ÙˆØ§Ø±Ø¯! ÙŠØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹Ø¸Ù… Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø­Ø¯ÙŠØ«Ø© ÙÙŠ Ø¯Ù‚Ø© \"bfloat16\"ØŒ ÙˆØ§Ù„ØªÙŠ ØªØ³ØªØ®Ø¯Ù… ÙÙ‚Ø· 2 Ø¨Ø§ÙŠØª Ù„ÙƒÙ„ Ù…Ø¹Ù„Ù…Ø©. Ø¥Ø°Ø§ ÙƒØ§Ù† Ø¹ØªØ§Ø¯Ùƒ ÙŠØ¯Ø¹Ù… Ø°Ù„Ùƒ (Nvidia 30xx/Axxx Ø£Ùˆ Ø£Ø­Ø¯Ø«)ØŒ ÙÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ Ø¯Ù‚Ø© \"bfloat16\"ØŒ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ù…Ù„ \"torch_dtype\" ÙƒÙ…Ø§ ÙØ¹Ù„Ù†Ø§ Ø£Ø¹Ù„Ø§Ù‡.\n+Ø¨Ø´ÙƒÙ„ Ø§ÙØªØ±Ø§Ø¶ÙŠØŒ ØªÙ‚ÙˆÙ… ÙØ¦Ø§Øª Hugging Face Ù…Ø«Ù„ [`TextGenerationPipeline`] Ø£Ùˆ [`AutoModelForCausalLM`] Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ Ø¯Ù‚Ø© \"float32\". ÙˆÙ‡Ø°Ø§ ÙŠØ¹Ù†ÙŠ Ø£Ù†Ù‡ ÙŠØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ 4 Ø¨Ø§ÙŠØªØ§Øª (32 Ø¨Øª) Ù„ÙƒÙ„ Ù…Ø¹Ù„Ù…Ø©ØŒ Ù„Ø°Ø§ ÙØ¥Ù† Ù†Ù…ÙˆØ°Ø¬ \"8B\" Ø¨Ø­Ø¬Ù… 8 Ù…Ù„ÙŠØ§Ø± Ù…Ø¹Ù„Ù…Ø© Ø³ÙŠØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ~32 Ø¬ÙŠØ¬Ø§Ø¨Ø§ÙŠØª Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©. ÙˆÙ…Ø¹ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙƒÙˆÙ† Ù‡Ø°Ø§ Ù…Ø¶ÙŠØ¹Ø© Ù„Ù„Ù…ÙˆØ§Ø±Ø¯! ÙŠØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹Ø¸Ù… Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø­Ø¯ÙŠØ«Ø© ÙÙŠ Ø¯Ù‚Ø© \"bfloat16\"ØŒ ÙˆØ§Ù„ØªÙŠ ØªØ³ØªØ®Ø¯Ù… ÙÙ‚Ø· 2 Ø¨Ø§ÙŠØª Ù„ÙƒÙ„ Ù…Ø¹Ù„Ù…Ø©. Ø¥Ø°Ø§ ÙƒØ§Ù† Ø¹ØªØ§Ø¯Ùƒ ÙŠØ¯Ø¹Ù… Ø°Ù„Ùƒ (Nvidia 30xx/Axxx Ø£Ùˆ Ø£Ø­Ø¯Ø«)ØŒ ÙÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ Ø¯Ù‚Ø© \"bfloat16\"ØŒ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ù…Ù„ \"dtype\" ÙƒÙ…Ø§ ÙØ¹Ù„Ù†Ø§ Ø£Ø¹Ù„Ø§Ù‡.\n \n ÙˆÙ…Ù† Ø§Ù„Ù…Ù…ÙƒÙ† Ø£ÙŠØ¶Ù‹Ø§ Ø§Ù„Ù†Ø²ÙˆÙ„ Ø¥Ù„Ù‰ Ø£Ù‚Ù„ Ù…Ù† 16 Ø¨Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… \"Ø§Ù„ØªÙƒÙ…ÙŠÙ…\"ØŒ ÙˆÙ‡ÙŠ Ø·Ø±ÙŠÙ‚Ø© Ù„Ø¶ØºØ· Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø·Ø±ÙŠÙ‚Ø© ØªÙÙ‚Ø¯ Ø¨Ø¹Ø¶ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª. ÙŠØ³Ù…Ø­ Ù‡Ø°Ø§ Ø¨Ø¶ØºØ· ÙƒÙ„ Ù…Ø¹Ù„Ù…Ø© Ø¥Ù„Ù‰ 8 Ø¨ØªØ§Øª Ø£Ùˆ 4 Ø¨ØªØ§Øª Ø£Ùˆ Ø­ØªÙ‰ Ø£Ù‚Ù„. Ù„Ø§Ø­Ø¸ Ø£Ù†Ù‡ØŒ Ø®Ø§ØµØ© ÙÙŠ 4 Ø¨ØªØ§ØªØŒ Ù‚Ø¯ ØªØªØ£Ø«Ø± Ø¬ÙˆØ¯Ø© Ù†Ø§ØªØ¬ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø³Ù„Ø¨Ù‹Ø§ØŒ ÙˆÙ„ÙƒÙ† ØºØ§Ù„Ø¨Ù‹Ø§ Ù…Ø§ ÙŠÙƒÙˆÙ† Ù‡Ø°Ø§ Ù…Ù‚Ø§ÙŠØ¶Ø© ØªØ³ØªØ­Ù‚ Ø§Ù„Ù‚ÙŠØ§Ù… Ø¨Ù‡Ø§ Ù„ØªÙ†Ø§Ø³Ø¨ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­Ø§Ø¯Ø«Ø© Ø£ÙƒØ¨Ø± ÙˆØ£ÙƒØ«Ø± Ù‚Ø¯Ø±Ø© ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©. Ø¯Ø¹Ù†Ø§ ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†Ø§ ØªØ·Ø¨ÙŠÙ‚ Ø°Ù„Ùƒ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø© `bitsandbytes`:\n "
        },
        {
            "sha": "400c17f735c5b25d480fb9228b89ebd844f38df6",
            "filename": "docs/source/ar/llm_tutorial_optimization.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Far%2Fllm_tutorial_optimization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Far%2Fllm_tutorial_optimization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Fllm_tutorial_optimization.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -73,7 +73,7 @@ model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom\", device_map=\"aut\n from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n import torch\n \n-model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", torch_dtype=torch.bfloat16, device_map=\"auto\", pad_token_id=0)\n+model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", dtype=torch.bfloat16, device_map=\"auto\", pad_token_id=0)\n tokenizer = AutoTokenizer.from_pretrained(\"bigcode/octocoder\")\n \n pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n@@ -114,7 +114,7 @@ bytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n \n > ÙŠØªÙ… ØªØ¯Ø±ÙŠØ¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ØªÙ‚Ø±ÙŠØ¨Ù‹Ø§ Ø¨ØªÙ†Ø³ÙŠÙ‚ bfloat16 ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø­Ø§Ù„ÙŠØŒ ÙˆÙ„Ø§ ÙŠÙˆØ¬Ø¯ Ø³Ø¨Ø¨ Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© float32 Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ø¥Ø°Ø§ [ÙƒØ§Ù†Øª ÙˆØ­Ø¯Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª (GPU) Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ ØªØ¯Ø¹Ù… bfloat16](https://discuss.pytorch.org/t/bfloat16-native-support/117155/5). Ù„Ù† ØªÙˆÙØ± Ø¯Ù‚Ø© float32 Ù†ØªØ§Ø¦Ø¬ Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø£ÙØ¶Ù„ Ù…Ù† Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„ØªÙŠ ØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬.\n \n-Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…ØªØ£ÙƒØ¯Ù‹Ø§ Ù…Ù† ØªÙ†Ø³ÙŠÙ‚ ØªØ®Ø²ÙŠÙ† Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ HubØŒ ÙÙŠÙ…ÙƒÙ†Ùƒ Ø¯Ø§Ø¦Ù…Ù‹Ø§ Ø§Ù„Ø§Ø·Ù„Ø§Ø¹ Ø¹Ù„Ù‰ ØªÙ‡ÙŠØ¦Ø© Ù†Ù‚Ø·Ø© Ø§Ù„ØªÙØªÙŠØ´ ÙÙŠ `\"torch_dtype\"`ØŒ Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ [Ù‡Ù†Ø§](https://huggingface.co/meta-llama/Llama-2-7b-hf/blob/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9/config.json#L21). ÙŠÙˆØµÙ‰ Ø¨ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ Ù†ÙØ³ Ù†ÙˆØ¹ Ø§Ù„Ø¯Ù‚Ø© ÙƒÙ…Ø§ Ù‡Ùˆ Ù…ÙƒØªÙˆØ¨ ÙÙŠ Ø§Ù„ØªÙ‡ÙŠØ¦Ø© Ø¹Ù†Ø¯ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `from_pretrained(..., torch_dtype=...)` Ø¥Ù„Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†ÙˆØ¹ Ø§Ù„Ø£ØµÙ„ÙŠ Ù‡Ùˆ float32ØŒ ÙˆÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø© ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… `float16` Ø£Ùˆ `bfloat16` Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„.\n+Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…ØªØ£ÙƒØ¯Ù‹Ø§ Ù…Ù† ØªÙ†Ø³ÙŠÙ‚ ØªØ®Ø²ÙŠÙ† Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ HubØŒ ÙÙŠÙ…ÙƒÙ†Ùƒ Ø¯Ø§Ø¦Ù…Ù‹Ø§ Ø§Ù„Ø§Ø·Ù„Ø§Ø¹ Ø¹Ù„Ù‰ ØªÙ‡ÙŠØ¦Ø© Ù†Ù‚Ø·Ø© Ø§Ù„ØªÙØªÙŠØ´ ÙÙŠ `\"dtype\"`ØŒ Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ [Ù‡Ù†Ø§](https://huggingface.co/meta-llama/Llama-2-7b-hf/blob/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9/config.json#L21). ÙŠÙˆØµÙ‰ Ø¨ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ Ù†ÙØ³ Ù†ÙˆØ¹ Ø§Ù„Ø¯Ù‚Ø© ÙƒÙ…Ø§ Ù‡Ùˆ Ù…ÙƒØªÙˆØ¨ ÙÙŠ Ø§Ù„ØªÙ‡ÙŠØ¦Ø© Ø¹Ù†Ø¯ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `from_pretrained(..., dtype=...)` Ø¥Ù„Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†ÙˆØ¹ Ø§Ù„Ø£ØµÙ„ÙŠ Ù‡Ùˆ float32ØŒ ÙˆÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø© ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… `float16` Ø£Ùˆ `bfloat16` Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„.\n \n \n Ø¯Ø¹ÙˆÙ†Ø§ Ù†Ø­Ø¯Ø¯ ÙˆØ¸ÙŠÙØ© `flush(...)` Ù„ØªØ­Ø±ÙŠØ± Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø®ØµØµØ© Ø¨Ø­ÙŠØ« ÙŠÙ…ÙƒÙ†Ù†Ø§ Ù‚ÙŠØ§Ø³ Ø°Ø±ÙˆØ© Ø°Ø§ÙƒØ±Ø© ÙˆØ­Ø¯Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª (GPU) Ø§Ù„Ù…Ø®ØµØµØ© Ø¨Ø¯Ù‚Ø©.\n@@ -389,7 +389,7 @@ long_prompt = 10 * system_prompt + prompt\n Ù†Ù‚ÙˆÙ… Ø¨ØªÙ†ÙÙŠØ° Ù†Ù…ÙˆØ°Ø¬Ù†Ø§ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø¨Ø¯Ù‚Ø© bfloat16.\n \n ```python\n-model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n+model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", dtype=torch.bfloat16, device_map=\"auto\")\n tokenizer = AutoTokenizer.from_pretrained(\"bigcode/octocoder\")\n \n pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
        },
        {
            "sha": "4f71ebb95fa6867ef43a6924c3db7f8a475c4bdf",
            "filename": "docs/source/ar/pipeline_tutorial.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Far%2Fpipeline_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Far%2Fpipeline_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Fpipeline_tutorial.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -90,7 +90,7 @@ out = transcriber(...)  # Ø³ÙŠØªÙ… Ø§Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… `my_parame\n transcriber = pipeline(model=\"openai/whisper-large-v2\", device=0)\n ```\n \n-Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙƒØ¨ÙŠØ±Ù‹Ø§ Ø¬Ø¯Ù‹Ø§ Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„ÙˆØ­Ø¯Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª (GPU) ÙˆØ§Ø­Ø¯Ø©ØŒ ÙˆØ£Ù†Øª ØªØ³ØªØ®Ø¯Ù… PyTorchØŒ ÙÙŠÙ…ÙƒÙ†Ùƒ ØªØ¹ÙŠÙŠÙ† `torch_dtype='float16'` Ù„ØªÙ…ÙƒÙŠÙ† Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø¨Ø¯Ù‚Ø© FP16. Ø¹Ø§Ø¯Ø©Ù‹ Ù…Ø§ Ù„Ø§ ÙŠØªØ³Ø¨Ø¨ Ø°Ù„Ùƒ ÙÙŠ Ø­Ø¯ÙˆØ« Ø§Ù†Ø®ÙØ§Ø¶Ø§Øª ÙƒØ¨ÙŠØ±Ø© ÙÙŠ Ø§Ù„Ø£Ø¯Ø§Ø¡ØŒ ÙˆÙ„ÙƒÙ† ØªØ£ÙƒØ¯ Ù…Ù† ØªÙ‚ÙŠÙŠÙ…Ù‡ Ø¹Ù„Ù‰ Ù†Ù…Ø§Ø°Ø¬Ùƒ!\n+Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙƒØ¨ÙŠØ±Ù‹Ø§ Ø¬Ø¯Ù‹Ø§ Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„ÙˆØ­Ø¯Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª (GPU) ÙˆØ§Ø­Ø¯Ø©ØŒ ÙˆØ£Ù†Øª ØªØ³ØªØ®Ø¯Ù… PyTorchØŒ ÙÙŠÙ…ÙƒÙ†Ùƒ ØªØ¹ÙŠÙŠÙ† `dtype='float16'` Ù„ØªÙ…ÙƒÙŠÙ† Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø¨Ø¯Ù‚Ø© FP16. Ø¹Ø§Ø¯Ø©Ù‹ Ù…Ø§ Ù„Ø§ ÙŠØªØ³Ø¨Ø¨ Ø°Ù„Ùƒ ÙÙŠ Ø­Ø¯ÙˆØ« Ø§Ù†Ø®ÙØ§Ø¶Ø§Øª ÙƒØ¨ÙŠØ±Ø© ÙÙŠ Ø§Ù„Ø£Ø¯Ø§Ø¡ØŒ ÙˆÙ„ÙƒÙ† ØªØ£ÙƒØ¯ Ù…Ù† ØªÙ‚ÙŠÙŠÙ…Ù‡ Ø¹Ù„Ù‰ Ù†Ù…Ø§Ø°Ø¬Ùƒ!\n \n Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¹ÙŠÙŠÙ† `device_map=\"auto\"` Ù„ØªØ­Ø¯ÙŠØ¯ ÙƒÙŠÙÙŠØ© ØªØ­Ù…ÙŠÙ„ Ù…Ø®Ø²Ù†Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØªØ®Ø²ÙŠÙ†Ù‡Ø§ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§. ÙŠØªØ·Ù„Ø¨ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ù…Ù„ `device_map` Ù…ÙƒØªØ¨Ù‡ ðŸ¤— [Accelerate](https://huggingface.co/docs/accelerate):\n \n@@ -273,7 +273,7 @@ pip install pytesseract\n import torch\n from transformers import pipeline\n \n-pipe = pipeline(model=\"facebook/opt-1.3b\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n+pipe = pipeline(model=\"facebook/opt-1.3b\", dtype=torch.bfloat16, device_map=\"auto\")\n output = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\n ```\n "
        },
        {
            "sha": "3c754a188e7ddb055896ea7184b6cb5122604227",
            "filename": "docs/source/en/cache_explanation.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fcache_explanation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fcache_explanation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fcache_explanation.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -104,7 +104,7 @@ from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache, infe\n device = f\"{infer_device()}:0\"\n \n model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n-model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=device)\n+model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16, device_map=device)\n tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n past_key_values = DynamicCache()\n@@ -150,7 +150,7 @@ from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache, infe\n device = f\"{infer_device()}:0\"\n \n model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n-model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=device)\n+model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16, device_map=device)\n tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n messages = [{\"role\": \"user\", \"content\": \"You are a helpful assistant.\"}]\n@@ -176,7 +176,7 @@ import torch\n from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", dtype=torch.float16, device_map=\"auto\")\n inputs = tokenizer(\"Hello, my name is\", return_tensors=\"pt\").to(model.device)\n \n # `return_dict_in_generate=True` is required to return the cache and `return_legacy_cache` forces the returned cache"
        },
        {
            "sha": "e954022692fda58eb7ce0c3fdbd23390934fcbd6",
            "filename": "docs/source/en/chat_extras.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fchat_extras.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fchat_extras.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_extras.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -68,7 +68,7 @@ from transformers import AutoModelForCausalLM, AutoTokenizer\n \n tokenizer = AutoTokenizer.from_pretrained( \"NousResearch/Hermes-2-Pro-Llama-3-8B\")\n tokenizer = AutoTokenizer.from_pretrained( \"NousResearch/Hermes-2-Pro-Llama-3-8B\")\n-model = AutoModelForCausalLM.from_pretrained( \"NousResearch/Hermes-2-Pro-Llama-3-8B\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n+model = AutoModelForCausalLM.from_pretrained( \"NousResearch/Hermes-2-Pro-Llama-3-8B\", dtype=torch.bfloat16, device_map=\"auto\")\n ```\n \n Create a chat message."
        },
        {
            "sha": "ea272193925deb15a40026c6c9844ebef03a3f33",
            "filename": "docs/source/en/chat_templating.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fchat_templating.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fchat_templating.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_templating.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -74,7 +74,7 @@ import torch\n from transformers import AutoModelForCausalLM, AutoTokenizer\n \n tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\n-model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n+model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\", device_map=\"auto\", dtype=torch.bfloat16)\n \n messages = [\n     {\"role\": \"system\", \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",},"
        },
        {
            "sha": "696b2ba0edf9636fefd2517a355cb92129ab4662",
            "filename": "docs/source/en/chat_templating_multimodal.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fchat_templating_multimodal.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fchat_templating_multimodal.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_templating_multimodal.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -56,7 +56,7 @@ Create a [`ImageTextToTextPipeline`] and pass the chat to it. For large models,\n import torch\n from transformers import pipeline\n \n-pipeline = pipeline(\"image-text-to-text\", model=\"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\", device_map=\"auto\", torch_dtype=torch.float16)\n+pipeline = pipeline(\"image-text-to-text\", model=\"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\", device_map=\"auto\", dtype=torch.float16)\n pipeline(text=messages, max_new_tokens=50, return_full_text=False)\n [{'input_text': [{'role': 'system',\n     'content': [{'type': 'text',"
        },
        {
            "sha": "5def03324e8add30ea0a1cc9d96c20917702b230",
            "filename": "docs/source/en/conversations.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fconversations.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fconversations.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fconversations.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -74,7 +74,7 @@ Create the [`TextGenerationPipeline`] and pass `chat` to it. For large models, s\n import torch\n from transformers import pipeline\n \n-pipeline = pipeline(task=\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n+pipeline = pipeline(task=\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\", dtype=torch.bfloat16, device_map=\"auto\")\n response = pipeline(chat, max_new_tokens=512)\n print(response[0][\"generated_text\"][-1][\"content\"])\n ```"
        },
        {
            "sha": "87ae0296e09cb2227fe40a7f07061723b9480d0a",
            "filename": "docs/source/en/deepspeed.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fdeepspeed.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fdeepspeed.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fdeepspeed.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -260,7 +260,7 @@ with deepspeed.zero.Init():\n The DeepSped config file needs to have `is_deepspeed_zero3_enabled: true` setup in [`TrainingArguments`] and it needs a ZeRO configuration enabled. The [`TrainingArguments`] object must be created **before** calling [`~PreTrainedModel.from_pretrained`].\n \n > [!TIP]\n-> You'll need ZeRO-3 when the fp16 weights don't fit on a single GPU. But if you're able to load the fp16 weights, set `torch_dtype=torch.float16` in [`~PreTrainedModel.from_pretrained`].\n+> You'll need ZeRO-3 when the fp16 weights don't fit on a single GPU. But if you're able to load the fp16 weights, set `dtype=torch.float16` in [`~PreTrainedModel.from_pretrained`].\n \n ```py\n from transformers import AutoModel, Trainer, TrainingArguments"
        },
        {
            "sha": "d9b736fcd66db52f51b8b2b92ba176a579a2c0f6",
            "filename": "docs/source/en/executorch.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fexecutorch.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fexecutorch.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fexecutorch.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -38,7 +38,7 @@ generation_config = GenerationConfig(\n )\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\", pad_token=\"</s>\", padding_side=\"right\")\n-model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\", device_map=\"auto\", torch_dtype=torch.bfloat16, attn_implementation=\"sdpa\", generation_config=generation_config)\n+model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\", device_map=\"auto\", dtype=torch.bfloat16, attn_implementation=\"sdpa\", generation_config=generation_config)\n \n exported_program = convert_and_export_with_cache(model)\n ```"
        },
        {
            "sha": "fd426bb1d30b14a6aa6e8b5def0982b88305fdf8",
            "filename": "docs/source/en/generation_strategies.md",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgeneration_strategies.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -39,7 +39,7 @@ device = infer_device()\n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(device)\n \n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16).to(device)\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", dtype=torch.float16).to(device)\n # explicitly set to default length because Llama2 generation length is 4096\n outputs = model.generate(**inputs, max_new_tokens=20)\n tokenizer.batch_decode(outputs, skip_special_tokens=True)\n@@ -61,7 +61,7 @@ device = infer_device()\n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(device)\n \n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16).to(device)\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", dtype=torch.float16).to(device)\n # explicitly set to 100 because Llama2 generation length is 4096\n outputs = model.generate(**inputs, max_new_tokens=50, do_sample=True, num_beams=1)\n tokenizer.batch_decode(outputs, skip_special_tokens=True)\n@@ -86,7 +86,7 @@ device = infer_device()\n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(device)\n \n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16).to(device)\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", dtype=torch.float16).to(device)\n # explicitly set to 100 because Llama2 generation length is 4096\n outputs = model.generate(**inputs, max_new_tokens=50, num_beams=2)\n tokenizer.batch_decode(outputs, skip_special_tokens=True)\n@@ -131,7 +131,7 @@ pipe = pipeline(\n     \"text-generation\",\n     model=\"meta-llama/Llama-3.1-8B\",\n     assistant_model=\"meta-llama/Llama-3.2-1B\",\n-    torch_dtype=torch.bfloat16\n+    dtype=torch.bfloat16\n )\n pipe_output = pipe(\"Once upon a time, \", max_new_tokens=50, do_sample=False)\n pipe_output[0][\"generated_text\"]\n@@ -171,8 +171,8 @@ from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device\n device = infer_device()\n \n tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\")\n-model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\", torch_dtype=torch.float16).to(device)\n-assistant_model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-135M\", torch_dtype=torch.float16).to(device)\n+model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\", dtype=torch.float16).to(device)\n+assistant_model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-135M\", dtype=torch.float16).to(device)\n inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(device)\n \n outputs = model.generate(**inputs, assistant_model=assistant_model, max_new_tokens=20, prompt_lookup_num_tokens=5)\n@@ -241,7 +241,7 @@ device = infer_device()\n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(device)\n \n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16).to(device)\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", dtype=torch.float16).to(device)\n # explicitly set to 100 because Llama2 generation length is 4096\n outputs = model.generate(**inputs, max_new_tokens=100, penalty_alpha=0.6, top_k=4)\n tokenizer.batch_decode(outputs, skip_special_tokens=True)\n@@ -277,7 +277,7 @@ from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device\n device = infer_device()\n \n tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\")\n-model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\", torch_dtype=torch.float16).to(device)\n+model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\", dtype=torch.float16).to(device)\n inputs = tokenizer(\"What is the highest peak in the world??\", return_tensors=\"pt\").to(device)\n \n outputs = model.generate(**inputs, max_new_tokens=50, dola_layers=\"high\", do_sample=False)\n@@ -297,7 +297,7 @@ from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device\n device = infer_device()\n \n tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\")\n-model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\", torch_dtype=torch.float16).to(device)\n+model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\", dtype=torch.float16).to(device)\n inputs = tokenizer(\"What is the highest peak in the world?\", return_tensors=\"pt\").to(device)\n \n outputs = model.generate(**inputs, max_new_tokens=50, dola_layers=[18,20], do_sample=False, repetition_penalty=1.2)\n@@ -323,7 +323,7 @@ device = infer_device()\n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(device)\n \n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16).to(device)\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", dtype=torch.float16).to(device)\n # explicitly set to 100 because Llama2 generation length is 4096\n outputs = model.generate(**inputs, max_new_tokens=50, num_beams=6, num_beam_groups=3, diversity_penalty=1.0, do_sample=False)\n tokenizer.batch_decode(outputs, skip_special_tokens=True)"
        },
        {
            "sha": "a77a9e96b60ee2731890058cad7c40eac8f5d869",
            "filename": "docs/source/en/gguf.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fgguf.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fgguf.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgguf.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -38,9 +38,9 @@ from transformers import AutoTokenizer, AutoModelForCausalLM\n model_id = \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\"\n filename = \"tinyllama-1.1b-chat-v1.0.Q6_K.gguf\"\n \n-torch_dtype = torch.float32 # could be torch.float16 or torch.bfloat16 too\n+dtype = torch.float32 # could be torch.float16 or torch.bfloat16 too\n tokenizer = AutoTokenizer.from_pretrained(model_id, gguf_file=filename)\n-model = AutoModelForCausalLM.from_pretrained(model_id, gguf_file=filename, torch_dtype=torch_dtype)\n+model = AutoModelForCausalLM.from_pretrained(model_id, gguf_file=filename, dtype=dtype)\n ```\n \n Once you're done tinkering with the model, save and convert it back to the GGUF format with the [convert-hf-to-gguf.py](https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py) script."
        },
        {
            "sha": "4795c054ce4efe69aaf7f62089d9b4151a852ef6",
            "filename": "docs/source/en/kv_cache.md",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fkv_cache.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fkv_cache.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fkv_cache.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -44,7 +44,7 @@ import torch\n from transformers import AutoTokenizer, AutoModelForCausalLM\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", dtype=torch.float16, device_map=\"auto\")\n inputs = tokenizer(\"I like rock music because\", return_tensors=\"pt\").to(model.device)\n \n model.generate(**inputs, do_sample=False, max_new_tokens=20, use_cache=False)\n@@ -59,7 +59,7 @@ import torch\n from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", dtype=torch.float16, device_map=\"auto\")\n inputs = tokenizer(\"I like rock music because\", return_tensors=\"pt\").to(model.device)\n \n past_key_values = DynamicCache()\n@@ -87,7 +87,7 @@ from transformers import AutoTokenizer, AutoModelForCausalLM\n \n ckpt = \"microsoft/Phi-3-mini-4k-instruct\"\n tokenizer = AutoTokenizer.from_pretrained(ckpt)\n-model = AutoModelForCausalLM.from_pretrained(ckpt, torch_dtype=torch.float16, device_map=\"auto\")\n+model = AutoModelForCausalLM.from_pretrained(ckpt, dtype=torch.float16, device_map=\"auto\")\n inputs = tokenizer(\"Fun fact: The shortest\", return_tensors=\"pt\").to(model.device)\n \n out = model.generate(**inputs, do_sample=False, max_new_tokens=23, cache_implementation=\"offloaded\")\n@@ -118,7 +118,7 @@ def resilient_generate(model, *args, **kwargs):\n \n ckpt = \"microsoft/Phi-3-mini-4k-instruct\"\n tokenizer = AutoTokenizer.from_pretrained(ckpt)\n-model = AutoModelForCausalLM.from_pretrained(ckpt, torch_dtype=torch.float16, device_map=\"auto\")\n+model = AutoModelForCausalLM.from_pretrained(ckpt, dtype=torch.float16, device_map=\"auto\")\n prompt = [\"okay \"*1000 + \"Fun fact: The most\"]\n inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n beams = { \"num_beams\": 40, \"num_beam_groups\": 40, \"num_return_sequences\": 40, \"diversity_penalty\": 1.0, \"max_new_tokens\": 23, \"early_stopping\": True, }\n@@ -148,7 +148,7 @@ import torch\n from transformers import AutoTokenizer, AutoModelForCausalLM, HQQQuantizedCache\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", dtype=torch.float16, device_map=\"auto\")\n inputs = tokenizer(\"I like rock music because\", return_tensors=\"pt\").to(model.device)\n \n out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation=\"quantized\", cache_config={\"backend\": \"HQQ\"})\n@@ -166,7 +166,7 @@ import torch\n from transformers import AutoTokenizer, AutoModelForCausalLM, QuantoQuantizedCache\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", dtype=torch.float16, device_map=\"auto\")\n inputs = tokenizer(\"I like rock music because\", return_tensors=\"pt\").to(model.device)\n \n out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation=\"quantized\", cache_config={\"nbits\": 4, \"backend\": \"quanto\"})\n@@ -192,7 +192,7 @@ import torch\n from transformers import AutoTokenizer, AutoModelForCausalLM\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", dtype=torch.float16, device_map=\"auto\")\n inputs = tokenizer(\"Hello, my name is\", return_tensors=\"pt\").to(model.device)\n \n out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation=\"static\")\n@@ -211,7 +211,7 @@ import torch\n from transformers import AutoTokenizer, AutoModelForCausalLM\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16, device_map={\"\": 0})\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", dtype=torch.float16, device_map={\"\": 0})\n inputs = tokenizer(\"Hello, my name is\", return_tensors=\"pt\").to(model.device)\n \n out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation=\"offloaded_static\")\n@@ -231,7 +231,7 @@ import torch\n from transformers import AutoTokenizer, AutoModelForCausalLM\n \n tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n-model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", torch_dtype=torch.float16, device_map=\"auto\")\n+model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", dtype=torch.float16, device_map=\"auto\")\n inputs = tokenizer(\"Yesterday I was on a rock concert and.\", return_tensors=\"pt\").to(model.device)\n \n out = model.generate(**inputs, do_sample=False, max_new_tokens=30, cache_implementation=\"sliding_window\")\n@@ -280,7 +280,7 @@ from transformers.cache_utils import (\n )\n \n model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n-model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map='auto')\n+model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16, device_map='auto')\n tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n user_prompts = [\"Hello, what's your name?\", \"Btw, yesterday I was on a rock concert.\"]\n@@ -309,7 +309,7 @@ import torch\n from transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache, StaticCache\n \n model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n-model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map={\"\": 0})\n+model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16, device_map={\"\": 0})\n tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n # Init StaticCache with big enough max-length (1024 tokens for the below example)"
        },
        {
            "sha": "e1043327bc4ad5588817841ce24d1f1b65332125",
            "filename": "docs/source/en/llm_optims.md",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fllm_optims.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fllm_optims.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_optims.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -53,7 +53,7 @@ import os\n os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # To prevent long warnings :)\n \n tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n-model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", torch_dtype=\"auto\", device_map=\"auto\")\n+model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", dtype=\"auto\", device_map=\"auto\")\n \n model.generation_config.cache_implementation = \"static\"\n \n@@ -83,7 +83,7 @@ import os\n os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # To prevent long warnings :)\n \n tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n-model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", torch_dtype=\"auto\", device_map=\"auto\")\n+model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", dtype=\"auto\", device_map=\"auto\")\n \n model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n input_text = \"The theory of special relativity states \"\n@@ -195,7 +195,7 @@ import os\n os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # To prevent long warnings :)\n \n tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n-model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", torch_dtype=\"auto\", device_map=\"auto\")\n+model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", dtype=\"auto\", device_map=\"auto\")\n \n model.generate = torch.compile(model.generate, mode=\"reduce-overhead\", fullgraph=True)\n input_text = \"The theory of special relativity states \"\n@@ -246,7 +246,7 @@ device = infer_device()\n tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n inputs = tokenizer(\"Einstein's theory of relativity states\", return_tensors=\"pt\").to(device)\n \n-model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\", torch_dtype=\"auto\").to(device)\n+model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\", dtype=\"auto\").to(device)\n assistant_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\").to(device)\n outputs = model.generate(**inputs, assistant_model=assistant_model)\n tokenizer.batch_decode(outputs, skip_special_tokens=True)\n@@ -267,7 +267,7 @@ device = infer_device()\n tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n inputs = tokenizer(\"Einstein's theory of relativity states\", return_tensors=\"pt\").to(device)\n \n-model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\", torch_dtype=\"auto\").to(device)\n+model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\", dtype=\"auto\").to(device)\n assistant_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\").to(device)\n outputs = model.generate(**inputs, assistant_model=assistant_model, do_sample=True, temperature=0.7)\n print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n@@ -295,7 +295,7 @@ device = infer_device()\n tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n inputs = tokenizer(\"The second law of thermodynamics states\", return_tensors=\"pt\").to(device)\n \n-model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\", torch_dtype=\"auto\").to(device)\n+model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\", dtype=\"auto\").to(device)\n assistant_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\").to(device)\n outputs = model.generate(**inputs, prompt_lookup_num_tokens=3)\n print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n@@ -316,7 +316,7 @@ device = infer_device()\n tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n inputs = tokenizer(\"The second law of thermodynamics states\", return_tensors=\"pt\").to(device)\n \n-model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\", torch_dtype=\"auto\").to(device)\n+model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\", dtype=\"auto\").to(device)\n outputs = model.generate(**inputs, prompt_lookup_num_tokens=3, do_sample=True, temperature=0.7)\n print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n [\"The second law of thermodynamics states that energy cannot be created nor destroyed. It's not a\"]\n@@ -342,15 +342,15 @@ quant_config = BitsAndBytesConfig(load_in_8bit=True)\n model = AutoModelForCausalLM.from_pretrained(\n     \"google/gemma-2b\",\n     quantization_config=quant_config,\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     attn_implementation=\"flash_attention_2\",\n )\n \n # Change the model's attention dynamically after loading\n model = AutoModelForCausalLM.from_pretrained(\n     \"google/gemma-2b\",\n     quantization_config=quant_config,\n-    torch_dtype=torch.bfloat16\n+    dtype=torch.bfloat16\n )\n model.set_attention_implementation(\"flash_attention_2\")\n ```\n@@ -371,7 +371,7 @@ from transformers import AutoModelForCausalLM\n \n model = AutoModelForCausalLM.from_pretrained(\n     \"google/gemma-2b\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n )\n \n with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n@@ -396,14 +396,14 @@ Use the Model Memory Calculator below to estimate and compare how much memory is\n \theight=\"450\"\n ></iframe>\n \n-To load a model in half-precision, set the [torch_dtype](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.PreTrainedModel.from_pretrained.torch_dtype) parameter in [`~transformers.AutoModelForCausalLM.from_pretrained`] to `torch.bfloat16`. This requires 13.74GB of memory.\n+To load a model in half-precision, set the [dtype](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.PreTrainedModel.from_pretrained.dtype) parameter in [`~transformers.AutoModelForCausalLM.from_pretrained`] to `torch.bfloat16`. This requires 13.74GB of memory.\n \n ```py\n from transformers import AutoTokenizer, AutoModelForCausalLM\n import torch\n \n model = AutoModelForCausalLM.from_pretrained(\n-    \"mistralai/Mistral-7B-v0.1\", torch_dtype=torch.bfloat16, device_map=\"auto\",\n+    \"mistralai/Mistral-7B-v0.1\", dtype=torch.bfloat16, device_map=\"auto\",\n )\n ```\n "
        },
        {
            "sha": "63d9308a84f4fba5ef2f384ac3650f5dcdbedee8",
            "filename": "docs/source/en/llm_tutorial_optimization.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -84,7 +84,7 @@ We first load the model and tokenizer and then pass both to Transformers' [pipel\n from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n import torch\n \n-model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", torch_dtype=torch.bfloat16, device_map=\"auto\", pad_token_id=0)\n+model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", dtype=torch.bfloat16, device_map=\"auto\", pad_token_id=0)\n tokenizer = AutoTokenizer.from_pretrained(\"bigcode/octocoder\")\n \n pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n@@ -125,7 +125,7 @@ Note that if we had tried to run the model in full float32 precision, a whopping\n \n > Almost all models are trained in bfloat16 nowadays, there is no reason to run the model in full float32 precision if [your GPU supports bfloat16](https://discuss.pytorch.org/t/bfloat16-native-support/117155/5). Float32 won't give better inference results than the precision that was used to train the model.\n \n-If you are unsure in which format the model weights are stored on the Hub, you can always look into the checkpoint's config under `\"torch_dtype\"`, *e.g.* [here](https://huggingface.co/meta-llama/Llama-2-7b-hf/blob/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9/config.json#L21). It is recommended to set the model to the same precision type as written in the config when loading with `from_pretrained(..., torch_dtype=...)` except when the original type is float32 in which case one can use both `float16` or `bfloat16` for inference.\n+If you are unsure in which format the model weights are stored on the Hub, you can always look into the checkpoint's config under `\"dtype\"`, *e.g.* [here](https://huggingface.co/meta-llama/Llama-2-7b-hf/blob/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9/config.json#L21). It is recommended to set the model to the same precision type as written in the config when loading with `from_pretrained(..., dtype=...)` except when the original type is float32 in which case one can use both `float16` or `bfloat16` for inference.\n \n \n Let's define a `flush(...)` function to free all allocated memory so that we can accurately measure the peak allocated GPU memory.\n@@ -394,7 +394,7 @@ long_prompt = 10 * system_prompt + prompt\n We instantiate our model again in bfloat16 precision.\n \n ```python\n-model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n+model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", dtype=torch.bfloat16, device_map=\"auto\")\n tokenizer = AutoTokenizer.from_pretrained(\"bigcode/octocoder\")\n \n pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
        },
        {
            "sha": "6d3bdb3f4b7cc698955bdd656288fbcc1913fd6f",
            "filename": "docs/source/en/main_classes/pipelines.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmain_classes%2Fpipelines.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmain_classes%2Fpipelines.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fpipelines.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -273,7 +273,7 @@ independently of the inputs. The caveats from the previous section still apply.\n ## Pipeline FP16 inference\n Models can be run in FP16 which can be significantly faster on GPU while saving memory. Most models will not suffer noticeable performance loss from this. The larger the model, the less likely that it will.\n \n-To enable FP16 inference, you can simply pass `torch_dtype=torch.float16` or `torch_dtype='float16'` to the pipeline constructor. Note that this only works for models with a PyTorch backend. Your inputs will be converted to FP16 internally.\n+To enable FP16 inference, you can simply pass `dtype=torch.float16` or `dtype='float16'` to the pipeline constructor. Note that this only works for models with a PyTorch backend. Your inputs will be converted to FP16 internally.\n \n ## Pipeline custom code\n "
        },
        {
            "sha": "5560289da444992b6558a269ca20332a0294c5ae",
            "filename": "docs/source/en/model_doc/albert.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Falbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Falbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Falbert.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -50,7 +50,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"fill-mask\",\n     model=\"albert-base-v2\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n pipeline(\"Plants create [MASK] through a process known as photosynthesis.\", top_k=5)\n@@ -66,7 +66,7 @@ from transformers import AutoModelForMaskedLM, AutoTokenizer\n tokenizer = AutoTokenizer.from_pretrained(\"albert/albert-base-v2\")\n model = AutoModelForMaskedLM.from_pretrained(\n     \"albert/albert-base-v2\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     attn_implementation=\"sdpa\",\n     device_map=\"auto\"\n )"
        },
        {
            "sha": "7379c84fc3a9a0d9027614d66d3de0b1b7979f99",
            "filename": "docs/source/en/model_doc/align.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Falign.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Falign.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Falign.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -44,7 +44,7 @@ pipeline = pipeline(\n     task=\"zero-shot-image-classification\",\n     model=\"kakaobrain/align-base\",\n     device=0,\n-    torch_dtype=torch.bfloat16\n+    dtype=torch.bfloat16\n )\n \n candidate_labels = ["
        },
        {
            "sha": "4e3ae60587920dcc05259095adde607dec37c830",
            "filename": "docs/source/en/model_doc/altclip.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Faltclip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Faltclip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Faltclip.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -40,7 +40,7 @@ import requests\n from PIL import Image\n from transformers import AltCLIPModel, AltCLIPProcessor\n \n-model = AltCLIPModel.from_pretrained(\"BAAI/AltCLIP\", torch_dtype=torch.bfloat16)\n+model = AltCLIPModel.from_pretrained(\"BAAI/AltCLIP\", dtype=torch.bfloat16)\n processor = AltCLIPProcessor.from_pretrained(\"BAAI/AltCLIP\")\n \n url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n@@ -74,7 +74,7 @@ from transformers import AltCLIPModel, AltCLIPProcessor, TorchAoConfig\n model = AltCLIPModel.from_pretrained(\n     \"BAAI/AltCLIP\",\n     quantization_config=TorchAoConfig(\"int4_weight_only\", group_size=128),\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n )\n \n processor = AltCLIPProcessor.from_pretrained(\"BAAI/AltCLIP\")"
        },
        {
            "sha": "a5335608edb1fad22d97f49d3d3d294cfd77b74d",
            "filename": "docs/source/en/model_doc/arcee.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Farcee.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Farcee.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Farcee.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -44,7 +44,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"text-generation\",\n     model=\"arcee-ai/AFM-4.5B\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n \n@@ -62,7 +62,7 @@ from transformers import AutoTokenizer, ArceeForCausalLM\n tokenizer = AutoTokenizer.from_pretrained(\"arcee-ai/AFM-4.5B\")\n model = ArceeForCausalLM.from_pretrained(\n     \"arcee-ai/AFM-4.5B\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\"\n )\n "
        },
        {
            "sha": "e5f4afa7b7aeb97d3d661bd584c1ba875596647d",
            "filename": "docs/source/en/model_doc/aria.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Faria.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Faria.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Faria.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -45,7 +45,7 @@ pipeline = pipeline(\n     \"image-to-text\",\n     model=\"rhymes-ai/Aria\",\n     device=0,\n-    torch_dtype=torch.bfloat16\n+    dtype=torch.bfloat16\n )\n pipeline(\n     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n@@ -63,7 +63,7 @@ from transformers import AutoModelForCausalLM, AutoProcessor\n model = AutoModelForCausalLM.from_pretrained(\n     \"rhymes-ai/Aria\",\n     device_map=\"auto\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     attn_implementation=\"sdpa\"\n )\n \n@@ -109,7 +109,7 @@ from transformers import TorchAoConfig, AutoModelForCausalLM, AutoProcessor\n quantization_config = TorchAoConfig(\"int4_weight_only\", group_size=128)\n model = AutoModelForCausalLM.from_pretrained(\n     \"rhymes-ai/Aria-sequential_mlp\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     quantization_config=quantization_config\n )"
        },
        {
            "sha": "40115810467af93dabe53dbcc221708a20bb699a",
            "filename": "docs/source/en/model_doc/audio-spectrogram-transformer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Faudio-spectrogram-transformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Faudio-spectrogram-transformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Faudio-spectrogram-transformer.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -63,7 +63,7 @@ SDPA is used by default for `torch>=2.1.1` when an implementation is available,\n \n ```\n from transformers import ASTForAudioClassification\n-model = ASTForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\", attn_implementation=\"sdpa\", torch_dtype=torch.float16)\n+model = ASTForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\", attn_implementation=\"sdpa\", dtype=torch.float16)\n ...\n ```\n "
        },
        {
            "sha": "1f02b30344a2d3e523b2e2335e8e151707df1f62",
            "filename": "docs/source/en/model_doc/aya_vision.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Faya_vision.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Faya_vision.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Faya_vision.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -67,7 +67,7 @@ model_id = \"CohereLabs/aya-vision-8b\"\n \n processor = AutoProcessor.from_pretrained(model_id)\n model = AutoModelForImageTextToText.from_pretrained(\n-    model_id, device_map=\"auto\", torch_dtype=torch.float16\n+    model_id, device_map=\"auto\", dtype=torch.float16\n )\n \n # Format message with the aya-vision chat template\n@@ -153,7 +153,7 @@ print(processor.tokenizer.decode(generated[0], skip_special_tokens=True))\n         \n     processor = AutoProcessor.from_pretrained(\"CohereForAI/aya-vision-8b\")\n     model = AutoModelForImageTextToText.from_pretrained(\n-        \"CohereForAI/aya-vision-8b\", device_map=\"auto\", torch_dtype=torch.float16\n+        \"CohereForAI/aya-vision-8b\", device_map=\"auto\", dtype=torch.float16\n     )\n     \n     messages = [\n@@ -199,7 +199,7 @@ print(processor.tokenizer.decode(generated[0], skip_special_tokens=True))\n         \n     processor = AutoProcessor.from_pretrained(model_id)\n     model = AutoModelForImageTextToText.from_pretrained(\n-        \"CohereForAI/aya-vision-8b\", device_map=\"auto\", torch_dtype=torch.float16\n+        \"CohereForAI/aya-vision-8b\", device_map=\"auto\", dtype=torch.float16\n     )\n     \n     batch_messages = ["
        },
        {
            "sha": "54aceb11f699e6832de00737b5737d7b28dfba58",
            "filename": "docs/source/en/model_doc/bamba.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fbamba.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fbamba.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbamba.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -46,7 +46,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"text-generation\",\n     model=\"ibm-ai-platform/Bamba-9B-v2\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device=0\n )\n pipeline(\"Plants create energy through a process known as\")\n@@ -61,7 +61,7 @@ import torch\n from transformers import AutoModelForCausalLM, AutoTokenizer\n \n tokenizer = AutoTokenizer.from_pretrained(\"ibm-ai-platform/Bamba-9B-v2\")\n-model = AutoModelForCausalLM.from_pretrained(\"ibm-ai-platform/Bamba-9B-v2\", torch_dtype=torch.bfloat16, device_map=\"auto\", attn_implementation=\"sdpa\")\n+model = AutoModelForCausalLM.from_pretrained(\"ibm-ai-platform/Bamba-9B-v2\", dtype=torch.bfloat16, device_map=\"auto\", attn_implementation=\"sdpa\")\n input_ids = tokenizer(\"Plants create energy through a process known as\", return_tensors=\"pt\").to(model.device)\n \n output = model.generate(**input_ids)"
        },
        {
            "sha": "a5787ab234ee01accd40252d888f8937516d9263",
            "filename": "docs/source/en/model_doc/bark.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fbark.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fbark.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbark.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -47,7 +47,7 @@ from transformers import BarkModel, infer_device\n import torch\n \n device = infer_device()\n-model = BarkModel.from_pretrained(\"suno/bark-small\", torch_dtype=torch.float16).to(device)\n+model = BarkModel.from_pretrained(\"suno/bark-small\", dtype=torch.float16).to(device)\n ```\n \n #### Using CPU offload\n@@ -92,7 +92,7 @@ pip install -U flash-attn --no-build-isolation\n To load a model using Flash Attention 2, we can pass the `attn_implementation=\"flash_attention_2\"` flag to [`.from_pretrained`](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained). We'll also load the model in half-precision (e.g. `torch.float16`), since it results in almost no degradation to audio quality but significantly lower memory usage and faster inference:\n \n ```python\n-model = BarkModel.from_pretrained(\"suno/bark-small\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(device)\n+model = BarkModel.from_pretrained(\"suno/bark-small\", dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(device)\n ```\n \n ##### Performance comparison\n@@ -120,7 +120,7 @@ import torch\n device = infer_device()\n \n # load in fp16 and use Flash Attention 2\n-model = BarkModel.from_pretrained(\"suno/bark-small\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(device)\n+model = BarkModel.from_pretrained(\"suno/bark-small\", dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(device)\n \n # enable CPU offload\n model.enable_cpu_offload()"
        },
        {
            "sha": "b0252ea92311819e6a03e3305e74424a423aedb2",
            "filename": "docs/source/en/model_doc/bart.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fbart.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fbart.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbart.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -40,7 +40,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"fill-mask\",\n     model=\"facebook/bart-large\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n pipeline(\"Plants create <mask> through a process known as photosynthesis.\")\n@@ -58,7 +58,7 @@ tokenizer = AutoTokenizer.from_pretrained(\n )\n model = AutoModelForMaskedLM.from_pretrained(\n     \"facebook/bart-large\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )"
        },
        {
            "sha": "43b6521f101385267d85a0e9063831eaed26d5aa",
            "filename": "docs/source/en/model_doc/barthez.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fbarthez.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fbarthez.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbarthez.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -44,7 +44,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"fill-mask\",\n     model=\"moussaKam/barthez\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n pipeline(\"Les plantes produisent <mask> grÃ¢ce Ã  un processus appelÃ© photosynthÃ¨se.\")\n@@ -62,7 +62,7 @@ tokenizer = AutoTokenizer.from_pretrained(\n )\n model = AutoModelForMaskedLM.from_pretrained(\n     \"moussaKam/barthez\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n )\n inputs = tokenizer(\"Les plantes produisent <mask> grÃ¢ce Ã  un processus appelÃ© photosynthÃ¨se.\", return_tensors=\"pt\").to(model.device)"
        },
        {
            "sha": "9e86a1b615d05cf677b5224a63b6de732544654a",
            "filename": "docs/source/en/model_doc/bartpho.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fbartpho.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fbartpho.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbartpho.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -46,7 +46,7 @@ from transformers import pipeline\n pipeline = pipeline(\n    task=\"summarization\",\n    model=\"vinai/bartpho-word\",\n-   torch_dtype=torch.float16,\n+   dtype=torch.float16,\n    device=0\n )\n \n@@ -70,7 +70,7 @@ tokenizer = AutoTokenizer.from_pretrained(\n )\n model = BartForConditionalGeneration.from_pretrained(\n     \"vinai/bartpho-word\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n )\n "
        },
        {
            "sha": "b66021ec8d987519673fed85af3842dd9a074afe",
            "filename": "docs/source/en/model_doc/beit.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fbeit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fbeit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbeit.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -89,7 +89,7 @@ SDPA is used by default for `torch>=2.1.1` when an implementation is available,\n \n ```\n from transformers import BeitForImageClassification\n-model = BeitForImageClassification.from_pretrained(\"microsoft/beit-base-patch16-224\", attn_implementation=\"sdpa\", torch_dtype=torch.float16)\n+model = BeitForImageClassification.from_pretrained(\"microsoft/beit-base-patch16-224\", attn_implementation=\"sdpa\", dtype=torch.float16)\n ...\n ```\n "
        },
        {
            "sha": "38cbe2137eb7b80f129466ee0a0b0f764f189f3e",
            "filename": "docs/source/en/model_doc/bert-generation.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert-generation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert-generation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert-generation.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -43,7 +43,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"text2text-generation\",\n     model=\"google/roberta2roberta_L-24_discofuse\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n pipeline(\"Plants create energy through \")\n@@ -56,7 +56,7 @@ pipeline(\"Plants create energy through \")\n import torch\n from transformers import EncoderDecoderModel, AutoTokenizer\n \n-model = EncoderDecoderModel.from_pretrained(\"google/roberta2roberta_L-24_discofuse\", torch_dtype=\"auto\")\n+model = EncoderDecoderModel.from_pretrained(\"google/roberta2roberta_L-24_discofuse\", dtype=\"auto\")\n tokenizer = AutoTokenizer.from_pretrained(\"google/roberta2roberta_L-24_discofuse\")\n \n input_ids = tokenizer(\n@@ -94,7 +94,7 @@ quantization_config = BitsAndBytesConfig(\n model = EncoderDecoderModel.from_pretrained(\n     \"google/roberta2roberta_L-24_discofuse\",\n     quantization_config=quantization_config,\n-    torch_dtype=\"auto\"\n+    dtype=\"auto\"\n )\n tokenizer = AutoTokenizer.from_pretrained(\"google/roberta2roberta_L-24_discofuse\")\n "
        },
        {
            "sha": "97637e98e1f3c230a582bc001b99f718f5dbe9b4",
            "filename": "docs/source/en/model_doc/bert.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -43,7 +43,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"fill-mask\",\n     model=\"google-bert/bert-base-uncased\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n pipeline(\"Plants create [MASK] through a process known as photosynthesis.\")\n@@ -61,7 +61,7 @@ tokenizer = AutoTokenizer.from_pretrained(\n )\n model = AutoModelForMaskedLM.from_pretrained(\n     \"google-bert/bert-base-uncased\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )"
        },
        {
            "sha": "4dffe29168d35b1bcbe57f49d59663e580a9d045",
            "filename": "docs/source/en/model_doc/bertweet.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fbertweet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fbertweet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbertweet.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -44,7 +44,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"fill-mask\",\n     model=\"vinai/bertweet-base\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n pipeline(\"Plants create <mask> through a process known as photosynthesis.\")\n@@ -61,7 +61,7 @@ tokenizer = AutoTokenizer.from_pretrained(\n )\n model = AutoModelForMaskedLM.from_pretrained(\n     \"vinai/bertweet-base\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\"\n )\n inputs = tokenizer(\"Plants create <mask> through a process known as photosynthesis.\", return_tensors=\"pt\").to(model.device)"
        },
        {
            "sha": "2d3b6d545faf46a1ec27182addd9effaeb1ab40b",
            "filename": "docs/source/en/model_doc/big_bird.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fbig_bird.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fbig_bird.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbig_bird.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -42,7 +42,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"fill-mask\",\n     model=\"google/bigbird-roberta-base\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n pipeline(\"Plants create [MASK] through a process known as photosynthesis.\")\n@@ -59,7 +59,7 @@ tokenizer = AutoTokenizer.from_pretrained(\n )\n model = AutoModelForMaskedLM.from_pretrained(\n     \"google/bigbird-roberta-base\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n )\n inputs = tokenizer(\"Plants create [MASK] through a process known as photosynthesis.\", return_tensors=\"pt\").to(model.device)"
        },
        {
            "sha": "cae1e8f779d4034b126972b410ab966d5f7496dd",
            "filename": "docs/source/en/model_doc/bigbird_pegasus.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fbigbird_pegasus.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fbigbird_pegasus.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbigbird_pegasus.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -44,7 +44,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"summarization\",\n     model=\"google/bigbird-pegasus-large-arxiv\",\n-    torch_dtype=torch.float32,\n+    dtype=torch.float32,\n     device=0\n )\n pipeline(\"\"\"Plants are among the most remarkable and essential life forms on Earth, possessing a unique ability to produce their own food through a process known as photosynthesis. This complex biochemical process is fundamental not only to plant life but to virtually all life on the planet.\n@@ -64,7 +64,7 @@ tokenizer = AutoTokenizer.from_pretrained(\n )\n model = AutoModelForSeq2SeqLM.from_pretrained(\n     \"google/bigbird-pegasus-large-arxiv\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n )\n \n@@ -102,7 +102,7 @@ quantization_config = BitsAndBytesConfig(\n )\n model = AutoModelForSeq2SeqLM.from_pretrained(\n     \"google/bigbird-pegasus-large-arxiv\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     quantization_config=quantization_config\n )"
        },
        {
            "sha": "60b84f01512260189a4c4861ab48799acc2992eb",
            "filename": "docs/source/en/model_doc/biogpt.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fbiogpt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fbiogpt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbiogpt.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -44,7 +44,7 @@ from transformers import pipeline\n generator = pipeline(\n     task=\"text-generation\",\n     model=\"microsoft/biogpt\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0,\n )\n result = generator(\"Ibuprofen is best used for\", truncation=True, max_length=50, do_sample=True)[0][\"generated_text\"]\n@@ -61,7 +61,7 @@ from transformers import AutoModelForCausalLM, AutoTokenizer\n tokenizer = AutoTokenizer.from_pretrained(\"microsoft/biogpt\")\n model = AutoModelForCausalLM.from_pretrained(\n     \"microsoft/biogpt\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )\n@@ -105,7 +105,7 @@ tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BioGPT-Large\")\n model = AutoModelForCausalLM.from_pretrained(\n     \"microsoft/BioGPT-Large\", \n     quantization_config=bnb_config,\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\"\n )\n "
        },
        {
            "sha": "6946ec65d437a709c72ed959a24363d252f0451f",
            "filename": "docs/source/en/model_doc/bitnet.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fbitnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fbitnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbitnet.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -90,7 +90,7 @@ model_id = \"microsoft/bitnet-b1.58-2B-4T\"\n tokenizer = AutoTokenizer.from_pretrained(model_id)\n model = AutoModelForCausalLM.from_pretrained(\n     model_id,\n-    torch_dtype=torch.bfloat16\n+    dtype=torch.bfloat16\n )\n \n # Apply the chat template"
        },
        {
            "sha": "13a2a5731a5f32e72df8b1e7ffa45f137a3b52d7",
            "filename": "docs/source/en/model_doc/blip.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -45,7 +45,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"visual-question-answering\",\n     model=\"Salesforce/blip-vqa-base\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n@@ -64,7 +64,7 @@ from transformers import AutoProcessor, AutoModelForVisualQuestionAnswering\n processor = AutoProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n model = AutoModelForVisualQuestionAnswering.from_pretrained(\n     \"Salesforce/blip-vqa-base\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\"\n )\n "
        },
        {
            "sha": "ffe2a76567f4a37673ce54af37a89b254cd102c0",
            "filename": "docs/source/en/model_doc/byt5.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fbyt5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fbyt5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbyt5.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -41,7 +41,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"text2text-generation\",\n     model=\"google/byt5-small\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n pipeline(\"translate English to French: The weather is nice today\")\n@@ -59,7 +59,7 @@ tokenizer = AutoTokenizer.from_pretrained(\n )\n model = AutoModelForSeq2SeqLM.from_pretrained(\n     \"google/byt5-small\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\"\n )\n \n@@ -94,7 +94,7 @@ quantization_config = TorchAoConfig(\"int4_weight_only\", group_size=128)\n \n model = AutoModelForSeq2SeqLM.from_pretrained(\n     \"google/byt5-xl\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     quantization_config=quantization_config\n )"
        },
        {
            "sha": "ddce66f2dedb06fc59538fd6bb4be81cd127f101",
            "filename": "docs/source/en/model_doc/camembert.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fcamembert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fcamembert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcamembert.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -47,7 +47,7 @@ The examples below demonstrate how to predict the `<mask>` token with [`Pipeline\n import torch\n from transformers import pipeline\n \n-pipeline = pipeline(\"fill-mask\", model=\"camembert-base\", torch_dtype=torch.float16, device=0)\n+pipeline = pipeline(\"fill-mask\", model=\"camembert-base\", dtype=torch.float16, device=0)\n pipeline(\"Le camembert est un dÃ©licieux fromage <mask>.\")\n ```\n </hfoption>\n@@ -59,7 +59,7 @@ import torch\n from transformers import AutoTokenizer, AutoModelForMaskedLM\n \n tokenizer = AutoTokenizer.from_pretrained(\"camembert-base\")\n-model = AutoModelForMaskedLM.from_pretrained(\"camembert-base\", torch_dtype=\"auto\", device_map=\"auto\", attn_implementation=\"sdpa\")\n+model = AutoModelForMaskedLM.from_pretrained(\"camembert-base\", dtype=\"auto\", device_map=\"auto\", attn_implementation=\"sdpa\")\n inputs = tokenizer(\"Le camembert est un dÃ©licieux fromage <mask>.\", return_tensors=\"pt\").to(model.device)\n \n with torch.no_grad():"
        },
        {
            "sha": "eb71349115ed1bbbf631d679eae7d19f70fc6c02",
            "filename": "docs/source/en/model_doc/chameleon.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fchameleon.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fchameleon.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fchameleon.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -78,7 +78,7 @@ from PIL import Image\n import requests\n \n processor = ChameleonProcessor.from_pretrained(\"facebook/chameleon-7b\")\n-model = ChameleonForConditionalGeneration.from_pretrained(\"facebook/chameleon-7b\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n+model = ChameleonForConditionalGeneration.from_pretrained(\"facebook/chameleon-7b\", dtype=torch.bfloat16, device_map=\"auto\")\n \n # prepare image and text prompt\n url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n@@ -104,7 +104,7 @@ import requests\n \n processor = ChameleonProcessor.from_pretrained(\"facebook/chameleon-7b\")\n \n-model = ChameleonForConditionalGeneration.from_pretrained(\"facebook/chameleon-7b\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n+model = ChameleonForConditionalGeneration.from_pretrained(\"facebook/chameleon-7b\", dtype=torch.bfloat16, device_map=\"auto\")\n \n # Get three different images\n url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n@@ -170,7 +170,7 @@ from transformers import ChameleonForConditionalGeneration\n model_id = \"facebook/chameleon-7b\"\n model = ChameleonForConditionalGeneration.from_pretrained(\n     model_id,\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     attn_implementation=\"flash_attention_2\"\n ).to(0)\n ```"
        },
        {
            "sha": "ff8428141c4a814741ca3c82442306267eaee6c3",
            "filename": "docs/source/en/model_doc/clap.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fclap.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fclap.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fclap.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -43,7 +43,7 @@ The example below demonstrates how to extract text embeddings with the [`AutoMod\n import torch\n from transformers import AutoTokenizer, AutoModel\n \n-model = AutoModel.from_pretrained(\"laion/clap-htsat-unfused\", torch_dtype=torch.float16, device_map=\"auto\")\n+model = AutoModel.from_pretrained(\"laion/clap-htsat-unfused\", dtype=torch.float16, device_map=\"auto\")\n tokenizer = AutoTokenizer.from_pretrained(\"laion/clap-htsat-unfused\")\n \n texts = [\"the sound of a cat\", \"the sound of a dog\", \"music playing\"]"
        },
        {
            "sha": "018044f2feb5d5e0cc931af292123e211cef2058",
            "filename": "docs/source/en/model_doc/clip.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fclip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fclip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fclip.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -44,7 +44,7 @@ from transformers import pipeline\n clip = pipeline(\n    task=\"zero-shot-image-classification\",\n    model=\"openai/clip-vit-base-patch32\",\n-   torch_dtype=torch.bfloat16,\n+   dtype=torch.bfloat16,\n    device=0\n )\n labels = [\"a photo of a cat\", \"a photo of a dog\", \"a photo of a car\"]\n@@ -60,7 +60,7 @@ import torch\n from PIL import Image\n from transformers import AutoProcessor, AutoModel\n \n-model = AutoModel.from_pretrained(\"openai/clip-vit-base-patch32\", torch_dtype=torch.bfloat16, attn_implementation=\"sdpa\")\n+model = AutoModel.from_pretrained(\"openai/clip-vit-base-patch32\", dtype=torch.bfloat16, attn_implementation=\"sdpa\")\n processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n \n url = \"http://images.cocodataset.org/val2017/000000039769.jpg\""
        },
        {
            "sha": "60e9cb4c3cf260cc891a7f2160e5305e0f993f30",
            "filename": "docs/source/en/model_doc/code_llama.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fcode_llama.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fcode_llama.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcode_llama.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -42,7 +42,7 @@ from transformers import pipeline\n pipe = pipeline(\n     \"text-generation\",\n     model=\"meta-llama/CodeLlama-7b-hf\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=0\n )\n \n@@ -65,7 +65,7 @@ from transformers import AutoModelForCausalLM, AutoTokenizer\n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/CodeLlama-7b-hf\")\n model = AutoModelForCausalLM.from_pretrained(\n     \"meta-llama/CodeLlama-7b-hf\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )\n@@ -113,7 +113,7 @@ bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.\n tokenizer = CodeLlamaTokenizer.from_pretrained(\"meta-llama/CodeLlama-34b-hf\")\n model = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/CodeLlama-34b-hf\",\n-   torch_dtype=torch.bfloat16,\n+   dtype=torch.bfloat16,\n    device_map=\"auto\",\n    quantization_config=bnb_config\n )"
        },
        {
            "sha": "9fc6d266d69ae4e24fc0273d7b5d048c914a7ffd",
            "filename": "docs/source/en/model_doc/cohere.md",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -45,7 +45,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"text-generation\",\n     model=\"CohereForAI/c4ai-command-r-v01\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n pipeline(\"Plants create energy through a process known as\")\n@@ -59,7 +59,7 @@ import torch\n from transformers import AutoTokenizer, AutoModelForCausalLM\n \n tokenizer = AutoTokenizer.from_pretrained(\"CohereForAI/c4ai-command-r-v01\")\n-model = AutoModelForCausalLM.from_pretrained(\"CohereForAI/c4ai-command-r-v01\", torch_dtype=torch.float16, device_map=\"auto\", attn_implementation=\"sdpa\")\n+model = AutoModelForCausalLM.from_pretrained(\"CohereForAI/c4ai-command-r-v01\", dtype=torch.float16, device_map=\"auto\", attn_implementation=\"sdpa\")\n \n # format message with the Command-R chat template\n messages = [{\"role\": \"user\", \"content\": \"How do plants make energy?\"}]\n@@ -79,7 +79,7 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n \n ```bash\n # pip install -U flash-attn --no-build-isolation\n-transformers chat CohereForAI/c4ai-command-r-v01 --torch_dtype auto --attn_implementation flash_attention_2\n+transformers chat CohereForAI/c4ai-command-r-v01 --dtype auto --attn_implementation flash_attention_2\n ```\n \n </hfoption>\n@@ -95,7 +95,7 @@ from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n \n bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n tokenizer = AutoTokenizer.from_pretrained(\"CohereForAI/c4ai-command-r-v01\")\n-model = AutoModelForCausalLM.from_pretrained(\"CohereForAI/c4ai-command-r-v01\", torch_dtype=torch.float16, device_map=\"auto\", quantization_config=bnb_config, attn_implementation=\"sdpa\")\n+model = AutoModelForCausalLM.from_pretrained(\"CohereForAI/c4ai-command-r-v01\", dtype=torch.float16, device_map=\"auto\", quantization_config=bnb_config, attn_implementation=\"sdpa\")\n \n # format message with the Command-R chat template\n messages = [{\"role\": \"user\", \"content\": \"How do plants make energy?\"}]\n@@ -125,7 +125,7 @@ visualizer(\"Plants create energy through a process known as\")\n \n \n ## Notes\n-- Donâ€™t use the torch_dtype parameter in [`~AutoModel.from_pretrained`] if youâ€™re using FlashAttention-2 because it only supports fp16 or bf16. You should use [Automatic Mixed Precision](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html), set fp16 or bf16 to True if using [`Trainer`], or use [torch.autocast](https://pytorch.org/docs/stable/amp.html#torch.autocast).\n+- Donâ€™t use the dtype parameter in [`~AutoModel.from_pretrained`] if youâ€™re using FlashAttention-2 because it only supports fp16 or bf16. You should use [Automatic Mixed Precision](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html), set fp16 or bf16 to True if using [`Trainer`], or use [torch.autocast](https://pytorch.org/docs/stable/amp.html#torch.autocast).\n \n ## CohereConfig\n "
        },
        {
            "sha": "bcfa05e98d198c099145702496c8d452b196cea7",
            "filename": "docs/source/en/model_doc/cohere2.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -47,7 +47,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"text-generation\", \n     model=\"CohereLabs/c4ai-command-r7b-12-2024\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=0\n )\n \n@@ -67,7 +67,7 @@ from transformers import AutoTokenizer, AutoModelForCausalLM\n tokenizer = AutoTokenizer.from_pretrained(\"CohereLabs/c4ai-command-r7b-12-2024\")\n model = AutoModelForCausalLM.from_pretrained(\n     \"CohereLabs/c4ai-command-r7b-12-2024\", \n-    torch_dtype=torch.float16, \n+    dtype=torch.float16, \n     device_map=\"auto\", \n     attn_implementation=\"sdpa\"\n )\n@@ -90,7 +90,7 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n \n ```bash\n # pip install -U flash-attn --no-build-isolation\n-transformers-cli chat CohereLabs/c4ai-command-r7b-12-2024 --torch_dtype auto --attn_implementation flash_attention_2\n+transformers-cli chat CohereLabs/c4ai-command-r7b-12-2024 --dtype auto --attn_implementation flash_attention_2\n ```\n \n </hfoption>\n@@ -108,7 +108,7 @@ bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n tokenizer = AutoTokenizer.from_pretrained(\"CohereLabs/c4ai-command-r7b-12-2024\")\n model = AutoModelForCausalLM.from_pretrained(\n     \"CohereLabs/c4ai-command-r7b-12-2024\", \n-    torch_dtype=torch.float16, \n+    dtype=torch.float16, \n     device_map=\"auto\", \n     quantization_config=bnb_config, \n     attn_implementation=\"sdpa\""
        },
        {
            "sha": "2e12ff3e4767db0b18084f96a48d038bc4389068",
            "filename": "docs/source/en/model_doc/cohere2_vision.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2_vision.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2_vision.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2_vision.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -48,7 +48,7 @@ model_id = \"CohereLabs/command-a-vision-07-2025\"\n \n processor = AutoProcessor.from_pretrained(model_id)\n model = AutoModelForImageTextToText.from_pretrained(\n-    model_id, device_map=\"auto\", torch_dtype=torch.float16\n+    model_id, device_map=\"auto\", dtype=torch.float16\n )\n \n # Format message with the Command-A-Vision chat template"
        },
        {
            "sha": "b6cc3d5c3e6add148518b19c401cf4c197521108",
            "filename": "docs/source/en/model_doc/colpali.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolpali.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolpali.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolpali.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -46,7 +46,7 @@ model_name = \"vidore/colpali-v1.3-hf\"\n \n model = ColPaliForRetrieval.from_pretrained(\n     model_name,\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",  # \"cpu\", \"cuda\", \"xpu\", or \"mps\" for Apple Silicon\n )\n processor = ColPaliProcessor.from_pretrained(model_name)"
        },
        {
            "sha": "1ed50cc4cf29e69fb5785e25b77de66c017cfe17",
            "filename": "docs/source/en/model_doc/colqwen2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolqwen2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolqwen2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolqwen2.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -49,7 +49,7 @@ model_name = \"vidore/colqwen2-v1.0-hf\"\n \n model = ColQwen2ForRetrieval.from_pretrained(\n     model_name,\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",  # \"cpu\", \"cuda\", \"xpu\" or \"mps\" for Apple Silicon\n     attn_implementation=\"flash_attention_2\" if is_flash_attn_2_available() else \"sdpa\",\n )"
        },
        {
            "sha": "2793fafd953ac9e29840cb1c2a3bfaa3483d2f03",
            "filename": "docs/source/en/model_doc/cvt.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fcvt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fcvt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcvt.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -44,7 +44,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"image-classification\",\n     model=\"microsoft/cvt-13\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n pipeline(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")\n@@ -62,7 +62,7 @@ from transformers import AutoModelForImageClassification, AutoImageProcessor\n image_processor = AutoImageProcessor.from_pretrained(\"microsoft/cvt-13\")\n model = AutoModelForImageClassification.from_pretrained(\n     \"microsoft/cvt-13\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\"\n )\n "
        },
        {
            "sha": "f975c0d35b35b9571fa76644850887d193ceb094",
            "filename": "docs/source/en/model_doc/data2vec.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fdata2vec.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fdata2vec.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdata2vec.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -70,7 +70,7 @@ The SDPA implementation is currently available for the Data2VecAudio and Data2Ve\n \n ```\n from transformers import Data2VecVisionForImageClassification\n-model = Data2VecVisionForImageClassification.from_pretrained(\"facebook/data2vec-vision-base\", attn_implementation=\"sdpa\", torch_dtype=torch.float16)\n+model = Data2VecVisionForImageClassification.from_pretrained(\"facebook/data2vec-vision-base\", attn_implementation=\"sdpa\", dtype=torch.float16)\n ...\n ```\n "
        },
        {
            "sha": "8b2e5ae75e34868f1e81d24d3f7b91d28bc663ec",
            "filename": "docs/source/en/model_doc/dbrx.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fdbrx.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fdbrx.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdbrx.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -52,7 +52,7 @@ tokenizer = AutoTokenizer.from_pretrained(\"databricks/dbrx-instruct\", token=\"YOU\n model = DbrxForCausalLM.from_pretrained(\n     \"databricks/dbrx-instruct\",\n     device_map=\"auto\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     token=\"YOUR_HF_TOKEN\",\n     )\n \n@@ -73,7 +73,7 @@ tokenizer = AutoTokenizer.from_pretrained(\"databricks/dbrx-instruct\", token=\"YOU\n model = DbrxForCausalLM.from_pretrained(\n     \"databricks/dbrx-instruct\",\n     device_map=\"auto\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     token=\"YOUR_HF_TOKEN\",\n     attn_implementation=\"flash_attention_2\",\n     )\n@@ -95,7 +95,7 @@ tokenizer = AutoTokenizer.from_pretrained(\"databricks/dbrx-instruct\", token=\"YOU\n model = DbrxForCausalLM.from_pretrained(\n     \"databricks/dbrx-instruct\",\n     device_map=\"auto\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     token=\"YOUR_HF_TOKEN\",\n     attn_implementation=\"sdpa\",\n     )"
        },
        {
            "sha": "7fc8bcdc5226c64ead5bdcba27857fdda075a6c5",
            "filename": "docs/source/en/model_doc/deberta-v2.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta-v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta-v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta-v2.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -47,7 +47,7 @@ pipeline = pipeline(\n     task=\"text-classification\",\n     model=\"microsoft/deberta-v2-xlarge-mnli\",\n     device=0,\n-    torch_dtype=torch.float16\n+    dtype=torch.float16\n )\n result = pipeline(\"DeBERTa-v2 is great at understanding context!\")\n print(result)\n@@ -65,7 +65,7 @@ tokenizer = AutoTokenizer.from_pretrained(\n )\n model = AutoModelForSequenceClassification.from_pretrained(\n     \"microsoft/deberta-v2-xlarge-mnli\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\"\n )\n \n@@ -107,7 +107,7 @@ tokenizer = AutoTokenizer.from_pretrained(model_id)\n model = AutoModelForSequenceClassification.from_pretrained(\n     model_id,\n     quantization_config=quantization_config,\n-    torch_dtype=\"float16\"\n+    dtype=\"float16\"\n )\n \n inputs = tokenizer(\"DeBERTa-v2 is great at understanding context!\", return_tensors=\"pt\").to(model.device)"
        },
        {
            "sha": "ed99932cf257694728a961dea8e644f083c75dd8",
            "filename": "docs/source/en/model_doc/deepseek_v3.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_v3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_v3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_v3.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -53,7 +53,7 @@ chat = [\n ]\n \n \n-model = AutoModelForCausalLM.from_pretrained(\"deepseek-r1\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n+model = AutoModelForCausalLM.from_pretrained(\"deepseek-r1\", device_map=\"auto\", dtype=torch.bfloat16)\n inputs = tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n import time\n start = time.time()"
        },
        {
            "sha": "58695db8348ca57b0c4447385fa642457d5e36f0",
            "filename": "docs/source/en/model_doc/deepseek_vl.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_vl.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -45,7 +45,7 @@ pipe = pipeline(\n     task=\"image-text-to-text\",\n     model=\"deepseek-community/deepseek-vl-1.3b-chat\",\n     device=0,\n-    torch_dtype=torch.float16\n+    dtype=torch.float16\n )\n \n messages = [\n@@ -73,7 +73,7 @@ from transformers import DeepseekVLForConditionalGeneration, AutoProcessor\n \n model = DeepseekVLForConditionalGeneration.from_pretrained(\n     \"deepseek-community/deepseek-vl-1.3b-chat\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )\n@@ -133,7 +133,7 @@ quantization_config = TorchAoConfig(\n \n model = DeepseekVLForConditionalGeneration.from_pretrained(\n     \"deepseek-community/deepseek-vl-1.3b-chat\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     quantization_config=quantization_config\n )\n@@ -147,7 +147,7 @@ model = DeepseekVLForConditionalGeneration.from_pretrained(\n \n     model = DeepseekVLForConditionalGeneration.from_pretrained(\n         \"deepseek-community/deepseek-vl-1.3b-chat\",\n-        torch_dtype=torch.float16,\n+        dtype=torch.float16,\n         device_map=\"auto\",\n         attn_implementation=\"sdpa\"\n     )"
        },
        {
            "sha": "d18ab7576adc42836d4b4b47abbaf1e4f497241c",
            "filename": "docs/source/en/model_doc/deepseek_vl_hybrid.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_vl_hybrid.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_vl_hybrid.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_vl_hybrid.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -44,7 +44,7 @@ pipe = pipeline(\n     task=\"image-text-to-text\",\n     model=\"deepseek-community/deepseek-vl-7b-chat\",\n     device=0,\n-    torch_dtype=torch.float16\n+    dtype=torch.float16\n )\n \n messages = [\n@@ -72,7 +72,7 @@ from transformers import DeepseekVLHybridForConditionalGeneration, AutoProcessor\n \n model = DeepseekVLHybridForConditionalGeneration.from_pretrained(\n     \"deepseek-community/deepseek-vl-7b-chat\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )\n@@ -132,7 +132,7 @@ quantization_config = TorchAoConfig(\n \n model = DeepseekVLHybridForConditionalGeneration.from_pretrained(\n     \"deepseek-community/deepseek-vl-7b-chat\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     quantization_config=quantization_config\n )\n@@ -146,7 +146,7 @@ model = DeepseekVLHybridForConditionalGeneration.from_pretrained(\n \n     model = DeepseekVLHybridForConditionalGeneration.from_pretrained(\n         \"deepseek-community/deepseek-vl-7b-chat\",\n-        torch_dtype=torch.float16,\n+        dtype=torch.float16,\n         device_map=\"auto\",\n         attn_implementation=\"sdpa\"\n     )"
        },
        {
            "sha": "da03770bcbe5564a86bbc84894b4b49eee05495d",
            "filename": "docs/source/en/model_doc/deformable_detr.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeformable_detr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeformable_detr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeformable_detr.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -49,7 +49,7 @@ import torch\n pipeline = pipeline(\n     \"object-detection\", \n     model=\"SenseTime/deformable-detr\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=0\n )\n "
        },
        {
            "sha": "b40db07365a158594eabf296487026b9de97e5db",
            "filename": "docs/source/en/model_doc/deit.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeit.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -88,7 +88,7 @@ SDPA is used by default for `torch>=2.1.1` when an implementation is available,\n \n ```\n from transformers import DeiTForImageClassification\n-model = DeiTForImageClassification.from_pretrained(\"facebook/deit-base-distilled-patch16-224\", attn_implementation=\"sdpa\", torch_dtype=torch.float16)\n+model = DeiTForImageClassification.from_pretrained(\"facebook/deit-base-distilled-patch16-224\", attn_implementation=\"sdpa\", dtype=torch.float16)\n ...\n ```\n "
        },
        {
            "sha": "5ac7007595ff47252f8995e00c1468e937cc4155",
            "filename": "docs/source/en/model_doc/depth_anything.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_anything.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_anything.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_anything.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -39,7 +39,7 @@ The example below demonstrates how to obtain a depth map with [`Pipeline`] or th\n import torch\n from transformers import pipeline\n \n-pipe = pipeline(task=\"depth-estimation\", model=\"LiheYoung/depth-anything-base-hf\", torch_dtype=torch.bfloat16, device=0)\n+pipe = pipeline(task=\"depth-estimation\", model=\"LiheYoung/depth-anything-base-hf\", dtype=torch.bfloat16, device=0)\n pipe(\"http://images.cocodataset.org/val2017/000000039769.jpg\")[\"depth\"]\n ```\n \n@@ -54,7 +54,7 @@ from PIL import Image\n from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n \n image_processor = AutoImageProcessor.from_pretrained(\"LiheYoung/depth-anything-base-hf\")\n-model = AutoModelForDepthEstimation.from_pretrained(\"LiheYoung/depth-anything-base-hf\", torch_dtype=torch.bfloat16)\n+model = AutoModelForDepthEstimation.from_pretrained(\"LiheYoung/depth-anything-base-hf\", dtype=torch.bfloat16)\n url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n image = Image.open(requests.get(url, stream=True).raw)\n inputs = image_processor(images=image, return_tensors=\"pt\")"
        },
        {
            "sha": "85423359ceb0fbd71f8c57c2d765a09613ee3029",
            "filename": "docs/source/en/model_doc/depth_pro.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_pro.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_pro.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_pro.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -134,7 +134,7 @@ SDPA is used by default for `torch>=2.1.1` when an implementation is available,\n \n ```py\n from transformers import DepthProForDepthEstimation\n-model = DepthProForDepthEstimation.from_pretrained(\"apple/DepthPro-hf\", attn_implementation=\"sdpa\", torch_dtype=torch.float16)\n+model = DepthProForDepthEstimation.from_pretrained(\"apple/DepthPro-hf\", attn_implementation=\"sdpa\", dtype=torch.float16)\n ```\n \n For the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`)."
        },
        {
            "sha": "425ab0f04c5162f45951aa868510371017401d22",
            "filename": "docs/source/en/model_doc/detr.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fdetr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fdetr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdetr.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -44,7 +44,7 @@ import torch\n pipeline = pipeline(\n     \"object-detection\", \n     model=\"facebook/detr-resnet-50\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=0\n )\n "
        },
        {
            "sha": "59256756acfd5a11b924a074b89bdba6c02bd965",
            "filename": "docs/source/en/model_doc/dinov2.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov2.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -41,7 +41,7 @@ from transformers import pipeline\n pipe = pipeline(\n     task=\"image-classification\",\n     model=\"facebook/dinov2-small-imagenet1k-1-layer\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n \n@@ -62,7 +62,7 @@ image = Image.open(requests.get(url, stream=True).raw)\n processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-small-imagenet1k-1-layer\")\n model = AutoModelForImageClassification.from_pretrained(\n     \"facebook/dinov2-small-imagenet1k-1-layer\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )\n@@ -97,7 +97,7 @@ quantization_config = TorchAoConfig(quant_type=quant_config)\n \n model = AutoModelForImageClassification.from_pretrained(\n     'facebook/dinov2-giant-imagenet1k-1-layer',\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     quantization_config=quantization_config\n )"
        },
        {
            "sha": "a11a8fd10ccafa102870df54e99b23a0c347945c",
            "filename": "docs/source/en/model_doc/dinov3.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov3.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -41,7 +41,7 @@ from transformers import pipeline\n pipe = pipeline(\n     task=\"image-feature-extraction\",\n     model=\"facebook/dinov3-vits16-pretrain-lvd1689m\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n )\n \n pipe(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")\n@@ -61,7 +61,7 @@ image = load_image(url)\n processor = AutoImageProcessor.from_pretrained(\"facebook/dinov3-vits16-pretrain-lvd1689m\")\n model = AutoModel.from_pretrained(\n     \"facebook/dinov3-vits16-pretrain-lvd1689m\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )\n@@ -99,7 +99,7 @@ quantization_config = TorchAoConfig(quant_type=quant_type)\n \n model = AutoModel.from_pretrained(\n     \"facebook/dinov3-vit7b16-pretrain-lvd1689m\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     quantization_config=quantization_config\n )"
        },
        {
            "sha": "93218bfad867f68b5df9f1f24f3c49051b253261",
            "filename": "docs/source/en/model_doc/distilbert.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fdistilbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fdistilbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdistilbert.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -44,7 +44,7 @@ from transformers import pipeline\n classifier = pipeline(\n     task=\"text-classification\",\n     model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n \n@@ -66,7 +66,7 @@ tokenizer = AutoTokenizer.from_pretrained(\n )\n model = AutoModelForSequenceClassification.from_pretrained(\n     \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )"
        },
        {
            "sha": "3027905fe38bf74c95bea37bba2bc31ccaab7ae0",
            "filename": "docs/source/en/model_doc/dit.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fdit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fdit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdit.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -43,7 +43,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"image-classification\",\n     model=\"microsoft/dit-base-finetuned-rvlcdip\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n pipeline(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/dit-example.jpg\")"
        },
        {
            "sha": "f06b6804d6e4d5855cc24c136a1dca4cc8151be4",
            "filename": "docs/source/en/model_doc/donut.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fdonut.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fdonut.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdonut.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -46,7 +46,7 @@ pipeline = pipeline(\n     task=\"document-question-answering\",\n     model=\"naver-clova-ix/donut-base-finetuned-docvqa\",\n     device=0,\n-    torch_dtype=torch.float16\n+    dtype=torch.float16\n )\n dataset = load_dataset(\"hf-internal-testing/example-documents\", split=\"test\")\n image = dataset[0][\"image\"]"
        },
        {
            "sha": "2fd2ac2d9b8e40e9136f77624ca9ca6efe5ea6de",
            "filename": "docs/source/en/model_doc/electra.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Felectra.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Felectra.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Felectra.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -45,7 +45,7 @@ from transformers import pipeline\n classifier = pipeline(\n     task=\"text-classification\",\n     model=\"bhadresh-savani/electra-base-emotion\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n classifier(\"This restaurant has amazing food!\")\n@@ -63,7 +63,7 @@ tokenizer = AutoTokenizer.from_pretrained(\n )\n model = AutoModelForSequenceClassification.from_pretrained(\n     \"bhadresh-savani/electra-base-emotion\",\n-    torch_dtype=torch.float16\n+    dtype=torch.float16\n )\n inputs = tokenizer(\"ELECTRA is more efficient than BERT\", return_tensors=\"pt\")\n "
        },
        {
            "sha": "799de2f0c5c0b6d5c2765f375a204bc45e411608",
            "filename": "docs/source/en/model_doc/emu3.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Femu3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Femu3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Femu3.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -63,7 +63,7 @@ from PIL import Image\n import requests\n \n processor = Emu3Processor.from_pretrained(\"BAAI/Emu3-Chat-hf\")\n-model = Emu3ForConditionalGeneration.from_pretrained(\"BAAI/Emu3-Chat-hf\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n+model = Emu3ForConditionalGeneration.from_pretrained(\"BAAI/Emu3-Chat-hf\", dtype=torch.bfloat16, device_map=\"auto\")\n \n # prepare image and text prompt\n url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n@@ -83,7 +83,7 @@ Emu3 can also generate images from textual input. Here is how you can do it:\n \n ```python\n processor = Emu3Processor.from_pretrained(\"BAAI/Emu3-Gen-hf\")\n-model = Emu3ForConditionalGeneration.from_pretrained(\"BAAI/Emu3-Gen-hf\", torch_dtype=\"bfloat16\", device_map=\"auto\", attn_implementation=\"flash_attention_2\")\n+model = Emu3ForConditionalGeneration.from_pretrained(\"BAAI/Emu3-Gen-hf\", dtype=\"bfloat16\", device_map=\"auto\", attn_implementation=\"flash_attention_2\")\n \n \n inputs = processor("
        },
        {
            "sha": "33346a153524744a7242a021c703bb3b612e3ed0",
            "filename": "docs/source/en/model_doc/encoder-decoder.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fencoder-decoder.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fencoder-decoder.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fencoder-decoder.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -57,7 +57,7 @@ import torch\n from transformers import AutoModelForCausalLM, AutoTokenizer\n \n tokenizer = AutoTokenizer.from_pretrained(\"patrickvonplaten/bert2bert-cnn_dailymail-fp16\")\n-model = AutoModelForCausalLM.from_pretrained(\"patrickvonplaten/bert2bert-cnn_dailymail-fp16\", torch_dtype=torch.bfloat16, device_map=\"auto\",attn_implementation=\"sdpa\")\n+model = AutoModelForCausalLM.from_pretrained(\"patrickvonplaten/bert2bert-cnn_dailymail-fp16\", dtype=torch.bfloat16, device_map=\"auto\",attn_implementation=\"sdpa\")\n \n text = \"Plants create energy through a process known as photosynthesis. This involves capturing sunlight and converting carbon dioxide and water into glucose and oxygen.\"\n "
        },
        {
            "sha": "5ffb7ff7e81dc861ef88ee5b60b028ee11c3da63",
            "filename": "docs/source/en/model_doc/ernie.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -63,7 +63,7 @@ tokenizer = AutoTokenizer.from_pretrained(\n )\n model = AutoModelForMaskedLM.from_pretrained(\n     \"nghuyong/ernie-3.0-xbase-zh\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\"\n )\n inputs = tokenizer(\"å·´é»Žæ˜¯[MASK]å›½çš„é¦–éƒ½ã€‚\", return_tensors=\"pt\").to(model.device)"
        },
        {
            "sha": "e48073bbe6c06d6f5284d17363e4865308001916",
            "filename": "docs/source/en/model_doc/ernie4_5.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie4_5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie4_5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie4_5.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -54,7 +54,7 @@ tokenizer = AutoTokenizer.from_pretrained(model_name)\n model = AutoModelForCausalLM.from_pretrained(\n     model_name,\n     device_map=\"auto\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n )\n \n # prepare the model input"
        },
        {
            "sha": "20c4dcfd5435dda2e67fce38a738d706ec62b78e",
            "filename": "docs/source/en/model_doc/ernie4_5_moe.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie4_5_moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie4_5_moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie4_5_moe.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -56,7 +56,7 @@ tokenizer = AutoTokenizer.from_pretrained(model_name)\n model = AutoModelForCausalLM.from_pretrained(\n     model_name,\n     device_map=\"auto\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n )\n \n # prepare the model input\n@@ -96,7 +96,7 @@ tokenizer = AutoTokenizer.from_pretrained(model_name)\n model = AutoModelForCausalLM.from_pretrained(\n     model_name,\n     device_map=\"auto\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     tp_plan=\"auto\",\n )\n "
        },
        {
            "sha": "69d7ee0b2a817313e0b5864a1b521590f672e315",
            "filename": "docs/source/en/model_doc/exaone4.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fexaone4.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fexaone4.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fexaone4.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -71,7 +71,7 @@ model_name = \"LGAI-EXAONE/EXAONE-4.0-32B\"\n \n model = AutoModelForCausalLM.from_pretrained(\n     model_name,\n-    torch_dtype=\"bfloat16\",\n+    dtype=\"bfloat16\",\n     device_map=\"auto\"\n )\n tokenizer = AutoTokenizer.from_pretrained(model_name)"
        },
        {
            "sha": "a6ba8ec5016078548dac37ca526e5bcf41ec380f",
            "filename": "docs/source/en/model_doc/falcon.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -44,7 +44,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"text-generation\",\n     model=\"tiiuae/falcon-7b-instruct\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device=0\n )\n pipeline(\n@@ -65,7 +65,7 @@ from transformers import AutoTokenizer, AutoModelForCausalLM\n tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b-instruct\")\n model = AutoModelForCausalLM.from_pretrained(\n     \"tiiuae/falcon-7b-instruct\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\",\n )\n@@ -81,7 +81,7 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n \n ```bash\n # pip install -U flash-attn --no-build-isolation\n-transformers chat tiiuae/falcon-7b-instruct --torch_dtype auto --attn_implementation flash_attention_2 --device 0\n+transformers chat tiiuae/falcon-7b-instruct --dtype auto --attn_implementation flash_attention_2 --device 0\n ```\n \n </hfoption>\n@@ -105,7 +105,7 @@ quantization_config = BitsAndBytesConfig(\n tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\")\n model = AutoModelForCausalLM.from_pretrained(\n     \"tiiuae/falcon-7b\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     quantization_config=quantization_config,\n )"
        },
        {
            "sha": "78b6e23a8127a99571f54cf54a6d855268915789",
            "filename": "docs/source/en/model_doc/falcon_mamba.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon_mamba.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon_mamba.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon_mamba.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -42,7 +42,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     \"text-generation\",\n     model=\"tiiuae/falcon-mamba-7b-instruct\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device=0\n )\n pipeline(\n@@ -63,7 +63,7 @@ from transformers import AutoTokenizer, AutoModelForCausalLM\n tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-mamba-7b-instruct\")\n model = AutoModelForCausalLM.from_pretrained(\n     \"tiiuae/falcon-mamba-7b-instruct\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\"\n )\n \n@@ -77,7 +77,7 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n <hfoption id=\"transformers CLI\">\n \n ```bash\n-transformers chat tiiuae/falcon-mamba-7b-instruct --torch_dtype auto --device 0\n+transformers chat tiiuae/falcon-mamba-7b-instruct --dtype auto --device 0\n ```\n \n </hfoption>\n@@ -101,7 +101,7 @@ quantization_config = BitsAndBytesConfig(\n tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-mamba-7b\")\n model = FalconMambaForCausalLM.from_pretrained(\n     \"tiiuae/falcon-mamba-7b\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     quantization_config=quantization_config,\n )"
        },
        {
            "sha": "e5cfee524fbd888ca186f72d4c79e6e9830c2279",
            "filename": "docs/source/en/model_doc/florence2.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fflorence2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fflorence2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fflorence2.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -46,7 +46,7 @@ pipeline = pipeline(\n     \"image-text-to-text\",\n     model=\"ducviet00/Florence-2-base-hf\",\n     device=0,\n-    torch_dtype=torch.bfloat16\n+    dtype=torch.bfloat16\n )\n \n pipeline(\n@@ -67,7 +67,7 @@ from transformers import AutoProcessor, Florence2ForConditionalGeneration\n url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\n image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n \n-model = Florence2ForConditionalGeneration.from_pretrained(\"microsoft/Florence-2-base\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n+model = Florence2ForConditionalGeneration.from_pretrained(\"microsoft/Florence-2-base\", dtype=torch.bfloat16, device_map=\"auto\")\n processor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-base\")\n \n task_prompt = \"<OD>\"\n@@ -103,7 +103,7 @@ quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n \n model = Florence2ForConditionalGeneration.from_pretrained(\n     \"microsoft/Florence-2-large\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     quantization_config=quantization_config\n )"
        },
        {
            "sha": "140216e2abc7827b4dfe298b32ae5e2239b6488f",
            "filename": "docs/source/en/model_doc/fuyu.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Ffuyu.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Ffuyu.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffuyu.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -31,10 +31,10 @@ By treating image tokens like text tokens and using a special image-newline char\n \n <Tip warning={true}>\n \n-The `Fuyu` models were trained using `bfloat16`, but the original inference uses `float16` The checkpoints uploaded on the hub use `torch_dtype = 'float16'` which will be\n+The `Fuyu` models were trained using `bfloat16`, but the original inference uses `float16` The checkpoints uploaded on the hub use `dtype = 'float16'` which will be\n used by the `AutoModel` API to cast the checkpoints from `torch.float32` to `torch.float16`.\n \n-The `dtype` of the online weights is mostly irrelevant, unless you are using `torch_dtype=\"auto\"` when initializing a model using `model = AutoModelForCausalLM.from_pretrained(\"path\", torch_dtype = \"auto\")`. The reason is that the model will first be downloaded ( using the `dtype` of the checkpoints online) then it will be cast to the default `dtype` of `torch` (becomes `torch.float32`). Users should specify the `torch_dtype` they want, and if they don't it will be `torch.float32`.\n+The `dtype` of the online weights is mostly irrelevant, unless you are using `dtype=\"auto\"` when initializing a model using `model = AutoModelForCausalLM.from_pretrained(\"path\", dtype = \"auto\")`. The reason is that the model will first be downloaded ( using the `dtype` of the checkpoints online) then it will be cast to the default `dtype` of `torch` (becomes `torch.float32`). Users should specify the `dtype` they want, and if they don't it will be `torch.float32`.\n \n Finetuning the model in `float16` is not recommended and known to produce `nan`, as such the model should be fine-tuned in `bfloat16`.\n "
        },
        {
            "sha": "11ffe60a96957351c712b66a82b2662c4585950d",
            "filename": "docs/source/en/model_doc/gemma.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -49,7 +49,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"text-generation\",\n     model=\"google/gemma-2b\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n )\n \n@@ -66,7 +66,7 @@ from transformers import AutoTokenizer, AutoModelForCausalLM\n tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n model = AutoModelForCausalLM.from_pretrained(\n     \"google/gemma-2b\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )\n@@ -144,7 +144,7 @@ visualizer(\"LLMs generate text through a process known as\")\n    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n    model = AutoModelForCausalLM.from_pretrained(\n        \"google/gemma-2b\",\n-       torch_dtype=torch.bfloat16,\n+       dtype=torch.bfloat16,\n        device_map=\"auto\",\n        attn_implementation=\"sdpa\"\n    )"
        },
        {
            "sha": "b668aaef6afb37f644f3bea7fbf426fa4d433419",
            "filename": "docs/source/en/model_doc/gemma2.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma2.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -48,7 +48,7 @@ from transformers import pipeline\n pipe = pipeline(\n     task=\"text-generation\",\n     model=\"google/gemma-2-9b\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n )\n \n@@ -65,7 +65,7 @@ from transformers import AutoTokenizer, AutoModelForCausalLM\n tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b\")\n model = AutoModelForCausalLM.from_pretrained(\n     \"google/gemma-2-9b\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )\n@@ -99,7 +99,7 @@ quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-27b\")\n model = AutoModelForCausalLM.from_pretrained(\n     \"google/gemma-2-27b\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )"
        },
        {
            "sha": "72ce31fbbed20793c3647f818c3f75cbc0b7fc2c",
            "filename": "docs/source/en/model_doc/gemma3.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -47,7 +47,7 @@ pipeline = pipeline(\n     task=\"image-text-to-text\",\n     model=\"google/gemma-3-4b-pt\",\n     device=0,\n-    torch_dtype=torch.bfloat16\n+    dtype=torch.bfloat16\n )\n pipeline(\n     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n@@ -64,7 +64,7 @@ from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n \n model = Gemma3ForConditionalGeneration.from_pretrained(\n     \"google/gemma-3-4b-it\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )\n@@ -121,7 +121,7 @@ from transformers import TorchAoConfig, Gemma3ForConditionalGeneration, AutoProc\n quantization_config = TorchAoConfig(\"int4_weight_only\", group_size=128)\n model = Gemma3ForConditionalGeneration.from_pretrained(\n     \"google/gemma-3-27b-it\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     quantization_config=quantization_config\n )\n@@ -220,7 +220,7 @@ visualizer(\"<img>What is shown in this image?\")\n     )\n     model = AutoModelForCausalLM.from_pretrained(\n         \"google/gemma-3-1b-pt\",\n-        torch_dtype=torch.bfloat16,\n+        dtype=torch.bfloat16,\n         device_map=\"auto\",\n         attn_implementation=\"sdpa\"\n     )"
        },
        {
            "sha": "2abac0afa138d96557b45106ff54e78a7ea207f0",
            "filename": "docs/source/en/model_doc/gemma3n.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3n.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3n.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3n.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -57,7 +57,7 @@ pipeline = pipeline(\n     task=\"image-text-to-text\",\n     model=\"google/gemma-3n-e4b\",\n     device=0,\n-    torch_dtype=torch.bfloat16\n+    dtype=torch.bfloat16\n )\n pipeline(\n     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n@@ -74,7 +74,7 @@ from transformers import AutoProcessor, Gemma3nForConditionalGeneration\n \n model = Gemma3nForConditionalGeneration.from_pretrained(\n     \"google/gemma-3n-e4b-it\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )"
        },
        {
            "sha": "be78c73b3fb4dc047abc6b26c17bd48c04c1b3cb",
            "filename": "docs/source/en/model_doc/glm4v.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4v.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4v.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4v.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -59,7 +59,7 @@ pipe = pipeline(\n     task=\"image-text-to-text\",\n     model=\"THUDM/GLM-4.1V-9B-Thinking\",\n     device=0,\n-    torch_dtype=torch.bfloat16\n+    dtype=torch.bfloat16\n )\n messages = [\n     {\n@@ -84,7 +84,7 @@ from transformers import Glm4vForConditionalGeneration, AutoProcessor\n \n model = Glm4vForConditionalGeneration.from_pretrained(\n     \"THUDM/GLM-4.1V-9B-Thinking\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )\n@@ -138,7 +138,7 @@ device = f\"{infer_device()}:0\"\n processor = AutoProcessor.from_pretrained(\"THUDM/GLM-4.1V-9B-Thinking\")\n model = Glm4vForConditionalGeneration.from_pretrained(\n     pretrained_model_name_or_path=\"THUDM/GLM-4.1V-9B-Thinking\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=device\n )\n "
        },
        {
            "sha": "026273aa158bf1fa08e3148461881ac3b0b4608a",
            "filename": "docs/source/en/model_doc/got_ocr2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fgot_ocr2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fgot_ocr2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgot_ocr2.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -164,7 +164,7 @@ Here is an example of how to process cropped patches:\n >>> from transformers import AutoProcessor, AutoModelForImageTextToText, infer_device\n \n >>> device = infer_device()\n->>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", torch_dtype=torch.bfloat16, device_map=device)\n+>>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", dtype=torch.bfloat16, device_map=device)\n >>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", use_fast=True)\n \n >>> image = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/one_column.png\""
        },
        {
            "sha": "1645a92f6346cd1a3480df850f3b6b159eb8748f",
            "filename": "docs/source/en/model_doc/gpt2.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt2.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -44,7 +44,7 @@ The example below demonstrates how to generate text with [`Pipeline`] or the [`A\n import torch\n from transformers import pipeline\n \n-pipeline = pipeline(task=\"text-generation\", model=\"openai-community/gpt2\", torch_dtype=torch.float16, device=0)\n+pipeline = pipeline(task=\"text-generation\", model=\"openai-community/gpt2\", dtype=torch.float16, device=0)\n pipeline(\"Hello, I'm a language model\")\n ```\n </hfoption>\n@@ -54,7 +54,7 @@ pipeline(\"Hello, I'm a language model\")\n import torch\n from transformers import AutoModelForCausalLM, AutoTokenizer\n \n-model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\", torch_dtype=torch.float16, device_map=\"auto\", attn_implementation=\"sdpa\")\n+model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\", dtype=torch.float16, device_map=\"auto\", attn_implementation=\"sdpa\")\n tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n \n input_ids = tokenizer(\"Hello, I'm a language model\", return_tensors=\"pt\").to(model.device)"
        },
        {
            "sha": "a16536cbbe5ccfb63c6822e562b96a76ee75189d",
            "filename": "docs/source/en/model_doc/gpt_bigcode.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_bigcode.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_bigcode.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_bigcode.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -70,7 +70,7 @@ To load and run a model using Flash Attention 2, refer to the snippet below:\n >>> from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device\n >>> device = infer_device() # the device to load the model onto\n \n->>> model = AutoModelForCausalLM.from_pretrained(\"bigcode/gpt_bigcode-santacoder\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\")\n+>>> model = AutoModelForCausalLM.from_pretrained(\"bigcode/gpt_bigcode-santacoder\", dtype=torch.float16, attn_implementation=\"flash_attention_2\")\n >>> tokenizer = AutoTokenizer.from_pretrained(\"bigcode/gpt_bigcode-santacoder\")\n \n >>> prompt = \"def hello_world():\""
        },
        {
            "sha": "f3de04d0e550d7e55f9d68dea88435c5679108bb",
            "filename": "docs/source/en/model_doc/gpt_neo.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neo.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neo.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neo.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -42,7 +42,7 @@ The example below demonstrates how to generate text with [`Pipeline`] or the [`A\n import torch\n from transformers import pipeline\n \n-pipeline = pipeline(task=\"text-generation\", model=\"EleutherAI/gpt-neo-1.3B\", torch_dtype=torch.float16, device=0)\n+pipeline = pipeline(task=\"text-generation\", model=\"EleutherAI/gpt-neo-1.3B\", dtype=torch.float16, device=0)\n pipeline(\"Hello, I'm a language model\")\n ```\n </hfoption>\n@@ -52,7 +52,7 @@ pipeline(\"Hello, I'm a language model\")\n import torch\n from transformers import AutoModelForCausalLM, AutoTokenizer\n \n-model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\", torch_dtype=torch.float16, device_map=\"auto\", attn_implementation=\"flash_attention_2\")\n+model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\", dtype=torch.float16, device_map=\"auto\", attn_implementation=\"flash_attention_2\")\n tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n \n input_ids = tokenizer(\"Hello, I'm a language model\", return_tensors=\"pt\").to(model.device)"
        },
        {
            "sha": "a24fc6aa1d71402ee7f39751af4c77956d41b00e",
            "filename": "docs/source/en/model_doc/gpt_neox.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -38,7 +38,7 @@ generous the support of [CoreWeave](https://www.coreweave.com/).\n GPT-NeoX-20B was trained with fp16, thus it is recommended to initialize the model as follows:\n \n ```python\n-model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\", device_map=\"auto\", torch_dtype=torch.float16)\n+model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\", device_map=\"auto\", dtype=torch.float16)\n ```\n \n GPT-NeoX-20B also has a different tokenizer from the one used in GPT-J-6B and GPT-Neo. The new tokenizer allocates\n@@ -88,7 +88,7 @@ To load a model using Flash Attention 2, we can pass the argument `attn_implemen\n ```python\n >>> from transformers import GPTNeoXForCausalLM, GPTNeoXTokenizerFast\n \n-model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(device)\n+model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\", dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(device)\n ...\n ```\n \n@@ -114,7 +114,7 @@ SDPA is used by default for `torch>=2.1.1` when an implementation is available,\n \n ```python\n from transformers import GPTNeoXForCausalLM\n-model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\", torch_dtype=torch.float16, attn_implementation=\"sdpa\")\n+model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\", dtype=torch.float16, attn_implementation=\"sdpa\")\n ...\n ```\n "
        },
        {
            "sha": "7b22484b9a762b8af375604d4c3b063f6b902316",
            "filename": "docs/source/en/model_doc/gpt_neox_japanese.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox_japanese.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox_japanese.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox_japanese.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -47,7 +47,7 @@ The example below demonstrates how to generate text with [`Pipeline`] or the [`A\n import torch\n from transformers import pipeline\n pipeline = pipeline(task=\"text-generation\", \n-                    model=\"abeja/gpt-neox-japanese-2.7b\", torch_dtype=torch.float16, device=0)\n+                    model=\"abeja/gpt-neox-japanese-2.7b\", dtype=torch.float16, device=0)\n pipeline(\"äººã¨AIãŒå”èª¿ã™ã‚‹ãŸã‚ã«ã¯ã€\")\n ```\n \n@@ -58,7 +58,7 @@ pipeline(\"äººã¨AIãŒå”èª¿ã™ã‚‹ãŸã‚ã«ã¯ã€\")\n import torch\n from transformers import AutoModelForCausalLM, AutoTokenizer\n \n-model = AutoModelForCausalLM.from_pretrained(\"abeja/gpt-neox-japanese-2.7b\", torch_dtype=torch.float16, device_map=\"auto\")\n+model = AutoModelForCausalLM.from_pretrained(\"abeja/gpt-neox-japanese-2.7b\", dtype=torch.float16, device_map=\"auto\")\n tokenizer = AutoTokenizer.from_pretrained(\"abeja/gpt-neox-japanese-2.7b\")\n input_ids = tokenizer(\"äººã¨AIãŒå”èª¿ã™ã‚‹ãŸã‚ã«ã¯ã€\", return_tensors=\"pt\").input_ids.to(model.device)\n outputs = model.generate(input_ids)"
        },
        {
            "sha": "59e84daea5c50ba3a2e7062e6cf8a895517bbf27",
            "filename": "docs/source/en/model_doc/gptj.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fgptj.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fgptj.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgptj.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -33,7 +33,7 @@ This model was contributed by [Stella Biderman](https://huggingface.co/stellaath\n \n - To load [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B) in float32 one would need at least 2x model size\n   RAM: 1x for initial weights and another 1x to load the checkpoint. So for GPT-J it would take at least 48GB\n-  RAM to just load the model. To reduce the RAM usage there are a few options. The `torch_dtype` argument can be\n+  RAM to just load the model. To reduce the RAM usage there are a few options. The `dtype` argument can be\n   used to initialize the model in half-precision on a CUDA device only. There is also a fp16 branch which stores the fp16 weights,\n   which could be used to further minimize the RAM usage:\n \n@@ -45,7 +45,7 @@ This model was contributed by [Stella Biderman](https://huggingface.co/stellaath\n >>> model = GPTJForCausalLM.from_pretrained(\n ...     \"EleutherAI/gpt-j-6B\",\n ...     revision=\"float16\",\n-...     torch_dtype=torch.float16,\n+...     dtype=torch.float16,\n ... ).to(device)\n ```\n \n@@ -97,7 +97,7 @@ model.\n >>> import torch\n \n >>> device = infer_device()\n->>> model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", torch_dtype=torch.float16).to(device)\n+>>> model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", dtype=torch.float16).to(device)\n >>> tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n \n >>> prompt = ("
        },
        {
            "sha": "3f99caf7f6851b69e35c5741d598cbc28c52842c",
            "filename": "docs/source/en/model_doc/granite.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -44,7 +44,7 @@ from transformers import pipeline\n pipe = pipeline(\n     task=\"text-generation\",\n     model=\"ibm-granite/granite-3.3-2b-base\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device=0\n )\n pipe(\"Explain quantum computing in simple terms \", max_new_tokens=50)\n@@ -60,7 +60,7 @@ from transformers import AutoModelForCausalLM, AutoTokenizer\n tokenizer = AutoTokenizer.from_pretrained(\"ibm-granite/granite-3.3-2b-base\")\n model = AutoModelForCausalLM.from_pretrained(\n     \"ibm-granite/granite-3.3-2b-base\",                                          \n-    torch_dtype=torch.bfloat16, \n+    dtype=torch.bfloat16, \n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )\n@@ -88,7 +88,7 @@ from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n \n quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n tokenizer = AutoTokenizer.from_pretrained(\"ibm-granite/granite-3.3-8b-base\")\n-model = AutoModelForCausalLM.from_pretrained(\"ibm-granite/granite-3.3-8b-base\", torch_dtype=torch.bfloat16, device_map=\"auto\", attn_implementation=\"sdpa\", quantization_config=quantization_config)\n+model = AutoModelForCausalLM.from_pretrained(\"ibm-granite/granite-3.3-8b-base\", dtype=torch.bfloat16, device_map=\"auto\", attn_implementation=\"sdpa\", quantization_config=quantization_config)\n \n inputs = tokenizer(\"Explain quantum computing in simple terms\", return_tensors=\"pt\").to(model.device)\n outputs = model.generate(**inputs, max_length=50, cache_implementation=\"static\")\n@@ -99,7 +99,7 @@ quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n tokenizer = AutoTokenizer.from_pretrained(\"ibm-granite/granite-3.3-2b-base\")\n model = AutoModelForCausalLM.from_pretrained(\n     \"ibm-granite/granite-3.3-2b-base\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\",\n     quantization_config=quantization_config,"
        },
        {
            "sha": "7461a19a03275beeb6d17ee78d52946e93fbd921",
            "filename": "docs/source/en/model_doc/hgnet_v2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fhgnet_v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fhgnet_v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fhgnet_v2.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -43,7 +43,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"image-classification\",\n     model=\"ustc-community/hgnet-v2\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n pipeline(\"http://images.cocodataset.org/val2017/000000039769.jpg\")"
        },
        {
            "sha": "18c8062da36e558706b991bc8cf01aefddad90f5",
            "filename": "docs/source/en/model_doc/hubert.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fhubert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fhubert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fhubert.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -46,7 +46,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"automatic-speech-recognition\",\n     model=\"facebook/hubert-large-ls960-ft\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n \n@@ -65,7 +65,7 @@ dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", spli\n sampling_rate = dataset.features[\"audio\"].sampling_rate\n \n processor = AutoProcessor.from_pretrained(\"facebook/hubert-base-ls960\")\n-model = AutoModelForCTC.from_pretrained(\"facebook/hubert-base-ls960\", torch_dtype=torch.float16, device_map=\"auto\", attn_implementation=\"sdpa\")\n+model = AutoModelForCTC.from_pretrained(\"facebook/hubert-base-ls960\", dtype=torch.float16, device_map=\"auto\", attn_implementation=\"sdpa\")\n \n inputs = processor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n with torch.no_grad():\n@@ -100,7 +100,7 @@ dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", spli\n sampling_rate = dataset.features[\"audio\"].sampling_rate\n \n processor = AutoProcessor.from_pretrained(\"facebook/hubert-base-ls960\")\n-model = AutoModelForCTC.from_pretrained(\"facebook/hubert-base-ls960\", quantization_config=bnb_config, torch_dtype=torch.float16, device_map=\"auto\", attn_implementation=\"sdpa\")\n+model = AutoModelForCTC.from_pretrained(\"facebook/hubert-base-ls960\", quantization_config=bnb_config, dtype=torch.float16, device_map=\"auto\", attn_implementation=\"sdpa\")\n \n inputs = processor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n with torch.no_grad():"
        },
        {
            "sha": "63dd1ec8277d27334423933ec25d8574c821bd0f",
            "filename": "docs/source/en/model_doc/idefics2.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics2.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -163,7 +163,7 @@ To load and run a model using Flash Attention-2, simply change the code snippet\n ```diff\n model = Idefics2ForConditionalGeneration.from_pretrained(\n     \"HuggingFaceM4/idefics2-8b\",\n-+    torch_dtype=torch.float16,\n++    dtype=torch.float16,\n +    attn_implementation=\"flash_attention_2\",\n ).to(device)\n ```\n@@ -185,7 +185,7 @@ Quantizing a model is as simple as passing a `quantization_config` to the model.\n + )\n model = Idefics2ForConditionalGeneration.from_pretrained(\n     \"HuggingFaceM4/idefics2-8b\",\n-+    torch_dtype=torch.float16,\n++    dtype=torch.float16,\n +    quantization_config=quantization_config,\n ).to(device)\n ```"
        },
        {
            "sha": "9d7c7874f1a56b30aff7cff844b68bac90086a47",
            "filename": "docs/source/en/model_doc/ijepa.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fijepa.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fijepa.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fijepa.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -49,7 +49,7 @@ feature_extractor = pipeline(\n     task=\"image-feature-extraction\",\n     model=\"facebook/ijepa_vith14_1k\",\n     device=0,\n-    torch_dtype=torch.bfloat16\n+    dtype=torch.bfloat16\n )\n features = feature_extractor(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\", return_tensors=True)  \n \n@@ -73,7 +73,7 @@ image_1 = Image.open(requests.get(url_1, stream=True).raw)\n image_2 = Image.open(requests.get(url_2, stream=True).raw)\n \n processor = AutoProcessor.from_pretrained(\"facebook/ijepa_vith14_1k\")  \n-model = AutoModel.from_pretrained(\"facebook/ijepa_vith14_1k\", torch_dtype=\"auto\", attn_implementation=\"sdpa\")  \n+model = AutoModel.from_pretrained(\"facebook/ijepa_vith14_1k\", dtype=\"auto\", attn_implementation=\"sdpa\")  \n \n \n def infer(image):  \n@@ -113,7 +113,7 @@ image_1 = Image.open(requests.get(url_1, stream=True).raw)\n image_2 = Image.open(requests.get(url_2, stream=True).raw)\n \n processor = AutoProcessor.from_pretrained(\"facebook/ijepa_vitg16_22k\")\n-model = AutoModel.from_pretrained(\"facebook/ijepa_vitg16_22k\", quantization_config=quantization_config, torch_dtype=\"auto\", attn_implementation=\"sdpa\")\n+model = AutoModel.from_pretrained(\"facebook/ijepa_vitg16_22k\", quantization_config=quantization_config, dtype=\"auto\", attn_implementation=\"sdpa\")\n \n \n def infer(image):"
        },
        {
            "sha": "bf760fdbdd71cdb73f145d981b1b836f5c368bd0",
            "filename": "docs/source/en/model_doc/internvl.md",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Finternvl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Finternvl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Finternvl.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -88,7 +88,7 @@ This example demonstrates how to perform inference on a single image with the In\n \n >>> model_checkpoint = \"OpenGVLab/InternVL3-1B-hf\"\n >>> processor = AutoProcessor.from_pretrained(model_checkpoint)\n->>> model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=\"auto\", torch_dtype=torch.bfloat16)\n+>>> model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=\"auto\", dtype=torch.bfloat16)\n \n >>> messages = [\n ...     {\n@@ -119,7 +119,7 @@ This example shows how to generate text using the InternVL model without providi\n \n >>> model_checkpoint = \"OpenGVLab/InternVL3-1B-hf\"\n >>> processor = AutoProcessor.from_pretrained(model_checkpoint)\n->>> model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=\"auto\", torch_dtype=torch.bfloat16)\n+>>> model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=\"auto\", dtype=torch.bfloat16)\n \n >>> messages = [\n ...     {\n@@ -148,7 +148,7 @@ InternVL models also support batched image and text inputs.\n \n >>> model_checkpoint = \"OpenGVLab/InternVL3-1B-hf\"\n >>> processor = AutoProcessor.from_pretrained(model_checkpoint)\n->>> model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=\"auto\", torch_dtype=torch.bfloat16)\n+>>> model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=\"auto\", dtype=torch.bfloat16)\n \n >>> messages = [\n ...     [\n@@ -191,7 +191,7 @@ This implementation of the InternVL models supports batched text-images inputs w\n \n >>> model_checkpoint = \"OpenGVLab/InternVL3-1B-hf\"\n >>> processor = AutoProcessor.from_pretrained(model_checkpoint)\n->>> model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=\"auto\", torch_dtype=torch.bfloat16)\n+>>> model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=\"auto\", dtype=torch.bfloat16)\n \n >>> messages = [\n ... Â  Â  [\n@@ -273,7 +273,7 @@ This example showcases how to handle a batch of chat conversations with interlea\n \n >>> model_checkpoint = \"OpenGVLab/InternVL3-1B-hf\"\n >>> processor = AutoProcessor.from_pretrained(model_checkpoint)\n->>> model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=\"auto\", torch_dtype=torch.bfloat16)\n+>>> model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=\"auto\", dtype=torch.bfloat16)\n \n >>> messages = [\n ... Â  Â  ["
        },
        {
            "sha": "0aa06b16e90f8027b8669a34da576f06a9e53115",
            "filename": "docs/source/en/model_doc/jamba.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fjamba.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fjamba.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fjamba.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -48,7 +48,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"text-generation\",\n     model=\"ai21labs/AI21-Jamba-Mini-1.6\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n pipeline(\"Plants create energy through a process known as\")\n@@ -66,7 +66,7 @@ tokenizer = AutoTokenizer.from_pretrained(\n )\n model = AutoModelForCausalLM.from_pretrained(\n     \"ai21labs/AI21-Jamba-Large-1.6\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )\n@@ -99,7 +99,7 @@ quantization_config = BitsAndBytesConfig(load_in_8bit=True,\n # a device map to distribute the model evenly across 8 GPUs\n device_map = {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 2, 'model.layers.26': 2, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.layers.32': 3, 'model.layers.33': 3, 'model.layers.34': 3, 'model.layers.35': 3, 'model.layers.36': 4, 'model.layers.37': 4, 'model.layers.38': 4, 'model.layers.39': 4, 'model.layers.40': 4, 'model.layers.41': 4, 'model.layers.42': 4, 'model.layers.43': 4, 'model.layers.44': 4, 'model.layers.45': 5, 'model.layers.46': 5, 'model.layers.47': 5, 'model.layers.48': 5, 'model.layers.49': 5, 'model.layers.50': 5, 'model.layers.51': 5, 'model.layers.52': 5, 'model.layers.53': 5, 'model.layers.54': 6, 'model.layers.55': 6, 'model.layers.56': 6, 'model.layers.57': 6, 'model.layers.58': 6, 'model.layers.59': 6, 'model.layers.60': 6, 'model.layers.61': 6, 'model.layers.62': 6, 'model.layers.63': 7, 'model.layers.64': 7, 'model.layers.65': 7, 'model.layers.66': 7, 'model.layers.67': 7, 'model.layers.68': 7, 'model.layers.69': 7, 'model.layers.70': 7, 'model.layers.71': 7, 'model.final_layernorm': 7, 'lm_head': 7}\n model = AutoModelForCausalLM.from_pretrained(\"ai21labs/AI21-Jamba-Large-1.6\",\n-                                             torch_dtype=torch.bfloat16,\n+                                             dtype=torch.bfloat16,\n                     attn_implementation=\"flash_attention_2\",\n                                              quantization_config=quantization_config,\n                                              device_map=device_map)"
        },
        {
            "sha": "c815b29cdbc0fbb47b610c6ad4a807da121d97d3",
            "filename": "docs/source/en/model_doc/janus.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fjanus.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fjanus.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fjanus.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -65,8 +65,8 @@ messages = [\n \n # Set generation mode to `text` to perform text generation.\n processor = JanusProcessor.from_pretrained(model_id)\n-model = JanusForConditionalGeneration.from_pretrained(model_id,\n-        torch_dtype=torch.bfloat16,\n+model = JanusForConditionalGeneration.from_pretrained(model_id,     \n+        dtype=torch.bfloat16,\n         device_map=\"auto\")\n \n inputs = processor.apply_chat_template(\n@@ -128,7 +128,7 @@ messages = [\n # Load model and processor\n processor = JanusProcessor.from_pretrained(model_id)\n model = JanusForConditionalGeneration.from_pretrained(\n-    model_id, torch_dtype=torch.bfloat16, device_map=\"auto\"\n+    model_id, dtype=torch.bfloat16, device_map=\"auto\"\n )\n \n inputs = processor.apply_chat_template(\n@@ -160,7 +160,7 @@ from transformers import JanusForConditionalGeneration, JanusProcessor\n model_id = \"deepseek-community/Janus-Pro-1B\"\n processor = JanusProcessor.from_pretrained(model_id)\n model = JanusForConditionalGeneration.from_pretrained(model_id,\n-        torch_dtype=torch.bfloat16,\n+        dtype=torch.bfloat16,\n         device_map=\"auto\")\n \n messages = ["
        },
        {
            "sha": "fa92b172cc54082d27e7c5d336ae520f68f19477",
            "filename": "docs/source/en/model_doc/kosmos2_5.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fkosmos2_5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fkosmos2_5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fkosmos2_5.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -51,7 +51,7 @@ from transformers import AutoProcessor, Kosmos2_5ForConditionalGeneration, infer\n repo = \"ydshieh/kosmos-2.5\"\n device = f\"{infer_device()}:0\"\n dtype = torch.bfloat16\n-model = Kosmos2_5ForConditionalGeneration.from_pretrained(repo, device_map=device, torch_dtype=dtype)\n+model = Kosmos2_5ForConditionalGeneration.from_pretrained(repo, device_map=device, dtype=dtype)\n processor = AutoProcessor.from_pretrained(repo)\n \n # sample image\n@@ -90,7 +90,7 @@ from transformers import AutoProcessor, Kosmos2_5ForConditionalGeneration, infer\n repo = \"ydshieh/kosmos-2.5\"\n device = f\"{infer_device()}:0\"\n dtype = torch.bfloat16\n-model = Kosmos2_5ForConditionalGeneration.from_pretrained(repo, device_map=device, torch_dtype=dtype)\n+model = Kosmos2_5ForConditionalGeneration.from_pretrained(repo, device_map=device, dtype=dtype)\n processor = AutoProcessor.from_pretrained(repo)\n \n # sample image"
        },
        {
            "sha": "30497e69594c2014ca6710f225ff1f0efe4bbfd1",
            "filename": "docs/source/en/model_doc/kyutai_speech_to_text.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fkyutai_speech_to_text.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fkyutai_speech_to_text.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fkyutai_speech_to_text.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -40,7 +40,7 @@ torch_device = infer_device()\n model_id = \"kyutai/stt-2.6b-en-trfs\"\n \n processor = KyutaiSpeechToTextProcessor.from_pretrained(model_id)\n-model = KyutaiSpeechToTextForConditionalGeneration.from_pretrained(model_id, device_map=torch_device, torch_dtype=\"auto\")\n+model = KyutaiSpeechToTextForConditionalGeneration.from_pretrained(model_id, device_map=torch_device, dtype=\"auto\")\n \n # 2. load audio samples\n ds = load_dataset(\n@@ -73,7 +73,7 @@ torch_device = infer_device()\n model_id = \"kyutai/stt-2.6b-en-trfs\"\n \n processor = KyutaiSpeechToTextProcessor.from_pretrained(model_id)\n-model = KyutaiSpeechToTextForConditionalGeneration.from_pretrained(model_id, device_map=torch_device, torch_dtype=\"auto\")\n+model = KyutaiSpeechToTextForConditionalGeneration.from_pretrained(model_id, device_map=torch_device, dtype=\"auto\")\n \n # 2. load audio samples\n ds = load_dataset("
        },
        {
            "sha": "708a5bc1ab407c556ae9fdc9257de73246756a7f",
            "filename": "docs/source/en/model_doc/layoutlm.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlm.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -41,7 +41,7 @@ from datasets import load_dataset\n from transformers import AutoTokenizer, LayoutLMForQuestionAnswering\n \n tokenizer = AutoTokenizer.from_pretrained(\"impira/layoutlm-document-qa\", add_prefix_space=True)\n-model = LayoutLMForQuestionAnswering.from_pretrained(\"impira/layoutlm-document-qa\", torch_dtype=torch.float16)\n+model = LayoutLMForQuestionAnswering.from_pretrained(\"impira/layoutlm-document-qa\", dtype=torch.float16)\n \n dataset = load_dataset(\"nielsr/funsd\", split=\"train\")\n example = dataset[0]"
        },
        {
            "sha": "8a732ae85cff5e17532b1a4204a02102055b07b0",
            "filename": "docs/source/en/model_doc/led.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fled.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fled.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fled.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -44,7 +44,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"summarization\",\n     model=\"allenai/led-base-16384\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n pipeline(\"\"\"Plants are among the most remarkable and essential life forms on Earth, possessing a unique ability to produce their own food through a process known as photosynthesis. This complex biochemical process is fundamental not only to plant life but to virtually all life on the planet.\n@@ -65,7 +65,7 @@ tokenizer = AutoTokenizer.from_pretrained(\n )\n model = AutoModelForSeq2SeqLM.from_pretrained(\n     \"allenai/led-base-16384\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\"\n )\n \n@@ -107,7 +107,7 @@ quantization_config = BitsAndBytesConfig(\n )\n model = AutoModelForSeq2SeqLM.from_pretrained(\n     \"allenai/led-large-16384\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     quantization_config=quantization_config\n )"
        },
        {
            "sha": "3ea0936b96be59c49fe3563a67302e83a14f6726",
            "filename": "docs/source/en/model_doc/lfm2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Flfm2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Flfm2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flfm2.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -45,7 +45,7 @@ model_id = \"LiquidAI/LFM2-1.2B\"\n model = AutoModelForCausalLM.from_pretrained(\n     model_id,\n     device_map=\"auto\",\n-    torch_dtype=\"bfloat16\",\n+    dtype=\"bfloat16\",\n )\n tokenizer = AutoTokenizer.from_pretrained(model_id)\n "
        },
        {
            "sha": "20f483f641239f3f305654732b8ce1e55cbff8e3",
            "filename": "docs/source/en/model_doc/llama.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -45,7 +45,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"text-generation\",\n     model=\"huggyllama/llama-7b\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n pipeline(\"Plants create energy through a process known as\")\n@@ -63,7 +63,7 @@ tokenizer = AutoTokenizer.from_pretrained(\n )\n model = AutoModelForCausalLM.from_pretrained(\n     \"huggyllama/llama-7b\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )\n@@ -95,7 +95,7 @@ from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n quantization_config = TorchAoConfig(\"int4_weight_only\", group_size=128)\n model = AutoModelForCausalLM.from_pretrained(\n     \"huggyllama/llama-30b\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     quantization_config=quantization_config\n )"
        },
        {
            "sha": "96c733d88fa42ec3650dfeb73dcc7459acb1c62c",
            "filename": "docs/source/en/model_doc/llama2.md",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama2.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -45,7 +45,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"text-generation\",\n     model=\"meta-llama/Llama-2-7b-hf\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n pipeline(\"Plants create energy through a process known as\")\n@@ -63,7 +63,7 @@ tokenizer = AutoTokenizer.from_pretrained(\n )\n model = AutoModelForCausalLM.from_pretrained(\n     \"meta-llama/Llama-2-7b-hf\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )\n@@ -77,7 +77,7 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n <hfoption id=\"transformers CLI\">\n \n ```bash\n-transformers chat meta-llama/Llama-2-7b-chat-hf --torch_dtype auto --attn_implementation flash_attention_2\n+transformers chat meta-llama/Llama-2-7b-chat-hf --dtype auto --attn_implementation flash_attention_2\n ```\n \n </hfoption>\n@@ -95,7 +95,7 @@ from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n quantization_config = TorchAoConfig(\"int4_weight_only\", group_size=128)\n model = AutoModelForCausalLM.from_pretrained(\n     \"meta-llama/Llama-2-13b-hf\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     quantization_config=quantization_config\n )\n@@ -136,7 +136,7 @@ visualizer(\"Plants create energy through a process known as\")\n     self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.config.padding_idx)\n     ```\n - The tokenizer is a byte-pair encoding model based on [SentencePiece](https://github.com/google/sentencepiece). During decoding, if the first token is the start of the word (for example, \"Banana\"), the tokenizer doesn't prepend the prefix space to the string.\n-- Don't use the `torch_dtype` parameter in [`~AutoModel.from_pretrained`] if you're using FlashAttention-2 because it only supports fp16 or bf16. You should use [Automatic Mixed Precision](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html), set fp16 or bf16 to `True` if using [`Trainer`], or use [torch.autocast](https://pytorch.org/docs/stable/amp.html#torch.autocast).\n+- Don't use the `dtype` parameter in [`~AutoModel.from_pretrained`] if you're using FlashAttention-2 because it only supports fp16 or bf16. You should use [Automatic Mixed Precision](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html), set fp16 or bf16 to `True` if using [`Trainer`], or use [torch.autocast](https://pytorch.org/docs/stable/amp.html#torch.autocast).\n \n ## LlamaConfig\n "
        },
        {
            "sha": "1764617a7d4ff591821013a18b75cfc0120be8ef",
            "filename": "docs/source/en/model_doc/llama3.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama3.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -28,7 +28,7 @@ import torch\n \n model_id = \"meta-llama/Meta-Llama-3-8B\"\n \n-pipeline = transformers.pipeline(\"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\")\n+pipeline = transformers.pipeline(\"text-generation\", model=model_id, model_kwargs={\"dtype\": torch.bfloat16}, device_map=\"auto\")\n pipeline(\"Hey how are you doing today?\")\n ```\n \n@@ -47,10 +47,10 @@ The original code of the authors can be found [here](https://github.com/meta-lla\n \n <Tip warning={true}>\n \n-The `Llama3` models were trained using `bfloat16`, but the original inference uses `float16`. The checkpoints uploaded on the Hub use `torch_dtype = 'float16'`, which will be\n+The `Llama3` models were trained using `bfloat16`, but the original inference uses `float16`. The checkpoints uploaded on the Hub use `dtype = 'float16'`, which will be\n used by the `AutoModel` API to cast the checkpoints from `torch.float32` to `torch.float16`.\n \n-The `dtype` of the online weights is mostly irrelevant unless you are using `torch_dtype=\"auto\"` when initializing a model using `model = AutoModelForCausalLM.from_pretrained(\"path\", torch_dtype = \"auto\")`. The reason is that the model will first be downloaded ( using the `dtype` of the checkpoints online), then it will be casted to the default `dtype` of `torch` (becomes `torch.float32`), and finally, if there is a `torch_dtype` provided in the config, it will be used.\n+The `dtype` of the online weights is mostly irrelevant unless you are using `dtype=\"auto\"` when initializing a model using `model = AutoModelForCausalLM.from_pretrained(\"path\", dtype = \"auto\")`. The reason is that the model will first be downloaded ( using the `dtype` of the checkpoints online), then it will be casted to the default `dtype` of `torch` (becomes `torch.float32`), and finally, if there is a `dtype` or `torch_dtype` provided in the config, it will be used.\n \n Training the model in `float16` is not recommended and is known to produce `nan`; as such, the model should be trained in `bfloat16`.\n \n@@ -81,7 +81,7 @@ Tips:\n     Note that executing the script requires enough CPU RAM to host the whole model in float16 precision (even if the biggest versions\n     come in several checkpoints they each contain a part of each weight of the model, so we need to load them all in RAM). For the 75B model, it's thus 145GB of RAM needed.\n \n-- When using Flash Attention 2 via `attn_implementation=\"flash_attention_2\"`, don't pass `torch_dtype` to the `from_pretrained` class method and use Automatic Mixed-Precision training. When using `Trainer`, it is simply specifying either `fp16` or `bf16` to `True`. Otherwise, make sure you are using `torch.autocast`. This is required because the Flash Attention only support `fp16` and `bf16` data type.\n+- When using Flash Attention 2 via `attn_implementation=\"flash_attention_2\"`, don't pass `dtype` to the `from_pretrained` class method and use Automatic Mixed-Precision training. When using `Trainer`, it is simply specifying either `fp16` or `bf16` to `True`. Otherwise, make sure you are using `torch.autocast`. This is required because the Flash Attention only support `fp16` and `bf16` data type.\n \n ## Resources\n "
        },
        {
            "sha": "28e168b904393b63ea3a4fa797fe5f03c3f451ff",
            "filename": "docs/source/en/model_doc/llama4.md",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama4.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama4.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama4.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -71,7 +71,7 @@ pipe = pipeline(\n     \"text-generation\",\n     model=model_id,\n     device_map=\"auto\",\n-    torch_dtype=torch.bfloat16\n+    dtype=torch.bfloat16\n )\n \n output = pipe(messages, do_sample=False, max_new_tokens=200)\n@@ -97,7 +97,7 @@ inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, ret\n model = Llama4ForConditionalGeneration.from_pretrained(\n     model_id,\n     device_map=\"auto\",\n-    torch_dtype=torch.bfloat16\n+    dtype=torch.bfloat16\n )\n \n outputs = model.generate(**inputs.to(model.device), max_new_tokens=100)\n@@ -118,7 +118,7 @@ processor = AutoProcessor.from_pretrained(model_id)\n model = Llama4ForConditionalGeneration.from_pretrained(\n     model_id,\n     device_map=\"auto\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n )\n \n img_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\n@@ -162,7 +162,7 @@ processor = AutoProcessor.from_pretrained(model_id)\n model = Llama4ForConditionalGeneration.from_pretrained(\n     model_id,\n     device_map=\"auto\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n )\n \n url1 = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\n@@ -220,7 +220,7 @@ model = Llama4ForConditionalGeneration.from_pretrained(\n     model_id,\n     device_map=\"auto\",\n     attn_implementation=\"flex_attention\",\n-    torch_dtype=torch.bfloat16\n+    dtype=torch.bfloat16\n )\n \n messages = [\n@@ -275,7 +275,7 @@ model = Llama4ForConditionalGeneration.from_pretrained(\n     model_id,\n     attn_implementation=\"flex_attention\",\n     device_map=\"auto\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n )\n ```\n </hfoption>\n@@ -290,7 +290,7 @@ model = Llama4ForConditionalGeneration.from_pretrained(\n     model_id,\n     attn_implementation=\"sdpa\",\n     device_map=\"auto\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n )\n ```\n </hfoption>\n@@ -304,7 +304,7 @@ import torch\n model = Llama4ForConditionalGeneration.from_pretrained(\n     model_id,\n     device_map=\"auto\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n )\n ```\n </hfoption>\n@@ -341,7 +341,7 @@ inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, ret\n model = Llama4ForConditionalGeneration.from_pretrained(\n     model_id,\n     device_map=\"auto\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     quantization_config=FbgemmFp8Config()\n )\n \n@@ -371,7 +371,7 @@ inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, ret\n model = Llama4ForConditionalGeneration.from_pretrained(\n     model_id,\n     tp_plan=\"auto\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n )\n \n outputs = model.generate(**inputs.to(model.device), max_new_tokens=100)\n@@ -396,7 +396,7 @@ import torch\n model = Llama4ForConditionalGeneration.from_pretrained(\n     model_id,\n     device_map=\"auto\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n )\n ```\n "
        },
        {
            "sha": "1d7427b9015ed40f84de01f3ca76e4f5b3beb9d9",
            "filename": "docs/source/en/model_doc/llava.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -138,7 +138,7 @@ import torch\n from transformers import AutoProcessor, LlavaForConditionalGeneration\n \n # Load the model in half-precision\n-model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n+model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", dtype=torch.float16, device_map=\"auto\")\n processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n \n conversation = [\n@@ -174,7 +174,7 @@ import torch\n from transformers import AutoProcessor, LlavaForConditionalGeneration\n \n # Load the model in half-precision\n-model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n+model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", dtype=torch.float16, device_map=\"auto\")\n processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n \n "
        },
        {
            "sha": "e7ff4c896e2527a5d61c505fe20a5a6532d1ad76",
            "filename": "docs/source/en/model_doc/llava_next.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -48,7 +48,7 @@ pipeline = pipeline(\n     task=\"image-text-to-text\",  \n     model=\"llava-hf/llava-v1.6-mistral-7b-hf\",  \n     device=0,  \n-    torch_dtype=torch.bfloat16  \n+    dtype=torch.bfloat16  \n )  \n messages = [  \n     {  \n@@ -78,7 +78,7 @@ from transformers import AutoProcessor, LlavaNextForConditionalGeneration, infer\n device = infer_device()\n \n processor = AutoProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n-model = LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", torch_dtype=torch.float16).to(device)\n+model = LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", dtype=torch.float16).to(device)\n \n url = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"\n image = Image.open(requests.get(url, stream=True).raw)\n@@ -169,7 +169,7 @@ import requests, torch\n \n processor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n model = LlavaNextForConditionalGeneration.from_pretrained(\n-    \"llava-hf/llava-v1.6-mistral-7b-hf\", torch_dtype=torch.float16, device_map=\"auto\"\n+    \"llava-hf/llava-v1.6-mistral-7b-hf\", dtype=torch.float16, device_map=\"auto\"\n )\n \n # Load multiple images"
        },
        {
            "sha": "9379c1cc2ed6809c44e51d21138f28d403058b5b",
            "filename": "docs/source/en/model_doc/llava_next_video.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -130,7 +130,7 @@ import torch\n from transformers import LlavaNextVideoForConditionalGeneration, LlavaNextVideoProcessor\n \n # Load the model in half-precision\n-model = LlavaNextVideoForConditionalGeneration.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n+model = LlavaNextVideoForConditionalGeneration.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\", dtype=torch.float16, device_map=\"auto\")\n processor = LlavaNextVideoProcessor.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\")\n \n # Load the video as an np.array, sampling uniformly 8 frames (can sample more for longer videos)\n@@ -244,7 +244,7 @@ from transformers import LlavaNextVideoForConditionalGeneration\n \n model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n     \"llava-hf/LLaVA-NeXT-Video-7B-hf\", \n-    torch_dtype=torch.float16, \n+    dtype=torch.float16, \n     attn_implementation=\"flash_attention_2\",\n ).to(0)\n ```"
        },
        {
            "sha": "e546530922adf02dd8e4ac92080426feabf124ec",
            "filename": "docs/source/en/model_doc/llava_onevision.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -123,7 +123,7 @@ device = f\"{infer_device}:0\"\n processor = AutoProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\") \n model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n     \"llava-hf/llava-onevision-qwen2-7b-ov-hf\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=device\n )\n \n@@ -158,7 +158,7 @@ import torch\n from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\n \n # Load the model in half-precision\n-model = LlavaOnevisionForConditionalGeneration.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n+model = LlavaOnevisionForConditionalGeneration.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\", dtype=torch.float16, device_map=\"auto\")\n processor = AutoProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\")\n \n # Prepare a batch of two prompts, where the first one is a multi-turn conversation and the second is not\n@@ -221,7 +221,7 @@ import torch\n from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\n \n # Load the model in half-precision\n-model = LlavaOnevisionForConditionalGeneration.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n+model = LlavaOnevisionForConditionalGeneration.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\", dtype=torch.float16, device_map=\"auto\")\n processor = AutoProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\")\n \n video_path = hf_hub_download(repo_id=\"raushan-testing-hf/videos-test\", filename=\"sample_demo_1.mp4\", repo_type=\"dataset\")\n@@ -288,7 +288,7 @@ from transformers import LlavaOnevisionForConditionalGeneration\n \n model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n     model_id,\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     use_flash_attention_2=True\n ).to(0)\n ```"
        },
        {
            "sha": "c80294ab7a04c5ef7ff491a29ca5181717c772eb",
            "filename": "docs/source/en/model_doc/longformer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Flongformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Flongformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flongformer.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -40,7 +40,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"fill-mask\",\n     model=\"allenai/longformer-base-4096\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n pipeline(\"\"\"San Francisco 49ers cornerback Shawntae Spencer will miss the rest of the <mask> with a torn ligament in his left knee."
        },
        {
            "sha": "29d43af97a2f933f54dd1fc220d3124641901f25",
            "filename": "docs/source/en/model_doc/m2m_100.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fm2m_100.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fm2m_100.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fm2m_100.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -154,7 +154,7 @@ To load a model using Flash Attention 2, we can pass the argument `attn_implemen\n >>> import torch\n >>> from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n \n->>> model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\", device_map=\"auto\").eval()\n+>>> model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\", dtype=torch.float16, attn_implementation=\"flash_attention_2\", device_map=\"auto\").eval()\n >>> tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\n \n >>> # translate Hindi to French\n@@ -186,7 +186,7 @@ SDPA is used by default for `torch>=2.1.1` when an implementation is available,\n \n ```python\n from transformers import M2M100ForConditionalGeneration\n-model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\", torch_dtype=torch.float16, attn_implementation=\"sdpa\")\n+model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\", dtype=torch.float16, attn_implementation=\"sdpa\")\n ...\n ```\n "
        },
        {
            "sha": "d243bcf7e40dc3198e1157a9c3ce39d605ceca4d",
            "filename": "docs/source/en/model_doc/mamba.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -44,7 +44,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"text-generation\",\n     model=\"state-spaces/mamba-130m-hf\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n pipeline(\"Plants create energy through a process known as\")\n@@ -58,7 +58,7 @@ import torch\n from transformers import AutoModelForCausalLM, AutoTokenizer  \n \n tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\n-model = AutoModelForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\", torch_dtype=torch.float16, device_map=\"auto\",)  \n+model = AutoModelForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\", dtype=torch.float16, device_map=\"auto\",)  \n input_ids = tokenizer(\"Plants create energy through a process known as\", return_tensors=\"pt\").to(model.device)  \n \n output = model.generate(**input_ids)  \n@@ -87,7 +87,7 @@ from torchao.quantization import Int4WeightOnlyConfig\n quantization_config = Int4WeightOnlyConfig(group_size=128)\n quantization_config = TorchAoConfig(quant_type=quant_config)\n tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-2.8b-hf\")\n-model = AutoModelForCausalLM.from_pretrained(\"state-spaces/mamba-2.8b-hf\", torch_dtype=torch.bfloat16, quantization_config=quantization_config, device_map=\"auto\",)\n+model = AutoModelForCausalLM.from_pretrained(\"state-spaces/mamba-2.8b-hf\", dtype=torch.bfloat16, quantization_config=quantization_config, device_map=\"auto\",)\n input_ids = tokenizer(\"Plants create energy through a process known as\", return_tensors=\"pt\").to(model.device)\n \n output = model.generate(**input_ids)"
        },
        {
            "sha": "f8532f3cfbe6b18c2f3a51be3306c78f865825b1",
            "filename": "docs/source/en/model_doc/mamba2.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba2.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -42,7 +42,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"text-generation\",\n     model=\"mistralai/Mamba-Codestral-7B-v0.1\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device=0\n )\n pipeline(\"Plants create energy through a process known as\")\n@@ -56,7 +56,7 @@ import torch\n from transformers import AutoModelForCausalLM, AutoTokenizer  \n \n tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mamba-Codestral-7B-v0.1\")\n-model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mamba-Codestral-7B-v0.1\", torch_dtype=torch.bfloat16, device_map=\"auto\")  \n+model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mamba-Codestral-7B-v0.1\", dtype=torch.bfloat16, device_map=\"auto\")  \n input_ids = tokenizer(\"Plants create energy through a process known as\", return_tensors=\"pt\").to(model.device)  \n \n output = model.generate(**input_ids)  \n@@ -83,7 +83,7 @@ from transformers import AutoModelForCausalLM, AutoTokenizer, TorchAoConfig\n \n quantization_config = TorchAoConfig(\"int4_weight_only\", group_size=128)\n tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mamba-Codestral-7B-v0.1\")\n-model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mamba-Codestral-7B-v0.1\", torch_dtype=torch.bfloat16, quantization_config=quantization_config, device_map=\"auto\")\n+model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mamba-Codestral-7B-v0.1\", dtype=torch.bfloat16, quantization_config=quantization_config, device_map=\"auto\")\n input_ids = tokenizer(\"Plants create energy through a process known as\", return_tensors=\"pt\").to(model.device)\n \n output = model.generate(**input_ids)"
        },
        {
            "sha": "4b08ac1901ca2542f9146b70564eeebe11ac4802",
            "filename": "docs/source/en/model_doc/marian.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fmarian.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fmarian.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmarian.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -52,7 +52,7 @@ The example below demonstrates how to translate text using [`Pipeline`] or the [\n import torch\n from transformers import pipeline\n \n-pipeline = pipeline(\"translation_en_to_de\", model=\"Helsinki-NLP/opus-mt-en-de\", torch_dtype=torch.float16, device=0)\n+pipeline = pipeline(\"translation_en_to_de\", model=\"Helsinki-NLP/opus-mt-en-de\", dtype=torch.float16, device=0)\n pipeline(\"Hello, how are you?\")\n \n ```\n@@ -67,7 +67,7 @@ import torch\n from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n \n tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n-model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\", torch_dtype=torch.float16, attn_implementation=\"sdpa\", device_map=\"auto\")\n+model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\", dtype=torch.float16, attn_implementation=\"sdpa\", device_map=\"auto\")\n \n inputs = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\").to(model.device)\n outputs = model.generate(**inputs, cache_implementation=\"static\")"
        },
        {
            "sha": "eca0173203756154bf9fcb058943440c6625b4c7",
            "filename": "docs/source/en/model_doc/mbart.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fmbart.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fmbart.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmbart.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -50,7 +50,7 @@ pipeline = pipeline(\n     task=\"translation\",\n     model=\"facebook/mbart-large-50-many-to-many-mmt\",\n     device=0,\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     src_lang=\"en_XX\",\n     tgt_lang=\"fr_XX\",\n )\n@@ -66,7 +66,7 @@ from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n \n article_en = \"UN Chief Says There Is No Military Solution in Syria\"\n \n-model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\", torch_dtype=torch.bfloat16, attn_implementation=\"sdpa\", device_map=\"auto\")\n+model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\", dtype=torch.bfloat16, attn_implementation=\"sdpa\", device_map=\"auto\")\n tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n \n tokenizer.src_lang = \"en_XX\"\n@@ -88,7 +88,7 @@ print(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True))\n     import torch\n     from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n \n-    model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/mbart-large-en-ro\", torch_dtype=torch.bfloat16, attn_implementation=\"sdpa\", device_map=\"auto\")\n+    model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/mbart-large-en-ro\", dtype=torch.bfloat16, attn_implementation=\"sdpa\", device_map=\"auto\")\n     tokenizer = MBartTokenizer.from_pretrained(\"facebook/mbart-large-en-ro\", src_lang=\"en_XX\")\n \n     article = \"UN Chief Says There Is No Military Solution in Syria\"\n@@ -105,7 +105,7 @@ print(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True))\n     import torch\n     from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n \n-    model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\", torch_dtype=torch.bfloat16, attn_implementation=\"sdpa\", device_map=\"auto\")\n+    model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\", dtype=torch.bfloat16, attn_implementation=\"sdpa\", device_map=\"auto\")\n     tokenizer = MBartTokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n \n     article_ar = \"Ø§Ù„Ø£Ù…ÙŠÙ† Ø§Ù„Ø¹Ø§Ù… Ù„Ù„Ø£Ù…Ù… Ø§Ù„Ù…ØªØ­Ø¯Ø© ÙŠÙ‚ÙˆÙ„ Ø¥Ù†Ù‡ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø­Ù„ Ø¹Ø³ÙƒØ±ÙŠ ÙÙŠ Ø³ÙˆØ±ÙŠØ§.\""
        },
        {
            "sha": "62d14fa66e18d1588f7ff111e3960425549cec9c",
            "filename": "docs/source/en/model_doc/metaclip_2.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fmetaclip_2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fmetaclip_2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmetaclip_2.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -49,7 +49,7 @@ from transformers import pipeline\n clip = pipeline(\n    task=\"zero-shot-image-classification\",\n    model=\"facebook/metaclip-2-worldwide-huge-quickgelu\",\n-   torch_dtype=torch.bfloat16,\n+   dtype=torch.bfloat16,\n    device=0\n )\n labels = [\"a photo of a cat\", \"a photo of a dog\", \"a photo of a car\"]\n@@ -65,7 +65,7 @@ import torch\n from PIL import Image\n from transformers import AutoProcessor, AutoModel\n \n-model = AutoModel.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\", torch_dtype=torch.bfloat16, attn_implementation=\"sdpa\")\n+model = AutoModel.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\", dtype=torch.bfloat16, attn_implementation=\"sdpa\")\n processor = AutoProcessor.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n \n url = \"http://images.cocodataset.org/val2017/000000039769.jpg\""
        },
        {
            "sha": "02d016c019ce5c7cbf62a86c8952ad209f94759c",
            "filename": "docs/source/en/model_doc/minimax.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fminimax.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fminimax.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fminimax.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -94,7 +94,7 @@ To load and run a model using Flash Attention-2, refer to the snippet below:\n >>> import torch\n >>> from transformers import AutoModelForCausalLM, AutoTokenizer\n \n->>> model = AutoModelForCausalLM.from_pretrained(\"MiniMaxAI/MiniMax-Text-01-hf\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\", device_map=\"auto\")\n+>>> model = AutoModelForCausalLM.from_pretrained(\"MiniMaxAI/MiniMax-Text-01-hf\", dtype=torch.float16, attn_implementation=\"flash_attention_2\", device_map=\"auto\")\n >>> tokenizer = AutoTokenizer.from_pretrained(\"MiniMaxAI/MiniMax-Text-01-hf\")\n \n >>> prompt = \"My favourite condiment is\""
        },
        {
            "sha": "3714f45e55a071aee3a20157b95b7e6a8129e111",
            "filename": "docs/source/en/model_doc/mistral.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -49,7 +49,7 @@ The example below demonstrates how to chat with [`Pipeline`] or the [`AutoModel`\n ...     {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n ... ]\n \n->>> chatbot = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.3\", torch_dtype=torch.bfloat16, device=0)\n+>>> chatbot = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.3\", dtype=torch.bfloat16, device=0)\n >>> chatbot(messages)\n ```\n \n@@ -60,7 +60,7 @@ The example below demonstrates how to chat with [`Pipeline`] or the [`AutoModel`\n >>> import torch\n >>> from transformers import AutoModelForCausalLM, AutoTokenizer\n \n->>> model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\", torch_dtype=torch.bfloat16, attn_implementation=\"sdpa\", device_map=\"auto\")\n+>>> model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\", dtype=torch.bfloat16, attn_implementation=\"sdpa\", device_map=\"auto\")\n >>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n \n >>> messages = [\n@@ -80,7 +80,7 @@ The example below demonstrates how to chat with [`Pipeline`] or the [`AutoModel`\n <hfoption id=\"transformers CLI\">\n \n ```python\n-echo -e \"My favorite condiment is\" | transformers chat mistralai/Mistral-7B-v0.3 --torch_dtype auto --device 0 --attn_implementation flash_attention_2\n+echo -e \"My favorite condiment is\" | transformers chat mistralai/Mistral-7B-v0.3 --dtype auto --device 0 --attn_implementation flash_attention_2\n ```\n \n </hfoption>\n@@ -102,7 +102,7 @@ The example below uses [bitsandbytes](../quantization/bitsandbytes) to only quan\n ...         bnb_4bit_compute_dtype=\"torch.float16\",\n ... )\n \n->>> model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\", quantization_config=True, torch_dtype=torch.bfloat16, device_map=\"auto\")\n+>>> model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\", quantization_config=True, dtype=torch.bfloat16, device_map=\"auto\")\n >>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n \n >>> prompt = \"My favourite condiment is\""
        },
        {
            "sha": "54af880ed46755ba409742b2db2bb4d445d9e6e5",
            "filename": "docs/source/en/model_doc/mistral3.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral3.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -54,7 +54,7 @@ messages = [\n pipeline = pipeline(\n     task=\"image-text-to-text\", \n     model=\"mistralai/Mistral-Small-3.1-24B-Instruct-2503\", \n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device=0\n )\n outputs = pipeline(text=messages, max_new_tokens=50, return_full_text=False)\n@@ -75,7 +75,7 @@ processor = AutoProcessor.from_pretrained(model_checkpoint)\n model = AutoModelForImageTextToText.from_pretrained(\n     model_checkpoint, \n     device_map=torch_device, \n-    torch_dtype=torch.bfloat16\n+    dtype=torch.bfloat16\n )\n \n messages = [\n@@ -113,7 +113,7 @@ from transformers import AutoProcessor, AutoModelForImageTextToText, infer_devic\n torch_device = infer_device()\n model_checkpoint = \".mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n processor = AutoProcessor.from_pretrained(model_checkpoint)\n-model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16)\n+model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=torch_device, dtype=torch.bfloat16)\n \n SYSTEM_PROMPT = \"You are a conversational agent that always answers straight to the point, always end your accurate response with an ASCII drawing of a cat.\"\n user_prompt = \"Give me 5 non-formal ways to say 'See you later' in French.\"\n@@ -150,7 +150,7 @@ from transformers import AutoProcessor, AutoModelForImageTextToText, infer_devic\n torch_device = infer_device()\n model_checkpoint = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n processor = AutoProcessor.from_pretrained(model_checkpoint)\n-model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16)\n+model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=torch_device, dtype=torch.bfloat16)\n \n messages = [\n      ["
        },
        {
            "sha": "ff501cd1a84ddc8731fc46c0a94124b04c3c83f0",
            "filename": "docs/source/en/model_doc/mixtral.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fmixtral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fmixtral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmixtral.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -116,7 +116,7 @@ To load and run a model using Flash Attention-2, refer to the snippet below:\n >>> import torch\n >>> from transformers import AutoModelForCausalLM, AutoTokenizer\n \n->>> model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\", device_map=\"auto\")\n+>>> model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\", dtype=torch.float16, attn_implementation=\"flash_attention_2\", device_map=\"auto\")\n >>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")\n \n >>> prompt = \"My favourite condiment is\""
        },
        {
            "sha": "1ea7f172bb3a268d86616c8412bc0af2afcfd06d",
            "filename": "docs/source/en/model_doc/mllama.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fmllama.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fmllama.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmllama.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -63,7 +63,7 @@ import torch\n from transformers import MllamaForConditionalGeneration, AutoProcessor\n \n model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n-model = MllamaForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.bfloat16)\n+model = MllamaForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\", dtype=torch.bfloat16)\n processor = AutoProcessor.from_pretrained(model_id)\n \n messages = [\n@@ -90,7 +90,7 @@ from PIL import Image\n from transformers import MllamaForConditionalGeneration, AutoProcessor\n \n model_id = \"meta-llama/Llama-3.2-11B-Vision\"\n-model = MllamaForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.bfloat16)\n+model = MllamaForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\", dtype=torch.bfloat16)\n processor = AutoProcessor.from_pretrained(model_id)\n \n prompt = \"<|image|>If I had to write a haiku for this one\""
        },
        {
            "sha": "4e3cc2e5d6477b9164c8dfaa98e5db8e1cd4e97e",
            "filename": "docs/source/en/model_doc/mobilebert.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilebert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilebert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilebert.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -42,7 +42,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"fill-mask\",\n     model=\"google/mobilebert-uncased\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n pipeline(\"The capital of France is [MASK].\")\n@@ -59,7 +59,7 @@ tokenizer = AutoTokenizer.from_pretrained(\n )\n model = AutoModelForMaskedLM.from_pretrained(\n     \"google/mobilebert-uncased\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n )\n inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\").to(model.device)"
        },
        {
            "sha": "c77bef730423d705607ef9d014571bd78dbac21d",
            "filename": "docs/source/en/model_doc/mobilenet_v1.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v1.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v1.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v1.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -43,7 +43,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"image-classification\",\n     model=\"google/mobilenet_v1_1.0_224\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n pipeline(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")"
        },
        {
            "sha": "3e1379e3f07900557b2e20766e8c2682b3280592",
            "filename": "docs/source/en/model_doc/mobilenet_v2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilenet_v2.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -44,7 +44,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"image-classification\",\n     model=\"google/mobilenet_v2_1.4_224\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n pipeline(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")"
        },
        {
            "sha": "b4a51bd200f221a91fc34a8120140fefc230fd4b",
            "filename": "docs/source/en/model_doc/mobilevit.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilevit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilevit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilevit.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -52,7 +52,7 @@ from transformers import pipeline\n classifier = pipeline(\n    task=\"image-classification\",\n    model=\"apple/mobilevit-small\",\n-   torch_dtype=torch.float16, device=0,\n+   dtype=torch.float16, device=0,\n )\n \n preds = classifier(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")"
        },
        {
            "sha": "013b9d24b5f4f747706ee732b4cc5450a561f886",
            "filename": "docs/source/en/model_doc/modernbert-decoder.md",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fmodernbert-decoder.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fmodernbert-decoder.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmodernbert-decoder.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -48,7 +48,7 @@ from transformers import pipeline\n generator = pipeline(\n     task=\"text-generation\",\n     model=\"jhu-clsp/ettin-decoder-17m\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n generator(\"The future of artificial intelligence is\", max_length=50, num_return_sequences=1)\n@@ -57,7 +57,7 @@ generator(\"The future of artificial intelligence is\", max_length=50, num_return_\n classifier = pipeline(\n     task=\"text-classification\",\n     model=\"jhu-clsp/ettin-decoder-17m\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n classifier(\"This movie is really great!\")\n@@ -73,7 +73,7 @@ from transformers import AutoModelForCausalLM, AutoTokenizer\n tokenizer = AutoTokenizer.from_pretrained(\"jhu-clsp/ettin-decoder-17m\")\n model = AutoModelForCausalLM.from_pretrained(\n     \"jhu-clsp/ettin-decoder-17m\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n )\n \n@@ -98,7 +98,7 @@ from transformers import AutoModelForSequenceClassification\n \n classifier_model = AutoModelForSequenceClassification.from_pretrained(\n     \"jhu-clsp/ettin-decoder-17m\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n     num_labels=2\n )\n@@ -130,7 +130,7 @@ quantization_config = BitsAndBytesConfig(\n tokenizer = AutoTokenizer.from_pretrained(\"jhu-clsp/ettin-decoder-1b\")\n model = AutoModelForCausalLM.from_pretrained(\n     \"jhu-clsp/ettin-decoder-1b\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n     quantization_config=quantization_config\n )"
        },
        {
            "sha": "baef3ca863e18deb88a4baf7bb8626864987ae03",
            "filename": "docs/source/en/model_doc/modernbert.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fmodernbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fmodernbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmodernbert.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -44,7 +44,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"fill-mask\",\n     model=\"answerdotai/ModernBERT-base\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n pipeline(\"Plants create [MASK] through a process known as photosynthesis.\")\n@@ -62,7 +62,7 @@ tokenizer = AutoTokenizer.from_pretrained(\n )\n model = AutoModelForMaskedLM.from_pretrained(\n     \"answerdotai/ModernBERT-base\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )"
        },
        {
            "sha": "7abe123b88e25823d626dafa44dbf285375b38a9",
            "filename": "docs/source/en/model_doc/moonshine.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fmoonshine.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fmoonshine.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmoonshine.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -44,7 +44,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"automatic-speech-recognition\",\n     model=\"UsefulSensors/moonshine-base\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n pipeline(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n@@ -64,7 +64,7 @@ processor = AutoProcessor.from_pretrained(\n )\n model = MoonshineForConditionalGeneration.from_pretrained(\n     \"UsefulSensors/moonshine-base\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )"
        },
        {
            "sha": "fa02ee4c3c08f1862403dfda6baf371bc3399de7",
            "filename": "docs/source/en/model_doc/mt5.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fmt5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fmt5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmt5.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -44,7 +44,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"text2text-generation\",\n     model=\"csebuetnlp/mT5_multilingual_XLSum\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n pipeline(\"\"\"Plants are remarkable organisms that produce their own food using a method called photosynthesis.\n@@ -64,7 +64,7 @@ tokenizer = AutoTokenizer.from_pretrained(\n )\n model = AutoModelForSeq2SeqLM.from_pretrained(\n     \"csebuetnlp/mT5_multilingual_XLSum\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n )\n \n@@ -102,7 +102,7 @@ quantization_config = BitsAndBytesConfig(\n )\n model = AutoModelForSeq2SeqLM.from_pretrained(\n     \"csebuetnlp/mT5_multilingual_XLSum\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     quantization_config=quantization_config\n )"
        },
        {
            "sha": "360a6ba22267c2fd33498eec39569587f0cb6b29",
            "filename": "docs/source/en/model_doc/nemotron.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fnemotron.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fnemotron.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnemotron.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -65,7 +65,7 @@ tokenizer  = AutoTokenizer.from_pretrained(model_path)\n \n device = infer_device()\n dtype  = torch.bfloat16\n-model  = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=dtype, device_map=device)\n+model  = AutoModelForCausalLM.from_pretrained(model_path, dtype=dtype, device_map=device)\n \n # Prepare the input text\n prompt = 'Complete the paragraph: our solar system is'"
        },
        {
            "sha": "95c3bf3c9d2dfa72bbec14cc727a604a5e504e88",
            "filename": "docs/source/en/model_doc/nllb.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -45,7 +45,7 @@ The example below demonstrates how to translate text with [`Pipeline`] or the [`\n import torch\n from transformers import pipeline\n \n-pipeline = pipeline(task=\"translation\", model=\"facebook/nllb-200-distilled-600M\", src_lang=\"eng_Latn\", tgt_lang=\"fra_Latn\", torch_dtype=torch.float16, device=0)\n+pipeline = pipeline(task=\"translation\", model=\"facebook/nllb-200-distilled-600M\", src_lang=\"eng_Latn\", tgt_lang=\"fra_Latn\", dtype=torch.float16, device=0)\n pipeline(\"UN Chief says there is no military solution in Syria\")\n ```\n \n@@ -56,7 +56,7 @@ pipeline(\"UN Chief says there is no military solution in Syria\")\n from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n \n tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n-model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\", torch_dtype=\"auto\", attn_implementaiton=\"sdpa\")\n+model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\", dtype=\"auto\", attn_implementaiton=\"sdpa\")\n \n article = \"UN Chief says there is no military solution in Syria\"\n inputs = tokenizer(article, return_tensors=\"pt\")"
        },
        {
            "sha": "197e8bfa7aa9108cacf1adc0594e81dd6d64cfe3",
            "filename": "docs/source/en/model_doc/olmo.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -46,7 +46,7 @@ from transformers import pipeline\n pipe = pipeline(\n     task=\"text-generation\",\n     model=\"allenai/OLMo-7B-hf\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0,\n )\n \n@@ -67,7 +67,7 @@ tokenizer = AutoTokenizer.from_pretrained(\n \n model = AutoModelForCausalLM.from_pretrained(\n     \"allenai/OLMo-7B-hf\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )\n@@ -105,7 +105,7 @@ quantization_config = BitsAndBytesConfig(\n model = AutoModelForCausalLM.from_pretrained(\n     \"allenai/OLMo-7B-hf\",\n     attn_implementation=\"sdpa\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n     quantization_config=quantization_config\n )"
        },
        {
            "sha": "158909c085c3180467dc50a82d6c6cf0c2244c69",
            "filename": "docs/source/en/model_doc/olmo2.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo2.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -43,7 +43,7 @@ from transformers import pipeline\n pipe = pipeline(\n     task=\"text-generation\",\n     model=\"allenai/OLMo-2-0425-1B\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0,\n )\n     \n@@ -64,7 +64,7 @@ tokenizer = AutoTokenizer.from_pretrained(\n \n model = AutoModelForCausalLM.from_pretrained(\n     \"allenai/OLMo-2-0425-1B\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )\n@@ -105,7 +105,7 @@ tokenizer = AutoTokenizer.from_pretrained(\n model = AutoModelForCausalLM.from_pretrained(\n     \"allenai/OLMo-2-0425-1B\",\n     quantization_config=torchao_config,\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )"
        },
        {
            "sha": "3b771a47e638e5a7ff8c035e08c39bcf20f6abd7",
            "filename": "docs/source/en/model_doc/olmoe.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Folmoe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Folmoe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Folmoe.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -46,7 +46,7 @@ from transformers import pipeline\n pipe = pipeline(\n     task=\"text-generation\",\n     model=\"allenai/OLMoE-1B-7B-0125\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0,\n )\n \n@@ -63,7 +63,7 @@ from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device\n \n device = infer_device()\n \n-model = AutoModelForCausalLM.from_pretrained(\"allenai/OLMoE-1B-7B-0924\", attn_implementation=\"sdpa\", torch_dtype=\"auto\", device_map=\"auto\").to(device)\n+model = AutoModelForCausalLM.from_pretrained(\"allenai/OLMoE-1B-7B-0924\", attn_implementation=\"sdpa\", dtype=\"auto\", device_map=\"auto\").to(device)\n tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMoE-1B-7B-0924\")\n \n inputs = tokenizer(\"Bitcoin is\", return_tensors=\"pt\")\n@@ -90,7 +90,7 @@ quantization_config = BitsAndBytesConfig(\n    bnb_4bit_quant_type=\"nf4\"\n )\n \n-model = AutoModelForCausalLM.from_pretrained(\"allenai/OLMoE-1B-7B-0924\", attn_implementation=\"sdpa\", torch_dtype=\"auto\", device_map=\"auto\", quantization_config=quantization_config).to(device)\n+model = AutoModelForCausalLM.from_pretrained(\"allenai/OLMoE-1B-7B-0924\", attn_implementation=\"sdpa\", dtype=\"auto\", device_map=\"auto\", quantization_config=quantization_config).to(device)\n tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMoE-1B-7B-0924\")\n \n inputs = tokenizer(\"Bitcoin is\", return_tensors=\"pt\")"
        },
        {
            "sha": "b45b205e25925ff0dfa4781cf365817a68f9b727",
            "filename": "docs/source/en/model_doc/openai-gpt.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fopenai-gpt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fopenai-gpt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fopenai-gpt.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -49,7 +49,7 @@ The example below demonstrates how to generate text with [`Pipeline`], [`AutoMod\n import torch\n from transformers import pipeline\n \n-generator = pipeline(task=\"text-generation\", model=\"openai-community/gpt\", torch_dtype=torch.float16, device=0)\n+generator = pipeline(task=\"text-generation\", model=\"openai-community/gpt\", dtype=torch.float16, device=0)\n output = generator(\"The future of AI is\", max_length=50, do_sample=True)\n print(output[0][\"generated_text\"])\n ```\n@@ -61,7 +61,7 @@ print(output[0][\"generated_text\"])\n from transformers import AutoModelForCausalLM, AutoTokenizer\n \n tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt\")\n-model = AutoModelForCausalLM.from_pretrained(\"openai-community/openai-gpt\", torch_dtype=torch.float16)\n+model = AutoModelForCausalLM.from_pretrained(\"openai-community/openai-gpt\", dtype=torch.float16)\n \n inputs = tokenizer(\"The future of AI is\", return_tensors=\"pt\")\n outputs = model.generate(**inputs, max_length=50)"
        },
        {
            "sha": "ec9cc474b6e4ea739926f16a1871e323337abab8",
            "filename": "docs/source/en/model_doc/opt.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fopt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fopt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fopt.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -44,7 +44,7 @@ The example below demonstrates how to generate text with [`Pipeline`], [`AutoMod\n import torch\n from transformers import pipeline\n \n-pipeline = pipeline(task=\"text-generation\", model=\"facebook/opt-125m\", torch_dtype=torch.float16, device=0)\n+pipeline = pipeline(task=\"text-generation\", model=\"facebook/opt-125m\", dtype=torch.float16, device=0)\n pipeline(\"Once upon a time, in a land far, far away,\", max_length=50, num_return_sequences=1)\n ```\n \n@@ -55,7 +55,7 @@ pipeline(\"Once upon a time, in a land far, far away,\", max_length=50, num_return\n import torch\n from transformers import AutoModelForCausalLM, AutoTokenizer\n \n-model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", torch_dtype=torch.float16, device_map=\"auto\", attn_implementation=\"sdpa\")\n+model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", dtype=torch.float16, device_map=\"auto\", attn_implementation=\"sdpa\")\n tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n \n prompt = (\"Once upon a time, in a land far, far away, \")\n@@ -85,7 +85,7 @@ from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n device = infer_device()\n \n bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n-model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-13b\", torch_dtype=torch.float16, attn_implementation=\"sdpa\", quantization_config=bnb_config).to(device)\n+model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-13b\", dtype=torch.float16, attn_implementation=\"sdpa\", quantization_config=bnb_config).to(device)\n tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-13b\")\n \n prompt = (\"Once upon a time, in a land far, far away, \")"
        },
        {
            "sha": "d7fcad940a23b1ed9c6b3cb079a6517438684360",
            "filename": "docs/source/en/model_doc/ovis2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fovis2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fovis2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fovis2.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -44,7 +44,7 @@ device = f\"{infer_device()}:0\"\n \n model = AutoModelForVision2Seq.from_pretrained(\n     \"thisisiron/Ovis2-2B-hf\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n ).eval().to(device)\n processor = AutoProcessor.from_pretrained(\"thisisiron/Ovis2-2B-hf\")\n "
        },
        {
            "sha": "58aa622a0d371f9934f2136954c558ea71e4e210",
            "filename": "docs/source/en/model_doc/paligemma.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fpaligemma.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fpaligemma.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpaligemma.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -47,7 +47,7 @@ pipeline = pipeline(\n     task=\"image-text-to-text\",\n     model=\"google/paligemma2-3b-mix-224\",\n     device=0,\n-    torch_dtype=torch.bfloat16\n+    dtype=torch.bfloat16\n )\n pipeline(\n     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n@@ -66,7 +66,7 @@ from transformers import AutoProcessor, PaliGemmaForConditionalGeneration\n \n model = PaliGemmaForConditionalGeneration.from_pretrained(\n     \"google/paligemma2-3b-mix-224\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )\n@@ -100,7 +100,7 @@ from transformers import TorchAoConfig, AutoProcessor, PaliGemmaForConditionalGe\n quantization_config = TorchAoConfig(\"int4_weight_only\", group_size=128)\n model = PaliGemmaForConditionalGeneration.from_pretrained(\n     \"google/paligemma2-28b-mix-224\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     quantization_config=quantization_config\n )"
        },
        {
            "sha": "9b92bda82a470d878c94b1489bb3de5c4f6bed19",
            "filename": "docs/source/en/model_doc/pegasus.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fpegasus.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fpegasus.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpegasus.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -44,7 +44,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"summarization\",\n     model=\"google/pegasus-xsum\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n pipeline(\"\"\"Plants are remarkable organisms that produce their own food using a method called photosynthesis.\n@@ -64,7 +64,7 @@ tokenizer = AutoTokenizer.from_pretrained(\n )\n model = AutoModelForSeq2SeqLM.from_pretrained(\n     \"google/pegasus-xsum\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )\n@@ -103,7 +103,7 @@ quantization_config = BitsAndBytesConfig(\n )\n model = AutoModelForSeq2SeqLM.from_pretrained(\n     \"google/pegasus-xsum\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     quantization_config=quantization_config\n )"
        },
        {
            "sha": "791618c67d302d95ae7a196202c4a1cf677d853c",
            "filename": "docs/source/en/model_doc/pegasus_x.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fpegasus_x.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fpegasus_x.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpegasus_x.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -45,7 +45,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"summarization\",\n     model=\"google/pegasus-x-large\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device=0\n )\n pipeline(\"\"\"Plants are among the most remarkable and essential life forms on Earth, possessing a unique ability to produce their own food through a process known as photosynthesis. This complex biochemical process is fundamental not only to plant life but to virtually all life on the planet.\n@@ -65,7 +65,7 @@ tokenizer = AutoTokenizer.from_pretrained(\n )\n model = AutoModelForSeq2SeqLM.from_pretrained(\n     \"google/pegasus-x-large\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n )\n \n@@ -102,7 +102,7 @@ quantization_config = BitsAndBytesConfig(\n )\n model = AutoModelForSeq2SeqLM.from_pretrained(\n     \"google/pegasus-x-large\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     quantization_config=quantization_config\n )"
        },
        {
            "sha": "764c959879adc2f4ec52af930b6eaa35bf1a11b7",
            "filename": "docs/source/en/model_doc/persimmon.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fpersimmon.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fpersimmon.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpersimmon.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -38,10 +38,10 @@ The original code can be found [here](https://github.com/persimmon-ai-labs/adept\n \n <Tip warning={true}>\n \n-The `Persimmon` models were trained using `bfloat16`, but the original inference uses `float16` The checkpoints uploaded on the hub use `torch_dtype = 'float16'` which will be\n+The `Persimmon` models were trained using `bfloat16`, but the original inference uses `float16` The checkpoints uploaded on the hub use `dtype = 'float16'` which will be\n used by the `AutoModel` API to cast the checkpoints from `torch.float32` to `torch.float16`. \n \n-The `dtype` of the online weights is mostly irrelevant, unless you are using `torch_dtype=\"auto\"` when initializing a model using `model = AutoModelForCausalLM.from_pretrained(\"path\", torch_dtype = \"auto\")`. The reason is that the model will first be downloaded ( using the `dtype` of the checkpoints online) then it will be cast to the default `dtype` of `torch` (becomes `torch.float32`). Users should specify the `torch_dtype` they want, and if they don't it will be `torch.float32`.\n+The `dtype` of the online weights is mostly irrelevant, unless you are using `dtype=\"auto\"` when initializing a model using `model = AutoModelForCausalLM.from_pretrained(\"path\", dtype = \"auto\")`. The reason is that the model will first be downloaded ( using the `dtype` of the checkpoints online) then it will be cast to the default `dtype` of `torch` (becomes `torch.float32`). Users should specify the `dtype` they want, and if they don't it will be `torch.float32`.\n \n Finetuning the model in `float16` is not recommended and known to produce `nan`, as such the model should be fine-tuned in `bfloat16`.\n "
        },
        {
            "sha": "3258fda73e69cec3ce330da5718255b9ec3d585e",
            "filename": "docs/source/en/model_doc/phi.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -41,7 +41,7 @@ The example below demonstrates how to generate text with [`Pipeline`], [`AutoMod\n import torch\n from transformers import pipeline\n \n-pipeline = pipeline(task=\"text-generation\", model=\"microsoft/phi-1.5\", device=0, torch_dtype=torch.bfloat16)\n+pipeline = pipeline(task=\"text-generation\", model=\"microsoft/phi-1.5\", device=0, dtype=torch.bfloat16)\n pipeline(\"pipeline('''def print_prime(n): \"\"\" Print all primes between 1 and n\"\"\"''')\")\n \n ```\n@@ -55,7 +55,7 @@ import torch\n from transformers import AutoTokenizer, AutoModelForCausalLM\n \n tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1\")\n-model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1\", torch_dtype=torch.float16, device_map=\"auto\", attn_implementation=\"sdpa\")\n+model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1\", dtype=torch.float16, device_map=\"auto\", attn_implementation=\"sdpa\")\n \n input_ids = tokenizer('''def print_prime(n):\n    \"\"\"\n@@ -86,7 +86,7 @@ from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n \n bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type=\"nf4\", bnb_4bit_use_double_quant=True)\n tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1\")\n-model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1\", torch_dtype=torch.float16, device_map=\"auto\", attn_implementation=\"sdpa\", quantization_config=bnb_config)\n+model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1\", dtype=torch.float16, device_map=\"auto\", attn_implementation=\"sdpa\", quantization_config=bnb_config)\n \n input_ids = tokenizer('''def print_prime(n):\n    \"\"\"\n@@ -108,7 +108,7 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n     tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1\")\n     model = AutoModelForCausalLM.from_pretrained(\n         \"microsoft/phi-1\",\n-        torch_dtype=torch.float16,\n+        dtype=torch.float16,\n         device_map=\"auto\",\n         trust_remote_code=True,\n         attn_implementation=\"sdpa\")"
        },
        {
            "sha": "358f35c542f51b4ae194fedb4ca155de66a262c8",
            "filename": "docs/source/en/model_doc/phi4_multimodal.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi4_multimodal.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi4_multimodal.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi4_multimodal.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -34,7 +34,7 @@ The example below demonstrates how to generate text based on an image with [`Pip\n \n ```python\n from transformers import pipeline\n-generator = pipeline(\"text-generation\", model=\"microsoft/Phi-4-multimodal-instruct\", torch_dtype=\"auto\", device=0)\n+generator = pipeline(\"text-generation\", model=\"microsoft/Phi-4-multimodal-instruct\", dtype=\"auto\", device=0)\n \n prompt = \"Explain the concept of multimodal AI in simple terms.\"\n \n@@ -53,7 +53,7 @@ model_path = \"microsoft/Phi-4-multimodal-instruct\"\n device = f\"{infer_device()}:0\"\n \n processor = AutoProcessor.from_pretrained(model_path)\n-model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, torch_dtype=torch.float16)\n+model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, dtype=torch.float16)\n \n model.load_adapter(model_path, adapter_name=\"vision\", device_map=device, adapter_kwargs={\"subfolder\": 'vision-lora'})\n \n@@ -103,7 +103,7 @@ model_path = \"microsoft/Phi-4-multimodal-instruct\"\n device = f\"{infer_device()}:0\"\n \n processor = AutoProcessor.from_pretrained(model_path)\n-model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device,  torch_dtype=torch.float16)\n+model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device,  dtype=torch.float16)\n \n model.load_adapter(model_path, adapter_name=\"speech\", device_map=device, adapter_kwargs={\"subfolder\": 'speech-lora'})\n model.set_adapter(\"speech\")"
        },
        {
            "sha": "319cbc470b91ae0951caa3b94feb1c3ec5d4b4d3",
            "filename": "docs/source/en/model_doc/phimoe.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fphimoe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fphimoe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fphimoe.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -68,8 +68,7 @@ torch.random.manual_seed(0)\n model = AutoModelForCausalLM.from_pretrained( \n     \"microsoft/Phi-3.5-MoE-instruct\",  \n     device_map=\"auto\",  \n-    torch_dtype=\"auto\",  \n-    trust_remote_code=True,  \n+    dtype=\"auto\",\n ) \n \n tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-MoE-instruct\") "
        },
        {
            "sha": "3f872302cc278cca370a9dcee5b516fa5dfd11d3",
            "filename": "docs/source/en/model_doc/qwen2.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -45,7 +45,7 @@ from transformers import pipeline\n pipe = pipeline(\n     task=\"text-generation\",\n     model=\"Qwen/Qwen2-1.5B-Instruct\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=0\n )\n \n@@ -66,7 +66,7 @@ from transformers import AutoModelForCausalLM, AutoTokenizer\n \n model = AutoModelForCausalLM.from_pretrained(\n     \"Qwen/Qwen2-1.5B-Instruct\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )\n@@ -106,7 +106,7 @@ print(response)\n \n ```bash\n # pip install -U flash-attn --no-build-isolation\n-transformers chat Qwen/Qwen2-7B-Instruct --torch_dtype auto --attn_implementation flash_attention_2 --device 0\n+transformers chat Qwen/Qwen2-7B-Instruct --dtype auto --attn_implementation flash_attention_2 --device 0\n ```\n \n </hfoption>\n@@ -131,7 +131,7 @@ quantization_config = BitsAndBytesConfig(\n tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-7B\")\n model = AutoModelForCausalLM.from_pretrained(\n     \"Qwen/Qwen2-7B\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     quantization_config=quantization_config,\n     attn_implementation=\"flash_attention_2\""
        },
        {
            "sha": "560f3f1b14c13d1224f3fcff9fea5a54a2d80ce9",
            "filename": "docs/source/en/model_doc/qwen2_5_omni.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_omni.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_omni.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_omni.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -55,7 +55,7 @@ from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcess\n \n model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n     \"Qwen/Qwen2.5-Omni-7B\",\n-    torch_dtype=\"auto\",\n+    dtype=\"auto\",\n     device_map=\"auto\"\n )\n processor = Qwen2_5OmniProcessor.from_pretrained(\"Qwen/Qwen2.5-Omni-7B\")\n@@ -111,7 +111,7 @@ from transformers import Qwen2_5OmniThinkerForConditionalGeneration, Qwen2_5Omni\n \n model = Qwen2_5OmniThinkerForConditionalGeneration.from_pretrained(\n     \"Qwen/Qwen2.5-Omni-7B\",\n-    torch_dtype=\"auto\",\n+    dtype=\"auto\",\n     device_map=\"auto\",\n )\n processor = Qwen2_5OmniProcessor.from_pretrained(\"Qwen/Qwen2.5-Omni-7B\")\n@@ -168,7 +168,7 @@ from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcess\n \n model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n     \"Qwen/Qwen2.5-Omni-7B\",\n-    torch_dtype=\"auto\",\n+    dtype=\"auto\",\n     device_map=\"auto\"\n )\n processor = Qwen2_5OmniProcessor.from_pretrained(\"Qwen/Qwen2.5-Omni-7B\")\n@@ -288,7 +288,7 @@ The model supports both text and audio outputs, if users do not need audio outpu\n ```python\n model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n     \"Qwen/Qwen2.5-Omni-7B\",\n-    torch_dtype=\"auto\",\n+    dtype=\"auto\",\n     device_map=\"auto\",\n     enable_audio_output=False,\n )\n@@ -299,7 +299,7 @@ In order to obtain a flexible experience, we recommend that users set `enable_au\n ```python\n model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n     \"Qwen/Qwen2.5-Omni-7B\",\n-    torch_dtype=\"auto\",\n+    dtype=\"auto\",\n     device_map=\"auto\",\n     enable_audio_output=True,\n )\n@@ -336,7 +336,7 @@ from transformers import Qwen2_5OmniForConditionalGeneration\n model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n     \"Qwen/Qwen2.5-Omni-7B\",\n     device_map=\"auto\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     attn_implementation=\"flash_attention_2\",\n )\n ```"
        },
        {
            "sha": "d1251cec3e624c5d656f58751b21b622a2e3ae81",
            "filename": "docs/source/en/model_doc/qwen2_5_vl.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_vl.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -44,7 +44,7 @@ pipe = pipeline(\n     task=\"image-text-to-text\",\n     model=\"Qwen/Qwen2.5-VL-7B-Instruct\",\n     device=0,\n-    torch_dtype=torch.bfloat16\n+    dtype=torch.bfloat16\n )\n messages = [\n     {\n@@ -71,7 +71,7 @@ from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n \n model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n     \"Qwen/Qwen2.5-VL-7B-Instruct\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )\n@@ -124,7 +124,7 @@ from transformers import TorchAoConfig, Qwen2_5_VLForConditionalGeneration, Auto\n quantization_config = TorchAoConfig(\"int4_weight_only\", group_size=128)\n model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n     \"Qwen/Qwen2.5-VL-7B-Instruct\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     quantization_config=quantization_config\n )\n@@ -167,7 +167,7 @@ model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n     \n     model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n         \"Qwen/Qwen2.5-VL-7B-Instruct\",\n-        torch_dtype=torch.float16,\n+        dtype=torch.float16,\n         device_map=\"auto\",\n         attn_implementation=\"sdpa\"\n     )"
        },
        {
            "sha": "b8a3fe65d3108ff8f1d54904520c5ee4bd9aa945",
            "filename": "docs/source/en/model_doc/qwen2_moe.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_moe.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -46,7 +46,7 @@ from transformers import pipeline\n pipe = pipeline(\n     task=\"text-generation\",\n     model=\"Qwen/Qwen1.5-MoE-A2.7B\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=0\n )\n \n@@ -66,7 +66,7 @@ from transformers import AutoModelForCausalLM, AutoTokenizer\n \n model = AutoModelForCausalLM.from_pretrained(\n     \"Qwen/Qwen1.5-MoE-A2.7B-Chat\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )\n@@ -103,7 +103,7 @@ print(response)\n </hfoption> \n <hfoption id=\"transformers CLI\">\n ```bash\n-transformers chat Qwen/Qwen1.5-MoE-A2.7B-Chat --torch_dtype auto --attn_implementation flash_attention_2\n+transformers chat Qwen/Qwen1.5-MoE-A2.7B-Chat --dtype auto --attn_implementation flash_attention_2\n ```\n </hfoption>\n  </hfoptions> \n@@ -125,7 +125,7 @@ quantization_config = BitsAndBytesConfig(\n tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-MoE-A2.7B-Chat\")\n model = AutoModelForCausalLM.from_pretrained(\n     \"Qwen/Qwen1.5-MoE-A2.7B-Chat\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     quantization_config=quantization_config,\n     attn_implementation=\"flash_attention_2\""
        },
        {
            "sha": "c3c130bd8acaac3573360963911b0f7ce9904cc1",
            "filename": "docs/source/en/model_doc/qwen2_vl.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_vl.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -271,7 +271,7 @@ from transformers import Qwen2VLForConditionalGeneration\n \n model = Qwen2VLForConditionalGeneration.from_pretrained(\n     \"Qwen/Qwen2-VL-7B-Instruct\", \n-    torch_dtype=torch.bfloat16, \n+    dtype=torch.bfloat16, \n     attn_implementation=\"flash_attention_2\",\n )\n ```"
        },
        {
            "sha": "16d1fa7e42c123909c2a0d8f859a653858c253f4",
            "filename": "docs/source/en/model_doc/rag.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Frag.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Frag.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Frag.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -50,7 +50,7 @@ retriever = RagRetriever.from_pretrained(\n model = RagSequenceForGeneration.from_pretrained(\n     \"facebook/rag-token-nq\",\n     retriever=retriever,\n-    torch_dtype=\"auto\",\n+    dtype=\"auto\",\n     attn_implementation=\"flash_attention_2\",\n )\n input_dict = tokenizer.prepare_seq2seq_batch(\"How many people live in Paris?\", return_tensors=\"pt\")"
        },
        {
            "sha": "da393646442a22688942a4585eac9f1ea00ab9e6",
            "filename": "docs/source/en/model_doc/roberta.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Froberta.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Froberta.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Froberta.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -44,7 +44,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"fill-mask\",\n     model=\"FacebookAI/roberta-base\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n pipeline(\"Plants create <mask> through a process known as photosynthesis.\")\n@@ -62,7 +62,7 @@ tokenizer = AutoTokenizer.from_pretrained(\n )\n model = AutoModelForMaskedLM.from_pretrained(\n     \"FacebookAI/roberta-base\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )"
        },
        {
            "sha": "e8b0ededd603b52d5cc65c16bdf7e1d3eafd89aa",
            "filename": "docs/source/en/model_doc/roc_bert.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Froc_bert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Froc_bert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Froc_bert.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -44,7 +44,7 @@ from transformers import pipeline\n pipeline = pipeline(\n    task=\"fill-mask\",\n    model=\"weiweishi/roc-bert-base-zh\",\n-   torch_dtype=torch.float16,\n+   dtype=torch.float16,\n    device=0\n )\n pipeline(\"é€™å®¶é¤å»³çš„æ‹‰éºµæ˜¯æˆ‘[MASK]éŽçš„æœ€å¥½çš„æ‹‰éºµä¹‹\")\n@@ -62,7 +62,7 @@ tokenizer = AutoTokenizer.from_pretrained(\n )\n model = AutoModelForMaskedLM.from_pretrained(\n    \"weiweishi/roc-bert-base-zh\",\n-   torch_dtype=torch.float16,\n+   dtype=torch.float16,\n    device_map=\"auto\",\n )\n inputs = tokenizer(\"é€™å®¶é¤å»³çš„æ‹‰éºµæ˜¯æˆ‘[MASK]éŽçš„æœ€å¥½çš„æ‹‰éºµä¹‹\", return_tensors=\"pt\").to(model.device)"
        },
        {
            "sha": "313ed71f38fa8890925d62cddb6a96e342821a6f",
            "filename": "docs/source/en/model_doc/roformer.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Froformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Froformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Froformer.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -44,7 +44,7 @@ from transformers import pipeline\n pipe = pipeline(\n     task=\"fill-mask\",\n     model=\"junnyu/roformer_chinese_base\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n output = pipe(\"æ°´åœ¨é›¶åº¦æ—¶ä¼š[MASK]\")\n@@ -61,7 +61,7 @@ import torch\n from transformers import AutoModelForMaskedLM, AutoTokenizer\n \n model = AutoModelForMaskedLM.from_pretrained(\n-    \"junnyu/roformer_chinese_base\", torch_dtype=torch.float16\n+    \"junnyu/roformer_chinese_base\", dtype=torch.float16\n )\n tokenizer = AutoTokenizer.from_pretrained(\"junnyu/roformer_chinese_base\")\n "
        },
        {
            "sha": "1f773be30738cde45654728ec7505d83df6aafb3",
            "filename": "docs/source/en/model_doc/sam2_video.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam2_video.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam2_video.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam2_video.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -71,7 +71,7 @@ SAM2's key strength is its ability to track objects across video frames. Here's\n >>> inference_session = processor.init_video_session(\n ...     video=video_frames,\n ...     inference_device=device,\n-...     torch_dtype=torch.bfloat16,\n+...     dtype=torch.bfloat16,\n ... )\n \n >>> # Add click on first frame to select object\n@@ -190,7 +190,7 @@ For real-time applications, SAM2 supports processing video frames as they arrive\n >>> # Initialize session for streaming\n >>> inference_session = processor.init_video_session(\n ...     inference_device=device,\n-...     torch_dtype=torch.bfloat16,\n+...     dtype=torch.bfloat16,\n ... )\n \n >>> # Process frames one by one\n@@ -226,7 +226,7 @@ Track multiple objects simultaneously in video by adding them all at once:\n >>> inference_session = processor.init_video_session(\n ...     video=video_frames,\n ...     inference_device=device,\n-...     torch_dtype=torch.bfloat16,\n+...     dtype=torch.bfloat16,\n ... )\n \n >>> # Add multiple objects on the first frame using batch processing"
        },
        {
            "sha": "c0eb9a8ac6b5bd602c7d3a03de48857ccaac941e",
            "filename": "docs/source/en/model_doc/siglip.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -47,7 +47,7 @@ from transformers import pipeline\n image = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n candidate_labels = [\"a Pallas cat\", \"a lion\", \"a Siberian tiger\"]\n \n-pipeline = pipeline(task=\"zero-shot-image-classification\", model=\"google/siglip-base-patch16-224\", device=0, torch_dtype=torch.bfloat16)\n+pipeline = pipeline(task=\"zero-shot-image-classification\", model=\"google/siglip-base-patch16-224\", device=0, dtype=torch.bfloat16)\n pipeline(image, candidate_labels=candidate_labels)\n ```\n \n@@ -60,7 +60,7 @@ import requests\n from PIL import Image\n from transformers import AutoProcessor, AutoModel\n \n-model = AutoModel.from_pretrained(\"google/siglip-base-patch16-224\", torch_dtype=torch.float16, device_map=\"auto\", attn_implementation=\"sdpa\")\n+model = AutoModel.from_pretrained(\"google/siglip-base-patch16-224\", dtype=torch.float16, device_map=\"auto\", attn_implementation=\"sdpa\")\n processor = AutoProcessor.from_pretrained(\"google/siglip-base-patch16-224\")\n \n url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n@@ -121,7 +121,7 @@ print(f\"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'\")\n     model = SiglipModel.from_pretrained(\n         \"google/siglip-so400m-patch14-384\",\n         attn_implementation=\"flash_attention_2\",\n-        torch_dtype=torch.float16,\n+        dtype=torch.float16,\n         device_map=device,\n     )\n     ```"
        },
        {
            "sha": "f2684c6defcff32dd6b497ed54d06c98f27f18af",
            "filename": "docs/source/en/model_doc/siglip2.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip2.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -50,7 +50,7 @@ from transformers import pipeline\n image = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n candidate_labels = [\"a Pallas cat\", \"a lion\", \"a Siberian tiger\"]\n \n-pipeline = pipeline(task=\"zero-shot-image-classification\", model=\"google/siglip2-base-patch16-224\", device=0, torch_dtype=torch.bfloat16)\n+pipeline = pipeline(task=\"zero-shot-image-classification\", model=\"google/siglip2-base-patch16-224\", device=0, dtype=torch.bfloat16)\n pipeline(image, candidate_labels=candidate_labels)\n ```\n \n@@ -63,7 +63,7 @@ import requests\n from PIL import Image\n from transformers import AutoProcessor, AutoModel\n \n-model = AutoModel.from_pretrained(\"google/siglip2-base-patch16-224\", torch_dtype=torch.float16, device_map=\"auto\", attn_implementation=\"sdpa\")\n+model = AutoModel.from_pretrained(\"google/siglip2-base-patch16-224\", dtype=torch.float16, device_map=\"auto\", attn_implementation=\"sdpa\")\n processor = AutoProcessor.from_pretrained(\"google/siglip2-base-patch16-224\")\n \n url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n@@ -93,7 +93,7 @@ import requests\n from PIL import Image\n from transformers import AutoProcessor, AutoModel\n \n-model = AutoModel.from_pretrained(\"google/siglip2-base-patch16-naflex\", torch_dtype=torch.float16, device_map=\"auto\", attn_implementation=\"sdpa\")\n+model = AutoModel.from_pretrained(\"google/siglip2-base-patch16-naflex\", dtype=torch.float16, device_map=\"auto\", attn_implementation=\"sdpa\")\n processor = AutoProcessor.from_pretrained(\"google/siglip2-base-patch16-naflex\")\n \n url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n@@ -165,7 +165,7 @@ print(f\"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'\")\n     model = SiglipModel.from_pretrained(\n         \"google/siglip2-so400m-patch14-384\",\n         attn_implementation=\"flash_attention_2\",\n-        torch_dtype=torch.float16,\n+        dtype=torch.float16,\n         device_map=device,\n     )\n     ```"
        },
        {
            "sha": "da98a15e33b5fd880b5d0992a1a2a5ef2ee68726",
            "filename": "docs/source/en/model_doc/smollm3.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fsmollm3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fsmollm3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsmollm3.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -42,7 +42,7 @@ from transformers import pipeline\n pipe = pipeline(\n     task=\"text-generation\",\n     model=\"HuggingFaceTB/SmolLM3-3B\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=0\n )\n \n@@ -63,7 +63,7 @@ from transformers import AutoModelForCausalLM, AutoTokenizer\n \n model = AutoModelForCausalLM.from_pretrained(\n     \"HuggingFaceTB/SmolLM3-3B\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )\n@@ -103,7 +103,7 @@ print(response)\n \n ```bash\n # pip install -U flash-attn --no-build-isolation\n-transformers chat HuggingFaceTB/SmolLM3-3B --torch_dtype auto --attn_implementation flash_attention_2 --device 0\n+transformers chat HuggingFaceTB/SmolLM3-3B --dtype auto --attn_implementation flash_attention_2 --device 0\n ```\n \n </hfoption>\n@@ -128,7 +128,7 @@ quantization_config = BitsAndBytesConfig(\n tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM3-3B\")\n model = AutoModelForCausalLM.from_pretrained(\n     \"HuggingFaceTB/SmolLM3-3B\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     quantization_config=quantization_config,\n     attn_implementation=\"flash_attention_2\""
        },
        {
            "sha": "c9a886ac87692a07dbe76c631fb41c7d7c7c7159",
            "filename": "docs/source/en/model_doc/smolvlm.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fsmolvlm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fsmolvlm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsmolvlm.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -62,7 +62,7 @@ from transformers import AutoProcessor, AutoModelForImageTextToText\n processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM2-256M-Video-Instruct\")\n model = AutoModelForImageTextToText.from_pretrained(\n     \"HuggingFaceTB/SmolVLM2-256M-Video-Instruct\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\"\n )\n \n@@ -124,7 +124,7 @@ from transformers import AutoProcessor, AutoModelForImageTextToText\n processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM2-256M-Video-Instruct\")\n model = AutoModelForImageTextToText.from_pretrained(\n     \"HuggingFaceTB/SmolVLM2-256M-Video-Instruct\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\"\n )\n "
        },
        {
            "sha": "29f32a0004e2dc0cbffdf8d90d5f78a9fddf4a65",
            "filename": "docs/source/en/model_doc/stablelm.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fstablelm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fstablelm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fstablelm.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -81,7 +81,7 @@ Now, to run the model with Flash Attention 2, refer to the snippet below:\n >>> set_seed(0)\n \n >>> tokenizer = AutoTokenizer.from_pretrained(\"stabilityai/stablelm-3b-4e1t\")\n->>> model = AutoModelForCausalLM.from_pretrained(\"stabilityai/stablelm-3b-4e1t\", torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\")  # doctest: +SKIP\n+>>> model = AutoModelForCausalLM.from_pretrained(\"stabilityai/stablelm-3b-4e1t\", dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\")  # doctest: +SKIP\n >>> model.to(device)  # doctest: +SKIP\n \n >>> model_inputs = tokenizer(\"The weather is always wonderful in\", return_tensors=\"pt\").to(model.device)"
        },
        {
            "sha": "f6a994ef69bcaa14ee5865210c0a384e0de820b2",
            "filename": "docs/source/en/model_doc/swin.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fswin.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fswin.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fswin.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -42,7 +42,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"image-classification\",\n     model=\"microsoft/swin-tiny-patch4-window7-224\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n pipeline(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")"
        },
        {
            "sha": "507b79fc7cf1da412eafc8706f5f271ef25aa301",
            "filename": "docs/source/en/model_doc/swinv2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fswinv2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fswinv2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fswinv2.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -40,7 +40,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"image-classification\",\n     model=\"microsoft/swinv2-tiny-patch4-window8-256\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n pipeline(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")"
        },
        {
            "sha": "efa6bd499dbc164918496967af8a607b86386610",
            "filename": "docs/source/en/model_doc/switch_transformers.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fswitch_transformers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fswitch_transformers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fswitch_transformers.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -45,7 +45,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"text2text-generation\", \n     model=\"google/switch-base-8\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n print(pipeline(\"The capital of France is <extra_id_0>.\"))\n@@ -59,7 +59,7 @@ import torch\n from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n \n tokenizer = AutoTokenizer.from_pretrained(\"google/switch-base-8\")\n-model = AutoModelForSeq2SeqLM.from_pretrained(\"google/switch-base-8\", device_map=\"auto\", torch_dtype=torch.float16)\n+model = AutoModelForSeq2SeqLM.from_pretrained(\"google/switch-base-8\", device_map=\"auto\", dtype=torch.float16)\n \n input_text = \"The capital of France is <extra_id_0>.\"\n input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(0)"
        },
        {
            "sha": "00c8c418527d41698f101d54b14c5391a11ee041",
            "filename": "docs/source/en/model_doc/t5.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -44,7 +44,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"text2text-generation\",\n     model=\"google-t5/t5-base\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n pipeline(\"translate English to French: The weather is nice today.\")\n@@ -62,7 +62,7 @@ tokenizer = AutoTokenizer.from_pretrained(\n     )\n model = AutoModelForSeq2SeqLM.from_pretrained(\n     \"google-t5/t5-base\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\"\n     )\n \n@@ -94,7 +94,7 @@ from transformers import TorchAoConfig, AutoModelForSeq2SeqLM, AutoTokenizer\n quantization_config = TorchAoConfig(\"int4_weight_only\", group_size=128)\n model = AutoModelForSeq2SeqLM.from_pretrained(\n     \"google/t5-v1_1-xl\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     quantization_config=quantization_config\n )"
        },
        {
            "sha": "aa8d3b7880ed7c0389ee3c0ac0678af6a3533eaf",
            "filename": "docs/source/en/model_doc/t5gemma.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5gemma.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5gemma.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5gemma.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -47,7 +47,7 @@ from transformers import pipeline\n pipe = pipeline(\n     \"text2text-generation\",\n     model=\"google/t5gemma-2b-2b-prefixlm-it\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n )\n \n@@ -71,7 +71,7 @@ tokenizer = AutoTokenizer.from_pretrained(\"google/t5gemma-2b-2b-prefixlm-it\")\n model = AutoModelForSeq2SeqLM.from_pretrained(\n     \"google/t5gemma-2b-2b-prefixlm-it\",\n     device_map=\"auto\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n )\n \n messages = ["
        },
        {
            "sha": "83dee48e71bec136dd3e32defa414998b773b95c",
            "filename": "docs/source/en/model_doc/timesfm.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Ftimesfm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Ftimesfm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftimesfm.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -45,7 +45,7 @@ from transformers import TimesFmModelForPrediction\n \n model = TimesFmModelForPrediction.from_pretrained(\n     \"google/timesfm-2.0-500m-pytorch\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     attn_implementation=\"sdpa\",\n     device_map=\"auto\"\n )"
        },
        {
            "sha": "6b09367f37c85d22ac6129dd5182b9f88719fd4d",
            "filename": "docs/source/en/model_doc/video_llava.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideo_llava.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideo_llava.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideo_llava.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -100,7 +100,7 @@ def read_video_pyav(container, indices):\n     return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n \n # Load the model in half-precision\n-model = VideoLlavaForConditionalGeneration.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n+model = VideoLlavaForConditionalGeneration.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\", dtype=torch.float16, device_map=\"auto\")\n processor = VideoLlavaProcessor.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\")\n \n # Load the video as an np.arrau, sampling uniformly 8 frames\n@@ -198,7 +198,7 @@ from transformers import VideoLlavaForConditionalGeneration\n \n model = VideoLlavaForConditionalGeneration.from_pretrained(\n     \"LanguageBind/Video-LLaVA-7B-hf\", \n-    torch_dtype=torch.float16, \n+    dtype=torch.float16, \n     attn_implementation=\"flash_attention_2\",\n ).to(0)\n ```"
        },
        {
            "sha": "e0ebbaa428850ae5be664c88ac2ca7b88aadc7b5",
            "filename": "docs/source/en/model_doc/videomae.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideomae.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideomae.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideomae.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -53,7 +53,7 @@ SDPA is used by default for `torch>=2.1.1` when an implementation is available,\n \n ```\n from transformers import VideoMAEForVideoClassification\n-model = VideoMAEForVideoClassification.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\", attn_implementation=\"sdpa\", torch_dtype=torch.float16)\n+model = VideoMAEForVideoClassification.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\", attn_implementation=\"sdpa\", dtype=torch.float16)\n ...\n ```\n "
        },
        {
            "sha": "93a651ff135e15c6cbbeee103a7b8d90ccbcf850",
            "filename": "docs/source/en/model_doc/vit.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -43,7 +43,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"image-classification\",\n     model=\"google/vit-base-patch16-224\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n pipeline(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")\n@@ -64,7 +64,7 @@ image_processor = AutoImageProcessor.from_pretrained(\n )\n model = AutoModelForImageClassification.from_pretrained(\n     \"google/vit-base-patch16-224\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )"
        },
        {
            "sha": "86c2c7229f5856bb959f8d3cdf68919a4fecc90d",
            "filename": "docs/source/en/model_doc/vit_hybrid.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_hybrid.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_hybrid.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_hybrid.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -66,7 +66,7 @@ SDPA is used by default for `torch>=2.1.1` when an implementation is available,\n \n ```\n from transformers import ViTHybridForImageClassification\n-model = ViTHybridForImageClassification.from_pretrained(\"google/vit-hybrid-base-bit-384\", attn_implementation=\"sdpa\", torch_dtype=torch.float16)\n+model = ViTHybridForImageClassification.from_pretrained(\"google/vit-hybrid-base-bit-384\", attn_implementation=\"sdpa\", dtype=torch.float16)\n ...\n ```\n "
        },
        {
            "sha": "5b727f34256c34654ad73c152cd9b594bec2a00d",
            "filename": "docs/source/en/model_doc/vit_msn.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_msn.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_msn.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_msn.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -69,7 +69,7 @@ SDPA is used by default for `torch>=2.1.1` when an implementation is available,\n \n ```\n from transformers import ViTMSNForImageClassification\n-model = ViTMSNForImageClassification.from_pretrained(\"facebook/vit-msn-base\", attn_implementation=\"sdpa\", torch_dtype=torch.float16)\n+model = ViTMSNForImageClassification.from_pretrained(\"facebook/vit-msn-base\", attn_implementation=\"sdpa\", dtype=torch.float16)\n ...\n ```\n "
        },
        {
            "sha": "2c1777b77f183389aa858cafd1cb5b492bbe4657",
            "filename": "docs/source/en/model_doc/vits.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fvits.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fvits.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvits.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -40,7 +40,7 @@ set_seed(555)\n pipe = pipeline(\n     task=\"text-to-speech\",\n     model=\"facebook/mms-tts-eng\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n \n@@ -64,7 +64,7 @@ from IPython.display import Audio\n from transformers import AutoTokenizer, VitsModel, set_seed\n \n tokenizer = AutoTokenizer.from_pretrained(\"facebook/mms-tts-eng\")\n-model = VitsModel.from_pretrained(\"facebook/mms-tts-eng\", device_map=\"auto\", torch_dtype=torch.float16)\n+model = VitsModel.from_pretrained(\"facebook/mms-tts-eng\", device_map=\"auto\", dtype=torch.float16)\n inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\").to(model.device)\n \n set_seed(555)"
        },
        {
            "sha": "041f80f61ae66c6c655fb90fad615b0db51f2a44",
            "filename": "docs/source/en/model_doc/vivit.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fvivit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fvivit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvivit.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -43,7 +43,7 @@ SDPA is used by default for `torch>=2.1.1` when an implementation is available,\n \n ```\n from transformers import VivitModel\n-model = VivitModel.from_pretrained(\"google/vivit-b-16x2-kinetics400\", attn_implementation=\"sdpa\", torch_dtype=torch.float16)\n+model = VivitModel.from_pretrained(\"google/vivit-b-16x2-kinetics400\", attn_implementation=\"sdpa\", dtype=torch.float16)\n ...\n ```\n "
        },
        {
            "sha": "93960f0518938d7676c1e80a756a6a33ee719f05",
            "filename": "docs/source/en/model_doc/vjepa2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fvjepa2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fvjepa2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvjepa2.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -49,7 +49,7 @@ import numpy as np\n processor = AutoVideoProcessor.from_pretrained(\"facebook/vjepa2-vitl-fpc64-256\")\n model = AutoModel.from_pretrained(\n     \"facebook/vjepa2-vitl-fpc64-256\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )"
        },
        {
            "sha": "71f0661c8276c146f5b04d97155c3207078f2e8e",
            "filename": "docs/source/en/model_doc/voxtral.md",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fvoxtral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fvoxtral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvoxtral.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -51,7 +51,7 @@ device = infer_device()\n repo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n \n processor = AutoProcessor.from_pretrained(repo_id)\n-model = VoxtralForConditionalGeneration.from_pretrained(repo_id, torch_dtype=torch.bfloat16, device_map=device)\n+model = VoxtralForConditionalGeneration.from_pretrained(repo_id, dtype=torch.bfloat16, device_map=device)\n \n conversation = [\n     {\n@@ -87,7 +87,7 @@ device = infer_device()\n repo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n \n processor = AutoProcessor.from_pretrained(repo_id)\n-model = VoxtralForConditionalGeneration.from_pretrained(repo_id, torch_dtype=torch.bfloat16, device_map=device)\n+model = VoxtralForConditionalGeneration.from_pretrained(repo_id, dtype=torch.bfloat16, device_map=device)\n \n conversation = [\n     {\n@@ -127,7 +127,7 @@ device = infer_device()\n repo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n \n processor = AutoProcessor.from_pretrained(repo_id)\n-model = VoxtralForConditionalGeneration.from_pretrained(repo_id, torch_dtype=torch.bfloat16, device_map=device)\n+model = VoxtralForConditionalGeneration.from_pretrained(repo_id, dtype=torch.bfloat16, device_map=device)\n \n conversation = [\n     {\n@@ -181,7 +181,7 @@ device = infer_device()\n repo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n \n processor = AutoProcessor.from_pretrained(repo_id)\n-model = VoxtralForConditionalGeneration.from_pretrained(repo_id, torch_dtype=torch.bfloat16, device_map=device)\n+model = VoxtralForConditionalGeneration.from_pretrained(repo_id, dtype=torch.bfloat16, device_map=device)\n \n conversation = [\n     {\n@@ -216,7 +216,7 @@ device = infer_device()\n repo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n \n processor = AutoProcessor.from_pretrained(repo_id)\n-model = VoxtralForConditionalGeneration.from_pretrained(repo_id, torch_dtype=torch.bfloat16, device_map=device)\n+model = VoxtralForConditionalGeneration.from_pretrained(repo_id, dtype=torch.bfloat16, device_map=device)\n \n conversation = [\n     {\n@@ -251,7 +251,7 @@ device = infer_device()\n repo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n \n processor = AutoProcessor.from_pretrained(repo_id)\n-model = VoxtralForConditionalGeneration.from_pretrained(repo_id, torch_dtype=torch.bfloat16, device_map=device)\n+model = VoxtralForConditionalGeneration.from_pretrained(repo_id, dtype=torch.bfloat16, device_map=device)\n \n conversations = [\n     [\n@@ -312,7 +312,7 @@ device = infer_device()\n repo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n \n processor = AutoProcessor.from_pretrained(repo_id)\n-model = VoxtralForConditionalGeneration.from_pretrained(repo_id, torch_dtype=torch.bfloat16, device_map=device)\n+model = VoxtralForConditionalGeneration.from_pretrained(repo_id, dtype=torch.bfloat16, device_map=device)\n \n inputs = processor.apply_transcription_request(language=\"en\", audio=\"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/obama.mp3\", model_id=repo_id)\n inputs = inputs.to(device, dtype=torch.bfloat16)"
        },
        {
            "sha": "6c4772f90bc8f4c6fdf9624013b2b0c5a6e54848",
            "filename": "docs/source/en/model_doc/wav2vec2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -72,7 +72,7 @@ To load a model using Flash Attention 2, we can pass the argument `attn_implemen\n ```python\n >>> from transformers import Wav2Vec2Model\n \n-model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(device)\n+model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\", dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(device)\n ...\n ```\n "
        },
        {
            "sha": "673085ac3e7d66217fa5254c0da483fca55de2c7",
            "filename": "docs/source/en/model_doc/whisper.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fwhisper.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fwhisper.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fwhisper.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -48,7 +48,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"automatic-speech-recognition\",\n     model=\"openai/whisper-large-v3-turbo\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n pipeline(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n@@ -68,7 +68,7 @@ processor = AutoProcessor.from_pretrained(\n )\n model = WhisperForConditionalGeneration.from_pretrained(\n     \"openai/whisper-large-v3-turbo\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )"
        },
        {
            "sha": "988107fdacc6776e4fbc748878d97095824b3d64",
            "filename": "docs/source/en/model_doc/xlm-roberta-xl.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-roberta-xl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-roberta-xl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-roberta-xl.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -43,7 +43,7 @@ from transformers import pipeline\n pipeline = pipeline(  \n     task=\"fill-mask\",  \n     model=\"facebook/xlm-roberta-xl\",  \n-    torch_dtype=torch.float16,  \n+    dtype=torch.float16,  \n     device=0  \n )  \n pipeline(\"Bonjour, je suis un modÃ¨le <mask>.\")  \n@@ -61,7 +61,7 @@ tokenizer = AutoTokenizer.from_pretrained(\n )  \n model = AutoModelForMaskedLM.from_pretrained(  \n     \"facebook/xlm-roberta-xl\",  \n-    torch_dtype=torch.float16,  \n+    dtype=torch.float16,  \n     device_map=\"auto\",  \n     attn_implementation=\"sdpa\"  \n )  \n@@ -101,7 +101,7 @@ tokenizer = AutoTokenizer.from_pretrained(\n )\n model = AutoModelForMaskedLM.from_pretrained(\n     \"facebook/xlm-roberta-xl\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\",\n     quantization_config=quantization_config"
        },
        {
            "sha": "a662742c267445a4ecce4da6112a9fc459ca7586",
            "filename": "docs/source/en/model_doc/xlm-roberta.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-roberta.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-roberta.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-roberta.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -43,7 +43,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"fill-mask\",\n     model=\"FacebookAI/xlm-roberta-base\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n # Example in French\n@@ -62,7 +62,7 @@ tokenizer = AutoTokenizer.from_pretrained(\n )\n model = AutoModelForMaskedLM.from_pretrained(\n     \"FacebookAI/xlm-roberta-base\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )\n@@ -107,7 +107,7 @@ quantization_config = BitsAndBytesConfig(\n tokenizer = AutoTokenizer.from_pretrained(\"facebook/xlm-roberta-large\")\n model = AutoModelForMaskedLM.from_pretrained(\n     \"facebook/xlm-roberta-large\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n     attn_implementation=\"flash_attention_2\",\n     quantization_config=quantization_config"
        },
        {
            "sha": "dc51fa4be4cd5088dd67289e11d3b4c4bdbcea92",
            "filename": "docs/source/en/model_doc/xlm.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -42,7 +42,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"fill-mask\",\n     model=\"facebook/xlm-roberta-xl\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n pipeline(\"Bonjour, je suis un modÃ¨le <mask>.\")\n@@ -60,7 +60,7 @@ tokenizer = AutoTokenizer.from_pretrained(\n )\n model = AutoModelForMaskedLM.from_pretrained(\n     \"FacebookAI/xlm-mlm-en-2048\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n )\n inputs = tokenizer(\"Hello, I'm a <mask> model.\", return_tensors=\"pt\").to(model.device)"
        },
        {
            "sha": "5c31b539e59c5f3971aa6946a09acfdc17702422",
            "filename": "docs/source/en/model_doc/yolos.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fyolos.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fyolos.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fyolos.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -50,7 +50,7 @@ from transformers import pipeline\n detector = pipeline(\n     task=\"object-detection\",\n     model=\"hustvl/yolos-base\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n detector(\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\")\n@@ -68,7 +68,7 @@ from transformers import AutoImageProcessor, AutoModelForObjectDetection, infer_\n device = infer_device()\n \n processor = AutoImageProcessor.from_pretrained(\"hustvl/yolos-base\")\n-model = AutoModelForObjectDetection.from_pretrained(\"hustvl/yolos-base\", torch_dtype=torch.float16, attn_implementation=\"sdpa\").to(device)\n+model = AutoModelForObjectDetection.from_pretrained(\"hustvl/yolos-base\", dtype=torch.float16, attn_implementation=\"sdpa\").to(device)\n \n url = \"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\"\n image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")"
        },
        {
            "sha": "bb9740807703e7fa2a81b79413ac2fdfba08d9b1",
            "filename": "docs/source/en/model_doc/zamba.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fzamba.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fzamba.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fzamba.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -57,7 +57,7 @@ from transformers import AutoTokenizer, AutoModelForCausalLM\n import torch\n \n tokenizer = AutoTokenizer.from_pretrained(\"Zyphra/Zamba-7B-v1\")\n-model = AutoModelForCausalLM.from_pretrained(\"Zyphra/Zamba-7B-v1\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n+model = AutoModelForCausalLM.from_pretrained(\"Zyphra/Zamba-7B-v1\", device_map=\"auto\", dtype=torch.bfloat16)\n \n input_text = \"A funny prompt would be \"\n input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)"
        },
        {
            "sha": "1d911a59c2771f1d77051c629525dc4eb48fa0aa",
            "filename": "docs/source/en/model_doc/zamba2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fzamba2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fzamba2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fzamba2.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -50,7 +50,7 @@ import torch\n from transformers import AutoTokenizer, AutoModelForCausalLM\n \n tokenizer = AutoTokenizer.from_pretrained(\"Zyphra/Zamba2-7B\")\n-model = AutoModelForCausalLM.from_pretrained(\"Zyphra/Zamba2-7B\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n+model = AutoModelForCausalLM.from_pretrained(\"Zyphra/Zamba2-7B\", device_map=\"auto\", dtype=torch.bfloat16)\n \n input_text = \"What factors contributed to the fall of the Roman Empire?\"\n input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)"
        },
        {
            "sha": "367c630a322425f6120840dc1084d6128143cfcc",
            "filename": "docs/source/en/model_doc/zoedepth.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fzoedepth.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodel_doc%2Fzoedepth.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fzoedepth.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -47,7 +47,7 @@ image = Image.open(requests.get(url, stream=True).raw)\n pipeline = pipeline(\n     task=\"depth-estimation\",\n     model=\"Intel/zoedepth-nyu-kitti\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n results = pipeline(image)"
        },
        {
            "sha": "ab4cf6635f0d011899c07209d1b85e2fd5ebbb55",
            "filename": "docs/source/en/models.md",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodels.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fmodels.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodels.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -26,7 +26,7 @@ Call [`~PreTrainedModel.from_pretrained`] to download and load a model's weights\n ```py\n from transformers import AutoModelForCausalLM\n \n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=\"auto\", device_map=\"auto\")\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", dtype=\"auto\", device_map=\"auto\")\n ```\n \n This guide explains how models are loaded, the different ways you can load a model, how to overcome memory issues for really big models, and how to load custom models.\n@@ -54,8 +54,8 @@ For each model type, there is a separate class for each machine learning framewo\n from transformers import AutoModelForCausalLM, MistralForCausalLM\n \n # load with AutoClass or model-specific class\n-model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", torch_dtype=\"auto\", device_map=\"auto\")\n-model = MistralForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", torch_dtype=\"auto\", device_map=\"auto\")\n+model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", dtype=\"auto\", device_map=\"auto\")\n+model = MistralForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", dtype=\"auto\", device_map=\"auto\")\n ```\n \n </hfoption>\n@@ -261,7 +261,7 @@ model.hf_device_map\n \n PyTorch model weights are initialized in `torch.float32` by default. Loading a model in a different data type, like `torch.float16`, requires additional memory because the model is loaded again in the desired data type.\n \n-Explicitly set the [torch_dtype](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype) parameter to directly initialize the model in the desired data type instead of loading the weights twice (`torch.float32` then `torch.float16`). You could also set `torch_dtype=\"auto\"` to automatically load the weights in the data type they are stored in.\n+Explicitly set the [dtype](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype) parameter to directly initialize the model in the desired data type instead of loading the weights twice (`torch.float32` then `torch.float16`). You could also set `dtype=\"auto\"` to automatically load the weights in the data type they are stored in.\n \n <hfoptions id=\"dtype\">\n <hfoption id=\"specific dtype\">\n@@ -270,7 +270,7 @@ Explicitly set the [torch_dtype](https://pytorch.org/docs/stable/tensor_attribut\n import torch\n from transformers import AutoModelForCausalLM\n \n-gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\", torch_dtype=torch.float16)\n+gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\", dtype=torch.float16)\n ```\n \n </hfoption>\n@@ -279,19 +279,19 @@ gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\", torch_dtype=torc\n ```py\n from transformers import AutoModelForCausalLM\n \n-gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\", torch_dtype=\"auto\")\n+gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\", dtype=\"auto\")\n ```\n \n </hfoption>\n </hfoptions>\n \n-The `torch_dtype` parameter can also be configured in [`AutoConfig`] for models instantiated from scratch.\n+The `dtype` parameter can also be configured in [`AutoConfig`] for models instantiated from scratch.\n \n ```py\n import torch\n from transformers import AutoConfig, AutoModel\n \n-my_config = AutoConfig.from_pretrained(\"google/gemma-2b\", torch_dtype=torch.float16)\n+my_config = AutoConfig.from_pretrained(\"google/gemma-2b\", dtype=torch.float16)\n model = AutoModel.from_config(my_config)\n ```\n "
        },
        {
            "sha": "01823dd5b200bbd8add3211ef1f40fc28014b0a5",
            "filename": "docs/source/en/perf_infer_gpu_multi.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -61,7 +61,7 @@ from transformers import AutoModelForCausalLM, AutoTokenizer\n # model_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\" # better to visualize all the possible strategies\n model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"  # better for smaller number of GPUs\n \n-model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, tp_plan=\"auto\")\n+model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16, tp_plan=\"auto\")\n print(model._tp_plan)\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n@@ -97,7 +97,7 @@ tp_plan = {\n     ...\n }\n \n-model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, tp_plan=tp_plan)\n+model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16, tp_plan=tp_plan)\n print(model._tp_plan)\n ```\n \n@@ -247,7 +247,7 @@ The example below shows how to implement `ColwiseParallel` with this workflow.\n         \"model.layers.*.self_attn.q_proj\": \"colwise_custom\",\n         ...\n     }\n-    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, tp_plan=tp_plan)\n+    model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16, tp_plan=tp_plan)\n     ```\n \n ## Benchmarks"
        },
        {
            "sha": "33fe9358fe7d3b1dd0c991ad50600cccbe320ddd",
            "filename": "docs/source/en/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -245,7 +245,7 @@ Enable FlashAttention2 by setting `attn_implementation=\"flash_attention_2\"` in [\n ```py\n from transformers import AutoModelForCausalLM\n \n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B\", device_map=\"auto\", torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\")\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B\", device_map=\"auto\", dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\")\n ```\n \n ### Benchmarks"
        },
        {
            "sha": "e70ba35ea852e219edbcc50d1e2bc47e047b3a5d",
            "filename": "docs/source/en/pipeline_tutorial.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fpipeline_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fpipeline_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fpipeline_tutorial.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -347,7 +347,7 @@ Lastly, [`Pipeline`] also accepts quantized models to reduce memory usage even f\n import torch\n from transformers import pipeline, BitsAndBytesConfig\n \n-pipeline = pipeline(model=\"google/gemma-7b\", torch_dtype=torch.bfloat16, device_map=\"auto\", model_kwargs={\"quantization_config\": BitsAndBytesConfig(load_in_8bit=True)})\n+pipeline = pipeline(model=\"google/gemma-7b\", dtype=torch.bfloat16, device_map=\"auto\", model_kwargs={\"quantization_config\": BitsAndBytesConfig(load_in_8bit=True)})\n pipeline(\"the secret to baking a good cake is \")\n [{'generated_text': 'the secret to baking a good cake is 1. the right ingredients 2. the right'}]\n ```"
        },
        {
            "sha": "9751146827a27f2efd85660c936b2f0bd4347000",
            "filename": "docs/source/en/quantization/aqlm.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fquantization%2Faqlm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fquantization%2Faqlm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Faqlm.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -33,7 +33,7 @@ from transformers import AutoTokenizer, AutoModelForCausalLM\n \n quantized_model = AutoModelForCausalLM.from_pretrained(\n     \"ISTA-DASLab/Mixtral-8x7b-AQLM-2Bit-1x16-hf\",\n-    torch_dtype=\"auto\", \n+    dtype=\"auto\", \n     device_map=\"auto\"\n )\n ```"
        },
        {
            "sha": "15abf9faa8462f197d24016aa30ceb452ad7e042",
            "filename": "docs/source/en/quantization/auto_round.md",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fquantization%2Fauto_round.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fquantization%2Fauto_round.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fauto_round.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -73,7 +73,7 @@ from transformers import AutoModelForCausalLM, AutoTokenizer\n from auto_round import AutoRound\n \n model_name = \"facebook/opt-125m\"\n-model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\")\n+model = AutoModelForCausalLM.from_pretrained(model_name, dtype=\"auto\")\n tokenizer = AutoTokenizer.from_pretrained(model_name)\n bits, group_size, sym = 4, 128, True\n # mixed bits config\n@@ -104,7 +104,7 @@ from transformers import AutoModelForCausalLM, AutoTokenizer\n from auto_round import AutoRound\n \n model_name = \"facebook/opt-125m\"\n-model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\")\n+model = AutoModelForCausalLM.from_pretrained(model_name, dtype=\"auto\")\n tokenizer = AutoTokenizer.from_pretrained(model_name)\n bits, group_size, sym = 4, 128, True\n autoround = AutoRound(\n@@ -133,7 +133,7 @@ from transformers import AutoModelForCausalLM, AutoTokenizer\n from auto_round import AutoRound\n \n model_name = \"facebook/opt-125m\"\n-model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\")\n+model = AutoModelForCausalLM.from_pretrained(model_name, dtype=\"auto\")\n tokenizer = AutoTokenizer.from_pretrained(model_name)\n bits, group_size, sym = 4, 128, True\n autoround = AutoRound(\n@@ -177,7 +177,7 @@ Supports 2, 4, and 8 bits. We recommend using intel-extension-for-pytorch (IPEX)\n from transformers import AutoModelForCausalLM, AutoTokenizer\n \n model_name = \"OPEA/Qwen2.5-1.5B-Instruct-int4-sym-inc\"\n-model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cpu\", torch_dtype=\"auto\")\n+model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cpu\", dtype=\"auto\")\n tokenizer = AutoTokenizer.from_pretrained(model_name)\n text = \"There is a girl who likes adventure,\"\n inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n@@ -196,7 +196,7 @@ Supports 4 bits only. We recommend using intel-extension-for-pytorch (IPEX) for\n from transformers import AutoModelForCausalLM, AutoTokenizer\n \n model_name = \"OPEA/Qwen2.5-1.5B-Instruct-int4-sym-inc\"\n-model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"xpu\", torch_dtype=\"auto\")\n+model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"xpu\", dtype=\"auto\")\n tokenizer = AutoTokenizer.from_pretrained(model_name)\n text = \"There is a girl who likes adventure,\"\n inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n@@ -215,7 +215,7 @@ Supports 2, 3, 4, and 8 bits. We recommend using GPTQModel for 4 and 8 bits infe\n from transformers import AutoModelForCausalLM, AutoTokenizer\n \n model_name = \"OPEA/Qwen2.5-1.5B-Instruct-int4-sym-inc\"\n-model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cuda\", torch_dtype=\"auto\")\n+model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cuda\", dtype=\"auto\")\n tokenizer = AutoTokenizer.from_pretrained(model_name)\n text = \"There is a girl who likes adventure,\"\n inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n@@ -238,7 +238,7 @@ from transformers import AutoModelForCausalLM, AutoTokenizer, AutoRoundConfig\n \n model_name = \"OPEA/Qwen2.5-1.5B-Instruct-int4-sym-inc\"\n quantization_config = AutoRoundConfig(backend=\"ipex\")\n-model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cpu\", quantization_config=quantization_config, torch_dtype=\"auto\")\n+model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cpu\", quantization_config=quantization_config, dtype=\"auto\")\n tokenizer = AutoTokenizer.from_pretrained(model_name)\n text = \"There is a girl who likes adventure,\"\n inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n@@ -259,7 +259,7 @@ from transformers import AutoModelForCausalLM, AutoTokenizer, AutoRoundConfig\n \n model_name = \"ybelkada/opt-125m-gptq-4bit\"\n quantization_config = AutoRoundConfig()\n-model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cpu\", quantization_config=quantization_config, torch_dtype=\"auto\")\n+model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cpu\", quantization_config=quantization_config, dtype=\"auto\")\n tokenizer = AutoTokenizer.from_pretrained(model_name)\n text = \"There is a girl who likes adventure,\"\n inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)"
        },
        {
            "sha": "b6437e2588a867b91912b71f8a88373de2657d15",
            "filename": "docs/source/en/quantization/awq.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fquantization%2Fawq.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fquantization%2Fawq.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fawq.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -49,7 +49,7 @@ Identify an AWQ-quantized model by checking the `quant_method` key in the models\n }\n ```\n \n-Load the AWQ-quantized model with [`~PreTrainedModel.from_pretrained`]. This automatically sets the other weights to fp16 by default for performance reasons. Use the `torch_dtype` parameter to load these other weights in a different format.\n+Load the AWQ-quantized model with [`~PreTrainedModel.from_pretrained`]. This automatically sets the other weights to fp16 by default for performance reasons. Use the `dtype` parameter to load these other weights in a different format.\n \n If the model is loaded on the CPU, use the `device_map` parameter to move it to an accelerator.\n \n@@ -61,7 +61,7 @@ device = f\"{infer_device()}:0\"\n \n model = AutoModelForCausalLM.from_pretrained(\n   \"TheBloke/zephyr-7B-alpha-AWQ\",\n-  torch_dtype=torch.float32,\n+  dtype=torch.float32,\n   device_map=device\n )\n ```"
        },
        {
            "sha": "60c3c2dfebf9812ccd40bba3482be5068dc54b69",
            "filename": "docs/source/en/quantization/bitsandbytes.md",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fquantization%2Fbitsandbytes.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fquantization%2Fbitsandbytes.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fbitsandbytes.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -84,7 +84,7 @@ model_8bit = AutoModelForCausalLM.from_pretrained(\n )\n ```\n \n-By default, all other modules such as [torch.nn.LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html) are set to the default torch dtype. You can change the data type of these modules with the `torch_dtype` parameter. Setting `torch_dtype=\"auto\"` loads the model in the data type defined in a model's `config.json` file.\n+By default, all other modules such as [torch.nn.LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html) are set to the default torch dtype. You can change the data type of these modules with the `dtype` parameter. Setting `dtype=\"auto\"` loads the model in the data type defined in a model's `config.json` file.\n \n ```py\n import torch\n@@ -96,7 +96,7 @@ model_8bit = AutoModelForCausalLM.from_pretrained(\n     \"facebook/opt-350m\", \n     device_map=\"auto\",\n     quantization_config=quantization_config, \n-    torch_dtype=\"auto\"\n+    dtype=\"auto\"\n )\n model_8bit.model.decoder.layers[-1].final_layer_norm.weight.dtype\n ```\n@@ -134,7 +134,7 @@ model_4bit = AutoModelForCausalLM.from_pretrained(\n )\n ```\n \n-By default, all other modules such as [torch.nn.LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html) are converted to `torch.float16`. You can change the data type of these modules with the `torch_dtype` parameter.. Setting `torch_dtype=\"auto\"` loads the model in the data type defined in a model's `config.json` file.\n+By default, all other modules such as [torch.nn.LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html) are converted to `torch.float16`. You can change the data type of these modules with the `dtype` parameter.. Setting `dtype=\"auto\"` loads the model in the data type defined in a model's `config.json` file.\n \n ```py\n import torch\n@@ -146,7 +146,7 @@ model_4bit = AutoModelForCausalLM.from_pretrained(\n     \"facebook/opt-350m\",\n     device_map=\"auto\",\n     quantization_config=quantization_config, \n-    torch_dtype=\"auto\"\n+    dtype=\"auto\"\n )\n model_4bit.model.decoder.layers[-1].final_layer_norm.weight.dtype\n ```\n@@ -218,7 +218,7 @@ Now load your model with the custom `device_map` and `quantization_config`.\n ```py\n model_8bit = AutoModelForCausalLM.from_pretrained(\n     \"bigscience/bloom-1b7\",\n-    torch_dtype=\"auto\",\n+    dtype=\"auto\",\n     device_map=device_map,\n     quantization_config=quantization_config,\n )\n@@ -242,7 +242,7 @@ quantization_config = BitsAndBytesConfig(\n \n model_8bit = AutoModelForCausalLM.from_pretrained(\n     model_id,\n-    torch_dtype=\"auto\",\n+    dtype=\"auto\",\n     device_map=device_map,\n     quantization_config=quantization_config,\n )\n@@ -263,7 +263,7 @@ quantization_config = BitsAndBytesConfig(\n \n model_8bit = AutoModelForCausalLM.from_pretrained(\n     model_id,\n-    torch_dtype=\"auto\",\n+    dtype=\"auto\",\n     device_map=\"auto\",\n     quantization_config=quantization_config,\n )\n@@ -300,10 +300,10 @@ nf4_config = BitsAndBytesConfig(\n     bnb_4bit_quant_type=\"nf4\",\n )\n \n-model_nf4 = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", quantization_config=nf4_config)\n+model_nf4 = AutoModelForCausalLM.from_pretrained(model_id, dtype=\"auto\", quantization_config=nf4_config)\n ```\n \n-For inference, the `bnb_4bit_quant_type` does not have a huge impact on performance. However, to remain consistent with the model weights, you should use the `bnb_4bit_compute_dtype` and `torch_dtype` values.\n+For inference, the `bnb_4bit_quant_type` does not have a huge impact on performance. However, to remain consistent with the model weights, you should use the `bnb_4bit_compute_dtype` and `dtype` values.\n \n ### Nested quantization\n \n@@ -317,7 +317,7 @@ double_quant_config = BitsAndBytesConfig(\n     bnb_4bit_use_double_quant=True,\n )\n \n-model_double_quant = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-13b-chat-hf\", torch_dtype=\"auto\", quantization_config=double_quant_config)\n+model_double_quant = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-13b-chat-hf\", dtype=\"auto\", quantization_config=double_quant_config)\n ```\n \n ## Dequantizing bitsandbytes models"
        },
        {
            "sha": "ff300b9d48a54b44c034cb2ec39fa9697a0b318b",
            "filename": "docs/source/en/quantization/concept_guide.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fquantization%2Fconcept_guide.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fquantization%2Fconcept_guide.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fconcept_guide.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -160,7 +160,7 @@ quantization_config = BitsAndBytesConfig(\n model = AutoModelForCausalLM.from_pretrained(\n     model_id,\n     quantization_config=quantization_config,\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\"\n )\n ```"
        },
        {
            "sha": "1a990f1f0f6f06315b3ee7dbfba02f2a17fc9c0f",
            "filename": "docs/source/en/quantization/contribute.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fquantization%2Fcontribute.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fquantization%2Fcontribute.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fcontribute.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -58,7 +58,7 @@ Some quantization methods may require \"pre-quantizing\" the model through data ca\n     - `is_serializable`: A property method to determine whether the method is serializable or not.\n     - `is_trainable`:  A property method to determine whether you can fine-tune models on top of the quantization method (with or without PEFT approaches).\n \n-4. Write the `validate_environment` and `update_torch_dtype` methods. These methods are called before creating the quantized model to ensure users use the right configuration. Refer to other quantizers for an example of it is implemented.\n+4. Write the `validate_environment` and `update_dtype` methods. These methods are called before creating the quantized model to ensure users use the right configuration. Refer to other quantizers for an example of it is implemented.\n \n 5. Write the `_process_model_before_weight_loading` method. In Transformers, the quantized models are initialized first on the `\"meta\"` device before loading the weights. This means the `_process_model_before_weight_loading` method takes care of manipulating the model skeleton to replace some modules ([nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)) with the target modules (quantization modules).\n "
        },
        {
            "sha": "9adfef76bc687a6cfea8f38bbc1c258903ebcfa5",
            "filename": "docs/source/en/quantization/eetq.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fquantization%2Feetq.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fquantization%2Feetq.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Feetq.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -50,7 +50,7 @@ from transformers import AutoModelForCausalLM, EetqConfig\n quantization_config = EetqConfig(\"int8\")\n model = AutoModelForCausalLM.from_pretrained(\n     \"meta-llama/Llama-3.1-8B\",\n-    torch_dtype=\"auto\",\n+    dtype=\"auto\",\n     device_map=\"auto\",\n     quantization_config=quantization_config\n )"
        },
        {
            "sha": "ff52576df6d6ac8af8291ee66eae08f5d10add18",
            "filename": "docs/source/en/quantization/fbgemm_fp8.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fquantization%2Ffbgemm_fp8.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fquantization%2Ffbgemm_fp8.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Ffbgemm_fp8.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -37,7 +37,7 @@ from transformers import FbgemmFp8Config, AutoModelForCausalLM\n quantization_config = FbgemmFp8Config()\n quantized_model = AutoModelForCausalLM.from_pretrained(\n     \"meta-llama/Meta-Llama-3-8B\",\n-    torch_dtype=\"auto\",\n+    dtype=\"auto\",\n     device_map=\"auto\",\n     quantization_config=quantization_config\n )"
        },
        {
            "sha": "bbf273d8d93365e0396de8b8c6ccc8309e5af4d3",
            "filename": "docs/source/en/quantization/finegrained_fp8.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fquantization%2Ffinegrained_fp8.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fquantization%2Ffinegrained_fp8.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Ffinegrained_fp8.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -36,14 +36,14 @@ Install Accelerate and upgrade to the latest version of PyTorch.\n pip install --upgrade accelerate torch\n ```\n \n-Create a [`FineGrainedFP8Config`] class and pass it to [`~PreTrainedModel.from_pretrained`] to quantize it. The weights are loaded in full precision (`torch.float32`) by default regardless of the actual data type the weights are stored in. Set `torch_dtype=\"auto\"` to load the weights in the data type defined in a models `config.json` file to automatically load the most memory-optiomal data type.\n+Create a [`FineGrainedFP8Config`] class and pass it to [`~PreTrainedModel.from_pretrained`] to quantize it. The weights are loaded in full precision (`torch.float32`) by default regardless of the actual data type the weights are stored in. Set `dtype=\"auto\"` to load the weights in the data type defined in a models `config.json` file to automatically load the most memory-optiomal data type.\n \n ```py\n from transformers import FineGrainedFP8Config, AutoModelForCausalLM, AutoTokenizer\n \n model_name = \"meta-llama/Meta-Llama-3-8B\"\n quantization_config = FineGrainedFP8Config()\n-quantized_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\", quantization_config=quantization_config)\n+quantized_model = AutoModelForCausalLM.from_pretrained(model_name, dtype=\"auto\", device_map=\"auto\", quantization_config=quantization_config)\n \n tokenizer = AutoTokenizer.from_pretrained(model_name)\n input_text = \"What are we having for dinner?\""
        },
        {
            "sha": "7c12fb87053189624f9006c07e3604c341458d9c",
            "filename": "docs/source/en/quantization/fp_quant.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fquantization%2Ffp_quant.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fquantization%2Ffp_quant.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Ffp_quant.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -28,7 +28,7 @@ model = AutoModelForCausalLM.from_pretrained(\n     \"qwen/Qwen3-8B\",\n     quantization_config=FPQuantConfig(),\n     device_map=\"auto\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n )\n ```\n \n@@ -53,7 +53,7 @@ model = AutoModelForCausalLM.from_pretrained(\n     \"qwen/Qwen3-8B\",\n     quantization_config=FPQuantConfig(),\n     device_map=\"auto\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n )\n \n model.forward = torch.compile(model.forward, mode=\"max-autotune\", fullgraph=True)"
        },
        {
            "sha": "a2df7b3ad3ce137ab5ebc9358f8fc56fa1f92d05",
            "filename": "docs/source/en/quantization/hqq.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fquantization%2Fhqq.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fquantization%2Fhqq.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fhqq.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -40,7 +40,7 @@ from transformers import AutoModelForCausalLM, AutoTokenizer, HqqConfig\n quant_config = HqqConfig(nbits=8, group_size=64)\n model = transformers.AutoModelForCausalLM.from_pretrained(\n     \"meta-llama/Llama-3.1-8B\", \n-    torch_dtype=torch.float16, \n+    dtype=torch.float16, \n     device_map=\"auto\", \n     quantization_config=quant_config\n )\n@@ -67,7 +67,7 @@ quant_config  = HqqConfig(dynamic_config={\n \n model = transformers.AutoModelForCausalLM.from_pretrained(\n     \"meta-llama/Llama-3.1-8B\", \n-    torch_dtype=torch.float16, \n+    dtype=torch.float16, \n     device_map=\"auto\", \n     quantization_config=quant_config\n )"
        },
        {
            "sha": "b3cf58b5b6ad6ef1c91a241e558fde5629b0087c",
            "filename": "docs/source/en/quantization/quanto.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fquantization%2Fquanto.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fquantization%2Fquanto.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fquanto.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -37,7 +37,7 @@ from transformers import AutoModelForCausalLM, AutoTokenizer, QuantoConfig\n quant_config = QuantoConfig(weights=\"int8\")\n model = transformers.AutoModelForCausalLM.from_pretrained(\n     \"meta-llama/Llama-3.1-8B\", \n-    torch_dtype=\"auto\", \n+    dtype=\"auto\", \n     device_map=\"auto\", \n     quantization_config=quant_config\n )\n@@ -54,7 +54,7 @@ from transformers import AutoModelForSpeechSeq2Seq, QuantoConfig\n quant_config = QuantoConfig(weights=\"int8\")\n model = AutoModelForSpeechSeq2Seq.from_pretrained(\n   \"openai/whisper-large-v2\",\n-  torch_dtype=\"auto\",\n+  dtype=\"auto\",\n   device_map=\"auto\",\n   quantization_config=quant_config\n )"
        },
        {
            "sha": "9cabf0d5538882b4fa4e59ab86f184c9c65cddf0",
            "filename": "docs/source/en/quantization/spqr.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fquantization%2Fspqr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fquantization%2Fspqr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fspqr.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -33,7 +33,7 @@ import torch\n \n quantized_model = AutoModelForCausalLM.from_pretrained(\n     \"elvircrn/Llama-2-7b-SPQR-3Bit-16x16-red_pajama-hf\",\n-    torch_dtype=torch.half,\n+    dtype=torch.half,\n     device_map=\"auto\"\n )\n tokenizer = AutoTokenizer.from_pretrained(\"elvircrn/Llama-2-7b-SPQR-3Bit-16x16-red_pajama-hf\")"
        },
        {
            "sha": "6427866d0229101affa79b0dbfff981fea7295bf",
            "filename": "docs/source/en/quantization/torchao.md",
            "status": "modified",
            "additions": 18,
            "deletions": 18,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -105,7 +105,7 @@ quantization_config = TorchAoConfig(quant_type=quant_config)\n # Load and quantize the model\n quantized_model = AutoModelForCausalLM.from_pretrained(\n     \"meta-llama/Llama-3.1-8B-Instruct\",\n-    torch_dtype=\"auto\",\n+    dtype=\"auto\",\n     device_map=\"auto\",\n     quantization_config=quantization_config\n )\n@@ -133,7 +133,7 @@ quantization_config = TorchAoConfig(quant_type=quant_config)\n # Load and quantize the model\n quantized_model = AutoModelForCausalLM.from_pretrained(\n     \"meta-llama/Llama-3.1-8B-Instruct\",\n-    torch_dtype=\"auto\",\n+    dtype=\"auto\",\n     device_map=\"auto\",\n     quantization_config=quantization_config\n )\n@@ -164,7 +164,7 @@ quantization_config = TorchAoConfig(quant_type=quant_config)\n # Load and quantize the model with sparsity. A sparse checkpoint is needed to accelerate without accuracy loss\n quantized_model = AutoModelForCausalLM.from_pretrained(\n     \"RedHatAI/Sparse-Llama-3.1-8B-2of4\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n     quantization_config=quantization_config\n )\n@@ -197,7 +197,7 @@ quantization_config = TorchAoConfig(quant_type=quant_config)\n # Load and quantize the model\n quantized_model = AutoModelForCausalLM.from_pretrained(\n     \"meta-llama/Llama-3.1-8B-Instruct\",\n-    torch_dtype=\"auto\",\n+    dtype=\"auto\",\n     device_map=\"auto\",\n     quantization_config=quantization_config\n )\n@@ -232,7 +232,7 @@ quantization_config = TorchAoConfig(quant_type=quant_config)\n # Load and quantize the model\n quantized_model = AutoModelForCausalLM.from_pretrained(\n     \"meta-llama/Llama-3.1-8B-Instruct\",\n-    torch_dtype=\"auto\",\n+    dtype=\"auto\",\n     device_map=\"auto\",\n     quantization_config=quantization_config\n )\n@@ -263,7 +263,7 @@ quantization_config = TorchAoConfig(quant_type=quant_config)\n # Load and quantize the model with sparsity. A sparse checkpoint is needed to accelerate without accuracy loss\n quantized_model = AutoModelForCausalLM.from_pretrained(\n     \"RedHatAI/Sparse-Llama-3.1-8B-2of4\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n     quantization_config=quantization_config\n )\n@@ -296,7 +296,7 @@ quantization_config = TorchAoConfig(quant_type=quant_config)\n # Load and quantize the model\n quantized_model = AutoModelForCausalLM.from_pretrained(\n     \"meta-llama/Llama-3.1-8B-Instruct\",\n-    torch_dtype=\"auto\",\n+    dtype=\"auto\",\n     device_map=\"auto\",\n     quantization_config=quantization_config\n )\n@@ -327,7 +327,7 @@ quantization_config = TorchAoConfig(quant_type=quant_config)\n # Load and quantize the model\n quantized_model = AutoModelForCausalLM.from_pretrained(\n     \"meta-llama/Llama-3.1-8B-Instruct\",\n-    torch_dtype=\"auto\",\n+    dtype=\"auto\",\n     device_map=\"auto\",\n     quantization_config=quantization_config\n )\n@@ -360,7 +360,7 @@ quantization_config = TorchAoConfig(quant_type=quant_config)\n # Load and quantize the model\n quantized_model = AutoModelForCausalLM.from_pretrained(\n     \"meta-llama/Llama-3.1-8B-Instruct\",\n-    torch_dtype=\"auto\",\n+    dtype=\"auto\",\n     device_map=\"cpu\",\n     quantization_config=quantization_config\n )\n@@ -391,7 +391,7 @@ quantization_config = TorchAoConfig(quant_type=quant_config)\n # Load and quantize the model\n quantized_model = AutoModelForCausalLM.from_pretrained(\n     \"meta-llama/Llama-3.1-8B-Instruct\",\n-    torch_dtype=\"auto\",\n+    dtype=\"auto\",\n     device_map=\"cpu\",\n     quantization_config=quantization_config\n )\n@@ -422,7 +422,7 @@ config = Int4WeightOnlyConfig(group_size=128)\n # set default to int4 (for linears), and skip quantizing `model.layers.0.self_attn.q_proj`\n quant_config = ModuleFqnToConfig({\"_default\": config, \"model.layers.0.self_attn.q_proj\": None})\n quantization_config = TorchAoConfig(quant_type=quant_config)\n-quantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.bfloat16, quantization_config=quantization_config)\n+quantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", dtype=torch.bfloat16, quantization_config=quantization_config)\n # lm_head is not quantized and model.layers.0.self_attn.q_proj is not quantized\n print(\"quantized model:\", quantized_model)\n tokenizer = AutoTokenizer.from_pretrained(model_id)\n@@ -459,7 +459,7 @@ quant_config = ModuleFqnToConfig({\"_default\": linear_config, \"model.decoder.embe\n # set `include_embedding` to True in order to include embedding in quantization\n # when `include_embedding` is True, we'll remove input embedding from `modules_not_to_convert` as well\n quantization_config = TorchAoConfig(quant_type=quant_config, include_embedding=True)\n-quantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cpu\", torch_dtype=torch.bfloat16, quantization_config=quantization_config)\n+quantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cpu\", dtype=torch.bfloat16, quantization_config=quantization_config)\n print(\"quantized model:\", quantized_model)\n # make sure embedding is quantized\n print(\"embed_tokens weight:\", quantized_model.model.decoder.embed_tokens.weight)\n@@ -493,7 +493,7 @@ from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n quantization_config = TorchAoConfig(\"autoquant\", min_sqnr=None)\n quantized_model = AutoModelForCausalLM.from_pretrained(\n     \"meta-llama/Llama-3.1-8B-Instruct\",\n-    torch_dtype=\"auto\",\n+    dtype=\"auto\",\n     device_map=\"auto\",\n     quantization_config=quantization_config\n )\n@@ -552,7 +552,7 @@ quantization_config = TorchAoConfig(quant_type=quant_config)\n # Load and quantize the model\n quantized_model = AutoModelForCausalLM.from_pretrained(\n     \"meta-llama/Llama-3.1-8B-Instruct\",\n-    torch_dtype=\"auto\",\n+    dtype=\"auto\",\n     device_map=\"cpu\",\n     quantization_config=quantization_config\n )\n@@ -564,7 +564,7 @@ quantized_model.save_pretrained(output_dir, safe_serialization=False)\n reloaded_model = AutoModelForCausalLM.from_pretrained(\n     output_dir,\n     device_map=\"auto\",\n-    torch_dtype=torch.bfloat16\n+    dtype=torch.bfloat16\n )\n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n input_text = \"What are we having for dinner?\"\n@@ -588,7 +588,7 @@ quantization_config = TorchAoConfig(quant_type=quant_config)\n # Load and quantize the model\n quantized_model = AutoModelForCausalLM.from_pretrained(\n     \"meta-llama/Llama-3.1-8B-Instruct\",\n-    torch_dtype=\"auto\",\n+    dtype=\"auto\",\n     device_map=\"cpu\",\n     quantization_config=quantization_config\n )\n@@ -600,7 +600,7 @@ quantized_model.save_pretrained(output_dir, safe_serialization=False)\n reloaded_model = AutoModelForCausalLM.from_pretrained(\n     output_dir,\n     device_map=\"cpu\",\n-    torch_dtype=torch.bfloat16\n+    dtype=torch.bfloat16\n )\n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n input_text = \"What are we having for dinner?\"\n@@ -660,7 +660,7 @@ def benchmark_fn(func: Callable, *args, **kwargs) -> float:\n MAX_NEW_TOKENS = 1000\n print(\"int4wo-128 model:\", benchmark_fn(quantized_model.generate, **input_ids, max_new_tokens=MAX_NEW_TOKENS, cache_implementation=\"static\"))\n \n-bf16_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.bfloat16)\n+bf16_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", dtype=torch.bfloat16)\n output = bf16_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\") # auto-compile\n print(\"bf16 model:\", benchmark_fn(bf16_model.generate, **input_ids, max_new_tokens=MAX_NEW_TOKENS, cache_implementation=\"static\"))\n ```"
        },
        {
            "sha": "3ddb46dba1418fc6009b02187519155f36bdb12e",
            "filename": "docs/source/en/quantization/vptq.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fquantization%2Fvptq.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fquantization%2Fvptq.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fvptq.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -45,7 +45,7 @@ from transformers import AutoTokenizer, AutoModelForCausalLM\n \n quantized_model = AutoModelForCausalLM.from_pretrained(\n     \"VPTQ-community/Meta-Llama-3.1-70B-Instruct-v16-k65536-65536-woft\",\n-    torch_dtype=\"auto\", \n+    dtype=\"auto\", \n     device_map=\"auto\"\n )\n ```"
        },
        {
            "sha": "1138b10ec4b9086def34cf425378f275c8afb229",
            "filename": "docs/source/en/quicktour.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fquicktour.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Fquicktour.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquicktour.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -100,12 +100,12 @@ Use [`~PreTrainedModel.from_pretrained`] to load the weights and configuration f\n When you load a model, configure the following parameters to ensure the model is optimally loaded.\n \n - `device_map=\"auto\"` automatically allocates the model weights to your fastest device first.\n-- `torch_dtype=\"auto\"` directly initializes the model weights in the data type they're stored in, which can help avoid loading the weights twice (PyTorch loads weights in `torch.float32` by default).\n+- `dtype=\"auto\"` directly initializes the model weights in the data type they're stored in, which can help avoid loading the weights twice (PyTorch loads weights in `torch.float32` by default).\n \n ```py\n from transformers import AutoModelForCausalLM, AutoTokenizer\n \n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=\"auto\", device_map=\"auto\")\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", dtype=\"auto\", device_map=\"auto\")\n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n ```\n "
        },
        {
            "sha": "3f8915f3cc99c754b9ecebe9c8ae4dea270d2706",
            "filename": "docs/source/en/tasks/idefics.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Ftasks%2Fidefics.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Ftasks%2Fidefics.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fidefics.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -76,7 +76,7 @@ preparing text and image inputs for the model.\n \n >>> processor = AutoProcessor.from_pretrained(checkpoint)\n \n->>> model = IdeficsForVisionText2Text.from_pretrained(checkpoint, torch_dtype=torch.bfloat16, device_map=\"auto\")\n+>>> model = IdeficsForVisionText2Text.from_pretrained(checkpoint, dtype=torch.bfloat16, device_map=\"auto\")\n ```\n \n Setting `device_map` to `\"auto\"` will automatically determine how to load and store the model weights in the most optimized \n@@ -388,7 +388,7 @@ The use and prompting for the conversational use is very similar to using the ba\n >>> from transformers import IdeficsForVisionText2Text, AutoProcessor\n \n >>> checkpoint = \"HuggingFaceM4/idefics-9b-instruct\"\n->>> model = IdeficsForVisionText2Text.from_pretrained(checkpoint, torch_dtype=torch.bfloat16, device_map=\"auto\")\n+>>> model = IdeficsForVisionText2Text.from_pretrained(checkpoint, dtype=torch.bfloat16, device_map=\"auto\")\n >>> processor = AutoProcessor.from_pretrained(checkpoint)\n \n >>> prompts = ["
        },
        {
            "sha": "b34f4edf90f6ac0f14888a62034ee4cf242050d5",
            "filename": "docs/source/en/tasks/image_text_to_text.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Ftasks%2Fimage_text_to_text.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Ftasks%2Fimage_text_to_text.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fimage_text_to_text.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -44,7 +44,7 @@ import torch\n device = torch.device(infer_device())\n model = AutoModelForImageTextToText.from_pretrained(\n     \"HuggingFaceM4/idefics2-8b\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     attn_implementation=\"flash_attention_2\",\n ).to(device)\n "
        },
        {
            "sha": "eb8e61d67aafebb2b351291acd43ed876ce11947",
            "filename": "docs/source/en/tasks/prompting.md",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Ftasks%2Fprompting.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Ftasks%2Fprompting.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fprompting.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -26,7 +26,7 @@ Try prompting a LLM to classify some text. When you create a prompt, it's import\n from transformers import pipeline\n import torch\n \n-pipeline = pipeline(task=\"text-generation\", model=\"mistralai/Mistal-7B-Instruct-v0.1\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n+pipeline = pipeline(task=\"text-generation\", model=\"mistralai/Mistal-7B-Instruct-v0.1\", dtype=torch.bfloat16, device_map=\"auto\")\n prompt = \"\"\"Classify the text into neutral, negative or positive.\n Text: This movie is definitely one of my favorite movies of its kind. The interaction between respectable and morally strong characters is an ode to chivalry and the honor code amongst thieves and policemen.\n Sentiment:\n@@ -86,7 +86,7 @@ Few-shot prompting improves accuracy and performance by including specific examp\n from transformers import pipeline\n import torch\n \n-pipeline = pipeline(model=\"mistralai/Mistral-7B-Instruct-v0.1\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n+pipeline = pipeline(model=\"mistralai/Mistral-7B-Instruct-v0.1\", dtype=torch.bfloat16, device_map=\"auto\")\n prompt = \"\"\"Text: The first human went into space and orbited the Earth on April 12, 1961.\n Date: 04/12/1961\n Text: The first-ever televised presidential debate in the United States took place on September 28, 1960, between presidential candidates John F. Kennedy and Richard Nixon.\n@@ -111,7 +111,7 @@ Structure your prompt as a turn-based conversation and use the [`apply_chat_temp\n from transformers import pipeline\n import torch\n \n-pipeline = pipeline(model=\"mistralai/Mistral-7B-Instruct-v0.1\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n+pipeline = pipeline(model=\"mistralai/Mistral-7B-Instruct-v0.1\", dtype=torch.bfloat16, device_map=\"auto\")\n \n messages = [\n     {\"role\": \"user\", \"content\": \"Text: The first human went into space and orbited the Earth on April 12, 1961.\"},\n@@ -145,7 +145,7 @@ The example below provides the model with several prompts to work through interm\n from transformers import pipeline\n import torch\n \n-pipeline = pipeline(model=\"mistralai/Mistral-7B-Instruct-v0.1\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n+pipeline = pipeline(model=\"mistralai/Mistral-7B-Instruct-v0.1\", dtype=torch.bfloat16, device_map=\"auto\")\n prompt = \"\"\"Let's go through this step-by-step:\n 1. You start with 15 muffins.\n 2. You eat 2 muffins, leaving you with 13 muffins.\n@@ -193,7 +193,7 @@ The examples below demonstrate prompting a LLM for different tasks.\n from transformers import pipeline\n import torch\n \n-pipeline = pipeline(model=\"mistralai/Mistral-7B-Instruct-v0.1\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n+pipeline = pipeline(model=\"mistralai/Mistral-7B-Instruct-v0.1\", dtype=torch.bfloat16, device_map=\"auto\")\n prompt = \"\"\"Return a list of named entities in the text.\n Text: The company was founded in 2016 by French entrepreneurs ClÃ©ment Delangue, Julien Chaumond, and Thomas Wolf in New York City, originally as a company that developed a chatbot app targeted at teenagers.\n Named entities:\n@@ -212,7 +212,7 @@ Result:  [ClÃ©ment Delangue, Julien Chaumond, Thomas Wolf, company, New York Cit\n from transformers import pipeline\n import torch\n \n-pipeline = pipeline(model=\"mistralai/Mistral-7B-Instruct-v0.1\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n+pipeline = pipeline(model=\"mistralai/Mistral-7B-Instruct-v0.1\", dtype=torch.bfloat16, device_map=\"auto\")\n prompt = \"\"\"Translate the English text to French.\n Text: Sometimes, I've believed as many as six impossible things before breakfast.\n Translation:\n@@ -231,7 +231,7 @@ Result: Ã€ l'occasion, j'ai croyu plus de six choses impossibles\n from transformers import pipeline\n import torch\n \n-pipeline = pipeline(model=\"mistralai/Mistral-7B-Instruct-v0.1\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n+pipeline = pipeline(model=\"mistralai/Mistral-7B-Instruct-v0.1\", dtype=torch.bfloat16, device_map=\"auto\")\n prompt = \"\"\"Permaculture is a design process mimicking the diversity, functionality and resilience of natural ecosystems. The principles and practices are drawn from traditional ecological knowledge of indigenous cultures combined with modern scientific understanding and technological innovations. Permaculture design provides a framework helping individuals and communities develop innovative, creative and effective strategies for meeting basic needs while preparing for and mitigating the projected impacts of climate change.\n Write a summary of the above text.\n Summary:\n@@ -250,7 +250,7 @@ Result: Permaculture is the design process that involves mimicking natural ecosy\n from transformers import pipeline\n import torch\n \n-pipeline = pipeline(model=\"mistralai/Mistral-7B-Instruct-v0.1\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n+pipeline = pipeline(model=\"mistralai/Mistral-7B-Instruct-v0.1\", dtype=torch.bfloat16, device_map=\"auto\")\n prompt = \"\"\"Answer the question using the context below.\n Context: Gazpacho is a cold soup and drink made of raw, blended vegetables. Most gazpacho includes stale bread, tomato, cucumbers, onion, bell peppers, garlic, olive oil, wine vinegar, water, and salt. Northern recipes often include cumin and/or pimentÃ³n (smoked sweet paprika). Traditionally, gazpacho was made by pounding the vegetables in a mortar with a pestle; this more laborious method is still sometimes used as it helps keep the gazpacho cool and avoids the foam and silky consistency of smoothie versions made in blenders or food processors.\n Question: What modern tool is used to make gazpacho?"
        },
        {
            "sha": "0e0191af5884697df3176e8adb82fb72870d3c7a",
            "filename": "docs/source/en/tasks/video_text_to_text.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Ftasks%2Fvideo_text_to_text.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Ftasks%2Fvideo_text_to_text.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fvideo_text_to_text.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -46,7 +46,7 @@ model_id = \"llava-hf/llava-interleave-qwen-0.5b-hf\"\n \n processor = LlavaProcessor.from_pretrained(model_id)\n \n-model = LlavaForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.float16)\n+model = LlavaForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\", dtype=torch.float16)\n ```\n \n Some models directly consume the `<video>` token, and others accept `<image>` tokens equal to the number of sampled frames. This model handles videos in the latter fashion. We will write a simple utility to handle image tokens, and another utility to get a video from a url and sample frames from it. "
        },
        {
            "sha": "47ef9e56702a1df15ce76142c7704ba23c0d1451",
            "filename": "docs/source/en/tasks/visual_document_retrieval.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Ftasks%2Fvisual_document_retrieval.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Ftasks%2Fvisual_document_retrieval.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fvisual_document_retrieval.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -61,7 +61,7 @@ processor = ColPaliProcessor.from_pretrained(model_name)\n \n model = ColPaliForRetrieval.from_pretrained(\n     model_name,\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n ).eval()\n ```"
        },
        {
            "sha": "fb73ae34772363bea3884cb29f7c19cab8e05369",
            "filename": "docs/source/en/tasks/visual_question_answering.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Ftasks%2Fvisual_question_answering.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fen%2Ftasks%2Fvisual_question_answering.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fvisual_question_answering.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -365,7 +365,7 @@ GPU, if available, which we didn't need to do earlier when training, as [`Traine\n >>> import torch\n \n >>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n->>> model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n+>>> model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", dtype=torch.float16)\n >>> device = infer_device()\n >>> model.to(device)\n ```"
        },
        {
            "sha": "149ae9d9a257c2341c6f8b16d287780701727a14",
            "filename": "docs/source/es/pipeline_tutorial.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fes%2Fpipeline_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fes%2Fpipeline_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Fpipeline_tutorial.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -282,7 +282,7 @@ pip install pytesseract\n import torch\n from transformers import pipeline\n \n-pipe = pipeline(model=\"facebook/opt-1.3b\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n+pipe = pipeline(model=\"facebook/opt-1.3b\", dtype=torch.bfloat16, device_map=\"auto\")\n output = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\n ```\n "
        },
        {
            "sha": "e5028c9d0bda5287321663e277cdd7bd23a5fa88",
            "filename": "docs/source/fr/tutoriel_pipeline.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Ffr%2Ftutoriel_pipeline.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Ffr%2Ftutoriel_pipeline.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Ffr%2Ftutoriel_pipeline.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -272,7 +272,7 @@ Chargez d'abord votre modÃ¨le en utilisant `device_map=\"auto\"` ! Nous utiliseron\n import torch\n from transformers import pipeline\n \n-pipe = pipeline(model=\"facebook/opt-1.3b\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n+pipe = pipeline(model=\"facebook/opt-1.3b\", dtype=torch.bfloat16, device_map=\"auto\")\n output = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\n ```\n Vous pouvez Ã©galement passer des modÃ¨les chargÃ©s en 8 bits si vous installez `bitsandbytes` et ajoutez l'argument `load_in_8bit=True`"
        },
        {
            "sha": "d2103643d5091ff37a3b5eaefb9a10abba6bf8a1",
            "filename": "docs/source/hi/pipeline_tutorial.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fhi%2Fpipeline_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fhi%2Fpipeline_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fhi%2Fpipeline_tutorial.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -301,7 +301,7 @@ pip install pytesseract\n import torch\n from transformers import pipeline\n \n-pipe = pipeline(model=\"facebook/opt-1.3b\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n+pipe = pipeline(model=\"facebook/opt-1.3b\", dtype=torch.bfloat16, device_map=\"auto\")\n output = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\n ```\n "
        },
        {
            "sha": "750d9b7035efc1b6d2306eb9d6e70910514cc9cc",
            "filename": "docs/source/ja/main_classes/deepspeed.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fja%2Fmain_classes%2Fdeepspeed.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fja%2Fmain_classes%2Fdeepspeed.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmain_classes%2Fdeepspeed.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -1776,7 +1776,7 @@ ZeRO-3 è¨­å®šã‚’æœ‰åŠ¹ã«ã™ã‚‹ã¨ã€ã“ã‚ŒãŒã‚µãƒ³ãƒ—ãƒ« ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®è¨˜\n ã“ã®æ–¹æ³•ã¨ãã®ä»–ã®é–¢é€£æ©Ÿèƒ½ã®è©³ç´°ã«ã¤ã„ã¦ã¯ã€[å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰](https://deepspeed.readthedocs.io/en/latest/zero3.html#constructing-massive-models) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n \n ã¾ãŸã€fp16 ã§äº‹å‰è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨ãã¯ã€`from_pretrained` ã«ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«æŒ‡ç¤ºã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n-`torch_dtype=torch.float16`ã€‚è©³ç´°ã«ã¤ã„ã¦ã¯ã€[from_pretrained-torch-dtype](#from_pretrained-torch-dtype) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n+`dtype=torch.float16`ã€‚è©³ç´°ã«ã¤ã„ã¦ã¯ã€[from_pretrained-torch-dtype](#from_pretrained-torch-dtype) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n \n #### Gathering Parameters\n "
        },
        {
            "sha": "b98d3ac952cfa835d0b38510fca7a86a96b24eaa",
            "filename": "docs/source/ja/main_classes/model.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fja%2Fmain_classes%2Fmodel.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fja%2Fmain_classes%2Fmodel.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmain_classes%2Fmodel.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -99,16 +99,16 @@ device_map = {\"shared\": 0, \"encoder\": 0, \"decoder\": 1, \"lm_head\": 1}\n \n Pytorch ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã¯é€šå¸¸ `torch.float32` å½¢å¼ã§ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã•ã‚Œã¾ã™ã€‚ã“ã‚Œã¯ã€ã—ã‚ˆã†ã¨ã™ã‚‹ã¨å•é¡Œã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™\n é‡ã¿ãŒ fp16 ã«ã‚ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨ã€2 å€ã®ãƒ¡ãƒ¢ãƒªãŒå¿…è¦ã«ãªã‚‹ãŸã‚ã§ã™ã€‚ã“ã®åˆ¶é™ã‚’å…‹æœã™ã‚‹ã«ã¯ã€æ¬¡ã®ã“ã¨ãŒã§ãã¾ã™ã€‚\n-`torch_dtype` å¼•æ•°ã‚’ä½¿ç”¨ã—ã¦ã€ç›®çš„ã® `dtype` ã‚’æ˜Žç¤ºçš„ã«æ¸¡ã—ã¾ã™ã€‚\n+`dtype` å¼•æ•°ã‚’ä½¿ç”¨ã—ã¦ã€ç›®çš„ã® `dtype` ã‚’æ˜Žç¤ºçš„ã«æ¸¡ã—ã¾ã™ã€‚\n \n ```python\n-model = T5ForConditionalGeneration.from_pretrained(\"t5\", torch_dtype=torch.float16)\n+model = T5ForConditionalGeneration.from_pretrained(\"t5\", dtype=torch.float16)\n ```\n ã¾ãŸã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚’å¸¸ã«æœ€é©ãªãƒ¡ãƒ¢ãƒª ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒ­ãƒ¼ãƒ‰ã—ãŸã„å ´åˆã¯ã€ç‰¹åˆ¥ãªå€¤ `\"auto\"` ã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚\n ãã—ã¦ã€`dtype` ã¯ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‹ã‚‰è‡ªå‹•çš„ã«å°Žå‡ºã•ã‚Œã¾ã™ã€‚\n \n ```python\n-model = T5ForConditionalGeneration.from_pretrained(\"t5\", torch_dtype=\"auto\")\n+model = T5ForConditionalGeneration.from_pretrained(\"t5\", dtype=\"auto\")\n ```\n \n ã‚¹ã‚¯ãƒ©ãƒƒãƒã‹ã‚‰ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã«ã¯ã€ã©ã® `dtype` ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã‚’æŒ‡ç¤ºã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚"
        },
        {
            "sha": "d0f0d2c8ae325aab8baab722ef07de4d80ce8601",
            "filename": "docs/source/ja/main_classes/quantization.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fja%2Fmain_classes%2Fquantization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fja%2Fmain_classes%2Fquantization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmain_classes%2Fquantization.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -182,13 +182,13 @@ model_8bit = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", load_in_8\n model_4bit = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", load_in_4bit=True)\n ```\n \n-ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€ä»–ã®ã™ã¹ã¦ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« (ä¾‹: `torch.nn.LayerNorm`) ã¯ `torch.float16` ã«å¤‰æ›ã•ã‚Œã¾ã™ãŒã€ãã® `dtype` ã‚’å¤‰æ›´ã—ãŸã„å ´åˆã¯ã€`torch_dtype` å¼•æ•°ã‚’ä¸Šæ›¸ãã§ãã¾ã™ã€‚\n+ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€ä»–ã®ã™ã¹ã¦ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« (ä¾‹: `torch.nn.LayerNorm`) ã¯ `torch.float16` ã«å¤‰æ›ã•ã‚Œã¾ã™ãŒã€ãã® `dtype` ã‚’å¤‰æ›´ã—ãŸã„å ´åˆã¯ã€`dtype` å¼•æ•°ã‚’ä¸Šæ›¸ãã§ãã¾ã™ã€‚\n \n ```python\n >>> import torch\n >>> from transformers import AutoModelForCausalLM\n \n->>> model_8bit = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", load_in_8bit=True, torch_dtype=torch.float32)\n+>>> model_8bit = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", load_in_8bit=True, dtype=torch.float32)\n >>> model_8bit.model.decoder.layers[-1].final_layer_norm.weight.dtype\n torch.float32\n ```\n@@ -216,7 +216,7 @@ torch.float32\n \n - **ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°:** [QLoRA è«–æ–‡](https://huggingface.co/papers/2305.14314) ã«ã‚ˆã‚‹ã¨ã€4 ãƒ“ãƒƒãƒˆåŸºæœ¬ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹å ´åˆ (ä¾‹: LoRA ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ä½¿ç”¨)ã€`bnb_4bit_quant_type='nf4'` ã‚’ä½¿ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ ã€‚\n \n-- **æŽ¨è«–:** æŽ¨è«–ã®å ´åˆã€`bnb_4bit_quant_type` ã¯ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ã«å¤§ããªå½±éŸ¿ã‚’ä¸Žãˆã¾ã›ã‚“ã€‚ãŸã ã—ã€ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã¨ã®ä¸€è²«æ€§ã‚’ä¿ã¤ãŸã‚ã«ã€å¿…ãšåŒã˜ `bnb_4bit_compute_dtype` ãŠã‚ˆã³ `torch_dtype` å¼•æ•°ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚\n+- **æŽ¨è«–:** æŽ¨è«–ã®å ´åˆã€`bnb_4bit_quant_type` ã¯ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ã«å¤§ããªå½±éŸ¿ã‚’ä¸Žãˆã¾ã›ã‚“ã€‚ãŸã ã—ã€ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã¨ã®ä¸€è²«æ€§ã‚’ä¿ã¤ãŸã‚ã«ã€å¿…ãšåŒã˜ `bnb_4bit_compute_dtype` ãŠã‚ˆã³ `dtype` å¼•æ•°ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚\n \n \n #### Load a large model in 4bit"
        },
        {
            "sha": "4ef79ea2fadf28e2fc2671ea4257a91e5c0995a7",
            "filename": "docs/source/ja/model_doc/bark.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fja%2Fmodel_doc%2Fbark.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fja%2Fmodel_doc%2Fbark.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fbark.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -39,7 +39,7 @@ from transformers import BarkModel\n import torch\n \n device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n-model = BarkModel.from_pretrained(\"suno/bark-small\", torch_dtype=torch.float16).to(device)\n+model = BarkModel.from_pretrained(\"suno/bark-small\", dtype=torch.float16).to(device)\n ```\n \n #### Using ðŸ¤— Better Transformer\n@@ -75,7 +75,7 @@ import torch\n device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n \n # load in fp16\n-model = BarkModel.from_pretrained(\"suno/bark-small\", torch_dtype=torch.float16).to(device)\n+model = BarkModel.from_pretrained(\"suno/bark-small\", dtype=torch.float16).to(device)\n \n # convert to bettertransformer\n model = BetterTransformer.transform(model, keep_original_model=False)"
        },
        {
            "sha": "1f7ee3051bfcd563bc31eb5333c97835444d38a4",
            "filename": "docs/source/ja/model_doc/code_llama.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fja%2Fmodel_doc%2Fcode_llama.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fja%2Fmodel_doc%2Fcode_llama.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fcode_llama.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -33,11 +33,11 @@ Code Llama ãƒ¢ãƒ‡ãƒ«ã¯ã«ã‚ˆã£ã¦ [Code Llama: Open Foundation Models for Code\n \n Code Llama ã®ãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹`Llama2`ãƒ•ã‚¡ãƒŸãƒªãƒ¼ ãƒ¢ãƒ‡ãƒ«ã¯ã€`bfloat16`ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¾ã—ãŸãŒã€å…ƒã®æŽ¨è«–ã§ã¯`float16`ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ã•ã¾ã–ã¾ãªç²¾åº¦ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n \n-* `float32`: ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«é–¢ã™ã‚‹ PyTorch ã®è¦ç´„ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ãŒã©ã® `dtype` ã§æ ¼ç´ã•ã‚ŒãŸã‹ã«é–¢ä¿‚ãªãã€ãƒ¢ãƒ‡ãƒ«ã‚’ `float32` ã«ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚ ã€Œtransformersã€ã‚‚ã€PyTorch ã¨ã®ä¸€è²«æ€§ã‚’ä¿ã¤ãŸã‚ã«ã“ã®è¦å‰‡ã«å¾“ã£ã¦ã„ã¾ã™ã€‚ã“ã‚Œã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§é¸æŠžã•ã‚Œã¾ã™ã€‚ `AutoModel` API ã§ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã®é‡ã¿ä»˜ã‘ã‚¿ã‚¤ãƒ—ã‚’ä½¿ç”¨ã—ã¦ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ãƒ­ãƒ¼ãƒ‰ã‚’ã‚­ãƒ£ã‚¹ãƒˆã™ã‚‹å ´åˆã¯ã€`torch_dtype=\"auto\"` ã‚’æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ `model = AutoModelForCausalLM.from_pretrained(\"path\", torch_dtype = \"auto\")`ã€‚\n+* `float32`: ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«é–¢ã™ã‚‹ PyTorch ã®è¦ç´„ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ãŒã©ã® `dtype` ã§æ ¼ç´ã•ã‚ŒãŸã‹ã«é–¢ä¿‚ãªãã€ãƒ¢ãƒ‡ãƒ«ã‚’ `float32` ã«ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚ ã€Œtransformersã€ã‚‚ã€PyTorch ã¨ã®ä¸€è²«æ€§ã‚’ä¿ã¤ãŸã‚ã«ã“ã®è¦å‰‡ã«å¾“ã£ã¦ã„ã¾ã™ã€‚ã“ã‚Œã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§é¸æŠžã•ã‚Œã¾ã™ã€‚ `AutoModel` API ã§ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã®é‡ã¿ä»˜ã‘ã‚¿ã‚¤ãƒ—ã‚’ä½¿ç”¨ã—ã¦ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ãƒ­ãƒ¼ãƒ‰ã‚’ã‚­ãƒ£ã‚¹ãƒˆã™ã‚‹å ´åˆã¯ã€`dtype=\"auto\"` ã‚’æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ `model = AutoModelForCausalLM.from_pretrained(\"path\", dtype = \"auto\")`ã€‚\n * `bfloat16`: ã‚³ãƒ¼ãƒ‰ Llama ã¯ã“ã®ç²¾åº¦ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€ã•ã‚‰ãªã‚‹ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚„å¾®èª¿æ•´ã«ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚\n * `float16`: ã“ã®ç²¾åº¦ã‚’ä½¿ç”¨ã—ã¦æŽ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚é€šå¸¸ã¯ `bfloat16` ã‚ˆã‚Šé«˜é€Ÿã§ã‚ã‚Šã€è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«ã¯ `bfloat16` ã¨æ¯”ã¹ã¦æ˜Žã‚‰ã‹ãªä½Žä¸‹ãŒè¦‹ã‚‰ã‚Œãªã„ãŸã‚ã§ã™ã€‚ bfloat16 ã‚’ä½¿ç”¨ã—ã¦æŽ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚å¾®èª¿æ•´å¾Œã€float16 ã¨ bfloat16 ã®ä¸¡æ–¹ã§æŽ¨è«–çµæžœã‚’ç¢ºèªã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚\n \n-ä¸Šã§è¿°ã¹ãŸã‚ˆã†ã«ã€ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã™ã‚‹ã¨ãã« `torch_dtype=\"auto\"` ã‚’ä½¿ç”¨ã—ãªã„é™ã‚Šã€ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã®é‡ã¿ã® `dtype` ã¯ã»ã¨ã‚“ã©ç„¡é–¢ä¿‚ã§ã™ã€‚ãã®ç†ç”±ã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒæœ€åˆã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œ (ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã® `dtype` ã‚’ä½¿ç”¨)ã€æ¬¡ã« `torch` ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã® `dtype` ã«ã‚­ãƒ£ã‚¹ãƒˆã•ã‚Œã‚‹ãŸã‚ã§ã™ (`torch.float32` ã«ãªã‚Šã¾ã™)ã€‚æŒ‡å®šã•ã‚ŒãŸ `torch_dtype` ãŒã‚ã‚‹å ´åˆã¯ã€ä»£ã‚ã‚Šã«ãã‚ŒãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚\n+ä¸Šã§è¿°ã¹ãŸã‚ˆã†ã«ã€ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã™ã‚‹ã¨ãã« `dtype=\"auto\"` ã‚’ä½¿ç”¨ã—ãªã„é™ã‚Šã€ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã®é‡ã¿ã® `dtype` ã¯ã»ã¨ã‚“ã©ç„¡é–¢ä¿‚ã§ã™ã€‚ãã®ç†ç”±ã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒæœ€åˆã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œ (ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã® `dtype` ã‚’ä½¿ç”¨)ã€æ¬¡ã« `torch` ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã® `dtype` ã«ã‚­ãƒ£ã‚¹ãƒˆã•ã‚Œã‚‹ãŸã‚ã§ã™ (`torch.float32` ã«ãªã‚Šã¾ã™)ã€‚æŒ‡å®šã•ã‚ŒãŸ `dtype` ãŒã‚ã‚‹å ´åˆã¯ã€ä»£ã‚ã‚Šã«ãã‚ŒãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚\n \n </Tip>\n \n@@ -93,7 +93,7 @@ def remove_non_ascii(s: str) -> str:\n >>> from transformers import pipeline\n >>> import torch\n \n->>> generator = pipeline(\"text-generation\",model=\"meta-llama/CodeLlama-7b-hf\",torch_dtype=torch.float16, device_map=\"auto\")\n+>>> generator = pipeline(\"text-generation\",model=\"meta-llama/CodeLlama-7b-hf\",dtype=torch.float16, device_map=\"auto\")\n >>> generator('def remove_non_ascii(s: str) -> str:\\n    \"\"\" <FILL_ME>\\n    return result', max_new_tokens = 128)\n [{'generated_text': 'def remove_non_ascii(s: str) -> str:\\n    \"\"\" <FILL_ME>\\n    return resultRemove non-ASCII characters from a string. \"\"\"\\n    result = \"\"\\n    for c in s:\\n        if ord(c) < 128:\\n            result += c'}]\n ```"
        },
        {
            "sha": "478cd382cd2c6917cb6b35ca2d8c2c2a1f7642bb",
            "filename": "docs/source/ja/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fja%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fja%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fperf_infer_gpu_one.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -56,7 +56,7 @@ tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n model = AutoModelForCausalLM.from_pretrained(\n     model_id,\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     attn_implementation=\"flash_attention_2\",\n )\n ```\n@@ -223,7 +223,7 @@ import torch\n from transformers import AutoModelForCausalLM, AutoTokenizer\n \n tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n-model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", torch_dtype=torch.float16).to(\"cuda\")\n+model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", dtype=torch.float16).to(\"cuda\")\n # convert the model to BetterTransformer\n model.to_bettertransformer()\n "
        },
        {
            "sha": "c580d30a7c0075b229f74cf9729f75eb3dc8aa9f",
            "filename": "docs/source/ja/pipeline_tutorial.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fja%2Fpipeline_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fja%2Fpipeline_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fpipeline_tutorial.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -277,7 +277,7 @@ pip install pytesseract\n import torch\n from transformers import pipeline\n \n-pipe = pipeline(model=\"facebook/opt-1.3b\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n+pipe = pipeline(model=\"facebook/opt-1.3b\", dtype=torch.bfloat16, device_map=\"auto\")\n output = pipe(\"ã“ã‚Œã¯ç´ æ™´ã‚‰ã—ã„ä¾‹ã§ã™ï¼\", do_sample=True, top_p=0.95)\n ```\n "
        },
        {
            "sha": "83ed1278496ec89925da9538b2b72386d1571a5c",
            "filename": "docs/source/ja/tasks/idefics.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fja%2Ftasks%2Fidefics.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fja%2Ftasks%2Fidefics.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftasks%2Fidefics.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -78,7 +78,7 @@ IDEFICS ãƒ—ãƒ­ã‚»ãƒƒã‚µã¯ã€[`LlamaTokenizer`] ã¨ IDEFICS ç”»åƒãƒ—ãƒ­ã‚»ãƒƒ\n \n >>> processor = AutoProcessor.from_pretrained(checkpoint)\n \n->>> model = IdeficsForVisionText2Text.from_pretrained(checkpoint, torch_dtype=torch.bfloat16, device_map=\"auto\")\n+>>> model = IdeficsForVisionText2Text.from_pretrained(checkpoint, dtype=torch.bfloat16, device_map=\"auto\")\n ```\n \n `device_map`ã‚’`auto`ã«è¨­å®šã™ã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’æœ€ã‚‚æœ€é©åŒ–ã•ã‚ŒãŸçŠ¶æ…‹ã§ãƒ­ãƒ¼ãƒ‰ãŠã‚ˆã³ä¿å­˜ã™ã‚‹æ–¹æ³•ãŒè‡ªå‹•çš„ã«æ±ºå®šã•ã‚Œã¾ã™ã€‚\n@@ -395,7 +395,7 @@ This is an image of a vegetable stand.\n >>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n \n >>> checkpoint = \"HuggingFaceM4/idefics-9b-instruct\"\n->>> model = IdeficsForVisionText2Text.from_pretrained(checkpoint, torch_dtype=torch.bfloat16).to(device)\n+>>> model = IdeficsForVisionText2Text.from_pretrained(checkpoint, dtype=torch.bfloat16).to(device)\n >>> processor = AutoProcessor.from_pretrained(checkpoint)\n \n >>> prompts = ["
        },
        {
            "sha": "ffafc7f9156fc34288fac5db2ee57b1fc5a9d10e",
            "filename": "docs/source/ja/tasks/prompting.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fja%2Ftasks%2Fprompting.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fja%2Ftasks%2Fprompting.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftasks%2Fprompting.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -128,7 +128,7 @@ pip install -q transformers accelerate\n ...     \"text-generation\",\n ...     model=model,\n ...     tokenizer=tokenizer,\n-...     torch_dtype=torch.bfloat16,\n+...     dtype=torch.bfloat16,\n ...     device_map=\"auto\",\n ... )\n ```"
        },
        {
            "sha": "4c0a3887f4be7b6cc321409f638a28135c805ff4",
            "filename": "docs/source/ja/tasks/visual_question_answering.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fja%2Ftasks%2Fvisual_question_answering.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fja%2Ftasks%2Fvisual_question_answering.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftasks%2Fvisual_question_answering.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -368,7 +368,7 @@ GPU (åˆ©ç”¨å¯èƒ½ãªå ´åˆ)ã€‚ã“ã‚Œã¯ [`Trainer`] ãŒè‡ªå‹•çš„ã«å‡¦ç†ã™ã‚‹\n >>> import torch\n \n >>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n->>> model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n+>>> model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", dtype=torch.float16)\n >>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n >>> model.to(device)\n ```"
        },
        {
            "sha": "e550847922a239af0f6cb01e8d3652eab4adeda6",
            "filename": "docs/source/ko/cache_explanation.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fcache_explanation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fcache_explanation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fcache_explanation.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -104,7 +104,7 @@ from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache, infe\n device = f\"{infer_device()}:0\"\n \n model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n-model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=device)\n+model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16, device_map=device)\n tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n past_key_values = DynamicCache()\n@@ -150,7 +150,7 @@ from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache, infe\n device = f\"{infer_device()}:0\"\n \n model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n-model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=device)\n+model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16, device_map=device)\n tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n messages = [{\"role\": \"user\", \"content\": \"You are a helpful assistant.\"}]\n@@ -176,7 +176,7 @@ import torch\n from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", dtype=torch.float16, device_map=\"auto\")\n inputs = tokenizer(\"Hello, my name is\", return_tensors=\"pt\").to(model.device)\n \n # ìºì‹œë¥¼ ë°˜í™˜í•˜ë ¤ë©´ `return_dict_in_generate=True`ê°€ í•„ìš”í•˜ê³  `return_legacy_cache`ëŠ” ë°˜í™˜ëœ ìºì‹œë¥¼"
        },
        {
            "sha": "922b7d8856598e6e044b765aeab3e0801a5613f9",
            "filename": "docs/source/ko/chat_templating.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fchat_templating.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fchat_templating.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fchat_templating.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -283,7 +283,7 @@ from transformers import AutoModelForCausalLM, AutoTokenizer\n checkpoint = \"NousResearch/Hermes-2-Pro-Llama-3-8B\"\n \n tokenizer = AutoTokenizer.from_pretrained(checkpoint, revision=\"pr/13\")\n-model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=torch.bfloat16, device_map=\"auto\")\n+model = AutoModelForCausalLM.from_pretrained(checkpoint, dtype=torch.bfloat16, device_map=\"auto\")\n ```\n \n ë‹¤ìŒìœ¼ë¡œ, ë„êµ¬ ëª©ë¡ì„ ì •ì˜í•´ ë³´ê² ìŠµë‹ˆë‹¤:"
        },
        {
            "sha": "ee61d41dd3d7ed738df8a107444b20421b4ef362",
            "filename": "docs/source/ko/conversations.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fconversations.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fconversations.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fconversations.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -65,7 +65,7 @@ chat = [\n import torch\n from transformers import pipeline\n \n-pipe = pipeline(\"text-generation\", \"meta-llama/Meta-Llama-3-8B-Instruct\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n+pipe = pipeline(\"text-generation\", \"meta-llama/Meta-Llama-3-8B-Instruct\", dtype=torch.bfloat16, device_map=\"auto\")\n response = pipe(chat, max_new_tokens=512)\n print(response[0]['generated_text'][-1]['content'])\n ```\n@@ -188,7 +188,7 @@ chat = [\n ]\n \n # 1: ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤\n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", device_map=\"auto\", dtype=torch.bfloat16)\n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n \n # 2: ì±„íŒ… í…œí”Œë¦¿ì— ì ìš©í•©ë‹ˆë‹¤\n@@ -233,7 +233,7 @@ Hugging Face í´ëž˜ìŠ¤ëŠ” ëª¨ë¸ì„ `float32` ì •ë°€ë„(Precision)ë¡œ ë¡œë“œí•©\n í•˜ì§€ë§Œ ì´ëŠ” ë‚­ë¹„ì¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤! \n ëŒ€ë¶€ë¶„ì˜ ìµœì‹  ì–¸ì–´ ëª¨ë¸ì€ íŒŒë¼ë¯¸í„°ë‹¹ 2ë°”ì´íŠ¸ë¥¼ ì‚¬ìš©í•˜ëŠ” \"bfloat16\" ì •ë°€ë„(Precision)ë¡œ í•™ìŠµë©ë‹ˆë‹¤. \n í•˜ë“œì›¨ì–´ê°€ ì´ë¥¼ ì§€ì›í•˜ëŠ” ê²½ìš°(Nvidia 30xx/Axxx ì´ìƒ), \n-`torch_dtype` íŒŒë¼ë¯¸í„°ë¡œ ìœ„ì™€ ê°™ì´ `bfloat16` ì •ë°€ë„(Precision)ë¡œ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n+`dtype` íŒŒë¼ë¯¸í„°ë¡œ ìœ„ì™€ ê°™ì´ `bfloat16` ì •ë°€ë„(Precision)ë¡œ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n \n ë˜í•œ, 16ë¹„íŠ¸ë³´ë‹¤ ë” ë‚®ì€ ì •ë°€ë„(Precision)ë¡œ ëª¨ë¸ì„ ì••ì¶•í•˜ëŠ” \n \"ì–‘ìží™”(quantization)\" ë°©ë²•ì„ ì‚¬ìš©í•  ìˆ˜ë„ ìžˆìŠµë‹ˆë‹¤. "
        },
        {
            "sha": "69aeeccc0d19500742779e259801a9828d391a30",
            "filename": "docs/source/ko/deepspeed.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fdeepspeed.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fdeepspeed.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fdeepspeed.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -280,7 +280,7 @@ model = AutoModel.from_pretrained(\"google-t5/t5-small\")\n trainer = Trainer(model=model, args=training_args, ...)\n ```\n \n-fp16 ê°€ì¤‘ì¹˜ê°€ ë‹¨ì¼ GPUì— ë§žì§€ ì•ŠëŠ” ê²½ìš° ZeRO-3ì´ í•„ìš”í•©ë‹ˆë‹¤. fp16 ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•  ìˆ˜ ìžˆëŠ” ê²½ìš°, [`~PreTrainedModel.from_pretrained`]ì— `torch_dtype=torch.float16`ì„ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.\n+fp16 ê°€ì¤‘ì¹˜ê°€ ë‹¨ì¼ GPUì— ë§žì§€ ì•ŠëŠ” ê²½ìš° ZeRO-3ì´ í•„ìš”í•©ë‹ˆë‹¤. fp16 ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•  ìˆ˜ ìžˆëŠ” ê²½ìš°, [`~PreTrainedModel.from_pretrained`]ì— `dtype=torch.float16`ì„ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.\n \n ZeRO-3ì˜ ë˜ ë‹¤ë¥¸ ê³ ë ¤ ì‚¬í•­ì€ ì—¬ëŸ¬ ê°œì˜ GPUë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° í˜„ìž¬ ì‹¤í–‰ ì¤‘ì¸ ë ˆì´ì–´ì˜ ë§¤ê°œë³€ìˆ˜ê°€ ì•„ë‹Œ í•œ ë‹¨ì¼ GPUì— ëª¨ë“  ë§¤ê°œë³€ìˆ˜ê°€ ì—†ë‹¤ëŠ” ê²ƒìž…ë‹ˆë‹¤. ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ [`~PreTrainedModel.from_pretrained`]ì— ë¡œë“œí•˜ëŠ” ë“± ëª¨ë“  ë ˆì´ì–´ì˜ ëª¨ë“  ë§¤ê°œë³€ìˆ˜ì— í•œ ë²ˆì— ì•¡ì„¸ìŠ¤í•˜ë ¤ë©´ í•œ ë²ˆì— í•˜ë‚˜ì˜ ë ˆì´ì–´ë¥¼ ë¡œë“œí•˜ê³  ì¦‰ì‹œ ëª¨ë“  GPUì— íŒŒí‹°ì…”ë‹í•©ë‹ˆë‹¤. ì´ëŠ” ë§¤ìš° í° ëª¨ë¸ì˜ ê²½ìš° ë©”ëª¨ë¦¬ ì œí•œìœ¼ë¡œ ì¸í•´ í•˜ë‚˜ì˜ GPUì— ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•œ ë‹¤ìŒ ë‹¤ë¥¸ GPUì— ë¶„ì‚°í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ìž…ë‹ˆë‹¤.\n "
        },
        {
            "sha": "b264e5f710f618dc50b6786ce2daf0297d34acf1",
            "filename": "docs/source/ko/llm_optims.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fllm_optims.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fllm_optims.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fllm_optims.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -340,7 +340,7 @@ quant_config = BitsAndBytesConfig(load_in_8bit=True)\n model = AutoModelForCausalLM.from_pretrained(\n     \"google/gemma-2b\",\n     quantization_config=quant_config,\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     attn_implementation=\"flash_attention_2\",\n )\n ```\n@@ -360,7 +360,7 @@ from transformers import AutoModelForCausalLM\n \n model = AutoModelForCausalLM.from_pretrained(\n     \"google/gemma-2b\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n )\n \n with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n@@ -383,14 +383,14 @@ with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable\n \theight=\"450\"\n ></iframe>\n \n-Mistral-7B-v0.1ì„ ë°˜ì •ë°€ë„ë¡œ ë¡œë“œí•˜ë ¤ë©´ [`~transformers.AutoModelForCausalLM.from_pretrained`] ë©”ì„œë“œì—ì„œ `torch_dtype` ë§¤ê°œë³€ìˆ˜ë¥¼ `torch.bfloat16`ìœ¼ë¡œ ì„¤ì •í•˜ì‹­ì‹œì˜¤. ì´ ê²½ìš° 13.74GBì˜ ë©”ëª¨ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n+Mistral-7B-v0.1ì„ ë°˜ì •ë°€ë„ë¡œ ë¡œë“œí•˜ë ¤ë©´ [`~transformers.AutoModelForCausalLM.from_pretrained`] ë©”ì„œë“œì—ì„œ `dtype` ë§¤ê°œë³€ìˆ˜ë¥¼ `torch.bfloat16`ìœ¼ë¡œ ì„¤ì •í•˜ì‹­ì‹œì˜¤. ì´ ê²½ìš° 13.74GBì˜ ë©”ëª¨ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n \n ```py\n from transformers import AutoTokenizer, AutoModelForCausalLM\n import torch\n \n model = AutoModelForCausalLM.from_pretrained(\n-    \"mistralai/Mistral-7B-v0.1\", torch_dtype=torch.bfloat16, device_map=\"auto\",\n+    \"mistralai/Mistral-7B-v0.1\", dtype=torch.bfloat16, device_map=\"auto\",\n )\n ```\n "
        },
        {
            "sha": "63c9f1db45d433c67c6c8d63b1db5ea5c9ef47c2",
            "filename": "docs/source/ko/llm_tutorial_optimization.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fllm_tutorial_optimization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fllm_tutorial_optimization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fllm_tutorial_optimization.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -81,7 +81,7 @@ model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom\", device_map=\"aut\n from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n import torch\n \n-model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", torch_dtype=torch.bfloat16, device_map=\"auto\", pad_token_id=0)\n+model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", dtype=torch.bfloat16, device_map=\"auto\", pad_token_id=0)\n tokenizer = AutoTokenizer.from_pretrained(\"bigcode/octocoder\")\n \n pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n@@ -121,7 +121,7 @@ bytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n \n > ê±°ì˜ ëª¨ë“  ëª¨ë¸ì´ ìš”ì¦˜ bfloat16ìœ¼ë¡œ í•™ìŠµë˜ë¯€ë¡œ, [GPUê°€ bfloat16ì„ ì§€ì›](https://discuss.pytorch.org/t/bfloat16-native-support/117155/5)í•œë‹¤ë©´ ëª¨ë¸ì„ float32 ì •ë°€ë„ë¡œ ì‹¤í–‰í•  ì´ìœ ê°€ ì—†ìŠµë‹ˆë‹¤. float32ë¡œ ëŒë¦¬ëŠ” ëª¨ë¸ì€ í•™ìŠµí•  ë•Œ ì‚¬ìš©í–ˆë˜ ì •ë°€ë„ë³´ë‹¤ ë” ë‚˜ì€ ì¶”ë¡  ê²°ê³¼ë¥¼ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n \n-ëª¨ë¸ ê°€ì¤‘ì¹˜ê°€ ì–´ë–¤ ì •ë°€ë„ í˜•ì‹ìœ¼ë¡œ Hubì— ì €ìž¥ë˜ì–´ ìžˆëŠ”ì§€ í™•ì‹¤í•˜ì§€ ì•Šì€ ê²½ìš°, HuggingFace Hubì—ì„œ í•´ë‹¹ ì²´í¬í¬ì¸íŠ¸ configì˜ `\"torch_dtype\"`ì„ í™•ì¸í•˜ë©´ ë©ë‹ˆë‹¤, *ì˜ˆ*ë¥¼ ë“¤ì–´ [ì—¬ê¸°](https://huggingface.co/meta-llama/Llama-2-7b-hf/blob/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9/config.json#L21)ë¥¼ í™•ì¸í•˜ì„¸ìš”. ëª¨ë¸ì„ `from_pretrained(..., torch_dtype=...)`ë¡œ ë¡œë“œí•  ë•ŒëŠ” configì— ëª…ì‹œëœ ì •ë°€ë„ ìœ í˜•ê³¼ ë™ì¼í•œ ì •ë°€ë„ë¡œ ì„¤ì •í•˜ëŠ” ê²ƒì´ ê¶Œìž¥ë©ë‹ˆë‹¤. ë‹¨, ì›ëž˜ ìœ í˜•ì´ float32ì¸ ê²½ìš° ì¶”ë¡ ì„ ìœ„í•´ `float16` ë˜ëŠ” `bfloat16`ì„ ë‘˜ ë‹¤ ì‚¬ìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n+ëª¨ë¸ ê°€ì¤‘ì¹˜ê°€ ì–´ë–¤ ì •ë°€ë„ í˜•ì‹ìœ¼ë¡œ Hubì— ì €ìž¥ë˜ì–´ ìžˆëŠ”ì§€ í™•ì‹¤í•˜ì§€ ì•Šì€ ê²½ìš°, HuggingFace Hubì—ì„œ í•´ë‹¹ ì²´í¬í¬ì¸íŠ¸ configì˜ `\"dtype\"`ì„ í™•ì¸í•˜ë©´ ë©ë‹ˆë‹¤, *ì˜ˆ*ë¥¼ ë“¤ì–´ [ì—¬ê¸°](https://huggingface.co/meta-llama/Llama-2-7b-hf/blob/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9/config.json#L21)ë¥¼ í™•ì¸í•˜ì„¸ìš”. ëª¨ë¸ì„ `from_pretrained(..., dtype=...)`ë¡œ ë¡œë“œí•  ë•ŒëŠ” configì— ëª…ì‹œëœ ì •ë°€ë„ ìœ í˜•ê³¼ ë™ì¼í•œ ì •ë°€ë„ë¡œ ì„¤ì •í•˜ëŠ” ê²ƒì´ ê¶Œìž¥ë©ë‹ˆë‹¤. ë‹¨, ì›ëž˜ ìœ í˜•ì´ float32ì¸ ê²½ìš° ì¶”ë¡ ì„ ìœ„í•´ `float16` ë˜ëŠ” `bfloat16`ì„ ë‘˜ ë‹¤ ì‚¬ìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n \n ì´ì œ `flush(...)` í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ì—¬ ëª¨ë“  ë©”ëª¨ë¦¬ë¥¼ í•´ì œí•˜ê³ , GPU ë©”ëª¨ë¦¬ì˜ ìµœëŒ€ í• ë‹¹ëŸ‰ì„ ì •í™•í•˜ê²Œ ì¸¡ì •í•˜ë„ë¡ í•©ì‹œë‹¤.\n \n@@ -380,7 +380,7 @@ long_prompt = 10 * system_prompt + prompt\n ëª¨ë¸ì„ ë‹¤ì‹œ bfloat16 ì •ë°€ë„ë¡œ ì¸ìŠ¤í„´ìŠ¤í™”í•©ë‹ˆë‹¤.\n \n ```python\n-model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n+model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", dtype=torch.bfloat16, device_map=\"auto\")\n tokenizer = AutoTokenizer.from_pretrained(\"bigcode/octocoder\")\n \n pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
        },
        {
            "sha": "fa39fe9f5def3d670fc1074ecf626f846e6b1c9e",
            "filename": "docs/source/ko/main_classes/pipelines.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fmain_classes%2Fpipelines.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fmain_classes%2Fpipelines.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmain_classes%2Fpipelines.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -249,7 +249,7 @@ outputs = pipe.postprocess(all_model_outputs)\n \n ëª¨ë¸ì€ FP16 ëª¨ë“œë¡œ ì‹¤í–‰í•  ìˆ˜ ìžˆìœ¼ë©°, GPUì—ì„œ ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•˜ë©´ì„œ ì²˜ë¦¬ ì†ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ì€ ì„±ëŠ¥ ì €í•˜ ì—†ì´ FP16ì„ ì§€ì›í•˜ë©°, ëª¨ë¸ì´ í´ìˆ˜ë¡ ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥ì„±ì€ ë” ë‚®ì•„ì§‘ë‹ˆë‹¤.\n \n-FP16 ì¶”ë¡ ì„ í™œì„±í™”í•˜ë ¤ë©´ íŒŒì´í”„ë¼ì¸ ìƒì„±ìžì— `torch_dtype=torch.float16` ë˜ëŠ” `torch_dtype='float16'`ì„ ì „ë‹¬í•˜ì„¸ìš”. ì´ ê¸°ëŠ¥ì€ íŒŒì´í† ì¹˜ ë°±ì—”ë“œë¥¼ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì—ì„œë§Œ ìž‘ë™í•˜ë©°, ìž…ë ¥ì€ ë‚´ë¶€ì ìœ¼ë¡œ FP16 í˜•ì‹ìœ¼ë¡œ ë³€í™˜ë©ë‹ˆë‹¤.\n+FP16 ì¶”ë¡ ì„ í™œì„±í™”í•˜ë ¤ë©´ íŒŒì´í”„ë¼ì¸ ìƒì„±ìžì— `dtype=torch.float16` ë˜ëŠ” `dtype='float16'`ì„ ì „ë‹¬í•˜ì„¸ìš”. ì´ ê¸°ëŠ¥ì€ íŒŒì´í† ì¹˜ ë°±ì—”ë“œë¥¼ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì—ì„œë§Œ ìž‘ë™í•˜ë©°, ìž…ë ¥ì€ ë‚´ë¶€ì ìœ¼ë¡œ FP16 í˜•ì‹ìœ¼ë¡œ ë³€í™˜ë©ë‹ˆë‹¤.\n \n ## íŒŒì´í”„ë¼ì¸ ì‚¬ìš©ìž ì •ì˜ ì½”ë“œ [[pipeline-custom-code]]\n "
        },
        {
            "sha": "7e2bc2f463288b1b5ae84de7272a13674a5b8e97",
            "filename": "docs/source/ko/model_doc/albert.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fmodel_doc%2Falbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fmodel_doc%2Falbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Falbert.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -51,7 +51,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"fill-mask\",\n     model=\"albert-base-v2\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n pipeline(\"ì‹ë¬¼ì€ ê´‘í•©ì„±ì´ë¼ê³  ì•Œë ¤ì§„ ê³¼ì •ì„ í†µí•´ [MASK]ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\", top_k=5)\n@@ -67,7 +67,7 @@ from transformers import AutoModelForMaskedLM, AutoTokenizer\n tokenizer = AutoTokenizer.from_pretrained(\"albert/albert-base-v2\")\n model = AutoModelForMaskedLM.from_pretrained(\n     \"albert/albert-base-v2\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     attn_implementation=\"sdpa\",\n     device_map=\"auto\"\n )"
        },
        {
            "sha": "28aee583c0cb1179ad4f92450fb6645acc97e3ce",
            "filename": "docs/source/ko/model_doc/bert.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fmodel_doc%2Fbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fmodel_doc%2Fbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fbert.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -58,7 +58,7 @@ PytorchëŠ” `torch.nn.functional`ì˜ ì¼ë¶€ë¡œ Scaled Dot Product Attention(SDPA)\n ```\n from transformers import BertModel\n \n-model = BertModel.from_pretrained(\"bert-base-uncased\", torch_dtype=torch.float16, attn_implementation=\"sdpa\")\n+model = BertModel.from_pretrained(\"bert-base-uncased\", dtype=torch.float16, attn_implementation=\"sdpa\")\n ...\n ```\n "
        },
        {
            "sha": "2de52be398eda6972c9bf04ae4bcde4f5760fdfb",
            "filename": "docs/source/ko/model_doc/biogpt.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fmodel_doc%2Fbiogpt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fmodel_doc%2Fbiogpt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fbiogpt.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -40,7 +40,7 @@ PyTorchëŠ” `torch.nn.functional`ì˜ ì¼ë¶€ë¡œ ìŠ¤ì¼€ì¼ëœ ì ê³± ì–´í…ì…˜(SDPA\n \n ```\n from transformers import BioGptForCausalLM\n-model = BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\", attn_implementation=\"sdpa\", torch_dtype=torch.float16)\n+model = BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\", attn_implementation=\"sdpa\", dtype=torch.float16)\n ```\n \n NVIDIA GeForce RTX 2060-8GB, PyTorch 2.3.1, Ubuntu 20.04 í™˜ê²½ì—ì„œ `float16` ë° CausalLM í—¤ë“œê°€ ìžˆëŠ” `microsoft/biogpt` ëª¨ë¸ë¡œ ë¡œì»¬ ë²¤ì¹˜ë§ˆí¬ë¥¼ ìˆ˜í–‰í•œ ê²°ê³¼, í›ˆë ¨ ì¤‘ ë‹¤ìŒê³¼ ê°™ì€ ì†ë„ í–¥ìƒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤."
        },
        {
            "sha": "0c4eca628db764e8a10abca08a435913d0e641ae",
            "filename": "docs/source/ko/model_doc/chameleon.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fmodel_doc%2Fchameleon.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fmodel_doc%2Fchameleon.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fchameleon.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -55,7 +55,7 @@ from PIL import Image\n import requests\n \n processor = ChameleonProcessor.from_pretrained(\"facebook/chameleon-7b\")\n-model = ChameleonForConditionalGeneration.from_pretrained(\"facebook/chameleon-7b\", torch_dtype=torch.bfloat16, device_map=\"cuda\")\n+model = ChameleonForConditionalGeneration.from_pretrained(\"facebook/chameleon-7b\", dtype=torch.bfloat16, device_map=\"cuda\")\n \n # ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„\n url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n@@ -81,7 +81,7 @@ import requests\n \n processor = ChameleonProcessor.from_pretrained(\"facebook/chameleon-7b\")\n \n-model = ChameleonForConditionalGeneration.from_pretrained(\"facebook/chameleon-7b\", torch_dtype=torch.bfloat16, device_map=\"cuda\")\n+model = ChameleonForConditionalGeneration.from_pretrained(\"facebook/chameleon-7b\", dtype=torch.bfloat16, device_map=\"cuda\")\n \n # ì„¸ ê°€ì§€ ë‹¤ë¥¸ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°\n url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n@@ -147,7 +147,7 @@ from transformers import ChameleonForConditionalGeneration\n model_id = \"facebook/chameleon-7b\"\n model = ChameleonForConditionalGeneration.from_pretrained(\n     model_id,\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     attn_implementation=\"flash_attention_2\"\n ).to(0)\n ```"
        },
        {
            "sha": "d5af5a24f3973e7bea391ae609a6ff1b7655609b",
            "filename": "docs/source/ko/model_doc/clip.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fmodel_doc%2Fclip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fmodel_doc%2Fclip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fclip.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -85,13 +85,13 @@ pip install -U flash-attn --no-build-isolation\n >>> from transformers import CLIPProcessor, CLIPModel\n \n >>> device = \"cuda\"\n->>> torch_dtype = torch.float16\n+>>> dtype = torch.float16\n \n >>> model = CLIPModel.from_pretrained(\n ...     \"openai/clip-vit-base-patch32\",\n ...     attn_implementation=\"flash_attention_2\",\n ...     device_map=device,\n-...     torch_dtype=torch_dtype,\n+...     dtype=dtype,\n ... )\n >>> processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n \n@@ -121,7 +121,7 @@ tensor([[0.9946, 0.0052]], device='cuda:0', dtype=torch.float16)\n ```python\n from transformers import CLIPModel\n \n-model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", torch_dtype=torch.float16, attn_implementation=\"sdpa\")\n+model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", dtype=torch.float16, attn_implementation=\"sdpa\")\n ```\n \n ìµœê³ ì˜ ì†ë„í–¥ìƒì„ ìœ„í•´ì„œ, ë°˜ì •ë°€ë„ë¡œ ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ê²ƒì„ ì¶”ì²œí•©ë‹ˆë‹¤. (ì˜ˆë¥¼ë“¤ë©´ `torch.float16` ë˜ëŠ” `torch.bfloat16`)."
        },
        {
            "sha": "b53738ded8606715c873f849b17c6c6fef54685b",
            "filename": "docs/source/ko/model_doc/cohere.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fmodel_doc%2Fcohere.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fmodel_doc%2Fcohere.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fcohere.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -22,10 +22,10 @@ The Cohere Command-R ëª¨ë¸ì€ CohereíŒ€ì´ [Command-R: í”„ë¡œë•ì…˜ ê·œëª¨ì˜ \n \n <Tip warning={true}>\n \n-Hubì— ì—…ë¡œë“œëœ ì²´í¬í¬ì¸íŠ¸ë“¤ì€ `torch_dtype = 'float16'`ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. \n+Hubì— ì—…ë¡œë“œëœ ì²´í¬í¬ì¸íŠ¸ë“¤ì€ `dtype = 'float16'`ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. \n ì´ëŠ” `AutoModel` APIê°€ ì²´í¬í¬ì¸íŠ¸ë¥¼ `torch.float32`ì—ì„œ `torch.float16`ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. \n \n-ì˜¨ë¼ì¸ ê°€ì¤‘ì¹˜ì˜ `dtype`ì€ `model = AutoModelForCausalLM.from_pretrained(\"path\", torch_dtype = \"auto\")`ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì´ˆê¸°í™”í•  ë•Œ `torch_dtype=\"auto\"`ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” í•œ ëŒ€ë¶€ë¶„ ë¬´ê´€í•©ë‹ˆë‹¤. ê·¸ ì´ìœ ëŠ” ëª¨ë¸ì´ ë¨¼ì € ë‹¤ìš´ë¡œë“œë˜ê³ (ì˜¨ë¼ì¸ ì²´í¬í¬ì¸íŠ¸ì˜ `dtype` ì‚¬ìš©), ê·¸ ë‹¤ìŒ `torch`ì˜ ê¸°ë³¸ `dtype`ìœ¼ë¡œ ë³€í™˜ë˜ë©°(ì´ë•Œ `torch.float32`ê°€ ë¨), ë§ˆì§€ë§‰ìœ¼ë¡œ configì— `torch_dtype`ì´ ì œê³µëœ ê²½ìš° ì´ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ìž…ë‹ˆë‹¤.\n+ì˜¨ë¼ì¸ ê°€ì¤‘ì¹˜ì˜ `dtype`ì€ `model = AutoModelForCausalLM.from_pretrained(\"path\", dtype = \"auto\")`ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì´ˆê¸°í™”í•  ë•Œ `dtype=\"auto\"`ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” í•œ ëŒ€ë¶€ë¶„ ë¬´ê´€í•©ë‹ˆë‹¤. ê·¸ ì´ìœ ëŠ” ëª¨ë¸ì´ ë¨¼ì € ë‹¤ìš´ë¡œë“œë˜ê³ (ì˜¨ë¼ì¸ ì²´í¬í¬ì¸íŠ¸ì˜ `dtype` ì‚¬ìš©), ê·¸ ë‹¤ìŒ `torch`ì˜ ê¸°ë³¸ `dtype`ìœ¼ë¡œ ë³€í™˜ë˜ë©°(ì´ë•Œ `torch.float32`ê°€ ë¨), ë§ˆì§€ë§‰ìœ¼ë¡œ configì— `dtype`ì´ ì œê³µëœ ê²½ìš° ì´ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ìž…ë‹ˆë‹¤.\n \n ëª¨ë¸ì„ `float16`ìœ¼ë¡œ í›ˆë ¨í•˜ëŠ” ê²ƒì€ ê¶Œìž¥ë˜ì§€ ì•Šìœ¼ë©° `nan`ì„ ìƒì„±í•˜ëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ ìžˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ëª¨ë¸ì€ `bfloat16`ìœ¼ë¡œ í›ˆë ¨í•´ì•¼ í•©ë‹ˆë‹¤.\n </Tip>\n@@ -55,7 +55,7 @@ gen_text = tokenizer.decode(gen_tokens[0])\n print(gen_text)\n ```\n \n-- Flash Attention 2ë¥¼ `attn_implementation=\"flash_attention_2\"`ë¥¼ í†µí•´ ì‚¬ìš©í•  ë•ŒëŠ”, `from_pretrained` í´ëž˜ìŠ¤ ë©”ì„œë“œì— `torch_dtype`ì„ ì „ë‹¬í•˜ì§€ ë§ê³  ìžë™ í˜¼í•© ì •ë°€ë„ í›ˆë ¨(Automatic Mixed-Precision training)ì„ ì‚¬ìš©í•˜ì„¸ìš”. `Trainer`ë¥¼ ì‚¬ìš©í•  ë•ŒëŠ” ë‹¨ìˆœížˆ `fp16` ë˜ëŠ” `bf16`ì„ `True`ë¡œ ì§€ì •í•˜ë©´ ë©ë‹ˆë‹¤. ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš°ì—ëŠ” `torch.autocast`ë¥¼ ì‚¬ìš©í•˜ê³  ìžˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. ì´ëŠ” Flash Attentionì´ `fp16`ì™€ `bf16` ë°ì´í„° íƒ€ìž…ë§Œ ì§€ì›í•˜ê¸° ë•Œë¬¸ì— í•„ìš”í•©ë‹ˆë‹¤.\n+- Flash Attention 2ë¥¼ `attn_implementation=\"flash_attention_2\"`ë¥¼ í†µí•´ ì‚¬ìš©í•  ë•ŒëŠ”, `from_pretrained` í´ëž˜ìŠ¤ ë©”ì„œë“œì— `dtype`ì„ ì „ë‹¬í•˜ì§€ ë§ê³  ìžë™ í˜¼í•© ì •ë°€ë„ í›ˆë ¨(Automatic Mixed-Precision training)ì„ ì‚¬ìš©í•˜ì„¸ìš”. `Trainer`ë¥¼ ì‚¬ìš©í•  ë•ŒëŠ” ë‹¨ìˆœížˆ `fp16` ë˜ëŠ” `bf16`ì„ `True`ë¡œ ì§€ì •í•˜ë©´ ë©ë‹ˆë‹¤. ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš°ì—ëŠ” `torch.autocast`ë¥¼ ì‚¬ìš©í•˜ê³  ìžˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. ì´ëŠ” Flash Attentionì´ `fp16`ì™€ `bf16` ë°ì´í„° íƒ€ìž…ë§Œ ì§€ì›í•˜ê¸° ë•Œë¬¸ì— í•„ìš”í•©ë‹ˆë‹¤.\n \n ## ë¦¬ì†ŒìŠ¤[[resources]]\n "
        },
        {
            "sha": "85fb4b4f5591ed8765986a4a217c16e286b2d102",
            "filename": "docs/source/ko/model_doc/dbrx.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fmodel_doc%2Fdbrx.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fmodel_doc%2Fdbrx.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fdbrx.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -47,7 +47,7 @@ tokenizer = AutoTokenizer.from_pretrained(\"databricks/dbrx-instruct\", token=\"YOU\n model = DbrxForCausalLM.from_pretrained(\n     \"databricks/dbrx-instruct\",\n     device_map=\"auto\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     token=\"YOUR_HF_TOKEN\",\n     )\n \n@@ -70,7 +70,7 @@ tokenizer = AutoTokenizer.from_pretrained(\"databricks/dbrx-instruct\", token=\"YOU\n model = DbrxForCausalLM.from_pretrained(\n     \"databricks/dbrx-instruct\",\n     device_map=\"auto\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     token=\"YOUR_HF_TOKEN\",\n     attn_implementation=\"flash_attention_2\",\n     )\n@@ -94,7 +94,7 @@ tokenizer = AutoTokenizer.from_pretrained(\"databricks/dbrx-instruct\", token=\"YOU\n model = DbrxForCausalLM.from_pretrained(\n     \"databricks/dbrx-instruct\",\n     device_map=\"auto\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     token=\"YOUR_HF_TOKEN\",\n     attn_implementation=\"sdpa\",\n     )"
        },
        {
            "sha": "9516c868100ef3730a547902899c2a3365514fa5",
            "filename": "docs/source/ko/model_doc/exaone4.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fmodel_doc%2Fexaone4.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fmodel_doc%2Fexaone4.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fexaone4.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -67,7 +67,7 @@ model_name = \"LGAI-EXAONE/EXAONE-4.0-32B\"\n \n model = AutoModelForCausalLM.from_pretrained(\n     model_name,\n-    torch_dtype=\"bfloat16\",\n+    dtype=\"bfloat16\",\n     device_map=\"auto\"\n )\n tokenizer = AutoTokenizer.from_pretrained(model_name)"
        },
        {
            "sha": "2eecd01671258da8c926d2d1d4a7c1fce48104c7",
            "filename": "docs/source/ko/model_doc/gemma3.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fmodel_doc%2Fgemma3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fmodel_doc%2Fgemma3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fgemma3.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -46,7 +46,7 @@ pipeline = pipeline(\n     task=\"image-text-to-text\",\n     model=\"google/gemma-3-4b-pt\",\n     device=0,\n-    torch_dtype=torch.bfloat16\n+    dtype=torch.bfloat16\n )\n pipeline(\n     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n@@ -63,7 +63,7 @@ from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n \n model = Gemma3ForConditionalGeneration.from_pretrained(\n     \"google/gemma-3-4b-it\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )\n@@ -120,7 +120,7 @@ from transformers import TorchAoConfig, Gemma3ForConditionalGeneration, AutoProc\n quantization_config = TorchAoConfig(\"int4_weight_only\", group_size=128)\n model = Gemma3ForConditionalGeneration.from_pretrained(\n     \"google/gemma-3-27b-it\",\n-    torch_dtype=torch.bfloat16,\n+    dtype=torch.bfloat16,\n     device_map=\"auto\",\n     quantization_config=quantization_config\n )\n@@ -219,7 +219,7 @@ visualizer(\"<img>What is shown in this image?\")\n     )\n     model = AutoModelForCausalLM.from_pretrained(\n         \"google/gemma-3-1b-pt\",\n-        torch_dtype=torch.bfloat16,\n+        dtype=torch.bfloat16,\n         device_map=\"auto\",\n         attn_implementation=\"sdpa\"\n     )"
        },
        {
            "sha": "53596bb9824dad152eb0929cea3cd981c616a29c",
            "filename": "docs/source/ko/model_doc/gpt2.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fmodel_doc%2Fgpt2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fmodel_doc%2Fgpt2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fgpt2.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -45,7 +45,7 @@ import torch\n from transformers import pipeline\n \n # í…ìŠ¤íŠ¸ ìƒì„±ì„ ìœ„í•œ íŒŒì´í”„ë¼ì¸ ìƒì„±\n-pipeline = pipeline(task=\"text-generation\", model=\"openai-community/gpt2\", torch_dtype=torch.float16, device=0)\n+pipeline = pipeline(task=\"text-generation\", model=\"openai-community/gpt2\", dtype=torch.float16, device=0)\n pipeline(\"Hello, I'm a language model\")\n ```\n </hfoption>\n@@ -56,7 +56,7 @@ import torch\n from transformers import AutoModelForCausalLM, AutoTokenizer\n \n # ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ\n-model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\", torch_dtype=torch.float16, device_map=\"auto\", attn_implementation=\"sdpa\")\n+model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\", dtype=torch.float16, device_map=\"auto\", attn_implementation=\"sdpa\")\n tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n \n # ìž…ë ¥ í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê³  GPUë¡œ ì´ë™"
        },
        {
            "sha": "8c8fcbe5c6dd72bc981f7ee823d57d939d44cf2d",
            "filename": "docs/source/ko/model_doc/jamba.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fmodel_doc%2Fjamba.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fmodel_doc%2Fjamba.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fjamba.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -47,7 +47,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"text-generation\",\n     model=\"ai21labs/AI21-Jamba-Mini-1.6\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n pipeline(\"Plants create energy through a process known as\")\n@@ -65,7 +65,7 @@ tokenizer = AutoTokenizer.from_pretrained(\n )\n model = AutoModelForCausalLM.from_pretrained(\n     \"ai21labs/AI21-Jamba-Large-1.6\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )\n@@ -98,7 +98,7 @@ quantization_config = BitsAndBytesConfig(load_in_8bit=True,\n # ëª¨ë¸ì„ 8ê°œì˜ GPUì— ê³ ë¥´ê²Œ ë¶„ì‚°ì‹œí‚¤ê¸° ìœ„í•œ ë””ë°”ì´ìŠ¤ ë§µ\n device_map = {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 2, 'model.layers.26': 2, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.layers.32': 3, 'model.layers.33': 3, 'model.layers.34': 3, 'model.layers.35': 3, 'model.layers.36': 4, 'model.layers.37': 4, 'model.layers.38': 4, 'model.layers.39': 4, 'model.layers.40': 4, 'model.layers.41': 4, 'model.layers.42': 4, 'model.layers.43': 4, 'model.layers.44': 4, 'model.layers.45': 5, 'model.layers.46': 5, 'model.layers.47': 5, 'model.layers.48': 5, 'model.layers.49': 5, 'model.layers.50': 5, 'model.layers.51': 5, 'model.layers.52': 5, 'model.layers.53': 5, 'model.layers.54': 6, 'model.layers.55': 6, 'model.layers.56': 6, 'model.layers.57': 6, 'model.layers.58': 6, 'model.layers.59': 6, 'model.layers.60': 6, 'model.layers.61': 6, 'model.layers.62': 6, 'model.layers.63': 7, 'model.layers.64': 7, 'model.layers.65': 7, 'model.layers.66': 7, 'model.layers.67': 7, 'model.layers.68': 7, 'model.layers.69': 7, 'model.layers.70': 7, 'model.layers.71': 7, 'model.final_layernorm': 7, 'lm_head': 7}\n model = AutoModelForCausalLM.from_pretrained(\"ai21labs/AI21-Jamba-Large-1.6\",\n-                                             torch_dtype=torch.bfloat16,\n+                                             dtype=torch.bfloat16,\n                     attn_implementation=\"flash_attention_2\",\n                                              quantization_config=quantization_config,\n                                              device_map=device_map)"
        },
        {
            "sha": "6fd74861be6d11670c70428d3a02994299f965fa",
            "filename": "docs/source/ko/model_doc/llama2.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fmodel_doc%2Fllama2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fmodel_doc%2Fllama2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fllama2.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -28,9 +28,9 @@ Llama2 ëª¨ë¸ì€ Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Al\n \n <Tip warning={true}>\n \n-`Llama2` ëª¨ë¸ì€ `bfloat16`ì„ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ë˜ì—ˆì§€ë§Œ, ì›ëž˜ ì¶”ë¡ ì€ `float16`ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. í—ˆë¸Œì— ì—…ë¡œë“œëœ ì²´í¬í¬ì¸íŠ¸ëŠ” `torch_dtype = 'float16'`ì„ ì‚¬ìš©í•˜ë©°, ì´ëŠ” `AutoModel` APIì— ì˜í•´ ì²´í¬í¬ì¸íŠ¸ë¥¼ `torch.float32`ì—ì„œ `torch.float16`ìœ¼ë¡œ ìºìŠ¤íŒ…í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. \n+`Llama2` ëª¨ë¸ì€ `bfloat16`ì„ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ë˜ì—ˆì§€ë§Œ, ì›ëž˜ ì¶”ë¡ ì€ `float16`ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. í—ˆë¸Œì— ì—…ë¡œë“œëœ ì²´í¬í¬ì¸íŠ¸ëŠ” `dtype = 'float16'`ì„ ì‚¬ìš©í•˜ë©°, ì´ëŠ” `AutoModel` APIì— ì˜í•´ ì²´í¬í¬ì¸íŠ¸ë¥¼ `torch.float32`ì—ì„œ `torch.float16`ìœ¼ë¡œ ìºìŠ¤íŒ…í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. \n \n-ì˜¨ë¼ì¸ ê°€ì¤‘ì¹˜ì˜ `dtype`ì€ `model = AutoModelForCausalLM.from_pretrained(\"path\", torch_dtype = \"auto\")`ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì´ˆê¸°í™”í•  ë•Œ `torch_dtype=\"auto\"`ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” í•œ ëŒ€ë¶€ë¶„ ê´€ë ¨ì´ ì—†ìŠµë‹ˆë‹¤. ê·¸ ì´ìœ ëŠ” ëª¨ë¸ì´ ë¨¼ì € ë‹¤ìš´ë¡œë“œë  ê²ƒì´ê³  (ì˜¨ë¼ì¸ ì²´í¬í¬ì¸íŠ¸ì˜ `dtype`ì„ ì‚¬ìš©í•˜ì—¬) ê·¸ë‹¤ìŒì— ê¸°ë³¸ `dtype`ì¸ `torch`ë¡œ ìºìŠ¤íŒ…í•˜ê³ (`torch.float32`ê°€ ë¨), ë§ˆì§€ë§‰ìœ¼ë¡œ êµ¬ì„±(configuration)ì—ì„œ ì œê³µëœ `torch_dtype`ì´ ìžˆëŠ” ê²½ìš° ì´ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ìž…ë‹ˆë‹¤.\n+ì˜¨ë¼ì¸ ê°€ì¤‘ì¹˜ì˜ `dtype`ì€ `model = AutoModelForCausalLM.from_pretrained(\"path\", dtype = \"auto\")`ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì´ˆê¸°í™”í•  ë•Œ `dtype=\"auto\"`ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” í•œ ëŒ€ë¶€ë¶„ ê´€ë ¨ì´ ì—†ìŠµë‹ˆë‹¤. ê·¸ ì´ìœ ëŠ” ëª¨ë¸ì´ ë¨¼ì € ë‹¤ìš´ë¡œë“œë  ê²ƒì´ê³  (ì˜¨ë¼ì¸ ì²´í¬í¬ì¸íŠ¸ì˜ `dtype`ì„ ì‚¬ìš©í•˜ì—¬) ê·¸ë‹¤ìŒì— ê¸°ë³¸ `dtype`ì¸ `torch`ë¡œ ìºìŠ¤íŒ…í•˜ê³ (`torch.float32`ê°€ ë¨), ë§ˆì§€ë§‰ìœ¼ë¡œ êµ¬ì„±(configuration)ì—ì„œ ì œê³µëœ `dtype`ì´ ìžˆëŠ” ê²½ìš° ì´ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ìž…ë‹ˆë‹¤.\n \n ëª¨ë¸ì„ `float16`ì—ì„œ í›ˆë ¨í•˜ëŠ” ê²ƒì€ ê¶Œìž¥ë˜ì§€ ì•Šìœ¼ë©° `nan`ì„ ìƒì„±í•˜ëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ ìžˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ëª¨ë¸ì€ `bfloat16`ì—ì„œ í›ˆë ¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n "
        },
        {
            "sha": "8cbd9cde9b66cc1cf5d064ca87b5de945f3930f7",
            "filename": "docs/source/ko/model_doc/llama3.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fmodel_doc%2Fllama3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fmodel_doc%2Fllama3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fllama3.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -22,7 +22,7 @@ import torch\n \n model_id = \"meta-llama/Meta-Llama-3-8B\"\n \n-pipeline = transformers.pipeline(\"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\")\n+pipeline = transformers.pipeline(\"text-generation\", model=model_id, model_kwargs={\"dtype\": torch.bfloat16}, device_map=\"auto\")\n pipeline(\"Hey how are you doing today?\")\n ```\n \n@@ -41,9 +41,9 @@ pipeline(\"Hey how are you doing today?\")\n \n <Tip warning={true}>\n \n-`ë¼ë§ˆ3` ëª¨ë¸ë“¤ì€ `bfloat16`ë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ë˜ì—ˆì§€ë§Œ, ì›ëž˜ì˜ ì¶”ë¡ ì€ `float16`ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. Hubì— ì—…ë¡œë“œëœ ì²´í¬í¬ì¸íŠ¸ë“¤ì€ `torch_dtype = 'float16'`ì„ ì‚¬ìš©í•˜ëŠ”ë°, ì´ëŠ” `AutoModel` APIê°€ ì²´í¬í¬ì¸íŠ¸ë¥¼ `torch.float32`ì—ì„œ `torch.float16`ìœ¼ë¡œ ë³€í™˜í•˜ëŠ”ë° ì´ìš©ë©ë‹ˆë‹¤. \n+`ë¼ë§ˆ3` ëª¨ë¸ë“¤ì€ `bfloat16`ë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ë˜ì—ˆì§€ë§Œ, ì›ëž˜ì˜ ì¶”ë¡ ì€ `float16`ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. Hubì— ì—…ë¡œë“œëœ ì²´í¬í¬ì¸íŠ¸ë“¤ì€ `dtype = 'float16'`ì„ ì‚¬ìš©í•˜ëŠ”ë°, ì´ëŠ” `AutoModel` APIê°€ ì²´í¬í¬ì¸íŠ¸ë¥¼ `torch.float32`ì—ì„œ `torch.float16`ìœ¼ë¡œ ë³€í™˜í•˜ëŠ”ë° ì´ìš©ë©ë‹ˆë‹¤. \n \n- `model = AutoModelForCausalLM.from_pretrained(\"path\", torch_dtype = \"auto\")`ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì´ˆê¸°í™”í•  ë•Œ, ì˜¨ë¼ì¸ ê°€ì¤‘ì¹˜ì˜ `dtype`ëŠ” `torch_dtype=\"auto\"`ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” í•œ ëŒ€ë¶€ë¶„ ë¬´ê´€í•©ë‹ˆë‹¤. ê·¸ ì´ìœ ëŠ” ëª¨ë¸ì´ ë¨¼ì € ë‹¤ìš´ë¡œë“œë˜ê³ (ì˜¨ë¼ì¸ ì²´í¬í¬ì¸íŠ¸ì˜ `dtype`ë¥¼ ì‚¬ìš©), ê·¸ ë‹¤ìŒ `torch`ì˜ `dtype`ìœ¼ë¡œ ë³€í™˜ë˜ì–´(`torch.float32`ê°€ ë¨), ë§ˆì§€ë§‰ìœ¼ë¡œ configì— `torch_dtype`ì´ ì œê³µëœ ê²½ìš° ê°€ì¤‘ì¹˜ê°€ ì‚¬ìš©ë˜ê¸° ë•Œë¬¸ìž…ë‹ˆë‹¤.\n+ `model = AutoModelForCausalLM.from_pretrained(\"path\", dtype = \"auto\")`ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì´ˆê¸°í™”í•  ë•Œ, ì˜¨ë¼ì¸ ê°€ì¤‘ì¹˜ì˜ `dtype`ëŠ” `dtype=\"auto\"`ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” í•œ ëŒ€ë¶€ë¶„ ë¬´ê´€í•©ë‹ˆë‹¤. ê·¸ ì´ìœ ëŠ” ëª¨ë¸ì´ ë¨¼ì € ë‹¤ìš´ë¡œë“œë˜ê³ (ì˜¨ë¼ì¸ ì²´í¬í¬ì¸íŠ¸ì˜ `dtype`ë¥¼ ì‚¬ìš©), ê·¸ ë‹¤ìŒ `torch`ì˜ `dtype`ìœ¼ë¡œ ë³€í™˜ë˜ì–´(`torch.float32`ê°€ ë¨), ë§ˆì§€ë§‰ìœ¼ë¡œ configì— `dtype`ì´ ì œê³µëœ ê²½ìš° ê°€ì¤‘ì¹˜ê°€ ì‚¬ìš©ë˜ê¸° ë•Œë¬¸ìž…ë‹ˆë‹¤.\n \n `float16`ìœ¼ë¡œ ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” ê²ƒì€ ê¶Œìž¥ë˜ì§€ ì•Šìœ¼ë©° `nan`ì„ ìƒì„±í•˜ëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ ìžˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ëª¨ë“  ëª¨ë¸ì€ `bfloat16`ìœ¼ë¡œ í›ˆë ¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n \n@@ -73,7 +73,7 @@ pipeline(\"Hey how are you doing today?\")\n \n     ì´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰ì‹œí‚¤ë ¤ë©´ ëª¨ë¸ ì „ì²´ë¥¼ float16 ì •ë°€ë„ë¡œ í˜¸ìŠ¤íŒ…í•  ìˆ˜ ìžˆëŠ” ì¶©ë¶„í•œ ë©”ì¸ë©”ëª¨ë¦¬ê°€ í•„ìš”í•˜ë‹¤ëŠ” ì ì„ ìœ ì˜í•˜ì„¸ìš”. ê°€ìž¥ í° ë²„ì „ì´ ì—¬ëŸ¬ ì²´í¬í¬ì¸íŠ¸ë¡œ ë‚˜ë‰˜ì–´ ìžˆë”ë¼ë„, ê° ì²´í¬í¬ì¸íŠ¸ê°€ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ ì¼ë¶€ë¥¼ í¬í•¨í•˜ê³  ìžˆê¸° ë•Œë¬¸ì— ì´ë¥¼ ëª¨ë‘ RAMì— ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤. 75B ëª¨ë¸ì„ ì˜ˆë¡œ ë“¤ë©´ ëŒ€ëžµ 145GBì˜ RAMì´ í•„ìš”í•©ë‹ˆë‹¤. \n \n-- `attn_implementation=\"flash_attention_2\"`ë¥¼ í†µí•´ì„œ í”Œëž˜ì‹œ ì–´í…ì…˜2ë¥¼ ì‚¬ìš©í•  ë•Œ, `from_pretrained` í´ëž˜ìŠ¤ ë©”ì„œë“œì— `torch_dtype`ë¥¼ ì „ë‹¬í•˜ì§€ ë§ê³  ìžë™ í˜¼í•© ì •ë°€ë„(Automatic Mixed-Precision) í•™ìŠµì„ ì‚¬ìš©í•˜ì„¸ìš”. `Trainer`ë¥¼ ì‚¬ìš©í•  ë•ŒëŠ” ë‹¨ìˆœížˆ `fp16` ë˜ëŠ” `bf16`ì„ `True`ë¡œ ì„¤ì •í•˜ë©´ ë©ë‹ˆë‹¤. ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ë°˜ë“œì‹œ `torch.autocast`ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. í”Œëž˜ì‹œ ì–´í…ì…˜ì€ `fp16`ê³¼ `bf16` ë°ì´í„° ìœ í˜•ë§Œ ì§€ì›í•˜ê¸° ë•Œë¬¸ìž…ë‹ˆë‹¤.\n+- `attn_implementation=\"flash_attention_2\"`ë¥¼ í†µí•´ì„œ í”Œëž˜ì‹œ ì–´í…ì…˜2ë¥¼ ì‚¬ìš©í•  ë•Œ, `from_pretrained` í´ëž˜ìŠ¤ ë©”ì„œë“œì— `dtype`ë¥¼ ì „ë‹¬í•˜ì§€ ë§ê³  ìžë™ í˜¼í•© ì •ë°€ë„(Automatic Mixed-Precision) í•™ìŠµì„ ì‚¬ìš©í•˜ì„¸ìš”. `Trainer`ë¥¼ ì‚¬ìš©í•  ë•ŒëŠ” ë‹¨ìˆœížˆ `fp16` ë˜ëŠ” `bf16`ì„ `True`ë¡œ ì„¤ì •í•˜ë©´ ë©ë‹ˆë‹¤. ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ë°˜ë“œì‹œ `torch.autocast`ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. í”Œëž˜ì‹œ ì–´í…ì…˜ì€ `fp16`ê³¼ `bf16` ë°ì´í„° ìœ í˜•ë§Œ ì§€ì›í•˜ê¸° ë•Œë¬¸ìž…ë‹ˆë‹¤.\n \n ## ìžë£Œ[[resources]]\n "
        },
        {
            "sha": "a05f00a4233fd1899d3e70d9af518cf8d52b649e",
            "filename": "docs/source/ko/model_doc/mistral.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fmodel_doc%2Fmistral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fmodel_doc%2Fmistral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fmistral.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -107,7 +107,7 @@ pip install -U flash-attn --no-build-isolation\n >>> import torch\n >>> from transformers import AutoModelForCausalLM, AutoTokenizer\n \n->>> model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\", device_map=\"auto\")\n+>>> model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", dtype=torch.float16, attn_implementation=\"flash_attention_2\", device_map=\"auto\")\n >>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n \n >>> prompt = \"My favourite condiment is\""
        },
        {
            "sha": "1e2b83871f4afe4d0f364b6b86098112ccaa7e3d",
            "filename": "docs/source/ko/model_doc/qwen2_vl.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fmodel_doc%2Fqwen2_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fmodel_doc%2Fqwen2_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fqwen2_vl.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -269,7 +269,7 @@ from transformers import Qwen2VLForConditionalGeneration\n \n model = Qwen2VLForConditionalGeneration.from_pretrained(\n     \"Qwen/Qwen2-VL-7B-Instruct\", \n-    torch_dtype=torch.bfloat16, \n+    dtype=torch.bfloat16, \n     attn_implementation=\"flash_attention_2\",\n )\n ```"
        },
        {
            "sha": "c25823848cd59222b37fe9ce1d44e7b23b25bc15",
            "filename": "docs/source/ko/model_doc/siglip.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fmodel_doc%2Fsiglip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fmodel_doc%2Fsiglip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fsiglip.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -136,7 +136,7 @@ Flash Attention 2ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ì‹¤í–‰í•˜ë ¤ë©´ ì•„ëž˜\n >>> model = SiglipModel.from_pretrained(\n ...     \"google/siglip-so400m-patch14-384\",\n ...     attn_implementation=\"flash_attention_2\",\n-...     torch_dtype=torch.float16,\n+...     dtype=torch.float16,\n ...     device_map=device,\n ... )\n >>> processor = SiglipProcessor.from_pretrained(\"google/siglip-so400m-patch14-384\")\n@@ -177,7 +177,7 @@ PyTorchëŠ” `torch.nn.functional`ì˜ ì¼ë¶€ë¡œ ìŠ¤ì¼€ì¼ëœ ì ê³± ì–´í…ì…˜(SDPA\n >>> model = SiglipModel.from_pretrained(\n ...     \"google/siglip-so400m-patch14-384\",\n ...     attn_implementation=\"sdpa\",\n-...     torch_dtype=torch.float16,\n+...     dtype=torch.float16,\n ...     device_map=device,\n ... )\n ```"
        },
        {
            "sha": "e02740f5fff313f28dd70340169b78f06fc05587",
            "filename": "docs/source/ko/model_doc/vit.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fmodel_doc%2Fvit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fmodel_doc%2Fvit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fvit.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -63,7 +63,7 @@ SDPAëŠ” `torch>=2.1.1`ì—ì„œ êµ¬í˜„ì´ ê°€ëŠ¥í•œ ê²½ìš° ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš©\n \n ```\n from transformers import ViTForImageClassification\n-model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\", attn_implementation=\"sdpa\", torch_dtype=torch.float16)\n+model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\", attn_implementation=\"sdpa\", dtype=torch.float16)\n ...\n ```\n "
        },
        {
            "sha": "304b798796f6984e6f75dc38aebc79ee76efe88e",
            "filename": "docs/source/ko/perf_infer_gpu_multi.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fperf_infer_gpu_multi.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fperf_infer_gpu_multi.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fperf_infer_gpu_multi.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -61,7 +61,7 @@ from transformers import AutoModelForCausalLM, AutoTokenizer\n # model_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\" # ëª¨ë“  ê°€ëŠ¥í•œ ì „ëžµì„ ì‹œê°í™”í•˜ê¸°ì— ë” ì¢‹ìŒ\n model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"  # ì ì€ ìˆ˜ì˜ GPUì— ë” ì¢‹ìŒ\n \n-model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, tp_plan=\"auto\")\n+model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16, tp_plan=\"auto\")\n print(model._tp_plan)\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n@@ -97,7 +97,7 @@ tp_plan = {\n     ...\n }\n \n-model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, tp_plan=tp_plan)\n+model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16, tp_plan=tp_plan)\n print(model._tp_plan)\n ```\n \n@@ -247,7 +247,7 @@ Readd this when I get the exact error message\n         \"model.layers.*.self_attn.q_proj\": \"colwise_custom\",\n         ...\n     }\n-    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, tp_plan=tp_plan)\n+    model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16, tp_plan=tp_plan)\n     ```\n \n ## ë²¤ì¹˜ë§ˆí¬[[benchmarks]]"
        },
        {
            "sha": "c8f472a95e12052c053fa9e208962e7fdbf6e083",
            "filename": "docs/source/ko/quantization/awq.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fquantization%2Fawq.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fquantization%2Fawq.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fquantization%2Fawq.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -62,13 +62,13 @@ model_id = \"TheBloke/zephyr-7B-alpha-AWQ\"\n model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda:0\")\n ```\n \n-AWQ ì–‘ìží™” ëª¨ë¸ì„ ê°€ì ¸ì˜¤ë©´ ìžë™ìœ¼ë¡œ ì„±ëŠ¥ìƒì˜ ì´ìœ ë¡œ ì¸í•´ ê°€ì¤‘ì¹˜ë“¤ì˜ ê¸°ë³¸ê°’ì´ fp16ìœ¼ë¡œ ì„¤ì •ë©ë‹ˆë‹¤. ê°€ì¤‘ì¹˜ë¥¼ ë‹¤ë¥¸ í˜•ì‹ìœ¼ë¡œ ê°€ì ¸ì˜¤ë ¤ë©´, `torch_dtype` íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:\n+AWQ ì–‘ìží™” ëª¨ë¸ì„ ê°€ì ¸ì˜¤ë©´ ìžë™ìœ¼ë¡œ ì„±ëŠ¥ìƒì˜ ì´ìœ ë¡œ ì¸í•´ ê°€ì¤‘ì¹˜ë“¤ì˜ ê¸°ë³¸ê°’ì´ fp16ìœ¼ë¡œ ì„¤ì •ë©ë‹ˆë‹¤. ê°€ì¤‘ì¹˜ë¥¼ ë‹¤ë¥¸ í˜•ì‹ìœ¼ë¡œ ê°€ì ¸ì˜¤ë ¤ë©´, `dtype` íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:\n \n ```py\n from transformers import AutoModelForCausalLM, AutoTokenizer\n \n model_id = \"TheBloke/zephyr-7B-alpha-AWQ\"\n-model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float32)\n+model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.float32)\n ```\n \n ì¶”ë¡ ì„ ë”ìš± ê°€ì†í™”í•˜ê¸° ìœ„í•´ AWQ ì–‘ìží™”ì™€ [FlashAttention-2](../perf_infer_gpu_one#flashattention-2) ë¥¼ ê²°í•© í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤:"
        },
        {
            "sha": "5944239670997150e53b24ba2f403fc3bd65b196",
            "filename": "docs/source/ko/quantization/bitsandbytes.md",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fquantization%2Fbitsandbytes.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Fquantization%2Fbitsandbytes.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fquantization%2Fbitsandbytes.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -56,7 +56,7 @@ model_8bit = AutoModelForCausalLM.from_pretrained(\n )\n ```\n \n-ê¸°ë³¸ì ìœ¼ë¡œ `torch.nn.LayerNorm`ê³¼ ê°™ì€ ë‹¤ë¥¸ ëª¨ë“ˆì€ `torch.float16`ìœ¼ë¡œ ë³€í™˜ë©ë‹ˆë‹¤. ì›í•œë‹¤ë©´ `torch_dtype` ë§¤ê°œë³€ìˆ˜ë¡œ ì´ë“¤ ëª¨ë“ˆì˜ ë°ì´í„° ìœ í˜•ì„ ë³€ê²½í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤:\n+ê¸°ë³¸ì ìœ¼ë¡œ `torch.nn.LayerNorm`ê³¼ ê°™ì€ ë‹¤ë¥¸ ëª¨ë“ˆì€ `torch.float16`ìœ¼ë¡œ ë³€í™˜ë©ë‹ˆë‹¤. ì›í•œë‹¤ë©´ `dtype` ë§¤ê°œë³€ìˆ˜ë¡œ ì´ë“¤ ëª¨ë“ˆì˜ ë°ì´í„° ìœ í˜•ì„ ë³€ê²½í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤:\n \n ```py\n import torch\n@@ -67,7 +67,7 @@ quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n model_8bit = AutoModelForCausalLM.from_pretrained(\n     \"facebook/opt-350m\", \n     quantization_config=quantization_config, \n-    torch_dtype=torch.float32\n+    dtype=torch.float32\n )\n model_8bit.model.decoder.layers[-1].final_layer_norm.weight.dtype\n ```\n@@ -104,7 +104,7 @@ model_4bit = AutoModelForCausalLM.from_pretrained(\n )\n ```\n \n-ê¸°ë³¸ì ìœ¼ë¡œ `torch.nn.LayerNorm`ê³¼ ê°™ì€ ë‹¤ë¥¸ ëª¨ë“ˆì€ `torch.float16`ìœ¼ë¡œ ë³€í™˜ë©ë‹ˆë‹¤. ì›í•œë‹¤ë©´ `torch_dtype` ë§¤ê°œë³€ìˆ˜ë¡œ ì´ë“¤ ëª¨ë“ˆì˜ ë°ì´í„° ìœ í˜•ì„ ë³€ê²½í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤:\n+ê¸°ë³¸ì ìœ¼ë¡œ `torch.nn.LayerNorm`ê³¼ ê°™ì€ ë‹¤ë¥¸ ëª¨ë“ˆì€ `torch.float16`ìœ¼ë¡œ ë³€í™˜ë©ë‹ˆë‹¤. ì›í•œë‹¤ë©´ `dtype` ë§¤ê°œë³€ìˆ˜ë¡œ ì´ë“¤ ëª¨ë“ˆì˜ ë°ì´í„° ìœ í˜•ì„ ë³€ê²½í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤:\n \n ```py\n import torch\n@@ -115,7 +115,7 @@ quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n model_4bit = AutoModelForCausalLM.from_pretrained(\n     \"facebook/opt-350m\",\n     quantization_config=quantization_config, \n-    torch_dtype=torch.float32\n+    dtype=torch.float32\n )\n model_4bit.model.decoder.layers[-1].final_layer_norm.weight.dtype\n ```\n@@ -270,7 +270,7 @@ nf4_config = BitsAndBytesConfig(\n model_nf4 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config)\n ```\n \n-ì¶”ë¡ ì˜ ê²½ìš°, `bnb_4bit_quant_type`ì€ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ëª¨ë¸ ê°€ì¤‘ì¹˜ì™€ ì¼ê´€ì„±ì„ ìœ ì§€í•˜ê¸° ìœ„í•´ `bnb_4bit_compute_dtype` ë° `torch_dtype` ê°’ì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.\n+ì¶”ë¡ ì˜ ê²½ìš°, `bnb_4bit_quant_type`ì€ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ëª¨ë¸ ê°€ì¤‘ì¹˜ì™€ ì¼ê´€ì„±ì„ ìœ ì§€í•˜ê¸° ìœ„í•´ `bnb_4bit_compute_dtype` ë° `dtype` ê°’ì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.\n \n ### ì¤‘ì²© ì–‘ìží™”[[nested-quantization]]\n "
        },
        {
            "sha": "00ce40e97607ae11acfe2c4ddb93533f0c00ad1f",
            "filename": "docs/source/ko/tasks/idefics.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Ftasks%2Fidefics.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Ftasks%2Fidefics.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftasks%2Fidefics.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -64,7 +64,7 @@ IDEFICS í”„ë¡œì„¸ì„œëŠ” [`LlamaTokenizer`]ì™€ IDEFICS ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œë¥¼\n \n >>> processor = AutoProcessor.from_pretrained(checkpoint)\n \n->>> model = IdeficsForVisionText2Text.from_pretrained(checkpoint, torch_dtype=torch.bfloat16, device_map=\"auto\")\n+>>> model = IdeficsForVisionText2Text.from_pretrained(checkpoint, dtype=torch.bfloat16, device_map=\"auto\")\n ```\n \n `device_map`ì„ `\"auto\"`ë¡œ ì„¤ì •í•˜ë©´ ì‚¬ìš© ì¤‘ì¸ ìž¥ì¹˜ë¥¼ ê³ ë ¤í•˜ì—¬ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ê°€ìž¥ ìµœì í™”ëœ ë°©ì‹ìœ¼ë¡œ ë¡œë“œí•˜ê³  ì €ìž¥í•˜ëŠ” ë°©ë²•ì„ ìžë™ìœ¼ë¡œ ê²°ì •í•©ë‹ˆë‹¤.\n@@ -354,7 +354,7 @@ This is an image of a vegetable stand.\n >>> from transformers import IdeficsForVisionText2Text, AutoProcessor\n \n >>> checkpoint = \"HuggingFaceM4/idefics-9b-instruct\"\n->>> model = IdeficsForVisionText2Text.from_pretrained(checkpoint, torch_dtype=torch.bfloat16, device_map=\"auto\")\n+>>> model = IdeficsForVisionText2Text.from_pretrained(checkpoint, dtype=torch.bfloat16, device_map=\"auto\")\n >>> processor = AutoProcessor.from_pretrained(checkpoint)\n \n >>> prompts = ["
        },
        {
            "sha": "dfbb0b8fa5e7b323b072db60631c791a7be46e1b",
            "filename": "docs/source/ko/tasks/prompting.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Ftasks%2Fprompting.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Ftasks%2Fprompting.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftasks%2Fprompting.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -107,7 +107,7 @@ pip install -q transformers accelerate\n ...     \"text-generation\",\n ...     model=model,\n ...     tokenizer=tokenizer,\n-...     torch_dtype=torch.bfloat16,\n+...     dtype=torch.bfloat16,\n ...     device_map=\"auto\",\n ... )\n ```"
        },
        {
            "sha": "622ce75fa9c0c604d5ba8207ae3dce3de47c9779",
            "filename": "docs/source/ko/tasks/visual_question_answering.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Ftasks%2Fvisual_question_answering.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fko%2Ftasks%2Fvisual_question_answering.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftasks%2Fvisual_question_answering.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -342,7 +342,7 @@ Predicted answer: down\n >>> import torch\n \n >>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n->>> model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n+>>> model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", dtype=torch.float16)\n >>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n >>> model.to(device)\n ```"
        },
        {
            "sha": "7cdf3b62e4274e47362dbdf87e140b142f86b1b0",
            "filename": "docs/source/zh/main_classes/deepspeed.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fzh%2Fmain_classes%2Fdeepspeed.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fzh%2Fmain_classes%2Fdeepspeed.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fmain_classes%2Fdeepspeed.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -1650,7 +1650,7 @@ trainer = Trainer(model=model, args=training_args, ...)\n \n æœ‰å…³æ­¤æ–¹æ³•å’Œå…¶ä»–ç›¸å…³åŠŸèƒ½çš„å®Œæ•´è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[æž„å»ºå¤§æ¨¡åž‹](https://deepspeed.readthedocs.io/en/latest/zero3.html#constructing-massive-models)ã€‚\n \n-æ­¤å¤–ï¼Œåœ¨åŠ è½½fp16é¢„è®­ç»ƒæ¨¡åž‹æ—¶ï¼Œæ‚¨å¸Œæœ›`from_pretrained`ä½¿ç”¨`torch_dtype=torch.float16`ã€‚è¯¦æƒ…è¯·å‚è§[from_pretrained-torch-dtype](#from_pretrained-torch-dtype)ã€‚\n+æ­¤å¤–ï¼Œåœ¨åŠ è½½fp16é¢„è®­ç»ƒæ¨¡åž‹æ—¶ï¼Œæ‚¨å¸Œæœ›`from_pretrained`ä½¿ç”¨`dtype=torch.float16`ã€‚è¯¦æƒ…è¯·å‚è§[from_pretrained-torch-dtype](#from_pretrained-torch-dtype)ã€‚\n \n \n #### å‚æ•°æ”¶é›†"
        },
        {
            "sha": "dd9fb57b15c6383af12ad39653930b8162afe5c2",
            "filename": "docs/source/zh/main_classes/model.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fzh%2Fmain_classes%2Fmodel.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fzh%2Fmain_classes%2Fmodel.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fmain_classes%2Fmodel.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -83,14 +83,14 @@ device_map = {\"shared\": 0, \"encoder\": 0, \"decoder\": 1, \"lm_head\": 1}\n \n ### æ¨¡åž‹å®žä¾‹åŒ– dtype\n \n-åœ¨ PyTorch ä¸‹ï¼Œæ¨¡åž‹é€šå¸¸ä»¥ `torch.float32` æ ¼å¼å®žä¾‹åŒ–ã€‚å¦‚æžœå°è¯•åŠ è½½æƒé‡ä¸º fp16 çš„æ¨¡åž‹ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´é—®é¢˜ï¼Œå› ä¸ºå®ƒå°†éœ€è¦ä¸¤å€çš„å†…å­˜ã€‚ä¸ºäº†å…‹æœæ­¤é™åˆ¶ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ `torch_dtype` å‚æ•°æ˜¾å¼ä¼ é€’æ‰€éœ€çš„ `dtype`ï¼š\n+åœ¨ PyTorch ä¸‹ï¼Œæ¨¡åž‹é€šå¸¸ä»¥ `torch.float32` æ ¼å¼å®žä¾‹åŒ–ã€‚å¦‚æžœå°è¯•åŠ è½½æƒé‡ä¸º fp16 çš„æ¨¡åž‹ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´é—®é¢˜ï¼Œå› ä¸ºå®ƒå°†éœ€è¦ä¸¤å€çš„å†…å­˜ã€‚ä¸ºäº†å…‹æœæ­¤é™åˆ¶ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ `dtype` å‚æ•°æ˜¾å¼ä¼ é€’æ‰€éœ€çš„ `dtype`ï¼š\n \n ```python\n-model = T5ForConditionalGeneration.from_pretrained(\"t5\", torch_dtype=torch.float16)\n+model = T5ForConditionalGeneration.from_pretrained(\"t5\", dtype=torch.float16)\n ```\n æˆ–è€…ï¼Œå¦‚æžœæ‚¨å¸Œæœ›æ¨¡åž‹å§‹ç»ˆä»¥æœ€ä¼˜çš„å†…å­˜æ¨¡å¼åŠ è½½ï¼Œåˆ™å¯ä»¥ä½¿ç”¨ç‰¹æ®Šå€¼ `\"auto\"`ï¼Œç„¶åŽ `dtype` å°†è‡ªåŠ¨ä»Žæ¨¡åž‹çš„æƒé‡ä¸­æŽ¨å¯¼å‡ºï¼š\n ```python\n-model = T5ForConditionalGeneration.from_pretrained(\"t5\", torch_dtype=\"auto\")\n+model = T5ForConditionalGeneration.from_pretrained(\"t5\", dtype=\"auto\")\n ```\n \n ä¹Ÿå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å‘ŠçŸ¥ä»Žå¤´å¼€å§‹å®žä¾‹åŒ–çš„æ¨¡åž‹è¦ä½¿ç”¨å“ªç§ `dtype`ï¼š"
        },
        {
            "sha": "7d837aacbb45ac4eb404406dba5cf8298682368b",
            "filename": "docs/source/zh/main_classes/quantization.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fzh%2Fmain_classes%2Fquantization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fzh%2Fmain_classes%2Fquantization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fmain_classes%2Fquantization.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -37,7 +37,7 @@ AWQæ–¹æ³•å·²ç»åœ¨[*AWQ: Activation-aware Weight Quantization for LLM Compressio\n \n ### åŠ è½½ä¸€ä¸ªé‡åŒ–çš„æ¨¡åž‹\n \n-æ‚¨å¯ä»¥ä½¿ç”¨`from_pretrained`æ–¹æ³•ä»ŽHubåŠ è½½ä¸€ä¸ªé‡åŒ–æ¨¡åž‹ã€‚é€šè¿‡æ£€æŸ¥æ¨¡åž‹é…ç½®æ–‡ä»¶ï¼ˆ`configuration.json`ï¼‰ä¸­æ˜¯å¦å­˜åœ¨`quantization_config`å±žæ€§ï¼Œæ¥è¿›è¡Œç¡®è®¤æŽ¨é€çš„æƒé‡æ˜¯é‡åŒ–çš„ã€‚æ‚¨å¯ä»¥é€šè¿‡æ£€æŸ¥å­—æ®µ`quantization_config.quant_method`æ¥ç¡®è®¤æ¨¡åž‹æ˜¯å¦ä»¥AWQæ ¼å¼è¿›è¡Œé‡åŒ–ï¼Œè¯¥å­—æ®µåº”è¯¥è®¾ç½®ä¸º`\"awq\"`ã€‚è¯·æ³¨æ„ï¼Œä¸ºäº†æ€§èƒ½åŽŸå› ï¼Œé»˜è®¤æƒ…å†µä¸‹åŠ è½½æ¨¡åž‹å°†è®¾ç½®å…¶ä»–æƒé‡ä¸º`float16`ã€‚å¦‚æžœæ‚¨æƒ³æ›´æ”¹è¿™ç§è®¾ç½®ï¼Œå¯ä»¥é€šè¿‡å°†`torch_dtype`å‚æ•°è®¾ç½®ä¸º`torch.float32`æˆ–`torch.bfloat16`ã€‚åœ¨ä¸‹é¢çš„éƒ¨åˆ†ä¸­ï¼Œæ‚¨å¯ä»¥æ‰¾åˆ°ä¸€äº›ç¤ºä¾‹ç‰‡æ®µå’Œnotebookã€‚\n+æ‚¨å¯ä»¥ä½¿ç”¨`from_pretrained`æ–¹æ³•ä»ŽHubåŠ è½½ä¸€ä¸ªé‡åŒ–æ¨¡åž‹ã€‚é€šè¿‡æ£€æŸ¥æ¨¡åž‹é…ç½®æ–‡ä»¶ï¼ˆ`configuration.json`ï¼‰ä¸­æ˜¯å¦å­˜åœ¨`quantization_config`å±žæ€§ï¼Œæ¥è¿›è¡Œç¡®è®¤æŽ¨é€çš„æƒé‡æ˜¯é‡åŒ–çš„ã€‚æ‚¨å¯ä»¥é€šè¿‡æ£€æŸ¥å­—æ®µ`quantization_config.quant_method`æ¥ç¡®è®¤æ¨¡åž‹æ˜¯å¦ä»¥AWQæ ¼å¼è¿›è¡Œé‡åŒ–ï¼Œè¯¥å­—æ®µåº”è¯¥è®¾ç½®ä¸º`\"awq\"`ã€‚è¯·æ³¨æ„ï¼Œä¸ºäº†æ€§èƒ½åŽŸå› ï¼Œé»˜è®¤æƒ…å†µä¸‹åŠ è½½æ¨¡åž‹å°†è®¾ç½®å…¶ä»–æƒé‡ä¸º`float16`ã€‚å¦‚æžœæ‚¨æƒ³æ›´æ”¹è¿™ç§è®¾ç½®ï¼Œå¯ä»¥é€šè¿‡å°†`dtype`å‚æ•°è®¾ç½®ä¸º`torch.float32`æˆ–`torch.bfloat16`ã€‚åœ¨ä¸‹é¢çš„éƒ¨åˆ†ä¸­ï¼Œæ‚¨å¯ä»¥æ‰¾åˆ°ä¸€äº›ç¤ºä¾‹ç‰‡æ®µå’Œnotebookã€‚\n \n \n ## ç¤ºä¾‹ä½¿ç”¨\n@@ -295,13 +295,13 @@ model_8bit = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", load_in_8\n model_4bit = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", load_in_4bit=True)\n ```\n \n-é»˜è®¤æƒ…å†µä¸‹ï¼Œæ‰€æœ‰å…¶ä»–æ¨¡å—ï¼ˆä¾‹å¦‚ `torch.nn.LayerNorm`ï¼‰å°†è¢«è½¬æ¢ä¸º `torch.float16` ç±»åž‹ã€‚ä½†å¦‚æžœæ‚¨æƒ³æ›´æ”¹å®ƒä»¬çš„ `dtype`ï¼Œå¯ä»¥é‡è½½ `torch_dtype` å‚æ•°ï¼š\n+é»˜è®¤æƒ…å†µä¸‹ï¼Œæ‰€æœ‰å…¶ä»–æ¨¡å—ï¼ˆä¾‹å¦‚ `torch.nn.LayerNorm`ï¼‰å°†è¢«è½¬æ¢ä¸º `torch.float16` ç±»åž‹ã€‚ä½†å¦‚æžœæ‚¨æƒ³æ›´æ”¹å®ƒä»¬çš„ `dtype`ï¼Œå¯ä»¥é‡è½½ `dtype` å‚æ•°ï¼š\n \n ```python\n >>> import torch\n >>> from transformers import AutoModelForCausalLM\n \n->>> model_8bit = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", load_in_8bit=True, torch_dtype=torch.float32)\n+>>> model_8bit = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", load_in_8bit=True, dtype=torch.float32)\n >>> model_8bit.model.decoder.layers[-1].final_layer_norm.weight.dtype\n torch.float32\n ```\n@@ -331,7 +331,7 @@ torch.float32\n \n - **è®­ç»ƒï¼š** æ ¹æ® [QLoRA è®ºæ–‡](https://huggingface.co/papers/2305.14314)ï¼Œå¯¹äºŽ4ä½åŸºæ¨¡åž‹è®­ç»ƒï¼ˆä½¿ç”¨ LoRA é€‚é…å™¨ï¼‰ï¼Œåº”ä½¿ç”¨ `bnb_4bit_quant_type='nf4'`ã€‚\n \n-- **æŽ¨ç†ï¼š** å¯¹äºŽæŽ¨ç†ï¼Œ`bnb_4bit_quant_type` å¯¹æ€§èƒ½å½±å“ä¸å¤§ã€‚ä½†æ˜¯ä¸ºäº†ä¸Žæ¨¡åž‹çš„æƒé‡ä¿æŒä¸€è‡´ï¼Œè¯·ç¡®ä¿ä½¿ç”¨ç›¸åŒçš„ `bnb_4bit_compute_dtype` å’Œ `torch_dtype` å‚æ•°ã€‚\n+- **æŽ¨ç†ï¼š** å¯¹äºŽæŽ¨ç†ï¼Œ`bnb_4bit_quant_type` å¯¹æ€§èƒ½å½±å“ä¸å¤§ã€‚ä½†æ˜¯ä¸ºäº†ä¸Žæ¨¡åž‹çš„æƒé‡ä¿æŒä¸€è‡´ï¼Œè¯·ç¡®ä¿ä½¿ç”¨ç›¸åŒçš„ `bnb_4bit_compute_dtype` å’Œ `dtype` å‚æ•°ã€‚\n \n #### åŠ è½½ 4 ä½é‡åŒ–çš„å¤§æ¨¡åž‹\n "
        },
        {
            "sha": "a3356e0bf2d4dfb45bc987b6771afe416786b94d",
            "filename": "docs/source/zh/model_doc/bert.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fzh%2Fmodel_doc%2Fbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fzh%2Fmodel_doc%2Fbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fmodel_doc%2Fbert.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -45,7 +45,7 @@ from transformers import pipeline\n pipeline = pipeline(\n     task=\"fill-mask\",\n     model=\"google-bert/bert-base-uncased\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device=0\n )\n pipeline(\"Plants create [MASK] through a process known as photosynthesis.\")\n@@ -63,7 +63,7 @@ tokenizer = AutoTokenizer.from_pretrained(\n )\n model = AutoModelForMaskedLM.from_pretrained(\n     \"google-bert/bert-base-uncased\",\n-    torch_dtype=torch.float16,\n+    dtype=torch.float16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n )"
        },
        {
            "sha": "92fbcbba31e4972b04106e0dc9b178cb194d07fe",
            "filename": "docs/source/zh/pipeline_tutorial.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fzh%2Fpipeline_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/docs%2Fsource%2Fzh%2Fpipeline_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fpipeline_tutorial.md?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -290,7 +290,7 @@ pip install pytesseract\n import torch\n from transformers import pipeline\n \n-pipe = pipeline(model=\"facebook/opt-1.3b\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n+pipe = pipeline(model=\"facebook/opt-1.3b\", dtype=torch.bfloat16, device_map=\"auto\")\n output = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\n ```\n "
        },
        {
            "sha": "4b04231dc5330cb3dffaef97e5e864904844dbdd",
            "filename": "examples/3D_parallel.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/examples%2F3D_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/examples%2F3D_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2F3D_parallel.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -142,7 +142,7 @@ def main():\n         model_name,\n         device_mesh=tp_mesh if dist.is_initialized() else None,\n         tp_plan=\"auto\",\n-        torch_dtype=torch.bfloat16,\n+        dtype=torch.bfloat16,\n     )\n     logger.info(f\"Model loaded onto device mesh: {tp_mesh}\")\n     device = torch.device(f\"cuda:{local_rank}\")"
        },
        {
            "sha": "5dd1d2d2b17c031bdcb42a3268dd7637eb6b6a1c",
            "filename": "examples/pytorch/3d_parallel_checks.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/examples%2Fpytorch%2F3d_parallel_checks.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/examples%2Fpytorch%2F3d_parallel_checks.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2F3d_parallel_checks.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -165,7 +165,7 @@ def main():\n         model_name,\n         device_mesh=tp_mesh if dist.is_initialized() else None,\n         tp_plan=\"auto\",\n-        torch_dtype=torch.bfloat16,\n+        dtype=torch.bfloat16,\n     )\n     logger.info(f\"Model loaded onto device mesh: {tp_mesh}\")\n \n@@ -469,7 +469,7 @@ def collate_fn(batch):\n         new_model = AutoModelForCausalLM.from_pretrained(\n             model_name,\n             device_mesh=tp_mesh,\n-            torch_dtype=torch.bfloat16,  # Use same dtype\n+            dtype=torch.bfloat16,  # Use same dtype\n         )\n         new_optimizer = optim.AdamW(new_model.parameters(), lr=LR)\n "
        },
        {
            "sha": "d24ff45873db4195396f86e45b4f72a49b802bc6",
            "filename": "examples/pytorch/context_parallel.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/examples%2Fpytorch%2Fcontext_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/examples%2Fpytorch%2Fcontext_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fcontext_parallel.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -54,7 +54,7 @@\n \n # model and optimizer\n repo_id = \"Qwen/Qwen2.5-Coder-0.5B-Instruct\"\n-model = AutoModelForCausalLM.from_pretrained(repo_id, torch_dtype=dtype, device_map=device)\n+model = AutoModelForCausalLM.from_pretrained(repo_id, dtype=dtype, device_map=device)\n optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n \n model.train()"
        },
        {
            "sha": "821e3d9a271b2581595eb3cac0c7d1983eab4b8f",
            "filename": "examples/pytorch/continuous_batching.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/examples%2Fpytorch%2Fcontinuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/examples%2Fpytorch%2Fcontinuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fcontinuous_batching.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -14,7 +14,7 @@\n     AutoModelForCausalLM.from_pretrained(\n         model_id,\n         attn_implementation=\"paged_attention|kernels-community/flash-attn\",\n-        torch_dtype=torch.bfloat16,\n+        dtype=torch.bfloat16,\n     )\n     .eval()\n     .cuda()"
        },
        {
            "sha": "8537f0b0308813a9ad9798f59f8117332e620e02",
            "filename": "examples/pytorch/language-modeling/run_clm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -144,7 +144,7 @@ class ModelArguments:\n             )\n         },\n     )\n-    torch_dtype: Optional[str] = field(\n+    dtype: Optional[str] = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -487,11 +487,7 @@ def main():\n         )\n \n     if model_args.model_name_or_path:\n-        torch_dtype = (\n-            model_args.torch_dtype\n-            if model_args.torch_dtype in [\"auto\", None]\n-            else getattr(torch, model_args.torch_dtype)\n-        )\n+        dtype = model_args.dtype if model_args.dtype in [\"auto\", None] else getattr(torch, model_args.dtype)\n         model = AutoModelForCausalLM.from_pretrained(\n             model_args.model_name_or_path,\n             from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n@@ -500,7 +496,7 @@ def main():\n             revision=model_args.model_revision,\n             token=model_args.token,\n             trust_remote_code=model_args.trust_remote_code,\n-            torch_dtype=torch_dtype,\n+            dtype=dtype,\n         )\n     else:\n         model = AutoModelForCausalLM.from_config(config, trust_remote_code=model_args.trust_remote_code)"
        },
        {
            "sha": "301b709958d07a3cf1ac5c80fa339c4adb7acc8e",
            "filename": "examples/pytorch/language-modeling/run_fim.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -147,7 +147,7 @@ class ModelArguments:\n             )\n         },\n     )\n-    torch_dtype: Optional[str] = field(\n+    dtype: Optional[str] = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -493,11 +493,7 @@ def main():\n         )\n \n     if model_args.model_name_or_path:\n-        torch_dtype = (\n-            model_args.torch_dtype\n-            if model_args.torch_dtype in [\"auto\", None]\n-            else getattr(torch, model_args.torch_dtype)\n-        )\n+        dtype = model_args.dtype if model_args.dtype in [\"auto\", None] else getattr(torch, model_args.dtype)\n         model = AutoModelForCausalLM.from_pretrained(\n             model_args.model_name_or_path,\n             from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n@@ -506,7 +502,7 @@ def main():\n             revision=model_args.model_revision,\n             token=model_args.token,\n             trust_remote_code=model_args.trust_remote_code,\n-            torch_dtype=torch_dtype,\n+            dtype=dtype,\n             attn_implementation=model_args.attn_implementation,\n         )\n "
        },
        {
            "sha": "682684f88b073487eaa2c23c2b0e9279f3f3ae55",
            "filename": "examples/pytorch/language-modeling/run_mlm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/examples%2Fpytorch%2Flanguage-modeling%2Frun_mlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/examples%2Fpytorch%2Flanguage-modeling%2Frun_mlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_mlm.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -141,7 +141,7 @@ class ModelArguments:\n             )\n         },\n     )\n-    torch_dtype: Optional[str] = field(\n+    dtype: Optional[str] = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -428,11 +428,7 @@ def main():\n         )\n \n     if model_args.model_name_or_path:\n-        torch_dtype = (\n-            model_args.torch_dtype\n-            if model_args.torch_dtype in [\"auto\", None]\n-            else getattr(torch, model_args.torch_dtype)\n-        )\n+        dtype = model_args.dtype if model_args.dtype in [\"auto\", None] else getattr(torch, model_args.dtype)\n         model = AutoModelForMaskedLM.from_pretrained(\n             model_args.model_name_or_path,\n             from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n@@ -441,7 +437,7 @@ def main():\n             revision=model_args.model_revision,\n             token=model_args.token,\n             trust_remote_code=model_args.trust_remote_code,\n-            torch_dtype=torch_dtype,\n+            dtype=dtype,\n         )\n     else:\n         logger.info(\"Training new model from scratch\")"
        },
        {
            "sha": "f6008d07b07ab2e4e420d219db1abc6221690db6",
            "filename": "examples/quantization/custom_quantization.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/examples%2Fquantization%2Fcustom_quantization.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/examples%2Fquantization%2Fcustom_quantization.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fquantization%2Fcustom_quantization.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -45,7 +45,7 @@ def __init__(self, quantization_config: QuantizationConfigMixin, **kwargs):\n         self.quantization_config = quantization_config\n         self.scale_map = {}\n         self.device = kwargs.get(\"device\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n-        self.torch_dtype = kwargs.get(\"torch_dtype\", torch.float32)\n+        self.dtype = kwargs.get(\"dtype\", torch.float32)\n \n     def _process_model_before_weight_loading(self, model, **kwargs):\n         return True\n@@ -61,7 +61,7 @@ def is_trainable(self) -> bool:\n \n \n model_8bit = AutoModelForCausalLM.from_pretrained(\n-    \"facebook/opt-350m\", quantization_config=CustomConfig(), torch_dtype=\"auto\"\n+    \"facebook/opt-350m\", quantization_config=CustomConfig(), dtype=\"auto\"\n )\n \n tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")"
        },
        {
            "sha": "4bf907b77fe5db58b617eed3ea7741ebc2fcb7fc",
            "filename": "examples/quantization/custom_quantization_int8_example.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/examples%2Fquantization%2Fcustom_quantization_int8_example.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/examples%2Fquantization%2Fcustom_quantization_int8_example.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fquantization%2Fcustom_quantization_int8_example.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -232,7 +232,7 @@ def is_trainable(self) -> bool:\n # Example usage\n if __name__ == \"__main__\":\n     model_int8 = AutoModelForCausalLM.from_pretrained(\n-        \"meta-llama/Llama-3.2-1B\", quantization_config=Int8SymmetricConfig(), torch_dtype=torch.float, device_map=\"cpu\"\n+        \"meta-llama/Llama-3.2-1B\", quantization_config=Int8SymmetricConfig(), dtype=torch.float, device_map=\"cpu\"\n     )\n \n     tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")"
        },
        {
            "sha": "1685e7635c24d4bdae8eb7187e0d205ba59b5be1",
            "filename": "src/transformers/commands/chat.py",
            "status": "modified",
            "additions": 19,
            "deletions": 6,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fcommands%2Fchat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fcommands%2Fchat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fchat.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -248,6 +248,13 @@ class ChatArguments:\n     )\n     device: str = field(default=\"auto\", metadata={\"help\": \"Device to use for inference.\"})\n     torch_dtype: Optional[str] = field(\n+        default=None,\n+        metadata={\n+            \"help\": \"`torch_dtype` is deprecated! Please use `dtype` argument instead.\",\n+            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n+        },\n+    )\n+    dtype: Optional[str] = field(\n         default=\"auto\",\n         metadata={\n             \"help\": \"Override the default `torch.dtype` and load the model under this dtype. If `'auto'` is passed, \"\n@@ -280,6 +287,12 @@ class ChatArguments:\n     host: str = field(default=\"localhost\", metadata={\"help\": \"Interface the server will listen to..\"})\n     port: int = field(default=8000, metadata={\"help\": \"Port the server will listen to.\"})\n \n+    def __post_init__(self):\n+        \"\"\"Only used for BC `torch_dtype` argument.\"\"\"\n+        # In this case only the BC torch_dtype was given\n+        if self.torch_dtype is not None and self.dtype == \"auto\":\n+            self.dtype = self.torch_dtype\n+\n \n def chat_command_factory(args: Namespace):\n     \"\"\"\n@@ -514,11 +527,11 @@ def get_quantization_config(model_args: ChatArguments) -> Optional[\"BitsAndBytes\n         if model_args.load_in_4bit:\n             quantization_config = BitsAndBytesConfig(\n                 load_in_4bit=True,\n-                # For consistency with model weights, we use the same value as `torch_dtype`\n-                bnb_4bit_compute_dtype=model_args.torch_dtype,\n+                # For consistency with model weights, we use the same value as `dtype`\n+                bnb_4bit_compute_dtype=model_args.dtype,\n                 bnb_4bit_quant_type=model_args.bnb_4bit_quant_type,\n                 bnb_4bit_use_double_quant=model_args.use_bnb_nested_quant,\n-                bnb_4bit_quant_storage=model_args.torch_dtype,\n+                bnb_4bit_quant_storage=model_args.dtype,\n             )\n         elif model_args.load_in_8bit:\n             quantization_config = BitsAndBytesConfig(\n@@ -536,12 +549,12 @@ def load_model_and_tokenizer(self, args: ChatArguments) -> tuple[\"AutoModelForCa\n             trust_remote_code=args.trust_remote_code,\n         )\n \n-        torch_dtype = args.torch_dtype if args.torch_dtype in [\"auto\", None] else getattr(torch, args.torch_dtype)\n+        dtype = args.dtype if args.dtype in [\"auto\", None] else getattr(torch, args.dtype)\n         quantization_config = self.get_quantization_config(args)\n         model_kwargs = {\n             \"revision\": args.model_revision,\n             \"attn_implementation\": args.attn_implementation,\n-            \"torch_dtype\": torch_dtype,\n+            \"dtype\": dtype,\n             \"device_map\": \"auto\",\n             \"quantization_config\": quantization_config,\n         }\n@@ -647,7 +660,7 @@ async def _inner_run(self):\n         if self.spawn_backend:\n             serve_args = ServeArguments(\n                 device=self.args.device,\n-                torch_dtype=self.args.torch_dtype,\n+                dtype=self.args.dtype,\n                 trust_remote_code=self.args.trust_remote_code,\n                 attn_implementation=self.args.attn_implementation,\n                 load_in_8bit=self.args.load_in_8bit,"
        },
        {
            "sha": "d7b852c9e932d68fd16baf0dfede0c28e27cb70d",
            "filename": "src/transformers/commands/serving.py",
            "status": "modified",
            "additions": 18,
            "deletions": 5,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fcommands%2Fserving.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fcommands%2Fserving.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fserving.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -361,6 +361,13 @@ class ServeArguments:\n         },\n     )\n     torch_dtype: Optional[str] = field(\n+        default=None,\n+        metadata={\n+            \"help\": \"`torch_dtype` is deprecated! Please use `dtype` argument instead.\",\n+            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n+        },\n+    )\n+    dtype: Optional[str] = field(\n         default=\"auto\",\n         metadata={\n             \"help\": \"Override the default `torch.dtype` and load the model under this dtype. If `'auto'` is passed, \"\n@@ -434,6 +441,12 @@ class ServeArguments:\n         },\n     )\n \n+    def __post_init__(self):\n+        \"\"\"Only used for BC `torch_dtype` argument.\"\"\"\n+        # In this case only the BC torch_dtype was given\n+        if self.torch_dtype is not None and self.dtype == \"auto\":\n+            self.dtype = self.torch_dtype\n+\n \n class ServeCommand(BaseTransformersCLICommand):\n     @staticmethod\n@@ -1463,11 +1476,11 @@ def get_quantization_config(args: ServeArguments) -> Optional[\"BitsAndBytesConfi\n         if args.load_in_4bit:\n             quantization_config = BitsAndBytesConfig(\n                 load_in_4bit=True,\n-                # For consistency with model weights, we use the same value as `torch_dtype`\n-                bnb_4bit_compute_dtype=args.torch_dtype,\n+                # For consistency with model weights, we use the same value as `dtype`\n+                bnb_4bit_compute_dtype=args.dtype,\n                 bnb_4bit_quant_type=args.bnb_4bit_quant_type,\n                 bnb_4bit_use_double_quant=args.use_bnb_nested_quant,\n-                bnb_4bit_quant_storage=args.torch_dtype,\n+                bnb_4bit_quant_storage=args.dtype,\n             )\n         elif args.load_in_8bit:\n             quantization_config = BitsAndBytesConfig(\n@@ -1524,13 +1537,13 @@ def _load_model_and_data_processor(self, model_id_and_revision: str):\n             trust_remote_code=args.trust_remote_code,\n         )\n \n-        torch_dtype = args.torch_dtype if args.torch_dtype in [\"auto\", None] else getattr(torch, args.torch_dtype)\n+        dtype = args.dtype if args.dtype in [\"auto\", None] else getattr(torch, args.dtype)\n         quantization_config = self.get_quantization_config(args)\n \n         model_kwargs = {\n             \"revision\": revision,\n             \"attn_implementation\": args.attn_implementation,\n-            \"torch_dtype\": torch_dtype,\n+            \"dtype\": dtype,\n             \"device_map\": \"auto\",\n             \"trust_remote_code\": args.trust_remote_code,\n         }"
        },
        {
            "sha": "7dc681feb3bb87adb77b2e61e0dabc0c2bc216c2",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 40,
            "deletions": 24,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -179,16 +179,11 @@ class PretrainedConfig(PushToHubMixin):\n         tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n             Whether the model's input and output word embeddings should be tied. Note that this is only relevant if the\n             model has a output word embedding layer.\n-        torch_dtype (`str`, *optional*):\n+        dtype (`str`, *optional*):\n             The `dtype` of the weights. This attribute can be used to initialize the model to a non-default `dtype`\n             (which is normally `float32`) and thus allow for optimal storage allocation. For example, if the saved\n             model is `float16`, ideally we want to load it back using the minimal amount of memory needed to load\n-            `float16` weights. Since the config object is stored in plain text, this attribute contains just the\n-            floating type string without the `torch.` prefix. For example, for `torch.float16` ``torch_dtype` is the\n-            `\"float16\"` string.\n-\n-            This attribute is currently not being used during model loading time, but this may change in the future\n-            versions. But we can already start preparing for the future by saving the dtype with save_pretrained.\n+            `float16` weights.\n     \"\"\"\n \n     model_type: str = \"\"\n@@ -219,7 +214,7 @@ def __init__(\n         output_attentions: bool = False,\n         return_dict: bool = True,\n         torchscript: bool = False,\n-        torch_dtype: Optional[Union[str, \"torch.dtype\"]] = None,\n+        dtype: Optional[Union[str, \"torch.dtype\"]] = None,\n         # Common arguments\n         pruned_heads: Optional[dict[int, list[int]]] = None,\n         tie_word_embeddings: bool = True,\n@@ -266,18 +261,23 @@ def __init__(\n                 f\"The config parameter `problem_type` was not understood: received {problem_type} \"\n                 \"but only 'regression', 'single_label_classification' and 'multi_label_classification' are valid.\"\n             )\n-        if torch_dtype is not None and isinstance(torch_dtype, str) and is_torch_available():\n-            # we will start using self.torch_dtype in v5, but to be consistent with\n-            # from_pretrained's torch_dtype arg convert it to an actual torch.dtype object\n+        # BC for the `torch_dtype` argument instead of the simpler `dtype`\n+        # Do not warn, as it would otherwise always be triggered since most configs on the hub have `torch_dtype`\n+        if (torch_dtype := kwargs.pop(\"torch_dtype\", None)) is not None:\n+            # If both are provided, keep `dtype`\n+            dtype = dtype if dtype is not None else torch_dtype\n+        if dtype is not None and isinstance(dtype, str) and is_torch_available():\n+            # we will start using self.dtype in v5, but to be consistent with\n+            # from_pretrained's dtype arg convert it to an actual torch.dtype object\n             import torch\n \n-            torch_dtype = getattr(torch, torch_dtype)\n+            dtype = getattr(torch, dtype)\n \n         # Attributes common for all models\n         self.return_dict = return_dict\n         self.output_hidden_states = output_hidden_states\n         self.torchscript = torchscript\n-        self.torch_dtype = torch_dtype\n+        self.dtype = dtype\n         self._output_attentions = output_attentions  # has public property\n \n         # Less common kwargs, only used by some models\n@@ -425,6 +425,16 @@ def _attn_implementation(self, value: Optional[Union[str, dict]]):\n                 )\n                 subconfig._attn_implementation = sub_implementation\n \n+    @property\n+    def torch_dtype(self):\n+        logger.warning_once(\"`torch_dtype` is deprecated! Use `dtype` instead!\")\n+        return self.dtype\n+\n+    @torch_dtype.setter\n+    def torch_dtype(self, value):\n+        logger.warning_once(\"`torch_dtype` is deprecated! Use `dtype` instead!\")\n+        self.dtype = value\n+\n     def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub: bool = False, **kwargs):\n         \"\"\"\n         Save a configuration object to the directory `save_directory`, so that it can be re-loaded using the\n@@ -786,6 +796,12 @@ def from_dict(\n         if \"_commit_hash\" in kwargs and \"_commit_hash\" in config_dict:\n             kwargs[\"_commit_hash\"] = config_dict[\"_commit_hash\"]\n \n+        # For BC on the old `torch_dtype`\n+        if (torch_dtype := kwargs.pop(\"torch_dtype\", None)) is not None:\n+            logger.warning_once(\"`torch_dtype` is deprecated! Use `dtype` instead!\")\n+            # If both are present, use `dtype`\n+            kwargs[\"dtype\"] = kwargs.get(\"dtype\", torch_dtype)\n+\n         # We remove it from kwargs so that it does not appear in `return_unused_kwargs`.\n         config_dict[\"attn_implementation\"] = kwargs.pop(\"attn_implementation\", None)\n \n@@ -812,7 +828,7 @@ def from_dict(\n                 if isinstance(current_attr, PretrainedConfig) and isinstance(value, dict):\n                     value = current_attr.__class__(**value)\n                 setattr(config, key, value)\n-                if key != \"torch_dtype\":\n+                if key != \"dtype\":\n                     to_remove.append(key)\n         for key in to_remove:\n             kwargs.pop(key, None)\n@@ -912,7 +928,7 @@ def to_diff_dict(self) -> dict[str, Any]:\n                 if not isinstance(self.quantization_config, dict)\n                 else self.quantization_config\n             )\n-        self.dict_torch_dtype_to_str(serializable_config_dict)\n+        self.dict_dtype_to_str(serializable_config_dict)\n \n         return serializable_config_dict\n \n@@ -946,7 +962,7 @@ def to_dict(self) -> dict[str, Any]:\n                 if not isinstance(self.quantization_config, dict)\n                 else self.quantization_config\n             )\n-        self.dict_torch_dtype_to_str(output)\n+        self.dict_dtype_to_str(output)\n \n         return output\n \n@@ -1030,20 +1046,20 @@ def update_from_string(self, update_str: str):\n \n             setattr(self, k, v)\n \n-    def dict_torch_dtype_to_str(self, d: dict[str, Any]) -> None:\n+    def dict_dtype_to_str(self, d: dict[str, Any]) -> None:\n         \"\"\"\n-        Checks whether the passed dictionary and its nested dicts have a *torch_dtype* key and if it's not None,\n+        Checks whether the passed dictionary and its nested dicts have a *dtype* key and if it's not None,\n         converts torch.dtype to a string of just the type. For example, `torch.float32` get converted into *\"float32\"*\n         string, which can then be stored in the json format.\n         \"\"\"\n-        if d.get(\"torch_dtype\") is not None:\n-            if isinstance(d[\"torch_dtype\"], dict):\n-                d[\"torch_dtype\"] = {k: str(v).split(\".\")[-1] for k, v in d[\"torch_dtype\"].items()}\n-            elif not isinstance(d[\"torch_dtype\"], str):\n-                d[\"torch_dtype\"] = str(d[\"torch_dtype\"]).split(\".\")[1]\n+        if d.get(\"dtype\") is not None:\n+            if isinstance(d[\"dtype\"], dict):\n+                d[\"dtype\"] = {k: str(v).split(\".\")[-1] for k, v in d[\"dtype\"].items()}\n+            elif not isinstance(d[\"dtype\"], str):\n+                d[\"dtype\"] = str(d[\"dtype\"]).split(\".\")[1]\n         for value in d.values():\n             if isinstance(value, dict):\n-                self.dict_torch_dtype_to_str(value)\n+                self.dict_dtype_to_str(value)\n \n     def _remove_keys_not_serialized(self, d: dict[str, Any]) -> None:\n         \"\"\""
        },
        {
            "sha": "601e057bda3023edd6d7494239746345f20d145b",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -1079,17 +1079,17 @@ def from_dict(cls, config_dict: dict[str, Any], **kwargs) -> \"GenerationConfig\":\n         else:\n             return config\n \n-    def dict_torch_dtype_to_str(self, d: dict[str, Any]) -> None:\n+    def dict_dtype_to_str(self, d: dict[str, Any]) -> None:\n         \"\"\"\n-        Checks whether the passed dictionary and its nested dicts have a *torch_dtype* key and if it's not None,\n+        Checks whether the passed dictionary and its nested dicts have a *dtype* key and if it's not None,\n         converts torch.dtype to a string of just the type. For example, `torch.float32` get converted into *\"float32\"*\n         string, which can then be stored in the json format.\n         \"\"\"\n-        if d.get(\"torch_dtype\") is not None and not isinstance(d[\"torch_dtype\"], str):\n-            d[\"torch_dtype\"] = str(d[\"torch_dtype\"]).split(\".\")[1]\n+        if d.get(\"dtype\") is not None and not isinstance(d[\"dtype\"], str):\n+            d[\"dtype\"] = str(d[\"dtype\"]).split(\".\")[1]\n         for value in d.values():\n             if isinstance(value, dict):\n-                self.dict_torch_dtype_to_str(value)\n+                self.dict_dtype_to_str(value)\n \n     def to_diff_dict(self) -> dict[str, Any]:\n         \"\"\"\n@@ -1111,7 +1111,7 @@ def to_diff_dict(self) -> dict[str, Any]:\n             if key not in default_config_dict or key == \"transformers_version\" or value != default_config_dict[key]:\n                 serializable_config_dict[key] = value\n \n-        self.dict_torch_dtype_to_str(serializable_config_dict)\n+        self.dict_dtype_to_str(serializable_config_dict)\n         return serializable_config_dict\n \n     def to_dict(self) -> dict[str, Any]:\n@@ -1134,7 +1134,7 @@ def to_dict(self) -> dict[str, Any]:\n         # Transformers version when serializing this file\n         output[\"transformers_version\"] = __version__\n \n-        self.dict_torch_dtype_to_str(output)\n+        self.dict_dtype_to_str(output)\n         return output\n \n     def to_json_string(self, use_diff: bool = True, ignore_metadata: bool = False) -> str:"
        },
        {
            "sha": "87dd6cffc2faba51d94c8bc15fdbdff1910f3ce4",
            "filename": "src/transformers/integrations/peft.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fpeft.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -577,7 +577,7 @@ def delete_adapter(self, adapter_names: Union[list[str], str]) -> None:\n         import torch\n \n         pipeline = AutoPipelineForText2Image.from_pretrained(\n-            \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16\n+            \"stabilityai/stable-diffusion-xl-base-1.0\", dtype=torch.float16\n         ).to(\"cuda\")\n         pipeline.load_lora_weights(\n             \"jbilcke-hf/sdxl-cinematic-1\", weight_name=\"pytorch_lora_weights.safetensors\", adapter_names=\"cinematic\""
        },
        {
            "sha": "0b98af002f2e06f488fc5f82dfd528efe9a7362a",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -138,7 +138,7 @@ def _get_parameter_tp_plan(parameter_name: str, tp_plan: dict[str, str], is_weig\n     return None\n \n \n-str_to_torch_dtype = {\n+str_to_dtype = {\n     \"BOOL\": torch.bool,\n     \"U8\": torch.uint8,\n     \"I8\": torch.int8,\n@@ -218,7 +218,7 @@ def get_packed_weights(param, empty_param, device_mesh, rank, dim):\n     if casted:\n         return tensor\n     else:\n-        return tensor.to(str_to_torch_dtype[slice_dtype])\n+        return tensor.to(str_to_dtype[slice_dtype])\n \n \n def repack_weights("
        },
        {
            "sha": "6226898c9a2a422ce154568d7e391fdca73db831",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 92,
            "deletions": 82,
            "changes": 174,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -291,7 +291,7 @@ def set_zero3_state():\n         _is_ds_init_called = False\n \n \n-def restore_default_torch_dtype(func):\n+def restore_default_dtype(func):\n     \"\"\"\n     Decorator to restore the default torch dtype\n     at the end of the function. Serves\n@@ -1341,92 +1341,92 @@ def _get_resolved_checkpoint_files(\n     return checkpoint_files, sharded_metadata\n \n \n-def _get_torch_dtype(\n+def _get_dtype(\n     cls,\n-    torch_dtype: Optional[Union[str, torch.dtype, dict]],\n+    dtype: Optional[Union[str, torch.dtype, dict]],\n     checkpoint_files: Optional[list[str]],\n     config: PretrainedConfig,\n     sharded_metadata: Optional[dict],\n     state_dict: Optional[dict],\n     weights_only: bool,\n ) -> tuple[PretrainedConfig, Optional[torch.dtype], Optional[torch.dtype]]:\n-    \"\"\"Find the correct `torch_dtype` to use based on provided arguments. Also update the `config` based on the\n+    \"\"\"Find the correct `dtype` to use based on provided arguments. Also update the `config` based on the\n     inferred dtype. We do the following:\n-    1. If torch_dtype is not None, we use that dtype\n-    2. If torch_dtype is \"auto\", we auto-detect dtype from the loaded state_dict, by checking its first\n+    1. If dtype is not None, we use that dtype\n+    2. If dtype is \"auto\", we auto-detect dtype from the loaded state_dict, by checking its first\n         weights entry that is of a floating type - we assume all floating dtype weights are of the same dtype\n-    we also may have config.torch_dtype available, but we won't rely on it till v5\n+    we also may have config.dtype available, but we won't rely on it till v5\n     \"\"\"\n     dtype_orig = None\n     is_sharded = sharded_metadata is not None\n \n-    if torch_dtype is not None:\n-        if isinstance(torch_dtype, str):\n-            if torch_dtype == \"auto\":\n-                if hasattr(config, \"torch_dtype\") and config.torch_dtype is not None:\n-                    torch_dtype = config.torch_dtype\n-                    logger.info(f\"Will use torch_dtype={torch_dtype} as defined in model's config object\")\n+    if dtype is not None:\n+        if isinstance(dtype, str):\n+            if dtype == \"auto\":\n+                if hasattr(config, \"dtype\") and config.dtype is not None:\n+                    dtype = config.dtype\n+                    logger.info(f\"Will use dtype={dtype} as defined in model's config object\")\n                 else:\n                     if is_sharded and \"dtype\" in sharded_metadata:\n-                        torch_dtype = sharded_metadata[\"dtype\"]\n+                        dtype = sharded_metadata[\"dtype\"]\n                     elif state_dict is not None:\n-                        torch_dtype = get_state_dict_dtype(state_dict)\n+                        dtype = get_state_dict_dtype(state_dict)\n                     else:\n                         state_dict = load_state_dict(\n                             checkpoint_files[0], map_location=\"meta\", weights_only=weights_only\n                         )\n-                        torch_dtype = get_state_dict_dtype(state_dict)\n+                        dtype = get_state_dict_dtype(state_dict)\n                     logger.info(\n-                        \"Since the `torch_dtype` attribute can't be found in model's config object, \"\n-                        \"will use torch_dtype={torch_dtype} as derived from model's weights\"\n+                        \"Since the `dtype` attribute can't be found in model's config object, \"\n+                        \"will use dtype={dtype} as derived from model's weights\"\n                     )\n-            elif hasattr(torch, torch_dtype):\n-                torch_dtype = getattr(torch, torch_dtype)\n-                config.torch_dtype = torch_dtype\n+            elif hasattr(torch, dtype):\n+                dtype = getattr(torch, dtype)\n+                config.dtype = dtype\n                 for sub_config_key in config.sub_configs:\n                     sub_config = getattr(config, sub_config_key)\n-                    sub_config.torch_dtype = torch_dtype\n-        elif isinstance(torch_dtype, torch.dtype):\n-            config.torch_dtype = torch_dtype\n+                    sub_config.dtype = dtype\n+        elif isinstance(dtype, torch.dtype):\n+            config.dtype = dtype\n             for sub_config_key in config.sub_configs:\n                 sub_config = getattr(config, sub_config_key)\n-                sub_config.torch_dtype = torch_dtype\n-        elif isinstance(torch_dtype, dict):\n-            for key, curr_dtype in torch_dtype.items():\n+                sub_config.dtype = dtype\n+        elif isinstance(dtype, dict):\n+            for key, curr_dtype in dtype.items():\n                 if hasattr(config, key):\n                     value = getattr(config, key)\n                     curr_dtype = curr_dtype if not isinstance(curr_dtype, str) else getattr(torch, curr_dtype)\n-                    value.torch_dtype = curr_dtype\n+                    value.dtype = curr_dtype\n             # main torch dtype for modules that aren't part of any sub-config\n-            torch_dtype = torch_dtype.get(\"\")\n-            torch_dtype = torch_dtype if not isinstance(torch_dtype, str) else getattr(torch, torch_dtype)\n-            config.torch_dtype = torch_dtype\n-            if torch_dtype is None:\n-                torch_dtype = torch.float32\n+            dtype = dtype.get(\"\")\n+            dtype = dtype if not isinstance(dtype, str) else getattr(torch, dtype)\n+            config.dtype = dtype\n+            if dtype is None:\n+                dtype = torch.float32\n         else:\n             raise ValueError(\n-                f\"`torch_dtype` can be one of: `torch.dtype`, `'auto'`, a string of a valid `torch.dtype` or a `dict` with valid `torch_dtype` \"\n-                f\"for each sub-config in composite configs, but received {torch_dtype}\"\n+                f\"`dtype` can be one of: `torch.dtype`, `'auto'`, a string of a valid `torch.dtype` or a `dict` with valid `dtype` \"\n+                f\"for each sub-config in composite configs, but received {dtype}\"\n             )\n \n-        dtype_orig = cls._set_default_torch_dtype(torch_dtype)\n+        dtype_orig = cls._set_default_dtype(dtype)\n     else:\n         # set fp32 as the default dtype for BC\n         default_dtype = torch.get_default_dtype()\n-        config.torch_dtype = default_dtype\n+        config.dtype = default_dtype\n         for key in config.sub_configs:\n             value = getattr(config, key)\n-            value.torch_dtype = default_dtype\n+            value.dtype = default_dtype\n \n-    return config, torch_dtype, dtype_orig\n+    return config, dtype, dtype_orig\n \n \n def _get_device_map(\n     model: \"PreTrainedModel\",\n     device_map: Optional[Union[dict, str]],\n     max_memory: Optional[dict],\n     hf_quantizer: Optional[HfQuantizer],\n-    torch_dtype: Optional[torch.dtype],\n+    dtype: Optional[torch.dtype],\n     keep_in_fp32_regex: Optional[re.Pattern],\n ) -> dict:\n     \"\"\"Compute the final `device_map` to use if we passed a value in ['auto', 'balanced', 'balanced_low_0', 'sequential'].\n@@ -1435,13 +1435,13 @@ def _get_device_map(\n     if isinstance(device_map, str):\n         special_dtypes = {}\n         if hf_quantizer is not None:\n-            special_dtypes.update(hf_quantizer.get_special_dtypes_update(model, torch_dtype))\n+            special_dtypes.update(hf_quantizer.get_special_dtypes_update(model, dtype))\n         if keep_in_fp32_regex is not None:\n             special_dtypes.update(\n                 {name: torch.float32 for name, _ in model.named_parameters() if keep_in_fp32_regex.search(name)}\n             )\n \n-        target_dtype = torch_dtype\n+        target_dtype = dtype\n \n         if hf_quantizer is not None:\n             target_dtype = hf_quantizer.adjust_target_dtype(target_dtype)\n@@ -2409,26 +2409,31 @@ def add_model_tags(self, tags: Union[list[str], str]) -> None:\n                 self.model_tags.append(tag)\n \n     @classmethod\n-    @restore_default_torch_dtype\n+    @restore_default_dtype\n     def _from_config(cls, config, **kwargs):\n         \"\"\"\n         All context managers that the model should be initialized under go here.\n \n         Args:\n-            torch_dtype (`torch.dtype`, *optional*):\n-                Override the default `torch.dtype` and load the model under this dtype.\n+            dtype (`torch.dtype`, *optional*):\n+                Override the default `dtype` and load the model under this dtype.\n         \"\"\"\n         # when we init a model from within another model (e.g. VLMs) and dispatch on FA2\n         # a warning is raised that dtype should be fp16. Since we never pass dtype from within\n         # modeling code, we can try to infer it here same way as done in `from_pretrained`\n-        torch_dtype = kwargs.pop(\"torch_dtype\", config.torch_dtype)\n-        if isinstance(torch_dtype, str):\n-            torch_dtype = getattr(torch, torch_dtype)\n+        # For BC on the old `torch_dtype`\n+        dtype = kwargs.pop(\"dtype\", config.dtype)\n+        if (torch_dtype := kwargs.pop(\"torch_dtype\", None)) is not None:\n+            logger.warning_once(\"`torch_dtype` is deprecated! Use `dtype` instead!\")\n+            # if both kwargs are provided, use `dtype`\n+            dtype = dtype if dtype != config.dtype else torch_dtype\n+        if isinstance(dtype, str):\n+            dtype = getattr(torch, dtype)\n \n         # override default dtype if needed\n         dtype_orig = None\n-        if torch_dtype is not None:\n-            dtype_orig = cls._set_default_torch_dtype(torch_dtype)\n+        if dtype is not None:\n+            dtype_orig = cls._set_default_dtype(dtype)\n \n         # If passing `attn_implementation` as kwargs, respect it (it will be applied recursively on subconfigs)\n         if \"attn_implementation\" in kwargs:\n@@ -2454,7 +2459,7 @@ def _from_config(cls, config, **kwargs):\n         return model\n \n     @classmethod\n-    def _set_default_torch_dtype(cls, dtype: torch.dtype) -> torch.dtype:\n+    def _set_default_dtype(cls, dtype: torch.dtype) -> torch.dtype:\n         \"\"\"\n         Change the default dtype and return the previous one. This is needed when wanting to instantiate the model\n         under specific dtype.\n@@ -2536,7 +2541,7 @@ def _flash_attn_2_can_dispatch(self, is_init_check: bool = False) -> bool:\n                 BetterTransformer, which are only available later after __init__. This allows to raise proper exceptions early\n                 before instantiating the full models if we know that the model does not support the requested attention.\n         \"\"\"\n-        torch_dtype = self.config.torch_dtype\n+        dtype = self.config.dtype\n \n         # check `supports_flash_attn_2` for BC with custom code. TODO: remove after a few releases\n         if not (self._supports_flash_attn or getattr(self, \"_supports_flash_attn_2\", False)):\n@@ -2579,15 +2584,15 @@ def _flash_attn_2_can_dispatch(self, is_init_check: bool = False) -> bool:\n                     else:\n                         raise ImportError(f\"{preface} Flash Attention 2 is not available. {install_message}\")\n \n-        if torch_dtype is None:\n+        if dtype is None:\n             logger.warning_once(\n                 \"You are attempting to use Flash Attention 2 without specifying a torch dtype. This might lead to unexpected behaviour\"\n             )\n-        elif torch_dtype is not None and torch_dtype not in [torch.float16, torch.bfloat16]:\n+        elif dtype is not None and dtype not in [torch.float16, torch.bfloat16]:\n             logger.warning_once(\n                 \"Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but\"\n-                f\" the current dype in {self.__class__.__name__} is {torch_dtype}. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator,\"\n-                ' or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`'\n+                f\" the current dype in {self.__class__.__name__} is {dtype}. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator,\"\n+                ' or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", dtype=torch.float16)`'\n             )\n \n         # With the early check, the parameters are not yet initialized correctly\n@@ -2630,7 +2635,7 @@ def _flash_attn_3_can_dispatch(self, is_init_check: bool = False) -> bool:\n                 BetterTransformer, which are only available later after __init__. This allows to raise proper exceptions early\n                 before instantiating the full models if we know that the model does not support the requested attention.\n         \"\"\"\n-        torch_dtype = self.config.torch_dtype\n+        dtype = self.config.dtype\n \n         if not self._supports_flash_attn:\n             raise ValueError(\n@@ -2658,15 +2663,15 @@ def _flash_attn_3_can_dispatch(self, is_init_check: bool = False) -> bool:\n                     f\"{preface} Flash Attention 3 is not available on CPU. Please make sure torch can access a CUDA device.\"\n                 )\n \n-        if torch_dtype is None:\n+        if dtype is None:\n             logger.warning_once(\n                 \"You are attempting to use Flash Attention 3 without specifying a torch dtype. This might lead to unexpected behaviour\"\n             )\n-        elif torch_dtype is not None and torch_dtype not in [torch.float16, torch.bfloat16]:\n+        elif dtype is not None and dtype not in [torch.float16, torch.bfloat16]:\n             logger.warning_once(\n                 \"Flash Attention 3 only supports torch.float16 and torch.bfloat16 dtypes, but\"\n-                f\" the current dype in {self.__class__.__name__} is {torch_dtype}. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator,\"\n-                ' or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"meta-llama/Llama-3.2-1B\", attn_implementation=\"flash_attention_3\", torch_dtype=torch.float16)`'\n+                f\" the current dype in {self.__class__.__name__} is {dtype}. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator,\"\n+                ' or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained(\"meta-llama/Llama-3.2-1B\", attn_implementation=\"flash_attention_3\", dtype=torch.float16)`'\n             )\n \n         if getattr(self.config, \"alibi\", False) or getattr(self.config, \"use_alibi\", False):\n@@ -4000,7 +4005,7 @@ def save_pretrained(\n         # save the string version of dtype to the config, e.g. convert torch.float32 => \"float32\"\n         # we currently don't use this setting automatically, but may start to use with v5\n         dtype = get_parameter_dtype(model_to_save)\n-        model_to_save.config.torch_dtype = str(dtype).split(\".\")[1]\n+        model_to_save.config.dtype = str(dtype).split(\".\")[1]\n \n         # Attach architecture to the config\n         model_to_save.config.architectures = [model_to_save.__class__.__name__]\n@@ -4421,7 +4426,7 @@ def to(self, *args, **kwargs):\n             if dtype_present_in_args:\n                 raise ValueError(\n                     \"You cannot cast a bitsandbytes model in a new `dtype`. Make sure to load the model using `from_pretrained` using the\"\n-                    \" desired `dtype` by passing the correct `torch_dtype` argument.\"\n+                    \" desired `dtype` by passing the correct `dtype` argument.\"\n                 )\n \n             if getattr(self, \"is_loaded_in_8bit\", False):\n@@ -4438,7 +4443,7 @@ def to(self, *args, **kwargs):\n             if dtype_present_in_args:\n                 raise ValueError(\n                     \"You cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\"\n-                    \" `dtype` by passing the correct `torch_dtype` argument.\"\n+                    \" `dtype` by passing the correct `dtype` argument.\"\n                 )\n         return super().to(*args, **kwargs)\n \n@@ -4480,7 +4485,7 @@ def get_init_context(cls, is_quantized: bool, _is_ds_init_called: bool):\n         return init_contexts\n \n     @classmethod\n-    @restore_default_torch_dtype\n+    @restore_default_dtype\n     def from_pretrained(\n         cls: type[SpecificPreTrainedModelType],\n         pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n@@ -4608,15 +4613,15 @@ def from_pretrained(\n \n             > Parameters for big model inference\n \n-            torch_dtype (`str` or `torch.dtype`, *optional*):\n-                Override the default `torch.dtype` and load the model under a specific `dtype`. The different options\n+            dtype (`str` or `torch.dtype`, *optional*):\n+                Override the default `torch_dtype` and load the model under a specific `dtype`. The different options\n                 are:\n \n                 1. `torch.float16` or `torch.bfloat16` or `torch.float`: load in a specified\n-                  `dtype`, ignoring the model's `config.torch_dtype` if one exists. If not specified\n+                  `dtype`, ignoring the model's `config.dtype` if one exists. If not specified\n                   - the model will get loaded in `torch.float` (fp32).\n \n-                2. `\"auto\"` - A `torch_dtype` entry in the `config.json` file of the model will be\n+                2. `\"auto\"` - A `dtype` or `torch_dtype` entry in the `config.json` file of the model will be\n                   attempted to be used. If this entry isn't found then next check the `dtype` of the first weight in\n                   the checkpoint that's of a floating point type and use that as `dtype`. This will load the model\n                   using the `dtype` it was saved in at the end of the training. It can't be used as an indicator of how\n@@ -4628,7 +4633,7 @@ def from_pretrained(\n \n                 For some models the `dtype` they were trained in is unknown - you may try to check the model's paper or\n                 reach out to the authors and ask them to add this information to the model's card and to insert the\n-                `torch_dtype` entry in `config.json` on the hub.\n+                `dtype` or `torch_dtype` entry in `config.json` on the hub.\n \n                 </Tip>\n \n@@ -4732,7 +4737,8 @@ def from_pretrained(\n         use_auth_token = kwargs.pop(\"use_auth_token\", None)\n         from_pipeline = kwargs.pop(\"_from_pipeline\", None)\n         from_auto_class = kwargs.pop(\"_from_auto\", False)\n-        torch_dtype = kwargs.pop(\"torch_dtype\", None)\n+        dtype = kwargs.pop(\"dtype\", None)\n+        torch_dtype = kwargs.pop(\"torch_dtype\", None)  # kept for BC\n         device_map = kwargs.pop(\"device_map\", None)\n         max_memory = kwargs.pop(\"max_memory\", None)\n         offload_folder = kwargs.pop(\"offload_folder\", None)\n@@ -4771,6 +4777,12 @@ def from_pretrained(\n         _ = kwargs.pop(\"_fast_init\", True)\n         _ = kwargs.pop(\"low_cpu_mem_usage\", None)\n \n+        # For BC on torch_dtype argument\n+        if torch_dtype is not None:\n+            logger.warning_once(\"`torch_dtype` is deprecated! Use `dtype` instead!\")\n+            # If both kwargs are provided, use `dtype`\n+            dtype = dtype if dtype is not None else torch_dtype\n+\n         if state_dict is not None and (pretrained_model_name_or_path is not None or gguf_file is not None):\n             raise ValueError(\n                 \"`state_dict` cannot be passed together with a model name or a `gguf_file`. Use one of the two loading strategies.\"\n@@ -5003,13 +5015,13 @@ def from_pretrained(\n \n         if hf_quantizer is not None:\n             hf_quantizer.validate_environment(\n-                torch_dtype=torch_dtype,\n+                dtype=dtype,\n                 from_tf=from_tf,\n                 from_flax=from_flax,\n                 device_map=device_map,\n                 weights_only=weights_only,\n             )\n-            torch_dtype = hf_quantizer.update_torch_dtype(torch_dtype)\n+            dtype = hf_quantizer.update_dtype(dtype)\n             device_map = hf_quantizer.update_device_map(device_map)\n             config = hf_quantizer.update_tp_plan(config)\n \n@@ -5100,8 +5112,8 @@ def from_pretrained(\n                 ]\n \n             # Find the correct dtype based on current state\n-            config, torch_dtype, dtype_orig = _get_torch_dtype(\n-                cls, torch_dtype, checkpoint_files, config, sharded_metadata, state_dict, weights_only\n+            config, dtype, dtype_orig = _get_dtype(\n+                cls, dtype, checkpoint_files, config, sharded_metadata, state_dict, weights_only\n             )\n \n         config.name_or_path = pretrained_model_name_or_path\n@@ -5123,13 +5135,11 @@ def from_pretrained(\n         # in case of force loading a model that should stay bf16 in fp16 (which includes a few quantizers as this is a pre-processing\n         # step for e.g. bitsandbytes). See https://github.com/huggingface/transformers/issues/20287 for details.\n         if model._keep_in_fp32_modules is not None and (\n-            torch_dtype == torch.float16 or getattr(hf_quantizer, \"use_keep_in_fp32_modules\", False)\n+            dtype == torch.float16 or getattr(hf_quantizer, \"use_keep_in_fp32_modules\", False)\n         ):\n             keep_in_fp32_modules.extend(model._keep_in_fp32_modules)\n \n-        if model._keep_in_fp32_modules_strict is not None and (\n-            torch_dtype == torch.float16 or torch_dtype == torch.bfloat16\n-        ):\n+        if model._keep_in_fp32_modules_strict is not None and (dtype == torch.float16 or dtype == torch.bfloat16):\n             keep_in_fp32_modules.extend(model._keep_in_fp32_modules_strict)\n \n         keep_in_fp32_regex = None\n@@ -5149,7 +5159,7 @@ def from_pretrained(\n             # once the weights have been quantized\n             # Note that once you have loaded a quantized model, you can't change its dtype so this will\n             # remain a single source of truth\n-            original_dtype = torch_dtype if torch_dtype is not None else torch.get_default_dtype()\n+            original_dtype = dtype if dtype is not None else torch.get_default_dtype()\n \n             def _assign_original_dtype(module):\n                 for child in module.children():\n@@ -5165,7 +5175,7 @@ def _assign_original_dtype(module):\n \n         # Prepare the full device map\n         if device_map is not None:\n-            device_map = _get_device_map(model, device_map, max_memory, hf_quantizer, torch_dtype, keep_in_fp32_regex)\n+            device_map = _get_device_map(model, device_map, max_memory, hf_quantizer, dtype, keep_in_fp32_regex)\n \n         # Finalize model weight initialization\n         if from_tf:\n@@ -5194,7 +5204,7 @@ def _assign_original_dtype(module):\n                 device_map=device_map,\n                 disk_offload_folder=offload_folder,\n                 offload_state_dict=offload_state_dict,\n-                dtype=torch_dtype,\n+                dtype=dtype,\n                 hf_quantizer=hf_quantizer,\n                 keep_in_fp32_regex=keep_in_fp32_regex,\n                 device_mesh=device_mesh,"
        },
        {
            "sha": "4931595f92cfa8c867c17a2d72495f633b2a1795",
            "filename": "src/transformers/models/aria/convert_aria_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Faria%2Fconvert_aria_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Faria%2Fconvert_aria_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fconvert_aria_weights_to_hf.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -36,7 +36,7 @@\n     from aria.model.language_model.aria_llama import AriaTextForCausalLM\n \n     # load model\n-    kwargs = {\"device_map\": \"auto\", \"torch_dtype\": torch.float16}\n+    kwargs = {\"device_map\": \"auto\", \"dtype\": torch.float16}\n     model = AriaTextForCausalLM.from_pretrained(\"rhymes-ai/Aria\", **kwargs)\n \n     # load vision tower"
        },
        {
            "sha": "b2cc34b5e69f34ad39230327c8b9d7f4c5d9bef7",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -1177,7 +1177,7 @@ def forward(\n         >>> image3 = load_image(\"https://cdn.britannica.com/68/170868-050-8DDE8263/Golden-Gate-Bridge-San-Francisco.jpg\")\n \n         >>> processor = AutoProcessor.from_pretrained(\"Rhymes-AI/Aria\")\n-        >>> model = AutoModel.from_pretrained(\"Rhymes-AI/Aria\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n+        >>> model = AutoModel.from_pretrained(\"Rhymes-AI/Aria\", dtype=torch.bfloat16, device_map=\"auto\")\n \n         >>> # Create inputs\n         >>> messages = ["
        },
        {
            "sha": "31469966262c9a533000d1419fe5e3ffb439bcb3",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -1513,7 +1513,7 @@ def forward(\n         >>> image3 = load_image(\"https://cdn.britannica.com/68/170868-050-8DDE8263/Golden-Gate-Bridge-San-Francisco.jpg\")\n \n         >>> processor = AutoProcessor.from_pretrained(\"Rhymes-AI/Aria\")\n-        >>> model = AutoModel.from_pretrained(\"Rhymes-AI/Aria\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n+        >>> model = AutoModel.from_pretrained(\"Rhymes-AI/Aria\", dtype=torch.bfloat16, device_map=\"auto\")\n \n         >>> # Create inputs\n         >>> messages = ["
        },
        {
            "sha": "a8781c8042a6f51aa2b68f03a847f6a6320ec9ba",
            "filename": "src/transformers/models/auto/auto_factory.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -536,10 +536,12 @@ def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike[s\n \n         if not isinstance(config, PretrainedConfig):\n             kwargs_orig = copy.deepcopy(kwargs)\n-            # ensure not to pollute the config object with torch_dtype=\"auto\" - since it's\n+            # ensure not to pollute the config object with dtype=\"auto\" - since it's\n             # meaningless in the context of the config object - torch.dtype values are acceptable\n             if kwargs.get(\"torch_dtype\") == \"auto\":\n                 _ = kwargs.pop(\"torch_dtype\")\n+            if kwargs.get(\"dtype\") == \"auto\":\n+                _ = kwargs.pop(\"dtype\")\n             # to not overwrite the quantization_config if config has a quantization_config\n             if kwargs.get(\"quantization_config\") is not None:\n                 _ = kwargs.pop(\"quantization_config\")\n@@ -556,6 +558,8 @@ def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike[s\n             # if torch_dtype=auto was passed here, ensure to pass it on\n             if kwargs_orig.get(\"torch_dtype\", None) == \"auto\":\n                 kwargs[\"torch_dtype\"] = \"auto\"\n+            if kwargs_orig.get(\"dtype\", None) == \"auto\":\n+                kwargs[\"dtype\"] = \"auto\"\n             if kwargs_orig.get(\"quantization_config\", None) is not None:\n                 kwargs[\"quantization_config\"] = kwargs_orig[\"quantization_config\"]\n "
        },
        {
            "sha": "2a9bda9f9a00f34062127105330495bbd97fe8eb",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -1510,7 +1510,7 @@ def forward(\n         >>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n \n         >>> processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n-        >>> model = Blip2Model.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n+        >>> model = Blip2Model.from_pretrained(\"Salesforce/blip2-opt-2.7b\", dtype=torch.float16)\n         >>> model.to(device)  # doctest: +IGNORE_RESULT\n \n         >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n@@ -1663,7 +1663,7 @@ def forward(\n         >>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n \n         >>> model = Blip2TextModelWithProjection.from_pretrained(\n-        ...     \"Salesforce/blip2-itm-vit-g\", torch_dtype=torch.float16\n+        ...     \"Salesforce/blip2-itm-vit-g\", dtype=torch.float16\n         ... )\n \n         >>> model.to(device)  # doctest: +IGNORE_RESULT\n@@ -1755,7 +1755,7 @@ def forward(\n \n         >>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-itm-vit-g\")\n         >>> model = Blip2VisionModelWithProjection.from_pretrained(\n-        ...     \"Salesforce/blip2-itm-vit-g\", torch_dtype=torch.float16\n+        ...     \"Salesforce/blip2-itm-vit-g\", dtype=torch.float16\n         ... )\n         >>> model.to(device)  # doctest: +IGNORE_RESULT\n \n@@ -2005,7 +2005,7 @@ def forward(\n \n         >>> processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n         >>> model = Blip2ForConditionalGeneration.from_pretrained(\n-        ...     \"Salesforce/blip2-opt-2.7b\", load_in_8bit=True, device_map={\"\": 0}, torch_dtype=torch.float16\n+        ...     \"Salesforce/blip2-opt-2.7b\", load_in_8bit=True, device_map={\"\": 0}, dtype=torch.float16\n         ... )  # doctest: +IGNORE_RESULT\n \n         >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n@@ -2040,7 +2040,7 @@ def forward(\n \n         ```python\n         >>> model = Blip2ForConditionalGeneration.from_pretrained(\n-        ...     \"Salesforce/blip2-opt-2.7b\", load_in_8bit=True, device_map={\"\": 0}, torch_dtype=torch.bfloat16\n+        ...     \"Salesforce/blip2-opt-2.7b\", load_in_8bit=True, device_map={\"\": 0}, dtype=torch.bfloat16\n         ... )  # doctest: +IGNORE_RESULT\n \n         >>> inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device=\"cuda\", dtype=torch.bfloat16)\n@@ -2282,7 +2282,7 @@ def forward(\n \n         >>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n \n-        >>> model = Blip2ForImageTextRetrieval.from_pretrained(\"Salesforce/blip2-itm-vit-g\", torch_dtype=torch.float16)\n+        >>> model = Blip2ForImageTextRetrieval.from_pretrained(\"Salesforce/blip2-itm-vit-g\", dtype=torch.float16)\n         >>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-itm-vit-g\")\n \n         >>> model.to(device)  # doctest: +IGNORE_RESULT"
        },
        {
            "sha": "148706176b127aec3482c94bf4ae0f2874d99d34",
            "filename": "src/transformers/models/bloom/convert_bloom_original_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Fbloom%2Fconvert_bloom_original_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Fbloom%2Fconvert_bloom_original_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fconvert_bloom_original_checkpoint_to_pytorch.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -202,9 +202,9 @@ def convert_bloom_checkpoint_to_pytorch(\n         os.makedirs(pytorch_dump_folder_path, exist_ok=True)\n         pytorch_weights_dump_path = pytorch_dump_folder_path + \"/\" + WEIGHTS_NAME\n         pytorch_config_dump_path = pytorch_dump_folder_path + \"/\" + CONFIG_NAME\n-        print(f\"Save PyTorch model to {pytorch_weights_dump_path} with dtype {config.torch_dtype}\")\n-        if config.torch_dtype is not None:\n-            model = model.to(config.torch_dtype)\n+        print(f\"Save PyTorch model to {pytorch_weights_dump_path} with dtype {config.dtype}\")\n+        if config.dtype is not None:\n+            model = model.to(config.dtype)\n         torch.save(model.state_dict(), pytorch_weights_dump_path)\n         print(f\"Save configuration file to {pytorch_config_dump_path}\")\n         with open(pytorch_config_dump_path, \"w\", encoding=\"utf-8\") as f:"
        },
        {
            "sha": "27661ec2bac495eb1b4d631230bd087c648b62f8",
            "filename": "src/transformers/models/chameleon/convert_chameleon_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconvert_chameleon_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconvert_chameleon_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconvert_chameleon_weights_to_hf.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -400,7 +400,7 @@ def permute(w, n_heads, dim1=dim, dim2=dim):\n     print(\"Loading the checkpoint in a Chameleon model...\")\n     print(\"*\" * 100)\n     model = ChameleonForConditionalGeneration.from_pretrained(\n-        model_path, attn_implementation=\"eager\", torch_dtype=torch.bfloat16, device_map=\"auto\"\n+        model_path, attn_implementation=\"eager\", dtype=torch.bfloat16, device_map=\"auto\"\n     )\n     processor = ChameleonProcessor.from_pretrained(model_path)\n "
        },
        {
            "sha": "0044c6a15ca18c328fc2470624ccd1a85e77ccbe",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -1087,7 +1087,7 @@ def forward(\n         >>> import requests\n         >>> from PIL import Image\n \n-        >>> model = ChameleonForConditionalGeneration.from_pretrained(\"facebook/chameleon-7b\", torch_dtype=torch.bfloat16)\n+        >>> model = ChameleonForConditionalGeneration.from_pretrained(\"facebook/chameleon-7b\", dtype=torch.bfloat16)\n         >>> processor = ChameleonProcessor.from_pretrained(\"facebook/chameleon-7b\")\n \n         >>> prompt = \"I used to know a lot about constellations when I was younger, but as I grew older, I forgot most of what I knew. These are the only two constellations that I really remember now.<image><image>I would like for you to tell me about 3 more constellations and give me a little bit of history about the constellation.\""
        },
        {
            "sha": "55de46730074b2c918ba60047d659fa3016603d0",
            "filename": "src/transformers/models/colpali/convert_colpali_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Fcolpali%2Fconvert_colpali_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Fcolpali%2Fconvert_colpali_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fconvert_colpali_weights_to_hf.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -130,7 +130,7 @@ def convert_colpali_weights_to_hf(\n \n     # NOTE: The model was initialized with float32 weights. We need to convert it to the desired precision.\n     # There are two ways to set the model's dtype:\n-    # - Using `model.from_pretrained(..., torch_dtype=dtype_precision)` doesn't convert the hyperparameters to the desired precision.\n+    # - Using `model.from_pretrained(..., dtype=dtype_precision)` doesn't convert the hyperparameters to the desired precision.\n     # - Using `model.to(dtype_precision)` converts all values - including the hyperparameters - to the desired precision.\n     # The following snippet allows a fine-grained control over the model's dtype, making sure that all\n     # the new weights' dtypes match the original model."
        },
        {
            "sha": "ca990a6d42d422ea4dc7b1f5179f4ed21d589171",
            "filename": "src/transformers/models/colqwen2/convert_colqwen2_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fconvert_colqwen2_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fconvert_colqwen2_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fconvert_colqwen2_weights_to_hf.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -129,7 +129,7 @@ def convert_colqwen2_weights_to_hf(\n \n     # NOTE: The new model was initialized with float32 weights. We need to convert it to the desired precision.\n     # There are two ways to set the model's dtype:\n-    # - Using `model.from_pretrained(..., torch_dtype=dtype_precision)` doesn't convert the hyperparameters to the desired precision.\n+    # - Using `model.from_pretrained(..., dtype=dtype_precision)` doesn't convert the hyperparameters to the desired precision.\n     # - Using `model.to(dtype_precision)` converts all values - including the hyperparameters - to the desired precision.\n     # The following snippet allows a fine-grained control over the model's dtype, making sure that all\n     # the new weights' dtypes match the original model."
        },
        {
            "sha": "28fbc9fe490d31a8f0d81e29dd877e1ae9f07115",
            "filename": "src/transformers/models/csm/convert_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Fcsm%2Fconvert_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Fcsm%2Fconvert_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fconvert_csm.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -228,7 +228,7 @@ def write_model(\n     # Safety check: reload the converted model\n     gc.collect()\n     print(\"Reloading the model to check if it's saved correctly.\")\n-    CsmForConditionalGeneration.from_pretrained(output_dir, torch_dtype=torch.bfloat16, device_map=\"auto\")\n+    CsmForConditionalGeneration.from_pretrained(output_dir, dtype=torch.bfloat16, device_map=\"auto\")\n     print(\"Model reloaded successfully.\")\n \n "
        },
        {
            "sha": "17b6b2a368cc5f13783ae7b333dc7af78aee9d05",
            "filename": "src/transformers/models/dbrx/configuration_dbrx.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Fdbrx%2Fconfiguration_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Fdbrx%2Fconfiguration_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fconfiguration_dbrx.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -57,7 +57,7 @@ def __init__(\n         self.kv_n_heads = kv_n_heads\n         self.rope_theta = rope_theta\n \n-        for k in [\"model_type\", \"attn_implementation\", \"transformers_version\", \"_commit_hash\", \"torch_dtype\"]:\n+        for k in [\"model_type\", \"attn_implementation\", \"transformers_version\", \"_commit_hash\", \"torch_dtype\", \"dtype\"]:\n             if k in kwargs:\n                 kwargs.pop(k)\n         if len(kwargs) != 0:\n@@ -109,7 +109,7 @@ def __init__(\n         self.moe_loss_weight = moe_loss_weight\n         self.moe_normalize_expert_weights = moe_normalize_expert_weights\n \n-        for k in [\"model_type\", \"attn_implementation\", \"transformers_version\", \"_commit_hash\", \"torch_dtype\"]:\n+        for k in [\"model_type\", \"attn_implementation\", \"transformers_version\", \"_commit_hash\", \"torch_dtype\", \"dtype\"]:\n             if k in kwargs:\n                 kwargs.pop(k)\n         if len(kwargs) != 0:"
        },
        {
            "sha": "1427288878be3353a28ca80ea78b4213dda4dccb",
            "filename": "src/transformers/models/emu3/convert_emu3_weights_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Femu3%2Fconvert_emu3_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Femu3%2Fconvert_emu3_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fconvert_emu3_weights_to_hf.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -313,7 +313,7 @@ def convert_model(vq_model_id, llm_model_id, output_dir, hub_model_id=None, test\n         # Short inference on a few examples to check if generation makes sense\n         print(\"Loading the checkpoint in a Emu3 model...\")\n         print(\"*\" * 100)\n-        model = Emu3ForConditionalGeneration.from_pretrained(output_dir, torch_dtype=torch.bfloat16, device_map=\"auto\")\n+        model = Emu3ForConditionalGeneration.from_pretrained(output_dir, dtype=torch.bfloat16, device_map=\"auto\")\n         processor = Emu3Processor.from_pretrained(output_dir)\n \n         conversation = [\n@@ -348,7 +348,7 @@ def convert_model(vq_model_id, llm_model_id, output_dir, hub_model_id=None, test\n         print(\"*\" * 100)\n     elif test_inference and llm_model_id.endswith(\"Gen\"):\n         processor = Emu3Processor.from_pretrained(output_dir)\n-        model = Emu3ForConditionalGeneration.from_pretrained(output_dir, torch_dtype=torch.bfloat16, device_map=\"auto\")\n+        model = Emu3ForConditionalGeneration.from_pretrained(output_dir, dtype=torch.bfloat16, device_map=\"auto\")\n \n         inputs = processor(\n             text=["
        },
        {
            "sha": "082f480baeb63412e8882d446768b58058499929",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -1273,7 +1273,7 @@ def forward(\n         >>> import requests\n         >>> from PIL import Image\n \n-        >>> model = Emu3ForCausalLM.from_pretrained(\"BAAI/Emu3-Chat-hf\", torch_dtype=torch.bfloat16)\n+        >>> model = Emu3ForCausalLM.from_pretrained(\"BAAI/Emu3-Chat-hf\", dtype=torch.bfloat16)\n         >>> processor = Emu3Processor.from_pretrained(\"BAAI/Emu3-Chat-hf\")\n \n         >>> inputs = processor(text=[\"Can you write me a poem about winter.\"], return_tensors=\"pt\").to(model.device)\n@@ -1545,7 +1545,7 @@ def forward(\n         >>> import requests\n         >>> from PIL import Image\n \n-        >>> model = Emu3ForConditionalGeneration.from_pretrained(\"BAAI/Emu3-Chat-hf\", torch_dtype=torch.bfloat16)\n+        >>> model = Emu3ForConditionalGeneration.from_pretrained(\"BAAI/Emu3-Chat-hf\", dtype=torch.bfloat16)\n         >>> processor = Emu3Processor.from_pretrained(\"BAAI/Emu3-Chat-hf\")\n \n         >>> conversation = ["
        },
        {
            "sha": "730216d969265f1ff29fb0fd1378438369cfb83a",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -878,7 +878,7 @@ def forward(**super_kwargs):\n         >>> import requests\n         >>> from PIL import Image\n \n-        >>> model = Emu3ForCausalLM.from_pretrained(\"BAAI/Emu3-Chat-hf\", torch_dtype=torch.bfloat16)\n+        >>> model = Emu3ForCausalLM.from_pretrained(\"BAAI/Emu3-Chat-hf\", dtype=torch.bfloat16)\n         >>> processor = Emu3Processor.from_pretrained(\"BAAI/Emu3-Chat-hf\")\n \n         >>> inputs = processor(text=[\"Can you write me a poem about winter.\"], return_tensors=\"pt\").to(model.device)\n@@ -1124,7 +1124,7 @@ def forward(\n         >>> import requests\n         >>> from PIL import Image\n \n-        >>> model = Emu3ForConditionalGeneration.from_pretrained(\"BAAI/Emu3-Chat-hf\", torch_dtype=torch.bfloat16)\n+        >>> model = Emu3ForConditionalGeneration.from_pretrained(\"BAAI/Emu3-Chat-hf\", dtype=torch.bfloat16)\n         >>> processor = Emu3Processor.from_pretrained(\"BAAI/Emu3-Chat-hf\")\n \n         >>> conversation = ["
        },
        {
            "sha": "6ec4ba39015b8ba6dcec6ba386bc81c6a7028321",
            "filename": "src/transformers/models/falcon_h1/convert_mamba_ssm_checkpoint.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fconvert_mamba_ssm_checkpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fconvert_mamba_ssm_checkpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fconvert_mamba_ssm_checkpoint.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -41,7 +41,7 @@\n def convert_falcon_h1_to_hf(input_model_path, output_path):\n     tokenizer = AutoTokenizer.from_pretrained(input_model_path)\n \n-    model = AutoModelForCausalLM.from_pretrained(input_model_path, torch_dtype=torch.bfloat16, trust_remote_code=True)\n+    model = AutoModelForCausalLM.from_pretrained(input_model_path, dtype=torch.bfloat16, trust_remote_code=True)\n \n     intermediate_size = int(model.config.expansion_factor * model.config.hidden_size)\n "
        },
        {
            "sha": "de77d4e4c72a460df1938c53f28e800f89ee81d2",
            "filename": "src/transformers/models/florence2/convert_florence2_original_pytorch_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Fflorence2%2Fconvert_florence2_original_pytorch_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Fflorence2%2Fconvert_florence2_original_pytorch_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fconvert_florence2_original_pytorch_to_hf.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -337,7 +337,7 @@ def convert_florence2_checkpoint(hf_model_id, pytorch_dump_folder, output_hub_pa\n \n     hf_config = AutoConfig.from_pretrained(hf_model_id, trust_remote_code=True)\n     hf_model = AutoModelForCausalLM.from_pretrained(\n-        hf_model_id, trust_remote_code=True, torch_dtype=torch.float16, attn_implementation=\"eager\"\n+        hf_model_id, trust_remote_code=True, dtype=torch.float16, attn_implementation=\"eager\"\n     )\n     hf_processor = AutoProcessor.from_pretrained(hf_model_id, trust_remote_code=True)\n     huggingface_weights = OrderedDict()\n@@ -477,7 +477,7 @@ def convert_florence2_checkpoint(hf_model_id, pytorch_dump_folder, output_hub_pa\n         text_config=text_config,\n         vision_config=vision_config,\n         image_token_id=tokenizer.image_token_id,\n-        torch_dtype=torch.float16,\n+        dtype=torch.float16,\n     )\n \n     for stage_idx in range(len(config.vision_config.embed_dim)):"
        },
        {
            "sha": "ac624df78505cb19f183af9b158957b9cfce4b62",
            "filename": "src/transformers/models/gemma/convert_gemma_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Fgemma%2Fconvert_gemma_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Fgemma%2Fconvert_gemma_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fconvert_gemma_weights_to_hf.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -114,7 +114,7 @@ def write_model(save_path, input_base_path, config, safe_serialization=True, pus\n         model = GemmaForCausalLM(config)\n     model.load_state_dict(state_dict, assign=True, strict=False)\n \n-    model.config.torch_dtype = torch.float32\n+    model.config.dtype = torch.float32\n     del model.config._name_or_path\n     print(\"Saving in the Transformers format.\")\n "
        },
        {
            "sha": "ba8705534fd0b1d2fced6ff7789e4e99f77989c3",
            "filename": "src/transformers/models/gemma2/convert_gemma2_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconvert_gemma2_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconvert_gemma2_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconvert_gemma2_weights_to_hf.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -147,7 +147,7 @@ def write_model(save_path, input_base_path, config, safe_serialization=True, pus\n         model = Gemma2ForCausalLM(config)\n     model.load_state_dict(state_dict, assign=True, strict=False)\n \n-    model.config.torch_dtype = torch.float32\n+    model.config.dtype = torch.float32\n     del model.config._name_or_path\n     print(\"Saving in the Transformers format.\")\n "
        },
        {
            "sha": "10afd98396263fa4cc7b047c76468b42ed128d0c",
            "filename": "src/transformers/models/gemma3/convert_gemma3_weights_orbax_to_hf.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconvert_gemma3_weights_orbax_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconvert_gemma3_weights_orbax_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconvert_gemma3_weights_orbax_to_hf.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -479,13 +479,13 @@ def update_tree(path: str, weights: np.ndarray, target_dtype: torch.dtype) -> No\n                 continue\n \n             path, weights = convert_siglip_weight(config=config.vision_config, paths=paths, weights=value)\n-            update_tree(path, weights, config.vision_config.torch_dtype)\n+            update_tree(path, weights, config.vision_config.dtype)\n         else:\n             for path, weights in convert_transformer_weights(config=config.text_config, paths=paths, weights=value):\n                 if config.vision_config is None:\n                     path = path[len(\"language_model.\") :]\n \n-                update_tree(path, weights, config.text_config.torch_dtype)\n+                update_tree(path, weights, config.text_config.dtype)\n \n     if config.vision_config is None:\n         hf_tree[\"lm_head.weight\"] = hf_tree[\"model.embed_tokens.weight\"]\n@@ -502,12 +502,12 @@ def main(*args):\n     variant = _VARIANT.value\n \n     config = _VARIANTS[variant]\n-    config.text_config.torch_dtype = getattr(torch, _TRANSFORMER_DTYPE.value)\n+    config.text_config.dtype = getattr(torch, _TRANSFORMER_DTYPE.value)\n \n     if variant == _VARIANT_GEMMA_3_1B:\n         config.vision_config = None\n     else:\n-        config.vision_config.torch_dtype = getattr(torch, _VISION_DTYPE.value)\n+        config.vision_config.dtype = getattr(torch, _VISION_DTYPE.value)\n \n     if _INCLUDE_CHAT_TEMPLATE.value:\n         # Chat template is included for instruction tuned models, which treat"
        },
        {
            "sha": "867098a4667cab6351578bbaa194b93627eccd6e",
            "filename": "src/transformers/models/gemma3n/convert_gemma3n_weights.py",
            "status": "modified",
            "additions": 14,
            "deletions": 16,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconvert_gemma3n_weights.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconvert_gemma3n_weights.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconvert_gemma3n_weights.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -663,36 +663,34 @@ def update_tree(path: str, weights: np.ndarray, target_dtype: torch.dtype) -> No\n \n     for (path, param), value in tree.flatten_with_path(ckpt):\n         if param == \"audio_input_embedding_extra\":\n-            update_tree(\"model.embed_audio.embedding.weight\", value, config.audio_config.torch_dtype)\n+            update_tree(\"model.embed_audio.embedding.weight\", value, config.audio_config.dtype)\n         elif path.endswith(\"audio_embedding_norm\"):\n-            update_tree(\"model.embed_audio.hard_embedding_norm.weight\", value, config.audio_config.torch_dtype)\n+            update_tree(\"model.embed_audio.hard_embedding_norm.weight\", value, config.audio_config.dtype)\n         elif path.endswith(\"audio_input_projection\"):\n-            update_tree(\n-                \"model.embed_audio.embedding_projection.weight\", value.transpose(), config.audio_config.torch_dtype\n-            )\n+            update_tree(\"model.embed_audio.embedding_projection.weight\", value.transpose(), config.audio_config.dtype)\n         elif path.endswith(\"audio_soft_embedding_norm\"):\n-            update_tree(\"model.embed_audio.soft_embedding_norm.weight\", value, config.audio_config.torch_dtype)\n+            update_tree(\"model.embed_audio.soft_embedding_norm.weight\", value, config.audio_config.dtype)\n         elif param == \"mm_input_embedding_extra\":\n-            update_tree(\"model.embed_vision.embedding.weight\", value, config.vision_config.torch_dtype)\n+            update_tree(\"model.embed_vision.embedding.weight\", value, config.vision_config.dtype)\n         elif path.endswith(\"mm_hard_embedding_norm\"):\n-            update_tree(\"model.embed_vision.hard_embedding_norm.weight\", value, config.vision_config.torch_dtype)\n+            update_tree(\"model.embed_vision.hard_embedding_norm.weight\", value, config.vision_config.dtype)\n         elif path.endswith(\"mm_input_projection\"):\n             update_tree(\n-                \"model.embed_vision.embedding_projection.weight\", value.transpose(), config.vision_config.torch_dtype\n+                \"model.embed_vision.embedding_projection.weight\", value.transpose(), config.vision_config.dtype\n             )\n         elif path.endswith(\"mm_soft_embedding_norm\"):\n-            update_tree(\"model.embed_vision.soft_embedding_norm.weight\", value, config.vision_config.torch_dtype)\n+            update_tree(\"model.embed_vision.soft_embedding_norm.weight\", value, config.vision_config.dtype)\n         elif path.startswith(_TRANSFORMER_PARAMETER):\n             for path, weights in convert_transformer_weights(config.text_config, path, param, value):\n-                update_tree(f\"model.language_model.{path}\", weights, config.text_config.torch_dtype)\n+                update_tree(f\"model.language_model.{path}\", weights, config.text_config.dtype)\n         elif _MOBILE_NET_PREFIX in path:\n             mobilenet_prefix_idx = path.index(_MOBILE_NET_PREFIX)\n             path = path[mobilenet_prefix_idx:]\n             for path, weights in convert_vision_weights(config.vision_config, path, param, value):\n-                update_tree(f\"model.vision_tower.timm_model.{path}\", weights, config.vision_config.torch_dtype)\n+                update_tree(f\"model.vision_tower.timm_model.{path}\", weights, config.vision_config.dtype)\n         elif path.startswith(_AUDIO_ENCODER_PARAMETER):\n             for path, weights in convert_audio_encoder_weights(config.audio_config, path, param, value):\n-                update_tree(f\"model.audio_tower.{path}\", weights, config.audio_config.torch_dtype)\n+                update_tree(f\"model.audio_tower.{path}\", weights, config.audio_config.dtype)\n \n     hf_tree[\"lm_head.weight\"] = hf_tree[\"model.language_model.embed_tokens.weight\"]\n \n@@ -706,9 +704,9 @@ def main(*args):\n     variant = _VARIANT.value\n \n     config = _VARIANTS[variant]\n-    config.audio_config.torch_dtype = getattr(torch, _AUDIO_DTYPE.value)\n-    config.text_config.torch_dtype = getattr(torch, _TRANSFORMER_DTYPE.value)\n-    config.vision_config.torch_dtype = getattr(torch, _VISION_DTYPE.value)\n+    config.audio_config.dtype = getattr(torch, _AUDIO_DTYPE.value)\n+    config.text_config.dtype = getattr(torch, _TRANSFORMER_DTYPE.value)\n+    config.vision_config.dtype = getattr(torch, _VISION_DTYPE.value)\n     if _INCLUDE_CHAT_TEMPLATE.value:\n         # Chat template is included for instruction tuned models, which treat\n         # both \"<eos>\" and \"<end_of_turn>\" as generation stoppers."
        },
        {
            "sha": "964bab4e6d4e324ea973c42889e1f5c71e92b40d",
            "filename": "src/transformers/models/glm4v/convert_glm4v_mgt_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconvert_glm4v_mgt_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconvert_glm4v_mgt_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconvert_glm4v_mgt_weights_to_hf.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -589,7 +589,7 @@ def merge_tp_weights(model_path, output_path, vllm_config_path=None):\n         \"rms_norm_eps\": model_config.get(\"layernorm_epsilon\", 1e-05),\n         \"rope_theta\": model_config.get(\"rotary_base\", 10000.0),\n         \"tie_word_embeddings\": False,\n-        \"torch_dtype\": model_config.get(\"torch_dtype\", \"bfloat16\"),\n+        \"dtype\": model_config.get(\"dtype\", \"bfloat16\"),\n         \"transformers_version\": \"4.53.0dev\",\n         \"use_cache\": model_config.get(\"use_cache\", True),\n         \"vocab_size\": model_config.get(\"vocab_size\", 151552),"
        },
        {
            "sha": "1bf7ed2aebb364c644d3177462c1a85874da3cd4",
            "filename": "src/transformers/models/gpt_oss/convert_gpt_oss_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fconvert_gpt_oss_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8f6d3790acdd7033e199875b5b57691adf10735/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fconvert_gpt_oss_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fconvert_gpt_oss_weights_to_hf.py?ref=d8f6d3790acdd7033e199875b5b57691adf10735",
            "patch": "@@ -265,7 +265,7 @@ def write_model(\n \n     gc.collect()\n     print(\"Reloading the model to check if it's saved correctly.\")\n-    GptOssForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, device_map=\"auto\")\n+    GptOssForCausalLM.from_pretrained(model_path, dtype=torch.bfloat16, device_map=\"auto\")\n     print(\"Model reloaded successfully.\")\n \n     # generation config"
        }
    ],
    "stats": {
        "total": 4361,
        "additions": 2198,
        "deletions": 2163
    }
}