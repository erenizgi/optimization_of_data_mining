{
    "author": "richardodliu",
    "message": "Add cosine_with_min_lr_schedule_with_warmup_lr_rate scheduler in Trainer (#31870)\n\n* add cosine_with_min_lr_schedule_with_warmup_lr_rate scheduler in trainer\n\n* Update src/transformers/optimization.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update optimization.py\n\nfix the error of the unclosed \"(\"\n\n* Update optimization.py\n\nremove whitespace in line 402 in order to pass the quality test\n\n* Update src/transformers/optimization.py\n\n* Update src/transformers/optimization.py\n\n* Apply style fixes\n\n---------\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\nCo-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "e048d48bd0f87a0331020f55966e715faf0671d4",
    "files": [
        {
            "sha": "1c71487dbb620a3bf9eddfd88e9486dce41d7d42",
            "filename": "src/transformers/optimization.py",
            "status": "modified",
            "additions": 82,
            "deletions": 0,
            "changes": 82,
            "blob_url": "https://github.com/huggingface/transformers/blob/e048d48bd0f87a0331020f55966e715faf0671d4/src%2Ftransformers%2Foptimization.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e048d48bd0f87a0331020f55966e715faf0671d4/src%2Ftransformers%2Foptimization.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Foptimization.py?ref=e048d48bd0f87a0331020f55966e715faf0671d4",
            "patch": "@@ -384,6 +384,87 @@ def get_cosine_with_min_lr_schedule_with_warmup(\n     return LambdaLR(optimizer, lr_lambda, last_epoch)\n \n \n+def _get_cosine_with_min_lr_schedule_with_warmup_lr_rate_lambda(\n+    current_step: int,\n+    *,\n+    num_warmup_steps: int,\n+    num_training_steps: int,\n+    num_cycles: float,\n+    min_lr_rate: float = 0.0,\n+    warmup_lr_rate: Optional[float] = None,\n+):\n+    current_step = float(current_step)\n+    num_warmup_steps = float(num_warmup_steps)\n+    num_training_steps = float(num_training_steps)\n+\n+    if current_step < num_warmup_steps:\n+        if warmup_lr_rate is None:\n+            return (current_step + 1.0) / max(1.0, num_warmup_steps)\n+        else:\n+            warmup_lr_rate = float(warmup_lr_rate)\n+            return warmup_lr_rate + (1.0 - warmup_lr_rate) * (current_step) / (max(1, num_warmup_steps - 1))\n+    progress = (current_step - num_warmup_steps + 1.0) / (max(1.0, num_training_steps - num_warmup_steps))\n+    factor = 0.5 * (1.0 + math.cos(math.pi * num_cycles * 2.0 * progress))\n+    factor = factor * (1 - min_lr_rate) + min_lr_rate\n+    return max(0, factor)\n+\n+\n+def get_cosine_with_min_lr_schedule_with_warmup_lr_rate(\n+    optimizer: Optimizer,\n+    num_warmup_steps: int,\n+    num_training_steps: int,\n+    num_cycles: float = 0.5,\n+    last_epoch: int = -1,\n+    min_lr: Optional[float] = None,\n+    min_lr_rate: Optional[float] = None,\n+    warmup_lr_rate: Optional[float] = None,\n+):\n+    \"\"\"\n+    Create a schedule with a learning rate that decreases following the values of the cosine function between the\n+    initial lr set in the optimizer to min_lr, after a warmup period during which it increases linearly between 0 and the\n+    initial lr set in the optimizer.\n+\n+    Args:\n+        optimizer ([`~torch.optim.Optimizer`]):\n+            The optimizer for which to schedule the learning rate.\n+        num_warmup_steps (`int`):\n+            The number of steps for the warmup phase.\n+        num_training_steps (`int`):\n+            The total number of training steps.\n+        num_cycles (`float`, *optional*, defaults to 0.5):\n+            The number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0\n+            following a half-cosine).\n+        last_epoch (`int`, *optional*, defaults to -1):\n+            The index of the last epoch when resuming training.\n+        min_lr (`float`, *optional*):\n+            The minimum learning rate to reach after the cosine schedule.\n+        min_lr_rate (`float`, *optional*):\n+            The minimum learning rate as a ratio of the initial learning rate. If set, `min_lr` should not be set.\n+        warmup_lr_rate (`float`, *optional*):\n+            The minimum learning rate as a ratio of the start learning rate. If not set, `warmup_lr_rate` will be treated as float(1/num_warmup_steps).\n+\n+    Return:\n+        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n+    \"\"\"\n+\n+    if min_lr is not None and min_lr_rate is not None:\n+        raise ValueError(\"Only one of min_lr or min_lr_rate should be set\")\n+    elif min_lr is not None:\n+        min_lr_rate = min_lr / optimizer.defaults[\"lr\"]\n+    elif min_lr_rate is None:\n+        raise ValueError(\"One of min_lr or min_lr_rate should be set through the `lr_scheduler_kwargs`\")\n+\n+    lr_lambda = partial(\n+        _get_cosine_with_min_lr_schedule_with_warmup_lr_rate_lambda,\n+        num_warmup_steps=num_warmup_steps,\n+        num_training_steps=num_training_steps,\n+        num_cycles=num_cycles,\n+        min_lr_rate=min_lr_rate,\n+        warmup_lr_rate=warmup_lr_rate,\n+    )\n+    return LambdaLR(optimizer, lr_lambda, last_epoch)\n+\n+\n def _get_wsd_scheduler_lambda(\n     current_step: int,\n     *,\n@@ -505,6 +586,7 @@ def get_wsd_schedule(\n     SchedulerType.INVERSE_SQRT: get_inverse_sqrt_schedule,\n     SchedulerType.REDUCE_ON_PLATEAU: get_reduce_on_plateau_schedule,\n     SchedulerType.COSINE_WITH_MIN_LR: get_cosine_with_min_lr_schedule_with_warmup,\n+    SchedulerType.COSINE_WARMUP_WITH_MIN_LR: get_cosine_with_min_lr_schedule_with_warmup_lr_rate,\n     SchedulerType.WARMUP_STABLE_DECAY: get_wsd_schedule,\n }\n "
        },
        {
            "sha": "317e50fb68b3a274a84229f4527916c24bc7a089",
            "filename": "src/transformers/trainer_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e048d48bd0f87a0331020f55966e715faf0671d4/src%2Ftransformers%2Ftrainer_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e048d48bd0f87a0331020f55966e715faf0671d4/src%2Ftransformers%2Ftrainer_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_utils.py?ref=e048d48bd0f87a0331020f55966e715faf0671d4",
            "patch": "@@ -444,6 +444,7 @@ class SchedulerType(ExplicitEnum):\n     INVERSE_SQRT = \"inverse_sqrt\"\n     REDUCE_ON_PLATEAU = \"reduce_lr_on_plateau\"\n     COSINE_WITH_MIN_LR = \"cosine_with_min_lr\"\n+    COSINE_WARMUP_WITH_MIN_LR = \"cosine_warmup_with_min_lr\"\n     WARMUP_STABLE_DECAY = \"warmup_stable_decay\"\n \n "
        },
        {
            "sha": "e91ff1e21d882e1acdc89e54739e0956b82b7c71",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/e048d48bd0f87a0331020f55966e715faf0671d4/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e048d48bd0f87a0331020f55966e715faf0671d4/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=e048d48bd0f87a0331020f55966e715faf0671d4",
            "patch": "@@ -1143,6 +1143,34 @@ def test_cosine_with_min_lr_scheduler(self):\n                 trainer.lr_scheduler.step()\n             self.assertEqual(trainer.lr_scheduler.get_last_lr()[0], 1e-5)\n \n+    def test_cosine_with_min_lr_schedule_with_warmup_lr_rate(self):\n+        train_dataset = RegressionDataset()\n+        model = RegressionModel()\n+        num_steps, num_warmup_steps = 10, 2\n+        extra_kwargs = {\"min_lr\": 1e-5}  # Non-default arguments\n+        args = TrainingArguments(\n+            \"./regression\",\n+            lr_scheduler_type=\"cosine_warmup_with_min_lr\",\n+            lr_scheduler_kwargs=extra_kwargs,\n+            learning_rate=0.2,\n+            warmup_steps=num_warmup_steps,\n+            report_to=\"none\",\n+        )\n+        trainer = Trainer(model, args, train_dataset=train_dataset)\n+        trainer.create_optimizer_and_scheduler(num_training_steps=num_steps)\n+\n+        # Checking that the scheduler was created\n+        self.assertIsNotNone(trainer.lr_scheduler)\n+\n+        # Check the last learning rate\n+        step_lrs = []\n+        for _ in range(num_steps):\n+            step_lrs.append(trainer.optimizer.param_groups[0][\"lr\"])\n+            trainer.lr_scheduler.step()\n+        self.assertEqual(step_lrs[0], 0.1)\n+        self.assertEqual(step_lrs[1], 0.2)\n+        self.assertEqual(step_lrs[-1], 1e-05)\n+\n     def test_reduce_lr_on_plateau_args(self):\n         # test passed arguments for a custom ReduceLROnPlateau scheduler\n         train_dataset = RegressionDataset(length=64)"
        }
    ],
    "stats": {
        "total": 111,
        "additions": 111,
        "deletions": 0
    }
}