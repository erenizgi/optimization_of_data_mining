{
    "author": "Judy-Choi",
    "message": "ğŸŒ [i18n-KO] Translated `models.md` to Korean (#39518)\n\n* docs: ko: models.md\n\n* feat: nmt draft\n\n* fix: manual edits\n\n* Resolved _toctree.yaml conflict during merge from main\n\n* Apply suggestions from code review\n\nCo-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>\nCo-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>\n\n* Apply suggestions from code review\n\n* fix: update toctree\n\n* Update docs/source/ko/_toctree.yml\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>\nCo-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "c81723d31b8a532803feee2bbce66e4f03829759",
    "files": [
        {
            "sha": "b08e2c7cbc98f250fa8f57b8533e9e5488e9d579",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c81723d31b8a532803feee2bbce66e4f03829759/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/c81723d31b8a532803feee2bbce66e4f03829759/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=c81723d31b8a532803feee2bbce66e4f03829759",
            "patch": "@@ -9,8 +9,8 @@\n - isExpanded: false\n   sections:\n   - sections:\n-    - local: in_translation\n-      title: (ë²ˆì—­ì¤‘) Loading models\n+    - local: models\n+      title: ëª¨ë¸ ë¡œë“œí•˜ê¸°\n     - local: custom_models\n       title: ì‚¬ìš©ì ì •ì˜ ëª¨ë¸ ê³µìœ í•˜ê¸°\n     - local: how_to_hack_models\n@@ -1243,4 +1243,4 @@\n     - local: in_translation\n       title: (ë²ˆì—­ì¤‘)Environment Variables\n     title: Reference\n-  title: API\n+  title: API\n\\ No newline at end of file"
        },
        {
            "sha": "d3a258db9390aac628698667aeac010dcc6f8a47",
            "filename": "docs/source/ko/models.md",
            "status": "added",
            "additions": 321,
            "deletions": 0,
            "changes": 321,
            "blob_url": "https://github.com/huggingface/transformers/blob/c81723d31b8a532803feee2bbce66e4f03829759/docs%2Fsource%2Fko%2Fmodels.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c81723d31b8a532803feee2bbce66e4f03829759/docs%2Fsource%2Fko%2Fmodels.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodels.md?ref=c81723d31b8a532803feee2bbce66e4f03829759",
            "patch": "@@ -0,0 +1,321 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# ëª¨ë¸ ë¡œë“œí•˜ê¸°[[loading-models]]\n+\n+TransformersëŠ” í•œ ì¤„ì˜ ì½”ë“œë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë§ì€ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ì œê³µí•©ë‹ˆë‹¤. ëª¨ë¸ í´ë˜ìŠ¤ì™€ [`~PreTrainedModel.from_pretrained`] ë©”ì†Œë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.\n+\n+[`~PreTrainedModel.from_pretrained`]ë¥¼ í˜¸ì¶œí•˜ì—¬ Hugging Face [Hub](https://hf.co/models)ì— ì €ì¥ëœ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ì™€ êµ¬ì„±ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ë¡œë“œí•˜ì„¸ìš”.\n+\n+> [!TIP]\n+> [`~PreTrainedModel.from_pretrained`] ë©”ì†Œë“œëŠ” [safetensors](https://hf.co/docs/safetensors/index) íŒŒì¼ í˜•ì‹ìœ¼ë¡œ ì €ì¥ëœ ê°€ì¤‘ì¹˜ê°€ ìˆìœ¼ë©´ ì´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. ì „í†µì ìœ¼ë¡œ PyTorch ëª¨ë¸ ê°€ì¤‘ì¹˜ëŠ” ë³´ì•ˆì— ì·¨ì•½í•œ ê²ƒìœ¼ë¡œ ì•Œë ¤ì§„ [pickle](https://docs.python.org/3/library/pickle.html) ìœ í‹¸ë¦¬í‹°ë¡œ ì§ë ¬í™”ë©ë‹ˆë‹¤. Safetensor íŒŒì¼ì€ ë” ì•ˆì „í•˜ê³  ë¡œë“œ ì†ë„ê°€ ë¹ ë¦…ë‹ˆë‹¤.\n+\n+```py\n+from transformers import AutoModelForCausalLM\n+\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=\"auto\", device_map=\"auto\")\n+```\n+\n+ì´ ê°€ì´ë“œëŠ” ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ë°©ë²•, ë‹¤ì–‘í•œ ë¡œë”© ë°©ì‹, ë§¤ìš° í° ëª¨ë¸ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë©”ëª¨ë¦¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë°©ë²•, ê·¸ë¦¬ê³  ì‚¬ìš©ì ì •ì˜ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.\n+\n+## ëª¨ë¸ê³¼ êµ¬ì„±[[models-and-configurations]]\n+\n+ëª¨ë“  ëª¨ë¸ì—ëŠ” ì€ë‹‰ ë ˆì´ì–´ ìˆ˜, ì–´íœ˜ ì‚¬ì „ í¬ê¸°, í™œì„±í™” í•¨ìˆ˜ ë“±ê³¼ ê°™ì€ íŠ¹ì • ì†ì„±ì´ í¬í•¨ëœ `configuration.py` íŒŒì¼ì´ ìˆìŠµë‹ˆë‹¤. ë˜í•œ ê° ë ˆì´ì–´ì˜ ì •ì˜ì™€ ê°ê°ì˜ ë ˆì´ì–´ ì•ˆì—ì„œ ì¼ì–´ë‚˜ëŠ” ìˆ˜í•™ì  ì—°ì‚°ì„ ì •ì˜í•˜ëŠ” `modeling.py` íŒŒì¼ë„ ìˆìŠµë‹ˆë‹¤. `modeling.py` íŒŒì¼ì€ `configuration.py`ì— ì •ì˜ëœ ëª¨ë¸ ì†ì„±ì„ ë°”íƒ•ìœ¼ë¡œ ëª¨ë¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤. ì´ ë‹¨ê³„ì—ì„œëŠ” ì•„ì§ í•™ìŠµë˜ì§€ ì•Šì€ ë¬´ì‘ìœ„ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§„ ìƒíƒœì´ê¸° ë•Œë¬¸ì—, ì˜ë¯¸ ìˆëŠ” ì¶œë ¥ì„ ì–»ê¸° ìœ„í•´ì„œëŠ” í•™ìŠµì´ í•„ìš”í•©ë‹ˆë‹¤.\n+\n+<!-- insert diagram of model and configuration -->\n+\n+> [!TIP]\n+> *ì•„í‚¤í…ì²˜(Architecture)*ëŠ” ëª¨ë¸ì˜ ê³¨ê²©ì„ ì˜ë¯¸í•˜ê³  *ì²´í¬í¬ì¸íŠ¸(checkpoint)*ëŠ” ì£¼ì–´ì§„ ì•„í‚¤í…ì²˜ì— ëŒ€í•œ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, [BERT](./model_doc/bert)ëŠ” ì•„í‚¤í…ì²˜ì´ê³  [google-bert/bert-base-uncased](https://huggingface.co/google-bert/bert-base-uncased)ëŠ” í•´ë‹¹ ì•„í‚¤í…ì²˜ì˜ ì²´í¬í¬ì¸íŠ¸(checkpoint)ì…ë‹ˆë‹¤. *ëª¨ë¸*ì´ë¼ëŠ” ìš©ì–´ëŠ” ì•„í‚¤í…ì²˜ ë° ì²´í¬í¬ì¸íŠ¸(checkpoint)ì™€ í˜¼ìš©í•˜ì—¬ ì‚¬ìš©ë˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+ë¡œë“œí•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì€ ì¼ë°˜ì ìœ¼ë¡œ ë‘ ê°€ì§€ íƒ€ì…ì´ ìˆìŠµë‹ˆë‹¤.\n+\n+1. ì€ë‹‰ ìƒíƒœë¥¼ ì¶œë ¥í•˜ëŠ” [`AutoModel`] ë˜ëŠ” [`LlamaModel`]ê³¼ ê°™ì€ ê¸°ë³¸ ëª¨ë¸ì…ë‹ˆë‹¤.\n+2. íŠ¹ì • ì‘ì—…ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ íŠ¹ì • *í—¤ë“œ*ê°€ ë¶™ì€ [`AutoModelForCausalLM`] ë˜ëŠ” [`LlamaForCausalLM`]ê³¼ ê°™ì€ ëª¨ë¸ì…ë‹ˆë‹¤.\n+\n+ê° ëª¨ë¸ íƒ€ì…ë§ˆë‹¤, ê°ê°ì˜ ê¸°ê³„í•™ìŠµ í”„ë ˆì„ì›Œí¬(PyTorch, TensorFlow, Flax)ë¥¼ ìœ„í•œ ë³„ë„ì˜ í´ë˜ìŠ¤ê°€ ìˆìŠµë‹ˆë‹¤. ì‚¬ìš© ì¤‘ì¸ í”„ë ˆì„ì›Œí¬ì— í•´ë‹¹í•˜ëŠ” ì ‘ë‘ì–´(prefix)ë¥¼ ì„ íƒí•˜ì„¸ìš”.\n+\n+<hfoptions id=\"backend\">\n+<hfoption id=\"PyTorch\">\n+\n+```py\n+from transformers import AutoModelForCausalLM, MistralForCausalLM\n+\n+# AutoClass ë˜ëŠ” ëª¨ë¸ë³„ í´ë˜ìŠ¤(model-specific class) ë¥¼ ì´ìš©í•´ ë¡œë“œ\n+model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", torch_dtype=\"auto\", device_map=\"auto\")\n+model = MistralForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", torch_dtype=\"auto\", device_map=\"auto\")\n+```\n+\n+</hfoption>\n+<hfoption id=\"TensorFlow\">\n+\n+```py\n+from transformers import TFAutoModelForCausalLM, TFMistralForCausalLM\n+\n+# AutoClass ë˜ëŠ” ëª¨ë¸ë³„ í´ë˜ìŠ¤(model-specific class) ë¥¼ ì´ìš©í•´ ë¡œë“œ\n+model = TFAutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n+model = TFMistralForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n+```\n+\n+</hfoption>\n+<hfoption id=\"Flax\">\n+\n+```py\n+from transformers import FlaxAutoModelForCausalLM, FlaxMistralForCausalLM\n+\n+# AutoClass ë˜ëŠ” ëª¨ë¸ë³„ í´ë˜ìŠ¤(model-specific class) ë¥¼ ì´ìš©í•´ ë¡œë“œ\n+model = FlaxAutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n+model = FlaxMistralForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+## ëª¨ë¸ í´ë˜ìŠ¤[[model-classes]]\n+\n+ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ê°€ì ¸ì˜¤ë ¤ë©´ ëª¨ë¸ì— ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤. ì´ëŠ” Hugging Face Hub ë˜ëŠ” ë¡œì»¬ ë””ë ‰í„°ë¦¬ì—ì„œ ê°€ì¤‘ì¹˜ë¥¼ ë°›ì•„ë“¤ì´ëŠ” [`~PreTrainedModel.from_pretrained`]ë¥¼ í˜¸ì¶œí•˜ì—¬ ìˆ˜í–‰ë©ë‹ˆë‹¤.\n+\n+ë‘ ê°€ì§€ ëª¨ë¸ í´ë˜ìŠ¤ë¡œ [AutoModel](./model_doc/auto) í´ë˜ìŠ¤ì™€ ëª¨ë¸ë³„ í´ë˜ìŠ¤ê°€ ìˆìŠµë‹ˆë‹¤.\n+\n+<hfoptions id=\"model-classes\">\n+<hfoption id=\"AutoModel\">\n+\n+<Youtube id=\"AhChOFRegn4\"/>\n+\n+[AutoModel](./model_doc/auto) í´ë˜ìŠ¤ëŠ” ì •í™•í•œ ëª¨ë¸ í´ë˜ìŠ¤ ì´ë¦„ì„ ëª°ë¼ë„ ì•„í‚¤í…ì²˜ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆëŠ” í¸ë¦¬í•œ ë°©ë²•ì…ë‹ˆë‹¤. ë§ì€ ëª¨ë¸ì´ ì œê³µë˜ê¸° ë•Œë¬¸ì—, ì´ í´ë˜ìŠ¤ëŠ” êµ¬ì„± íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ ì˜¬ë°”ë¥¸ ëª¨ë¸ í´ë˜ìŠ¤ë¥¼ ìë™ìœ¼ë¡œ ì„ íƒí•´ ì¤ë‹ˆë‹¤. ì›í•˜ëŠ” ì‘ì—…ê³¼ ì‚¬ìš©í•˜ë ¤ëŠ” ì²´í¬í¬ì¸íŠ¸ë§Œ ì•Œê³  ìˆìœ¼ë©´ ë©ë‹ˆë‹¤.\n+\n+ì£¼ì–´ì§„ ì‘ì—…ì„ ì•„í‚¤í…ì²˜ê°€ ì§€ì›í•˜ëŠ” í•œ, ëª¨ë¸ì´ë‚˜ ì‘ì—…ì„ ì‰½ê²Œ ì „í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+ì˜ˆë¥¼ ë“¤ì–´, ë™ì¼í•œ ëª¨ë¸ì„ ì„œë¡œ ë‹¤ë¥¸ ì‘ì—…ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+```py\n+from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification, AutoModelForQuestionAnswering\n+\n+# ë™ì¼í•œ APIë¥¼ 3ê°€ì§€ ë‹¤ë¥¸ ì‘ì—…ì— ì‚¬ìš©\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n+model = AutoModelForSequenceClassification.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n+model = AutoModelForQuestionAnswering.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n+```\n+\n+ë‹¤ë¥¸ ê²½ìš°ì—ëŠ”, í•˜ë‚˜ì˜ ì‘ì—…ì— ëŒ€í•´ ì—¬ëŸ¬ ê°€ì§€ ëª¨ë¸ì„ ë¹ ë¥´ê²Œ ì‹œí—˜í•´ë³´ê³  ì‹¶ì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n+\n+```py\n+from transformers import AutoModelForCausalLM\n+\n+# ë™ì¼í•œ APIë¥¼ ì‚¬ìš©í•˜ì—¬ 3ê°€ì§€ ë‹¤ë¥¸ ëª¨ë¸ ë¡œë“œ\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n+model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n+model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\")\n+```\n+\n+</hfoption>\n+<hfoption id=\"model-specific class\">\n+\n+[AutoModel](./model_doc/auto) í´ë˜ìŠ¤ëŠ” ëª¨ë¸ë³„ í´ë˜ìŠ¤ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¶•ë©ë‹ˆë‹¤. íŠ¹ì • ì‘ì—…ì„ ì§€ì›í•˜ëŠ” ëª¨ë“  ëª¨ë¸ í´ë˜ìŠ¤ë“¤ì€ ê°ê°ì˜ `AutoModelFor` ì‘ì—… í´ë˜ìŠ¤ì— ë§¤í•‘ë©ë‹ˆë‹¤.\n+\n+ì´ë¯¸ ì‚¬ìš©í•˜ë ¤ëŠ” ëª¨ë¸ í´ë˜ìŠ¤ë¥¼ ì•Œê³  ìˆë‹¤ë©´ í•´ë‹¹ ëª¨ë¸ë³„ í´ë˜ìŠ¤ë¥¼ ì§ì ‘ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+```py\n+from transformers import LlamaModel, LlamaForCausalLM\n+\n+model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+## ëŒ€ê·œëª¨ ëª¨ë¸[[large-models]]\n+\n+ëŒ€ê·œëª¨ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì€ ë¡œë“œí•˜ëŠ” ë° ë§ì€ ë©”ëª¨ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. ë¡œë“œ ê³¼ì •ì€ ë‹¤ìŒì„ í¬í•¨í•©ë‹ˆë‹¤:\n+\n+1. ë¬´ì‘ìœ„ ê°€ì¤‘ì¹˜ë¡œ ëª¨ë¸ ìƒì„±\n+2. ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ\n+3. ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ë¥¼ ëª¨ë¸ì— ì ìš©\n+\n+ëª¨ë¸ ê°€ì¤‘ì¹˜ì˜ ë³µì‚¬ë³¸ ë‘ ê°€ì§€(ë¬´ì‘ìœ„ ê°€ì¤‘ì¹˜ì™€ ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜)ë¥¼ ë³´ê´€í•  ìˆ˜ ìˆëŠ” ì¶©ë¶„í•œ ë©”ëª¨ë¦¬ê°€ í•„ìš”í•˜ë©°, ì´ëŠ” ë³´ìœ í•œ í•˜ë“œì›¨ì–´ì— ë”°ë¼ ë¶ˆê°€ëŠ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¶„ì‚° í•™ìŠµ í™˜ê²½ì—ì„œëŠ” ê° í”„ë¡œì„¸ìŠ¤ê°€ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ë¡œë“œí•˜ê¸° ë•Œë¬¸ì— ì´ëŠ” ë”ìš± ì–´ë ¤ìš´ ê³¼ì œì…ë‹ˆë‹¤.\n+\n+transformersëŠ” ë¹ ë¥¸ ì´ˆê¸°í™”, ë¶„í• ëœ ì²´í¬í¬ì¸íŠ¸, Accelerateì˜ [Big Model Inference](https://hf.co/docs/accelerate/usage_guides/big_modeling) ê¸°ëŠ¥, ê·¸ë¦¬ê³  ë” ë‚®ì€ ë¹„íŠ¸ ë°ì´í„° íƒ€ì… ì§€ì›ì„ í†µí•´ ì´ëŸ¬í•œ ë©”ëª¨ë¦¬ ê´€ë ¨ ë¬¸ì œë“¤ì„ ì¼ë¶€ ì¤„ì—¬ì¤ë‹ˆë‹¤.\n+\n+\n+### ë¶„í• ëœ ì²´í¬í¬ì¸íŠ¸[[sharded-checkpoints]]\n+\n+[`~PreTrainedModel.save_pretrained`] ë©”ì†Œë“œëŠ” 10GBë³´ë‹¤ í° ì²´í¬í¬ì¸íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ìƒ¤ë“œí•©ë‹ˆë‹¤.\n+\n+ê° ìƒ¤ë“œ(shard)ëŠ” ì´ì „ ìƒ¤ë“œê°€ ë¡œë“œëœ í›„ ìˆœì°¨ì ìœ¼ë¡œ ë¡œë“œë˜ì–´, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ëª¨ë¸ í¬ê¸°ì™€ ê°€ì¥ í° ìƒ¤ë“œ í¬ê¸°ë¡œë§Œ ì œí•œí•©ë‹ˆë‹¤.\n+\n+`max_shard_size` ë§¤ê°œë³€ìˆ˜ëŠ” ê° ìƒ¤ë“œì— ëŒ€í•´ ê¸°ë³¸ì ìœ¼ë¡œ 5GBë¡œ ì„¤ì •ë˜ì–´ ìˆëŠ”ë°, ì´ëŠ” ë©”ëª¨ë¦¬ ë¶€ì¡± ì—†ì´ ë¬´ë£Œ ë“±ê¸‰ GPU ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ë” ì‰½ê²Œ ì‹¤í–‰í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n+\n+ì˜ˆë¥¼ ë“¤ì–´, [`~PreTrainedModel.save_pretrained`]ì—ì„œ [BioMistral/BioMistral-7B](https://hf.co/BioMistral/BioMistral-7B)ì— ëŒ€í•œ ë¶„í• ëœ ì²´í¬í¬ì¸íŠ¸ë¥¼ ìƒì„±í•´ë³´ê² ìŠµë‹ˆë‹¤.\n+\n+```py\n+from transformers import AutoModel\n+import tempfile\n+import os\n+\n+model = AutoModel.from_pretrained(\"biomistral/biomistral-7b\")\n+with tempfile.TemporaryDirectory() as tmp_dir:\n+    model.save_pretrained(tmp_dir, max_shard_size=\"5GB\")\n+    print(sorted(os.listdir(tmp_dir)))\n+```\n+\n+[`~PreTrainedModel.from_pretrained`]ë¡œ ë¶„í• ëœ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë‹¤ì‹œ ë¡œë“œí•©ë‹ˆë‹¤.\n+\n+```py\n+with tempfile.TemporaryDirectory() as tmp_dir:\n+    model.save_pretrained(tmp_dir)\n+    new_model = AutoModel.from_pretrained(tmp_dir)\n+```\n+\n+ë¶„í• ëœ ì²´í¬í¬ì¸íŠ¸ëŠ” [`~transformers.modeling_utils.load_sharded_checkpoint`]ë¡œë„ ì§ì ‘ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+```py\n+from transformers.modeling_utils import load_sharded_checkpoint\n+\n+with tempfile.TemporaryDirectory() as tmp_dir:\n+    model.save_pretrained(tmp_dir, max_shard_size=\"5GB\")\n+    load_sharded_checkpoint(model, tmp_dir)\n+```\n+\n+[`~PreTrainedModel.save_pretrained`] ë©”ì†Œë“œëŠ” ë§¤ê°œë³€ìˆ˜ ì´ë¦„ì„ ì €ì¥ëœ íŒŒì¼ì— ë§¤í•‘í•˜ëŠ” ì¸ë±ìŠ¤ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤. ì¸ë±ìŠ¤ íŒŒì¼ì—ëŠ” `metadata`ì™€ `weight_map`ì´ë¼ëŠ” ë‘ ê°œì˜ í‚¤ê°€ ìˆìŠµë‹ˆë‹¤.\n+\n+```py\n+import json\n+\n+with tempfile.TemporaryDirectory() as tmp_dir:\n+    model.save_pretrained(tmp_dir, max_shard_size=\"5GB\")\n+    with open(os.path.join(tmp_dir, \"model.safetensors.index.json\"), \"r\") as f:\n+        index = json.load(f)\n+\n+print(index.keys())\n+```\n+\n+`metadata` í‚¤ëŠ” ì „ì²´ ëª¨ë¸ í¬ê¸°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n+\n+```py\n+index[\"metadata\"]\n+{'total_size': 28966928384}\n+```\n+\n+`weight_map` í‚¤ëŠ” ê° ë§¤ê°œë³€ìˆ˜ë¥¼ ì €ì¥ëœ ìƒ¤ë“œì— ë§¤í•‘í•©ë‹ˆë‹¤.\n+\n+```py\n+index[\"weight_map\"]\n+{'lm_head.weight': 'model-00006-of-00006.safetensors',\n+ 'model.embed_tokens.weight': 'model-00001-of-00006.safetensors',\n+ 'model.layers.0.input_layernorm.weight': 'model-00001-of-00006.safetensors',\n+ 'model.layers.0.mlp.down_proj.weight': 'model-00001-of-00006.safetensors',\n+ ...\n+}\n+```\n+\n+### ëŒ€í˜• ëª¨ë¸ ì¶”ë¡ [[big-model-inference]]\n+\n+> [!TIP]\n+> ì´ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ Accelerate v0.9.0 ë° PyTorch v1.9.0 ì´ìƒì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”!\n+\n+<Youtube id=\"MWCSGj9jEAo\"/>\n+\n+[`~PreTrainedModel.from_pretrained`]ëŠ” Accelerateì˜ [ëŒ€í˜• ëª¨ë¸ ì¶”ë¡ ](https://hf.co/docs/accelerate/usage_guides/big_modeling) ê¸°ëŠ¥ìœ¼ë¡œ ê°•í™”ë˜ì—ˆìŠµë‹ˆë‹¤.\n+\n+ëŒ€í˜• ëª¨ë¸ ì¶”ë¡ ì€ PyTorch [meta](https://pytorch.org/docs/main/meta.html) ì¥ì¹˜ì—ì„œ *ëª¨ë¸ ìŠ¤ì¼ˆë ˆí†¤*ì„ ìƒì„±í•©ë‹ˆë‹¤. meta ì¥ì¹˜ëŠ” ì‹¤ì œ ë°ì´í„°ë¥¼ ì €ì¥í•˜ì§€ ì•Šê³  ë©”íƒ€ë°ì´í„°ë§Œ ì €ì¥í•©ë‹ˆë‹¤.\n+\n+ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”ëœ ê°€ì¤‘ì¹˜ëŠ” ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ê°€ ë¡œë“œë  ë•Œë§Œ ìƒì„±ë˜ì–´ ë©”ëª¨ë¦¬ì— ë™ì‹œì— ëª¨ë¸ì˜ ë‘ ë³µì‚¬ë³¸ì„ ìœ ì§€í•˜ëŠ” ê²ƒì„ ë°©ì§€í•©ë‹ˆë‹¤. ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì€ ëª¨ë¸ í¬ê¸°ë§Œí¼ì…ë‹ˆë‹¤.\n+\n+> [!TIP]\n+> ì¥ì¹˜ í• ë‹¹ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ [ì¥ì¹˜ ë§µ ì„¤ê³„í•˜ê¸°](https://hf.co/docs/accelerate/v0.33.0/en/concept_guides/big_model_inference#designing-a-device-map)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n+\n+ëŒ€í˜• ëª¨ë¸ ì¶”ë¡ ì˜ ë‘ ë²ˆì§¸ ê¸°ëŠ¥ì€ ë¶ˆëŸ¬ì˜¨ ê°€ì¤‘ì¹˜ê°€ ëª¨ë¸ ìŠ¤ì¼ˆë ˆí†¤ì— í• ë‹¹ë˜ëŠ” ë°©ì‹ê³¼ ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ ê°€ì¤‘ì¹˜ëŠ” ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ë””ë°”ì´ìŠ¤ì— ë¶„ì‚°ë˜ë©°, ê°€ì¥ ë¹ ë¥¸ ë””ë°”ì´ìŠ¤(ë³´í†µ GPU)ë¶€í„° ì‹œì‘í•´ ë‚˜ë¨¸ì§€ ê°€ì¤‘ì¹˜ëŠ” ëŠë¦° ë””ë°”ì´ìŠ¤(CPU ë° í•˜ë“œ ë””ìŠ¤í¬)ë¡œ ìˆœì°¨ì ìœ¼ë¡œ í• ë‹¹ë©ë‹ˆë‹¤.\n+\n+ë‘ ê¸°ëŠ¥ì„ ê²°í•©í•˜ë©´ ëŒ€í˜• ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ê³¼ ë¡œë”© ì‹œê°„ì´ ì¤„ì–´ë“­ë‹ˆë‹¤.\n+\n+ëŒ€í˜• ëª¨ë¸ ì¶”ë¡ ì„ í™œì„±í™”í•˜ë ¤ë©´ [device_map](https://github.com/huggingface/transformers/blob/026a173a64372e9602a16523b8fae9de4b0ff428/src/transformers/modeling_utils.py#L3061)ì„ `\"auto\"`ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.\n+\n+```py\n+from transformers import AutoModelForCausalLM\n+\n+model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\", device_map=\"auto\")\n+```\n+\n+`device_map`ì„ ì‚¬ìš©í•˜ì—¬ ë ˆì´ì–´ë¥¼ ì›í•˜ëŠ” ë””ë°”ì´ìŠ¤ì— ìˆ˜ë™ìœ¼ë¡œ í• ë‹¹í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ë ˆì´ì–´ ì „ì²´ê°€ ë™ì¼í•œ ë””ë°”ì´ìŠ¤ì— í• ë‹¹ë˜ì–´ ìˆë‹¤ë©´, í•´ë‹¹ ë ˆì´ì–´ì˜ ëª¨ë“  ì„œë¸Œëª¨ë“ˆì´ ì–´ë””ì— ë°°ì¹˜ë˜ëŠ”ì§€ ì¼ì¼ì´ ì§€ì •í•  í•„ìš”ëŠ” ì—†ìŠµë‹ˆë‹¤.\n+\n+ëª¨ë¸ì´ ê° ë””ë°”ì´ìŠ¤ì— ì–´ë–»ê²Œ ë¶„ì‚°ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ë ¤ë©´ `hf_device_map` ì†ì„±ì„ ì¡°íšŒí•©ë‹ˆë‹¤.\n+\n+```py\n+device_map = {\"model.layers.1\": 0, \"model.layers.14\": 1, \"model.layers.31\": \"cpu\", \"lm_head\": \"disk\"}\n+model.hf_device_map\n+```\n+\n+### ëª¨ë¸ ë°ì´í„° íƒ€ì…[[model-data-type]]\n+\n+PyTorch ëª¨ë¸ ê°€ì¤‘ì¹˜ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ `torch.float32`ë¡œ ì´ˆê¸°í™”ë©ë‹ˆë‹¤. `torch.float16`ê³¼ ê°™ì´ ë‹¤ë¥¸ ë°ì´í„° íƒ€ì…ìœ¼ë¡œ ëª¨ë¸ì„ ë¡œë“œí•˜ë©´ ëª¨ë¸ì´ ì›í•˜ëŠ” ë°ì´í„° íƒ€ì…ìœ¼ë¡œ ë‹¤ì‹œ ë¡œë“œë˜ê¸° ë•Œë¬¸ì— ì¶”ê°€ ë©”ëª¨ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n+\n+[torch_dtype](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype) ë§¤ê°œë³€ìˆ˜ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ ë‘ ë²ˆ ë¡œë“œí•˜ëŠ” ëŒ€ì‹ (`torch.float32` í›„ `torch.float16`) ì›í•˜ëŠ” ë°ì´í„° íƒ€ì…ìœ¼ë¡œ ëª¨ë¸ì„ ì§ì ‘ ì´ˆê¸°í™”í•˜ì„¸ìš”. ë˜í•œ `torch_dtype=\"auto\"`ë¥¼ ì„¤ì •í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ ì €ì¥ëœ ë°ì´í„° íƒ€ì…ìœ¼ë¡œ ìë™ìœ¼ë¡œ ë¡œë“œí•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n+\n+<hfoptions id=\"dtype\">\n+<hfoption id=\"specific dtype\">\n+\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM\n+\n+gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\", torch_dtype=torch.float16)\n+```\n+\n+</hfoption>\n+<hfoption id=\"auto dtype\">\n+\n+```py\n+from transformers import AutoModelForCausalLM\n+\n+gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\", torch_dtype=\"auto\")\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+ëª¨ë¸ì„ ì²˜ìŒë¶€í„° ì¸ìŠ¤í„´ìŠ¤í™”í•˜ëŠ” ê²½ìš°, `torch_dtype` íŒŒë¼ë¯¸í„°ëŠ” [`AutoConfig`]ì—ì„œ ì„¤ì •í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n+\n+```py\n+import torch\n+from transformers import AutoConfig, AutoModel\n+\n+my_config = AutoConfig.from_pretrained(\"google/gemma-2b\", torch_dtype=torch.float16)\n+model = AutoModel.from_config(my_config)\n+```\n+\n+## ì»¤ìŠ¤í…€ ëª¨ë¸[[custom-models]]\n+\n+ì»¤ìŠ¤í…€ ëª¨ë¸ì€ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ êµ¬ì„± ë° ëª¨ë¸ë§ í´ë˜ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¶•ë˜ë©°, [AutoClass](#autoclass) APIë¥¼ ì§€ì›í•˜ê³  [`~PreTrainedModel.from_pretrained`]ë¡œ ë¡œë“œë©ë‹ˆë‹¤. ì°¨ì´ì ì€ ëª¨ë¸ë§ ì½”ë“œê°€ íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œ ì œê³µë˜ëŠ” ê²ƒì´ *ì•„ë‹ˆë¼ëŠ”* ì ì…ë‹ˆë‹¤.\n+\n+ì»¤ìŠ¤í…€ ëª¨ë¸ì„ ë¡œë“œí•  ë•ŒëŠ” íŠ¹ë³„íˆ ì£¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤. Hubì—ëŠ” ëª¨ë“  ì €ì¥ì†Œì— ëŒ€í•œ [ì•…ì„±ì½”ë“œ ìŠ¤ìº”](https://hf.co/docs/hub/security-malware#malware-scanning)ì´ í¬í•¨ë˜ì–´ ìˆì§€ë§Œ, ì—¬ì „íˆ ì‹¤ìˆ˜ë¡œ ì•…ì„±ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤.\n+\n+ì»¤ìŠ¤í…€ ëª¨ë¸ì„ ë¡œë“œí•˜ë ¤ë©´ [`~PreTrainedModel.from_pretrained`]ì—ì„œ `trust_remote_code=True`ë¥¼ ì„¤ì •í•˜ì„¸ìš”.\n+\n+```py\n+from transformers import AutoModelForImageClassification\n+\n+model = AutoModelForImageClassification.from_pretrained(\"sgugger/custom-resnet50d\", trust_remote_code=True)\n+```\n+\n+ì¶”ê°€ì ì¸ ë³´ì•ˆ ì¡°ì¹˜ë¡œ, ë³€ê²½ë˜ì—ˆì„ ìˆ˜ë„ ìˆëŠ” ëª¨ë¸ ì½”ë“œë¥¼ ë¡œë“œí•˜ëŠ” ê²ƒì„ í”¼í•˜ê¸° ìœ„í•´ íŠ¹ì • ë¦¬ë¹„ì „ì—ì„œ ì»¤ìŠ¤í…€ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤. ì»¤ë°‹ í•´ì‹œëŠ” ëª¨ë¸ì˜ [ì»¤ë°‹ ê¸°ë¡](https://hf.co/sgugger/custom-resnet50d/commits/main)ì—ì„œ ë³µì‚¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+```py\n+commit_hash = \"ed94a7c6247d8aedce4647f00f20de6875b5b292\"\n+model = AutoModelForImageClassification.from_pretrained(\n+    \"sgugger/custom-resnet50d\", trust_remote_code=True, revision=commit_hash\n+)\n+```\n+\n+ìì„¸í•œ ë‚´ìš©ì€ [ì‚¬ìš©ì ì •ì˜ ëª¨ë¸ ê³µìœ í•˜ê¸°](./custom_models) ê°€ì´ë“œë¥¼ ì°¸ì¡°í•˜ì„¸ìš”."
        }
    ],
    "stats": {
        "total": 327,
        "additions": 324,
        "deletions": 3
    }
}