{
    "author": "faaany",
    "message": "[tests] enable GemmaIntegrationTest on XPU  (#33555)\n\nenable GemmaIntegrationTest",
    "sha": "4d8908df272c0a9db2e5fbcc8aaed73cdf75442a",
    "files": [
        {
            "sha": "a02541d585447c0b1b231634e0c35d8315d6ec29",
            "filename": "tests/models/gemma/test_modeling_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/4d8908df272c0a9db2e5fbcc8aaed73cdf75442a/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4d8908df272c0a9db2e5fbcc8aaed73cdf75442a/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py?ref=4d8908df272c0a9db2e5fbcc8aaed73cdf75442a",
            "patch": "@@ -528,7 +528,7 @@ def test_flash_attn_2_equivalence(self):\n \n \n @slow\n-@require_torch_gpu\n+@require_torch_accelerator\n class GemmaIntegrationTest(unittest.TestCase):\n     input_text = [\"Hello I am doing\", \"Hi today\"]\n     # This variable is used to determine which CUDA device are we using for our runners (A10 or T4)\n@@ -748,7 +748,6 @@ def test_model_7b_bf16(self):\n \n         output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n         output_text = tokenizer.batch_decode(output, skip_special_tokens=True)\n-\n         self.assertEqual(output_text, EXPECTED_TEXTS[self.cuda_compute_capability_major_version])\n \n     @require_read_token\n@@ -770,10 +769,8 @@ def test_model_7b_fp16_static_cache(self):\n \n         tokenizer = AutoTokenizer.from_pretrained(model_id)\n         inputs = tokenizer(self.input_text, return_tensors=\"pt\", padding=True).to(torch_device)\n-\n         output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n         output_text = tokenizer.batch_decode(output, skip_special_tokens=True)\n-\n         self.assertEqual(output_text, EXPECTED_TEXTS)\n \n     @require_bitsandbytes"
        }
    ],
    "stats": {
        "total": 5,
        "additions": 1,
        "deletions": 4
    }
}