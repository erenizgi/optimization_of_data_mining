{
    "author": "h3110Fr13nd",
    "message": "Example doc for token classification of Llama and Dependent/Copied Models (#34139)\n\n* Added Example Doc for token classification on all tokenClassificationModels copied from llama\r\n\r\n* Refactor code to add code sample docstrings for Gemma and Gemma2 models (including modular Gemma)\r\n\r\n* Refactor code to update model checkpoint names for Qwen2 models",
    "sha": "049682a5a63042f087fb45ff128bfe281b2ff98b",
    "files": [
        {
            "sha": "6f364ffcf7edd8a6ca2228c8c9d8862e42c2ea87",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/049682a5a63042f087fb45ff128bfe281b2ff98b/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/049682a5a63042f087fb45ff128bfe281b2ff98b/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=049682a5a63042f087fb45ff128bfe281b2ff98b",
            "patch": "@@ -39,6 +39,7 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n+    add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_greater_or_equal_2_10,\n@@ -48,6 +49,9 @@\n from .configuration_gemma import GemmaConfig\n \n \n+_CHECKPOINT_FOR_DOC = \"google/gemma-7b\"\n+\n+\n class GemmaRMSNorm(nn.Module):\n     def __init__(self, dim: int, eps: float = 1e-6):\n         super().__init__()\n@@ -1233,6 +1237,11 @@ def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n     @add_start_docstrings_to_model_forward(GEMMA_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=TokenClassifierOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+    )\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "c3d780bc571ade7645d65ece4feea4742102556d",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/049682a5a63042f087fb45ff128bfe281b2ff98b/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/049682a5a63042f087fb45ff128bfe281b2ff98b/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=049682a5a63042f087fb45ff128bfe281b2ff98b",
            "patch": "@@ -49,6 +49,7 @@\n \n SPIECE_UNDERLINE = \"‚ñÅ\"\n \n+_CHECKPOINT_FOR_DOC = \"google/gemma-7b\"\n \n logger = logging.get_logger(__name__)\n "
        },
        {
            "sha": "467981bb78d025084bce1b541d2cc97da9e8a873",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/049682a5a63042f087fb45ff128bfe281b2ff98b/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/049682a5a63042f087fb45ff128bfe281b2ff98b/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=049682a5a63042f087fb45ff128bfe281b2ff98b",
            "patch": "@@ -37,6 +37,7 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n+    add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_greater_or_equal,\n@@ -47,6 +48,9 @@\n from .configuration_gemma2 import Gemma2Config\n \n \n+_CHECKPOINT_FOR_DOC = \"google/gemma2-7b\"\n+\n+\n class Gemma2RMSNorm(nn.Module):\n     def __init__(self, dim: int, eps: float = 1e-6):\n         super().__init__()\n@@ -1292,6 +1296,11 @@ def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n     @add_start_docstrings_to_model_forward(GEMMA2_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=TokenClassifierOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+    )\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "49010152b81cc5a13d0c80a38d166e2976d0b3fc",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/049682a5a63042f087fb45ff128bfe281b2ff98b/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/049682a5a63042f087fb45ff128bfe281b2ff98b/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=049682a5a63042f087fb45ff128bfe281b2ff98b",
            "patch": "@@ -50,6 +50,8 @@\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n \n+_CHECKPOINT_FOR_DOC = \"google/gemma2-7b\"\n+\n logger = logging.get_logger(__name__)\n \n "
        },
        {
            "sha": "617ef38e4ae3deef42814485578eb9eb723ef31b",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/049682a5a63042f087fb45ff128bfe281b2ff98b/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/049682a5a63042f087fb45ff128bfe281b2ff98b/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=049682a5a63042f087fb45ff128bfe281b2ff98b",
            "patch": "@@ -41,6 +41,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n from ...utils import (\n+    add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_greater_or_equal_2_10,\n@@ -52,6 +53,7 @@\n \n logger = logging.get_logger(__name__)\n \n+_CHECKPOINT_FOR_DOC = \"meta-llama/Llama-2-7b-hf\"\n _CONFIG_FOR_DOC = \"LlamaConfig\"\n \n \n@@ -1446,6 +1448,11 @@ def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n     @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=TokenClassifierOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+    )\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "f198e4abc855110529a053f27c9eab939ef4f332",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/049682a5a63042f087fb45ff128bfe281b2ff98b/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/049682a5a63042f087fb45ff128bfe281b2ff98b/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=049682a5a63042f087fb45ff128bfe281b2ff98b",
            "patch": "@@ -40,6 +40,7 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n+    add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n@@ -55,6 +56,7 @@\n \n logger = logging.get_logger(__name__)\n \n+_CHECKPOINT_FOR_DOC = \"mistralai/Mistral-7B-v0.1\"\n _CONFIG_FOR_DOC = \"MistralConfig\"\n \n \n@@ -1242,6 +1244,11 @@ def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n     @add_start_docstrings_to_model_forward(MISTRAL_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=TokenClassifierOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+    )\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "f5f11ba995c8028bd663dab601ca38625cd19cb6",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/049682a5a63042f087fb45ff128bfe281b2ff98b/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/049682a5a63042f087fb45ff128bfe281b2ff98b/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=049682a5a63042f087fb45ff128bfe281b2ff98b",
            "patch": "@@ -41,6 +41,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import is_torch_greater_or_equal_than_1_13\n from ...utils import (\n+    add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n@@ -65,6 +66,7 @@\n \n logger = logging.get_logger(__name__)\n \n+_CHECKPOINT_FOR_DOC = \"mistralai/Mixtral-8x7B-v0.1\"\n _CONFIG_FOR_DOC = \"MixtralConfig\"\n \n \n@@ -1468,6 +1470,11 @@ def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n     @add_start_docstrings_to_model_forward(MIXTRAL_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=TokenClassifierOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+    )\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "d5470dbbaa190408852fea50fb3c5dc597d6d851",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/049682a5a63042f087fb45ff128bfe281b2ff98b/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/049682a5a63042f087fb45ff128bfe281b2ff98b/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=049682a5a63042f087fb45ff128bfe281b2ff98b",
            "patch": "@@ -39,6 +39,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n from ...utils import (\n+    add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_greater_or_equal_2_10,\n@@ -50,6 +51,7 @@\n \n logger = logging.get_logger(__name__)\n \n+_CHECKPOINT_FOR_DOC = \"nvidia/nemotron-3-8b-base-4k-hf\"\n _CONFIG_FOR_DOC = \"NemotronConfig\"\n \n \n@@ -1323,6 +1325,11 @@ def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n     @add_start_docstrings_to_model_forward(NEMOTRON_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=TokenClassifierOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+    )\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "cd580ab0dc0f8c0395ee072109a10243af4b0393",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 13,
            "deletions": 1,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/049682a5a63042f087fb45ff128bfe281b2ff98b/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/049682a5a63042f087fb45ff128bfe281b2ff98b/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=049682a5a63042f087fb45ff128bfe281b2ff98b",
            "patch": "@@ -39,12 +39,19 @@\n )\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n-from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n+from ...utils import (\n+    add_code_sample_docstrings,\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    logging,\n+    replace_return_docstrings,\n+)\n from .configuration_persimmon import PersimmonConfig\n \n \n logger = logging.get_logger(__name__)\n \n+_CHECKPOINT_FOR_DOC = \"adept/persimmon-8b-base\"\n _CONFIG_FOR_DOC = \"PersimmonConfig\"\n \n \n@@ -1120,6 +1127,11 @@ def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n     @add_start_docstrings_to_model_forward(PERSIMMON_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=TokenClassifierOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+    )\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "a6e4d12d799b07f0a1c6fdc7707895e891db4c83",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/049682a5a63042f087fb45ff128bfe281b2ff98b/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/049682a5a63042f087fb45ff128bfe281b2ff98b/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=049682a5a63042f087fb45ff128bfe281b2ff98b",
            "patch": "@@ -41,6 +41,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n+    add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n@@ -58,7 +59,7 @@\n logger = logging.get_logger(__name__)\n \n \n-_CHECKPOINT_FOR_DOC = \"Qwen/Qwen2-7B-beta\"\n+_CHECKPOINT_FOR_DOC = \"Qwen/Qwen2-7B\"\n _CONFIG_FOR_DOC = \"Qwen2Config\"\n \n \n@@ -1348,6 +1349,11 @@ def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n     @add_start_docstrings_to_model_forward(QWEN2_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=TokenClassifierOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+    )\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "d482316b5b8bace1331032086b5c98ac60fa6bcb",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/049682a5a63042f087fb45ff128bfe281b2ff98b/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/049682a5a63042f087fb45ff128bfe281b2ff98b/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=049682a5a63042f087fb45ff128bfe281b2ff98b",
            "patch": "@@ -41,6 +41,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n+    add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n@@ -56,7 +57,7 @@\n \n logger = logging.get_logger(__name__)\n \n-_CHECKPOINT_FOR_DOC = \"Qwen/Qwen1.5-MoE-A2.7B\"\n+_CHECKPOINT_FOR_DOC = \"Qwen/Qwen2-57B-A14B\"\n _CONFIG_FOR_DOC = \"Qwen2MoeConfig\"\n \n \n@@ -1533,6 +1534,11 @@ def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n     @add_start_docstrings_to_model_forward(QWEN2MOE_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=TokenClassifierOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+    )\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "004e4ff3f6c030bc0d50edbdfbfc6050aed1a408",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/049682a5a63042f087fb45ff128bfe281b2ff98b/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/049682a5a63042f087fb45ff128bfe281b2ff98b/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=049682a5a63042f087fb45ff128bfe281b2ff98b",
            "patch": "@@ -40,6 +40,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n+    add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n@@ -56,6 +57,7 @@\n \n logger = logging.get_logger(__name__)\n \n+_CHECKPOINT_FOR_DOC = \"stabilityai/stablelm-3b-4e1t\"\n _CONFIG_FOR_DOC = \"StableLmConfig\"\n \n \n@@ -1396,6 +1398,11 @@ def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n     @add_start_docstrings_to_model_forward(STABLELM_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=TokenClassifierOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+    )\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "c8f22dee43fe2c3d475ce9285f62d7c2edad9d5d",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/049682a5a63042f087fb45ff128bfe281b2ff98b/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/049682a5a63042f087fb45ff128bfe281b2ff98b/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=049682a5a63042f087fb45ff128bfe281b2ff98b",
            "patch": "@@ -40,6 +40,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n+    add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n@@ -56,6 +57,7 @@\n \n logger = logging.get_logger(__name__)\n \n+_CHECKPOINT_FOR_DOC = \"bigcode/starcoder2-7b\"\n _CONFIG_FOR_DOC = \"Starcoder2Config\"\n \n \n@@ -1316,6 +1318,11 @@ def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n     @add_start_docstrings_to_model_forward(STARCODER2_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=TokenClassifierOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+    )\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,"
        }
    ],
    "stats": {
        "total": 93,
        "additions": 90,
        "deletions": 3
    }
}