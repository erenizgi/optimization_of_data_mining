{
    "author": "remi-or",
    "message": "Fix EncoderDecoder cache (#41612)\n\n* Fix EncoderDecoder cache\n\n* Add the option for the ddp data tuples to have 2 elems\n\n* Modifiy the order of the KV and sliding\n\n* Adapted RAG and Whisper to new EncoderDecoderCache\n\n* A single comma\n\n* Remove kwargs in map\n\n* Fixed order in manual injection cache test\n\n* Slight changes to support legacy format\n\n* Removed Nonnes",
    "sha": "eef9fb2af3db888cf93f81b425f9db453336726c",
    "files": [
        {
            "sha": "28f40952f2cde44cf44c058f0460faf86ed02954",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 27,
            "deletions": 14,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/eef9fb2af3db888cf93f81b425f9db453336726c/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eef9fb2af3db888cf93f81b425f9db453336726c/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=eef9fb2af3db888cf93f81b425f9db453336726c",
            "patch": "@@ -937,7 +937,7 @@ class DynamicCache(Cache):\n \n     def __init__(\n         self,\n-        ddp_cache_data: Optional[Iterable[tuple[Optional[torch.Tensor], torch.Tensor, torch.Tensor]]] = None,\n+        ddp_cache_data: Optional[Iterable[tuple[Optional[torch.Tensor], ...]]] = None,\n         config: Optional[PreTrainedConfig] = None,\n         offloading: bool = False,\n         offload_only_non_sliding: bool = False,\n@@ -970,17 +970,21 @@ def __init__(\n         # In this case, use the passed data to already fill in the Cache\n         if ddp_cache_data is not None:\n             # Init all the layers with the data\n-            for layer_idx, (sliding_window_tensor, key_states, value_states) in enumerate(ddp_cache_data):\n+            for layer_idx, kv_and_optional_sliding in enumerate(ddp_cache_data):\n                 # If the config was not passed above, initialize a new cache layer for each entry of the ddp_data\n                 if config is None:\n+                    # kv_and_optional_sliding contains at least two elements: the key and value states. It can also\n+                    # contain a third element, which is an optional sliding window tensor.\n+                    sliding_window_tensor = kv_and_optional_sliding[2] if len(kv_and_optional_sliding) == 3 else None\n+                    # If there is a sliding window tensor, use it to initialize the layer\n                     if sliding_window_tensor is not None:\n                         # Since the same layer is dispatched across replicas, sliding_window is the same for all\n                         sliding_window = sliding_window_tensor[0].item()\n                         layers.append(DynamicSlidingWindowLayer(sliding_window=sliding_window))\n                     else:\n                         layers.append(DynamicLayer())\n                 # Update the layer with the data\n-                _, _ = layers[layer_idx].update(key_states, value_states)\n+                _, _ = layers[layer_idx].update(kv_and_optional_sliding[0], kv_and_optional_sliding[1])\n \n         # If neither of config nor ddp_data was passed, then simply lazy init a full cache of DynamicLayer\n         if len(layers) == 0:\n@@ -994,7 +998,7 @@ def __init__(\n \n     def __iter__(self):\n         for layer in self.layers:\n-            yield getattr(layer, \"_sliding_window_tensor\", None), layer.keys, layer.values\n+            yield layer.keys, layer.values, getattr(layer, \"_sliding_window_tensor\", None)\n \n \n class StaticCache(Cache):\n@@ -1166,17 +1170,21 @@ class EncoderDecoderCache(Cache):\n     \"\"\"\n \n     def __init__(self, *caches) -> None:\n-        # For dp and ddp support, if only one argument is passed, it should be an iterable of tuples of tensors\n+        # For dp and ddp support, if only one argument is passed, it should be an iterable of DynamicCache ddp data\n         if len(caches) == 1:\n-            self.self_attention_cache = DynamicCache()\n-            self.cross_attention_cache = DynamicCache()\n-            # Populate cache from the iterable\n-            for layer_idx, key_value_states in enumerate(caches[0]):\n-                key_states, value_states = key_value_states[:2]\n-                self.self_attention_cache.update(key_states, value_states, layer_idx)\n-                if len(key_value_states) > 2:\n-                    key_states, value_states = key_value_states[2:]\n-                    self.cross_attention_cache.update(key_states, value_states, layer_idx)\n+            self_attention_cache_data, cross_attention_cache_data = [], []\n+            for combined_cache_data in caches[0]:\n+                if len(combined_cache_data) == 6:  # two tuple of style (self_attn_k, self_attn_v, self_attn_sliding)\n+                    self_attention_cache_data.append(combined_cache_data[:3])\n+                    cross_attention_cache_data.append(combined_cache_data[3:])\n+                # To support old DDP-style init, we handle the case where the tuple has no sliding window tensor\n+                elif len(combined_cache_data) == 4:  # two tuple of style (self_attn_k, self_attn_v)\n+                    self_attention_cache_data.append(combined_cache_data[:2])\n+                    cross_attention_cache_data.append(combined_cache_data[2:])\n+                else:\n+                    raise ValueError(f\"Expected {len(combined_cache_data) = } to be 4 or 6.\\n{combined_cache_data = }\")\n+            self.self_attention_cache = DynamicCache(self_attention_cache_data)\n+            self.cross_attention_cache = DynamicCache(cross_attention_cache_data)\n         # Otherwise, we should get two arguments, a self-attention cache and a cross-attention cache\n         elif len(caches) == 2:\n             if not isinstance(caches[0], Cache) or not isinstance(caches[1], Cache):\n@@ -1191,6 +1199,11 @@ def __init__(self, *caches) -> None:\n         for layer_idx in range(len(self.cross_attention_cache)):\n             self.is_updated[layer_idx] = bool(self.cross_attention_cache.get_seq_length(layer_idx) > 0)\n \n+    def __iter__(self):\n+        \"\"\"Returns tuples of style (self_attn_k, self_attn_v, self_attn_sliding, cross_attn_k, cross_attn_v, cross_attn_sliding)\"\"\"\n+        for self_attention_layer, cross_attention_layer in zip(self.self_attention_cache, self.cross_attention_cache):\n+            yield self_attention_layer + cross_attention_layer\n+\n     def __repr__(self) -> str:\n         return (\n             f\"{self.__class__.__name__}(self_attention_cache={self.self_attention_cache}, cross_attention_cache=\""
        },
        {
            "sha": "9d909237560464e855ff85cc6f96bdfab62a3a5b",
            "filename": "src/transformers/models/rag/modeling_rag.py",
            "status": "modified",
            "additions": 16,
            "deletions": 14,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/eef9fb2af3db888cf93f81b425f9db453336726c/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eef9fb2af3db888cf93f81b425f9db453336726c/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py?ref=eef9fb2af3db888cf93f81b425f9db453336726c",
            "patch": "@@ -1187,22 +1187,24 @@ def _reorder_stacked(hidden_states, new_order):\n         reordered_past = ()\n         for idx in range(len(past_key_values)):\n             if isinstance(past_key_values, EncoderDecoderCache):\n-                layer_past = (\n-                    past_key_values.self_attention_cache.layers[idx].keys,\n-                    past_key_values.self_attention_cache.layers[idx].values,\n-                    past_key_values.cross_attention_cache.layers[idx].keys,\n-                    past_key_values.cross_attention_cache.layers[idx].values,\n+                self_attention_k, self_attention_v, cross_attention_k, cross_attention_v = (\n+                    _reorder_stacked(x, beam_idx.to(x.device))\n+                    for x in (\n+                        past_key_values.self_attention_cache.layers[idx].keys,\n+                        past_key_values.self_attention_cache.layers[idx].values,\n+                        past_key_values.cross_attention_cache.layers[idx].keys,\n+                        past_key_values.cross_attention_cache.layers[idx].values,\n+                    )\n                 )\n+                new_tuple = (self_attention_k, self_attention_v, cross_attention_k, cross_attention_v)\n             else:\n-                layer_past = (past_key_values.layers[idx].keys, past_key_values.layers[idx].values)\n-            # get the correct batch idx from decoder layer's batch dim for cross and self-attn\n-            reordered_past += (\n-                tuple(_reorder_stacked(past_state, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-\n-        # Cast back to the correct cache class\n-        reordered_cache = type(past_key_values)(reordered_past)\n-        return reordered_cache\n+                self_attention_k, self_attention_v = (\n+                    _reorder_stacked(x, beam_idx.to(x.device))\n+                    for x in (past_key_values.layers[idx].keys, past_key_values.layers[idx].values)\n+                )\n+                new_tuple = (self_attention_k, self_attention_v)\n+            reordered_past += (new_tuple,)\n+        return type(past_key_values)(reordered_past)\n \n     def marginalize(self, seq_logits, doc_scores, n_docs=None):\n         n_docs = n_docs if n_docs is not None else self.config.n_docs"
        },
        {
            "sha": "6b3bd20373b79ea8cd1ac2c6026d3b74d65f84a6",
            "filename": "src/transformers/models/whisper/generation_whisper.py",
            "status": "modified",
            "additions": 12,
            "deletions": 8,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/eef9fb2af3db888cf93f81b425f9db453336726c/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eef9fb2af3db888cf93f81b425f9db453336726c/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py?ref=eef9fb2af3db888cf93f81b425f9db453336726c",
            "patch": "@@ -1180,12 +1180,14 @@ def split_by_batch_index(values, key, batch_idx, is_shortform, beam_indices=None\n                     return None\n                 all_past_key_values = []\n                 for layer_idx in range(self.config.decoder_layers):\n-                    layer_past_key_values = []\n-                    for cache_cls in [values.self_attention_cache, values.cross_attention_cache]:\n-                        for v in [cache_cls.layers[layer_idx].keys, cache_cls.layers[layer_idx].values]:\n-                            layer_past_key_values.append(v[batch_idx][None].cpu())\n-                    all_past_key_values.append(tuple(layer_past_key_values))\n-                return EncoderDecoderCache(tuple(all_past_key_values))\n+                    layer_cache = (\n+                        values.self_attention_cache.layers[layer_idx].keys[batch_idx][None].cpu(),\n+                        values.self_attention_cache.layers[layer_idx].values[batch_idx][None].cpu(),\n+                        values.cross_attention_cache.layers[layer_idx].keys[batch_idx][None].cpu(),\n+                        values.cross_attention_cache.layers[layer_idx].values[batch_idx][None].cpu(),\n+                    )\n+                    all_past_key_values.append(layer_cache)\n+                return EncoderDecoderCache(all_past_key_values)\n \n             return values[batch_idx].cpu()\n \n@@ -1224,7 +1226,7 @@ def _stack_split_outputs(self, seek_outputs, model_output_type, device, kwargs):\n                 if seek_outputs[0][key] is not None:\n                     all_past_key_values = []\n                     for layer_idx in range(len(seek_outputs[0][key])):\n-                        layer_past_key_values = tuple(\n+                        self_attention_k, self_attention_v, cross_attention_k, cross_attention_v = (\n                             torch.stack(\n                                 [\n                                     getattr(getattr(sub_output[key], sub_cache).layers[layer_idx], sub_key)\n@@ -1236,7 +1238,9 @@ def _stack_split_outputs(self, seek_outputs, model_output_type, device, kwargs):\n                             for sub_cache in [\"self_attention_cache\", \"cross_attention_cache\"]\n                             for sub_key in [\"keys\", \"values\"]\n                         )\n-                        all_past_key_values.append(layer_past_key_values)\n+                        all_past_key_values.append(\n+                            (self_attention_k, self_attention_v, cross_attention_k, cross_attention_v)\n+                        )\n                     outputs[key] = EncoderDecoderCache(tuple(all_past_key_values))\n                 else:\n                     outputs[key] = None"
        },
        {
            "sha": "99f842188c037486ecf8110087f37dafd2fcc81a",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/eef9fb2af3db888cf93f81b425f9db453336726c/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eef9fb2af3db888cf93f81b425f9db453336726c/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=eef9fb2af3db888cf93f81b425f9db453336726c",
            "patch": "@@ -1807,8 +1807,8 @@ def test_cache_when_needed_at_train_time(self):\n         # simulate injecting virtual tokens like in prefix tuning\n         num_virtual_tokens = 3\n         past_key_values = [\n-            (None, torch.randn(1, 2, num_virtual_tokens, 8), torch.randn(1, 2, num_virtual_tokens, 8)),\n-            (None, torch.randn(1, 2, num_virtual_tokens, 8), torch.randn(1, 2, num_virtual_tokens, 8)),\n+            (torch.randn(1, 2, num_virtual_tokens, 8), torch.randn(1, 2, num_virtual_tokens, 8)),\n+            (torch.randn(1, 2, num_virtual_tokens, 8), torch.randn(1, 2, num_virtual_tokens, 8)),\n         ]\n         past_key_values = DynamicCache(past_key_values)\n         model_inputs[\"attention_mask\"] = torch.cat("
        }
    ],
    "stats": {
        "total": 95,
        "additions": 57,
        "deletions": 38
    }
}