{
    "author": "divyanshsinghvi",
    "message": "remove the redundant non maintained jieba and use rjieba instead (#40383)\n\n* porting not maintained jieba to rjieba\n\n* Fix format\n\n* replaced the line with rjieba instead of removing it\n\n* cut_all is not included as a parameter. cut_all is a seperate function rjieba\n\n* rev\n\n* jieba remove installation\n\n* Trigger tests\n\n* Update tokenization_cpm.py\n\n* Update tokenization_cpm_fast.py\n\n---------\n\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>",
    "sha": "1363fceeec955db125141c3d7768f8ef345ed288",
    "files": [
        {
            "sha": "73b34e7114aac4d40942e492047a3820edff02e7",
            "filename": "docker/custom-tokenizers.dockerfile",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1363fceeec955db125141c3d7768f8ef345ed288/docker%2Fcustom-tokenizers.dockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/1363fceeec955db125141c3d7768f8ef345ed288/docker%2Fcustom-tokenizers.dockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Fcustom-tokenizers.dockerfile?ref=1363fceeec955db125141c3d7768f8ef345ed288",
            "patch": "@@ -18,7 +18,7 @@ RUN make install -j 10\n \n RUN uv pip install --no-cache --upgrade 'torch' --index-url https://download.pytorch.org/whl/cpu\n RUN uv pip install --no-cache-dir  --no-deps accelerate --extra-index-url https://download.pytorch.org/whl/cpu\n-RUN uv pip install  --no-cache-dir \"git+https://github.com/huggingface/transformers.git@${REF}#egg=transformers[ja,testing,sentencepiece,jieba,spacy,ftfy,rjieba]\" unidic unidic-lite\n+RUN uv pip install  --no-cache-dir \"git+https://github.com/huggingface/transformers.git@${REF}#egg=transformers[ja,testing,sentencepiece,spacy,ftfy,rjieba]\" unidic unidic-lite\n # spacy is not used so not tested. Causes to failures. TODO fix later\n RUN uv run python -m unidic download\n RUN uv pip uninstall transformers"
        },
        {
            "sha": "c635db02e33c01f072b4e712630c06facdcf489f",
            "filename": "setup.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1363fceeec955db125141c3d7768f8ef345ed288/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1363fceeec955db125141c3d7768f8ef345ed288/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=1363fceeec955db125141c3d7768f8ef345ed288",
            "patch": "@@ -122,7 +122,6 @@\n     \"ipadic>=1.0.0,<2.0\",\n     \"jax>=0.4.1,<=0.4.13\",\n     \"jaxlib>=0.4.1,<=0.4.13\",\n-    \"jieba\",\n     \"jinja2>=3.1.0\",\n     \"kenlm\",\n     # Keras pin - this is to make sure Keras 3 doesn't destroy us. Remove or change when we have proper support."
        },
        {
            "sha": "ab6e747d14dbc2fa1d3a15528bf8a091495867eb",
            "filename": "src/transformers/dependency_versions_table.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1363fceeec955db125141c3d7768f8ef345ed288/src%2Ftransformers%2Fdependency_versions_table.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1363fceeec955db125141c3d7768f8ef345ed288/src%2Ftransformers%2Fdependency_versions_table.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdependency_versions_table.py?ref=1363fceeec955db125141c3d7768f8ef345ed288",
            "patch": "@@ -29,7 +29,6 @@\n     \"ipadic\": \"ipadic>=1.0.0,<2.0\",\n     \"jax\": \"jax>=0.4.1,<=0.4.13\",\n     \"jaxlib\": \"jaxlib>=0.4.1,<=0.4.13\",\n-    \"jieba\": \"jieba\",\n     \"jinja2\": \"jinja2>=3.1.0\",\n     \"kenlm\": \"kenlm\",\n     \"keras\": \"keras>2.9,<2.16\","
        },
        {
            "sha": "5ecfedd0a61432e406abcd322c0eb43146d939a4",
            "filename": "src/transformers/models/cpm/tokenization_cpm.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/1363fceeec955db125141c3d7768f8ef345ed288/src%2Ftransformers%2Fmodels%2Fcpm%2Ftokenization_cpm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1363fceeec955db125141c3d7768f8ef345ed288/src%2Ftransformers%2Fmodels%2Fcpm%2Ftokenization_cpm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcpm%2Ftokenization_cpm.py?ref=1363fceeec955db125141c3d7768f8ef345ed288",
            "patch": "@@ -33,7 +33,7 @@\n \n @requires(backends=(\"sentencepiece\",))\n class CpmTokenizer(PreTrainedTokenizer):\n-    \"\"\"Runs pre-tokenization with Jieba segmentation tool. It is used in CPM models.\"\"\"\n+    \"\"\"Runs pre-tokenization with Jieba-RS segmentation tool. It is used in CPM models.\"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n \n@@ -55,7 +55,7 @@ def __init__(\n         **kwargs,\n     ) -> None:\n         \"\"\"\n-        Construct a CPM tokenizer. Based on [Jieba](https://pypi.org/project/jieba/) and\n+        Construct a CPM tokenizer. Based on [Jieba-RS](https://pypi.org/project/rjieba/) and\n         [SentencePiece](https://github.com/google/sentencepiece).\n \n         This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should\n@@ -129,13 +129,13 @@ def __init__(\n         self.sp_model.Load(vocab_file)\n \n         try:\n-            import jieba\n+            import rjieba\n         except ModuleNotFoundError as error:\n             raise error.__class__(\n-                \"You need to install jieba to use CpmTokenizer or CpmTokenizerFast. \"\n-                \"See https://pypi.org/project/jieba/ for installation.\"\n+                \"You need to install rjieba to use CpmTokenizer or CpmTokenizerFast. \"\n+                \"See https://pypi.org/project/rjieba/ for installation.\"\n             )\n-        self.jieba = jieba\n+        self.jieba = rjieba\n         self.translator = str.maketrans(\" \\n\", \"\\u2582\\u2583\")\n \n         super().__init__("
        },
        {
            "sha": "3e828ca9e0b56151de256497ad3c19d918d3c8ed",
            "filename": "src/transformers/models/cpm/tokenization_cpm_fast.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/1363fceeec955db125141c3d7768f8ef345ed288/src%2Ftransformers%2Fmodels%2Fcpm%2Ftokenization_cpm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1363fceeec955db125141c3d7768f8ef345ed288/src%2Ftransformers%2Fmodels%2Fcpm%2Ftokenization_cpm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcpm%2Ftokenization_cpm_fast.py?ref=1363fceeec955db125141c3d7768f8ef345ed288",
            "patch": "@@ -28,7 +28,7 @@\n \n \n class CpmTokenizerFast(PreTrainedTokenizerFast):\n-    \"\"\"Runs pre-tokenization with Jieba segmentation tool. It is used in CPM models.\"\"\"\n+    \"\"\"Runs pre-tokenization with Jieba-RS segmentation tool. It is used in CPM models.\"\"\"\n \n     def __init__(\n         self,\n@@ -48,7 +48,7 @@ def __init__(\n         **kwargs,\n     ):\n         \"\"\"\n-        Construct a CPM tokenizer. Based on [Jieba](https://pypi.org/project/jieba/) and\n+        Construct a CPM tokenizer. Based on [Jieba-RS](https://pypi.org/project/rjieba/) and\n         [SentencePiece](https://github.com/google/sentencepiece).\n \n         This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should\n@@ -135,13 +135,13 @@ def __init__(\n         self.vocab_file = vocab_file\n \n         try:\n-            import jieba\n+            import rjieba\n         except ModuleNotFoundError as error:\n             raise error.__class__(\n-                \"You need to install jieba to use CpmTokenizer or CpmTokenizerFast. \"\n-                \"See https://pypi.org/project/jieba/ for installation.\"\n+                \"You need to install rjieba to use CpmTokenizer or CpmTokenizerFast. \"\n+                \"See https://pypi.org/project/rjieba/ for installation.\"\n             )\n-        self.jieba = jieba\n+        self.jieba = rjieba\n         self.translator = str.maketrans(\" \\n\", \"\\u2582\\u2583\")\n \n     # Copied from transformers.models.xlnet.tokenization_xlnet_fast.XLNetTokenizerFast.build_inputs_with_special_tokens\n@@ -223,7 +223,7 @@ def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] =\n \n     def _batch_encode_plus(self, batch_text_or_text_pairs, *args, **kwargs):\n         batch_text_or_text_pairs = [\n-            \" \".join([x.translate(self.translator) for x in self.jieba.cut(text, cut_all=False)])\n+            \" \".join([x.translate(self.translator) for x in self.jieba.cut(text, False)])\n             for text in batch_text_or_text_pairs\n         ]\n         return super()._batch_encode_plus(batch_text_or_text_pairs, *args, **kwargs)"
        },
        {
            "sha": "38cd9f0c6a25dd5ace5a51afc31be8ff8b10b3eb",
            "filename": "src/transformers/models/cpmant/tokenization_cpmant.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/1363fceeec955db125141c3d7768f8ef345ed288/src%2Ftransformers%2Fmodels%2Fcpmant%2Ftokenization_cpmant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1363fceeec955db125141c3d7768f8ef345ed288/src%2Ftransformers%2Fmodels%2Fcpmant%2Ftokenization_cpmant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcpmant%2Ftokenization_cpmant.py?ref=1363fceeec955db125141c3d7768f8ef345ed288",
            "patch": "@@ -18,11 +18,11 @@\n import os\n from typing import Optional\n \n-from transformers.utils import is_jieba_available, requires_backends\n+from transformers.utils import is_rjieba_available, requires_backends\n \n \n-if is_jieba_available():\n-    import jieba\n+if is_rjieba_available():\n+    import rjieba\n \n from ...tokenization_utils import PreTrainedTokenizer\n from ...utils import logging\n@@ -119,7 +119,7 @@ def __init__(\n         padding_side=\"left\",\n         **kwargs,\n     ):\n-        requires_backends(self, [\"jieba\"])\n+        requires_backends(self, [\"rjieba\"])\n         self.bod_token = bod_token\n         self.eod_token = eod_token\n         self.encoder = load_vocab(vocab_file)\n@@ -169,7 +169,7 @@ def get_vocab(self):\n     def _tokenize(self, text):\n         \"\"\"Tokenize a string.\"\"\"\n         output_tokens = []\n-        for x in jieba.cut(text, cut_all=False):\n+        for x in rjieba.cut(text, False):\n             output_tokens.extend(self.wordpiece_tokenizer.tokenize(x))\n         return output_tokens\n "
        },
        {
            "sha": "8c4471a38436fba495bf48c2245ebf3e39f8cbbd",
            "filename": "src/transformers/models/xlm/tokenization_xlm.py",
            "status": "modified",
            "additions": 10,
            "deletions": 8,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/1363fceeec955db125141c3d7768f8ef345ed288/src%2Ftransformers%2Fmodels%2Fxlm%2Ftokenization_xlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1363fceeec955db125141c3d7768f8ef345ed288/src%2Ftransformers%2Fmodels%2Fxlm%2Ftokenization_xlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm%2Ftokenization_xlm.py?ref=1363fceeec955db125141c3d7768f8ef345ed288",
            "patch": "@@ -383,8 +383,8 @@ def _tokenize(self, text, lang=\"en\", bypass_tokenizer=False):\n                 git clone git@github.com:neubig/kytea.git && cd kytea autoreconf -i ./configure --prefix=$HOME/local\n                 make && make install pip install kytea\n \n-            - [jieba](https://github.com/fxsjy/jieba): Chinese tokenizer (*)\n-            - Install with `pip install jieba`\n+            - [rjieba](https://github.com/messense/rjieba-py): Chinese tokenizer (*)\n+            - Install with `pip install rjieba`\n \n         (*) The original XLM used [Stanford\n         Segmenter](https://nlp.stanford.edu/software/stanford-segmenter-2018-10-16.zip). However, the wrapper\n@@ -432,15 +432,17 @@ def _tokenize(self, text, lang=\"en\", bypass_tokenizer=False):\n             text = th_word_tokenize(text)\n         elif lang == \"zh\":\n             try:\n-                if \"jieba\" not in sys.modules:\n-                    import jieba\n+                if \"rjieba\" not in sys.modules:\n+                    import rjieba\n                 else:\n-                    jieba = sys.modules[\"jieba\"]\n+                    rjieba = sys.modules[\"rjieba\"]\n             except (AttributeError, ImportError):\n-                logger.error(\"Make sure you install Jieba (https://github.com/fxsjy/jieba) with the following steps\")\n-                logger.error(\"1. pip install jieba\")\n+                logger.error(\n+                    \"Make sure you install rjieba (https://github.com/messense/rjieba-py) with the following steps\"\n+                )\n+                logger.error(\"1. pip install rjieba\")\n                 raise\n-            text = \" \".join(jieba.cut(text))\n+            text = \" \".join(rjieba.cut(text))\n             text = self.moses_pipeline(text, lang=lang)\n             text = text.split()\n         elif lang == \"ja\":"
        },
        {
            "sha": "b0c4bace52a3a3c0f6d2679346fe628050613497",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1363fceeec955db125141c3d7768f8ef345ed288/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1363fceeec955db125141c3d7768f8ef345ed288/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=1363fceeec955db125141c3d7768f8ef345ed288",
            "patch": "@@ -103,7 +103,6 @@\n     is_hqq_available,\n     is_huggingface_hub_greater_or_equal,\n     is_ipex_available,\n-    is_jieba_available,\n     is_jinja_available,\n     is_jumanpp_available,\n     is_keras_nlp_available,\n@@ -508,13 +507,6 @@ def require_rjieba(test_case):\n     return unittest.skipUnless(is_rjieba_available(), \"test requires rjieba\")(test_case)\n \n \n-def require_jieba(test_case):\n-    \"\"\"\n-    Decorator marking a test that requires jieba. These tests are skipped when jieba isn't installed.\n-    \"\"\"\n-    return unittest.skipUnless(is_jieba_available(), \"test requires jieba\")(test_case)\n-\n-\n def require_jinja(test_case):\n     \"\"\"\n     Decorator marking a test that requires jinja. These tests are skipped when jinja isn't installed."
        },
        {
            "sha": "e9df734240a87e02efdabd6fddf55a341e82b154",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1363fceeec955db125141c3d7768f8ef345ed288/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1363fceeec955db125141c3d7768f8ef345ed288/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=1363fceeec955db125141c3d7768f8ef345ed288",
            "patch": "@@ -173,7 +173,6 @@\n     is_huggingface_hub_greater_or_equal,\n     is_in_notebook,\n     is_ipex_available,\n-    is_jieba_available,\n     is_jinja_available,\n     is_jumanpp_available,\n     is_kenlm_available,"
        },
        {
            "sha": "4b4a99f7364bbd475ac718b33c914f5c93ad43c6",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 9,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/1363fceeec955db125141c3d7768f8ef345ed288/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1363fceeec955db125141c3d7768f8ef345ed288/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=1363fceeec955db125141c3d7768f8ef345ed288",
            "patch": "@@ -167,7 +167,6 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n _g2p_en_available = _is_package_available(\"g2p_en\")\n _hadamard_available = _is_package_available(\"fast_hadamard_transform\")\n _ipex_available, _ipex_version = _is_package_available(\"intel_extension_for_pytorch\", return_version=True)\n-_jieba_available = _is_package_available(\"jieba\")\n _jinja_available = _is_package_available(\"jinja2\")\n _kenlm_available = _is_package_available(\"kenlm\")\n _keras_nlp_available = _is_package_available(\"keras_nlp\")\n@@ -1588,10 +1587,6 @@ def is_cython_available() -> bool:\n     return importlib.util.find_spec(\"pyximport\") is not None\n \n \n-def is_jieba_available() -> Union[tuple[bool, str], bool]:\n-    return _jieba_available\n-\n-\n def is_jinja_available() -> Union[tuple[bool, str], bool]:\n     return _jinja_available\n \n@@ -2017,9 +2012,9 @@ def check_torch_load_is_safe() -> None:\n Cython`. Please note that you may need to restart your runtime after installation.\n \"\"\"\n \n-JIEBA_IMPORT_ERROR = \"\"\"\n-{0} requires the jieba library but it was not found in your environment. You can install it with pip: `pip install\n-jieba`. Please note that you may need to restart your runtime after installation.\n+RJIEBA_IMPORT_ERROR = \"\"\"\n+{0} requires the rjieba library but it was not found in your environment. You can install it with pip: `pip install\n+rjieba`. Please note that you may need to restart your runtime after installation.\n \"\"\"\n \n PEFT_IMPORT_ERROR = \"\"\"\n@@ -2085,7 +2080,7 @@ def check_torch_load_is_safe() -> None:\n         (\"accelerate\", (is_accelerate_available, ACCELERATE_IMPORT_ERROR)),\n         (\"oneccl_bind_pt\", (is_ccl_available, CCL_IMPORT_ERROR)),\n         (\"cython\", (is_cython_available, CYTHON_IMPORT_ERROR)),\n-        (\"jieba\", (is_jieba_available, JIEBA_IMPORT_ERROR)),\n+        (\"rjieba\", (is_rjieba_available, RJIEBA_IMPORT_ERROR)),\n         (\"peft\", (is_peft_available, PEFT_IMPORT_ERROR)),\n         (\"jinja\", (is_jinja_available, JINJA_IMPORT_ERROR)),\n         (\"yt_dlp\", (is_yt_dlp_available, YT_DLP_IMPORT_ERROR)),"
        },
        {
            "sha": "e04d9504cb86cd58b1799249fd5c88306c8c904b",
            "filename": "tests/models/cpmant/test_tokenization_cpmant.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/1363fceeec955db125141c3d7768f8ef345ed288/tests%2Fmodels%2Fcpmant%2Ftest_tokenization_cpmant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1363fceeec955db125141c3d7768f8ef345ed288/tests%2Fmodels%2Fcpmant%2Ftest_tokenization_cpmant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcpmant%2Ftest_tokenization_cpmant.py?ref=1363fceeec955db125141c3d7768f8ef345ed288",
            "patch": "@@ -16,12 +16,12 @@\n import unittest\n \n from transformers.models.cpmant.tokenization_cpmant import VOCAB_FILES_NAMES, CpmAntTokenizer\n-from transformers.testing_utils import require_jieba, tooslow\n+from transformers.testing_utils import require_rjieba, tooslow\n \n from ...test_tokenization_common import TokenizerTesterMixin\n \n \n-@require_jieba\n+@require_rjieba\n class CPMAntTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     from_pretrained_id = \"openbmb/cpm-ant-10b\"\n     tokenizer_class = CpmAntTokenizer\n@@ -57,14 +57,14 @@ def setUpClass(cls):\n     def test_pre_tokenization(self):\n         tokenizer = CpmAntTokenizer.from_pretrained(\"openbmb/cpm-ant-10b\")\n         texts = \"今天天气真好！\"\n-        jieba_tokens = [\"今天\", \"天气\", \"真\", \"好\", \"！\"]\n+        rjieba_tokens = [\"今天\", \"天气\", \"真\", \"好\", \"！\"]\n         tokens = tokenizer.tokenize(texts)\n-        self.assertListEqual(tokens, jieba_tokens)\n+        self.assertListEqual(tokens, rjieba_tokens)\n         normalized_text = \"今天天气真好！\"\n         input_tokens = [tokenizer.bos_token] + tokens\n \n-        input_jieba_tokens = [6, 9802, 14962, 2082, 831, 244]\n-        self.assertListEqual(tokenizer.convert_tokens_to_ids(input_tokens), input_jieba_tokens)\n+        input_rjieba_tokens = [6, 9802, 14962, 2082, 831, 244]\n+        self.assertListEqual(tokenizer.convert_tokens_to_ids(input_tokens), input_rjieba_tokens)\n \n-        reconstructed_text = tokenizer.decode(input_jieba_tokens)\n+        reconstructed_text = tokenizer.decode(input_rjieba_tokens)\n         self.assertEqual(reconstructed_text, normalized_text)"
        }
    ],
    "stats": {
        "total": 94,
        "additions": 40,
        "deletions": 54
    }
}