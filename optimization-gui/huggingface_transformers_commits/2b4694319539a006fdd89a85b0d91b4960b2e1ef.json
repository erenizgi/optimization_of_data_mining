{
    "author": "yonigozlan",
    "message": "Add GOT-OCR 2.0 to Transformers (#34721)\n\n* init modular got_ocr2\r\n\r\n* Get correct got_ocr architecture\r\n\r\n* add processing\r\n\r\n* run modular with processing\r\n\r\n* add working inference\r\n\r\n* apply modular\r\n\r\n* Refactor and fix style\r\n\r\n* Refactor, cleanup, fix style\r\n\r\n* fix init order\r\n\r\n* Fix docs\r\n\r\n* add base modeling tests\r\n\r\n* fix style and consistency\r\n\r\n* rename doc file\r\n\r\n* fix repo consistency\r\n\r\n* fix inference with box\r\n\r\n* add image processing and support for crop_to_multi_page\r\n\r\n* Fix batch inference\r\n\r\n* add tests\r\n\r\n* fixup\r\n\r\n* fix slow test\r\n\r\n* fix docstrings\r\n\r\n* Add model doc\r\n\r\n* update to new init\r\n\r\n* fix input autocast pixel_values dtype\r\n\r\n* update doc\r\n\r\n* move doc to multimodal\r\n\r\n* Reformat crop_image_to_patches and add docstrings\r\n\r\n* Fix example in forward docstring\r\n\r\n* Address Pablo review\r\n\r\n* [run slow] got_ocr2\r\n\r\n* remove defaults defined twice\r\n\r\n* apply modular\r\n\r\n* add torch_device to integration tests\r\n\r\n* update modular\r\n\r\n* follow-up Pavel review\r\n\r\n* add device variable in doc\r\n\r\n* fix doc multi-page\r\n\r\n* Force eager attention for vision encoder to avoid attn implementation conflict\r\n\r\n* revert qwen2vl doc changes\r\n\r\n* use Qwen2ForCausalLM instead of Qwen2Model\r\n\r\n* make fixup\r\n\r\n* refactor gotocr2 to llava style\r\n\r\n* uniformize function names and reduce checks\r\n\r\n* final nits\r\n\r\n* fix pixel_values dtype error\r\n\r\n* change checkpoint names\r\n\r\n* fix modular",
    "sha": "2b4694319539a006fdd89a85b0d91b4960b2e1ef",
    "files": [
        {
            "sha": "2a2cf4512af416c28dce1ce9321e015463cc6740",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4694319539a006fdd89a85b0d91b4960b2e1ef/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4694319539a006fdd89a85b0d91b4960b2e1ef/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=2b4694319539a006fdd89a85b0d91b4960b2e1ef",
            "patch": "@@ -872,6 +872,8 @@\n         title: FLAVA\n       - local: model_doc/git\n         title: GIT\n+      - local: model_doc/got_ocr2\n+        title: GOT-OCR2\n       - local: model_doc/grounding-dino\n         title: Grounding DINO\n       - local: model_doc/groupvit"
        },
        {
            "sha": "92a082f0a911a34b024cd34396cd317e41842e50",
            "filename": "docs/source/en/index.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4694319539a006fdd89a85b0d91b4960b2e1ef/docs%2Fsource%2Fen%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4694319539a006fdd89a85b0d91b4960b2e1ef/docs%2Fsource%2Fen%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Findex.md?ref=2b4694319539a006fdd89a85b0d91b4960b2e1ef",
            "patch": "@@ -161,6 +161,7 @@ Flax), PyTorch, and/or TensorFlow.\n |                           [GIT](model_doc/git)                           |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n |                           [GLM](model_doc/glm)                           |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n |                          [GLPN](model_doc/glpn)                          |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n+|                      [GOT-OCR2](model_doc/got_ocr2)                      |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n |                       [GPT Neo](model_doc/gpt_neo)                       |       ‚úÖ        |         ‚ùå         |      ‚úÖ      |\n |                      [GPT NeoX](model_doc/gpt_neox)                      |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n |             [GPT NeoX Japanese](model_doc/gpt_neox_japanese)             |       ‚úÖ        |         ‚ùå         |      ‚ùå      |"
        },
        {
            "sha": "a560f78269cc721a0c6ebb781364eacd434d2f98",
            "filename": "docs/source/en/model_doc/got_ocr2.md",
            "status": "added",
            "additions": 269,
            "deletions": 0,
            "changes": 269,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4694319539a006fdd89a85b0d91b4960b2e1ef/docs%2Fsource%2Fen%2Fmodel_doc%2Fgot_ocr2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4694319539a006fdd89a85b0d91b4960b2e1ef/docs%2Fsource%2Fen%2Fmodel_doc%2Fgot_ocr2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgot_ocr2.md?ref=2b4694319539a006fdd89a85b0d91b4960b2e1ef",
            "patch": "@@ -0,0 +1,269 @@\n+<!--Copyright 2024 StepFun and The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# GOT-OCR2\n+\n+## Overview\n+\n+The GOT-OCR2 model was proposed in [General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model](https://arxiv.org/abs/2409.01704) by Haoran Wei, Chenglong Liu, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu, Zheng Ge, Liang Zhao, Jianjian Sun, Yuang Peng, Chunrui Han, Xiangyu Zhang.\n+\n+The abstract from the paper is the following:\n+\n+*Traditional OCR systems (OCR-1.0) are increasingly unable to meet people‚Äôsnusage due to the growing demand for intelligent processing of man-made opticalncharacters. In this paper, we collectively refer to all artificial optical signals (e.g., plain texts, math/molecular formulas, tables, charts, sheet music, and even geometric shapes) as \"characters\" and propose the General OCR Theory along with an excellent model, namely GOT, to promote the arrival of OCR-2.0. The GOT, with 580M parameters, is a unified, elegant, and end-to-end model, consisting of a high-compression encoder and a long-contexts decoder. As an OCR-2.0 model, GOT can handle all the above \"characters\" under various OCR tasks. On the input side, the model supports commonly used scene- and document-style images in slice and whole-page styles. On the output side, GOT can generate plain or formatted results (markdown/tikz/smiles/kern) via an easy prompt. Besides, the model enjoys interactive OCR features, i.e., region-level recognition guided by coordinates or colors. Furthermore, we also adapt dynamic resolution and multipage OCR technologies to GOT for better practicality. In experiments, we provide sufficient results to prove the superiority of our model.*\n+\n+<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/got_ocr_overview.png\"\n+alt=\"drawing\" width=\"600\"/>\n+\n+<small> GOT-OCR2 training stages. Taken from the <a href=\"https://arxiv.org/abs/2409.01704\">original paper.</a> </small>\n+\n+\n+Tips:\n+\n+GOT-OCR2 works on a wide range of tasks, including plain document OCR, scene text OCR, formatted document OCR, and even OCR for tables, charts, mathematical formulas, geometric shapes, molecular formulas and sheet music. While this implementation of the model will only output plain text, the outputs can be further processed to render the desired format, with packages like `pdftex`, `mathpix`, `matplotlib`, `tikz`, `verovio` or `pyecharts`.\n+The model can also be used for interactive OCR, where the user can specify the region to be recognized by providing the coordinates or the color of the region's bounding box.\n+\n+This model was contributed by [yonigozlan](https://huggingface.co/yonigozlan).\n+The original code can be found [here](https://github.com/Ucas-HaoranWei/GOT-OCR2.0).\n+\n+## Usage example\n+\n+### Plain text inference\n+\n+```python\n+>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n+\n+>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+>>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)\n+>>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\")\n+\n+>>> image = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/image_ocr.jpg\"\n+>>> inputs = processor(image, return_tensors=\"pt\").to(device)\n+\n+>>> generate_ids = model.generate(\n+...     **inputs,\n+...     do_sample=False,\n+...     tokenizer=processor.tokenizer,\n+...     stop_strings=\"<|im_end|>\",\n+...     max_new_tokens=4096,\n+... )\n+\n+>>> processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n+\"R&D QUALITY IMPROVEMENT\\nSUGGESTION/SOLUTION FORM\\nName/Phone Ext. : (...)\"\n+```\n+\n+### Plain text inference batched\n+\n+```python\n+>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n+\n+>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+>>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)\n+>>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\")\n+\n+>>> image1 = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/multi_box.png\"\n+>>> image2 = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/image_ocr.jpg\"\n+\n+>>> inputs = processor([image1, image2], return_tensors=\"pt\").to(device)\n+\n+>>> generate_ids = model.generate(\n+...     **inputs,\n+...     do_sample=False,\n+...     tokenizer=processor.tokenizer,\n+...     stop_strings=\"<|im_end|>\",\n+...     max_new_tokens=4,\n+... )\n+\n+>>> processor.batch_decode(generate_ids[:, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n+[\"Reducing the number\", \"R&D QUALITY\"]\n+```\n+\n+### Formatted text inference\n+\n+GOT-OCR2 can also generate formatted text, such as markdown or LaTeX. Here is an example of how to generate formatted text:\n+\n+```python\n+>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n+\n+>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+>>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)\n+>>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\")\n+\n+>>> image = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/latex.png\"\n+>>> inputs = processor(image, return_tensors=\"pt\", format=True).to(device)\n+\n+>>> generate_ids = model.generate(\n+...     **inputs,\n+...     do_sample=False,\n+...     tokenizer=processor.tokenizer,\n+...     stop_strings=\"<|im_end|>\",\n+...     max_new_tokens=4096,\n+... )\n+\n+>>> processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n+\"\\\\author{\\nHanwen Jiang* \\\\(\\\\quad\\\\) Arjun Karpur \\\\({ }^{\\\\dagger} \\\\quad\\\\) Bingyi Cao \\\\({ }^{\\\\dagger} \\\\quad\\\\) (...)\"\n+```\n+\n+### Inference on multiple pages\n+\n+Although it might be reasonable in most cases to use a ‚Äúfor loop‚Äù for multi-page processing, some text data with formatting across several pages make it necessary to process all pages at once. GOT introduces a multi-page OCR (without ‚Äúfor loop‚Äù) feature, where multiple pages can be processed by the model at once, whith the output being one continuous text.\n+Here is an example of how to process multiple pages at once:\n+\n+\n+```python\n+>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n+\n+>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+>>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)\n+>>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\")\n+\n+>>> image1 = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/page1.png\"\n+>>> image2 = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/page2.png\"\n+>>> inputs = processor([image1, image2], return_tensors=\"pt\", multi_page=True, format=True).to(device)\n+\n+>>> generate_ids = model.generate(\n+...     **inputs,\n+...     do_sample=False,\n+...     tokenizer=processor.tokenizer,\n+...     stop_strings=\"<|im_end|>\",\n+...     max_new_tokens=4096,\n+... )\n+\n+>>> processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n+\"\\\\title{\\nGeneral OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model\\n}\\n\\\\author{\\nHaoran Wei (...)\"\n+```\n+\n+### Inference on cropped patches\n+\n+GOT supports a 1024√ó1024 input resolution, which is sufficient for most OCR tasks, such as scene OCR or processing A4-sized PDF pages. However, certain scenarios, like horizontally stitched two-page PDFs commonly found in academic papers or images with unusual aspect ratios, can lead to accuracy issues when processed as a single image. To address this, GOT can dynamically crop an image into patches, process them all at once, and merge the results for better accuracy with such inputs.\n+Here is an example of how to process cropped patches:\n+\n+```python\n+>>> import torch\n+>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n+\n+>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+>>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", torch_dtype=torch.bfloat16, device_map=device)\n+>>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\")\n+\n+>>> image = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/one_column.png\"\n+>>> inputs = processor(image, return_tensors=\"pt\", format=True, crop_to_patches=True, max_patches=3).to(device)\n+\n+>>> generate_ids = model.generate(\n+...     **inputs,\n+...     do_sample=False,\n+...     tokenizer=processor.tokenizer,\n+...     stop_strings=\"<|im_end|>\",\n+...     max_new_tokens=4096,\n+... )\n+\n+>>> processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n+\"on developing architectural improvements to make learnable matching methods generalize.\\nMotivated by the above observations, (...)\"\n+```\n+\n+### Inference on a specific region\n+\n+GOT supports interactive OCR, where the user can specify the region to be recognized by providing the coordinates or the color of the region's bounding box. Here is an example of how to process a specific region:\n+\n+```python\n+>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n+\n+>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+>>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)\n+>>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\")\n+\n+>>> image = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/multi_box.png\"\n+>>> inputs = processor(image, return_tensors=\"pt\", color=\"green\").to(device) # or box=[x1, y1, x2, y2] for coordinates (image pixels)\n+\n+>>> generate_ids = model.generate(\n+...     **inputs,\n+...     do_sample=False,\n+...     tokenizer=processor.tokenizer,\n+...     stop_strings=\"<|im_end|>\",\n+...     max_new_tokens=4096,\n+... )\n+\n+>>> processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n+\"You should keep in mind what features from the module should be used, especially \\nwhen you‚Äôre planning to sell a template.\"\n+```\n+\n+### Inference on general OCR data example: sheet music\n+\n+Although this implementation of the model will only output plain text, the outputs can be further processed to render the desired format, with packages like `pdftex`, `mathpix`, `matplotlib`, `tikz`, `verovio` or `pyecharts`.\n+Here is an example of how to process sheet music:\n+\n+```python\n+>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n+>>> import verovio\n+\n+>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+>>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)\n+>>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\")\n+\n+>>> image = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/sheet_music.png\"\n+>>> inputs = processor(image, return_tensors=\"pt\", format=True).to(device)\n+\n+>>> generate_ids = model.generate(\n+...     **inputs,\n+...     do_sample=False,\n+...     tokenizer=processor.tokenizer,\n+...     stop_strings=\"<|im_end|>\",\n+...     max_new_tokens=4096,\n+... )\n+\n+>>> outputs = processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n+>>> tk = verovio.toolkit()\n+>>> tk.loadData(outputs)\n+>>> tk.setOptions(\n+...     {\n+...         \"pageWidth\": 2100,\n+...         \"pageHeight\": 800,\n+...         \"footer\": \"none\",\n+...         \"barLineWidth\": 0.5,\n+...         \"beamMaxSlope\": 15,\n+...         \"staffLineWidth\": 0.2,\n+...         \"spacingStaff\": 6,\n+...     }\n+... )\n+>>> tk.getPageCount()\n+>>> svg = tk.renderToSVG()\n+>>> svg = svg.replace('overflow=\"inherit\"', 'overflow=\"visible\"')\n+>>> with open(\"output.svg\", \"w\") as f:\n+>>>     f.write(svg)\n+```\n+<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/sheet_music.svg\"\n+alt=\"drawing\" width=\"600\"/>\n+\n+## GotOcr2Config\n+\n+[[autodoc]] GotOcr2Config\n+\n+## GotOcr2VisionConfig\n+\n+[[autodoc]] GotOcr2VisionConfig\n+\n+## GotOcr2ImageProcessor\n+\n+[[autodoc]] GotOcr2ImageProcessor\n+\n+## GotOcr2Processor\n+\n+[[autodoc]] GotOcr2Processor\n+\n+## GotOcr2ForConditionalGeneration\n+\n+[[autodoc]] GotOcr2ForConditionalGeneration\n+    - forward\n+"
        },
        {
            "sha": "5ffdefec9e84eada29357e4734ef26730c2e85c3",
            "filename": "docs/source/en/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4694319539a006fdd89a85b0d91b4960b2e1ef/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4694319539a006fdd89a85b0d91b4960b2e1ef/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md?ref=2b4694319539a006fdd89a85b0d91b4960b2e1ef",
            "patch": "@@ -52,6 +52,7 @@ FlashAttention-2 is currently supported for the following architectures:\n * [Emu3](https://huggingface.co/docs/transformers/model_doc/emu3)\n * [Gemma](https://huggingface.co/docs/transformers/model_doc/gemma#transformers.GemmaModel)\n * [Gemma2](https://huggingface.co/docs/transformers/model_doc/gemma2#transformers.Gemma2Model)\n+* [GotOcr2](https://huggingface.co/docs/transformers/model_doc/got_ocr2#transformers.GotOcr2ForConditionalGeneration)\n * [GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)\n * [GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode#transformers.GPTBigCodeModel)\n * [GPTNeo](https://huggingface.co/docs/transformers/model_doc/gpt_neo#transformers.GPTNeoModel)\n@@ -253,6 +254,7 @@ For now, Transformers supports SDPA inference and training for the following arc\n * [Falcon](https://huggingface.co/docs/transformers/model_doc/falcon#transformers.FalconModel)\n * [Gemma](https://huggingface.co/docs/transformers/model_doc/gemma#transformers.GemmaModel)\n * [Gemma2](https://huggingface.co/docs/transformers/model_doc/gemma2#transformers.Gemma2Model)\n+* [GotOcr2](https://huggingface.co/docs/transformers/model_doc/got_ocr2#transformers.GotOcr2ForConditionalGeneration)\n * [Granite](https://huggingface.co/docs/transformers/model_doc/granite#transformers.GraniteModel)\n * [GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)\n * [GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode#transformers.GPTBigCodeModel)"
        },
        {
            "sha": "ae92f21dcc2ae6a3aa3687a78361db4cd343356d",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4694319539a006fdd89a85b0d91b4960b2e1ef/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4694319539a006fdd89a85b0d91b4960b2e1ef/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=2b4694319539a006fdd89a85b0d91b4960b2e1ef",
            "patch": "@@ -476,6 +476,11 @@\n     ],\n     \"models.glm\": [\"GlmConfig\"],\n     \"models.glpn\": [\"GLPNConfig\"],\n+    \"models.got_ocr2\": [\n+        \"GotOcr2Config\",\n+        \"GotOcr2Processor\",\n+        \"GotOcr2VisionConfig\",\n+    ],\n     \"models.gpt2\": [\n         \"GPT2Config\",\n         \"GPT2Tokenizer\",\n@@ -1238,6 +1243,7 @@\n     _import_structure[\"models.flava\"].extend([\"FlavaFeatureExtractor\", \"FlavaImageProcessor\", \"FlavaProcessor\"])\n     _import_structure[\"models.fuyu\"].extend([\"FuyuImageProcessor\", \"FuyuProcessor\"])\n     _import_structure[\"models.glpn\"].extend([\"GLPNFeatureExtractor\", \"GLPNImageProcessor\"])\n+    _import_structure[\"models.got_ocr2\"].extend([\"GotOcr2ImageProcessor\"])\n     _import_structure[\"models.grounding_dino\"].extend([\"GroundingDinoImageProcessor\"])\n     _import_structure[\"models.idefics\"].extend([\"IdeficsImageProcessor\"])\n     _import_structure[\"models.idefics2\"].extend([\"Idefics2ImageProcessor\"])\n@@ -2426,6 +2432,12 @@\n             \"GLPNPreTrainedModel\",\n         ]\n     )\n+    _import_structure[\"models.got_ocr2\"].extend(\n+        [\n+            \"GotOcr2ForConditionalGeneration\",\n+            \"GotOcr2PreTrainedModel\",\n+        ]\n+    )\n     _import_structure[\"models.gpt2\"].extend(\n         [\n             \"GPT2DoubleHeadsModel\",\n@@ -5540,6 +5552,7 @@\n     )\n     from .models.glm import GlmConfig\n     from .models.glpn import GLPNConfig\n+    from .models.got_ocr2 import GotOcr2Config, GotOcr2Processor, GotOcr2VisionConfig\n     from .models.gpt2 import (\n         GPT2Config,\n         GPT2Tokenizer,\n@@ -6342,6 +6355,7 @@\n         )\n         from .models.fuyu import FuyuImageProcessor, FuyuProcessor\n         from .models.glpn import GLPNFeatureExtractor, GLPNImageProcessor\n+        from .models.got_ocr2 import GotOcr2ImageProcessor\n         from .models.grounding_dino import GroundingDinoImageProcessor\n         from .models.idefics import IdeficsImageProcessor\n         from .models.idefics2 import Idefics2ImageProcessor\n@@ -7346,6 +7360,10 @@\n             GLPNModel,\n             GLPNPreTrainedModel,\n         )\n+        from .models.got_ocr2 import (\n+            GotOcr2ForConditionalGeneration,\n+            GotOcr2PreTrainedModel,\n+        )\n         from .models.gpt2 import (\n             GPT2DoubleHeadsModel,\n             GPT2ForQuestionAnswering,"
        },
        {
            "sha": "f62d5d71672bea3c73087a9a41d005b7bbe532c0",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4694319539a006fdd89a85b0d91b4960b2e1ef/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4694319539a006fdd89a85b0d91b4960b2e1ef/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=2b4694319539a006fdd89a85b0d91b4960b2e1ef",
            "patch": "@@ -106,6 +106,7 @@\n     git,\n     glm,\n     glpn,\n+    got_ocr2,\n     gpt2,\n     gpt_bigcode,\n     gpt_neo,"
        },
        {
            "sha": "699e307ac1b6975f32eeeb796d3fbd6ef9884925",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4694319539a006fdd89a85b0d91b4960b2e1ef/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4694319539a006fdd89a85b0d91b4960b2e1ef/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=2b4694319539a006fdd89a85b0d91b4960b2e1ef",
            "patch": "@@ -124,6 +124,7 @@\n         (\"git\", \"GitConfig\"),\n         (\"glm\", \"GlmConfig\"),\n         (\"glpn\", \"GLPNConfig\"),\n+        (\"got_ocr2\", \"GotOcr2Config\"),\n         (\"gpt-sw3\", \"GPT2Config\"),\n         (\"gpt2\", \"GPT2Config\"),\n         (\"gpt_bigcode\", \"GPTBigCodeConfig\"),\n@@ -450,6 +451,7 @@\n         (\"git\", \"GIT\"),\n         (\"glm\", \"GLM\"),\n         (\"glpn\", \"GLPN\"),\n+        (\"got_ocr2\", \"GOT-OCR2\"),\n         (\"gpt-sw3\", \"GPT-Sw3\"),\n         (\"gpt2\", \"OpenAI GPT-2\"),\n         (\"gpt_bigcode\", \"GPTBigCode\"),"
        },
        {
            "sha": "0c4bb9b9a7f7adfd4739840fccced83a0393b149",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4694319539a006fdd89a85b0d91b4960b2e1ef/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4694319539a006fdd89a85b0d91b4960b2e1ef/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=2b4694319539a006fdd89a85b0d91b4960b2e1ef",
            "patch": "@@ -87,6 +87,7 @@\n             (\"fuyu\", (\"FuyuImageProcessor\",)),\n             (\"git\", (\"CLIPImageProcessor\",)),\n             (\"glpn\", (\"GLPNImageProcessor\",)),\n+            (\"got_ocr2\", (\"GotOcr2ImageProcessor\",)),\n             (\"grounding-dino\", (\"GroundingDinoImageProcessor\",)),\n             (\"groupvit\", (\"CLIPImageProcessor\",)),\n             (\"hiera\", (\"BitImageProcessor\",)),"
        },
        {
            "sha": "3b023251e1d9d912be27b2dc60d13f0424c3bfc3",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4694319539a006fdd89a85b0d91b4960b2e1ef/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4694319539a006fdd89a85b0d91b4960b2e1ef/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=2b4694319539a006fdd89a85b0d91b4960b2e1ef",
            "patch": "@@ -119,6 +119,7 @@\n         (\"git\", \"GitModel\"),\n         (\"glm\", \"GlmModel\"),\n         (\"glpn\", \"GLPNModel\"),\n+        (\"got_ocr2\", \"GotOcr2ForConditionalGeneration\"),\n         (\"gpt-sw3\", \"GPT2Model\"),\n         (\"gpt2\", \"GPT2Model\"),\n         (\"gpt_bigcode\", \"GPTBigCodeModel\"),\n@@ -512,6 +513,7 @@\n         (\"gemma2\", \"Gemma2ForCausalLM\"),\n         (\"git\", \"GitForCausalLM\"),\n         (\"glm\", \"GlmForCausalLM\"),\n+        (\"got_ocr2\", \"GotOcr2ForConditionalGeneration\"),\n         (\"gpt-sw3\", \"GPT2LMHeadModel\"),\n         (\"gpt2\", \"GPT2LMHeadModel\"),\n         (\"gpt_bigcode\", \"GPTBigCodeForCausalLM\"),\n@@ -811,6 +813,7 @@\n         (\"emu3\", \"Emu3ForConditionalGeneration\"),\n         (\"fuyu\", \"FuyuForCausalLM\"),\n         (\"git\", \"GitForCausalLM\"),\n+        (\"got_ocr2\", \"GotOcr2ForConditionalGeneration\"),\n         (\"idefics\", \"IdeficsForVisionText2Text\"),\n         (\"idefics2\", \"Idefics2ForConditionalGeneration\"),\n         (\"idefics3\", \"Idefics3ForConditionalGeneration\"),"
        },
        {
            "sha": "f329d9e465e5c0513877ad21def9b9ba8e242a80",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4694319539a006fdd89a85b0d91b4960b2e1ef/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4694319539a006fdd89a85b0d91b4960b2e1ef/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=2b4694319539a006fdd89a85b0d91b4960b2e1ef",
            "patch": "@@ -63,6 +63,7 @@\n         (\"flava\", \"FlavaProcessor\"),\n         (\"fuyu\", \"FuyuProcessor\"),\n         (\"git\", \"GitProcessor\"),\n+        (\"got_ocr2\", \"GotOcr2Processor\"),\n         (\"grounding-dino\", \"GroundingDinoProcessor\"),\n         (\"groupvit\", \"CLIPProcessor\"),\n         (\"hubert\", \"Wav2Vec2Processor\"),"
        },
        {
            "sha": "071a7ea7408112310f2c0eab13845149a95882d2",
            "filename": "src/transformers/models/got_ocr2/__init__.py",
            "status": "added",
            "additions": 31,
            "deletions": 0,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4694319539a006fdd89a85b0d91b4960b2e1ef/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4694319539a006fdd89a85b0d91b4960b2e1ef/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2F__init__.py?ref=2b4694319539a006fdd89a85b0d91b4960b2e1ef",
            "patch": "@@ -0,0 +1,31 @@\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_got_ocr2 import *\n+    from .image_processing_got_ocr2 import *\n+    from .modeling_got_ocr2 import *\n+    from .processing_got_ocr2 import *\n+\n+\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "480252ab1471be660e14ec4ae942ac8b3efe24bc",
            "filename": "src/transformers/models/got_ocr2/configuration_got_ocr2.py",
            "status": "added",
            "additions": 212,
            "deletions": 0,
            "changes": 212,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4694319539a006fdd89a85b0d91b4960b2e1ef/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fconfiguration_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4694319539a006fdd89a85b0d91b4960b2e1ef/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fconfiguration_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fconfiguration_got_ocr2.py?ref=2b4694319539a006fdd89a85b0d91b4960b2e1ef",
            "patch": "@@ -0,0 +1,212 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/got_ocr2/modular_got_ocr2.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_got_ocr2.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# coding=utf-8\n+# Copyright 2024 HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+from ...configuration_utils import PretrainedConfig\n+from ..auto import CONFIG_MAPPING, AutoConfig\n+\n+\n+class GotOcr2VisionConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`GotOcr2VisionModel`]. It is used to instantiate a GOT_OCR2\n+    vision encoder according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    defaults will yield a similar configuration to that of the SAM ViT-h\n+    [facebook/sam-vit-huge](https://huggingface.co/facebook/sam-vit-huge) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 768):\n+            Dimensionality of the encoder layers and the pooler layer.\n+        output_channels (`int`, *optional*, defaults to 256):\n+            Dimensionality of the output channels in the Patch Encoder.\n+        num_hidden_layers (`int`, *optional*, defaults to 12):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 12):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        num_channels (`int`, *optional*, defaults to 3):\n+            Number of channels in the input image.\n+        image_size (`int`, *optional*, defaults to 1024):\n+            Expected resolution. Target size of the resized input image.\n+        patch_size (`int`, *optional*, defaults to 16):\n+            Size of the patches to be extracted from the input image.\n+        hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function (function or string)\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the layer normalization layers.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        initializer_range (`float`, *optional*, defaults to 1e-10):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        qkv_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to add a bias to query, key, value projections.\n+        use_abs_pos (`bool`, *optional*, defaults to `True`):\n+            Whether to use absolute position embedding.\n+        use_rel_pos (`bool`, *optional*, defaults to `True`):\n+            Whether to use relative position embedding.\n+        window_size (`int`, *optional*, defaults to 14):\n+            Window size for relative position.\n+        global_attn_indexes (`List[int]`, *optional*, defaults to `[2, 5, 8, 11]`):\n+            The indexes of the global attention layers.\n+        mlp_dim (`int`, *optional*, defaults to 3072):\n+            The dimensionality of the MLP layer in the Transformer encoder.\n+    \"\"\"\n+\n+    base_config_key = \"vision_config\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=768,\n+        output_channels=256,\n+        num_hidden_layers=12,\n+        num_attention_heads=12,\n+        num_channels=3,\n+        image_size=1024,\n+        patch_size=16,\n+        hidden_act=\"gelu\",\n+        layer_norm_eps=1e-06,\n+        attention_dropout=0.0,\n+        initializer_range=1e-10,\n+        qkv_bias=True,\n+        use_abs_pos=True,\n+        use_rel_pos=True,\n+        window_size=14,\n+        global_attn_indexes=[2, 5, 8, 11],\n+        mlp_dim=3072,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        self.hidden_size = hidden_size\n+        self.output_channels = output_channels\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.hidden_act = hidden_act\n+        self.layer_norm_eps = layer_norm_eps\n+        self.attention_dropout = attention_dropout\n+        self.initializer_range = initializer_range\n+        self.qkv_bias = qkv_bias\n+        self.use_abs_pos = use_abs_pos\n+        self.use_rel_pos = use_rel_pos\n+        self.window_size = window_size\n+        self.global_attn_indexes = global_attn_indexes\n+        self.mlp_dim = mlp_dim\n+\n+\n+class GotOcr2Config(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`GotOcr2ForConditionalGeneration`]. It is used to instantiate a\n+    GotOcr2 model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of GOT-OCR-2.0.\n+\n+    e.g [stepfun-ai/GOT-OCR-2.0-hf](https://huggingface.co/stepfun-ai/GOT-OCR-2.0-hf)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+\n+    Args:\n+        vision_config (`Union[AutoConfig, dict]`,  *optional*, defaults to `CLIPVisionConfig`):\n+            The config object or dictionary of the vision backbone.\n+        text_config (`Union[AutoConfig, dict]`, *optional*, defaults to `LlamaConfig`):\n+            The config object or dictionary of the text backbone.\n+        ignore_index (`int`, *optional*, defaults to -100):\n+            The ignore index for the loss function.\n+        image_token_index (`int`, *optional*, defaults to 151859):\n+            The image token index to encode the image prompt.\n+        image_seq_length (`int`, *optional*, defaults to 576):\n+            Sequence length of one image embedding.\n+        pad_token_id (`int`, *optional*, defaults to -1):\n+            Padding token id.\n+\n+    ```python\n+    >>> from transformers import GotOcr2ForConditionalGeneration, GotOcr2Config\n+\n+    >>> # Initializing a GotOcr2 style configuration\n+    >>> configuration = GotOcr2Config()\n+\n+    >>> # Initializing a model from the Qwen2-VL-7B style configuration\n+    >>> model = GotOcr2ForConditionalGeneration(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"got_ocr2\"\n+    sub_configs = {\"text_config\": AutoConfig, \"vision_config\": GotOcr2VisionConfig}\n+\n+    def __init__(\n+        self,\n+        vision_config=None,\n+        text_config=None,\n+        ignore_index=-100,\n+        image_token_index=151859,\n+        image_seq_length=576,\n+        pad_token_id=-1,\n+        **kwargs,\n+    ):\n+        self.ignore_index = ignore_index\n+        self.image_token_index = image_token_index\n+        self.image_seq_length = image_seq_length\n+        self.pad_token_id = pad_token_id\n+\n+        if vision_config is None:\n+            self.vision_config = GotOcr2VisionConfig()\n+        elif isinstance(vision_config, dict):\n+            self.vision_config = GotOcr2VisionConfig(**vision_config)\n+        elif isinstance(vision_config, GotOcr2VisionConfig):\n+            self.vision_config = vision_config\n+\n+        if isinstance(text_config, dict):\n+            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"qwen2\"\n+            text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n+        elif text_config is None:\n+            text_config = CONFIG_MAPPING[\"qwen2\"](\n+                vocab_size=151860,\n+                hidden_size=1024,\n+                intermediate_size=2816,\n+                num_hidden_layers=24,\n+                num_attention_heads=16,\n+                num_key_value_heads=16,\n+                hidden_act=\"silu\",\n+                max_position_embeddings=32768,\n+                initializer_range=0.02,\n+                rms_norm_eps=1e-6,\n+                use_cache=True,\n+                tie_word_embeddings=True,\n+                rope_theta=1000000.0,\n+                rope_scaling=None,\n+                use_sliding_window=False,\n+                sliding_window=4096,\n+                max_window_layers=21,\n+                attention_dropout=0.0,\n+            )\n+\n+        self.text_config = text_config\n+\n+        super().__init__(**kwargs)\n+\n+\n+__all__ = [\"GotOcr2VisionConfig\", \"GotOcr2Config\"]"
        },
        {
            "sha": "3df7214410e1f73904bf5287b7c03587b4c20dff",
            "filename": "src/transformers/models/got_ocr2/convert_got_ocr2_weights_to_hf.py",
            "status": "added",
            "additions": 274,
            "deletions": 0,
            "changes": 274,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4694319539a006fdd89a85b0d91b4960b2e1ef/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fconvert_got_ocr2_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4694319539a006fdd89a85b0d91b4960b2e1ef/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fconvert_got_ocr2_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fconvert_got_ocr2_weights_to_hf.py?ref=2b4694319539a006fdd89a85b0d91b4960b2e1ef",
            "patch": "@@ -0,0 +1,274 @@\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import argparse\n+import gc\n+import glob\n+import os\n+from typing import List, Optional\n+\n+import regex as re\n+import torch\n+from huggingface_hub import snapshot_download\n+from safetensors import safe_open\n+\n+from transformers import (\n+    GotOcr2Config,\n+    GotOcr2ForConditionalGeneration,\n+    GotOcr2ImageProcessor,\n+    GotOcr2Processor,\n+    PreTrainedTokenizerFast,\n+    is_vision_available,\n+)\n+from transformers.convert_slow_tokenizer import TikTokenConverter\n+from transformers.tokenization_utils import AddedToken\n+\n+\n+if is_vision_available():\n+    from transformers.image_utils import load_image\n+\n+\n+# fmt: off\n+ORIGINAL_TO_CONVERTED_KEY_MAPPING = {\n+    # Vision encoder mapping\n+    r\"model.vision_tower_high.pos_embed\":                           r\"vision_tower.pos_embed\",\n+    r\"model.vision_tower_high.patch_embed.proj\":                    r\"vision_tower.patch_embed.projection\",\n+    r\"model.vision_tower_high.blocks.(\\d+).norm\":                   r\"vision_tower.layers.\\1.layer_norm\",\n+    r\"model.vision_tower_high.blocks.(\\d+).attn\":                   r\"vision_tower.layers.\\1.attn\",\n+    r\"model.vision_tower_high.blocks.(\\d+).mlp\":                    r\"vision_tower.layers.\\1.mlp\",\n+    r\"model.vision_tower_high.neck.0\":                              r\"vision_tower.neck.conv1\",\n+    r\"model.vision_tower_high.neck.1\":                              r\"vision_tower.neck.layer_norm1\",\n+    r\"model.vision_tower_high.neck.2\":                              r\"vision_tower.neck.conv2\",\n+    r\"model.vision_tower_high.neck.3\":                              r\"vision_tower.neck.layer_norm2\",\n+    r\"model.vision_tower_high.net_(\\d+)\":                           lambda m: f\"multi_modal_projector.conv_upsampler{int(m.group(1)) - 1}\",\n+    r\"model.mm_projector_vary\" :                                    r\"multi_modal_projector.multimodal_projector\",\n+    r\"model.\":                                                      r\"language_model.model.\",\n+    r\"lm_head\":                                                     r\"language_model.lm_head\",\n+}\n+# fmt: on\n+\n+CONTEXT_LENGTH = 8000\n+\n+\n+def convert_old_keys_to_new_keys(state_dict_keys: dict = None):\n+    \"\"\"\n+    This function should be applied only once, on the concatenated keys to efficiently rename using\n+    the key mappings.\n+    \"\"\"\n+    output_dict = {}\n+    if state_dict_keys is not None:\n+        old_text = \"\\n\".join(state_dict_keys)\n+        new_text = old_text\n+        for pattern, replacement in ORIGINAL_TO_CONVERTED_KEY_MAPPING.items():\n+            new_text = re.sub(pattern, replacement, new_text)\n+        output_dict = dict(zip(old_text.split(\"\\n\"), new_text.split(\"\\n\")))\n+    return output_dict\n+\n+\n+def load_original_state_dict(model_id):\n+    directory_path = snapshot_download(repo_id=model_id, allow_patterns=[\"*.safetensors\"])\n+\n+    original_state_dict = {}\n+    for path in glob.glob(f\"{directory_path}/*\"):\n+        if path.endswith(\".safetensors\"):\n+            with safe_open(path, framework=\"pt\", device=\"cpu\") as f:\n+                for key in f.keys():\n+                    original_state_dict[key] = f.get_tensor(key)\n+\n+    return original_state_dict\n+\n+\n+def get_got_ocr2_config():\n+    config = GotOcr2Config()\n+\n+    return config\n+\n+\n+def write_model(\n+    model_path,\n+    input_base_path,\n+    push_to_hub=False,\n+):\n+    os.makedirs(model_path, exist_ok=True)\n+\n+    config = get_got_ocr2_config()\n+    config.architectures = [\"GotOcr2ForConditionalGeneration\"]\n+    config.save_pretrained(model_path)\n+    print(\"Model config saved successfully...\")\n+\n+    # ------------------------------------------------------------\n+    # Convert weights\n+    # ------------------------------------------------------------\n+\n+    print(f\"Fetching all parameters from the checkpoint at {input_base_path}...\")\n+    state_dict_old = load_original_state_dict(input_base_path)\n+    print(\"Converting model...\")\n+    all_keys = list(state_dict_old.keys())\n+    new_keys = convert_old_keys_to_new_keys(all_keys)\n+    state_dict = {}\n+    for key in all_keys:\n+        new_key = new_keys[key]\n+        state_dict[new_key] = state_dict_old[key]\n+\n+    del state_dict_old\n+    gc.collect()\n+\n+    print(\"Loading the checkpoint in a GotOcr2ForConditionalGeneration model.\")\n+    model = GotOcr2ForConditionalGeneration(config)\n+    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n+    model = model.to(torch.bfloat16)\n+    print(\"model dtype:\", model.dtype)\n+    print(\"Missing keys:\", missing_keys)\n+    print(\"Unexpected keys:\", unexpected_keys)\n+\n+    print(\"Saving the model.\")\n+    model.save_pretrained(model_path)\n+    if push_to_hub:\n+        model.push_to_hub(\"stepfun-ai/GOT-OCR-2.0-hf\", use_temp_dir=True)\n+    del state_dict, model\n+\n+    # Safety check: reload the converted model\n+    gc.collect()\n+    print(\"Reloading the model to check if it's saved correctly.\")\n+    model = GotOcr2ForConditionalGeneration.from_pretrained(model_path, device_map=\"auto\")\n+    processor = GotOcr2Processor.from_pretrained(model_path)\n+    image = load_image(\n+        \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/image_ocr.jpg\"\n+    )\n+\n+    inputs = processor(image, return_tensors=\"pt\", format=True).to(model.device, dtype=model.dtype)\n+    generate_ids = model.generate(**inputs, do_sample=False, num_beams=1, max_new_tokens=4)\n+    decoded_output = processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n+    expected_output = \"\\\\title{\\nR\"\n+    print(\"Decoded output:\", decoded_output)\n+    assert decoded_output == expected_output\n+    print(\"Model reloaded successfully.\")\n+    del model\n+\n+\n+class GotOcr2Converter(TikTokenConverter):\n+    def __init__(\n+        self,\n+        vocab_file,\n+        special_tokens: List[str],\n+        pattern: str,\n+        model_max_length: int,\n+        chat_template: Optional[str] = None,\n+        **kwargs,\n+    ):\n+        super().__init__(vocab_file, pattern=pattern)\n+        self.additional_special_tokens = special_tokens\n+        tokenizer = self.converted()\n+        if chat_template is not None:\n+            kwargs[\"chat_template\"] = chat_template\n+        self.tokenizer = PreTrainedTokenizerFast(\n+            tokenizer_object=tokenizer,\n+            model_input_names=[\"input_ids\", \"attention_mask\"],\n+            model_max_length=model_max_length,\n+            **kwargs,\n+        )\n+\n+\n+def write_tokenizer(tokenizer_path: str, save_dir: str, push_to_hub: bool = False):\n+    model_max_length = CONTEXT_LENGTH\n+    pattern = r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"  # noqa: W605\n+    # Special tokens\n+    special_tokens = (\n+        [\"<|endoftext|>\", \"<|im_start|>\", \"<|im_end|>\"]\n+        + [f\"<|extra_{i}|>\" for i in range(205)]\n+        + [\n+            \"<ref>\",\n+            \"</ref>\",\n+            \"<box>\",\n+            \"</box>\",\n+            \"<quad>\",\n+            \"</quad>\",\n+            \"<img>\",\n+            \"</img>\",\n+            \"<imgpad>\",\n+        ]\n+    )\n+\n+    pad_token = \"<|endoftext|>\"\n+    pad_token = AddedToken(pad_token, lstrip=False, rstrip=False, normalized=False, single_word=False)\n+\n+    converter = GotOcr2Converter(\n+        vocab_file=tokenizer_path,\n+        pattern=pattern,\n+        special_tokens=special_tokens,\n+        model_max_length=model_max_length,\n+        pad_token=pad_token,\n+        bos_token=\"<|endoftext|>\",\n+        eos_token=\"<|endoftext|>\",\n+        clean_up_tokenization_spaces=True,\n+    )\n+    tokenizer = converter.tokenizer\n+    tokenizer.save_pretrained(save_dir)\n+\n+    if push_to_hub:\n+        tokenizer.push_to_hub(\"stepfun-ai/GOT-OCR-2.0-hf\", use_temp_dir=True)\n+\n+\n+def write_image_processor(save_dir: str, push_to_hub: bool = False):\n+    image_processor = GotOcr2ImageProcessor(\n+        do_resize=True,\n+        size={\"height\": 1024, \"width\": 1024},\n+        do_rescale=True,\n+        rescale_factor=1 / 255,\n+        do_normalize=True,\n+        image_mean=[0.48145466, 0.4578275, 0.40821073],\n+        image_std=[0.26862954, 0.26130258, 0.27577711],\n+    )\n+\n+    image_processor.save_pretrained(save_dir)\n+    if push_to_hub:\n+        image_processor.push_to_hub(\"stepfun-ai/GOT-OCR-2.0-hf\", use_temp_dir=True)\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"--input_dir\",\n+        default=\"stepfun-ai/GOT-OCR2_0\",\n+        help=\"Location of LLaMA weights, which contains tokenizer.model and model folders\",\n+    )\n+    parser.add_argument(\n+        \"--output_dir\",\n+        default=\"GotOcr2\",\n+        help=\"Location to write HF model and tokenizer\",\n+    )\n+\n+    parser.add_argument(\n+        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+    )\n+    args = parser.parse_args()\n+    write_tokenizer(\n+        tokenizer_path=\"qwen.tiktoken\",\n+        save_dir=args.output_dir,\n+        push_to_hub=args.push_to_hub,\n+    )\n+\n+    write_image_processor(\n+        save_dir=args.output_dir,\n+        push_to_hub=args.push_to_hub,\n+    )\n+    write_model(\n+        model_path=args.output_dir,\n+        input_base_path=args.input_dir,\n+        push_to_hub=args.push_to_hub,\n+    )\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        },
        {
            "sha": "7f7a0d7ae4c8af41cf6e9ef24e5f72f12d3a9d75",
            "filename": "src/transformers/models/got_ocr2/image_processing_got_ocr2.py",
            "status": "added",
            "additions": 482,
            "deletions": 0,
            "changes": 482,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4694319539a006fdd89a85b0d91b4960b2e1ef/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4694319539a006fdd89a85b0d91b4960b2e1ef/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2.py?ref=2b4694319539a006fdd89a85b0d91b4960b2e1ef",
            "patch": "@@ -0,0 +1,482 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/got_ocr2/modular_got_ocr2.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_got_ocr2.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# coding=utf-8\n+# Copyright 2024 HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+from functools import lru_cache\n+from typing import Dict, List, Optional, Tuple, Union\n+\n+import numpy as np\n+\n+from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n+from ...image_transforms import (\n+    _rescale_for_pil_conversion,\n+    convert_to_rgb,\n+    resize,\n+    to_channel_dimension_format,\n+    to_pil_image,\n+)\n+from ...image_utils import (\n+    OPENAI_CLIP_MEAN,\n+    OPENAI_CLIP_STD,\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    infer_channel_dimension_format,\n+    is_scaled_image,\n+    make_flat_list_of_images,\n+    to_numpy_array,\n+    valid_images,\n+    validate_preprocess_arguments,\n+)\n+from ...utils import TensorType, filter_out_non_signature_kwargs, is_vision_available, logging\n+\n+\n+if is_vision_available():\n+    import PIL\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+# Similar to image_processing_mllama.get_all_supported_aspect_ratios\n+@lru_cache(maxsize=10)\n+def get_all_supported_aspect_ratios(min_image_tiles: int, max_image_tiles: int) -> List[Tuple[int, int]]:\n+    \"\"\"\n+    Computes all allowed aspect ratios for a given minimum and maximum number of input tiles.\n+\n+    This function calculates all possible arrangements of tiles that can be formed\n+    within the constraint of the minimum and maximum number of tiles. Each arrangement is\n+    represented by its aspect ratio (width/height) and the corresponding tile configuration.\n+\n+    Args:\n+        min_image_tiles (`int`):\n+            The minimum number of tiles allowed.\n+        max_image_tiles (`int`):\n+            The maximum number of tiles allowed.\n+\n+    Returns:\n+        `List[Tuple[int, int]]`: A list of tuples, each tuple representing a valid (width, height)\n+        configuration in terms of number of tiles.\n+\n+    Example:\n+        >>> get_all_supported_aspect_ratios(1, 4)\n+        [(1, 1), (1, 2), (2, 1), (1, 3), (3, 1), (1, 4), (2, 2), (4, 1)]\n+\n+    \"\"\"\n+    aspect_ratios = []\n+    for width in range(1, max_image_tiles + 1):\n+        for height in range(1, max_image_tiles + 1):\n+            if width * height <= max_image_tiles and width * height >= min_image_tiles:\n+                aspect_ratios.append((width, height))\n+\n+    aspect_ratios = sorted(aspect_ratios, key=lambda x: x[0] * x[1])\n+\n+    return aspect_ratios\n+\n+\n+@lru_cache(maxsize=100)\n+def get_optimal_tiled_canvas(\n+    original_image_size: Tuple[int, int],\n+    target_tile_size: Tuple[int, int],\n+    min_image_tiles: int,\n+    max_image_tiles: int,\n+) -> Tuple[int, int]:\n+    \"\"\"\n+    Given a minimum and maximum number of tiles, find the canvas with the closest aspect ratio to the\n+    original image aspect ratio.\n+    In case of tie-breaking condition when two canvases have the same aspect ratio difference, we favor the canvas with\n+    more tiles, until the area covered by the tiles is more than twice the target area, in order to avoid unnecessarily\n+    excessive tiling.\n+    \"\"\"\n+    possible_tile_arrangements = get_all_supported_aspect_ratios(min_image_tiles, max_image_tiles)\n+\n+    original_height, original_width = original_image_size\n+    target_tile_height, target_tile_width = target_tile_size\n+    aspect_ratio = original_width / original_height\n+    area = original_width * original_height\n+\n+    # find the grid with the best aspect ratio\n+    best_ratio_diff = float(\"inf\")\n+    best_grid = (1, 1)\n+    for grid in possible_tile_arrangements:\n+        grid_aspect_ratio = grid[0] / grid[1]\n+        ratio_diff = abs(aspect_ratio - grid_aspect_ratio)\n+        if ratio_diff < best_ratio_diff:\n+            best_ratio_diff = ratio_diff\n+            best_grid = grid\n+        elif ratio_diff == best_ratio_diff:\n+            # if the aspect ratio difference is the same, we favor the grid with more patches\n+            # until the area covered by the patches is more than twice the original image area\n+            if area > 0.5 * target_tile_height * target_tile_width * grid[0] * grid[1]:\n+                best_grid = grid\n+\n+    return best_grid\n+\n+\n+class GotOcr2ImageProcessor(BaseImageProcessor):\n+    r\"\"\"\n+    Constructs a GOT_OCR2 image processor.\n+\n+    Args:\n+        do_resize (`bool`, *optional*, defaults to `True`):\n+            Whether to resize the image's (height, width) dimensions to the specified `size`. Can be overridden by the\n+            `do_resize` parameter in the `preprocess` method.\n+        size (`dict`, *optional*, defaults to `{\"height\": 384, \"width\": 384}`):\n+            Size of the output image after resizing. Can be overridden by the `size` parameter in the `preprocess`\n+            method.\n+        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n+            Resampling filter to use if resizing the image. Only has an effect if `do_resize` is set to `True`. Can be\n+            overridden by the `resample` parameter in the `preprocess` method.\n+        do_rescale (`bool`, *optional*, defaults to `True`):\n+            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by the\n+            `do_rescale` parameter in the `preprocess` method.\n+        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n+            Scale factor to use if rescaling the image. Only has an effect if `do_rescale` is set to `True`. Can be\n+            overridden by the `rescale_factor` parameter in the `preprocess` method.\n+        do_normalize (`bool`, *optional*, defaults to `True`):\n+            Whether to normalize the image. Can be overridden by the `do_normalize` parameter in the `preprocess`\n+            method. Can be overridden by the `do_normalize` parameter in the `preprocess` method.\n+        image_mean (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`):\n+            Mean to use if normalizing the image. This is a float or list of floats the length of the number of\n+            channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method. Can be\n+            overridden by the `image_mean` parameter in the `preprocess` method.\n+        image_std (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`):\n+            Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n+            number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n+            Can be overridden by the `image_std` parameter in the `preprocess` method.\n+        do_convert_rgb (`bool`, *optional*, defaults to `True`):\n+            Whether to convert the image to RGB.\n+    \"\"\"\n+\n+    model_input_names = [\"pixel_values\"]\n+\n+    def __init__(\n+        self,\n+        do_resize: bool = True,\n+        size: Dict[str, int] = None,\n+        resample: PILImageResampling = PILImageResampling.BICUBIC,\n+        do_rescale: bool = True,\n+        rescale_factor: Union[int, float] = 1 / 255,\n+        do_normalize: bool = True,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        do_convert_rgb: bool = True,\n+        **kwargs,\n+    ) -> None:\n+        super().__init__(**kwargs)\n+        size = size if size is not None else {\"height\": 384, \"width\": 384}\n+        size = get_size_dict(size, default_to_square=True)\n+\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.resample = resample\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean if image_mean is not None else OPENAI_CLIP_MEAN\n+        self.image_std = image_std if image_std is not None else OPENAI_CLIP_STD\n+        self.do_convert_rgb = do_convert_rgb\n+\n+    def resize(\n+        self,\n+        image: np.ndarray,\n+        size: Dict[str, int],\n+        resample: PILImageResampling = PILImageResampling.BICUBIC,\n+        data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        **kwargs,\n+    ) -> np.ndarray:\n+        \"\"\"\n+        Resize an image to `(size[\"height\"], size[\"width\"])`.\n+\n+        Args:\n+            image (`np.ndarray`):\n+                Image to resize.\n+            size (`Dict[str, int]`):\n+                Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the output image.\n+            resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`):\n+                `PILImageResampling` filter to use when resizing the image e.g. `PILImageResampling.BICUBIC`.\n+            data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the output image. If unset, the channel dimension format of the input\n+                image is used. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+\n+        Returns:\n+            `np.ndarray`: The resized image.\n+        \"\"\"\n+        size = get_size_dict(size)\n+        if \"height\" not in size or \"width\" not in size:\n+            raise ValueError(f\"The `size` dictionary must contain the keys `height` and `width`. Got {size.keys()}\")\n+        output_size = (size[\"height\"], size[\"width\"])\n+        return resize(\n+            image,\n+            size=output_size,\n+            resample=resample,\n+            data_format=data_format,\n+            input_data_format=input_data_format,\n+            **kwargs,\n+        )\n+\n+    @filter_out_non_signature_kwargs()\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        do_resize: Optional[bool] = None,\n+        size: Optional[Dict[str, int]] = None,\n+        resample: PILImageResampling = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        do_convert_rgb: bool = None,\n+        data_format: ChannelDimension = ChannelDimension.FIRST,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ) -> PIL.Image.Image:\n+        \"\"\"\n+        Preprocess an image or batch of images.\n+\n+        Args:\n+            images (`ImageInput`):\n+                Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n+                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n+            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n+                Whether to resize the image.\n+            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n+                Controls the size of the image after `resize`. The shortest edge of the image is resized to\n+                `size[\"shortest_edge\"]` whilst preserving the aspect ratio. If the longest edge of this resized image\n+                is > `int(size[\"shortest_edge\"] * (1333 / 800))`, then the image is resized again to make the longest\n+                edge equal to `int(size[\"shortest_edge\"] * (1333 / 800))`.\n+            resample (`PILImageResampling`, *optional*, defaults to `self.resample`):\n+                Resampling filter to use if resizing the image. Only has an effect if `do_resize` is set to `True`.\n+            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n+                Whether to rescale the image values between [0 - 1].\n+            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n+                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n+            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n+                Whether to normalize the image.\n+            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n+                Image mean to normalize the image by if `do_normalize` is set to `True`.\n+            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n+                Image standard deviation to normalize the image by if `do_normalize` is set to `True`.\n+            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n+                Whether to convert the image to RGB.\n+            return_tensors (`str` or `TensorType`, *optional*):\n+                The type of tensors to return. Can be one of:\n+                    - Unset: Return a list of `np.ndarray`.\n+                    - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n+                    - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n+                    - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n+                    - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n+            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n+                The channel dimension format for the output image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - Unset: Use the channel dimension format of the input image.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+        \"\"\"\n+        do_resize = do_resize if do_resize is not None else self.do_resize\n+        resample = resample if resample is not None else self.resample\n+        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n+        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n+        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n+        image_mean = image_mean if image_mean is not None else self.image_mean\n+        image_std = image_std if image_std is not None else self.image_std\n+        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n+\n+        size = size if size is not None else self.size\n+        size = get_size_dict(size, default_to_square=False)\n+        images = make_flat_list_of_images(images)\n+\n+        if not valid_images(images):\n+            raise ValueError(\n+                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n+                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n+            )\n+\n+        validate_preprocess_arguments(\n+            do_rescale=do_rescale,\n+            rescale_factor=rescale_factor,\n+            do_normalize=do_normalize,\n+            image_mean=image_mean,\n+            image_std=image_std,\n+            do_resize=do_resize,\n+            size=size,\n+            resample=resample,\n+        )\n+        # PIL RGBA images are converted to RGB\n+        if do_convert_rgb:\n+            images = [convert_to_rgb(image) for image in images]\n+\n+        # All transformations expect numpy arrays.\n+        images = [to_numpy_array(image) for image in images]\n+\n+        if do_rescale and is_scaled_image(images[0]):\n+            logger.warning_once(\n+                \"It looks like you are trying to rescale already rescaled images. If the input\"\n+                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n+            )\n+\n+        if input_data_format is None:\n+            # We assume that all images have the same channel dimension format.\n+            input_data_format = infer_channel_dimension_format(images[0])\n+\n+        if do_resize:\n+            images = [\n+                self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n+                for image in images\n+            ]\n+\n+        if do_rescale:\n+            images = [\n+                self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n+                for image in images\n+            ]\n+\n+        if do_normalize:\n+            images = [\n+                self.normalize(image=image, mean=image_mean, std=image_std, input_data_format=input_data_format)\n+                for image in images\n+            ]\n+\n+        images = [\n+            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format) for image in images\n+        ]\n+\n+        encoded_outputs = BatchFeature(data={\"pixel_values\": images}, tensor_type=return_tensors)\n+\n+        return encoded_outputs\n+\n+    def crop_image_to_patches(\n+        self,\n+        image: ImageInput,\n+        min_patches: int,\n+        max_patches: int,\n+        use_thumbnail: bool = True,\n+        patch_size: Union[Tuple, int, dict] = None,\n+        return_numpy: bool = False,\n+        data_format: ChannelDimension = None,\n+    ):\n+        \"\"\"\n+        Crop the image to patches and return a list of cropped images.\n+        The number of patches and their grid arrangement are determined by the original image size,\n+        the target patch size and the minimum and maximum number of patches.\n+        The aspect ratio of the patches grid is chosen to be the closest to the original image aspect ratio.\n+\n+        Args:\n+            image (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`):\n+                The image to be cropped. The image can be a PIL image, NumPy array or PyTorch tensor.\n+            min_patches (`int`):\n+                The minimum number of patches to be extracted from the image.\n+            max_patches (`int`):\n+                The maximum number of patches to be extracted from the image.\n+            use_thumbnail (`bool`, *optional*, defaults to `True`):\n+                Whether to add a thumbnail image to the list of cropped patches.\n+            patch_size (`int`, `Tuple[int, int]`, `dict`, *optional*):\n+                The size of the output patches.\n+            return_numpy (`bool`, *optional*, defaults to `False`):\n+                Whether to return the cropped images as NumPy arrays.\n+            data_format (`ChannelDimension`, *optional*):\n+                The format of the image data. If `None`, the format is inferred from the input image.\n+\n+        Returns:\n+            List[`PIL.Image.Image`] or List[np.ndarray]: The list of cropped images.\n+        \"\"\"\n+        patch_size = patch_size if patch_size is not None else self.size\n+        patch_size = get_size_dict(patch_size, default_to_square=True)\n+        original_size = get_size_dict(image.size, height_width_order=False)\n+        do_rescale = False\n+        if not isinstance(image, PIL.Image.Image):\n+            do_rescale = _rescale_for_pil_conversion(image)\n+            image = to_pil_image(image, do_rescale=do_rescale)\n+\n+        patch_size_height, patch_size_width = patch_size[\"height\"], patch_size[\"width\"]\n+        original_height, original_width = original_size[\"height\"], original_size[\"width\"]\n+        # find the closest aspect ratio to the target\n+        num_columns, num_rows = get_optimal_tiled_canvas(\n+            (original_height, original_width), (patch_size_height, patch_size_width), min_patches, max_patches\n+        )\n+\n+        # calculate the target width and height\n+        target_width = patch_size_width * num_columns\n+        target_height = patch_size_height * num_rows\n+        num_blocks = num_columns * num_rows\n+\n+        # resize the image so that each patch is of patch_size\n+        resized_image = image.resize((target_width, target_height))\n+\n+        # split the image into patches\n+        processed_images = []\n+        for i in range(num_blocks):\n+            column = i % num_columns\n+            row = i // num_columns\n+            box = (\n+                column * patch_size_width,\n+                row * patch_size_height,\n+                (column + 1) * patch_size_width,\n+                (row + 1) * patch_size_height,\n+            )\n+            # split the image\n+            patch_image = resized_image.crop(box)\n+            processed_images.append(patch_image)\n+\n+        if use_thumbnail and len(processed_images) != 1:\n+            thumbnail_img = image.resize((patch_size_width, patch_size_height))\n+            processed_images.append(thumbnail_img)\n+\n+        if return_numpy:\n+            processed_images_numpy = []\n+            for processed_image in processed_images:\n+                processed_image = np.array(processed_image)\n+                # If the input image channel dimension was of size 1, then it is dropped when converting to a PIL image\n+                # so we need to add it back if necessary.\n+                processed_image = (\n+                    np.expand_dims(processed_image, axis=-1) if processed_image.ndim == 2 else processed_image\n+                )\n+                # The image is always in channels last format after converting from a PIL image\n+                if data_format is not None:\n+                    processed_image = to_channel_dimension_format(\n+                        processed_image, data_format, input_channel_dim=ChannelDimension.LAST\n+                    )\n+                # If an image was rescaled to be in the range [0, 255] before converting to a PIL image, then we need to\n+                # rescale it back to the original range.\n+                processed_image = self.rescale(processed_image, 1 / 255) if do_rescale else processed_image\n+                processed_images_numpy.append(processed_image)\n+            processed_images = processed_images_numpy\n+\n+        return processed_images\n+\n+\n+__all__ = [\"GotOcr2ImageProcessor\"]"
        },
        {
            "sha": "957e05bea75a2b92f672e7706ba6635b37083791",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "added",
            "additions": 1005,
            "deletions": 0,
            "changes": 1005,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4694319539a006fdd89a85b0d91b4960b2e1ef/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4694319539a006fdd89a85b0d91b4960b2e1ef/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=2b4694319539a006fdd89a85b0d91b4960b2e1ef",
            "patch": "@@ -0,0 +1,1005 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/got_ocr2/modular_got_ocr2.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_got_ocr2.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# coding=utf-8\n+# Copyright 2024 HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+import collections\n+from dataclasses import dataclass\n+from typing import List, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+\n+from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n+from ...modeling_outputs import ModelOutput\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import (\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    replace_return_docstrings,\n+)\n+from ..auto import AutoModelForCausalLM\n+from .configuration_got_ocr2 import GotOcr2Config, GotOcr2VisionConfig\n+\n+\n+_CONFIG_FOR_DOC = \"GotOcr2Config\"\n+\n+\n+class GotOcr2MLPBlock(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.lin1 = nn.Linear(config.hidden_size, config.mlp_dim)\n+        self.lin2 = nn.Linear(config.mlp_dim, config.hidden_size)\n+        self.act = ACT2FN[config.hidden_act]\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.lin1(hidden_states)\n+        hidden_states = self.act(hidden_states)\n+        hidden_states = self.lin2(hidden_states)\n+        return hidden_states\n+\n+\n+class GotOcr2VisionAttention(nn.Module):\n+    \"\"\"Multi-head Attention block with relative position embeddings.\"\"\"\n+\n+    def __init__(self, config, window_size):\n+        super().__init__()\n+        input_size = (\n+            (config.image_size // config.patch_size, config.image_size // config.patch_size)\n+            if window_size == 0\n+            else (window_size, window_size)\n+        )\n+\n+        self.num_attention_heads = config.num_attention_heads\n+        head_dim = config.hidden_size // config.num_attention_heads\n+        self.scale = head_dim**-0.5\n+        self.dropout = config.attention_dropout\n+\n+        self.qkv = nn.Linear(config.hidden_size, config.hidden_size * 3, bias=config.qkv_bias)\n+        self.proj = nn.Linear(config.hidden_size, config.hidden_size)\n+\n+        self.use_rel_pos = config.use_rel_pos\n+        if self.use_rel_pos:\n+            if input_size is None:\n+                raise ValueError(\"Input size must be provided if using relative positional encoding.\")\n+\n+            # initialize relative positional embeddings\n+            self.rel_pos_h = nn.Parameter(torch.zeros(2 * input_size[0] - 1, head_dim))\n+            self.rel_pos_w = nn.Parameter(torch.zeros(2 * input_size[1] - 1, head_dim))\n+\n+    def get_rel_pos(self, q_size: int, k_size: int, rel_pos: torch.Tensor) -> torch.Tensor:\n+        \"\"\"\n+        Get relative positional embeddings according to the relative positions of\n+            query and key sizes.\n+\n+        Args:\n+            q_size (int):\n+                size of the query.\n+            k_size (int):\n+                size of key k.\n+            rel_pos (`torch.Tensor`):\n+                relative position embeddings (L, channel).\n+\n+        Returns:\n+            Extracted positional embeddings according to relative positions.\n+        \"\"\"\n+        max_rel_dist = int(2 * max(q_size, k_size) - 1)\n+        # Interpolate rel pos.\n+        rel_pos_resized = F.interpolate(\n+            rel_pos.reshape(1, rel_pos.shape[0], -1).permute(0, 2, 1),\n+            size=max_rel_dist,\n+            mode=\"linear\",\n+        )\n+        rel_pos_resized = rel_pos_resized.reshape(-1, max_rel_dist).permute(1, 0)\n+\n+        # Scale the coords with short length if shapes for q and k are different.\n+        q_coords = torch.arange(q_size)[:, None] * max(k_size / q_size, 1.0)\n+        k_coords = torch.arange(k_size)[None, :] * max(q_size / k_size, 1.0)\n+        relative_coords = (q_coords - k_coords) + (k_size - 1) * max(q_size / k_size, 1.0)\n+\n+        return rel_pos_resized[relative_coords.long()]\n+\n+    def add_decomposed_rel_pos(\n+        self,\n+        attn: torch.Tensor,\n+        query: torch.Tensor,\n+        rel_pos_h: torch.Tensor,\n+        rel_pos_w: torch.Tensor,\n+        q_size: Tuple[int, int],\n+        k_size: Tuple[int, int],\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Calculate decomposed Relative Positional Embeddings from :paper:`mvitv2`.\n+        https://github.com/facebookresearch/mvit/blob/19786631e330df9f3622e5402b4a419a263a2c80/mvit/models/attention.py\n+\n+        Args:\n+            attn (`torch.Tensor`):\n+                attention map.\n+            query (`torch.Tensor`):\n+                query q in the attention layer with shape (batch_size, query_height * query_width, channel).\n+            rel_pos_h (`torch.Tensor`):\n+                relative position embeddings (Lh, channel) for height axis.\n+            rel_pos_w (`torch.Tensor`):\n+                relative position embeddings (Lw, channel) for width axis.\n+            q_size (tuple):\n+                spatial sequence size of query q with (query_height, query_width).\n+            k_size (tuple):\n+                spatial sequence size of key k with (key_height, key_width).\n+\n+        Returns:\n+            attn (`torch.Tensor`):\n+                attention map with added relative positional embeddings.\n+        \"\"\"\n+        query_height, query_width = q_size\n+        key_height, key_width = k_size\n+        relative_position_height = self.get_rel_pos(query_height, key_height, rel_pos_h)\n+        relative_position_width = self.get_rel_pos(query_width, key_width, rel_pos_w)\n+\n+        batch_size, _, dim = query.shape\n+        reshaped_query = query.reshape(batch_size, query_height, query_width, dim)\n+        rel_h = torch.einsum(\"bhwc,hkc->bhwk\", reshaped_query, relative_position_height)\n+        rel_w = torch.einsum(\"bhwc,wkc->bhwk\", reshaped_query, relative_position_width)\n+        attn = attn.reshape(batch_size, query_height, query_width, key_height, key_width)\n+        attn = attn + rel_h[:, :, :, :, None] + rel_w[:, :, :, None, :]\n+        attn = attn.reshape(batch_size, query_height * query_width, key_height * key_width)\n+        return attn\n+\n+    def forward(self, hidden_states: torch.Tensor, output_attentions=False) -> torch.Tensor:\n+        batch_size, height, width, _ = hidden_states.shape\n+        # qkv with shape (3, batch_size, nHead, height * width, channel)\n+        qkv = (\n+            self.qkv(hidden_states)\n+            .reshape(batch_size, height * width, 3, self.num_attention_heads, -1)\n+            .permute(2, 0, 3, 1, 4)\n+        )\n+        # q, k, v with shape (batch_size * nHead, height * width, channel)\n+        query, key, value = qkv.reshape(3, batch_size * self.num_attention_heads, height * width, -1).unbind(0)\n+\n+        attn_weights = (query * self.scale) @ key.transpose(-2, -1)\n+\n+        if self.use_rel_pos:\n+            attn_weights = self.add_decomposed_rel_pos(\n+                attn_weights, query, self.rel_pos_h, self.rel_pos_w, (height, width), (height, width)\n+            )\n+\n+        attn_weights = torch.nn.functional.softmax(attn_weights, dtype=torch.float32, dim=-1).to(query.dtype)\n+\n+        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n+\n+        attn_output = (attn_probs @ value).reshape(batch_size, self.num_attention_heads, height, width, -1)\n+        attn_output = attn_output.permute(0, 2, 3, 1, 4).reshape(batch_size, height, width, -1)\n+\n+        attn_output = self.proj(attn_output)\n+\n+        if output_attentions:\n+            outputs = (attn_output, attn_weights)\n+        else:\n+            outputs = (attn_output, None)\n+\n+        return outputs\n+\n+\n+class GotOcr2VisionLayer(nn.Module):\n+    def __init__(self, config, window_size):\n+        super().__init__()\n+        self.layer_norm1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.attn = GotOcr2VisionAttention(config, window_size)\n+        self.layer_norm2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.mlp = GotOcr2MLPBlock(config)\n+        self.window_size = window_size\n+\n+    def window_partition(self, hidden_states: torch.Tensor, window_size: int) -> Tuple[torch.Tensor, Tuple[int, int]]:\n+        \"\"\"\n+        Args:\n+        Partition into non-overlapping windows with padding if needed.\n+            hidden_states (tensor): input tokens with [batch_size, height, width, channel]. window_size (int): window\n+            size.\n+\n+        Returns:\n+            windows: windows after partition with [batch_size * num_windows, window_size, window_size, channel].\n+            (pad_height, pad_width): padded height and width before partition\n+        \"\"\"\n+        batch_size, height, width, channel = hidden_states.shape\n+\n+        pad_h = (window_size - height % window_size) % window_size\n+        pad_w = (window_size - width % window_size) % window_size\n+        hidden_states = F.pad(hidden_states, (0, 0, 0, pad_w, 0, pad_h))\n+        pad_height, pad_width = height + pad_h, width + pad_w\n+\n+        hidden_states = hidden_states.reshape(\n+            batch_size, pad_height // window_size, window_size, pad_width // window_size, window_size, channel\n+        )\n+        windows = hidden_states.permute(0, 1, 3, 2, 4, 5).contiguous().reshape(-1, window_size, window_size, channel)\n+        return windows, (pad_height, pad_width)\n+\n+    def window_unpartition(\n+        self, windows: torch.Tensor, window_size: int, padding_shape: Tuple[int, int], original_shape: Tuple[int, int]\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Args:\n+        Window unpartition into original sequences and removing padding.\n+            hidden_states (tensor):\n+                input tokens with [batch_size * num_windows, window_size, window_size, channel].\n+            window_size (int):\n+                window size.\n+            padding_shape (Tuple):\n+                padded height and width (pad_height, pad_width).\n+            original_shape (Tuple): original height and width (height, width) before padding.\n+\n+        Returns:\n+            hidden_states: unpartitioned sequences with [batch_size, height, width, channel].\n+        \"\"\"\n+        pad_height, pad_width = padding_shape\n+        height, width = original_shape\n+        batch_size = windows.shape[0] // (pad_height * pad_width // window_size // window_size)\n+        hidden_states = windows.reshape(\n+            batch_size, pad_height // window_size, pad_width // window_size, window_size, window_size, -1\n+        )\n+        hidden_states = (\n+            hidden_states.permute(0, 1, 3, 2, 4, 5).contiguous().reshape(batch_size, pad_height, pad_width, -1)\n+        )\n+\n+        hidden_states = hidden_states[:, :height, :width, :].contiguous()\n+        return hidden_states\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.FloatTensor]:\n+        residual = hidden_states\n+\n+        hidden_states = self.layer_norm1(hidden_states)\n+        # Window partition\n+        if self.window_size > 0:\n+            height, width = hidden_states.shape[1], hidden_states.shape[2]\n+            hidden_states, padding_shape = self.window_partition(hidden_states, self.window_size)\n+\n+        hidden_states, attn_weights = self.attn(\n+            hidden_states=hidden_states,\n+            output_attentions=output_attentions,\n+        )\n+        # Reverse window partition\n+        if self.window_size > 0:\n+            hidden_states = self.window_unpartition(hidden_states, self.window_size, padding_shape, (height, width))\n+\n+        hidden_states = residual + hidden_states\n+        layernorm_output = self.layer_norm2(hidden_states)\n+        hidden_states = hidden_states + self.mlp(layernorm_output)\n+\n+        outputs = (hidden_states,)\n+        if output_attentions:\n+            outputs += (attn_weights,)\n+\n+        return outputs\n+\n+\n+@dataclass\n+class GotOcr2VisionEncoderOutput(ModelOutput):\n+    \"\"\"\n+    Base class for got_ocr2 vision model's outputs that also contains image embeddings obtained by applying the projection\n+    layer to the pooler_output.\n+\n+    Args:\n+        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n+            The image embeddings obtained by applying the projection layer to the pooler_output.\n+        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+            Sequence of hidden-states at the output of the last layer of the model.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+    \"\"\"\n+\n+    image_embeds: Optional[torch.FloatTensor] = None\n+    last_hidden_state: torch.FloatTensor = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+\n+\n+class GotOcr2PatchEmbeddings(nn.Module):\n+    \"\"\"\n+    This class turns `pixel_values` of shape `(batch_size, num_channels, height, width)` into the initial\n+    `hidden_states` (patch embeddings) of shape `(batch_size, seq_length, hidden_size)` to be consumed by a\n+    Transformer.\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        image_size, patch_size = config.image_size, config.patch_size\n+        num_channels, hidden_size = config.num_channels, config.hidden_size\n+        image_size = image_size if isinstance(image_size, collections.abc.Iterable) else (image_size, image_size)\n+        patch_size = patch_size if isinstance(patch_size, collections.abc.Iterable) else (patch_size, patch_size)\n+        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.num_channels = num_channels\n+        self.num_patches = num_patches\n+\n+        self.projection = nn.Conv2d(num_channels, hidden_size, kernel_size=patch_size, stride=patch_size)\n+\n+    def forward(self, pixel_values):\n+        batch_size, num_channels, height, width = pixel_values.shape\n+        if num_channels != self.num_channels:\n+            raise ValueError(\n+                \"Make sure that the channel dimension of the pixel values match with the one set in the configuration.\"\n+            )\n+        if height != self.image_size[0] or width != self.image_size[1]:\n+            raise ValueError(\n+                f\"Input image size ({height}*{width}) doesn't match model ({self.image_size[0]}*{self.image_size[1]}).\"\n+            )\n+        embeddings = self.projection(pixel_values).permute(0, 2, 3, 1)\n+        return embeddings\n+\n+\n+class GotOcr2LayerNorm(nn.Module):\n+    r\"\"\"LayerNorm that supports two data formats: channels_last (default) or channels_first.\n+    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch_size, height,\n+    width, channels) while channels_first corresponds to inputs with shape (batch_size, channels, height, width).\n+    \"\"\"\n+\n+    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(normalized_shape))\n+        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n+        self.eps = eps\n+        self.data_format = data_format\n+        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n+            raise NotImplementedError(f\"Unsupported data format: {self.data_format}\")\n+        self.normalized_shape = (normalized_shape,)\n+\n+    def forward(self, x: torch.Tensor) -> torch.Tensor:\n+        if self.data_format == \"channels_last\":\n+            x = torch.nn.functional.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n+        elif self.data_format == \"channels_first\":\n+            input_dtype = x.dtype\n+            x = x.float()\n+            u = x.mean(1, keepdim=True)\n+            s = (x - u).pow(2).mean(1, keepdim=True)\n+            x = (x - u) / torch.sqrt(s + self.eps)\n+            x = x.to(dtype=input_dtype)\n+            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n+        return x\n+\n+\n+class GotOcr2VisionNeck(nn.Module):\n+    def __init__(self, config: GotOcr2VisionConfig):\n+        super().__init__()\n+        self.config = config\n+\n+        self.conv1 = nn.Conv2d(config.hidden_size, config.output_channels, kernel_size=1, bias=False)\n+        self.layer_norm1 = GotOcr2LayerNorm(config.output_channels, data_format=\"channels_first\")\n+        self.conv2 = nn.Conv2d(config.output_channels, config.output_channels, kernel_size=3, padding=1, bias=False)\n+        self.layer_norm2 = GotOcr2LayerNorm(config.output_channels, data_format=\"channels_first\")\n+\n+    def forward(self, hidden_states):\n+        hidden_states = hidden_states.permute(0, 3, 1, 2)\n+        hidden_states = self.conv1(hidden_states)\n+        hidden_states = self.layer_norm1(hidden_states)\n+\n+        hidden_states = self.conv2(hidden_states)\n+        hidden_states = self.layer_norm2(hidden_states)\n+        return hidden_states\n+\n+\n+class GotOcr2VisionEncoder(nn.Module):\n+    def __init__(self, config: GotOcr2VisionConfig):\n+        super().__init__()\n+        self.config = config\n+        self.image_size = config.image_size\n+\n+        self.patch_embed = GotOcr2PatchEmbeddings(config)\n+\n+        self.pos_embed = None\n+        if config.use_abs_pos:\n+            # Initialize absolute positional embedding with pretrain image size.\n+            self.pos_embed = nn.Parameter(\n+                torch.zeros(\n+                    1,\n+                    config.image_size // config.patch_size,\n+                    config.image_size // config.patch_size,\n+                    config.hidden_size,\n+                )\n+            )\n+\n+        self.layers = nn.ModuleList()\n+        for i in range(config.num_hidden_layers):\n+            layer = GotOcr2VisionLayer(\n+                config,\n+                window_size=config.window_size if i not in config.global_attn_indexes else 0,\n+            )\n+            self.layers.append(layer)\n+\n+        self.neck = GotOcr2VisionNeck(config)\n+\n+        self.gradient_checkpointing = False\n+\n+    def get_input_embeddings(self):\n+        return self.patch_embed\n+\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, GotOcr2VisionEncoderOutput]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if pixel_values is None:\n+            raise ValueError(\"You have to specify pixel_values\")\n+\n+        hidden_states = self.patch_embed(pixel_values)\n+        if self.pos_embed is not None:\n+            hidden_states = hidden_states + self.pos_embed\n+\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attentions = () if output_attentions else None\n+\n+        for i, layer_module in enumerate(self.layers):\n+            if output_hidden_states:\n+                all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    layer_module.__call__,\n+                    hidden_states,\n+                )\n+            else:\n+                layer_outputs = layer_module(hidden_states, output_attentions=output_attentions)\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n+\n+        if output_hidden_states:\n+            all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+        hidden_states = self.neck(hidden_states)\n+\n+        if not return_dict:\n+            outputs = (hidden_states,)\n+            if output_hidden_states:\n+                outputs = outputs + (all_hidden_states,)\n+            if output_attentions:\n+                outputs = outputs + (all_self_attentions,)\n+            return outputs\n+\n+        return GotOcr2VisionEncoderOutput(\n+            last_hidden_state=hidden_states,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attentions,\n+        )\n+\n+\n+class GotOcr2MultiModalProjector(nn.Module):\n+    def __init__(self, config: GotOcr2Config):\n+        super().__init__()\n+        vision_output_channels = config.vision_config.output_channels\n+        language_hidden_size = config.text_config.hidden_size\n+        self.conv_upsampler1 = nn.Conv2d(\n+            vision_output_channels, vision_output_channels * 2, kernel_size=3, stride=2, padding=1, bias=False\n+        )\n+        self.conv_upsampler2 = nn.Conv2d(\n+            vision_output_channels * 2, language_hidden_size, kernel_size=3, stride=2, padding=1, bias=False\n+        )\n+        self.multimodal_projector = nn.Linear(language_hidden_size, language_hidden_size)\n+\n+    def forward(self, vision_embeddings: torch.Tensor) -> torch.Tensor:\n+        hidden_state = self.conv_upsampler1(vision_embeddings)\n+        hidden_state = self.conv_upsampler2(hidden_state)\n+        hidden_state = hidden_state.flatten(2).permute(0, 2, 1)\n+        hidden_state = self.multimodal_projector(hidden_state)\n+        return hidden_state\n+\n+\n+@dataclass\n+class GotOcr2CausalLMOutputWithPast(ModelOutput):\n+    \"\"\"\n+    Base class for GotOcr2 causal language model (or autoregressive) outputs.\n+\n+    Args:\n+        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+            Language modeling loss (for next-token prediction).\n+        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+            `past_key_values` input) to speed up sequential decoding.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+        image_hidden_states (`torch.FloatTensor`, *optional*):\n+            A `torch.FloatTensor` of size (batch_size, num_images, sequence_length, hidden_size)`.\n+            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    logits: torch.FloatTensor = None\n+    past_key_values: Optional[List[torch.FloatTensor]] = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n+GOT_OCR2_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`GotOcr2Config`] or [`GotOcr2VisionConfig`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n+    GOT_OCR2_START_DOCSTRING,\n+)\n+class GotOcr2PreTrainedModel(PreTrainedModel):\n+    config_class = GotOcr2Config\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"GotOcr2VisionAttention\"]\n+    _skip_keys_device_placement = \"past_key_values\"\n+    _supports_cache_class = True\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+\n+    def _init_weights(self, module):\n+        # important: this ported version of GotOcr2 isn't meant for training from scratch - only\n+        # inference and fine-tuning - so the proper init weights code has been removed - the original codebase\n+        # https://github.com/haotian-liu/GotOcr2/tree/main/got_ocr2 should serve for that purpose\n+        std = (\n+            self.config.initializer_range\n+            if hasattr(self.config, \"initializer_range\")\n+            else self.config.text_config.initializer_range\n+        )\n+\n+        if hasattr(module, \"class_embedding\"):\n+            module.class_embedding.data.normal_(mean=0.0, std=std)\n+\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+\n+\n+GOT_OCR2_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):\n+            The tensors corresponding to the input images. Pixel values can be obtained using\n+            [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details ([]`GotOcr2Processor`] uses\n+            [`CLIPImageProcessor`] for processing images).\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n+            `past_key_values`).\n+\n+            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n+            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n+            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n+            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n+            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        vision_feature_layer (`Union[int, List[int]], *optional*, defaults to -2`):\n+            The index of the layer to select the vision feature. If multiple indices are provided,\n+            the vision feature of the corresponding indices will be concatenated to form the\n+            vision features.\n+        vision_feature_select_strategy (`str`, *optional*, defaults to `\"default\"`):\n+            The feature selection strategy used to select the vision feature from the vision backbone.\n+            Can be one of `\"default\"` or `\"full\"`.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"\"\"The GOT_OCR2 model which consists of a vision backbone and a language model.\"\"\",\n+    GOT_OCR2_START_DOCSTRING,\n+)\n+class GotOcr2ForConditionalGeneration(GotOcr2PreTrainedModel, GenerationMixin):\n+    def __init__(self, config: GotOcr2Config):\n+        super().__init__(config)\n+        self.vision_tower = GotOcr2VisionEncoder(config.vision_config)\n+\n+        self.multi_modal_projector = GotOcr2MultiModalProjector(config)\n+        self.vocab_size = config.text_config.vocab_size\n+        self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n+\n+        if self.language_model._tied_weights_keys is not None:\n+            self._tied_weights_keys = [f\"language_model.{k}\" for k in self.language_model._tied_weights_keys]\n+\n+        self.pad_token_id = config.pad_token_id\n+\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.language_model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.language_model.set_input_embeddings(value)\n+\n+    def get_output_embeddings(self):\n+        return self.language_model.get_output_embeddings()\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.language_model.set_output_embeddings(new_embeddings)\n+\n+    def set_decoder(self, decoder):\n+        self.language_model.set_decoder(decoder)\n+\n+    def get_decoder(self):\n+        return self.language_model.get_decoder()\n+\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+    ):\n+        \"\"\"\n+        Obtains image last hidden states from the vision tower and apply multimodal projection.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`)\n+        Returns:\n+            image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).\n+        \"\"\"\n+        image_outputs = self.vision_tower(pixel_values).last_hidden_state\n+        return self.multi_modal_projector(image_outputs)\n+\n+    def _merge_input_ids_with_image_features(self, image_features, inputs_embeds, input_ids, attention_mask, labels):\n+        num_images, num_image_patches, embed_dim = image_features.shape\n+        batch_size, sequence_length = input_ids.shape\n+        left_padding = not torch.sum(input_ids[:, -1] == torch.tensor(self.pad_token_id))\n+        # 1. Create a mask to know where special image tokens are\n+        special_image_token_mask = input_ids == self.config.image_token_index\n+        num_special_image_tokens = torch.sum(special_image_token_mask, dim=-1)\n+        # Compute the maximum embed dimension\n+        max_embed_dim = (num_special_image_tokens.max() * (num_image_patches - 1)) + sequence_length\n+        batch_indices, non_image_indices = torch.where(input_ids != self.config.image_token_index)\n+\n+        # 2. Compute the positions where text should be written\n+        # Calculate new positions for text tokens in merged image-text sequence.\n+        # `special_image_token_mask` identifies image tokens. Each image token will be replaced by `nb_text_tokens_per_images - 1` text tokens.\n+        # `torch.cumsum` computes how each image token shifts subsequent text token positions.\n+        # - 1 to adjust for zero-based indexing, as `cumsum` inherently increases indices by one.\n+        new_token_positions = torch.cumsum((special_image_token_mask * (num_image_patches - 1) + 1), -1) - 1\n+        nb_image_pad = max_embed_dim - 1 - new_token_positions[:, -1]\n+        if left_padding:\n+            new_token_positions += nb_image_pad[:, None]  # offset for left padding\n+        text_to_overwrite = new_token_positions[batch_indices, non_image_indices]\n+\n+        # 3. Create the full embedding, already padded to the maximum position\n+        final_embedding = torch.zeros(\n+            batch_size, max_embed_dim, embed_dim, dtype=inputs_embeds.dtype, device=inputs_embeds.device\n+        )\n+        final_attention_mask = torch.zeros(\n+            batch_size, max_embed_dim, dtype=attention_mask.dtype, device=inputs_embeds.device\n+        )\n+        if labels is not None:\n+            final_labels = torch.full(\n+                (batch_size, max_embed_dim), self.config.ignore_index, dtype=input_ids.dtype, device=input_ids.device\n+            )\n+        # In case the Vision model or the Language model has been offloaded to CPU, we need to manually\n+        # set the corresponding tensors into their correct target device.\n+        target_device = inputs_embeds.device\n+        batch_indices, non_image_indices, text_to_overwrite = (\n+            batch_indices.to(target_device),\n+            non_image_indices.to(target_device),\n+            text_to_overwrite.to(target_device),\n+        )\n+        attention_mask = attention_mask.to(target_device)\n+\n+        # 4. Fill the embeddings based on the mask. If we have [\"hey\" \"<image>\", \"how\", \"are\"]\n+        # we need to index copy on [0, 577, 578, 579] for the text and [1:576] for the image features\n+        final_embedding[batch_indices, text_to_overwrite] = inputs_embeds[batch_indices, non_image_indices]\n+        final_attention_mask[batch_indices, text_to_overwrite] = attention_mask[batch_indices, non_image_indices]\n+        if labels is not None:\n+            final_labels[batch_indices, text_to_overwrite] = labels[batch_indices, non_image_indices]\n+\n+        # 5. Fill the embeddings corresponding to the images. Anything that is not `text_positions` needs filling (#29835)\n+        image_to_overwrite = torch.full(\n+            (batch_size, max_embed_dim), True, dtype=torch.bool, device=inputs_embeds.device\n+        )\n+        image_to_overwrite[batch_indices, text_to_overwrite] = False\n+        if left_padding:\n+            image_to_overwrite &= image_to_overwrite.cumsum(-1) - 1 >= nb_image_pad[:, None].to(target_device)\n+        else:\n+            mask = torch.ones_like(image_to_overwrite, dtype=torch.bool).cumsum(-1) - 1\n+            padding_mask = mask <= new_token_positions[:, -1:].to(target_device)\n+            image_to_overwrite &= padding_mask\n+\n+        if image_to_overwrite.sum() != image_features.shape[:-1].numel():\n+            raise ValueError(\n+                f\"The input provided to the model are wrong. The number of image tokens is {torch.sum(special_image_token_mask)} while\"\n+                f\" the number of image given to the model is {num_images}. This prevents correct indexing and breaks batch generation.\"\n+            )\n+\n+        final_embedding[image_to_overwrite] = image_features.contiguous().reshape(-1, embed_dim).to(target_device)\n+        final_attention_mask |= image_to_overwrite\n+        position_ids = (final_attention_mask.cumsum(-1) - 1).masked_fill_((final_attention_mask == 0), 1)\n+\n+        # 6. Mask out the embedding at padding positions, as we later use the past_key_value value to determine the non-attended tokens.\n+        batch_indices, pad_indices = torch.where(input_ids == self.pad_token_id)\n+        indices_to_mask = new_token_positions[batch_indices, pad_indices]\n+\n+        final_embedding[batch_indices, indices_to_mask] = 0\n+\n+        if labels is None:\n+            final_labels = None\n+\n+        return final_embedding, final_attention_mask, final_labels, position_ids\n+\n+    @add_start_docstrings_to_model_forward(GOT_OCR2_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=GotOcr2CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+    ) -> Union[Tuple, GotOcr2CausalLMOutputWithPast]:\n+        r\"\"\"\n+        Args:\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n+\n+\n+        Returns:\n+\n+        Example:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, GotOcr2ForConditionalGeneration, TextStreamer\n+\n+        >>> model = GotOcr2ForConditionalGeneration.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\").to(\"cuda\")\n+        >>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\")\n+\n+        >>> url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/multi_box.png\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> inputs = processor(image, return_tensors=\"pt\", color=\"green\").to(\"cuda\")\n+\n+        >>> # Generate\n+        >>> streamer = TextStreamer(processor.tokenizer, skip_prompt=True, skip_special_tokens=True)\n+        >>> generate_ids = model.generate(\n+        ...     **inputs,\n+        ...     do_sample=False,\n+        ...     tokenizer = processor.tokenizer,\n+        ...     stop_strings='<|im_end|>',\n+        ...     streamer=streamer,\n+        ...     max_new_tokens=4096,\n+        ... )\n+        \"You should keep in mind what features from the module should be used, especially\n+        when you're planning to sell a template.\"\n+        ```\"\"\"\n+\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if pixel_values is not None and inputs_embeds is not None:\n+            raise ValueError(\n+                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n+            )\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if pixel_values is not None:\n+            image_features = self.get_image_features(pixel_values=pixel_values.to(inputs_embeds.dtype))\n+            n_image_tokens = (input_ids == self.config.image_token_index).sum()\n+            n_image_features = image_features.shape[0] * image_features.shape[1]\n+            if n_image_tokens != n_image_features:\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n+            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+        )\n+\n+        logits = outputs[0]\n+\n+        loss = None\n+        if labels is not None:\n+            # Shift so that tokens < n predict n\n+            if attention_mask is not None:\n+                # we use the input attention mask to shift the logits and labels, because it is 2D.\n+                # we also crop attn mask in case it is longer, which happens in PrefixTuning with peft\n+                shift_attention_mask = attention_mask[:, -(logits.shape[1] - 1) :].to(logits.device)\n+                shift_logits = logits[..., :-1, :][shift_attention_mask.to(logits.device) != 0].contiguous()\n+                shift_labels = labels[..., 1:][shift_attention_mask.to(labels.device) != 0].contiguous()\n+            else:\n+                shift_logits = logits[..., :-1, :].contiguous()\n+                shift_labels = labels[..., 1:].contiguous()\n+            # Flatten the tokens\n+            loss_fct = nn.CrossEntropyLoss()\n+            loss = loss_fct(\n+                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1).to(shift_logits.device)\n+            )\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[1:]\n+            return (loss,) + output if loss is not None else output\n+\n+        return GotOcr2CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+        )\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        inputs_embeds=None,\n+        pixel_values=None,\n+        attention_mask=None,\n+        cache_position=None,\n+        logits_to_keep=None,\n+        **kwargs,\n+    ):\n+        # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n+\n+        model_inputs = self.language_model.prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n+\n+        if cache_position[0] == 0:\n+            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n+            # Otherwise we need pixel values to be passed to model\n+            model_inputs[\"pixel_values\"] = pixel_values\n+\n+        return model_inputs\n+\n+\n+__all__ = [\"GotOcr2PreTrainedModel\", \"GotOcr2ForConditionalGeneration\"]"
        },
        {
            "sha": "899075683eb4fc8d0e7059ff6a77e201859e882b",
            "filename": "src/transformers/models/got_ocr2/modular_got_ocr2.py",
            "status": "added",
            "additions": 984,
            "deletions": 0,
            "changes": 984,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4694319539a006fdd89a85b0d91b4960b2e1ef/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4694319539a006fdd89a85b0d91b4960b2e1ef/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py?ref=2b4694319539a006fdd89a85b0d91b4960b2e1ef",
            "patch": "@@ -0,0 +1,984 @@\n+# coding=utf-8\n+# Copyright 2024 HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+from functools import lru_cache\n+from typing import List, Optional, Tuple, Union\n+\n+import numpy as np\n+import torch\n+import torch.nn as nn\n+import torch.utils.checkpoint\n+\n+from transformers.models.blip.image_processing_blip import BlipImageProcessor\n+from transformers.models.llava.modeling_llava import (\n+    LlavaCausalLMOutputWithPast,\n+    LlavaForConditionalGeneration,\n+    LlavaPreTrainedModel,\n+)\n+from transformers.models.sam.modeling_sam import SamMLPBlock, SamVisionAttention, SamVisionEncoder, SamVisionLayer\n+from transformers.processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, TextKwargs, Unpack\n+from transformers.tokenization_utils_base import (\n+    PreTokenizedInput,\n+    TextInput,\n+)\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...image_processing_utils import BatchFeature, get_size_dict\n+from ...image_transforms import (\n+    _rescale_for_pil_conversion,\n+    to_channel_dimension_format,\n+    to_pil_image,\n+)\n+from ...image_utils import ChannelDimension, ImageInput\n+from ...utils import (\n+    add_start_docstrings_to_model_forward,\n+    is_vision_available,\n+    logging,\n+    replace_return_docstrings,\n+)\n+from ..auto import CONFIG_MAPPING, AutoConfig, AutoModelForCausalLM\n+\n+\n+if is_vision_available():\n+    import PIL\n+\n+    from ...image_utils import load_images\n+\n+logger = logging.get_logger(__name__)\n+\n+_CONFIG_FOR_DOC = \"GotOcr2Config\"\n+\n+\n+class GotOcr2VisionConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`GotOcr2VisionModel`]. It is used to instantiate a GOT_OCR2\n+    vision encoder according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    defaults will yield a similar configuration to that of the SAM ViT-h\n+    [facebook/sam-vit-huge](https://huggingface.co/facebook/sam-vit-huge) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 768):\n+            Dimensionality of the encoder layers and the pooler layer.\n+        output_channels (`int`, *optional*, defaults to 256):\n+            Dimensionality of the output channels in the Patch Encoder.\n+        num_hidden_layers (`int`, *optional*, defaults to 12):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 12):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        num_channels (`int`, *optional*, defaults to 3):\n+            Number of channels in the input image.\n+        image_size (`int`, *optional*, defaults to 1024):\n+            Expected resolution. Target size of the resized input image.\n+        patch_size (`int`, *optional*, defaults to 16):\n+            Size of the patches to be extracted from the input image.\n+        hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function (function or string)\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the layer normalization layers.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        initializer_range (`float`, *optional*, defaults to 1e-10):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        qkv_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to add a bias to query, key, value projections.\n+        use_abs_pos (`bool`, *optional*, defaults to `True`):\n+            Whether to use absolute position embedding.\n+        use_rel_pos (`bool`, *optional*, defaults to `True`):\n+            Whether to use relative position embedding.\n+        window_size (`int`, *optional*, defaults to 14):\n+            Window size for relative position.\n+        global_attn_indexes (`List[int]`, *optional*, defaults to `[2, 5, 8, 11]`):\n+            The indexes of the global attention layers.\n+        mlp_dim (`int`, *optional*, defaults to 3072):\n+            The dimensionality of the MLP layer in the Transformer encoder.\n+    \"\"\"\n+\n+    base_config_key = \"vision_config\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=768,\n+        output_channels=256,\n+        num_hidden_layers=12,\n+        num_attention_heads=12,\n+        num_channels=3,\n+        image_size=1024,\n+        patch_size=16,\n+        hidden_act=\"gelu\",\n+        layer_norm_eps=1e-06,\n+        attention_dropout=0.0,\n+        initializer_range=1e-10,\n+        qkv_bias=True,\n+        use_abs_pos=True,\n+        use_rel_pos=True,\n+        window_size=14,\n+        global_attn_indexes=[2, 5, 8, 11],\n+        mlp_dim=3072,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        self.hidden_size = hidden_size\n+        self.output_channels = output_channels\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.hidden_act = hidden_act\n+        self.layer_norm_eps = layer_norm_eps\n+        self.attention_dropout = attention_dropout\n+        self.initializer_range = initializer_range\n+        self.qkv_bias = qkv_bias\n+        self.use_abs_pos = use_abs_pos\n+        self.use_rel_pos = use_rel_pos\n+        self.window_size = window_size\n+        self.global_attn_indexes = global_attn_indexes\n+        self.mlp_dim = mlp_dim\n+\n+\n+class GotOcr2Config(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`GotOcr2ForConditionalGeneration`]. It is used to instantiate a\n+    GotOcr2 model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of GOT-OCR-2.0.\n+\n+    e.g [stepfun-ai/GOT-OCR-2.0-hf](https://huggingface.co/stepfun-ai/GOT-OCR-2.0-hf)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+\n+    Args:\n+        vision_config (`Union[AutoConfig, dict]`,  *optional*, defaults to `CLIPVisionConfig`):\n+            The config object or dictionary of the vision backbone.\n+        text_config (`Union[AutoConfig, dict]`, *optional*, defaults to `LlamaConfig`):\n+            The config object or dictionary of the text backbone.\n+        ignore_index (`int`, *optional*, defaults to -100):\n+            The ignore index for the loss function.\n+        image_token_index (`int`, *optional*, defaults to 151859):\n+            The image token index to encode the image prompt.\n+        image_seq_length (`int`, *optional*, defaults to 576):\n+            Sequence length of one image embedding.\n+        pad_token_id (`int`, *optional*, defaults to -1):\n+            Padding token id.\n+\n+    ```python\n+    >>> from transformers import GotOcr2ForConditionalGeneration, GotOcr2Config\n+\n+    >>> # Initializing a GotOcr2 style configuration\n+    >>> configuration = GotOcr2Config()\n+\n+    >>> # Initializing a model from the Qwen2-VL-7B style configuration\n+    >>> model = GotOcr2ForConditionalGeneration(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"got_ocr2\"\n+    sub_configs = {\"text_config\": AutoConfig, \"vision_config\": GotOcr2VisionConfig}\n+\n+    def __init__(\n+        self,\n+        vision_config=None,\n+        text_config=None,\n+        ignore_index=-100,\n+        image_token_index=151859,\n+        image_seq_length=576,\n+        pad_token_id=-1,\n+        **kwargs,\n+    ):\n+        self.ignore_index = ignore_index\n+        self.image_token_index = image_token_index\n+        self.image_seq_length = image_seq_length\n+        self.pad_token_id = pad_token_id\n+\n+        if vision_config is None:\n+            self.vision_config = GotOcr2VisionConfig()\n+        elif isinstance(vision_config, dict):\n+            self.vision_config = GotOcr2VisionConfig(**vision_config)\n+        elif isinstance(vision_config, GotOcr2VisionConfig):\n+            self.vision_config = vision_config\n+\n+        if isinstance(text_config, dict):\n+            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"qwen2\"\n+            text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n+        elif text_config is None:\n+            text_config = CONFIG_MAPPING[\"qwen2\"](\n+                vocab_size=151860,\n+                hidden_size=1024,\n+                intermediate_size=2816,\n+                num_hidden_layers=24,\n+                num_attention_heads=16,\n+                num_key_value_heads=16,\n+                hidden_act=\"silu\",\n+                max_position_embeddings=32768,\n+                initializer_range=0.02,\n+                rms_norm_eps=1e-6,\n+                use_cache=True,\n+                tie_word_embeddings=True,\n+                rope_theta=1000000.0,\n+                rope_scaling=None,\n+                use_sliding_window=False,\n+                sliding_window=4096,\n+                max_window_layers=21,\n+                attention_dropout=0.0,\n+            )\n+\n+        self.text_config = text_config\n+\n+        super().__init__(**kwargs)\n+\n+\n+__all__ = [\"GotOcr2VisionConfig\", \"GotOcr2Config\"]\n+\n+\n+class GotOcr2TextKwargs(TextKwargs, total=False):\n+    format: Optional[bool]\n+\n+\n+class GotOcr2ImagesKwargs(ImagesKwargs, total=False):\n+    box: Optional[Union[List, Tuple[float, float], Tuple[float, float, float, float]]]\n+    color: Optional[str]\n+    num_image_tokens: Optional[int]\n+    multi_page: Optional[bool]\n+    crop_to_patches: Optional[bool]\n+    min_patches: Optional[int]\n+    max_patches: Optional[int]\n+\n+\n+class GotOcr2ProcessorKwargs(ProcessingKwargs, total=False):\n+    text_kwargs: GotOcr2TextKwargs\n+    images_kwargs: GotOcr2ImagesKwargs\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"padding\": False,\n+            \"format\": False,\n+        },\n+        \"images_kwargs\": {\n+            \"num_image_tokens\": 256,\n+            \"multi_page\": False,\n+            \"crop_to_patches\": False,\n+            \"min_patches\": 1,\n+            \"max_patches\": 12,\n+        },\n+    }\n+\n+\n+def preprocess_box_annotation(box: Union[List, Tuple], image_size: Tuple[int, int]) -> List:\n+    \"\"\"\n+    Convert box annotation to the format [x1, y1, x2, y2] in the range [0, 1000].\n+    \"\"\"\n+    width, height = image_size\n+    if len(box) == 4:\n+        box[0] = int(box[0] / width * 1000)\n+        box[1] = int(box[1] / height * 1000)\n+        box[2] = int(box[2] / width * 1000)\n+        box[3] = int(box[3] / height * 1000)\n+    else:\n+        raise ValueError(\"Box must be a list or tuple of lists in the form [x1, y1, x2, y2].\")\n+\n+    return list(box)\n+\n+\n+# Similar to image_processing_mllama.get_all_supported_aspect_ratios\n+@lru_cache(maxsize=10)\n+def get_all_supported_aspect_ratios(min_image_tiles: int, max_image_tiles: int) -> List[Tuple[int, int]]:\n+    \"\"\"\n+    Computes all allowed aspect ratios for a given minimum and maximum number of input tiles.\n+\n+    This function calculates all possible arrangements of tiles that can be formed\n+    within the constraint of the minimum and maximum number of tiles. Each arrangement is\n+    represented by its aspect ratio (width/height) and the corresponding tile configuration.\n+\n+    Args:\n+        min_image_tiles (`int`):\n+            The minimum number of tiles allowed.\n+        max_image_tiles (`int`):\n+            The maximum number of tiles allowed.\n+\n+    Returns:\n+        `List[Tuple[int, int]]`: A list of tuples, each tuple representing a valid (width, height)\n+        configuration in terms of number of tiles.\n+\n+    Example:\n+        >>> get_all_supported_aspect_ratios(1, 4)\n+        [(1, 1), (1, 2), (2, 1), (1, 3), (3, 1), (1, 4), (2, 2), (4, 1)]\n+\n+    \"\"\"\n+    aspect_ratios = []\n+    for width in range(1, max_image_tiles + 1):\n+        for height in range(1, max_image_tiles + 1):\n+            if width * height <= max_image_tiles and width * height >= min_image_tiles:\n+                aspect_ratios.append((width, height))\n+\n+    aspect_ratios = sorted(aspect_ratios, key=lambda x: x[0] * x[1])\n+\n+    return aspect_ratios\n+\n+\n+@lru_cache(maxsize=100)\n+def get_optimal_tiled_canvas(\n+    original_image_size: Tuple[int, int],\n+    target_tile_size: Tuple[int, int],\n+    min_image_tiles: int,\n+    max_image_tiles: int,\n+) -> Tuple[int, int]:\n+    \"\"\"\n+    Given a minimum and maximum number of tiles, find the canvas with the closest aspect ratio to the\n+    original image aspect ratio.\n+    In case of tie-breaking condition when two canvases have the same aspect ratio difference, we favor the canvas with\n+    more tiles, until the area covered by the tiles is more than twice the target area, in order to avoid unnecessarily\n+    excessive tiling.\n+    \"\"\"\n+    possible_tile_arrangements = get_all_supported_aspect_ratios(min_image_tiles, max_image_tiles)\n+\n+    original_height, original_width = original_image_size\n+    target_tile_height, target_tile_width = target_tile_size\n+    aspect_ratio = original_width / original_height\n+    area = original_width * original_height\n+\n+    # find the grid with the best aspect ratio\n+    best_ratio_diff = float(\"inf\")\n+    best_grid = (1, 1)\n+    for grid in possible_tile_arrangements:\n+        grid_aspect_ratio = grid[0] / grid[1]\n+        ratio_diff = abs(aspect_ratio - grid_aspect_ratio)\n+        if ratio_diff < best_ratio_diff:\n+            best_ratio_diff = ratio_diff\n+            best_grid = grid\n+        elif ratio_diff == best_ratio_diff:\n+            # if the aspect ratio difference is the same, we favor the grid with more patches\n+            # until the area covered by the patches is more than twice the original image area\n+            if area > 0.5 * target_tile_height * target_tile_width * grid[0] * grid[1]:\n+                best_grid = grid\n+\n+    return best_grid\n+\n+\n+class GotOcr2ImageProcessor(BlipImageProcessor):\n+    def crop_image_to_patches(\n+        self,\n+        image: ImageInput,\n+        min_patches: int,\n+        max_patches: int,\n+        use_thumbnail: bool = True,\n+        patch_size: Union[Tuple, int, dict] = None,\n+        return_numpy: bool = False,\n+        data_format: ChannelDimension = None,\n+    ):\n+        \"\"\"\n+        Crop the image to patches and return a list of cropped images.\n+        The number of patches and their grid arrangement are determined by the original image size,\n+        the target patch size and the minimum and maximum number of patches.\n+        The aspect ratio of the patches grid is chosen to be the closest to the original image aspect ratio.\n+\n+        Args:\n+            image (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`):\n+                The image to be cropped. The image can be a PIL image, NumPy array or PyTorch tensor.\n+            min_patches (`int`):\n+                The minimum number of patches to be extracted from the image.\n+            max_patches (`int`):\n+                The maximum number of patches to be extracted from the image.\n+            use_thumbnail (`bool`, *optional*, defaults to `True`):\n+                Whether to add a thumbnail image to the list of cropped patches.\n+            patch_size (`int`, `Tuple[int, int]`, `dict`, *optional*):\n+                The size of the output patches.\n+            return_numpy (`bool`, *optional*, defaults to `False`):\n+                Whether to return the cropped images as NumPy arrays.\n+            data_format (`ChannelDimension`, *optional*):\n+                The format of the image data. If `None`, the format is inferred from the input image.\n+\n+        Returns:\n+            List[`PIL.Image.Image`] or List[np.ndarray]: The list of cropped images.\n+        \"\"\"\n+        patch_size = patch_size if patch_size is not None else self.size\n+        patch_size = get_size_dict(patch_size, default_to_square=True)\n+        original_size = get_size_dict(image.size, height_width_order=False)\n+        do_rescale = False\n+        if not isinstance(image, PIL.Image.Image):\n+            do_rescale = _rescale_for_pil_conversion(image)\n+            image = to_pil_image(image, do_rescale=do_rescale)\n+\n+        patch_size_height, patch_size_width = patch_size[\"height\"], patch_size[\"width\"]\n+        original_height, original_width = original_size[\"height\"], original_size[\"width\"]\n+        # find the closest aspect ratio to the target\n+        num_columns, num_rows = get_optimal_tiled_canvas(\n+            (original_height, original_width), (patch_size_height, patch_size_width), min_patches, max_patches\n+        )\n+\n+        # calculate the target width and height\n+        target_width = patch_size_width * num_columns\n+        target_height = patch_size_height * num_rows\n+        num_blocks = num_columns * num_rows\n+\n+        # resize the image so that each patch is of patch_size\n+        resized_image = image.resize((target_width, target_height))\n+\n+        # split the image into patches\n+        processed_images = []\n+        for i in range(num_blocks):\n+            column = i % num_columns\n+            row = i // num_columns\n+            box = (\n+                column * patch_size_width,\n+                row * patch_size_height,\n+                (column + 1) * patch_size_width,\n+                (row + 1) * patch_size_height,\n+            )\n+            # split the image\n+            patch_image = resized_image.crop(box)\n+            processed_images.append(patch_image)\n+\n+        if use_thumbnail and len(processed_images) != 1:\n+            thumbnail_img = image.resize((patch_size_width, patch_size_height))\n+            processed_images.append(thumbnail_img)\n+\n+        if return_numpy:\n+            processed_images_numpy = []\n+            for processed_image in processed_images:\n+                processed_image = np.array(processed_image)\n+                # If the input image channel dimension was of size 1, then it is dropped when converting to a PIL image\n+                # so we need to add it back if necessary.\n+                processed_image = (\n+                    np.expand_dims(processed_image, axis=-1) if processed_image.ndim == 2 else processed_image\n+                )\n+                # The image is always in channels last format after converting from a PIL image\n+                if data_format is not None:\n+                    processed_image = to_channel_dimension_format(\n+                        processed_image, data_format, input_channel_dim=ChannelDimension.LAST\n+                    )\n+                # If an image was rescaled to be in the range [0, 255] before converting to a PIL image, then we need to\n+                # rescale it back to the original range.\n+                processed_image = self.rescale(processed_image, 1 / 255) if do_rescale else processed_image\n+                processed_images_numpy.append(processed_image)\n+            processed_images = processed_images_numpy\n+\n+        return processed_images\n+\n+\n+class GotOcr2Processor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs a GotOcr2 processor which wraps a [`GotOcr2ImageProcessor`] and\n+    [`PretrainedTokenizerFast`] tokenizer into a single processor that inherits both the image processor and\n+    tokenizer functionalities. See the [`~GotOcr2Processor.__call__`] and [`~GotOcr2Processor.decode`] for more information.\n+    Args:\n+        image_processor ([`GotOcr2ImageProcessor`], *optional*):\n+            The image processor is a required input.\n+        tokenizer ([`PreTrainedTokenizer`, `PreTrainedTokenizerFast`], *optional*):\n+            The tokenizer is a required input.\n+        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n+            in a chat into a tokenizable string.\n+    \"\"\"\n+\n+    attributes = [\"image_processor\", \"tokenizer\"]\n+    valid_kwargs = [\"chat_template\"]\n+    image_processor_class = \"GotOcr2ImageProcessor\"\n+    tokenizer_class = \"PreTrainedTokenizerFast\"\n+\n+    def __init__(self, image_processor=None, tokenizer=None, chat_template=None, **kwargs):\n+        super().__init__(image_processor, tokenizer, chat_template=chat_template)\n+\n+        self.message_start_token = \"<|im_start|>\"\n+        self.message_end_token = \"<|im_end|>\"\n+        self.img_start_token = \"<img>\"\n+        self.img_end_token = \"</img>\"\n+        self.img_pad_token = \"<imgpad>\"\n+        self.system_query = \"system\\nYou should follow the instructions carefully and explain your answers in detail.\"\n+\n+    def _make_list_of_inputs(self, images, text, box, color, multi_page):\n+        if not isinstance(images, (list, tuple)):\n+            images = [images]\n+            if multi_page:\n+                logger.warning(\"Multi-page inference is enabled but only one image is passed.\")\n+                images = [images]\n+        elif isinstance(images[0], (list, tuple)) and not multi_page:\n+            raise ValueError(\"Nested images are only supported with `multi_page` set to `True`.\")\n+        elif not isinstance(images[0], (list, tuple)) and multi_page:\n+            images = [images]\n+\n+        if isinstance(text, str):\n+            text = [text]\n+\n+        if not isinstance(box[0], (list, tuple)):\n+            # Use the same box for all images\n+            box = [box for _ in range(len(images))]\n+        if not isinstance(color, (list, tuple)):\n+            color = [color for _ in range(len(images))]\n+\n+        return images, text, box, color\n+\n+    def __call__(\n+        self,\n+        images: Optional[ImageInput] = None,\n+        text: Optional[Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]] = None,\n+        audio=None,\n+        videos=None,\n+        **kwargs: Unpack[GotOcr2ProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n+        and `kwargs` arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizerFast.__call__`] to encode the text if `text`\n+        is not `None`, otherwise encode default OCR queries which depends on the `format`, `box`, `color`, `multi_page` and\n+        `crop_to_patches` arguments. To prepare the vision inputs, this method forwards the `images` and `kwrags` arguments to\n+        GotOcr2ImageProcessor's [`~GotOcr2ImageProcessor.__call__`] if `images` is not `None`.\n+\n+        Args:\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. Both channels-first and channels-last formats are supported.\n+            text (`str`, `List[str]`, `List[List[str]]`):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n+            format (`bool`, *optional*):\n+                If set, will add the format token to the query, and the model will return the OCR result with formatting.\n+            box (`List[float]`, `List[Tuple[float, float]]`, `List[Tuple[float, float, float, float]]`, *optional*):\n+                The box annotation to be added to the query. If a list of floats or a tuple of floats is provided, it\n+                will be interpreted as [x1, y1, x2, y2]. If a list of tuples is provided, each tuple should be in the\n+                form (x1, y1, x2, y2).\n+            color (`str`, *optional*):\n+                The color annotation to be added to the query. The model will return the OCR result within the box with\n+                the specified color.\n+            multi_page (`bool`, *optional*):\n+                If set, will enable multi-page inference. The model will return the OCR result across multiple pages.\n+            crop_to_patches (`bool`, *optional*):\n+                If set, will crop the image to patches. The model will return the OCR result upon the patch reference.\n+            min_patches (`int`, *optional*):\n+                The minimum number of patches to be cropped from the image. Only used when `crop_to_patches` is set to\n+                `True`.\n+            max_patches (`int`, *optional*):\n+                The maximum number of patches to be cropped from the image. Only used when `crop_to_patches` is set to\n+                `True`.\n+\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+                If set, will return tensors of a particular framework. Acceptable values are:\n+                - `'tf'`: Return TensorFlow `tf.constant` objects.\n+                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                - `'np'`: Return NumPy `np.ndarray` objects.\n+                - `'jax'`: Return JAX `jnp.ndarray` objects.\n+\n+        Returns:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n+\n+            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n+            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n+              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n+              `None`).\n+            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n+        \"\"\"\n+\n+        output_kwargs = self._merge_kwargs(\n+            GotOcr2ProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+        format_output = output_kwargs[\"text_kwargs\"].pop(\"format\")\n+        num_image_tokens = output_kwargs[\"images_kwargs\"].pop(\"num_image_tokens\")\n+        box = output_kwargs[\"images_kwargs\"].pop(\"box\", [None])\n+        color = output_kwargs[\"images_kwargs\"].pop(\"color\", None)\n+        multi_page = output_kwargs[\"images_kwargs\"].pop(\"multi_page\")\n+        crop_to_patches = output_kwargs[\"images_kwargs\"].pop(\"crop_to_patches\")\n+        min_patches = output_kwargs[\"images_kwargs\"].pop(\"min_patches\")\n+        max_patches = output_kwargs[\"images_kwargs\"].pop(\"max_patches\")\n+\n+        images, text, box, color = self._make_list_of_inputs(images, text, box, color, multi_page)\n+\n+        # Load images as we need to know the image size\n+        images = load_images(images)\n+        if text is None:\n+            text = []\n+            for index, (image_group, box_single, color_single) in enumerate(zip(images, box, color)):\n+                if crop_to_patches:\n+                    image_group = self.image_processor.crop_image_to_patches(\n+                        image_group,\n+                        patch_size=output_kwargs[\"images_kwargs\"].get(\"size\"),\n+                        min_patches=min_patches,\n+                        max_patches=max_patches,\n+                    )\n+                    images[index] = image_group\n+                num_images = len(image_group) if (multi_page or crop_to_patches) else 1\n+                if box_single[0] is not None:\n+                    box_single = preprocess_box_annotation(box_single, image_group.size)\n+                query = (\n+                    f\"{f'[{color_single}] ' if color_single is not None else ''}\"\n+                    f\"{str(box_single) if box_single[0] is not None else ''} \"\n+                    \"OCR\"\n+                    f\"{' with format' if format_output else ''}\"\n+                    f\"{' across multi pages' if multi_page else ''}\"\n+                    f\"{' upon the patch reference' if crop_to_patches else ''}\"\n+                    \": \"\n+                )\n+                prompt = (\n+                    self.message_start_token\n+                    + self.system_query\n+                    + self.message_end_token\n+                    + self.message_start_token\n+                    + \"user\\n\"\n+                    + self.img_start_token\n+                    + self.img_pad_token * num_image_tokens * num_images\n+                    + self.img_end_token\n+                    + \"\\n\"\n+                    + query\n+                    + self.message_end_token\n+                    + self.message_start_token\n+                    + \"assistant\\n\"\n+                )\n+                text.append(prompt)\n+        elif crop_to_patches:\n+            for index, (image_group, box_single, color_single) in enumerate(zip(images, box, color)):\n+                image_group = self.image_processor.crop_image_to_patches(\n+                    image_group,\n+                    patch_size=output_kwargs[\"images_kwargs\"].get(\"size\"),\n+                    min_patches=min_patches,\n+                    max_patches=max_patches,\n+                )\n+                images[index] = image_group\n+\n+        text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n+        if multi_page or crop_to_patches:\n+            # flatten images\n+            images = [image for image_group in images for image in image_group]\n+        image_inputs = self.image_processor(images=images, **output_kwargs[\"images_kwargs\"])\n+\n+        return BatchFeature(data={**text_inputs, **image_inputs})\n+\n+    def batch_decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n+        refer to the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.batch_decode(*args, **kwargs)\n+\n+    def decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n+        the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.decode(*args, **kwargs)\n+\n+    @property\n+    def model_input_names(self):\n+        tokenizer_input_names = self.tokenizer.model_input_names\n+        image_processor_input_names = self.image_processor.model_input_names\n+        return list(tokenizer_input_names) + list(image_processor_input_names)\n+\n+\n+class GotOcr2MLPBlock(SamMLPBlock):\n+    pass\n+\n+\n+class GotOcr2VisionAttention(SamVisionAttention):\n+    pass\n+\n+\n+class GotOcr2VisionLayer(SamVisionLayer):\n+    def __init__(self, config, window_size):\n+        super().__init__()\n+        self.layer_norm1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.attn = GotOcr2VisionAttention(config, window_size)\n+        self.layer_norm2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.mlp = GotOcr2MLPBlock(config)\n+        self.window_size = window_size\n+\n+\n+class GotOcr2VisionEncoder(SamVisionEncoder):\n+    pass\n+\n+\n+class GotOcr2MultiModalProjector(nn.Module):\n+    def __init__(self, config: GotOcr2Config):\n+        super().__init__()\n+        vision_output_channels = config.vision_config.output_channels\n+        language_hidden_size = config.text_config.hidden_size\n+        self.conv_upsampler1 = nn.Conv2d(\n+            vision_output_channels, vision_output_channels * 2, kernel_size=3, stride=2, padding=1, bias=False\n+        )\n+        self.conv_upsampler2 = nn.Conv2d(\n+            vision_output_channels * 2, language_hidden_size, kernel_size=3, stride=2, padding=1, bias=False\n+        )\n+        self.multimodal_projector = nn.Linear(language_hidden_size, language_hidden_size)\n+\n+    def forward(self, vision_embeddings: torch.Tensor) -> torch.Tensor:\n+        hidden_state = self.conv_upsampler1(vision_embeddings)\n+        hidden_state = self.conv_upsampler2(hidden_state)\n+        hidden_state = hidden_state.flatten(2).permute(0, 2, 1)\n+        hidden_state = self.multimodal_projector(hidden_state)\n+        return hidden_state\n+\n+\n+class GotOcr2CausalLMOutputWithPast(LlavaCausalLMOutputWithPast):\n+    pass\n+\n+\n+class GotOcr2PreTrainedModel(LlavaPreTrainedModel):\n+    pass\n+\n+\n+GOT_OCR2_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n+            `past_key_values`).\n+\n+            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n+            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n+            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n+            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n+            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        pixel_values (`torch.FloatTensor` of shape `(seq_length, num_channels * image_size * image_size)):\n+            The tensors corresponding to the input images. Pixel values can be obtained using\n+            [`AutoImageProcessor`]. See [`GotOcr2ImageProcessor.__call__`] for details. [`GotOcr2Processor`] uses\n+            [`GotOcr2ImageProcessor`] for processing images.\n+\"\"\"\n+\n+\n+class GotOcr2ForConditionalGeneration(LlavaForConditionalGeneration):\n+    def __init__(self, config: GotOcr2Config):\n+        super().__init__(config)\n+        self.vision_tower = GotOcr2VisionEncoder(config.vision_config)\n+\n+        self.multi_modal_projector = GotOcr2MultiModalProjector(config)\n+        self.vocab_size = config.text_config.vocab_size\n+        self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n+\n+        if self.language_model._tied_weights_keys is not None:\n+            self._tied_weights_keys = [f\"language_model.{k}\" for k in self.language_model._tied_weights_keys]\n+\n+        self.pad_token_id = config.pad_token_id\n+\n+        self.post_init()\n+\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+    ):\n+        \"\"\"\n+        Obtains image last hidden states from the vision tower and apply multimodal projection.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`)\n+        Returns:\n+            image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).\n+        \"\"\"\n+        image_outputs = self.vision_tower(pixel_values).last_hidden_state\n+        return self.multi_modal_projector(image_outputs)\n+\n+    @add_start_docstrings_to_model_forward(GOT_OCR2_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=GotOcr2CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+    ) -> Union[Tuple, LlavaCausalLMOutputWithPast]:\n+        r\"\"\"\n+        Args:\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n+\n+\n+        Returns:\n+\n+        Example:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, GotOcr2ForConditionalGeneration, TextStreamer\n+\n+        >>> model = GotOcr2ForConditionalGeneration.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\").to(\"cuda\")\n+        >>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\")\n+\n+        >>> url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/multi_box.png\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> inputs = processor(image, return_tensors=\"pt\", color=\"green\").to(\"cuda\")\n+\n+        >>> # Generate\n+        >>> streamer = TextStreamer(processor.tokenizer, skip_prompt=True, skip_special_tokens=True)\n+        >>> generate_ids = model.generate(\n+        ...     **inputs,\n+        ...     do_sample=False,\n+        ...     tokenizer = processor.tokenizer,\n+        ...     stop_strings='<|im_end|>',\n+        ...     streamer=streamer,\n+        ...     max_new_tokens=4096,\n+        ... )\n+        \"You should keep in mind what features from the module should be used, especially\n+        when you're planning to sell a template.\"\n+        ```\"\"\"\n+\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if pixel_values is not None and inputs_embeds is not None:\n+            raise ValueError(\n+                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n+            )\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if pixel_values is not None:\n+            image_features = self.get_image_features(pixel_values=pixel_values.to(inputs_embeds.dtype))\n+            n_image_tokens = (input_ids == self.config.image_token_index).sum()\n+            n_image_features = image_features.shape[0] * image_features.shape[1]\n+            if n_image_tokens != n_image_features:\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n+            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+        )\n+\n+        logits = outputs[0]\n+\n+        loss = None\n+        if labels is not None:\n+            # Shift so that tokens < n predict n\n+            if attention_mask is not None:\n+                # we use the input attention mask to shift the logits and labels, because it is 2D.\n+                # we also crop attn mask in case it is longer, which happens in PrefixTuning with peft\n+                shift_attention_mask = attention_mask[:, -(logits.shape[1] - 1) :].to(logits.device)\n+                shift_logits = logits[..., :-1, :][shift_attention_mask.to(logits.device) != 0].contiguous()\n+                shift_labels = labels[..., 1:][shift_attention_mask.to(labels.device) != 0].contiguous()\n+            else:\n+                shift_logits = logits[..., :-1, :].contiguous()\n+                shift_labels = labels[..., 1:].contiguous()\n+            # Flatten the tokens\n+            loss_fct = nn.CrossEntropyLoss()\n+            loss = loss_fct(\n+                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1).to(shift_logits.device)\n+            )\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[1:]\n+            return (loss,) + output if loss is not None else output\n+\n+        return GotOcr2CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+        )\n+\n+\n+__all__ = [\n+    \"GotOcr2VisionConfig\",\n+    \"GotOcr2Config\",\n+    \"GotOcr2Processor\",\n+    \"GotOcr2PreTrainedModel\",\n+    \"GotOcr2ForConditionalGeneration\",\n+    \"GotOcr2ImageProcessor\",\n+]"
        },
        {
            "sha": "636db765f99375b48504ae8f52fa36387362bed1",
            "filename": "src/transformers/models/got_ocr2/processing_got_ocr2.py",
            "status": "added",
            "additions": 294,
            "deletions": 0,
            "changes": 294,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4694319539a006fdd89a85b0d91b4960b2e1ef/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fprocessing_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4694319539a006fdd89a85b0d91b4960b2e1ef/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fprocessing_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fprocessing_got_ocr2.py?ref=2b4694319539a006fdd89a85b0d91b4960b2e1ef",
            "patch": "@@ -0,0 +1,294 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/got_ocr2/modular_got_ocr2.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_got_ocr2.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# coding=utf-8\n+# Copyright 2024 HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+from typing import List, Optional, Tuple, Union\n+\n+from transformers.processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, TextKwargs, Unpack\n+from transformers.tokenization_utils_base import PreTokenizedInput, TextInput\n+\n+from ...image_processing_utils import BatchFeature\n+from ...image_utils import ImageInput\n+from ...utils import is_vision_available, logging\n+\n+\n+if is_vision_available():\n+    from ...image_utils import load_images\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class GotOcr2TextKwargs(TextKwargs, total=False):\n+    format: Optional[bool]\n+\n+\n+class GotOcr2ImagesKwargs(ImagesKwargs, total=False):\n+    box: Optional[Union[List, Tuple[float, float], Tuple[float, float, float, float]]]\n+    color: Optional[str]\n+    num_image_tokens: Optional[int]\n+    multi_page: Optional[bool]\n+    crop_to_patches: Optional[bool]\n+    min_patches: Optional[int]\n+    max_patches: Optional[int]\n+\n+\n+class GotOcr2ProcessorKwargs(ProcessingKwargs, total=False):\n+    text_kwargs: GotOcr2TextKwargs\n+    images_kwargs: GotOcr2ImagesKwargs\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"padding\": False,\n+            \"format\": False,\n+        },\n+        \"images_kwargs\": {\n+            \"num_image_tokens\": 256,\n+            \"multi_page\": False,\n+            \"crop_to_patches\": False,\n+            \"min_patches\": 1,\n+            \"max_patches\": 12,\n+        },\n+    }\n+\n+\n+def preprocess_box_annotation(box: Union[List, Tuple], image_size: Tuple[int, int]) -> List:\n+    \"\"\"\n+    Convert box annotation to the format [x1, y1, x2, y2] in the range [0, 1000].\n+    \"\"\"\n+    width, height = image_size\n+    if len(box) == 4:\n+        box[0] = int(box[0] / width * 1000)\n+        box[1] = int(box[1] / height * 1000)\n+        box[2] = int(box[2] / width * 1000)\n+        box[3] = int(box[3] / height * 1000)\n+    else:\n+        raise ValueError(\"Box must be a list or tuple of lists in the form [x1, y1, x2, y2].\")\n+\n+    return list(box)\n+\n+\n+class GotOcr2Processor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs a GotOcr2 processor which wraps a [`GotOcr2ImageProcessor`] and\n+    [`PretrainedTokenizerFast`] tokenizer into a single processor that inherits both the image processor and\n+    tokenizer functionalities. See the [`~GotOcr2Processor.__call__`] and [`~GotOcr2Processor.decode`] for more information.\n+    Args:\n+        image_processor ([`GotOcr2ImageProcessor`], *optional*):\n+            The image processor is a required input.\n+        tokenizer ([`PreTrainedTokenizer`, `PreTrainedTokenizerFast`], *optional*):\n+            The tokenizer is a required input.\n+        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n+            in a chat into a tokenizable string.\n+    \"\"\"\n+\n+    attributes = [\"image_processor\", \"tokenizer\"]\n+    valid_kwargs = [\"chat_template\"]\n+    image_processor_class = \"GotOcr2ImageProcessor\"\n+    tokenizer_class = \"PreTrainedTokenizerFast\"\n+\n+    def __init__(self, image_processor=None, tokenizer=None, chat_template=None, **kwargs):\n+        super().__init__(image_processor, tokenizer, chat_template=chat_template)\n+\n+        self.message_start_token = \"<|im_start|>\"\n+        self.message_end_token = \"<|im_end|>\"\n+        self.img_start_token = \"<img>\"\n+        self.img_end_token = \"</img>\"\n+        self.img_pad_token = \"<imgpad>\"\n+        self.system_query = \"system\\nYou should follow the instructions carefully and explain your answers in detail.\"\n+\n+    def _make_list_of_inputs(self, images, text, box, color, multi_page):\n+        if not isinstance(images, (list, tuple)):\n+            images = [images]\n+            if multi_page:\n+                logger.warning(\"Multi-page inference is enabled but only one image is passed.\")\n+                images = [images]\n+        elif isinstance(images[0], (list, tuple)) and not multi_page:\n+            raise ValueError(\"Nested images are only supported with `multi_page` set to `True`.\")\n+        elif not isinstance(images[0], (list, tuple)) and multi_page:\n+            images = [images]\n+\n+        if isinstance(text, str):\n+            text = [text]\n+\n+        if not isinstance(box[0], (list, tuple)):\n+            # Use the same box for all images\n+            box = [box for _ in range(len(images))]\n+        if not isinstance(color, (list, tuple)):\n+            color = [color for _ in range(len(images))]\n+\n+        return images, text, box, color\n+\n+    def __call__(\n+        self,\n+        images: Optional[ImageInput] = None,\n+        text: Optional[Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]] = None,\n+        audio=None,\n+        videos=None,\n+        **kwargs: Unpack[GotOcr2ProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n+        and `kwargs` arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizerFast.__call__`] to encode the text if `text`\n+        is not `None`, otherwise encode default OCR queries which depends on the `format`, `box`, `color`, `multi_page` and\n+        `crop_to_patches` arguments. To prepare the vision inputs, this method forwards the `images` and `kwrags` arguments to\n+        GotOcr2ImageProcessor's [`~GotOcr2ImageProcessor.__call__`] if `images` is not `None`.\n+\n+        Args:\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. Both channels-first and channels-last formats are supported.\n+            text (`str`, `List[str]`, `List[List[str]]`):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n+            format (`bool`, *optional*):\n+                If set, will add the format token to the query, and the model will return the OCR result with formatting.\n+            box (`List[float]`, `List[Tuple[float, float]]`, `List[Tuple[float, float, float, float]]`, *optional*):\n+                The box annotation to be added to the query. If a list of floats or a tuple of floats is provided, it\n+                will be interpreted as [x1, y1, x2, y2]. If a list of tuples is provided, each tuple should be in the\n+                form (x1, y1, x2, y2).\n+            color (`str`, *optional*):\n+                The color annotation to be added to the query. The model will return the OCR result within the box with\n+                the specified color.\n+            multi_page (`bool`, *optional*):\n+                If set, will enable multi-page inference. The model will return the OCR result across multiple pages.\n+            crop_to_patches (`bool`, *optional*):\n+                If set, will crop the image to patches. The model will return the OCR result upon the patch reference.\n+            min_patches (`int`, *optional*):\n+                The minimum number of patches to be cropped from the image. Only used when `crop_to_patches` is set to\n+                `True`.\n+            max_patches (`int`, *optional*):\n+                The maximum number of patches to be cropped from the image. Only used when `crop_to_patches` is set to\n+                `True`.\n+\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+                If set, will return tensors of a particular framework. Acceptable values are:\n+                - `'tf'`: Return TensorFlow `tf.constant` objects.\n+                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                - `'np'`: Return NumPy `np.ndarray` objects.\n+                - `'jax'`: Return JAX `jnp.ndarray` objects.\n+\n+        Returns:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n+\n+            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n+            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n+              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n+              `None`).\n+            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n+        \"\"\"\n+\n+        output_kwargs = self._merge_kwargs(\n+            GotOcr2ProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+        format_output = output_kwargs[\"text_kwargs\"].pop(\"format\")\n+        num_image_tokens = output_kwargs[\"images_kwargs\"].pop(\"num_image_tokens\")\n+        box = output_kwargs[\"images_kwargs\"].pop(\"box\", [None])\n+        color = output_kwargs[\"images_kwargs\"].pop(\"color\", None)\n+        multi_page = output_kwargs[\"images_kwargs\"].pop(\"multi_page\")\n+        crop_to_patches = output_kwargs[\"images_kwargs\"].pop(\"crop_to_patches\")\n+        min_patches = output_kwargs[\"images_kwargs\"].pop(\"min_patches\")\n+        max_patches = output_kwargs[\"images_kwargs\"].pop(\"max_patches\")\n+\n+        images, text, box, color = self._make_list_of_inputs(images, text, box, color, multi_page)\n+\n+        # Load images as we need to know the image size\n+        images = load_images(images)\n+        if text is None:\n+            text = []\n+            for index, (image_group, box_single, color_single) in enumerate(zip(images, box, color)):\n+                if crop_to_patches:\n+                    image_group = self.image_processor.crop_image_to_patches(\n+                        image_group,\n+                        patch_size=output_kwargs[\"images_kwargs\"].get(\"size\"),\n+                        min_patches=min_patches,\n+                        max_patches=max_patches,\n+                    )\n+                    images[index] = image_group\n+                num_images = len(image_group) if (multi_page or crop_to_patches) else 1\n+                if box_single[0] is not None:\n+                    box_single = preprocess_box_annotation(box_single, image_group.size)\n+                query = (\n+                    f\"{f'[{color_single}] ' if color_single is not None else ''}\"\n+                    f\"{str(box_single) if box_single[0] is not None else ''} \"\n+                    \"OCR\"\n+                    f\"{' with format' if format_output else ''}\"\n+                    f\"{' across multi pages' if multi_page else ''}\"\n+                    f\"{' upon the patch reference' if crop_to_patches else ''}\"\n+                    \": \"\n+                )\n+                prompt = (\n+                    self.message_start_token\n+                    + self.system_query\n+                    + self.message_end_token\n+                    + self.message_start_token\n+                    + \"user\\n\"\n+                    + self.img_start_token\n+                    + self.img_pad_token * num_image_tokens * num_images\n+                    + self.img_end_token\n+                    + \"\\n\"\n+                    + query\n+                    + self.message_end_token\n+                    + self.message_start_token\n+                    + \"assistant\\n\"\n+                )\n+                text.append(prompt)\n+        elif crop_to_patches:\n+            for index, (image_group, box_single, color_single) in enumerate(zip(images, box, color)):\n+                image_group = self.image_processor.crop_image_to_patches(\n+                    image_group,\n+                    patch_size=output_kwargs[\"images_kwargs\"].get(\"size\"),\n+                    min_patches=min_patches,\n+                    max_patches=max_patches,\n+                )\n+                images[index] = image_group\n+\n+        text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n+        if multi_page or crop_to_patches:\n+            # flatten images\n+            images = [image for image_group in images for image in image_group]\n+        image_inputs = self.image_processor(images=images, **output_kwargs[\"images_kwargs\"])\n+\n+        return BatchFeature(data={**text_inputs, **image_inputs})\n+\n+    def batch_decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n+        refer to the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.batch_decode(*args, **kwargs)\n+\n+    def decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n+        the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.decode(*args, **kwargs)\n+\n+    @property\n+    def model_input_names(self):\n+        tokenizer_input_names = self.tokenizer.model_input_names\n+        image_processor_input_names = self.image_processor.model_input_names\n+        return list(tokenizer_input_names) + list(image_processor_input_names)\n+\n+\n+__all__ = [\"GotOcr2Processor\"]"
        },
        {
            "sha": "50b666758f2a23cfaa1211bd7631367ea4587c69",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4694319539a006fdd89a85b0d91b4960b2e1ef/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4694319539a006fdd89a85b0d91b4960b2e1ef/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=2b4694319539a006fdd89a85b0d91b4960b2e1ef",
            "patch": "@@ -498,7 +498,7 @@ def forward(\n                 image_sizes=image_sizes,\n             )\n \n-            n_image_tokens = (input_ids == self.config.image_token_index).sum().item()\n+            n_image_tokens = (input_ids == self.config.image_token_index).sum()\n             n_image_features = image_features.shape[0] * image_features.shape[1]\n             if n_image_tokens != n_image_features:\n                 raise ValueError("
        },
        {
            "sha": "dbfb1ef534911e28e7e80ca0a31427c5ce06e931",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4694319539a006fdd89a85b0d91b4960b2e1ef/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4694319539a006fdd89a85b0d91b4960b2e1ef/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=2b4694319539a006fdd89a85b0d91b4960b2e1ef",
            "patch": "@@ -4637,6 +4637,20 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class GotOcr2ForConditionalGeneration(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class GotOcr2PreTrainedModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class GPT2DoubleHeadsModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "f9802ba42bbf66432223580d5ac4f22f806f08a8",
            "filename": "src/transformers/utils/dummy_vision_objects.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4694319539a006fdd89a85b0d91b4960b2e1ef/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4694319539a006fdd89a85b0d91b4960b2e1ef/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py?ref=2b4694319539a006fdd89a85b0d91b4960b2e1ef",
            "patch": "@@ -289,6 +289,13 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"vision\"])\n \n \n+class GotOcr2ImageProcessor(metaclass=DummyObject):\n+    _backends = [\"vision\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"vision\"])\n+\n+\n class GroundingDinoImageProcessor(metaclass=DummyObject):\n     _backends = [\"vision\"]\n "
        },
        {
            "sha": "321803a2179be64f4aa08b5a0079e56b2897daff",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4694319539a006fdd89a85b0d91b4960b2e1ef/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4694319539a006fdd89a85b0d91b4960b2e1ef/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=2b4694319539a006fdd89a85b0d91b4960b2e1ef",
            "patch": "@@ -1650,7 +1650,7 @@ def test_generate_from_inputs_embeds(self, _, num_beams):\n             #   checks without adding test complexity. Ditto for `pixel_values_videos` and `pixel_values_images`\n             pixel_values_is_mutually_exclusive = any(\n                 model_name in model_class.__name__.lower()\n-                for model_name in [\"llava\", \"idefics2\", \"idefics3\", \"mllama\", \"paligemma\", \"emu3\"]\n+                for model_name in [\"llava\", \"idefics2\", \"idefics3\", \"mllama\", \"paligemma\", \"emu3\", \"gotocr2\"]\n             )\n             if pixel_values_is_mutually_exclusive:\n                 inputs_dict.pop(\"pixel_values\", None)"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/got_ocr2/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4694319539a006fdd89a85b0d91b4960b2e1ef/tests%2Fmodels%2Fgot_ocr2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4694319539a006fdd89a85b0d91b4960b2e1ef/tests%2Fmodels%2Fgot_ocr2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgot_ocr2%2F__init__.py?ref=2b4694319539a006fdd89a85b0d91b4960b2e1ef"
        },
        {
            "sha": "c4e75feee660db00ec9f29d4fcb5a7dff07324f9",
            "filename": "tests/models/got_ocr2/test_image_processing_got_ocr2.py",
            "status": "added",
            "additions": 115,
            "deletions": 0,
            "changes": 115,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4694319539a006fdd89a85b0d91b4960b2e1ef/tests%2Fmodels%2Fgot_ocr2%2Ftest_image_processing_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4694319539a006fdd89a85b0d91b4960b2e1ef/tests%2Fmodels%2Fgot_ocr2%2Ftest_image_processing_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgot_ocr2%2Ftest_image_processing_got_ocr2.py?ref=2b4694319539a006fdd89a85b0d91b4960b2e1ef",
            "patch": "@@ -0,0 +1,115 @@\n+# coding=utf-8\n+# Copyright 2022 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+import unittest\n+\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_vision_available\n+\n+from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+\n+\n+if is_vision_available():\n+    from transformers import GotOcr2ImageProcessor\n+\n+\n+class GotOcr2ImageProcessingTester(unittest.TestCase):\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=7,\n+        num_channels=3,\n+        image_size=18,\n+        min_resolution=30,\n+        max_resolution=400,\n+        do_resize=True,\n+        size=None,\n+        do_normalize=True,\n+        do_pad=False,\n+        image_mean=[0.48145466, 0.4578275, 0.40821073],\n+        image_std=[0.26862954, 0.26130258, 0.27577711],\n+        do_convert_rgb=True,\n+    ):\n+        super().__init__()\n+        size = size if size is not None else {\"height\": 20, \"width\": 20}\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.min_resolution = min_resolution\n+        self.max_resolution = max_resolution\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+        self.do_pad = do_pad\n+        self.do_convert_rgb = do_convert_rgb\n+\n+    def prepare_image_processor_dict(self):\n+        return {\n+            \"do_resize\": self.do_resize,\n+            \"size\": self.size,\n+            \"do_normalize\": self.do_normalize,\n+            \"image_mean\": self.image_mean,\n+            \"image_std\": self.image_std,\n+            \"do_convert_rgb\": self.do_convert_rgb,\n+            \"do_pad\": self.do_pad,\n+        }\n+\n+    def expected_output_image_shape(self, images):\n+        return self.num_channels, self.size[\"height\"], self.size[\"width\"]\n+\n+    def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=False):\n+        return prepare_image_inputs(\n+            batch_size=self.batch_size,\n+            num_channels=self.num_channels,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=equal_resolution,\n+            numpify=numpify,\n+            torchify=torchify,\n+        )\n+\n+\n+@require_torch\n+@require_vision\n+class GotOcr2ProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n+    image_processing_class = GotOcr2ImageProcessor if is_vision_available() else None\n+\n+    def setUp(self):\n+        super().setUp()\n+        self.image_processor_tester = GotOcr2ImageProcessingTester(self)\n+\n+    @property\n+    def image_processor_dict(self):\n+        return self.image_processor_tester.prepare_image_processor_dict()\n+\n+    def test_image_processor_properties(self):\n+        image_processor = self.image_processing_class(**self.image_processor_dict)\n+        self.assertTrue(hasattr(image_processor, \"do_resize\"))\n+        self.assertTrue(hasattr(image_processor, \"size\"))\n+        self.assertTrue(hasattr(image_processor, \"do_normalize\"))\n+        self.assertTrue(hasattr(image_processor, \"image_mean\"))\n+        self.assertTrue(hasattr(image_processor, \"image_std\"))\n+        self.assertTrue(hasattr(image_processor, \"do_convert_rgb\"))\n+\n+    def test_crop_to_patches(self):\n+        image_processor = self.image_processing_class(**self.image_processor_dict)\n+        image = self.image_processor_tester.prepare_image_inputs(equal_resolution=True)[0]\n+        processed_images = image_processor.crop_image_to_patches(image, 1, 6, use_thumbnail=True)\n+        self.assertEqual(len(processed_images), 5)\n+        self.assertEqual(processed_images[0].size, (20, 20))"
        },
        {
            "sha": "ba3755237df17e12f0e3548d3d5e5819f1d7f43f",
            "filename": "tests/models/got_ocr2/test_modeling_got_ocr2.py",
            "status": "added",
            "additions": 386,
            "deletions": 0,
            "changes": 386,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4694319539a006fdd89a85b0d91b4960b2e1ef/tests%2Fmodels%2Fgot_ocr2%2Ftest_modeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4694319539a006fdd89a85b0d91b4960b2e1ef/tests%2Fmodels%2Fgot_ocr2%2Ftest_modeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgot_ocr2%2Ftest_modeling_got_ocr2.py?ref=2b4694319539a006fdd89a85b0d91b4960b2e1ef",
            "patch": "@@ -0,0 +1,386 @@\n+# coding=utf-8\n+# Copyright 2024 The Qwen team, Alibaba Group and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch GotOcr2 model.\"\"\"\n+\n+import unittest\n+\n+from transformers import (\n+    AutoProcessor,\n+    GotOcr2Config,\n+    is_torch_available,\n+    is_vision_available,\n+)\n+from transformers.testing_utils import cleanup, require_torch, slow, torch_device\n+\n+from ...generation.test_utils import GenerationTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor, ids_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import (\n+        GotOcr2ForConditionalGeneration,\n+    )\n+\n+\n+if is_vision_available():\n+    from transformers.image_utils import load_image\n+\n+\n+class GotOcr2VisionText2TextModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=3,\n+        seq_length=7,\n+        num_channels=3,\n+        ignore_index=-100,\n+        image_size=64,\n+        bos_token_id=0,\n+        eos_token_id=0,\n+        pad_token_id=0,\n+        image_token_index=1,\n+        model_type=\"got_ocr2\",\n+        is_training=True,\n+        text_config={\n+            \"model_type\": \"qwen2\",\n+            \"vocab_size\": 99,\n+            \"hidden_size\": 128,\n+            \"intermediate_size\": 37,\n+            \"num_hidden_layers\": 4,\n+            \"num_attention_heads\": 4,\n+            \"num_key_value_heads\": 2,\n+            \"output_channels\": 64,\n+            \"hidden_act\": \"silu\",\n+            \"max_position_embeddings\": 512,\n+            \"rope_theta\": 10000,\n+            \"mlp_ratio\": 4,\n+            \"tie_word_embeddings\": True,\n+        },\n+        vision_config={\n+            \"num_hidden_layers\": 2,\n+            \"output_channels\": 64,\n+            \"hidden_act\": \"quick_gelu\",\n+            \"hidden_size\": 32,\n+            \"mlp_dim\": 128,\n+            \"num_attention_heads\": 4,\n+            \"patch_size\": 2,\n+            \"image_size\": 64,\n+        },\n+    ):\n+        self.parent = parent\n+        self.ignore_index = ignore_index\n+        self.bos_token_id = bos_token_id\n+        self.eos_token_id = eos_token_id\n+        self.pad_token_id = pad_token_id\n+        self.image_token_index = image_token_index\n+        self.model_type = model_type\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.is_training = is_training\n+        self.num_image_tokens = 64\n+        self.seq_length = seq_length + self.num_image_tokens\n+\n+        self.num_hidden_layers = text_config[\"num_hidden_layers\"]\n+        self.vocab_size = text_config[\"vocab_size\"]\n+        self.hidden_size = text_config[\"hidden_size\"]\n+        self.num_attention_heads = text_config[\"num_attention_heads\"]\n+\n+    def get_config(self):\n+        return GotOcr2Config(\n+            text_config=self.text_config,\n+            vision_config=self.vision_config,\n+            model_type=self.model_type,\n+            bos_token_id=self.bos_token_id,\n+            eos_token_id=self.eos_token_id,\n+            pad_token_id=self.pad_token_id,\n+            image_token_index=self.image_token_index,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        config = self.get_config()\n+        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n+\n+        return config, pixel_values\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values = config_and_inputs\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n+        attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=torch_device)\n+\n+        # input_ids[:, -1] = self.pad_token_id\n+        input_ids[input_ids == self.image_token_index] = self.pad_token_id\n+        input_ids[:, : self.num_image_tokens] = self.image_token_index\n+\n+        inputs_dict = {\n+            \"pixel_values\": pixel_values,\n+            \"input_ids\": input_ids,\n+            \"attention_mask\": attention_mask,\n+        }\n+        return config, inputs_dict\n+\n+    def create_and_check_model_fp16_forward(self, config, input_ids, pixel_values, attention_mask):\n+        model = GotOcr2ForConditionalGeneration(config=config)\n+        model.to(torch_device)\n+        model.half()\n+        model.eval()\n+        logits = model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            pixel_values=pixel_values.to(torch.bfloat16),\n+            return_dict=True,\n+        )[\"logits\"]\n+        self.parent.assertFalse(torch.isnan(logits).any().item())\n+\n+    def create_and_check_model_fp16_autocast_forward(self, config, input_ids, pixel_values, attention_mask):\n+        config.torch_dtype = torch.float16\n+        model = GotOcr2ForConditionalGeneration(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n+            logits = model(\n+                input_ids=input_ids,\n+                attention_mask=attention_mask,\n+                pixel_values=pixel_values.to(torch.bfloat16),\n+                return_dict=True,\n+            )[\"logits\"]\n+        self.parent.assertFalse(torch.isnan(logits).any().item())\n+\n+\n+@require_torch\n+class GotOcr2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    all_model_classes = (GotOcr2ForConditionalGeneration,) if is_torch_available() else ()\n+    all_generative_model_classes = (GotOcr2ForConditionalGeneration,) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\n+            \"image-to-text\": GotOcr2ForConditionalGeneration,\n+            \"image-text-to-text\": GotOcr2ForConditionalGeneration,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+    test_headmasking = False\n+    test_pruning = False\n+\n+    def setUp(self):\n+        self.model_tester = GotOcr2VisionText2TextModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=GotOcr2Config, has_text_modality=False)\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_initialization(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        configs_no_init = _config_zero_init(config)\n+        for model_class in self.all_model_classes:\n+            model = model_class(config=configs_no_init)\n+            for name, param in model.named_parameters():\n+                if param.requires_grad:\n+                    self.assertIn(\n+                        ((param.data.mean() * 1e9).round() / 1e9).item(),\n+                        [0.0, 1.0],\n+                        msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                    )\n+\n+    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n+    def test_inputs_embeds(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = self._prepare_for_class(inputs_dict, model_class)\n+\n+            input_ids = inputs[\"input_ids\"]\n+            del inputs[\"input_ids\"]\n+            del inputs[\"pixel_values\"]\n+\n+            wte = model.get_input_embeddings()\n+            inputs[\"inputs_embeds\"] = wte(input_ids)\n+\n+            with torch.no_grad():\n+                model(**inputs)\n+\n+    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n+    # while some other models require pixel_values to be present\n+    def test_inputs_embeds_matches_input_ids(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = self._prepare_for_class(inputs_dict, model_class)\n+            input_ids = inputs[\"input_ids\"]\n+            del inputs[\"input_ids\"]\n+            del inputs[\"pixel_values\"]\n+\n+            inputs_embeds = model.get_input_embeddings()(input_ids)\n+\n+            with torch.no_grad():\n+                out_ids = model(input_ids=input_ids, **inputs)[0]\n+                out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n+            torch.testing.assert_close(out_embeds, out_ids)\n+\n+    @unittest.skip(\n+        reason=\"VLMs can't generate from inputs embeds and pixels. This can be tested as part of bacbone LM, no need to run the test for VLMs\"\n+    )\n+    def test_generate_from_inputs_embeds_with_static_cache(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"GotOcr2's language backbone is Qwen2 which uses GQA so the KV cache is a non standard format\"\n+    )\n+    def test_past_key_values_format(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"GotOcr2 needs a dynamic control flow to pass pixel values to the forward function only in the first generation step\"\n+    )\n+    def test_generate_compile_1_end_to_end(self):\n+        pass\n+\n+    @unittest.skip(\"FlashAttention only support fp16 and bf16 data type\")\n+    def test_flash_attn_2_fp32_ln(self):\n+        pass\n+\n+\n+@require_torch\n+class GotOcr2IntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        self.processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\")\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @slow\n+    def test_small_model_integration_test_got_ocr_stop_strings(self):\n+        model_id = \"stepfun-ai/GOT-OCR-2.0-hf\"\n+        model = GotOcr2ForConditionalGeneration.from_pretrained(model_id, device_map=torch_device)\n+        image = load_image(\n+            \"https://huggingface.co/datasets/hf-internal-testing/fixtures_ocr/resolve/main/iam_picture.jpeg\"\n+        )\n+\n+        inputs = self.processor(image, return_tensors=\"pt\").to(torch_device)\n+        generate_ids = model.generate(\n+            **inputs,\n+            do_sample=False,\n+            num_beams=1,\n+            tokenizer=self.processor.tokenizer,\n+            stop_strings=\"<|im_end|>\",\n+            max_new_tokens=4096,\n+        )\n+        decoded_output = self.processor.decode(\n+            generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n+        )\n+        expected_output = \"industre\"\n+        self.assertEqual(decoded_output, expected_output)\n+\n+    @slow\n+    def test_small_model_integration_test_got_ocr_format(self):\n+        model_id = \"stepfun-ai/GOT-OCR-2.0-hf\"\n+        model = GotOcr2ForConditionalGeneration.from_pretrained(model_id, device_map=torch_device)\n+        image = load_image(\n+            \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/image_ocr.jpg\"\n+        )\n+\n+        inputs = self.processor(image, return_tensors=\"pt\", format=True).to(torch_device)\n+        generate_ids = model.generate(**inputs, do_sample=False, num_beams=1, max_new_tokens=4)\n+        decoded_output = self.processor.decode(\n+            generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n+        )\n+        expected_output = \"\\\\title{\\nR\"\n+        self.assertEqual(decoded_output, expected_output)\n+\n+    @slow\n+    def test_small_model_integration_test_got_ocr_fine_grained(self):\n+        model_id = \"stepfun-ai/GOT-OCR-2.0-hf\"\n+        model = GotOcr2ForConditionalGeneration.from_pretrained(model_id, device_map=torch_device)\n+        image = load_image(\n+            \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/multi_box.png\"\n+        )\n+\n+        inputs = self.processor(image, return_tensors=\"pt\", color=\"green\").to(torch_device)\n+        generate_ids = model.generate(**inputs, do_sample=False, num_beams=1, max_new_tokens=4)\n+        decoded_output = self.processor.decode(\n+            generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n+        )\n+        expected_output = \"You should keep in\"\n+        self.assertEqual(decoded_output, expected_output)\n+\n+    @slow\n+    def test_small_model_integration_test_got_ocr_crop_to_patches(self):\n+        model_id = \"stepfun-ai/GOT-OCR-2.0-hf\"\n+        model = GotOcr2ForConditionalGeneration.from_pretrained(model_id, device_map=torch_device)\n+        image = load_image(\n+            \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/one_column.png\"\n+        )\n+\n+        inputs = self.processor(image, return_tensors=\"pt\", crop_to_patches=True).to(torch_device)\n+        generate_ids = model.generate(**inputs, do_sample=False, num_beams=1, max_new_tokens=4)\n+        decoded_output = self.processor.decode(\n+            generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n+        )\n+        expected_output = \"on developing architectural improvements\"\n+        self.assertEqual(decoded_output, expected_output)\n+\n+    @slow\n+    def test_small_model_integration_test_got_ocr_multi_pages(self):\n+        model_id = \"stepfun-ai/GOT-OCR-2.0-hf\"\n+        model = GotOcr2ForConditionalGeneration.from_pretrained(model_id, device_map=torch_device)\n+        image1 = load_image(\n+            \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/one_column.png\"\n+        )\n+        image2 = load_image(\n+            \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/multi_box.png\"\n+        )\n+\n+        inputs = self.processor([image1, image2], return_tensors=\"pt\", multi_page=True).to(torch_device)\n+        generate_ids = model.generate(**inputs, do_sample=False, num_beams=1, max_new_tokens=4)\n+        decoded_output = self.processor.decode(\n+            generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n+        )\n+        expected_output = \"on developing architectural improvements\"\n+        self.assertEqual(decoded_output, expected_output)\n+\n+    @slow\n+    def test_small_model_integration_test_got_ocr_batched(self):\n+        model_id = \"stepfun-ai/GOT-OCR-2.0-hf\"\n+        model = GotOcr2ForConditionalGeneration.from_pretrained(model_id, device_map=torch_device)\n+        image1 = load_image(\n+            \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/multi_box.png\"\n+        )\n+        image2 = load_image(\n+            \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/image_ocr.jpg\"\n+        )\n+\n+        inputs = self.processor([image1, image2], return_tensors=\"pt\").to(torch_device)\n+        generate_ids = model.generate(**inputs, do_sample=False, num_beams=1, max_new_tokens=4)\n+        decoded_output = self.processor.batch_decode(\n+            generate_ids[:, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n+        )\n+        expected_output = [\"Reducing the number\", \"R&D QUALITY\"]\n+        self.assertEqual(decoded_output, expected_output)"
        },
        {
            "sha": "530d8234ce695c8ee2a0528df688a25ee42c9578",
            "filename": "tests/models/got_ocr2/test_processor_got_ocr2.py",
            "status": "added",
            "additions": 77,
            "deletions": 0,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4694319539a006fdd89a85b0d91b4960b2e1ef/tests%2Fmodels%2Fgot_ocr2%2Ftest_processor_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4694319539a006fdd89a85b0d91b4960b2e1ef/tests%2Fmodels%2Fgot_ocr2%2Ftest_processor_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgot_ocr2%2Ftest_processor_got_ocr2.py?ref=2b4694319539a006fdd89a85b0d91b4960b2e1ef",
            "patch": "@@ -0,0 +1,77 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import shutil\n+import tempfile\n+import unittest\n+\n+from transformers import AutoProcessor, GotOcr2Processor, PreTrainedTokenizerFast\n+from transformers.testing_utils import require_vision\n+from transformers.utils import is_vision_available\n+\n+from ...test_processing_common import ProcessorTesterMixin\n+\n+\n+if is_vision_available():\n+    from transformers import GotOcr2ImageProcessor\n+\n+\n+@require_vision\n+class GotOcr2ProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = GotOcr2Processor\n+\n+    def setUp(self):\n+        self.tmpdirname = tempfile.mkdtemp()\n+\n+        image_processor = GotOcr2ImageProcessor()\n+        tokenizer = PreTrainedTokenizerFast.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\")\n+        processor_kwargs = self.prepare_processor_dict()\n+        processor = GotOcr2Processor(image_processor, tokenizer, **processor_kwargs)\n+        processor.save_pretrained(self.tmpdirname)\n+\n+    def get_tokenizer(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n+\n+    def get_image_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.tmpdirname)\n+\n+    def test_ocr_queries(self):\n+        processor = self.get_processor()\n+        image_input = self.prepare_image_inputs()\n+        inputs = processor(image_input, return_tensors=\"pt\")\n+        self.assertEqual(inputs[\"input_ids\"].shape, (1, 286))\n+        self.assertEqual(inputs[\"pixel_values\"].shape, (1, 3, 384, 384))\n+\n+        inputs = processor(image_input, return_tensors=\"pt\", format=True)\n+        self.assertEqual(inputs[\"input_ids\"].shape, (1, 288))\n+        self.assertEqual(inputs[\"pixel_values\"].shape, (1, 3, 384, 384))\n+\n+        inputs = processor(image_input, return_tensors=\"pt\", color=\"red\")\n+        self.assertEqual(inputs[\"input_ids\"].shape, (1, 290))\n+        self.assertEqual(inputs[\"pixel_values\"].shape, (1, 3, 384, 384))\n+\n+        inputs = processor(image_input, return_tensors=\"pt\", box=[0, 0, 100, 100])\n+        self.assertEqual(inputs[\"input_ids\"].shape, (1, 303))\n+        self.assertEqual(inputs[\"pixel_values\"].shape, (1, 3, 384, 384))\n+\n+        inputs = processor([image_input, image_input], return_tensors=\"pt\", multi_page=True, format=True)\n+        self.assertEqual(inputs[\"input_ids\"].shape, (1, 547))\n+        self.assertEqual(inputs[\"pixel_values\"].shape, (2, 3, 384, 384))\n+\n+        inputs = processor(image_input, return_tensors=\"pt\", crop_to_patches=True, max_patches=6)\n+        self.assertEqual(inputs[\"input_ids\"].shape, (1, 1826))\n+        self.assertEqual(inputs[\"pixel_values\"].shape, (7, 3, 384, 384))"
        },
        {
            "sha": "05cf22a3fdb2ebf83310fd8f7684e0d73bae09db",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4694319539a006fdd89a85b0d91b4960b2e1ef/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4694319539a006fdd89a85b0d91b4960b2e1ef/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=2b4694319539a006fdd89a85b0d91b4960b2e1ef",
            "patch": "@@ -327,7 +327,7 @@ def test_beam_search_low_memory(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"VLMs can't generate from inputs embeds and pixels. This can be tested as part of bacbone LM, no need to run the tes for VLMs\"\n+        reason=\"VLMs can't generate from inputs embeds and pixels. This can be tested as part of bacbone LM, no need to run the test for VLMs\"\n     )\n     def test_generate_from_inputs_embeds_with_static_cache(self):\n         pass"
        }
    ],
    "stats": {
        "total": 4187,
        "additions": 4184,
        "deletions": 3
    }
}