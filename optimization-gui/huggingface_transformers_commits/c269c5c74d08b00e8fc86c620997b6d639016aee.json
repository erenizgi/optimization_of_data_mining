{
    "author": "Adibvafa",
    "message": "Fix Mamba slow path bug with dtype mismatch. (#32691)\n\n* Fix Mamba slow path bug with dtype mismatch.\r\n\r\n* Update test_modeling_mamba.py\r\n\r\n* Improve style.\r\n\r\n* Fix issue with cache position of dtype mismatch test.\r\n\r\n* Change test for slow path.\r\n\r\n* Revert changes.\r\n\r\n* Switch to buggy code and add test to catch it.\r\n\r\n* Fix the dtype mismatch bug and add test code to verify it.\r\n\r\n* Fix minor bug with test.\r\n\r\n* Fix incorrect dtype of model output.\r\n\r\n* Fix incorrect dtype of cache.\r\n\r\n* Fix incorrect dtype of ssm cache.\r\n\r\n* Fix incorrect dtype of conv state.\r\n\r\n* Remove assertion for ssm state.\r\n\r\n* Add assertion for conv state dtype.\r\n\r\n* Fix all issues with dtype mismatch test.",
    "sha": "c269c5c74d08b00e8fc86c620997b6d639016aee",
    "files": [
        {
            "sha": "0b82b17dcde0a0dbb912ff7cad8f5511579a12f8",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c269c5c74d08b00e8fc86c620997b6d639016aee/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c269c5c74d08b00e8fc86c620997b6d639016aee/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=c269c5c74d08b00e8fc86c620997b6d639016aee",
            "patch": "@@ -1797,7 +1797,7 @@ def update_conv_state(\n         cache_position = cache_position.clamp(0, self.conv_kernel_size - 1)\n \n         conv_state = conv_state.roll(shifts=-1, dims=-1)\n-        conv_state[:, :, cache_position] = new_conv_state.to(conv_state.device)\n+        conv_state[:, :, cache_position] = new_conv_state.to(device=conv_state.device, dtype=conv_state.dtype)\n         self.conv_states[layer_idx].zero_()\n         self.conv_states[layer_idx] += conv_state\n         return self.conv_states[layer_idx]"
        },
        {
            "sha": "d432dfa93df4872e3521f8f1e603618e34ff61ae",
            "filename": "tests/models/mamba/test_modeling_mamba.py",
            "status": "modified",
            "additions": 24,
            "deletions": 0,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/c269c5c74d08b00e8fc86c620997b6d639016aee/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c269c5c74d08b00e8fc86c620997b6d639016aee/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py?ref=c269c5c74d08b00e8fc86c620997b6d639016aee",
            "patch": "@@ -421,6 +421,30 @@ def recursive_check(tuple_object, dict_object):\n     def test_beam_sample_generate(self):\n         pass\n \n+    def test_dtype_mismatch_handled_in_cache(self):\n+        config, input_ids, *args = self.model_tester.prepare_config_and_inputs()\n+        model = MambaModel(config)\n+        model.to(torch_device).to(torch.float16)\n+        model.eval()\n+\n+        # Create cache with float32 dtype\n+        cache_params = MambaCache(config, batch_size=input_ids.size(0), dtype=torch.float32, device=torch_device)\n+\n+        # If code is correct, no error occurs and test passes\n+        outputs = model(\n+            input_ids,\n+            cache_params=cache_params,\n+            use_cache=True,\n+            cache_position=torch.arange(0, config.conv_kernel, device=input_ids.device),\n+        )\n+\n+        self.assertIsNotNone(outputs)\n+        self.assertIsNotNone(outputs.last_hidden_state)\n+        self.assertEqual(\n+            outputs.last_hidden_state.shape,\n+            (self.model_tester.batch_size, self.model_tester.seq_length, self.model_tester.hidden_size),\n+        )\n+\n \n @require_torch\n class MambaIntegrationTests(unittest.TestCase):"
        }
    ],
    "stats": {
        "total": 26,
        "additions": 25,
        "deletions": 1
    }
}