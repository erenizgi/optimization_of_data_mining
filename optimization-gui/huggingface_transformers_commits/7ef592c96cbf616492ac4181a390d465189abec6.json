{
    "author": "bzhangGo",
    "message": "Update T5gemma (#39210)\n\n* bug fix: add vocab_size to t5gemmaconfig for pipeline.\n\n* Update checkpoint placeholder\n\n* minor change\n\n* minor change\n\n* minor change: update example.\n\n* fix: add vocab_size as an explict arg.\n\n* buf fix:\n\nremove vocab_size verification; instead, re-set encoder/decoder vocab size.\n\nNote, in t5gemma, vocab size of encoder/decoder shoud be always the same.\n\n* add `add_generation_prompt` for message preprocessing.",
    "sha": "7ef592c96cbf616492ac4181a390d465189abec6",
    "files": [
        {
            "sha": "72140b21d3bea4fab43600d013bd1c3e95e3775d",
            "filename": "docs/source/en/model_doc/t5gemma.md",
            "status": "modified",
            "additions": 32,
            "deletions": 14,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ef592c96cbf616492ac4181a390d465189abec6/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5gemma.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ef592c96cbf616492ac4181a390d465189abec6/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5gemma.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5gemma.md?ref=7ef592c96cbf616492ac4181a390d465189abec6",
            "patch": "@@ -14,7 +14,13 @@ specific language governing permissions and limitations under the License.\n rendered properly in your Markdown viewer.\n \n -->\n-\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n+</div>\n \n # T5Gemma\n \n@@ -24,6 +30,9 @@ T5Gemma has two groups of model sizes: 1) [Gemma 2](https://ai.google.dev/gemma/\n \n The pretrained varaints are trained with two objectives: prefix language modeling with knowledge distillation (PrefixLM) and UL2, separately. We release both variants for each model size. The instruction-turned varaints was post-trained with supervised fine-tuning and reinforcement learning.\n \n+> [!TIP]\n+> Click on the T5Gemma models in the right sidebar for more examples of how to apply T5Gemma to different language tasks.\n+\n The example below demonstrates how to chat with the model with [`Pipeline`] or the [`AutoModel`] class, and from the command line.\n \n <hfoptions id=\"usage\">\n@@ -35,43 +44,52 @@ import torch\n from transformers import pipeline\n \n pipe = pipeline(\n-    task=\"text2text-generation\",\n-    model=\"google/t5gemma-placeholder\",\n+    \"text2text-generation\",\n+    model=\"google/t5gemma-2b-2b-prefixlm-it\",\n     torch_dtype=torch.bfloat16,\n-    device=\"cuda\",\n+    device=\"cuda\",  # replace with \"mps\" to run on a Mac device\n )\n \n-pipe(\"Question: Why is the sky blue?\\nAnswer:\", max_new_tokens=50)\n+messages = [\n+    {\"role\": \"user\", \"content\": \"Tell me an unknown interesting biology fact about the brain.\"},\n+]\n+prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n+\n+pipe(prompt, max_new_tokens=32)\n ```\n \n </hfoption>\n <hfoption id=\"AutoModel\">\n \n ```python\n-import torch\n+# pip install accelerate\n from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n+import torch\n \n-tokenizer = AutoTokenizer.from_pretrained(\"google/t5gemma-placeholder\")\n+tokenizer = AutoTokenizer.from_pretrained(\"google/t5gemma-2b-2b-prefixlm-it\")\n model = AutoModelForSeq2SeqLM.from_pretrained(\n-    \"google/t5gemma-placeholder\",\n+    \"google/t5gemma-2b-2b-prefixlm-it\",\n+    device_map=\"auto\",\n     torch_dtype=torch.bfloat16,\n-    device_map=\"auto\"\n )\n \n-input_text = \"Question: Why is the sky blue?\\nAnswer:\"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+messages = [\n+    {\"role\": \"user\", \"content\": \"Tell me an unknown interesting biology fact about the brain.\"},\n+]\n+input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_dict=True, add_generation_prompt=True).to(\"cuda\")\n \n outputs = model.generate(**input_ids, max_new_tokens=32)\n-print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n-\n+print(tokenizer.decode(outputs[0]))\n ```\n \n </hfoption>\n <hfoption id=\"transformers CLI\">\n \n ```\n-echo -e \"Question: Why is the sky blue? Answer:\" | transformers run --task text2text-generation --model google/t5gemma-placeholder --device 0\n+echo -e \"Write me a poem about Machine Learning. Answer:\" | transformers run --task text2text-generation --model google/t5gemma-2b-2b-prefixlm --device 0\n ```\n+</hfoption>\n+</hfoptions>\n \n ## T5GemmaConfig\n "
        },
        {
            "sha": "bc195d562f2cfeda02f4386ea85d90348b4d780d",
            "filename": "src/transformers/models/t5gemma/configuration_t5gemma.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ef592c96cbf616492ac4181a390d465189abec6/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fconfiguration_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ef592c96cbf616492ac4181a390d465189abec6/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fconfiguration_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fconfiguration_t5gemma.py?ref=7ef592c96cbf616492ac4181a390d465189abec6",
            "patch": "@@ -186,10 +186,10 @@ class T5GemmaConfig(PretrainedConfig):\n     This is the configuration class to store the configuration of a [`T5GemmaModel`]. It is used to instantiate an T5Gemma\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to a hypothetical balanced Gemma2 encoder-decoder model.\n-    e.g. [google/t5gemma-placeholder](https://huggingface.co/google/t5gemma-placeholder)\n+    e.g. [google/t5gemma-2b-2b-prefixlm-it](https://huggingface.co/google/t5gemma-2b-2b-prefixlm-it)\n     ```python\n     >>> from transformers import T5GemmaConfig, T5GemmaModel\n-    >>> t5gemma_config = T5GemmaConfig.from_pretrained(\"google/t5gemma-placeholder\")\n+    >>> t5gemma_config = T5GemmaConfig.from_pretrained(\"google/t5gemma-2b-2b-prefixlm-it\")\n     >>> model = T5GemmaModel(t5gemma_config)\n     ```\n     Configuration objects inherit from [PretrainedConfig] and can be used to control the model outputs. Read the\n@@ -209,6 +209,8 @@ class T5GemmaConfig(PretrainedConfig):\n             The dropout ratio for attention.\n         tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n             Whether tie input and output embeddings.\n+        vocab_size (`int`, *optional*, defaults to 256000):\n+            Vocabulary size of the T5Gemma model (the same as Gemma 2).\n         kwargs (additional keyword arguments, optional, *optional*):\n             Will be passed to the PretrainedConfig base class.\n     \"\"\"\n@@ -257,6 +259,7 @@ def __init__(\n         classifier_dropout_rate: float = 0.0,\n         attention_dropout: float = 0.0,\n         tie_word_embeddings: bool = True,\n+        vocab_size: int = 256000,\n         **kwargs,\n     ):\n         if isinstance(encoder, dict):\n@@ -302,13 +305,17 @@ def __init__(\n         self.classifier_dropout_rate = classifier_dropout_rate\n         self.tie_word_embeddings = tie_word_embeddings\n \n+        # Used in pipeline generation.\n+        self.vocab_size = vocab_size\n+\n     def __setattr__(self, key, value):\n         shared_attr_with_submodules = [\n             \"output_hidden_states\",\n             \"output_attentions\",\n             \"_attn_implementation\",\n             \"dropout_rate\",\n             \"attention_dropout\",\n+            \"vocab_size\",\n         ]\n \n         if key in shared_attr_with_submodules:"
        },
        {
            "sha": "9360008e30e66efec1eebef452d1a33e5a1edc1e",
            "filename": "src/transformers/models/t5gemma/modular_t5gemma.py",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ef592c96cbf616492ac4181a390d465189abec6/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ef592c96cbf616492ac4181a390d465189abec6/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py?ref=7ef592c96cbf616492ac4181a390d465189abec6",
            "patch": "@@ -56,8 +56,7 @@\n )\n \n \n-# TODO(bzhanggo): figure out these documentations\n-_CHECKPOINT_FOR_DOC = \"google/t5gemma-placeholder\"\n+_CHECKPOINT_FOR_DOC = \"google/t5gemma-2b-2b-prefixlm-it\"\n \n \n if is_torch_flex_attn_available():\n@@ -76,10 +75,10 @@ class T5GemmaConfig(PretrainedConfig):\n     This is the configuration class to store the configuration of a [`T5GemmaModel`]. It is used to instantiate an T5Gemma\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to a hypothetical balanced Gemma2 encoder-decoder model.\n-    e.g. [google/t5gemma-placeholder](https://huggingface.co/google/t5gemma-placeholder)\n+    e.g. [google/t5gemma-2b-2b-prefixlm-it](https://huggingface.co/google/t5gemma-2b-2b-prefixlm-it)\n     ```python\n     >>> from transformers import T5GemmaConfig, T5GemmaModel\n-    >>> t5gemma_config = T5GemmaConfig.from_pretrained(\"google/t5gemma-placeholder\")\n+    >>> t5gemma_config = T5GemmaConfig.from_pretrained(\"google/t5gemma-2b-2b-prefixlm-it\")\n     >>> model = T5GemmaModel(t5gemma_config)\n     ```\n     Configuration objects inherit from [PretrainedConfig] and can be used to control the model outputs. Read the\n@@ -99,6 +98,8 @@ class T5GemmaConfig(PretrainedConfig):\n             The dropout ratio for attention.\n         tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n             Whether tie input and output embeddings.\n+        vocab_size (`int`, *optional*, defaults to 256000):\n+            Vocabulary size of the T5Gemma model (the same as Gemma 2).\n         kwargs (additional keyword arguments, optional, *optional*):\n             Will be passed to the PretrainedConfig base class.\n     \"\"\"\n@@ -147,6 +148,7 @@ def __init__(\n         classifier_dropout_rate: float = 0.0,\n         attention_dropout: float = 0.0,\n         tie_word_embeddings: bool = True,\n+        vocab_size: int = 256000,\n         **kwargs,\n     ):\n         if isinstance(encoder, dict):\n@@ -192,13 +194,17 @@ def __init__(\n         self.classifier_dropout_rate = classifier_dropout_rate\n         self.tie_word_embeddings = tie_word_embeddings\n \n+        # Used in pipeline generation.\n+        self.vocab_size = vocab_size\n+\n     def __setattr__(self, key, value):\n         shared_attr_with_submodules = [\n             \"output_hidden_states\",\n             \"output_attentions\",\n             \"_attn_implementation\",\n             \"dropout_rate\",\n             \"attention_dropout\",\n+            \"vocab_size\",\n         ]\n \n         if key in shared_attr_with_submodules:"
        }
    ],
    "stats": {
        "total": 71,
        "additions": 51,
        "deletions": 20
    }
}