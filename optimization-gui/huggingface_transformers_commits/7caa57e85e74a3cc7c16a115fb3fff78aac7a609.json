{
    "author": "yao-matrix",
    "message": "enable trainer test cases on xpu (#38138)\n\n* enable trainer test cases on xpu\n\nSigned-off-by: Matrix Yao <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: Matrix Yao <matrix.yao@intel.com>\n\n---------\n\nSigned-off-by: Matrix Yao <matrix.yao@intel.com>",
    "sha": "7caa57e85e74a3cc7c16a115fb3fff78aac7a609",
    "files": [
        {
            "sha": "6a16581e81d064272293e14d63a2373a5a549da6",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7caa57e85e74a3cc7c16a115fb3fff78aac7a609/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7caa57e85e74a3cc7c16a115fb3fff78aac7a609/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=7caa57e85e74a3cc7c16a115fb3fff78aac7a609",
            "patch": "@@ -2039,7 +2039,7 @@ def get_env(self):\n \n         \"\"\"\n         env = os.environ.copy()\n-        paths = [self.src_dir_str]\n+        paths = [self.repo_root_dir_str, self.src_dir_str]\n         if \"/examples\" in self.test_file_dir_str:\n             paths.append(self.examples_dir_str)\n         else:"
        },
        {
            "sha": "b5fb3d64e7d555225e75093b704221608a67f11d",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/7caa57e85e74a3cc7c16a115fb3fff78aac7a609/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7caa57e85e74a3cc7c16a115fb3fff78aac7a609/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=7caa57e85e74a3cc7c16a115fb3fff78aac7a609",
            "patch": "@@ -97,7 +97,6 @@\n     require_torch_fp16,\n     require_torch_gpu,\n     require_torch_multi_accelerator,\n-    require_torch_multi_gpu,\n     require_torch_non_multi_accelerator,\n     require_torch_non_multi_gpu,\n     require_torch_tensorrt_fx,\n@@ -3766,7 +3765,7 @@ def test_num_train_epochs_in_training(self):\n             train_output = trainer.train()\n             self.assertEqual(train_output.global_step, int(self.n_epochs))\n \n-    @require_torch_multi_gpu\n+    @require_torch_multi_accelerator\n     def test_num_batches_in_training_with_gradient_accumulation(self):\n         with tempfile.TemporaryDirectory() as tmp_dir:\n             for num_train_epochs in [1, 2]:"
        },
        {
            "sha": "9bae7c92657883df496508ba8f5eb6a1575dd3cd",
            "filename": "tests/trainer/test_trainer_distributed_loss.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/7caa57e85e74a3cc7c16a115fb3fff78aac7a609/tests%2Ftrainer%2Ftest_trainer_distributed_loss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7caa57e85e74a3cc7c16a115fb3fff78aac7a609/tests%2Ftrainer%2Ftest_trainer_distributed_loss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer_distributed_loss.py?ref=7caa57e85e74a3cc7c16a115fb3fff78aac7a609",
            "patch": "@@ -1,7 +1,6 @@\n import json\n \n import datasets\n-import torch\n \n from tests.trainer.test_trainer import StoreLossCallback\n from transformers import (\n@@ -15,16 +14,18 @@\n )\n from transformers.testing_utils import (\n     TestCasePlus,\n+    backend_device_count,\n     execute_subprocess_async,\n     get_torch_dist_unique_port,\n-    require_torch_multi_gpu,\n+    require_torch_multi_accelerator,\n+    torch_device,\n )\n \n \n class TestTrainerDistributedLoss(TestCasePlus):\n-    @require_torch_multi_gpu\n+    @require_torch_multi_accelerator\n     def test_trainer(self):\n-        device_count = torch.cuda.device_count()\n+        device_count = backend_device_count(torch_device)\n         min_bs = 1\n         output_dir = self.get_auto_remove_tmp_dir()\n         for gpu_num, enable, bs, name in ("
        },
        {
            "sha": "3fa625af748b0090fdb685b324417c5634ccbffd",
            "filename": "tests/trainer/test_trainer_distributed_worker_seed.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/7caa57e85e74a3cc7c16a115fb3fff78aac7a609/tests%2Ftrainer%2Ftest_trainer_distributed_worker_seed.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7caa57e85e74a3cc7c16a115fb3fff78aac7a609/tests%2Ftrainer%2Ftest_trainer_distributed_worker_seed.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer_distributed_worker_seed.py?ref=7caa57e85e74a3cc7c16a115fb3fff78aac7a609",
            "patch": "@@ -14,9 +14,11 @@\n )\n from transformers.testing_utils import (\n     TestCasePlus,\n+    backend_device_count,\n     execute_subprocess_async,\n     get_torch_dist_unique_port,\n-    require_torch_multi_gpu,\n+    require_torch_multi_accelerator,\n+    torch_device,\n )\n \n \n@@ -47,17 +49,17 @@ def __init__(self):\n         self.fc = nn.Linear(3, 1)\n \n     def forward(self, x):\n-        local_tensor = torch.tensor(x, device=\"cuda\")\n+        local_tensor = torch.tensor(x, device=torch_device)\n         gathered = gather_from_all_gpus(local_tensor, dist.get_world_size())\n         assert not all(torch.allclose(t, gathered[0]) for t in gathered[1:])\n         y = self.fc(x)\n         return (y.mean(), y)\n \n \n class TestTrainerDistributedWorkerSeed(TestCasePlus):\n-    @require_torch_multi_gpu\n+    @require_torch_multi_accelerator\n     def test_trainer(self):\n-        device_count = torch.cuda.device_count()\n+        device_count = backend_device_count(torch_device)\n         output_dir = self.get_auto_remove_tmp_dir()\n         distributed_args = f\"\"\"--nproc_per_node={device_count}\n             --master_port={get_torch_dist_unique_port()}"
        }
    ],
    "stats": {
        "total": 24,
        "additions": 13,
        "deletions": 11
    }
}