{
    "author": "SunMarc",
    "message": "[v5] remove load_in_4bit and load_in_8bit (#41287)\n\n* [v5] remove load_in_4bit and load_in_8bit\n\n* fix\n\n* reveert\n\n* fix\n\n---------\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "b44d91570fb4a5cf8a58150c7935a4edde82e27f",
    "files": [
        {
            "sha": "cf905db9c94925b3a36e3043a95786934e377a78",
            "filename": "docs/source/ar/llm_tutorial.md",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/docs%2Fsource%2Far%2Fllm_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/docs%2Fsource%2Far%2Fllm_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Fllm_tutorial.md?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -60,10 +60,10 @@ pip install transformers bitsandbytes>=0.39.0 -q\n Ø£ÙˆÙ„Ø§Ù‹ØŒ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬.\n \n ```py\n->>> from transformers import AutoModelForCausalLM\n+>>> from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n \n >>> model = AutoModelForCausalLM.from_pretrained(\n-...     \"mistralai/Mistral-7B-v0.1\", device_map=\"auto\", load_in_4bit=True\n+...     \"mistralai/Mistral-7B-v0.1\", device_map=\"auto\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n ... )\n ```\n \n@@ -113,12 +113,12 @@ pip install transformers bitsandbytes>=0.39.0 -q\n Ù‡Ù†Ø§Ùƒ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† [Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯](generation_strategies)ØŒ ÙˆÙÙŠ Ø¨Ø¹Ø¶ Ø§Ù„Ø£Ø­ÙŠØ§Ù† Ù‚Ø¯ Ù„Ø§ ØªÙƒÙˆÙ† Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ø­Ø§Ù„ØªÙƒ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…. Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ Ù…Ø§ ØªØªÙˆÙ‚Ø¹Ù‡ØŒ ÙÙ‚Ø¯ Ù‚Ù…Ù†Ø§ Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø£ÙƒØ«Ø± Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© ÙˆÙƒÙŠÙÙŠØ© ØªØ¬Ù†Ø¨Ù‡Ø§.\n \n ```py\n->>> from transformers import AutoModelForCausalLM, AutoTokenizer\n+>>> from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n \n >>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n >>> tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default\n >>> model = AutoModelForCausalLM.from_pretrained(\n-...     \"mistralai/Mistral-7B-v0.1\", device_map=\"auto\", load_in_4bit=True\n+...     \"mistralai/Mistral-7B-v0.1\", device_map=\"auto\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n ... )\n ```\n \n@@ -192,7 +192,7 @@ LLMs Ù‡ÙŠ [Ù…Ø¹Ù…Ø§Ø±ÙŠØ§Øª ÙÙƒ Ø§Ù„ØªØ´ÙÙŠØ± ÙÙ‚Ø·](https://huggingface.co/l\n ```python\n >>> tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\")\n >>> model = AutoModelForCausalLM.from_pretrained(\n-...     \"HuggingFaceH4/zephyr-7b-alpha\", device_map=\"auto\", load_in_4bit=True\n+...     \"HuggingFaceH4/zephyr-7b-alpha\", device_map=\"auto\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n ... )\n >>> set_seed(0)\n >>> prompt = \"\"\"How many helicopters can a human eat in one sitting? Reply as a thug.\"\"\""
        },
        {
            "sha": "91c1d2625469e0856735378a23416f89f520695e",
            "filename": "docs/source/ar/llm_tutorial_optimization.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/docs%2Fsource%2Far%2Fllm_tutorial_optimization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/docs%2Fsource%2Far%2Fllm_tutorial_optimization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Fllm_tutorial_optimization.md?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -231,7 +231,7 @@ flush()\n Ø¯Ø¹Ù†Ø§ Ù†Ø±Ù‰ Ù…Ø§ Ù‡Ùˆ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø°Ø§ÙƒØ±Ø© GPU Ø§Ù„Ø°Ø±ÙˆØ© Ø§Ù„Ø°ÙŠ ÙŠÙˆÙØ±Ù‡ ØªÙƒÙ…ÙŠÙ… 4 Ø¨Øª. ÙŠÙ…ÙƒÙ† ØªÙƒÙ…ÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ 4 Ø¨Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†ÙØ³ ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª ÙƒÙ…Ø§ ÙÙŠ Ø§Ù„Ø³Ø§Ø¨Ù‚ - Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø±Ø© Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± `load_in_4bit=True` Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† `load_in_8bit=True`.\n \n ```python\n-model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", load_in_4bit=True, pad_token_id=0)\n+model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", quantization_config=BitsAndBytesConfig(load_in_4bit=True), pad_token_id=0)\n \n pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n "
        },
        {
            "sha": "d664b7719bedfc1de4d3d925d1e44f2ef7ba3c21",
            "filename": "docs/source/de/llm_tutorial.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/docs%2Fsource%2Fde%2Fllm_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/docs%2Fsource%2Fde%2Fllm_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fde%2Fllm_tutorial.md?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -78,10 +78,10 @@ Wenn Sie an der grundlegenden Verwendung von LLMs interessiert sind, ist unsere\n ZunÃ¤chst mÃ¼ssen Sie das Modell laden.\n \n ```py\n->>> from transformers import AutoModelForCausalLM\n+>>> from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n \n >>> model = AutoModelForCausalLM.from_pretrained(\n-...     \"openlm-research/open_llama_7b\", device_map=\"auto\", load_in_4bit=True\n+...     \"openlm-research/open_llama_7b\", device_map=\"auto\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n ... )\n ```\n \n@@ -119,12 +119,12 @@ Und das war's! Mit ein paar Zeilen Code kÃ¶nnen Sie sich die Macht eines LLM zun\n Es gibt viele [Generierungsstrategien](generation_strategies), und manchmal sind die Standardwerte fÃ¼r Ihren Anwendungsfall vielleicht nicht geeignet. Wenn Ihre Ausgaben nicht mit dem Ã¼bereinstimmen, was Sie erwarten, haben wir eine Liste der hÃ¤ufigsten Fallstricke erstellt und wie Sie diese vermeiden kÃ¶nnen.\n \n ```py\n->>> from transformers import AutoModelForCausalLM, AutoTokenizer\n+>>> from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n \n >>> tokenizer = AutoTokenizer.from_pretrained(\"openlm-research/open_llama_7b\")\n >>> tokenizer.pad_token = tokenizer.eos_token  # Llama has no pad token by default\n >>> model = AutoModelForCausalLM.from_pretrained(\n-...     \"openlm-research/open_llama_7b\", device_map=\"auto\", load_in_4bit=True\n+...     \"openlm-research/open_llama_7b\", device_map=\"auto\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n ... )\n ```\n "
        },
        {
            "sha": "a192f840824d8fdc043bfd7ebc3abce12c192ec1",
            "filename": "docs/source/en/llm_tutorial.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/docs%2Fsource%2Fen%2Fllm_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/docs%2Fsource%2Fen%2Fllm_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_tutorial.md?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -259,11 +259,11 @@ Some models and tasks expect a certain input prompt format, and if the format is\n For example, a chat model expects the input as a [chat template](./chat_templating). Your prompt should include a `role` and `content` to indicate who is participating in the conversation. If you try to pass your prompt as a single string, the model doesn't always return the expected output.\n \n ```py\n-from transformers import AutoTokenizer, AutoModelForCausalLM\n+from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n \n tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\")\n model = AutoModelForCausalLM.from_pretrained(\n-    \"HuggingFaceH4/zephyr-7b-alpha\", device_map=\"auto\", load_in_4bit=True\n+    \"HuggingFaceH4/zephyr-7b-alpha\", device_map=\"auto\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n )\n ```\n "
        },
        {
            "sha": "2a368a84156de75792a13899ce0011b5ee117c42",
            "filename": "docs/source/en/llm_tutorial_optimization.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -194,7 +194,7 @@ the [`bitsandbytes`](https://github.com/bitsandbytes-foundation/bitsandbytes) li\n We can then load models in 8-bit quantization by simply adding a `load_in_8bit=True` flag to `from_pretrained`.\n \n ```python\n-model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", load_in_8bit=True, pad_token_id=0)\n+model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", quantization_config=BitsAndBytesConfig(load_in_8bit=True), pad_token_id=0)\n ```\n \n Now, let's run our example again and measure the memory usage.\n@@ -241,7 +241,7 @@ flush()\n Let's see what peak GPU memory consumption 4-bit quantization gives. Quantizing the model to 4-bit can be done with the same API as before - this time by passing `load_in_4bit=True` instead of `load_in_8bit=True`.\n \n ```python\n-model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", load_in_4bit=True, pad_token_id=0)\n+model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", quantization_config=BitsAndBytesConfig(load_in_4bit=True), pad_token_id=0)\n \n pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n "
        },
        {
            "sha": "bc0614c714c3efd1bbed5cc53282f594ad70276e",
            "filename": "docs/source/en/model_doc/flan-ul2.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/docs%2Fsource%2Fen%2Fmodel_doc%2Fflan-ul2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/docs%2Fsource%2Fen%2Fmodel_doc%2Fflan-ul2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fflan-ul2.md?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -40,9 +40,9 @@ The original checkpoints can be found [here](https://github.com/google-research/\n The model is pretty heavy (~40GB in half precision) so if you just want to run the model, make sure you load your model in 8bit, and use `device_map=\"auto\"` to make sure  you don't have any OOM issue!\n \n ```python\n->>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n+>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, BitsAndBytesConfig\n \n->>> model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-ul2\", load_in_8bit=True, device_map=\"auto\")\n+>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-ul2\", quantization_config=BitsAndBytesConfig(load_in_8bit=True), device_map=\"auto\")\n >>> tokenizer = AutoTokenizer.from_pretrained(\"google/flan-ul2\")\n \n >>> inputs = tokenizer(\"A step by step recipe to make bolognese pasta:\", return_tensors=\"pt\")"
        },
        {
            "sha": "165192408dcc266ddf93d538490efbdbb7144171",
            "filename": "docs/source/en/pipeline_tutorial.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/docs%2Fsource%2Fen%2Fpipeline_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/docs%2Fsource%2Fen%2Fpipeline_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fpipeline_tutorial.md?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -345,7 +345,7 @@ The `device_map=\"auto\"` setting is useful for automatically distributing the mod\n > [!TIP]\n > Inputs are internally converted to torch.float16 and it only works for models with a PyTorch backend.\n \n-Lastly, [`Pipeline`] also accepts quantized models to reduce memory usage even further. Make sure you have the [bitsandbytes](https://hf.co/docs/bitsandbytes/installation) library installed first, and then add `load_in_8bit=True` to `model_kwargs` in the pipeline.\n+Lastly, [`Pipeline`] also accepts quantized models to reduce memory usage even further. Make sure you have the [bitsandbytes](https://hf.co/docs/bitsandbytes/installation) library installed first, and then add `quantization_config` to `model_kwargs` in the pipeline.\n \n ```py\n import torch"
        },
        {
            "sha": "5a0f07f8ec39d16722952e886bc5255ba6b960c8",
            "filename": "docs/source/es/pipeline_tutorial.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/docs%2Fsource%2Fes%2Fpipeline_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/docs%2Fsource%2Fes%2Fpipeline_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Fpipeline_tutorial.md?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -286,14 +286,14 @@ pipe = pipeline(model=\"facebook/opt-1.3b\", dtype=torch.bfloat16, device_map=\"aut\n output = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\n ```\n \n-TambiÃ©n puedes pasar modelos cargados de 8 bits sÃ­ instalas `bitsandbytes` y agregas el argumento `load_in_8bit=True`\n+TambiÃ©n puedes pasar modelos cargados de 8 bits sÃ­ instalas `bitsandbytes` y agregas el argumento `quantization_config`\n \n ```py\n # pip install accelerate bitsandbytes\n import torch\n from transformers import pipeline\n \n-pipe = pipeline(model=\"facebook/opt-1.3b\", device_map=\"auto\", model_kwargs={\"load_in_8bit\": True})\n+pipe = pipeline(model=\"facebook/opt-1.3b\", device_map=\"auto\", model_kwargs={\"quantization_config\": BitsAndBytesConfig(load_in_8bit=True)})\n output = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\n ```\n "
        },
        {
            "sha": "8dc953f6b14e1360160eec8822f5f7de59f13cca",
            "filename": "docs/source/fr/tutoriel_pipeline.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/docs%2Fsource%2Ffr%2Ftutoriel_pipeline.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/docs%2Fsource%2Ffr%2Ftutoriel_pipeline.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Ffr%2Ftutoriel_pipeline.md?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -275,15 +275,15 @@ from transformers import pipeline\n pipe = pipeline(model=\"facebook/opt-1.3b\", dtype=torch.bfloat16, device_map=\"auto\")\n output = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\n ```\n-Vous pouvez Ã©galement passer des modÃ¨les chargÃ©s en 8 bits si vous installez `bitsandbytes` et ajoutez l'argument `load_in_8bit=True`\n+Vous pouvez Ã©galement passer des modÃ¨les chargÃ©s en 8 bits si vous installez `bitsandbytes` et ajoutez l'argument `quantization_config` \n Notez que vous pouvez remplacer le point de contrÃ´le par n'importe quel modÃ¨le.\n \n ```py\n # pip install accelerate bitsandbytes\n import torch\n-from transformers import pipeline\n+from transformers import pipeline, BitsAndBytesConfig\n \n-pipe = pipeline(model=\"facebook/opt-1.3b\", device_map=\"auto\", model_kwargs={\"load_in_8bit\": True})\n+pipe = pipeline(model=\"facebook/opt-1.3b\", device_map=\"auto\", model_kwargs={\"quantization_config\": BitsAndBytesConfig(load_in_8bit=True)})\n output = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\n ```\n "
        },
        {
            "sha": "438400e646694872ce0ed461f7ecb0929758f943",
            "filename": "docs/source/hi/pipeline_tutorial.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/docs%2Fsource%2Fhi%2Fpipeline_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/docs%2Fsource%2Fhi%2Fpipeline_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fhi%2Fpipeline_tutorial.md?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -310,9 +310,9 @@ output = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\n ```py\n # pip install accelerate bitsandbytes\n import torch\n-from transformers import pipeline\n+from transformers import pipeline, BitsAndBytesConfig\n \n-pipe = pipeline(model=\"facebook/opt-1.3b\", device_map=\"auto\", model_kwargs={\"load_in_8bit\": True})\n+pipe = pipeline(model=\"facebook/opt-1.3b\", device_map=\"auto\", model_kwargs={\"quantization_config\": BitsAndBytesConfig(load_in_8bit=True)})\n output = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\n ```\n "
        },
        {
            "sha": "0cf325edbe463f01108c29aa7301933ca9d8edee",
            "filename": "docs/source/ja/llm_tutorial.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/docs%2Fsource%2Fja%2Fllm_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/docs%2Fsource%2Fja%2Fllm_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fllm_tutorial.md?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -80,10 +80,10 @@ LLMï¼ˆLanguage Modelï¼‰ã«ã‚ˆã‚‹è‡ªå·±å›žå¸°ç”Ÿæˆã®é‡è¦ãªå´é¢ã®1ã¤ã¯\n \n \n ```py\n->>> from transformers import AutoModelForCausalLM\n+>>> from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n \n >>> model = AutoModelForCausalLM.from_pretrained(\n-...     \"openlm-research/open_llama_7b\", device_map=\"auto\", load_in_4bit=True\n+...     \"openlm-research/open_llama_7b\", device_map=\"auto\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n ... )\n ```\n \n@@ -123,12 +123,12 @@ LLMï¼ˆLanguage Modelï¼‰ã«ã‚ˆã‚‹è‡ªå·±å›žå¸°ç”Ÿæˆã®é‡è¦ãªå´é¢ã®1ã¤ã¯\n [ç”Ÿæˆæˆ¦ç•¥](generation_strategies)ã¯ãŸãã•ã‚“ã‚ã‚Šã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å€¤ãŒã‚ãªãŸã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«é©ã—ã¦ã„ãªã„ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚å‡ºåŠ›ãŒæœŸå¾…é€šã‚Šã§ãªã„å ´åˆã€æœ€ã‚‚ä¸€èˆ¬çš„ãªè½ã¨ã—ç©´ã¨ãã®å›žé¿æ–¹æ³•ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆã—ã¾ã—ãŸã€‚\n \n ```py\n->>> from transformers import AutoModelForCausalLM, AutoTokenizer\n+>>> from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n \n >>> tokenizer = AutoTokenizer.from_pretrained(\"openlm-research/open_llama_7b\")\n >>> tokenizer.pad_token = tokenizer.eos_token  # Llama has no pad token by default\n >>> model = AutoModelForCausalLM.from_pretrained(\n-...     \"openlm-research/open_llama_7b\", device_map=\"auto\", load_in_4bit=True\n+...     \"openlm-research/open_llama_7b\", device_map=\"auto\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n ... )\n ```\n "
        },
        {
            "sha": "2ef8c6ca683a66a8419558454d0e1f5bddc3cee2",
            "filename": "docs/source/ja/main_classes/quantization.md",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/docs%2Fsource%2Fja%2Fmain_classes%2Fquantization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/docs%2Fsource%2Fja%2Fmain_classes%2Fquantization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmain_classes%2Fquantization.md?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -176,10 +176,10 @@ GPTQ ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’é‡å­åŒ–ã™ã‚‹æ–¹æ³•ã¨ã€peft ã‚’ä½¿ç”¨ã—ã¦\n ãƒ¢ãƒ‡ãƒ«ãŒ ðŸ¤— Accelerate ã«ã‚ˆã‚‹èª­ã¿è¾¼ã¿ã‚’ã‚µãƒãƒ¼ãƒˆã—ã€`torch.nn.Linear` ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒå«ã¾ã‚Œã¦ã„ã‚‹é™ã‚Šã€ [`~PreTrainedModel.from_pretrained`] ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã™ã¨ãã« `load_in_8bit` ã¾ãŸã¯ `load_in_4bit` å¼•æ•°ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’é‡å­åŒ–ã§ãã¾ã™ã€‚ã“ã‚Œã¯ã©ã®ã‚ˆã†ãªãƒ¢ãƒ€ãƒªãƒ†ã‚£ã§ã‚‚åŒæ§˜ã«æ©Ÿèƒ½ã™ã‚‹ã¯ãšã§ã™ã€‚\n \n ```python\n-from transformers import AutoModelForCausalLM\n+from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n \n-model_8bit = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", load_in_8bit=True)\n-model_4bit = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", load_in_4bit=True)\n+model_8bit = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", quantization_config=BitsAndBytesConfig(load_in_8bit=True))\n+model_4bit = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", quantization_config=BitsAndBytesConfig(load_in_4bit=True))\n ```\n \n ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€ä»–ã®ã™ã¹ã¦ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« (ä¾‹: `torch.nn.LayerNorm`) ã¯ `torch.float16` ã«å¤‰æ›ã•ã‚Œã¾ã™ãŒã€ãã® `dtype` ã‚’å¤‰æ›´ã—ãŸã„å ´åˆã¯ã€`dtype` å¼•æ•°ã‚’ä¸Šæ›¸ãã§ãã¾ã™ã€‚\n@@ -188,7 +188,7 @@ model_4bit = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", load_in_4\n >>> import torch\n >>> from transformers import AutoModelForCausalLM\n \n->>> model_8bit = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", load_in_8bit=True, dtype=torch.float32)\n+>>> model_8bit = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", quantization_config=BitsAndBytesConfig(load_in_8bit=True), dtype=torch.float32)\n >>> model_8bit.model.decoder.layers[-1].final_layer_norm.weight.dtype\n torch.float32\n ```\n@@ -230,7 +230,7 @@ from transformers import AutoModelForCausalLM, AutoTokenizer\n model_id = \"bigscience/bloom-1b7\"\n \n tokenizer = AutoTokenizer.from_pretrained(model_id)\n-model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", load_in_4bit=True)\n+model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", quantization_config=BitsAndBytesConfig(load_in_4bit=True))\n ```\n \n <Tip warning={true}>"
        },
        {
            "sha": "b17c13cf8806e8b5d78e7f6ee799c7787bc6edf4",
            "filename": "docs/source/ja/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/docs%2Fsource%2Fja%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/docs%2Fsource%2Fja%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fperf_infer_gpu_one.md?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -116,14 +116,14 @@ model = AutoModelForCausalLM.from_pretrained(\n \n ```python\n import torch\n-from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM\n+from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, LlamaForCausalLM\n \n model_id = \"tiiuae/falcon-7b\"\n tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n model = AutoModelForCausalLM.from_pretrained(\n     model_id,\n-    load_in_4bit=True,\n+    quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n     attn_implementation=\"flash_attention_2\",\n )\n ```\n@@ -134,15 +134,15 @@ model = AutoModelForCausalLM.from_pretrained(\n \n ```python\n import torch\n-from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM\n+from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, LlamaForCausalLM\n from peft import LoraConfig\n \n model_id = \"tiiuae/falcon-7b\"\n tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n model = AutoModelForCausalLM.from_pretrained(\n     model_id,\n-    load_in_4bit=True,\n+    quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n     attn_implementation=\"flash_attention_2\",\n )\n \n@@ -190,10 +190,10 @@ Note that this feature can also be used in a multi GPU setup.\n \n \n ```py\n-from transformers import AutoModelForCausalLM\n+from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n \n model_name = \"bigscience/bloom-2b5\"\n-model_4bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_4bit=True)\n+model_4bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", quantization_config=BitsAndBytesConfig(load_in_4bit=True))\n ```\n \n æ³¨æ„: `device_map`ã¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã™ãŒã€æŽ¨è«–æ™‚ã« `device_map = 'auto'` ã‚’è¨­å®šã™ã‚‹ã“ã¨ãŒæŽ¨å¥¨ã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€åˆ©ç”¨å¯èƒ½ãªãƒªã‚½ãƒ¼ã‚¹ã«åŠ¹çŽ‡çš„ã«ãƒ¢ãƒ‡ãƒ«ãŒãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã•ã‚Œã¾ã™ã€‚\n@@ -204,7 +204,7 @@ model_4bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\",\n \n ```py\n model_name = \"bigscience/bloom-2b5\"\n-model_4bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_4bit=True)\n+model_4bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", quantization_config=BitsAndBytesConfig(load_in_4bit=True))\n ```\n \n ã—ã‹ã—ã€`accelerate`ã‚’ä½¿ç”¨ã—ã¦ã€å„GPUã«å‰²ã‚Šå½“ã¦ã‚‹GPU RAMã‚’åˆ¶å¾¡ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ä»¥ä¸‹ã®ã‚ˆã†ã«ã€`max_memory`å¼•æ•°ã‚’ä½¿ç”¨ã—ã¾ã™ï¼š\n@@ -214,7 +214,7 @@ model_4bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\",\n max_memory_mapping = {0: \"600MB\", 1: \"1GB\"}\n model_name = \"bigscience/bloom-3b\"\n model_4bit = AutoModelForCausalLM.from_pretrained(\n-    model_name, device_map=\"auto\", load_in_4bit=True, max_memory=max_memory_mapping\n+    model_name, device_map=\"auto\", quantization_config=BitsAndBytesConfig(load_in_4bit=True), max_memory=max_memory_mapping\n )\n ```\n "
        },
        {
            "sha": "ae9b78e537ea4a7970ca2c1acc0f6697da301dfa",
            "filename": "docs/source/ko/llm_tutorial_optimization.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/docs%2Fsource%2Fko%2Fllm_tutorial_optimization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/docs%2Fsource%2Fko%2Fllm_tutorial_optimization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fllm_tutorial_optimization.md?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -182,7 +182,7 @@ $$ Y = X * \\text{dequantize}(W) $$\n ê·¸ëŸ° ë‹¤ìŒ `from_pretrained`ì— `load_in_8bit=True` í”Œëž˜ê·¸ë¥¼ ì¶”ê°€í•˜ì—¬ 8ë¹„íŠ¸ ì–‘ìží™”ë¡œ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n \n ```python\n-model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", load_in_8bit=True, pad_token_id=0)\n+model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", quantization_config=BitsAndBytesConfig(load_in_8bit=True), pad_token_id=0)\n ```\n \n ì´ì œ ì˜ˆì œë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ê³  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¸¡ì •í•´ ë´…ì‹œë‹¤.\n@@ -227,7 +227,7 @@ flush()\n ì´ì œ 4ë¹„íŠ¸ ì–‘ìží™”ê°€ ì œê³µí•˜ëŠ” ìµœëŒ€ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ í™•ì¸í•´ ë´…ì‹œë‹¤. 4ë¹„íŠ¸ë¡œ ëª¨ë¸ì„ ì–‘ìží™”í•˜ë ¤ë©´ ì´ì „ê³¼ ë™ì¼í•œ APIë¥¼ ì‚¬ìš©í•˜ë˜ ì´ë²ˆì—ëŠ” `load_in_8bit=True` ëŒ€ì‹  `load_in_4bit=True`ë¥¼ ì „ë‹¬í•˜ë©´ ë©ë‹ˆë‹¤.\n \n ```python\n-model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", load_in_4bit=True, pad_token_id=0)\n+model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", quantization_config=BitsAndBytesConfig(load_in_8bit=True), pad_token_id=0)\n \n pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n "
        },
        {
            "sha": "73f6ad3df5afddd556fcfe849d6989ab17b19f4f",
            "filename": "docs/source/ko/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/docs%2Fsource%2Fko%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/docs%2Fsource%2Fko%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fperf_infer_gpu_one.md?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -43,10 +43,10 @@ rendered properly in your Markdown viewer.\n ë‹¤ìŒ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ ë‹¨ì¼ GPUì—ì„œ ë¹ ë¥´ê²Œ FP4 ëª¨ë¸ì„ ì‹¤í–‰í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n \n ```py\n-from transformers import AutoModelForCausalLM\n+from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n \n model_name = \"bigscience/bloom-2b5\"\n-model_4bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_4bit=True)\n+model_4bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", quantization_config=BitsAndBytesConfig(load_in_4bit=True))\n ```\n `device_map`ì€ ì„ íƒ ì‚¬í•­ìž…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ `device_map = 'auto'`ë¡œ ì„¤ì •í•˜ëŠ” ê²ƒì´ ì‚¬ìš© ê°€ëŠ¥í•œ ë¦¬ì†ŒìŠ¤ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë””ìŠ¤íŒ¨ì¹˜í•˜ê¸° ë•Œë¬¸ì— ì¶”ë¡ ì— ìžˆì–´ ê¶Œìž¥ë©ë‹ˆë‹¤.\n \n@@ -55,15 +55,15 @@ model_4bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\",\n ë‹¤ì¤‘ GPUì—ì„œ í˜¼í•© 4ë¹„íŠ¸ ëª¨ë¸ì„ ê°€ì ¸ì˜¤ëŠ” ë°©ë²•ì€ ë‹¨ì¼ GPU ì„¤ì •ê³¼ ë™ì¼í•©ë‹ˆë‹¤(ë™ì¼í•œ ëª…ë ¹ì–´ ì‚¬ìš©):\n ```py\n model_name = \"bigscience/bloom-2b5\"\n-model_4bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_4bit=True)\n+model_4bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", quantization_config=BitsAndBytesConfig(load_in_4bit=True))\n ```\n í•˜ì§€ë§Œ `accelerate`ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° GPUì— í• ë‹¹í•  GPU RAMì„ ì œì–´í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì´ `max_memory` ì¸ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:\n \n ```py\n max_memory_mapping = {0: \"600MB\", 1: \"1GB\"}\n model_name = \"bigscience/bloom-3b\"\n model_4bit = AutoModelForCausalLM.from_pretrained(\n-    model_name, device_map=\"auto\", load_in_4bit=True, max_memory=max_memory_mapping\n+    model_name, device_map=\"auto\", quantization_config=BitsAndBytesConfig(load_in_4bit=True), max_memory=max_memory_mapping\n )\n ```\n ì´ ì˜ˆì—ì„œëŠ” ì²« ë²ˆì§¸ GPUê°€ 600MBì˜ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•˜ê³  ë‘ ë²ˆì§¸ GPUê°€ 1GBë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n@@ -146,7 +146,7 @@ model_8bit = AutoModelForCausalLM.from_pretrained(model_name, quantization_confi\n max_memory_mapping = {0: \"1GB\", 1: \"2GB\"}\n model_name = \"bigscience/bloom-3b\"\n model_8bit = AutoModelForCausalLM.from_pretrained(\n-    model_name, device_map=\"auto\", load_in_8bit=True, max_memory=max_memory_mapping\n+    model_name, device_map=\"auto\", quantization_config=BitsAndBytesConfig(load_in_4bit=True), max_memory=max_memory_mapping\n )\n ```\n ì´ ì˜ˆì‹œì—ì„œëŠ” ì²« ë²ˆì§¸ GPUê°€ 1GBì˜ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•˜ê³  ë‘ ë²ˆì§¸ GPUê°€ 2GBë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
        },
        {
            "sha": "19e3a9ce77677e99624c2cebbed8c52644db1a96",
            "filename": "docs/source/zh/llm_tutorial.md",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/docs%2Fsource%2Fzh%2Fllm_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/docs%2Fsource%2Fzh%2Fllm_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fllm_tutorial.md?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -79,10 +79,10 @@ pip install transformers bitsandbytes>=0.39.0 -q\n é¦–å…ˆï¼Œæ‚¨éœ€è¦åŠ è½½æ¨¡åž‹ã€‚\n \n ```py\n->>> from transformers import AutoModelForCausalLM\n+>>> from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n \n >>> model = AutoModelForCausalLM.from_pretrained(\n-...     \"mistralai/Mistral-7B-v0.1\", device_map=\"auto\", load_in_4bit=True\n+...     \"mistralai/Mistral-7B-v0.1\", device_map=\"auto\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n ... )\n ```\n \n@@ -133,12 +133,12 @@ pip install transformers bitsandbytes>=0.39.0 -q\n æœ‰è®¸å¤š[ç”Ÿæˆç­–ç•¥](generation_strategies)ï¼Œæœ‰æ—¶é»˜è®¤å€¼å¯èƒ½ä¸é€‚åˆæ‚¨çš„ç”¨ä¾‹ã€‚å¦‚æžœæ‚¨çš„è¾“å‡ºä¸Žæ‚¨æœŸæœ›çš„ç»“æžœä¸åŒ¹é…ï¼Œæˆ‘ä»¬å·²ç»åˆ›å»ºäº†ä¸€ä¸ªæœ€å¸¸è§çš„é™·é˜±åˆ—è¡¨ä»¥åŠå¦‚ä½•é¿å…å®ƒä»¬ã€‚\n \n ```py\n->>> from transformers import AutoModelForCausalLM, AutoTokenizer\n+>>> from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n \n >>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n >>> tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default\n >>> model = AutoModelForCausalLM.from_pretrained(\n-...     \"mistralai/Mistral-7B-v0.1\", device_map=\"auto\", load_in_4bit=True\n+...     \"mistralai/Mistral-7B-v0.1\", device_map=\"auto\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n ... )\n ```\n \n@@ -214,7 +214,7 @@ LLMsæ˜¯[ä»…è§£ç å™¨](https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt)\n ```python\n >>> tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\")\n >>> model = AutoModelForCausalLM.from_pretrained(\n-...     \"HuggingFaceH4/zephyr-7b-alpha\", device_map=\"auto\", load_in_4bit=True\n+...     \"HuggingFaceH4/zephyr-7b-alpha\", device_map=\"auto\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n ... )\n >>> set_seed(0)\n >>> prompt = \"\"\"How many helicopters can a human eat in one sitting? Reply as a thug.\"\"\""
        },
        {
            "sha": "262558654341ee433254ace4da10058312c9771e",
            "filename": "docs/source/zh/main_classes/quantization.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/docs%2Fsource%2Fzh%2Fmain_classes%2Fquantization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/docs%2Fsource%2Fzh%2Fmain_classes%2Fquantization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fmain_classes%2Fquantization.md?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -289,19 +289,19 @@ model = AutoModelForCausalLM.from_pretrained(\"{your_username}/opt-125m-gptq\", de\n åªè¦æ‚¨çš„æ¨¡åž‹æ”¯æŒä½¿ç”¨ ðŸ¤— Accelerate è¿›è¡ŒåŠ è½½å¹¶åŒ…å« `torch.nn.Linear` å±‚ï¼Œæ‚¨å¯ä»¥åœ¨è°ƒç”¨ [`~PreTrainedModel.from_pretrained`] æ–¹æ³•æ—¶ä½¿ç”¨ `load_in_8bit` æˆ– `load_in_4bit` å‚æ•°æ¥é‡åŒ–æ¨¡åž‹ã€‚è¿™ä¹Ÿåº”è¯¥é€‚ç”¨äºŽä»»ä½•æ¨¡æ€ã€‚\n \n ```python\n-from transformers import AutoModelForCausalLM\n+from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n \n-model_8bit = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", load_in_8bit=True)\n-model_4bit = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", load_in_4bit=True)\n+model_8bit = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", quantization_config=BitsAndBytesConfig(load_in_8bit=True))\n+model_4bit = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", quantization_config=BitsAndBytesConfig(load_in_4bit=True))\n ```\n \n é»˜è®¤æƒ…å†µä¸‹ï¼Œæ‰€æœ‰å…¶ä»–æ¨¡å—ï¼ˆä¾‹å¦‚ `torch.nn.LayerNorm`ï¼‰å°†è¢«è½¬æ¢ä¸º `torch.float16` ç±»åž‹ã€‚ä½†å¦‚æžœæ‚¨æƒ³æ›´æ”¹å®ƒä»¬çš„ `dtype`ï¼Œå¯ä»¥é‡è½½ `dtype` å‚æ•°ï¼š\n \n ```python\n >>> import torch\n->>> from transformers import AutoModelForCausalLM\n+>>> from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n \n->>> model_8bit = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", load_in_8bit=True, dtype=torch.float32)\n+>>> model_8bit = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", quantization_config=BitsAndBytesConfig(load_in_8bit=True), dtype=torch.float32)\n >>> model_8bit.model.decoder.layers[-1].final_layer_norm.weight.dtype\n torch.float32\n ```\n@@ -344,7 +344,7 @@ from transformers import AutoModelForCausalLM, AutoTokenizer\n model_id = \"bigscience/bloom-1b7\"\n \n tokenizer = AutoTokenizer.from_pretrained(model_id)\n-model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", load_in_4bit=True)\n+model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", quantization_config=BitsAndBytesConfig(load_in_4bit=True))\n ```\n \n <Tip warning={true}>"
        },
        {
            "sha": "010e79b8d95e51dee59fe64b34c6e3ea47506d10",
            "filename": "docs/source/zh/pipeline_tutorial.md",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/docs%2Fsource%2Fzh%2Fpipeline_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/docs%2Fsource%2Fzh%2Fpipeline_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fpipeline_tutorial.md?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -302,7 +302,8 @@ output = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\n import torch\n from transformers import pipeline\n \n-pipe = pipeline(model=\"facebook/opt-1.3b\", device_map=\"auto\", model_kwargs={\"load_in_8bit\": True})\n+\n+pipe = pipeline(model=\"facebook/opt-1.3b\", device_map=\"auto\", model_kwargs={\"quantization_config\": BitsAndBytesConfig(load_in_8bit=True)})\n output = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\n ```\n "
        },
        {
            "sha": "e932ddf953e2b8b5840ac324be11a621eb73ee2d",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 26,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -118,7 +118,7 @@\n     is_torch_fx_proxy,\n     is_torchdynamo_compiling,\n )\n-from .utils.quantization_config import BitsAndBytesConfig, QuantizationMethod\n+from .utils.quantization_config import QuantizationMethod\n \n \n if is_accelerate_available():\n@@ -4381,10 +4381,7 @@ def from_pretrained(\n                 Whether or not to offload the buffers with the model parameters.\n             quantization_config (`Union[QuantizationConfigMixin,Dict]`, *optional*):\n                 A dictionary of configuration parameters or a QuantizationConfigMixin object for quantization (e.g\n-                bitsandbytes, gptq). There may be other quantization-related kwargs, including `load_in_4bit` and\n-                `load_in_8bit`, which are parsed by QuantizationConfigParser. Supported only for bitsandbytes\n-                quantizations and not preferred. consider inserting all such arguments into quantization_config\n-                instead.\n+                bitsandbytes, gptq).\n             subfolder (`str`, *optional*, defaults to `\"\"`):\n                 In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can\n                 specify the folder name here.\n@@ -4447,8 +4444,6 @@ def from_pretrained(\n         max_memory = kwargs.pop(\"max_memory\", None)\n         offload_folder = kwargs.pop(\"offload_folder\", None)\n         offload_buffers = kwargs.pop(\"offload_buffers\", False)\n-        load_in_8bit = kwargs.pop(\"load_in_8bit\", False)\n-        load_in_4bit = kwargs.pop(\"load_in_4bit\", False)\n         quantization_config = kwargs.pop(\"quantization_config\", None)\n         subfolder = kwargs.pop(\"subfolder\", \"\")\n         commit_hash = kwargs.pop(\"_commit_hash\", None)\n@@ -4626,25 +4621,6 @@ def from_pretrained(\n                     \"requires `accelerate`. You can install it with `pip install accelerate`\"\n                 )\n \n-        # handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.\n-        if load_in_4bit or load_in_8bit:\n-            if quantization_config is not None:\n-                raise ValueError(\n-                    \"You can't pass `load_in_4bit`or `load_in_8bit` as a kwarg when passing \"\n-                    \"`quantization_config` argument at the same time.\"\n-                )\n-\n-            # preparing BitsAndBytesConfig from kwargs\n-            config_dict = {k: v for k, v in kwargs.items() if k in inspect.signature(BitsAndBytesConfig).parameters}\n-            config_dict = {**config_dict, \"load_in_4bit\": load_in_4bit, \"load_in_8bit\": load_in_8bit}\n-            quantization_config, kwargs = BitsAndBytesConfig.from_dict(\n-                config_dict=config_dict, return_unused_kwargs=True, **kwargs\n-            )\n-            logger.warning(\n-                \"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. \"\n-                \"Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\"\n-            )\n-\n         user_agent = {\"file_type\": \"model\", \"framework\": \"pytorch\", \"from_auto_class\": from_auto_class}\n         if from_pipeline is not None:\n             user_agent[\"using_pipeline\"] = from_pipeline"
        },
        {
            "sha": "74dcd6483299997fe719cc3511afd84edc691d07",
            "filename": "src/transformers/quantizers/auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/src%2Ftransformers%2Fquantizers%2Fauto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/src%2Ftransformers%2Fquantizers%2Fauto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fauto.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -121,11 +121,7 @@ class AutoQuantizationConfig:\n     @classmethod\n     def from_dict(cls, quantization_config_dict: dict):\n         quant_method = quantization_config_dict.get(\"quant_method\")\n-        # We need a special care for bnb models to make sure everything is BC ..\n-        if quantization_config_dict.get(\"load_in_8bit\", False) or quantization_config_dict.get(\"load_in_4bit\", False):\n-            suffix = \"_4bit\" if quantization_config_dict.get(\"load_in_4bit\", False) else \"_8bit\"\n-            quant_method = QuantizationMethod.BITS_AND_BYTES + suffix\n-        elif quant_method is None:\n+        if quant_method is None:\n             raise ValueError(\n                 \"The model's quantization config from the arguments has no `quant_method` attribute. Make sure that the model has been correctly quantized\"\n             )"
        },
        {
            "sha": "2bdeb27514e376f3888bd0b4d7ac3b04d57f9109",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -407,8 +407,6 @@ class BitsAndBytesConfig(QuantizationConfigMixin):\n     This is a wrapper class about all possible attributes and features that you can play with a model that has been\n     loaded using `bitsandbytes`.\n \n-    This replaces `load_in_8bit` or `load_in_4bit`therefore both options are mutually exclusive.\n-\n     Currently only supports `LLM.int8()`, `FP4`, and `NF4` quantization. If more methods are added to `bitsandbytes`,\n     then more arguments will be added to this class.\n "
        },
        {
            "sha": "53572f639e5235cbc432bbfbb869c14bdd7a2678",
            "filename": "tests/models/aya_vision/test_modeling_aya_vision.py",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -20,6 +20,7 @@\n from transformers import (\n     AutoProcessor,\n     AyaVisionConfig,\n+    BitsAndBytesConfig,\n     is_torch_available,\n )\n from transformers.testing_utils import (\n@@ -231,12 +232,14 @@ def get_model(cls):\n         load_in_4bit = (device_type == \"cuda\") and (major < 8)\n         dtype = None if load_in_4bit else torch.float16\n \n+        if load_in_4bit:\n+            quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n+        else:\n+            quantization_config = None\n+\n         if cls.model is None:\n             cls.model = AyaVisionForConditionalGeneration.from_pretrained(\n-                cls.model_checkpoint,\n-                device_map=torch_device,\n-                dtype=dtype,\n-                load_in_4bit=load_in_4bit,\n+                cls.model_checkpoint, device_map=torch_device, dtype=dtype, quantization_config=quantization_config\n             )\n         return cls.model\n "
        },
        {
            "sha": "5babdce86214c772e7149afa716ec058687709ce",
            "filename": "tests/models/chameleon/test_modeling_chameleon.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -19,7 +19,7 @@\n import requests\n from parameterized import parameterized\n \n-from transformers import ChameleonConfig, is_torch_available, is_vision_available, set_seed\n+from transformers import BitsAndBytesConfig, ChameleonConfig, is_torch_available, is_vision_available, set_seed\n from transformers.testing_utils import (\n     Expectations,\n     require_bitsandbytes,\n@@ -366,7 +366,7 @@ class ChameleonIntegrationTest(unittest.TestCase):\n     @require_read_token\n     def test_model_7b(self):\n         model = ChameleonForConditionalGeneration.from_pretrained(\n-            \"facebook/chameleon-7b\", load_in_4bit=True, device_map=\"auto\"\n+            \"facebook/chameleon-7b\", quantization_config=BitsAndBytesConfig(load_in_4bit=True), device_map=\"auto\"\n         )\n         processor = ChameleonProcessor.from_pretrained(\"facebook/chameleon-7b\")\n \n@@ -396,7 +396,7 @@ def test_model_7b(self):\n     @require_read_token\n     def test_model_7b_batched(self):\n         model = ChameleonForConditionalGeneration.from_pretrained(\n-            \"facebook/chameleon-7b\", load_in_4bit=True, device_map=\"auto\"\n+            \"facebook/chameleon-7b\", quantization_config=BitsAndBytesConfig(load_in_4bit=True), device_map=\"auto\"\n         )\n         processor = ChameleonProcessor.from_pretrained(\"facebook/chameleon-7b\")\n \n@@ -443,7 +443,7 @@ def test_model_7b_batched(self):\n     @require_read_token\n     def test_model_7b_multi_image(self):\n         model = ChameleonForConditionalGeneration.from_pretrained(\n-            \"facebook/chameleon-7b\", load_in_4bit=True, device_map=\"auto\"\n+            \"facebook/chameleon-7b\", quantization_config=BitsAndBytesConfig(load_in_4bit=True), device_map=\"auto\"\n         )\n         processor = ChameleonProcessor.from_pretrained(\"facebook/chameleon-7b\")\n "
        },
        {
            "sha": "515fee4ccf8d85ea7bcd56ac967bf23fad21dcf6",
            "filename": "tests/models/colqwen2/test_modeling_colqwen2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fcolqwen2%2Ftest_modeling_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fcolqwen2%2Ftest_modeling_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolqwen2%2Ftest_modeling_colqwen2.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -23,7 +23,7 @@\n \n from tests.test_configuration_common import ConfigTester\n from tests.test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n-from transformers import is_torch_available\n+from transformers import BitsAndBytesConfig, is_torch_available\n from transformers.models.colqwen2.configuration_colqwen2 import ColQwen2Config\n from transformers.models.colqwen2.modeling_colqwen2 import ColQwen2ForRetrieval, ColQwen2ForRetrievalOutput\n from transformers.models.colqwen2.processing_colqwen2 import ColQwen2Processor\n@@ -301,7 +301,7 @@ def test_model_integration_test(self):\n         model = ColQwen2ForRetrieval.from_pretrained(\n             self.model_name,\n             dtype=torch.float16,\n-            load_in_8bit=True,\n+            quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n         ).eval()\n \n         # Load the test dataset"
        },
        {
            "sha": "1c98627b643e8c7f5703f14a9c77a98c761096b2",
            "filename": "tests/models/diffllama/test_modeling_diffllama.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -21,7 +21,7 @@\n from packaging import version\n from parameterized import parameterized\n \n-from transformers import AutoTokenizer, DiffLlamaConfig, StaticCache, is_torch_available, set_seed\n+from transformers import AutoTokenizer, BitsAndBytesConfig, DiffLlamaConfig, StaticCache, is_torch_available, set_seed\n from transformers.testing_utils import (\n     backend_empty_cache,\n     cleanup,\n@@ -437,7 +437,7 @@ def test_flash_attn_2_generate_padding_right(self):\n         \"\"\"\n         model = DiffLlamaForCausalLM.from_pretrained(\n             \"kajuma/DiffLlama-0.3B-handcut\",\n-            load_in_4bit=True,\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n             device_map={\"\": 0},\n         )\n \n@@ -455,7 +455,7 @@ def test_flash_attn_2_generate_padding_right(self):\n \n         model = DiffLlamaForCausalLM.from_pretrained(\n             \"kajuma/DiffLlama-0.3B-handcut\",\n-            load_in_4bit=True,\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n             device_map={\"\": 0},\n             attn_implementation=\"flash_attention_2\",\n         )"
        },
        {
            "sha": "06465e15fd1eb47bc873d204870fedfd570a7680",
            "filename": "tests/models/emu3/test_modeling_emu3.py",
            "status": "modified",
            "additions": 20,
            "deletions": 5,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -21,7 +21,14 @@\n from huggingface_hub import hf_hub_download\n from parameterized import parameterized\n \n-from transformers import Emu3Config, Emu3TextConfig, is_torch_available, is_vision_available, set_seed\n+from transformers import (\n+    BitsAndBytesConfig,\n+    Emu3Config,\n+    Emu3TextConfig,\n+    is_torch_available,\n+    is_vision_available,\n+    set_seed,\n+)\n from transformers.testing_utils import (\n     Expectations,\n     require_bitsandbytes,\n@@ -359,7 +366,9 @@ class Emu3IntegrationTest(unittest.TestCase):\n     @slow\n     @require_bitsandbytes\n     def test_model_generation(self):\n-        model = Emu3ForConditionalGeneration.from_pretrained(\"BAAI/Emu3-Chat-hf\", load_in_4bit=True)\n+        model = Emu3ForConditionalGeneration.from_pretrained(\n+            \"BAAI/Emu3-Chat-hf\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n+        )\n         processor = Emu3Processor.from_pretrained(\"BAAI/Emu3-Chat-hf\")\n \n         image = Image.open(requests.get(\"https://picsum.photos/id/237/200/200\", stream=True).raw)\n@@ -377,7 +386,9 @@ def test_model_generation(self):\n     @require_bitsandbytes\n     @require_torch_large_accelerator\n     def test_model_generation_batched(self):\n-        model = Emu3ForConditionalGeneration.from_pretrained(\"BAAI/Emu3-Chat-hf\", load_in_4bit=True)\n+        model = Emu3ForConditionalGeneration.from_pretrained(\n+            \"BAAI/Emu3-Chat-hf\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n+        )\n         processor = Emu3Processor.from_pretrained(\"BAAI/Emu3-Chat-hf\")\n         processor.tokenizer.padding_side = \"left\"\n \n@@ -420,7 +431,9 @@ def test_model_generation_batched(self):\n     @require_bitsandbytes\n     @require_torch_large_accelerator\n     def test_model_generation_multi_image(self):\n-        model = Emu3ForConditionalGeneration.from_pretrained(\"BAAI/Emu3-Chat-hf\", load_in_4bit=True)\n+        model = Emu3ForConditionalGeneration.from_pretrained(\n+            \"BAAI/Emu3-Chat-hf\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n+        )\n         processor = Emu3Processor.from_pretrained(\"BAAI/Emu3-Chat-hf\")\n \n         image = Image.open(requests.get(\"https://picsum.photos/id/237/50/50\", stream=True).raw)\n@@ -447,7 +460,9 @@ def test_model_generation_multi_image(self):\n     @require_bitsandbytes\n     @require_torch_large_accelerator\n     def test_model_generate_images(self):\n-        model = Emu3ForConditionalGeneration.from_pretrained(\"BAAI/Emu3-Gen-hf\", load_in_4bit=True)\n+        model = Emu3ForConditionalGeneration.from_pretrained(\n+            \"BAAI/Emu3-Gen-hf\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n+        )\n         processor = Emu3Processor.from_pretrained(\"BAAI/Emu3-Gen-hf\")\n \n         inputs = processor("
        },
        {
            "sha": "e1b1b0db47a215beea41f8c9e0c008385254baa7",
            "filename": "tests/models/ernie4_5_moe/test_modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fernie4_5_moe%2Ftest_modeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fernie4_5_moe%2Ftest_modeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fernie4_5_moe%2Ftest_modeling_ernie4_5_moe.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -18,7 +18,7 @@\n \n import pytest\n \n-from transformers import is_torch_available\n+from transformers import BitsAndBytesConfig, is_torch_available\n from transformers.testing_utils import (\n     cleanup,\n     is_flaky,\n@@ -159,7 +159,7 @@ def get_model(cls):\n             cls.model = Ernie4_5_MoeForCausalLM.from_pretrained(\n                 \"baidu/ERNIE-4.5-21B-A3B-PT\",\n                 device_map=\"auto\",\n-                load_in_4bit=True,\n+                quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n             )\n \n         return cls.model"
        },
        {
            "sha": "cbb050b1dfa9f92ddec2b037d52f5f0bff054a6e",
            "filename": "tests/models/esm/test_modeling_esm.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fesm%2Ftest_modeling_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fesm%2Ftest_modeling_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fesm%2Ftest_modeling_esm.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -18,7 +18,7 @@\n \n import pytest\n \n-from transformers import EsmConfig, is_torch_available\n+from transformers import BitsAndBytesConfig, EsmConfig, is_torch_available\n from transformers.testing_utils import (\n     TestCasePlus,\n     is_flaky,\n@@ -374,14 +374,18 @@ def test_inference_no_head(self):\n \n     @require_bitsandbytes\n     def test_inference_bitsandbytes(self):\n-        model = EsmForMaskedLM.from_pretrained(\"facebook/esm2_t36_3B_UR50D\", load_in_8bit=True)\n+        model = EsmForMaskedLM.from_pretrained(\n+            \"facebook/esm2_t36_3B_UR50D\", quantization_config=BitsAndBytesConfig(load_in_8bit=True)\n+        )\n \n         input_ids = torch.tensor([[0, 6, 4, 13, 5, 4, 16, 12, 11, 7, 2]]).to(model.device)\n         # Just test if inference works\n         with torch.no_grad():\n             _ = model(input_ids)[0]\n \n-        model = EsmForMaskedLM.from_pretrained(\"facebook/esm2_t36_3B_UR50D\", load_in_4bit=True)\n+        model = EsmForMaskedLM.from_pretrained(\n+            \"facebook/esm2_t36_3B_UR50D\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n+        )\n \n         input_ids = torch.tensor([[0, 6, 4, 13, 5, 4, 16, 12, 11, 7, 2]]).to(model.device)\n         # Just test if inference works"
        },
        {
            "sha": "da08aaa1e5328be1f3e16a21add508ddd3581068",
            "filename": "tests/models/falcon/test_modeling_falcon.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -18,6 +18,7 @@\n from transformers import (\n     AutoModelForCausalLM,\n     AutoTokenizer,\n+    BitsAndBytesConfig,\n     FalconConfig,\n     is_torch_available,\n )\n@@ -104,7 +105,9 @@ def test_lm_generate_falcon(self):\n     def test_lm_generate_falcon_11b(self):\n         tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-11B\", padding_side=\"left\")\n         model = FalconForCausalLM.from_pretrained(\n-            \"tiiuae/falcon-11B\", device_map={\"\": torch_device}, load_in_8bit=True\n+            \"tiiuae/falcon-11B\",\n+            device_map={\"\": torch_device},\n+            quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n         )\n         model.eval()\n         inputs = tokenizer(\n@@ -164,7 +167,7 @@ def test_batched_generation(self):\n         model = AutoModelForCausalLM.from_pretrained(\n             \"tiiuae/falcon-7b\",\n             device_map={\"\": torch_device},\n-            load_in_4bit=True,\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n         )\n \n         test_text = \"A sequence: 1, 2\"  # should generate the rest of the sequence"
        },
        {
            "sha": "6ab5308c81e29d8032228484b6892e7da622cadd",
            "filename": "tests/models/gemma/test_modeling_gemma.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -18,7 +18,7 @@\n import pytest\n from packaging import version\n \n-from transformers import AutoModelForCausalLM, AutoTokenizer, is_torch_available\n+from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, is_torch_available\n from transformers.generation.configuration_utils import GenerationConfig\n from transformers.testing_utils import (\n     DeviceProperties,\n@@ -197,7 +197,9 @@ def test_model_2b_4bit(self):\n             \"Hi today I'd like to share with you my experience with the new wattpad wattpad wattpad wattpad wattpad wattpad wattpad\",\n         ]\n \n-        model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True)\n+        model = AutoModelForCausalLM.from_pretrained(\n+            model_id, quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n+        )\n \n         tokenizer = AutoTokenizer.from_pretrained(model_id)\n         inputs = tokenizer(self.input_text, return_tensors=\"pt\", padding=True).to(torch_device)\n@@ -328,7 +330,9 @@ def test_model_7b_4bit(self):\n         )\n         EXPECTED_TEXTS = expectations.get_expectation()\n \n-        model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True)\n+        model = AutoModelForCausalLM.from_pretrained(\n+            model_id, quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n+        )\n \n         tokenizer = AutoTokenizer.from_pretrained(model_id)\n         inputs = tokenizer(self.input_text, return_tensors=\"pt\", padding=True).to(torch_device)"
        },
        {
            "sha": "7d22c0a3e098a719877ec75e7d16c7c3e12e39ca",
            "filename": "tests/models/idefics2/test_modeling_idefics2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -23,6 +23,7 @@\n \n from transformers import (\n     AutoProcessor,\n+    BitsAndBytesConfig,\n     Idefics2Config,\n     Idefics2ForConditionalGeneration,\n     Idefics2Model,\n@@ -591,8 +592,7 @@ def test_integration_test(self):\n     def test_integration_test_4bit(self):\n         # Let' s make sure we test the preprocessing to replace what is used\n         model = Idefics2ForConditionalGeneration.from_pretrained(\n-            \"HuggingFaceM4/idefics2-8b-base\",\n-            load_in_4bit=True,\n+            \"HuggingFaceM4/idefics2-8b-base\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n         )\n \n         # Create pixel inputs\n@@ -619,8 +619,7 @@ def test_integration_test_4bit_batch2(self):\n         # Let' s make sure we test the preprocessing to replace what is used\n \n         model = Idefics2ForConditionalGeneration.from_pretrained(\n-            \"HuggingFaceM4/idefics2-8b-base\",\n-            load_in_4bit=True,\n+            \"HuggingFaceM4/idefics2-8b-base\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n         )\n \n         from datasets import load_dataset\n@@ -662,7 +661,7 @@ def test_flash_attn_2_eager_equivalence(self):\n         model_eager = Idefics2ForConditionalGeneration.from_pretrained(\n             \"HuggingFaceM4/idefics2-8b-base\",\n             attn_implementation=\"eager\",\n-            load_in_4bit=True,\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n         )\n         generated_ids_eager = model_eager.generate(**inputs, max_new_tokens=10)\n         generated_texts_eager = self.processor.batch_decode(generated_ids_eager, skip_special_tokens=True)\n@@ -673,7 +672,7 @@ def test_flash_attn_2_eager_equivalence(self):\n         model_flash_attention_2 = Idefics2ForConditionalGeneration.from_pretrained(\n             \"HuggingFaceM4/idefics2-8b-base\",\n             attn_implementation=\"flash_attention_2\",\n-            load_in_4bit=True,\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n         )\n         generated_ids_flash_attention_2 = model_flash_attention_2.generate(**inputs, max_new_tokens=10)\n         generated_texts_flash_attention_2 = self.processor.batch_decode("
        },
        {
            "sha": "47e62c87cc48155eaa3a1d9925f11969cd217768",
            "filename": "tests/models/idefics3/test_modeling_idefics3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -42,6 +42,7 @@\n     import torch\n \n     from transformers import (\n+        BitsAndBytesConfig,\n         Idefics3Config,\n         Idefics3ForConditionalGeneration,\n         Idefics3Model,\n@@ -533,7 +534,7 @@ def test_integration_test_4bit(self):\n         # Let' s make sure we test the preprocessing to replace what is used\n         model = Idefics3ForConditionalGeneration.from_pretrained(\n             \"HuggingFaceM4/Idefics3-8B-Llama3\",\n-            load_in_4bit=True,\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n             device_map=\"auto\",\n         )\n "
        },
        {
            "sha": "2dd0f1d52ed784cedae288fb6d336f02a9c796fe",
            "filename": "tests/models/instructblip/test_modeling_instructblip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -22,6 +22,7 @@\n \n from transformers import (\n     CONFIG_MAPPING,\n+    BitsAndBytesConfig,\n     InstructBlipConfig,\n     InstructBlipProcessor,\n     InstructBlipQFormerConfig,\n@@ -631,7 +632,7 @@ def tearDown(self):\n     def test_inference_vicuna_7b(self):\n         processor = InstructBlipProcessor.from_pretrained(\"Salesforce/instructblip-vicuna-7b\")\n         model = InstructBlipForConditionalGeneration.from_pretrained(\n-            \"Salesforce/instructblip-vicuna-7b\", load_in_8bit=True\n+            \"Salesforce/instructblip-vicuna-7b\", quantization_config=BitsAndBytesConfig(load_in_8bit=True)\n         )\n \n         url = \"https://raw.githubusercontent.com/salesforce/LAVIS/main/docs/_static/Confusing-Pictures.jpg\""
        },
        {
            "sha": "ea4ecad76feba34c73d552ba054b35c76ae40506",
            "filename": "tests/models/instructblipvideo/test_modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -22,6 +22,7 @@\n \n from transformers import (\n     CONFIG_MAPPING,\n+    BitsAndBytesConfig,\n     InstructBlipVideoConfig,\n     InstructBlipVideoProcessor,\n     InstructBlipVideoQFormerConfig,\n@@ -643,7 +644,7 @@ def test_inference_vicuna_7b(self):\n         processor = InstructBlipVideoProcessor.from_pretrained(\"Salesforce/instructblip-vicuna-7b\")\n         model = InstructBlipVideoForConditionalGeneration.from_pretrained(\n             \"Salesforce/instructblip-vicuna-7b\",\n-            load_in_8bit=True,\n+            quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n         )\n \n         clip = prepare_video()"
        },
        {
            "sha": "983d132e41d64d7d5f2895a3077beb212029c8bb",
            "filename": "tests/models/jamba/test_modeling_jamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -19,7 +19,7 @@\n \n import pytest\n \n-from transformers import AutoTokenizer, JambaConfig, is_torch_available\n+from transformers import AutoTokenizer, BitsAndBytesConfig, JambaConfig, is_torch_available\n from transformers.testing_utils import (\n     DeviceProperties,\n     Expectations,\n@@ -538,7 +538,7 @@ def test_flash_attn_2_fp32_ln(self):\n                     tmpdirname,\n                     dtype=torch.float16,\n                     attn_implementation=\"flash_attention_2\",\n-                    load_in_4bit=True,\n+                    quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n                 )\n \n                 for _, param in model.named_parameters():"
        },
        {
            "sha": "3147f388d203bd0035fa4da920e9865e1ddfef1b",
            "filename": "tests/models/llava/test_modeling_llava.py",
            "status": "modified",
            "additions": 28,
            "deletions": 9,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -22,6 +22,7 @@\n from transformers import (\n     AutoProcessor,\n     AutoTokenizer,\n+    BitsAndBytesConfig,\n     LlavaConfig,\n     LlavaForConditionalGeneration,\n     LlavaModel,\n@@ -291,7 +292,9 @@ def tearDown(self):\n     @require_bitsandbytes\n     def test_small_model_integration_test(self):\n         # Let's make sure we test the preprocessing to replace what is used\n-        model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/bakLlava-v1-hf\", load_in_4bit=True)\n+        model = LlavaForConditionalGeneration.from_pretrained(\n+            \"llava-hf/bakLlava-v1-hf\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n+        )\n \n         prompt = \"<image>\\nUSER: What are the things I should be cautious about when I visit this place?\\nASSISTANT:\"\n         image_file = \"https://llava-vl.github.io/static/images/view.jpg\"\n@@ -317,7 +320,9 @@ def test_small_model_integration_test_llama_single(self):\n         # Let's make sure we test the preprocessing to replace what is used\n         model_id = \"llava-hf/llava-1.5-7b-hf\"\n \n-        model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", load_in_4bit=True)\n+        model = LlavaForConditionalGeneration.from_pretrained(\n+            \"llava-hf/llava-1.5-7b-hf\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n+        )\n         processor = AutoProcessor.from_pretrained(model_id)\n \n         prompt = \"USER: <image>\\nWhat are the things I should be cautious about when I visit this place? ASSISTANT:\"\n@@ -346,7 +351,9 @@ def test_small_model_integration_test_llama_batched(self):\n         # Let's make sure we test the preprocessing to replace what is used\n         model_id = \"llava-hf/llava-1.5-7b-hf\"\n \n-        model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", load_in_4bit=True)\n+        model = LlavaForConditionalGeneration.from_pretrained(\n+            \"llava-hf/llava-1.5-7b-hf\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n+        )\n         processor = AutoProcessor.from_pretrained(model_id)\n \n         prompts = [\n@@ -394,7 +401,9 @@ def test_small_model_integration_test_llama_batched(self):\n     @require_bitsandbytes\n     def test_small_model_integration_test_batch(self):\n         # Let's make sure we test the preprocessing to replace what is used\n-        model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/bakLlava-v1-hf\", load_in_4bit=True)\n+        model = LlavaForConditionalGeneration.from_pretrained(\n+            \"llava-hf/bakLlava-v1-hf\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n+        )\n         # The first batch is longer in terms of text, but only has 1 image. The second batch will be padded in text, but the first will be padded because images take more space!.\n         prompts = [\n             \"USER: <image>\\nWhat are the things I should be cautious about when I visit this place? What should I bring with me?\\nASSISTANT:\",\n@@ -444,7 +453,9 @@ def test_small_model_integration_test_llama_batched_regression(self):\n \n         # Multi-image & multi-prompt (e.g. 3 images and 2 prompts now fails with SDPA, this tests if \"eager\" works as before)\n         model = LlavaForConditionalGeneration.from_pretrained(\n-            \"llava-hf/llava-1.5-7b-hf\", load_in_4bit=True, attn_implementation=\"eager\"\n+            \"llava-hf/llava-1.5-7b-hf\",\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n+            attn_implementation=\"eager\",\n         )\n         processor = AutoProcessor.from_pretrained(model_id, pad_token=\"<pad>\")\n \n@@ -496,7 +507,9 @@ def test_small_model_integration_test_llama_batched_regression(self):\n     @require_vision\n     @require_bitsandbytes\n     def test_batched_generation(self):\n-        model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", load_in_4bit=True)\n+        model = LlavaForConditionalGeneration.from_pretrained(\n+            \"llava-hf/llava-1.5-7b-hf\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n+        )\n \n         processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n \n@@ -569,7 +582,9 @@ def test_tokenizer_integration(self):\n     @require_bitsandbytes\n     def test_generation_no_images(self):\n         model_id = \"llava-hf/llava-1.5-7b-hf\"\n-        model = LlavaForConditionalGeneration.from_pretrained(model_id, load_in_4bit=True)\n+        model = LlavaForConditionalGeneration.from_pretrained(\n+            model_id, quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n+        )\n         processor = AutoProcessor.from_pretrained(model_id)\n \n         # Prepare inputs with no images\n@@ -641,7 +656,9 @@ def test_pixtral(self):\n     @require_bitsandbytes\n     def test_pixtral_4bit(self):\n         model_id = \"mistral-community/pixtral-12b\"\n-        model = LlavaForConditionalGeneration.from_pretrained(model_id, load_in_4bit=True)\n+        model = LlavaForConditionalGeneration.from_pretrained(\n+            model_id, quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n+        )\n         processor = AutoProcessor.from_pretrained(model_id)\n \n         IMG_URLS = [\n@@ -668,7 +685,9 @@ def test_pixtral_4bit(self):\n     @require_bitsandbytes\n     def test_pixtral_batched(self):\n         model_id = \"mistral-community/pixtral-12b\"\n-        model = LlavaForConditionalGeneration.from_pretrained(model_id, load_in_4bit=True)\n+        model = LlavaForConditionalGeneration.from_pretrained(\n+            model_id, quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n+        )\n         processor = AutoProcessor.from_pretrained(model_id)\n         processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n "
        },
        {
            "sha": "547a37957be9626e5e380fcda3a03296930604a7",
            "filename": "tests/models/llava_next/test_modeling_llava_next.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -22,6 +22,7 @@\n \n from transformers import (\n     AutoProcessor,\n+    BitsAndBytesConfig,\n     LlavaNextConfig,\n     LlavaNextForConditionalGeneration,\n     LlavaNextModel,\n@@ -329,7 +330,7 @@ def tearDown(self):\n     def test_small_model_integration_test(self):\n         model = LlavaNextForConditionalGeneration.from_pretrained(\n             \"llava-hf/llava-v1.6-mistral-7b-hf\",\n-            load_in_4bit=True,\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n         )\n \n         inputs = self.processor(images=self.image, text=self.prompt, return_tensors=\"pt\").to(torch_device)\n@@ -372,7 +373,7 @@ def test_small_model_integration_test(self):\n     @require_bitsandbytes\n     def test_small_model_integration_test_batch(self):\n         model = LlavaNextForConditionalGeneration.from_pretrained(\n-            \"llava-hf/llava-v1.6-mistral-7b-hf\", load_in_4bit=True\n+            \"llava-hf/llava-v1.6-mistral-7b-hf\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n         )\n         url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n         cats_image = Image.open(requests.get(url, stream=True).raw)\n@@ -399,7 +400,7 @@ def test_small_model_integration_test_unk_token(self):\n         # related to (#29835)\n         model = LlavaNextForConditionalGeneration.from_pretrained(\n             \"llava-hf/llava-v1.6-mistral-7b-hf\",\n-            load_in_4bit=True,\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n         )\n \n         prompt_with_unk = \"[INST] <image>\\nWhat is shown in this <unk> image? [/INST]\"\n@@ -424,7 +425,7 @@ def test_small_model_integration_test_unk_token(self):\n     def test_small_model_integration_test_batch_different_resolutions(self):\n         model = LlavaNextForConditionalGeneration.from_pretrained(\n             \"llava-hf/llava-v1.6-mistral-7b-hf\",\n-            load_in_4bit=True,\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n         )\n \n         url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n@@ -464,7 +465,7 @@ def test_small_model_integration_test_batch_different_resolutions(self):\n     def test_small_model_integration_test_batch_matches_single(self):\n         model = LlavaNextForConditionalGeneration.from_pretrained(\n             \"llava-hf/llava-v1.6-mistral-7b-hf\",\n-            load_in_4bit=True,\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n         )\n \n         url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n@@ -493,7 +494,7 @@ def test_small_model_integration_test_batch_matches_single(self):\n     def test_small_model_integration_test_full_vision_state_selection(self):\n         model = LlavaNextForConditionalGeneration.from_pretrained(\n             \"llava-hf/llava-v1.6-mistral-7b-hf\",\n-            load_in_4bit=True,\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n         )\n         # test that changing `strategy` won't error out\n         model.vision_feature_select_strategy = \"full\""
        },
        {
            "sha": "aba7b644f733227332f957ca5b2b8bd0c5f913d3",
            "filename": "tests/models/llava_next_video/test_modeling_llava_next_video.py",
            "status": "modified",
            "additions": 11,
            "deletions": 4,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -22,6 +22,7 @@\n \n from transformers import (\n     AutoProcessor,\n+    BitsAndBytesConfig,\n     LlavaNextVideoConfig,\n     LlavaNextVideoForConditionalGeneration,\n     LlavaNextVideoModel,\n@@ -351,7 +352,9 @@ def tearDown(self):\n     @require_bitsandbytes\n     def test_small_model_integration_test(self):\n         model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n-            \"llava-hf/LLaVA-NeXT-Video-7B-hf\", load_in_4bit=True, cache_dir=\"./\"\n+            \"llava-hf/LLaVA-NeXT-Video-7B-hf\",\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n+            cache_dir=\"./\",\n         )\n \n         inputs = self.processor(text=self.prompt_video, videos=self.video, return_tensors=\"pt\")\n@@ -377,7 +380,9 @@ def test_small_model_integration_test(self):\n     @require_bitsandbytes\n     def test_small_model_integration_test_batch(self):\n         model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n-            \"llava-hf/LLaVA-NeXT-Video-7B-hf\", load_in_4bit=True, cache_dir=\"./\"\n+            \"llava-hf/LLaVA-NeXT-Video-7B-hf\",\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n+            cache_dir=\"./\",\n         )\n \n         inputs = self.processor(\n@@ -406,7 +411,7 @@ def test_small_model_integration_test_batch(self):\n     def test_small_model_integration_test_batch_different_vision_types(self):\n         model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n             \"llava-hf/LLaVA-NeXT-Video-7B-hf\",\n-            load_in_4bit=True,\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n             cache_dir=\"./\",\n         )\n \n@@ -441,7 +446,9 @@ def test_small_model_integration_test_batch_different_vision_types(self):\n     @require_bitsandbytes\n     def test_small_model_integration_test_batch_matches_single(self):\n         model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n-            \"llava-hf/LLaVA-NeXT-Video-7B-hf\", load_in_4bit=True, cache_dir=\"./\"\n+            \"llava-hf/LLaVA-NeXT-Video-7B-hf\",\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n+            cache_dir=\"./\",\n         )\n \n         inputs_batched = self.processor("
        },
        {
            "sha": "5d13e595911f8414cd84e6b6a4ab808ad383befc",
            "filename": "tests/models/ministral/test_modeling_ministral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fministral%2Ftest_modeling_ministral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fministral%2Ftest_modeling_ministral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fministral%2Ftest_modeling_ministral.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -20,7 +20,7 @@\n \n import pytest\n \n-from transformers import AutoTokenizer, GenerationConfig, is_torch_available\n+from transformers import AutoTokenizer, BitsAndBytesConfig, GenerationConfig, is_torch_available\n from transformers.testing_utils import (\n     backend_empty_cache,\n     cleanup,\n@@ -233,7 +233,7 @@ def test_past_sliding_window_generation(self):\n         model = MinistralForCausalLM.from_pretrained(\n             \"mistralai/Ministral-8B-Instruct-2410\",\n             device_map=\"auto\",\n-            load_in_4bit=True,\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n         )\n         tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Ministral-8B-Instruct-2410\", legacy=False)\n "
        },
        {
            "sha": "b457d7b345de0a27c4f53d051fda33090eadefb1",
            "filename": "tests/models/mistral/test_modeling_mistral.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -20,7 +20,7 @@\n from packaging import version\n from parameterized import parameterized\n \n-from transformers import AutoTokenizer, DynamicCache, is_torch_available, set_seed\n+from transformers import AutoTokenizer, BitsAndBytesConfig, DynamicCache, is_torch_available, set_seed\n from transformers.cache_utils import DynamicSlidingWindowLayer\n from transformers.testing_utils import (\n     DeviceProperties,\n@@ -136,7 +136,9 @@ def test_model_7b_generation(self):\n         prompt = \"My favourite condiment is \"\n         tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", use_fast=False)\n         model = MistralForCausalLM.from_pretrained(\n-            \"mistralai/Mistral-7B-v0.1\", device_map={\"\": torch_device}, load_in_4bit=True\n+            \"mistralai/Mistral-7B-v0.1\",\n+            device_map={\"\": torch_device},\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n         )\n         input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.model.embed_tokens.weight.device)\n \n@@ -185,7 +187,7 @@ def test_model_7b_long_prompt(self):\n         model = MistralForCausalLM.from_pretrained(\n             \"mistralai/Mistral-7B-v0.1\",\n             device_map={\"\": torch_device},\n-            load_in_4bit=True,\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n             attn_implementation=\"flash_attention_2\",\n         )\n         input_ids = torch.tensor([input_ids]).to(model.model.embed_tokens.weight.device)"
        },
        {
            "sha": "ca1fdc8b0e7408ed97810c41a06626065a5b2a36",
            "filename": "tests/models/mpt/test_modeling_mpt.py",
            "status": "modified",
            "additions": 25,
            "deletions": 5,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fmpt%2Ftest_modeling_mpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fmpt%2Ftest_modeling_mpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmpt%2Ftest_modeling_mpt.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -16,7 +16,7 @@\n import math\n import unittest\n \n-from transformers import MptConfig, is_torch_available\n+from transformers import BitsAndBytesConfig, MptConfig, is_torch_available\n from transformers.testing_utils import (\n     Expectations,\n     require_bitsandbytes,\n@@ -439,7 +439,12 @@ def test_generation_8k(self):\n         tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n         # Load in 4bit to fit the daily CI runner GPU RAM\n-        model = MptForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16, device_map={\"\": 0}, load_in_4bit=True)\n+        model = MptForCausalLM.from_pretrained(\n+            model_id,\n+            dtype=torch.bfloat16,\n+            device_map={\"\": 0},\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n+        )\n \n         input_text = \"Hello\"\n         expected_outputs = Expectations({\n@@ -460,7 +465,12 @@ def test_generation(self):\n         tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n         # Load in 4bit to fit the daily CI runner GPU RAM\n-        model = MptForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16, device_map={\"\": 0}, load_in_4bit=True)\n+        model = MptForCausalLM.from_pretrained(\n+            model_id,\n+            dtype=torch.bfloat16,\n+            device_map={\"\": 0},\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n+        )\n \n         input_text = \"Hello\"\n         expected_outputs = Expectations({\n@@ -483,7 +493,12 @@ def test_generation_batched(self):\n         tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n         # Load in 4bit to fit the daily CI runner GPU RAM\n-        model = MptForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16, device_map={\"\": 0}, load_in_4bit=True)\n+        model = MptForCausalLM.from_pretrained(\n+            model_id,\n+            dtype=torch.bfloat16,\n+            device_map={\"\": 0},\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n+        )\n \n         input_texts = [\"Hello my name is\", \"Today I am going at the gym and\"]\n         tokenizer.pad_token_id = tokenizer.eos_token_id\n@@ -522,7 +537,12 @@ def test_model_logits(self):\n         model_id = \"mosaicml/mpt-7b\"\n \n         # Load in 4bit to fit the daily CI runner GPU RAM\n-        model = MptForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16, device_map={\"\": 0}, load_in_4bit=True)\n+        model = MptForCausalLM.from_pretrained(\n+            model_id,\n+            dtype=torch.bfloat16,\n+            device_map={\"\": 0},\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n+        )\n \n         dummy_input = torch.LongTensor([[1, 2, 3, 4, 5]]).to(torch_device)\n "
        },
        {
            "sha": "a772a4e14c5f9334ffd59e52bc6a254ce1b0225c",
            "filename": "tests/models/perception_lm/test_modeling_perception_lm.py",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fperception_lm%2Ftest_modeling_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fperception_lm%2Ftest_modeling_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fperception_lm%2Ftest_modeling_perception_lm.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -19,6 +19,7 @@\n \n from transformers import (\n     AutoProcessor,\n+    BitsAndBytesConfig,\n     PerceptionLMConfig,\n     PerceptionLMForConditionalGeneration,\n     PerceptionLMModel,\n@@ -418,7 +419,7 @@ def tearDown(self):\n \n     def test_small_model_integration_test(self):\n         model = PerceptionLMForConditionalGeneration.from_pretrained(\n-            TEST_MODEL_PATH, load_in_4bit=True, cache_dir=\"./\"\n+            TEST_MODEL_PATH, quantization_config=BitsAndBytesConfig(load_in_4bit=True), cache_dir=\"./\"\n         )\n \n         inputs = self.processor.apply_chat_template(\n@@ -444,7 +445,9 @@ def test_small_model_integration_test(self):\n         )\n \n     def test_small_model_integration_test_batched(self):\n-        model = PerceptionLMForConditionalGeneration.from_pretrained(TEST_MODEL_PATH, load_in_4bit=True)\n+        model = PerceptionLMForConditionalGeneration.from_pretrained(\n+            TEST_MODEL_PATH, quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n+        )\n         processor = AutoProcessor.from_pretrained(TEST_MODEL_PATH)\n         inputs = processor.apply_chat_template(\n             [self.conversation1, self.conversation2],\n@@ -470,7 +473,9 @@ def test_small_model_integration_test_batched(self):\n \n     def test_generation_no_images(self):\n         # model_id = \"facebook/Perception-LM-1B\"\n-        model = PerceptionLMForConditionalGeneration.from_pretrained(TEST_MODEL_PATH, load_in_4bit=True)\n+        model = PerceptionLMForConditionalGeneration.from_pretrained(\n+            TEST_MODEL_PATH, quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n+        )\n         processor = AutoProcessor.from_pretrained(TEST_MODEL_PATH)\n \n         # Prepare inputs with no images"
        },
        {
            "sha": "a9632beeaf4ae225671337873c875a139146a2a1",
            "filename": "tests/models/persimmon/test_modeling_persimmon.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -33,6 +33,7 @@\n \n     from transformers import (\n         AutoTokenizer,\n+        BitsAndBytesConfig,\n         PersimmonForCausalLM,\n         PersimmonForSequenceClassification,\n         PersimmonForTokenClassification,\n@@ -84,7 +85,10 @@ class PersimmonIntegrationTest(unittest.TestCase):\n     def test_model_8b_chat_logits(self):\n         input_ids = [1, 306, 4658, 278, 6593, 310, 2834, 338]\n         model = PersimmonForCausalLM.from_pretrained(\n-            \"adept/persimmon-8b-chat\", load_in_8bit=True, device_map={\"\": 0}, dtype=torch.float16\n+            \"adept/persimmon-8b-chat\",\n+            quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n+            device_map={\"\": 0},\n+            dtype=torch.float16,\n         )\n         out = model(torch.tensor([input_ids], device=torch_device)).logits.float()\n \n@@ -115,7 +119,10 @@ def test_model_8b_chat_greedy_generation(self):\n         tokenizer = AutoTokenizer.from_pretrained(\"adept/persimmon-8b-chat\", use_fast=False)\n         input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(torch_device)\n         model = PersimmonForCausalLM.from_pretrained(\n-            \"adept/persimmon-8b-chat\", load_in_8bit=True, device_map={\"\": 0}, dtype=torch.float16\n+            \"adept/persimmon-8b-chat\",\n+            quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n+            device_map={\"\": 0},\n+            dtype=torch.float16,\n         )\n \n         # greedy generation outputs"
        },
        {
            "sha": "92634454b60e6fc72594876fe9c8d210dead4d8a",
            "filename": "tests/models/qwen2/test_modeling_qwen2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -19,7 +19,7 @@\n import pytest\n from packaging import version\n \n-from transformers import AutoTokenizer, is_torch_available, set_seed\n+from transformers import AutoTokenizer, BitsAndBytesConfig, is_torch_available, set_seed\n from transformers.generation.configuration_utils import GenerationConfig\n from transformers.testing_utils import (\n     Expectations,\n@@ -132,7 +132,7 @@ def test_model_450m_long_prompt(self):\n         model = Qwen2ForCausalLM.from_pretrained(\n             \"Qwen/Qwen2-0.5B\",\n             device_map=\"auto\",\n-            load_in_4bit=True,\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n             attn_implementation=\"flash_attention_2\",\n         )\n         input_ids = torch.tensor([input_ids]).to(model.model.embed_tokens.weight.device)"
        },
        {
            "sha": "d47f79d6d33f30166777e9d2788ee7f2f9857d7c",
            "filename": "tests/models/qwen3/test_modeling_qwen3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -18,7 +18,7 @@\n import pytest\n from packaging import version\n \n-from transformers import AutoTokenizer, is_torch_available, set_seed\n+from transformers import AutoTokenizer, BitsAndBytesConfig, is_torch_available, set_seed\n from transformers.generation.configuration_utils import GenerationConfig\n from transformers.testing_utils import (\n     Expectations,\n@@ -126,7 +126,7 @@ def test_model_600m_long_prompt(self):\n         model = Qwen3ForCausalLM.from_pretrained(\n             \"Qwen/Qwen3-0.6B-Base\",\n             device_map=\"auto\",\n-            load_in_4bit=True,\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n             attn_implementation=\"flash_attention_2\",\n         )\n         input_ids = torch.tensor([input_ids]).to(model.model.embed_tokens.weight.device)"
        },
        {
            "sha": "b562217b703ad6a5b21522d4a4084fcad8a2b42c",
            "filename": "tests/models/qwen3_moe/test_modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fqwen3_moe%2Ftest_modeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fqwen3_moe%2Ftest_modeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_moe%2Ftest_modeling_qwen3_moe.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -17,7 +17,7 @@\n \n import pytest\n \n-from transformers import AutoTokenizer, is_torch_available, set_seed\n+from transformers import AutoTokenizer, BitsAndBytesConfig, is_torch_available, set_seed\n from transformers.testing_utils import (\n     cleanup,\n     require_bitsandbytes,\n@@ -137,7 +137,7 @@ def tearDown(self):\n     def get_model(cls):\n         if cls.model is None:\n             cls.model = Qwen3MoeForCausalLM.from_pretrained(\n-                \"Qwen/Qwen3-30B-A3B-Base\", device_map=\"auto\", load_in_4bit=True\n+                \"Qwen/Qwen3-30B-A3B-Base\", device_map=\"auto\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n             )\n \n         return cls.model\n@@ -182,7 +182,7 @@ def test_model_15b_a2b_long_prompt(self):\n         model = Qwen3MoeForCausalLM.from_pretrained(\n             \"Qwen/Qwen3-30B-A3B-Base\",\n             device_map=\"auto\",\n-            load_in_4bit=True,\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n             attn_implementation=\"flash_attention_2\",\n         )\n         input_ids = torch.tensor([input_ids]).to(model.model.embed_tokens.weight.device)"
        },
        {
            "sha": "af4c7578e0577801de4d3e868c3ce4e75be7bc94",
            "filename": "tests/models/recurrent_gemma/test_modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -18,7 +18,7 @@\n import pytest\n from parameterized import parameterized\n \n-from transformers import AutoModelForCausalLM, AutoTokenizer, is_torch_available, set_seed\n+from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, is_torch_available, set_seed\n from transformers.testing_utils import (\n     Expectations,\n     require_bitsandbytes,\n@@ -204,7 +204,10 @@ def test_model_2b_8bit(self):\n         EXPECTED_TEXTS = ['Hello I am doing a project on the topic of \"The impact of social media on the society\" and I am looking', \"Hi today I'm going to show you how to make a simple and easy to make a 3D\"]  # fmt: skip\n \n         model = AutoModelForCausalLM.from_pretrained(\n-            \"gg-hf/recurrent-gemma-2b-hf\", device_map={\"\": torch_device}, load_in_8bit=True, dtype=torch.bfloat16\n+            \"gg-hf/recurrent-gemma-2b-hf\",\n+            device_map={\"\": torch_device},\n+            quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n+            dtype=torch.bfloat16,\n         )\n \n         tokenizer = AutoTokenizer.from_pretrained(self.model_id, padding_side=\"left\")"
        },
        {
            "sha": "99cbe2cd127b462ead48f47d2c4ec5669d46bdc5",
            "filename": "tests/models/shieldgemma2/test_modeling_shieldgemma2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fshieldgemma2%2Ftest_modeling_shieldgemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fshieldgemma2%2Ftest_modeling_shieldgemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fshieldgemma2%2Ftest_modeling_shieldgemma2.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -19,7 +19,7 @@\n import requests\n from PIL import Image\n \n-from transformers import is_torch_available\n+from transformers import BitsAndBytesConfig, is_torch_available\n from transformers.testing_utils import (\n     cleanup,\n     require_read_token,\n@@ -48,7 +48,9 @@ def test_model(self):\n         response = requests.get(url)\n         image = Image.open(BytesIO(response.content))\n \n-        model = ShieldGemma2ForImageClassification.from_pretrained(model_id, load_in_4bit=True)\n+        model = ShieldGemma2ForImageClassification.from_pretrained(\n+            model_id, quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n+        )\n \n         inputs = processor(images=[image], return_tensors=\"pt\").to(torch_device)\n         output = model(**inputs)"
        },
        {
            "sha": "7b27968bc98e347e9ef1ff33d36a9068e90940c1",
            "filename": "tests/models/smollm3/test_modeling_smollm3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fsmollm3%2Ftest_modeling_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fsmollm3%2Ftest_modeling_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmollm3%2Ftest_modeling_smollm3.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -20,7 +20,7 @@\n from packaging import version\n from parameterized import parameterized\n \n-from transformers import AutoTokenizer, SmolLM3Config, is_torch_available\n+from transformers import AutoTokenizer, BitsAndBytesConfig, SmolLM3Config, is_torch_available\n from transformers.generation.configuration_utils import GenerationConfig\n from transformers.testing_utils import (\n     backend_empty_cache,\n@@ -138,7 +138,7 @@ def test_model_3b_long_prompt(self):\n         model = SmolLM3ForCausalLM.from_pretrained(\n             self.model_id,\n             device_map=\"auto\",\n-            load_in_4bit=True,\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n             attn_implementation=\"flash_attention_2\",\n         )\n         input_ids = torch.tensor([input_ids]).to(model.model.embed_tokens.weight.device)"
        },
        {
            "sha": "ae443ece22954e3f5bae3c5b19dd3ef00934527a",
            "filename": "tests/models/stablelm/test_modeling_stablelm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -17,7 +17,7 @@\n \n import pytest\n \n-from transformers import is_torch_available\n+from transformers import BitsAndBytesConfig, is_torch_available\n from transformers.testing_utils import (\n     require_bitsandbytes,\n     require_flash_attn,\n@@ -142,7 +142,7 @@ def test_model_3b_long_prompt(self):\n             \"stabilityai/stablelm-3b-4e1t\",\n             device_map=\"auto\",\n             dtype=\"auto\",\n-            load_in_4bit=True,\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n             attn_implementation=\"flash_attention_2\",\n         )\n         input_ids = torch.tensor([input_ids]).to(model.model.embed_tokens.weight.device)"
        },
        {
            "sha": "cd05d5812781802ae34dd14e2b39d31654ba5cc4",
            "filename": "tests/models/starcoder2/test_modeling_starcoder2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -17,7 +17,7 @@\n \n import pytest\n \n-from transformers import is_torch_available\n+from transformers import BitsAndBytesConfig, is_torch_available\n from transformers.testing_utils import (\n     Expectations,\n     require_bitsandbytes,\n@@ -146,7 +146,9 @@ def test_starcoder2_batched_generation_4bit(self):\n \n         model_id = \"bigcode/starcoder2-7b\"\n \n-        model = Starcoder2ForCausalLM.from_pretrained(model_id, load_in_4bit=True)\n+        model = Starcoder2ForCausalLM.from_pretrained(\n+            model_id, quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n+        )\n         tokenizer = AutoTokenizer.from_pretrained(model_id)\n         tokenizer.pad_token = tokenizer.eos_token\n "
        },
        {
            "sha": "2baf0b96b9ec57dfeccd4e0f1253343bc8041bdf",
            "filename": "tests/models/video_llava/test_modeling_video_llava.py",
            "status": "modified",
            "additions": 13,
            "deletions": 4,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -22,6 +22,7 @@\n from parameterized import parameterized\n \n from transformers import (\n+    BitsAndBytesConfig,\n     VideoLlavaConfig,\n     VideoLlavaForConditionalGeneration,\n     VideoLlavaModel,\n@@ -414,7 +415,9 @@ def tearDown(self):\n     @require_bitsandbytes\n     def test_small_model_integration_test(self):\n         # Let' s make sure we test the preprocessing to replace what is used\n-        model = VideoLlavaForConditionalGeneration.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\", load_in_4bit=True)\n+        model = VideoLlavaForConditionalGeneration.from_pretrained(\n+            \"LanguageBind/Video-LLaVA-7B-hf\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n+        )\n \n         prompt = \"USER: <video>\\nWhy is this video funny? ASSISTANT:\"\n         video_file = hf_hub_download(\n@@ -438,7 +441,9 @@ def test_small_model_integration_test(self):\n     @slow\n     @require_bitsandbytes\n     def test_small_model_integration_test_mixed_inputs(self):\n-        model = VideoLlavaForConditionalGeneration.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\", load_in_4bit=True)\n+        model = VideoLlavaForConditionalGeneration.from_pretrained(\n+            \"LanguageBind/Video-LLaVA-7B-hf\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n+        )\n \n         prompts = [\n             \"USER: <image>\\nWhat are the cats in the image doing? ASSISTANT:\",\n@@ -469,7 +474,9 @@ def test_small_model_integration_test_mixed_inputs(self):\n     @slow\n     @require_bitsandbytes\n     def test_small_model_integration_test_llama(self):\n-        model = VideoLlavaForConditionalGeneration.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\", load_in_4bit=True)\n+        model = VideoLlavaForConditionalGeneration.from_pretrained(\n+            \"LanguageBind/Video-LLaVA-7B-hf\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n+        )\n         processor = VideoLlavaProcessor.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\")\n \n         prompt = \"USER: <video>\\nDescribe the video in details. ASSISTANT:\"\n@@ -494,7 +501,9 @@ def test_small_model_integration_test_llama(self):\n     @slow\n     @require_bitsandbytes\n     def test_small_model_integration_test_llama_batched(self):\n-        model = VideoLlavaForConditionalGeneration.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\", load_in_4bit=True)\n+        model = VideoLlavaForConditionalGeneration.from_pretrained(\n+            \"LanguageBind/Video-LLaVA-7B-hf\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n+        )\n         processor = VideoLlavaProcessor.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\")\n         processor.tokenizer.padding_side = \"left\"\n "
        },
        {
            "sha": "550561c7efbda1c4cfb34e65690be7fc61da341a",
            "filename": "tests/models/vipllava/test_modeling_vipllava.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -21,6 +21,7 @@\n \n from transformers import (\n     AutoProcessor,\n+    BitsAndBytesConfig,\n     VipLlavaConfig,\n     VipLlavaForConditionalGeneration,\n     VipLlavaModel,\n@@ -293,7 +294,9 @@ def tearDown(self):\n     def test_small_model_integration_test(self):\n         model_id = \"llava-hf/vip-llava-7b-hf\"\n \n-        model = VipLlavaForConditionalGeneration.from_pretrained(model_id, load_in_4bit=True)\n+        model = VipLlavaForConditionalGeneration.from_pretrained(\n+            model_id, quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n+        )\n         processor = AutoProcessor.from_pretrained(model_id)\n \n         url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/compel-neg.png\""
        },
        {
            "sha": "c57016ed308a8b15b220359263aca89dafa63489",
            "filename": "tests/models/zamba/test_modeling_zamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -19,7 +19,7 @@\n \n import pytest\n \n-from transformers import AutoTokenizer, ZambaConfig, is_torch_available\n+from transformers import AutoTokenizer, BitsAndBytesConfig, ZambaConfig, is_torch_available\n from transformers.testing_utils import (\n     require_bitsandbytes,\n     require_flash_attn,\n@@ -468,7 +468,7 @@ def test_flash_attn_2_fp32_ln(self):\n                     tmpdirname,\n                     dtype=torch.float16,\n                     attn_implementation=\"flash_attention_2\",\n-                    load_in_4bit=True,\n+                    quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n                 )\n \n                 for _, param in model.named_parameters():"
        },
        {
            "sha": "26193b8d635b45e2a9735c93884fe6091da045b1",
            "filename": "tests/models/zamba2/test_modeling_zamba2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -19,7 +19,7 @@\n import pytest\n from parameterized import parameterized\n \n-from transformers import AutoTokenizer, Zamba2Config, is_torch_available\n+from transformers import AutoTokenizer, BitsAndBytesConfig, Zamba2Config, is_torch_available\n from transformers.testing_utils import (\n     Expectations,\n     require_bitsandbytes,\n@@ -485,7 +485,7 @@ def test_flash_attn_2_fp32_ln(self):\n                     tmpdirname,\n                     dtype=torch.float16,\n                     attn_implementation=\"flash_attention_2\",\n-                    load_in_4bit=True,\n+                    quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n                 )\n \n                 for _, param in model.named_parameters():"
        },
        {
            "sha": "b28cf248ca978efd878f906531fcfb3714c2a004",
            "filename": "tests/quantization/bnb/test_4bit.py",
            "status": "modified",
            "additions": 30,
            "deletions": 18,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_4bit.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -125,7 +125,9 @@ def setUp(self):\n \n         # Models and tokenizer\n         self.model_fp16 = AutoModelForCausalLM.from_pretrained(self.model_name, dtype=torch.float16, device_map=\"auto\")\n-        self.model_4bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True, device_map=\"auto\")\n+        self.model_4bit = AutoModelForCausalLM.from_pretrained(\n+            self.model_name, quantization_config=BitsAndBytesConfig(load_in_4bit=True), device_map=\"auto\"\n+        )\n \n     def tearDown(self):\n         r\"\"\"\n@@ -354,7 +356,9 @@ def test_fp32_4bit_conversion(self):\n         r\"\"\"\n         Test whether it is possible to mix both `4bit` and `fp32` weights when using `keep_in_fp32_modules` correctly.\n         \"\"\"\n-        model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-small\", load_in_4bit=True, device_map=\"auto\")\n+        model = AutoModelForSeq2SeqLM.from_pretrained(\n+            \"google-t5/t5-small\", quantization_config=BitsAndBytesConfig(load_in_4bit=True), device_map=\"auto\"\n+        )\n         self.assertTrue(model.decoder.block[0].layer[2].DenseReluDense.wo.weight.dtype == torch.float32)\n \n     def test_bnb_4bit_wrong_config(self):\n@@ -398,13 +402,15 @@ def test_inference_without_keep_in_fp32(self):\n         T5ForConditionalGeneration._keep_in_fp32_modules = None\n \n         # test with `google-t5/t5-small`\n-        model = T5ForConditionalGeneration.from_pretrained(self.model_name, load_in_4bit=True, device_map=\"auto\")\n+        model = T5ForConditionalGeneration.from_pretrained(\n+            self.model_name, quantization_config=BitsAndBytesConfig(load_in_4bit=True), device_map=\"auto\"\n+        )\n         encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\").to(model.device)\n         _ = model.generate(**encoded_input)\n \n         # test with `flan-t5-small`\n         model = T5ForConditionalGeneration.from_pretrained(\n-            self.dense_act_model_name, load_in_4bit=True, device_map=\"auto\"\n+            self.dense_act_model_name, quantization_config=BitsAndBytesConfig(load_in_4bit=True), device_map=\"auto\"\n         )\n         encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\").to(model.device)\n         _ = model.generate(**encoded_input)\n@@ -419,7 +425,9 @@ def test_inference_with_keep_in_fp32(self):\n         from transformers import T5ForConditionalGeneration\n \n         # test with `google-t5/t5-small`\n-        model = T5ForConditionalGeneration.from_pretrained(self.model_name, load_in_4bit=True, device_map=\"auto\")\n+        model = T5ForConditionalGeneration.from_pretrained(\n+            self.model_name, quantization_config=BitsAndBytesConfig(load_in_4bit=True), device_map=\"auto\"\n+        )\n \n         # there was a bug with decoders - this test checks that it is fixed\n         self.assertTrue(isinstance(model.decoder.block[0].layer[0].SelfAttention.q, bnb.nn.Linear4bit))\n@@ -429,7 +437,7 @@ def test_inference_with_keep_in_fp32(self):\n \n         # test with `flan-t5-small`\n         model = T5ForConditionalGeneration.from_pretrained(\n-            self.dense_act_model_name, load_in_4bit=True, device_map=\"auto\"\n+            self.dense_act_model_name, quantization_config=BitsAndBytesConfig(load_in_4bit=True), device_map=\"auto\"\n         )\n         encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\").to(model.device)\n         _ = model.generate(**encoded_input)\n@@ -445,16 +453,20 @@ def setUp(self):\n \n         # Different types of model\n \n-        self.base_model = AutoModel.from_pretrained(self.model_name, load_in_4bit=True, device_map=\"auto\")\n+        self.base_model = AutoModel.from_pretrained(\n+            self.model_name, quantization_config=BitsAndBytesConfig(load_in_4bit=True), device_map=\"auto\"\n+        )\n         # Sequence classification model\n         self.sequence_model = AutoModelForSequenceClassification.from_pretrained(\n-            self.model_name, load_in_4bit=True, device_map=\"auto\"\n+            self.model_name, quantization_config=BitsAndBytesConfig(load_in_4bit=True), device_map=\"auto\"\n         )\n         # CausalLM model\n-        self.model_4bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True, device_map=\"auto\")\n+        self.model_4bit = AutoModelForCausalLM.from_pretrained(\n+            self.model_name, quantization_config=BitsAndBytesConfig(load_in_4bit=True), device_map=\"auto\"\n+        )\n         # Seq2seq model\n         self.seq_to_seq_model = AutoModelForSeq2SeqLM.from_pretrained(\n-            self.seq_to_seq_name, load_in_4bit=True, device_map=\"auto\"\n+            self.seq_to_seq_name, quantization_config=BitsAndBytesConfig(load_in_4bit=True), device_map=\"auto\"\n         )\n \n     def tearDown(self):\n@@ -513,7 +525,7 @@ def test_pipeline(self):\n             model=self.model_name,\n             model_kwargs={\n                 \"device_map\": \"auto\",\n-                \"load_in_4bit\": True,\n+                \"quantization_config\": BitsAndBytesConfig(load_in_4bit=True),\n                 # float16 isn't supported on CPU, use bfloat16 instead\n                 \"dtype\": torch.bfloat16 if torch_device == \"cpu\" else torch.float16,\n             },\n@@ -570,7 +582,7 @@ def test_multi_accelerator_loading(self):\n         }\n \n         model_parallel = AutoModelForCausalLM.from_pretrained(\n-            self.model_name, load_in_4bit=True, device_map=device_map\n+            self.model_name, quantization_config=BitsAndBytesConfig(load_in_4bit=True), device_map=device_map\n         )\n \n         # Check correct device map\n@@ -594,7 +606,9 @@ def setUp(self):\n \n     def test_training(self):\n         # Step 1: freeze all parameters\n-        model = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True)\n+        model = AutoModelForCausalLM.from_pretrained(\n+            self.model_name, quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n+        )\n \n         if torch_device in [\"cuda\", \"xpu\"]:\n             self.assertEqual(\n@@ -797,10 +811,6 @@ class LlamaSerializationTest(BaseSerializationTest):\n @slow\n @apply_skip_if_not_implemented\n class Bnb4BitTestBasicConfigTest(unittest.TestCase):\n-    def test_load_in_4_and_8_bit_fails(self):\n-        with self.assertRaisesRegex(ValueError, \"load_in_4bit and load_in_8bit are both True\"):\n-            AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\", load_in_4bit=True, load_in_8bit=True)\n-\n     def test_set_load_in_8_bit(self):\n         quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n         with self.assertRaisesRegex(ValueError, \"load_in_4bit and load_in_8bit are both True\"):\n@@ -818,7 +828,9 @@ class Bnb4bitCompile(unittest.TestCase):\n     def setUp(self):\n         # Models and tokenizer\n         self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n-        self.model_4bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True)\n+        self.model_4bit = AutoModelForCausalLM.from_pretrained(\n+            self.model_name, quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n+        )\n \n     @pytest.mark.torch_compile_test\n     def test_generate_compile(self):"
        },
        {
            "sha": "ecff095c0d8f02dc5ee85d3c34e5d252adc264ee",
            "filename": "tests/quantization/bnb/test_mixed_int8.py",
            "status": "modified",
            "additions": 40,
            "deletions": 33,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -126,7 +126,9 @@ def setUp(self):\n \n         # Models and tokenizer\n         self.model_fp16 = AutoModelForCausalLM.from_pretrained(self.model_name, dtype=torch.float16, device_map=\"auto\")\n-        self.model_8bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True, device_map=\"auto\")\n+        self.model_8bit = AutoModelForCausalLM.from_pretrained(\n+            self.model_name, quantization_config=BitsAndBytesConfig(load_in_8bit=True), device_map=\"auto\"\n+        )\n \n     def tearDown(self):\n         r\"\"\"\n@@ -297,21 +299,6 @@ def test_generate_quality_dequantize(self):\n \n         self.assertIn(self.tokenizer.decode(output_sequences[0], skip_special_tokens=True), self.EXPECTED_OUTPUTS)\n \n-    def test_raise_if_config_and_load_in_8bit(self):\n-        r\"\"\"\n-        Test that loading the model with the config and `load_in_8bit` raises an error\n-        \"\"\"\n-        bnb_config = BitsAndBytesConfig()\n-\n-        with self.assertRaises(ValueError):\n-            _ = AutoModelForCausalLM.from_pretrained(\n-                self.model_name,\n-                quantization_config=bnb_config,\n-                load_in_8bit=True,\n-                device_map=\"auto\",\n-                llm_int8_enable_fp32_cpu_offload=True,\n-            )\n-\n     def test_device_and_dtype_assignment(self):\n         r\"\"\"\n         Test whether attempting to change the device or cast the dtype of a model\n@@ -360,7 +347,9 @@ def test_fp32_int8_conversion(self):\n         r\"\"\"\n         Test whether it is possible to mix both `int8` and `fp32` weights when using `keep_in_fp32_modules` correctly.\n         \"\"\"\n-        model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-small\", load_in_8bit=True, device_map=\"auto\")\n+        model = AutoModelForSeq2SeqLM.from_pretrained(\n+            \"google-t5/t5-small\", quantization_config=BitsAndBytesConfig(load_in_8bit=True), device_map=\"auto\"\n+        )\n         self.assertTrue(model.decoder.block[0].layer[2].DenseReluDense.wo.weight.dtype == torch.float32)\n \n     def test_int8_serialization(self):\n@@ -376,7 +365,9 @@ def test_int8_serialization(self):\n             config = AutoConfig.from_pretrained(tmpdirname)\n             self.assertTrue(hasattr(config, \"quantization_config\"))\n \n-            model_from_saved = AutoModelForCausalLM.from_pretrained(tmpdirname, load_in_8bit=True, device_map=\"auto\")\n+            model_from_saved = AutoModelForCausalLM.from_pretrained(\n+                tmpdirname, quantization_config=BitsAndBytesConfig(load_in_8bit=True), device_map=\"auto\"\n+            )\n \n             linear = get_some_linear_layer(model_from_saved)\n             self.assertTrue(linear.weight.__class__ == Int8Params)\n@@ -403,7 +394,9 @@ def test_int8_serialization_regression(self):\n             config = AutoConfig.from_pretrained(tmpdirname)\n             self.assertTrue(hasattr(config, \"quantization_config\"))\n \n-            model_from_saved = AutoModelForCausalLM.from_pretrained(tmpdirname, load_in_8bit=True, device_map=\"auto\")\n+            model_from_saved = AutoModelForCausalLM.from_pretrained(\n+                tmpdirname, quantization_config=BitsAndBytesConfig(load_in_8bit=True), device_map=\"auto\"\n+            )\n \n             linear = get_some_linear_layer(model_from_saved)\n             self.assertTrue(linear.weight.__class__ == Int8Params)\n@@ -497,13 +490,15 @@ def test_inference_without_keep_in_fp32(self):\n         T5ForConditionalGeneration._keep_in_fp32_modules = None\n \n         # test with `google-t5/t5-small`\n-        model = T5ForConditionalGeneration.from_pretrained(self.model_name, load_in_8bit=True, device_map=\"auto\")\n+        model = T5ForConditionalGeneration.from_pretrained(\n+            self.model_name, quantization_config=BitsAndBytesConfig(load_in_8bit=True), device_map=\"auto\"\n+        )\n         encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\").to(model.device)\n         _ = model.generate(**encoded_input)\n \n         # test with `flan-t5-small`\n         model = T5ForConditionalGeneration.from_pretrained(\n-            self.dense_act_model_name, load_in_8bit=True, device_map=\"auto\"\n+            self.dense_act_model_name, quantization_config=BitsAndBytesConfig(load_in_8bit=True), device_map=\"auto\"\n         )\n         encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\").to(model.device)\n         _ = model.generate(**encoded_input)\n@@ -519,7 +514,9 @@ def test_inference_with_keep_in_fp32(self):\n         from transformers import T5ForConditionalGeneration\n \n         # test with `google-t5/t5-small`\n-        model = T5ForConditionalGeneration.from_pretrained(self.model_name, load_in_8bit=True, device_map=\"auto\")\n+        model = T5ForConditionalGeneration.from_pretrained(\n+            self.model_name, quantization_config=BitsAndBytesConfig(load_in_8bit=True), device_map=\"auto\"\n+        )\n \n         # there was a bug with decoders - this test checks that it is fixed\n         self.assertTrue(isinstance(model.decoder.block[0].layer[0].SelfAttention.q, bnb.nn.Linear8bitLt))\n@@ -529,7 +526,7 @@ def test_inference_with_keep_in_fp32(self):\n \n         # test with `flan-t5-small`\n         model = T5ForConditionalGeneration.from_pretrained(\n-            self.dense_act_model_name, load_in_8bit=True, device_map=\"auto\"\n+            self.dense_act_model_name, quantization_config=BitsAndBytesConfig(load_in_8bit=True), device_map=\"auto\"\n         )\n         encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\").to(model.device)\n         _ = model.generate(**encoded_input)\n@@ -545,7 +542,9 @@ def test_inference_with_keep_in_fp32_serialized(self):\n         from transformers import T5ForConditionalGeneration\n \n         # test with `google-t5/t5-small`\n-        model = T5ForConditionalGeneration.from_pretrained(self.model_name, load_in_8bit=True, device_map=\"auto\")\n+        model = T5ForConditionalGeneration.from_pretrained(\n+            self.model_name, quantization_config=BitsAndBytesConfig(load_in_8bit=True), device_map=\"auto\"\n+        )\n \n         with tempfile.TemporaryDirectory() as tmp_dir:\n             model.save_pretrained(tmp_dir)\n@@ -560,7 +559,7 @@ def test_inference_with_keep_in_fp32_serialized(self):\n \n             # test with `flan-t5-small`\n             model = T5ForConditionalGeneration.from_pretrained(\n-                self.dense_act_model_name, load_in_8bit=True, device_map=\"auto\"\n+                self.dense_act_model_name, quantization_config=BitsAndBytesConfig(load_in_8bit=True), device_map=\"auto\"\n             )\n             encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\").to(model.device)\n             _ = model.generate(**encoded_input)\n@@ -575,16 +574,20 @@ def setUp(self):\n \n         # Different types of model\n \n-        self.base_model = AutoModel.from_pretrained(self.model_name, load_in_8bit=True, device_map=\"auto\")\n+        self.base_model = AutoModel.from_pretrained(\n+            self.model_name, quantization_config=BitsAndBytesConfig(load_in_8bit=True), device_map=\"auto\"\n+        )\n         # Sequence classification model\n         self.sequence_model = AutoModelForSequenceClassification.from_pretrained(\n-            self.model_name, load_in_8bit=True, device_map=\"auto\"\n+            self.model_name, quantization_config=BitsAndBytesConfig(load_in_8bit=True), device_map=\"auto\"\n         )\n         # CausalLM model\n-        self.model_8bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True, device_map=\"auto\")\n+        self.model_8bit = AutoModelForCausalLM.from_pretrained(\n+            self.model_name, quantization_config=BitsAndBytesConfig(load_in_8bit=True), device_map=\"auto\"\n+        )\n         # Seq2seq model\n         self.seq_to_seq_model = AutoModelForSeq2SeqLM.from_pretrained(\n-            self.seq_to_seq_name, load_in_8bit=True, device_map=\"auto\"\n+            self.seq_to_seq_name, quantization_config=BitsAndBytesConfig(load_in_8bit=True), device_map=\"auto\"\n         )\n \n     def tearDown(self):\n@@ -696,7 +699,7 @@ def test_multi_gpu_loading(self):\n         }\n \n         model_parallel = AutoModelForCausalLM.from_pretrained(\n-            self.model_name, load_in_8bit=True, device_map=device_map\n+            self.model_name, quantization_config=BitsAndBytesConfig(load_in_8bit=True), device_map=device_map\n         )\n \n         # Check correct device map\n@@ -848,7 +851,7 @@ def test_cpu_accelerator_disk_loading_custom_device_map_kwargs(self):\n             model_8bit = AutoModelForCausalLM.from_pretrained(\n                 self.model_name,\n                 device_map=device_map,\n-                load_in_8bit=True,\n+                quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n                 llm_int8_enable_fp32_cpu_offload=True,\n                 offload_folder=tmpdirname,\n             )\n@@ -867,7 +870,9 @@ def setUp(self):\n \n     def test_training(self):\n         # Step 1: freeze all parameters\n-        model = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True)\n+        model = AutoModelForCausalLM.from_pretrained(\n+            self.model_name, quantization_config=BitsAndBytesConfig(load_in_8bit=True)\n+        )\n         model.train()\n \n         if torch_device in [\"cuda\", \"xpu\"]:\n@@ -984,7 +989,9 @@ class Bnb8bitCompile(unittest.TestCase):\n     def setUp(self):\n         # Models and tokenizer\n         self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n-        self.model_8bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True)\n+        self.model_8bit = AutoModelForCausalLM.from_pretrained(\n+            self.model_name, quantization_config=BitsAndBytesConfig(load_in_8bit=True)\n+        )\n \n     @pytest.mark.torch_compile_test\n     def test_generate_compile(self):"
        },
        {
            "sha": "4c056c0df2f09eb5a2c76371264775aa2383bb8b",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -34,6 +34,7 @@\n from transformers import (\n     AutoModel,\n     AutoModelForSequenceClassification,\n+    BitsAndBytesConfig,\n     PreTrainedConfig,\n     PreTrainedModel,\n     is_torch_available,\n@@ -3585,7 +3586,7 @@ def test_flash_attn_2_fp32_ln(self):\n                     tmpdirname,\n                     dtype=torch.float16,\n                     attn_implementation=\"flash_attention_2\",\n-                    load_in_4bit=True,\n+                    quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n                 )\n \n                 for _, param in model.named_parameters():"
        },
        {
            "sha": "88ff574ca5bb0a7d420a296588157c60e24b40e2",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b44d91570fb4a5cf8a58150c7935a4edde82e27f/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=b44d91570fb4a5cf8a58150c7935a4edde82e27f",
            "patch": "@@ -39,6 +39,7 @@\n     AutoImageProcessor,\n     AutoProcessor,\n     AutoTokenizer,\n+    BitsAndBytesConfig,\n     DataCollatorForLanguageModeling,\n     IntervalStrategy,\n     PreTrainedConfig,\n@@ -1486,7 +1487,8 @@ def test_bnb_compile(self):\n         # QLoRA + torch compile is not really supported yet, but we should at least support the model\n         # loading and let torch throw the\n         tiny_model = AutoModelForCausalLM.from_pretrained(\n-            \"hf-internal-testing/tiny-random-LlamaForCausalLM\", load_in_4bit=True\n+            \"hf-internal-testing/tiny-random-LlamaForCausalLM\",\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n         )\n \n         peft_config = LoraConfig("
        }
    ],
    "stats": {
        "total": 594,
        "additions": 349,
        "deletions": 245
    }
}