{
    "author": "Rocketknight1",
    "message": "Fix StopStringCriteria to handle tokens above len(tokenizer) (#35797)\n\n* Fix StopStringCriteria to handle tokens above len(tokenizer)\r\n\r\nThis fixes #35244 by clipping token IDs to be within the tokenizer's vocabulary size before performing the embedding lookup. This prevents index errors when model.config.vocab_size > len(tokenizer).\r\n\r\nThe fix:\r\n1. Adds a clamp operation to ensure token IDs are within bounds\r\n2. Adds a test case to verify the behavior\r\n\r\n* Use self.stop_strings instead of stop_strings\r\n\r\n* Handle clipping correctly\r\n\r\n* make fixup\r\n\r\n* Update test to the new embedding vecs\r\n\r\n* Use much bigger values in the mismatch test\r\n\r\n* Typo fix\r\n\r\n* Slight simplification\r\n\r\n---------\r\n\r\nCo-authored-by: openhands <openhands@all-hands.dev>",
    "sha": "4563ba2c6f88d77ef2e47d80b2ec6c82a3589d17",
    "files": [
        {
            "sha": "4627aeb9702707602d1f09e2f2121ee47338fe39",
            "filename": "src/transformers/generation/stopping_criteria.py",
            "status": "modified",
            "additions": 12,
            "deletions": 7,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/4563ba2c6f88d77ef2e47d80b2ec6c82a3589d17/src%2Ftransformers%2Fgeneration%2Fstopping_criteria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4563ba2c6f88d77ef2e47d80b2ec6c82a3589d17/src%2Ftransformers%2Fgeneration%2Fstopping_criteria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fstopping_criteria.py?ref=4563ba2c6f88d77ef2e47d80b2ec6c82a3589d17",
            "patch": "@@ -245,26 +245,26 @@ def __init__(self, tokenizer: PreTrainedTokenizerBase, stop_strings: Union[str,\n         vocab = tokenizer.get_vocab()\n         token_list, token_indices = tuple(vocab.keys()), tuple(vocab.values())\n         self.embedding_vec, self.max_valid_positions, self.max_valid_end_lens = self.clean_and_embed_tokens_with_cache(\n-            token_list, token_indices, self.stop_strings, tokenizer\n+            token_list, token_indices, tokenizer\n         )\n \n         self.maximum_token_len = max([len(stop_string) for stop_string in self.stop_strings])\n         self.num_stop_strings = len(self.stop_strings)\n         self.target_lens = torch.tensor([len(stop_string) for stop_string in stop_strings], dtype=torch.int32)\n \n-    def clean_and_embed_tokens_with_cache(self, token_list, token_indices, stop_strings, tokenizer):\n+    def clean_and_embed_tokens_with_cache(self, token_list, token_indices, tokenizer):\n         # We don't use the tokenizer in the cache key, because I don't trust it to have well-behaved equality\n-        if (token_list, token_indices, stop_strings) in STOP_STRING_EMBEDDING_CACHE:\n+        if (token_list, token_indices, self.stop_strings) in STOP_STRING_EMBEDDING_CACHE:\n             embedding_vec, max_valid_positions, max_valid_end_lens = STOP_STRING_EMBEDDING_CACHE[\n                 (token_list, token_indices, self.stop_strings)\n             ]\n-            STOP_STRING_EMBEDDING_CACHE.move_to_end((token_list, token_indices, stop_strings))\n+            STOP_STRING_EMBEDDING_CACHE.move_to_end((token_list, token_indices, self.stop_strings))\n         else:\n             clean_token_list, clean_token_indices = self.clean_tokenizer_vocab(tokenizer)\n             embedding_vec, max_valid_positions, max_valid_end_lens = self._stop_string_create_embedding_vec(\n-                clean_token_list, clean_token_indices, stop_strings\n+                clean_token_list, clean_token_indices, self.stop_strings\n             )\n-            STOP_STRING_EMBEDDING_CACHE[(token_list, token_indices, stop_strings)] = (\n+            STOP_STRING_EMBEDDING_CACHE[(token_list, token_indices, self.stop_strings)] = (\n                 embedding_vec,\n                 max_valid_positions,\n                 max_valid_end_lens,\n@@ -357,7 +357,9 @@ def _stop_string_create_embedding_vec(token_list, token_indices, stop_strings) -\n             )\n         max_valid_end_lens = max(valid_end_lens)\n         vec_size = len(stop_strings) * (max_valid_positions + max_valid_end_lens) + 1\n-        gather_vec = np.full((len(token_list), vec_size), dtype=np.int32, fill_value=-1)\n+        # We use +2 instead of +1 so we can have a dummy entry at the end. We will clamp all token values\n+        # over the max to this, ensuring they do not contribute to stop string matching.\n+        gather_vec = np.full((max(token_indices) + 2, vec_size), dtype=np.int32, fill_value=-1)\n \n         for i, stop_string in enumerate(stop_strings):\n             positions = token_valid_positions[stop_string]\n@@ -395,6 +397,9 @@ def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwa\n         # Flip input_ids because we're only matching strings at the end of the generated sequence\n         flipped_ids = torch.flip(input_ids, (1,))\n \n+        # Clip out-of-vocab values to the dummy value at the end of the embedding vector\n+        flipped_ids = torch.clamp(flipped_ids, max=self.embedding_vec.size(0) - 1)\n+\n         # Size of the vector of positions a single token can match\n         max_valid_positions = self.max_valid_positions\n "
        },
        {
            "sha": "ace7d496dab607e1190b7476395d17e027d57ae7",
            "filename": "tests/generation/test_stopping_criteria.py",
            "status": "modified",
            "additions": 15,
            "deletions": 3,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/4563ba2c6f88d77ef2e47d80b2ec6c82a3589d17/tests%2Fgeneration%2Ftest_stopping_criteria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4563ba2c6f88d77ef2e47d80b2ec6c82a3589d17/tests%2Fgeneration%2Ftest_stopping_criteria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_stopping_criteria.py?ref=4563ba2c6f88d77ef2e47d80b2ec6c82a3589d17",
            "patch": "@@ -176,6 +176,18 @@ def test_stop_string_criteria(self):\n         for i in range(len(false_strings)):\n             self.assertFalse(criteria(false_input_ids[\"input_ids\"][i : i + 1], scores))\n \n+    def test_stop_string_criteria_vocab_size_mismatch(self):\n+        \"\"\"Test that StopStringCriteria handles tokens above len(tokenizer) correctly.\"\"\"\n+        tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n+\n+        # Create input_ids with tokens above len(tokenizer)\n+        input_ids = torch.tensor([[len(tokenizer) + 1024, 1, 2]], device=torch_device)\n+        scores = None\n+        criteria = StopStringCriteria(tokenizer=tokenizer, stop_strings=[\"test\"])\n+\n+        # This should not raise an error and should return False since no stop string is matched\n+        self.assertFalse(criteria(input_ids, scores))\n+\n     def test_stop_string_matching_positions(self):\n         stop_string = \"stop\"\n         token_list = [\"last\", \"top\", \"topper\", \"s\", \"p\"]\n@@ -200,14 +212,14 @@ def test_stop_string_embedding_vecs(self):\n \n         # Positions inside the stop string where the token matches (excluding end overlaps)\n         valid_positions = embedding_vec[:, 0].tolist()\n-        self.assertEqual(valid_positions, [2, -1, -1, 3, -1])\n+        self.assertEqual(valid_positions, [2, -1, -1, 3, -1, -1])\n \n         # Overlap lengths between end of stop string and start of token\n         end_overlaps = embedding_vec[:, 1].tolist()\n-        self.assertEqual(end_overlaps, [-1, 3, 3, -1, 1])\n+        self.assertEqual(end_overlaps, [-1, 3, 3, -1, 1, -1])\n \n         # Length of each token\n-        token_lengths = embedding_vec[:, 2].tolist()\n+        token_lengths = embedding_vec[:-1, 2].tolist()\n         self.assertEqual(token_lengths, [len(token) for token in token_list])\n \n     def test_single_letter_stop_string(self):"
        }
    ],
    "stats": {
        "total": 37,
        "additions": 27,
        "deletions": 10
    }
}