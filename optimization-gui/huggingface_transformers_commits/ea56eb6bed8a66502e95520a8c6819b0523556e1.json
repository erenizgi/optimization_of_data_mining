{
    "author": "molbap",
    "message": "Fix important models CI (#39576)\n\n* relax test boundaries and fix from config\n\n* eager is always supported.",
    "sha": "ea56eb6bed8a66502e95520a8c6819b0523556e1",
    "files": [
        {
            "sha": "e4d7905678b80172f0ad1f97d8572c7c2dca872e",
            "filename": "tests/causal_lm_tester.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea56eb6bed8a66502e95520a8c6819b0523556e1/tests%2Fcausal_lm_tester.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea56eb6bed8a66502e95520a8c6819b0523556e1/tests%2Fcausal_lm_tester.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcausal_lm_tester.py?ref=ea56eb6bed8a66502e95520a8c6819b0523556e1",
            "patch": "@@ -451,5 +451,4 @@ def test_flash_attn_2_equivalence(self):\n \n                 logits = outputs.hidden_states[-1]\n                 logits_fa = outputs_fa.hidden_states[-1]\n-\n-                assert torch.allclose(logits_fa, logits, atol=2e-3)\n+                torch.testing.assert_close(logits_fa, logits, atol=3e-2, rtol=3e-2)"
        },
        {
            "sha": "0f94837b7832656c3ae8e02839767454240c4e6b",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea56eb6bed8a66502e95520a8c6819b0523556e1/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea56eb6bed8a66502e95520a8c6819b0523556e1/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=ea56eb6bed8a66502e95520a8c6819b0523556e1",
            "patch": "@@ -2309,7 +2309,7 @@ def _test_attention_implementation(self, attn_implementation):\n \n         set_model_tester_for_less_flaky_test(self)\n         for model_class in self.all_generative_model_classes:\n-            if not getattr(model_class, support_flag[attn_implementation]):\n+            if attn_implementation != \"eager\" and not getattr(model_class, support_flag[attn_implementation]):\n                 self.skipTest(f\"{model_class.__name__} does not support `attn_implementation={attn_implementation}`\")\n \n             config, original_inputs_dict = self.prepare_config_and_inputs_for_generate()"
        },
        {
            "sha": "b5c92267edc72d9dcc1ff7dd57507eef0c05d963",
            "filename": "tests/models/kyutai_speech_to_text/test_modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea56eb6bed8a66502e95520a8c6819b0523556e1/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea56eb6bed8a66502e95520a8c6819b0523556e1/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py?ref=ea56eb6bed8a66502e95520a8c6819b0523556e1",
            "patch": "@@ -552,7 +552,7 @@ def _test_attention_implementation(self, attn_implementation):\n         }\n \n         for model_class in self.all_generative_model_classes:\n-            if not getattr(model_class, support_flag[attn_implementation]):\n+            if attn_implementation != \"eager\" and not getattr(model_class, support_flag[attn_implementation]):\n                 self.skipTest(f\"{model_class.__name__} does not support `attn_implementation={attn_implementation}`\")\n \n             config, original_inputs_dict = self.prepare_config_and_inputs_for_generate()"
        },
        {
            "sha": "c60cd1eb66b2c60d340c453e5053878fbcd50d96",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea56eb6bed8a66502e95520a8c6819b0523556e1/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea56eb6bed8a66502e95520a8c6819b0523556e1/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=ea56eb6bed8a66502e95520a8c6819b0523556e1",
            "patch": "@@ -4107,7 +4107,9 @@ def test_flash_attn_2_can_compile_with_attention_mask_None_without_graph_break(s\n \n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n         cls = self._torch_compile_train_cls  # e.g. LlamaFroCausalLM\n-        model = cls(config, attn_implementation=\"flash_attention_2\").to(device=torch_device, dtype=torch_dtype)\n+        model = cls._from_config(config, attn_implementation=\"flash_attention_2\").to(\n+            device=torch_device, dtype=torch_dtype\n+        )\n \n         inputs = {\n             \"input_ids\": torch.randint(low=1, high=model.config.vocab_size, size=(2, 10), device=torch_device),\n@@ -4137,7 +4139,7 @@ def flash_attention_padding_matches_padding_free_with_position_ids(\n         }\n \n         for model_class in self.all_generative_model_classes:\n-            if not getattr(model_class, support_flag[attn_implementation]):\n+            if attn_implementation != \"eager\" and not getattr(model_class, support_flag[attn_implementation]):\n                 self.skipTest(f\"{model_class.__name__} does not support {attn_implementation}\")\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        }
    ],
    "stats": {
        "total": 13,
        "additions": 7,
        "deletions": 6
    }
}