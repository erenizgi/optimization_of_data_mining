{
    "author": "cyyever",
    "message": "Fix deprecated PT functions (#37237)\n\n* Fix deprecated PT functions\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n* Revert some changes\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: cyy <cyyever@outlook.com>",
    "sha": "edd345b52e54003bb5b0baf85f4f8f4e98089365",
    "files": [
        {
            "sha": "a01b33703e2bfd04f87b79860dee328434b84a72",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/edd345b52e54003bb5b0baf85f4f8f4e98089365/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/edd345b52e54003bb5b0baf85f4f8f4e98089365/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=edd345b52e54003bb5b0baf85f4f8f4e98089365",
            "patch": "@@ -2724,7 +2724,7 @@ def test_cache_dependant_input_preparation_exporting(self):\n         # Case 1\n         input_ids = torch.randint(0, 16, (2, 8), dtype=torch.int64)[:, :0]\n         inputs_embeds = torch.rand((2, 8), dtype=torch.float32)\n-        cache_position = torch.range(0, 7, dtype=torch.int64)\n+        cache_position = torch.arange(0, 8, dtype=torch.int64)\n         eager1, eager2 = GenerationMixin()._cache_dependant_input_preparation(input_ids, inputs_embeds, cache_position)\n         export1, export2 = GenerationMixin()._cache_dependant_input_preparation_exporting(\n             input_ids, inputs_embeds, cache_position\n@@ -2735,7 +2735,7 @@ def test_cache_dependant_input_preparation_exporting(self):\n         # Case 2\n         input_ids = torch.randint(0, 16, (2, 8), dtype=torch.int64)\n         inputs_embeds = torch.rand((2, 8), dtype=torch.float32)\n-        cache_position = torch.range(0, 7, dtype=torch.int64)\n+        cache_position = torch.arange(0, 8, dtype=torch.int64)\n         eager1, eager2 = GenerationMixin()._cache_dependant_input_preparation(input_ids, inputs_embeds, cache_position)\n         export1, export2 = GenerationMixin()._cache_dependant_input_preparation_exporting(\n             input_ids, inputs_embeds, cache_position\n@@ -2746,7 +2746,7 @@ def test_cache_dependant_input_preparation_exporting(self):\n         # Case 3\n         input_ids = torch.randint(0, 16, (2, 12), dtype=torch.int64)\n         inputs_embeds = None\n-        cache_position = torch.range(0, 7, dtype=torch.int64)\n+        cache_position = torch.arange(0, 8, dtype=torch.int64)\n         eager1, eager2 = GenerationMixin()._cache_dependant_input_preparation(input_ids, inputs_embeds, cache_position)\n         export1, export2 = GenerationMixin()._cache_dependant_input_preparation_exporting(\n             input_ids, inputs_embeds, cache_position\n@@ -2757,7 +2757,7 @@ def test_cache_dependant_input_preparation_exporting(self):\n         # Case 4\n         input_ids = torch.randint(0, 16, (2, 8), dtype=torch.int64)\n         inputs_embeds = None\n-        cache_position = torch.range(0, 7, dtype=torch.int64)\n+        cache_position = torch.arange(0, 8, dtype=torch.int64)\n         eager1, eager2 = GenerationMixin()._cache_dependant_input_preparation(input_ids, inputs_embeds, cache_position)\n         export1, export2 = GenerationMixin()._cache_dependant_input_preparation_exporting(\n             input_ids, inputs_embeds, cache_position"
        },
        {
            "sha": "4e372a5fd93e398e702d839b254de0dfa07a1233",
            "filename": "tests/models/falcon_mamba/test_modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/edd345b52e54003bb5b0baf85f4f8f4e98089365/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/edd345b52e54003bb5b0baf85f4f8f4e98089365/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py?ref=edd345b52e54003bb5b0baf85f4f8f4e98089365",
            "patch": "@@ -231,7 +231,7 @@ def create_and_check_falcon_mamba_cached_slow_forward_and_backwards(\n             token_emb, cache, cache_position=torch.arange(0, config.conv_kernel, device=input_ids.device)\n         )\n \n-        loss = torch.log(1 + torch.abs(outputs.sum()))\n+        loss = torch.log1p(torch.abs(outputs.sum()))\n         self.parent.assertEqual(loss.shape, ())\n         self.parent.assertEqual(outputs.shape, (self.batch_size, self.seq_length, self.hidden_size))\n         loss.backward()"
        },
        {
            "sha": "02ecbbfd3cbe336725ca6571345ea775f90715f4",
            "filename": "tests/models/mamba/test_modeling_mamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/edd345b52e54003bb5b0baf85f4f8f4e98089365/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/edd345b52e54003bb5b0baf85f4f8f4e98089365/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py?ref=edd345b52e54003bb5b0baf85f4f8f4e98089365",
            "patch": "@@ -205,7 +205,7 @@ def create_and_check_mamba_cached_slow_forward_and_backwards(\n             token_emb, cache, cache_position=torch.arange(0, config.conv_kernel, device=input_ids.device)\n         )\n \n-        loss = torch.log(1 + torch.abs(outputs.sum()))\n+        loss = torch.log1p(torch.abs(outputs.sum()))\n         self.parent.assertEqual(loss.shape, ())\n         self.parent.assertEqual(outputs.shape, (self.batch_size, self.seq_length, self.hidden_size))\n         loss.backward()"
        }
    ],
    "stats": {
        "total": 12,
        "additions": 6,
        "deletions": 6
    }
}