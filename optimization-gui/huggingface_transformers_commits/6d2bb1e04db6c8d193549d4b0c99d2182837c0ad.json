{
    "author": "kashif",
    "message": "[Trainer] accelerate contextparallel support in trainer (#40205)\n\n* initial context_parallel_size support in trainer\n\n* For context parallelism, use AVG instead of SUM to avoid over-accounting tokens\n\n* use parallelism_config.cp_enabled\n\n* add parallelism_config to trainer state\n\n* warn when auto-enabling FSDP\n\n* fix some reviews\n\n* WIP: somewhat matching loss\n\n* Feat: add back nested_gather\n\n* Feat: cleanup\n\n* Fix: raise on non-sdpa attn\n\n* remove context_parallel_size from TrainingArguments\n\n* if we have parallelism_config, we defer to get_state_dict from accelerate\n\n* fix form review\n\n* Feat: add parallelism config support\n\n* Chore: revert some unwanted formatting changes\n\n* Fix: check None\n\n* Check none 2\n\n* Fix: remove duplicate import\n\n* Update src/transformers/trainer.py\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n* Update src/transformers/training_args.py\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n* Fin\n\n* require accerelate 1.10.1 and higer\n\n---------\n\nCo-authored-by: S1ro1 <matej.sirovatka@gmail.com>\nCo-authored-by: Matej Sirovatka <54212263+S1ro1@users.noreply.github.com>\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "6d2bb1e04db6c8d193549d4b0c99d2182837c0ad",
    "files": [
        {
            "sha": "bf05563e9b36362b987f3e24dff60fe7b8333a76",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 188,
            "deletions": 50,
            "changes": 238,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d2bb1e04db6c8d193549d4b0c99d2182837c0ad/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d2bb1e04db6c8d193549d4b0c99d2182837c0ad/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=6d2bb1e04db6c8d193549d4b0c99d2182837c0ad",
            "patch": "@@ -3823,6 +3823,123 @@ def _prepare_inputs(self, inputs: dict[str, Union[torch.Tensor, Any]]) -> dict[s\n \n         return inputs\n \n+    def _is_attention_mask_causal(self, attention_mask):\n+        \"\"\"\n+        Check if an attention mask is causal (compatible with causal attention).\n+        Context parallelism only supports causal attention patterns. This function\n+        checks if the provided attention mask is compatible.\n+\n+        Args:\n+            attention_mask (torch.Tensor): The attention mask to check\n+\n+        Returns:\n+            bool: True if the mask is causal or compatible with causal attention\n+        \"\"\"\n+        if attention_mask is None:\n+            return True  # No mask is considered causal (model uses default causal masking)\n+\n+        # Handle different mask dimensions\n+        if attention_mask.dim() == 2:\n+            # (batch_size, seq_len) - standard padding mask, compatible with causal attention\n+            return True\n+        elif attention_mask.dim() in [3, 4]:\n+            # (batch_size, seq_len, seq_len) or (batch_size, num_heads, seq_len, seq_len)\n+            # Check if it's lower triangular (causal)\n+            seq_len = attention_mask.shape[-1]\n+            if seq_len <= 1:\n+                return True  # Single token or empty is always causal\n+\n+            # Take first batch and head (if 4D) for checking pattern\n+            if attention_mask.dim() == 4:\n+                mask = attention_mask[0, 0]  # First batch, first head\n+            else:\n+                mask = attention_mask[0]  # First batch\n+\n+            # Check if upper triangular part is masked (should be 0 or very negative for causal)\n+            upper_triangular = torch.triu(mask, diagonal=1)\n+\n+            # For causal masks, upper triangular should be 0 or very negative (like -inf)\n+            # Use a reasonable threshold to handle float precision issues\n+            is_causal = torch.all(upper_triangular <= 1e-6) or torch.all(upper_triangular < -1e4)\n+            return is_causal.item() if isinstance(is_causal, torch.Tensor) else is_causal\n+\n+        # For unknown dimensions, be conservative and reject\n+        return False\n+\n+    def _prepare_context_parallel_inputs(self, model, inputs: dict[str, Union[torch.Tensor, Any]]):\n+        \"\"\"\n+        Prepare inputs for context parallelism by setting up buffers and validation.\n+\n+        Args:\n+            model: The model being trained\n+            inputs: Input tensors to prepare\n+\n+        Returns:\n+            tuple: (context_manager, prepared_inputs) where context_manager is either\n+                   the context parallelism wrapper or a no-op context\n+        \"\"\"\n+        if (\n+            getattr(self.accelerator, \"parallelism_config\", None) is not None\n+            and self.accelerator.parallelism_config.cp_enabled\n+        ):\n+            if hasattr(model, \"config\"):\n+                if model.config._attn_implementation != \"sdpa\":\n+                    raise ValueError(\n+                        f\"Context parallelism is supported only with SDPA attention, you are using {model.config._attn_implementation}.\"\n+                    )\n+\n+            if \"position_ids\" not in inputs:\n+                logger.warning_once(\"Position IDs not found in the inputs, generating manually\")\n+                inputs[\"position_ids\"] = torch.arange(\n+                    inputs[\"input_ids\"].size(1), device=inputs[\"input_ids\"].device\n+                ).expand(inputs[\"input_ids\"].size(0), -1)\n+            if \"shift_labels\" not in inputs:\n+                logger.warning_once(\"Shift labels not found in the inputs, shifting manually\")\n+                if \"labels\" in inputs:\n+                    _ignore_index = -100\n+                    labels = nn.functional.pad(inputs[\"labels\"], (0, 1), value=_ignore_index)\n+                    inputs[\"shift_labels\"] = labels[:, 1:].contiguous()\n+\n+            buffers = []\n+            buffer_seq_dims = []\n+\n+            if \"input_ids\" in inputs:\n+                buffers.append(inputs[\"input_ids\"])\n+                buffer_seq_dims.append(1)  # Sequence dimension\n+            if \"labels\" in inputs:\n+                buffers.append(inputs[\"labels\"])\n+                buffer_seq_dims.append(1)\n+            if \"shift_labels\" in inputs:\n+                buffers.append(inputs[\"shift_labels\"])\n+                buffer_seq_dims.append(1)\n+            if \"attention_mask\" in inputs and not getattr(self, \"_attn_mask_causal_checked\", False):\n+                # Context parallel currently doesn't support other masks than causal\n+                # Accelerate applies hooks to replace mask with is_causal arg in SDPA\n+                # Check if the mask is really causal and if not throw an error\n+                # TODO: check this only once or always, with speed being the cost\n+                attention_mask = inputs[\"attention_mask\"]\n+                if not self._is_attention_mask_causal(attention_mask):\n+                    raise ValueError(\n+                        \"Context parallelism only supports causal attention masks. \"\n+                        \"The provided attention_mask is not causal. \"\n+                        \"Please ensure your data uses causal masking (lower triangular) \"\n+                        \"or remove the attention_mask to use the model's default causal masking.\"\n+                    )\n+                self._attn_mask_causal_checked = True\n+            # Include position_ids in context parallelism splitting\n+            if \"position_ids\" in inputs and inputs[\"position_ids\"] is not None:\n+                buffers.append(inputs[\"position_ids\"])\n+                buffer_seq_dims.append(1)\n+\n+            return partial(\n+                self.accelerator.maybe_context_parallel,\n+                buffers=buffers,\n+                buffer_seq_dims=buffer_seq_dims,\n+                no_restore_buffers=set(buffers),\n+            ), inputs\n+\n+        return contextlib.nullcontext, inputs\n+\n     def compute_loss_context_manager(self):\n         \"\"\"\n         A helper wrapper to group together context managers.\n@@ -3873,66 +3990,74 @@ def training_step(\n         Return:\n             `torch.Tensor`: The tensor with training loss on this batch.\n         \"\"\"\n-        model.train()\n-        if hasattr(self.optimizer, \"train\") and callable(self.optimizer.train):\n-            self.optimizer.train()\n+        # Prepare buffers for context parallelism\n \n-        inputs = self._prepare_inputs(inputs)\n-        if is_sagemaker_mp_enabled():\n-            loss_mb = smp_forward_backward(model, inputs, self.args.gradient_accumulation_steps)\n-            return loss_mb.reduce_mean().detach().to(self.args.device)\n+        cp_context, inputs = self._prepare_context_parallel_inputs(model, inputs)\n \n-        with self.compute_loss_context_manager():\n-            loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n+        # Context manager is no-op if CP isn't enabled\n+        with cp_context():\n+            model.train()\n+            if hasattr(self.optimizer, \"train\") and callable(self.optimizer.train):\n+                self.optimizer.train()\n \n-        del inputs\n-        if (\n-            self.args.torch_empty_cache_steps is not None\n-            and self.state.global_step % self.args.torch_empty_cache_steps == 0\n-        ):\n-            if is_torch_xpu_available():\n-                torch.xpu.empty_cache()\n-            elif is_torch_mlu_available():\n-                torch.mlu.empty_cache()\n-            elif is_torch_musa_available():\n-                torch.musa.empty_cache()\n-            elif is_torch_npu_available():\n-                torch.npu.empty_cache()\n-            elif is_torch_mps_available():\n-                torch.mps.empty_cache()\n-            elif is_torch_hpu_available():\n-                logger.warning(\n-                    \"`torch_empty_cache_steps` is set but HPU device/backend does not support empty_cache().\"\n-                )\n-            else:\n-                torch.cuda.empty_cache()\n+            inputs = self._prepare_inputs(inputs)\n+            if is_sagemaker_mp_enabled():\n+                loss_mb = smp_forward_backward(model, inputs, self.args.gradient_accumulation_steps)\n+                return loss_mb.reduce_mean().detach().to(self.args.device)\n \n-        kwargs = {}\n+            with self.compute_loss_context_manager():\n+                loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n \n-        # For LOMO optimizers you need to explicitly use the learning rate\n-        if self.args.optim in [OptimizerNames.LOMO, OptimizerNames.ADALOMO]:\n-            kwargs[\"learning_rate\"] = self._get_learning_rate()\n+            del inputs\n+            if (\n+                self.args.torch_empty_cache_steps is not None\n+                and self.state.global_step % self.args.torch_empty_cache_steps == 0\n+            ):\n+                if is_torch_xpu_available():\n+                    torch.xpu.empty_cache()\n+                elif is_torch_mlu_available():\n+                    torch.mlu.empty_cache()\n+                elif is_torch_musa_available():\n+                    torch.musa.empty_cache()\n+                elif is_torch_npu_available():\n+                    torch.npu.empty_cache()\n+                elif is_torch_mps_available():\n+                    torch.mps.empty_cache()\n+                elif is_torch_hpu_available():\n+                    logger.warning(\n+                        \"`torch_empty_cache_steps` is set but HPU device/backend does not support empty_cache().\"\n+                    )\n+                else:\n+                    torch.cuda.empty_cache()\n \n-        if self.args.n_gpu > 1:\n-            loss = loss.mean()  # mean() to average on multi-gpu parallel training\n+            kwargs = {}\n \n-        if self.use_apex:\n-            from apex import amp\n+            # For LOMO optimizers you need to explicitly use the learning rate\n+            if self.args.optim in [OptimizerNames.LOMO, OptimizerNames.ADALOMO]:\n+                kwargs[\"learning_rate\"] = self._get_learning_rate()\n \n-            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n-                scaled_loss.backward()\n-        else:\n-            # Finally we need to normalize the loss for reporting if GA loss bug is not fixed during compute loss\n-            if (not self.model_accepts_loss_kwargs or num_items_in_batch is None) and self.compute_loss_func is None:\n-                # If the model does not accept loss kwargs, we need to normalize the loss by the number of gradient accumulation steps\n-                loss = loss / self.current_gradient_accumulation_steps\n+            if self.args.n_gpu > 1:\n+                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n+\n+            if self.use_apex:\n+                from apex import amp\n+\n+                with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n+                    scaled_loss.backward()\n+            else:\n+                # Finally we need to normalize the loss for reporting if GA loss bug is not fixed during compute loss\n+                if (\n+                    not self.model_accepts_loss_kwargs or num_items_in_batch is None\n+                ) and self.compute_loss_func is None:\n+                    # If the model does not accept loss kwargs, we need to normalize the loss by the number of gradient accumulation steps\n+                    loss = loss / self.current_gradient_accumulation_steps\n \n-            # Turning off loss scaling w.r.t. gradient accumulation when DeepSpeed is enabled\n-            # https://github.com/huggingface/transformers/pull/35808\n-            if self.accelerator.distributed_type == DistributedType.DEEPSPEED:\n-                kwargs[\"scale_wrt_gas\"] = False\n+                # Turning off loss scaling w.r.t. gradient accumulation when DeepSpeed is enabled\n+                # https://github.com/huggingface/transformers/pull/35808\n+                if self.accelerator.distributed_type == DistributedType.DEEPSPEED:\n+                    kwargs[\"scale_wrt_gas\"] = False\n \n-            self.accelerator.backward(loss, **kwargs)\n+                self.accelerator.backward(loss, **kwargs)\n \n             return loss.detach()\n \n@@ -5337,6 +5462,16 @@ def create_accelerator_and_postprocess(self):\n         args = {\n             \"deepspeed_plugin\": self.args.deepspeed_plugin,\n         }\n+\n+        # We defer compatibility checks to accelerator\n+        if self.args.parallelism_config is not None:\n+            if not is_accelerate_available(\"1.10.1\"):\n+                raise ImportError(\n+                    \"ParallelismConfig requires accelerate v1.10.1 and above. Please upgrade accelerate to use this feature.\"\n+                )\n+\n+            args[\"parallelism_config\"] = self.args.parallelism_config\n+\n         if is_accelerate_available(\"0.28.0\"):\n             args[\"dataloader_config\"] = dataloader_config\n         else:\n@@ -5479,6 +5614,9 @@ def get_batch_samples(\n                 if self.args.n_gpu > 1 and num_items_in_batch.dim() == 0:\n                     # In the DataParallel case, convert the scalar tensor into a 1-dim tensor\n                     num_items_in_batch = num_items_in_batch.unsqueeze(0)\n+                # Divide by number of devices with the same batch\n+                if pc := self.accelerator.parallelism_config:\n+                    num_items_in_batch = num_items_in_batch // pc.non_data_parallel_size\n \n         return batch_samples, num_items_in_batch\n "
        },
        {
            "sha": "2337edc93b33f51932781f875837b00d40758894",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 12,
            "deletions": 1,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d2bb1e04db6c8d193549d4b0c99d2182837c0ad/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d2bb1e04db6c8d193549d4b0c99d2182837c0ad/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=6d2bb1e04db6c8d193549d4b0c99d2182837c0ad",
            "patch": "@@ -77,6 +77,9 @@\n \n     from .trainer_pt_utils import AcceleratorConfig\n \n+    if is_accelerate_available(\"1.10.1\"):\n+        from accelerate.parallelism_config import ParallelismConfig\n+\n if is_torch_xla_available():\n     import torch_xla.core.xla_model as xm\n \n@@ -597,7 +600,8 @@ class TrainingArguments:\n                     Whether or not to use a pre-configured `AcceleratorState` or `PartialState` defined before calling `TrainingArguments`.\n                     If `True`, an `Accelerator` or `PartialState` must be initialized. Note that by doing so, this could lead to issues\n                     with hyperparameter tuning.\n-\n+        parallelism_config (`ParallelismConfig`, *optional*):\n+            Parallelism configuration for the training run. Requires Accelerate `1.10.1`\n         label_smoothing_factor (`float`, *optional*, defaults to 0.0):\n             The label smoothing factor to use. Zero means no label smoothing, otherwise the underlying onehot-encoded\n             labels are changed from 0s and 1s to `label_smoothing_factor/num_labels` and `1 - label_smoothing_factor +\n@@ -1272,6 +1276,10 @@ class TrainingArguments:\n             )\n         },\n     )\n+    parallelism_config: Optional[\"ParallelismConfig\"] = field(\n+        default=None,\n+        metadata={\"help\": (\"Parallelism configuration for the training run. Requires Accelerate `1.10.1`\")},\n+    )\n     deepspeed: Optional[Union[dict, str]] = field(\n         default=None,\n         metadata={\n@@ -2561,6 +2569,9 @@ def to_dict(self):\n                 quantization_config = v.get(\"quantization_config\")\n                 if quantization_config and not isinstance(quantization_config, dict):\n                     d[k][\"quantization_config\"] = quantization_config.to_dict()\n+            if k == \"parallelism_config\" and v is not None:\n+                d[k] = v.to_json()\n+\n         self._dict_dtype_to_str(d)\n \n         return d"
        }
    ],
    "stats": {
        "total": 251,
        "additions": 200,
        "deletions": 51
    }
}