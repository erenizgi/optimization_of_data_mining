{
    "author": "ArthurZucker",
    "message": "Fix multi-gpu loss (#35395)\n\npush to device",
    "sha": "8cad65a698556023b8ea4f4b1bb87d54f50c1b3b",
    "files": [
        {
            "sha": "2e8e2bb5f149d735da2c363d487e710351291e86",
            "filename": "src/transformers/loss/loss_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8cad65a698556023b8ea4f4b1bb87d54f50c1b3b/src%2Ftransformers%2Floss%2Floss_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8cad65a698556023b8ea4f4b1bb87d54f50c1b3b/src%2Ftransformers%2Floss%2Floss_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_utils.py?ref=8cad65a698556023b8ea4f4b1bb87d54f50c1b3b",
            "patch": "@@ -34,6 +34,7 @@ def ForCausalLMLoss(\n ):\n     # Upcast to float if we need to compute the loss to avoid potential precision issues\n     logits = logits.float()\n+    labels = labels.to(logits.device)\n     # Shift so that tokens < n predict n\n     shift_logits = logits[..., :-1, :].contiguous()\n     shift_labels = labels[..., 1:].contiguous()\n@@ -52,6 +53,7 @@ def ForMaskedLMLoss(\n ):\n     # Upcast to float if we need to compute the loss to avoid potential precision issues\n     logits = logits.float()\n+    labels = labels.to(logits.device)\n \n     # Flatten the tokens\n     logits = logits.view(-1, vocab_size)\n@@ -73,6 +75,7 @@ def ForSequenceClassificationLoss(labels, pooled_logits, config, **kwargs):\n         else:\n             config.problem_type = \"multi_label_classification\"\n \n+    labels = labels.to(pooled_logits.device)\n     if config.problem_type == \"regression\":\n         loss_fct = MSELoss()\n         if num_labels == 1:\n@@ -109,7 +112,7 @@ def ForQuestionAnsweringLoss(start_logits, end_logits, start_positions, end_posi\n def ForTokenClassification(logits, labels, config, **kwargs):\n     # Upcast to float if we need to compute the loss to avoid potential precision issues\n     logits = logits.view(-1, config.num_labels)\n-    labels = labels.view(-1)\n+    labels = labels.view(-1).to(logits.device)\n     logits = logits.float()\n     # Flatten the tokens\n     return fixed_cross_entropy(logits, labels, **kwargs)"
        }
    ],
    "stats": {
        "total": 5,
        "additions": 4,
        "deletions": 1
    }
}