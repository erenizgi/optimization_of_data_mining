{
    "author": "zucchini-nlp",
    "message": "Move `original_max_position_embeddings` to rope params  (#42513)\n\n* move `original_max_position_embeddings` to rope param dict and resolve TODs from Joao\n\n* bring back truncate in yarn\n\n* move the patch to `standardize` helper, this one gets called every time we init rope comute fn\n\n* my bad\n\n* silly typo, I should read the code I write!\n\n* force the tester to use specific layer types, because rope is built with these types\n\n* revert, whyhow did it get deleted?!\n\n* factor isn't guaranteed to exist in the end\n\n* tiny test issue, needs to standardize first",
    "sha": "347328108aa79959eda0e2eae0695c12f0101046",
    "files": [
        {
            "sha": "ec6e748994cfacf70e0b8c1e166ab34802185ffb",
            "filename": "src/transformers/modeling_rope_utils.py",
            "status": "modified",
            "additions": 75,
            "deletions": 78,
            "changes": 153,
            "blob_url": "https://github.com/huggingface/transformers/blob/347328108aa79959eda0e2eae0695c12f0101046/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/347328108aa79959eda0e2eae0695c12f0101046/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_rope_utils.py?ref=347328108aa79959eda0e2eae0695c12f0101046",
            "patch": "@@ -46,17 +46,19 @@ def dynamic_rope_update(rope_forward):\n     def longrope_frequency_update(self, position_ids, device, layer_type=None):\n         \"\"\"Longrope uses long factor if sequence is larger than original pretraining length, short otherwise.\"\"\"\n         seq_len = torch.max(position_ids) + 1\n-        original_max_position_embeddings = getattr(\n-            self.config, \"original_max_position_embeddings\", self.config.max_position_embeddings\n-        )\n+\n         if layer_type is None:\n             rope_type = self.rope_type\n             original_inv_freq = self.original_inv_freq\n             prefix = \"\"\n+            original_max_position_embeddings = self.config.rope_parameters[\"original_max_position_embeddings\"]\n         else:\n             rope_type = self.rope_type[layer_type]\n             original_inv_freq = getattr(self, f\"{layer_type}_original_inv_freq\")\n             prefix = f\"{layer_type}_\"\n+            original_max_position_embeddings = self.config.rope_parameters[layer_type][\n+                \"original_max_position_embeddings\"\n+            ]\n \n         if seq_len > original_max_position_embeddings:\n             if not hasattr(self, f\"{layer_type}_long_inv_freq\"):\n@@ -223,7 +225,6 @@ def _compute_dynamic_ntk_parameters(\n         Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n         post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n     \"\"\"\n-    # TODO (joao): use the new `original_max_position_embeddings` from rope_parameters\n     # For backward compatibility standardize the `rope_parameters_dict` if it uses old format\n     config.standardize_rope_params()\n     rope_parameters_dict = config.rope_parameters[layer_type] if layer_type is not None else config.rope_parameters\n@@ -232,23 +233,22 @@ def _compute_dynamic_ntk_parameters(\n     partial_rotary_factor = rope_parameters_dict.get(\"partial_rotary_factor\", 1.0)\n     head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n     dim = int(head_dim * partial_rotary_factor)\n-    max_position_embeddings = config.max_position_embeddings\n     factor = rope_parameters_dict[\"factor\"]\n     attention_factor = 1.0  # Unused in this type of RoPE\n \n     # seq_len: default to max_position_embeddings, e.g. at init time\n     if seq_len is None:\n-        seq_len = max_position_embeddings\n+        seq_len = config.max_position_embeddings\n     elif isinstance(seq_len, torch.Tensor):\n         seq_len = torch.maximum(\n             seq_len,\n-            torch.tensor(max_position_embeddings, dtype=seq_len.dtype, device=seq_len.device),\n+            torch.tensor(config.max_position_embeddings, dtype=seq_len.dtype, device=seq_len.device),\n         )\n     else:\n-        seq_len = max(seq_len, max_position_embeddings)\n+        seq_len = max(seq_len, config.max_position_embeddings)\n \n     # Compute the inverse frequencies\n-    base = base * ((factor * seq_len / max_position_embeddings) - (factor - 1)) ** (dim / (dim - 2))\n+    base = base * ((factor * seq_len / config.max_position_embeddings) - (factor - 1)) ** (dim / (dim - 2))\n     inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim))\n     return inv_freq, attention_factor\n \n@@ -292,8 +292,7 @@ def _compute_yarn_parameters(\n                     `mscale_all_dim` are provided, `mscale_all_dim` acts scalar augmenting `log(factor)` when computing\n                     the denominator for the inferred value of `attention_factor`. If not provided, `attention_factor`\n                     will be calculated based on `factor` only.\n-                *   `original_max_position_embeddings` (`int`, *optional*): The original max position embeddings used\n-                    during pretraining. If not provided, the function falls back to `max_position_embeddings`.\n+                *   `original_max_position_embeddings` (`int`): The original max position embeddings used during pretraining.\n                 *   `truncate` (`bool`, *optional*): Whether to truncate the correction range.\n \n             Additionally, this function will make use of the following properties if they are found in the config:\n@@ -324,15 +323,13 @@ def _compute_yarn_parameters(\n     attention_factor = rope_parameters_dict.get(\"attention_factor\")\n     mscale = rope_parameters_dict.get(\"mscale\")\n     mscale_all_dim = rope_parameters_dict.get(\"mscale_all_dim\")\n+    original_max_position_embeddings = rope_parameters_dict[\"original_max_position_embeddings\"]\n \n-    # NOTE: DeekSeek-V3 (and potentially other models) modify `max_position_embeddings` and have a\n-    # `original_max_position_embeddings` field containing the pretrained value. They use the ratio between these two\n-    # values to compute the default attention scaling factor, instead of using `factor`.\n-    if \"original_max_position_embeddings\" in rope_parameters_dict:\n-        original_max_position_embeddings = rope_parameters_dict[\"original_max_position_embeddings\"]\n+    # NOTE: DeekSeek-V3 (and potentially other models) have `original_max_position_embeddings` field\n+    # containing the pretrained value. They use the ratio between `max_position_embeddings` and this value\n+    # to compute the default attention scaling factor, instead of using `factor`.\n+    if factor is None:\n         factor = config.max_position_embeddings / original_max_position_embeddings\n-    else:\n-        original_max_position_embeddings = config.max_position_embeddings\n \n     def get_mscale(scale, mscale=1):\n         if scale <= 1:\n@@ -440,7 +437,6 @@ def _compute_longrope_parameters(\n         Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n         post-processing scaling factor applied to the computed cos/sin.\n     \"\"\"\n-    # TODO (joao): use the new `original_max_position_embeddings` from rope_parameters\n     # For backward compatibility standardize the `rope_parameters_dict` if it uses old format\n     config.standardize_rope_params()\n     rope_parameters_dict = config.rope_parameters[layer_type] if layer_type is not None else config.rope_parameters\n@@ -454,14 +450,13 @@ def _compute_longrope_parameters(\n     short_factor = rope_parameters_dict[\"short_factor\"]\n     factor = rope_parameters_dict.get(\"factor\")\n     attention_factor = rope_parameters_dict.get(\"attention_factor\")\n+    original_max_position_embeddings = rope_parameters_dict[\"original_max_position_embeddings\"]\n \n     # NOTE: Phi3 (and potentially other models) modify `max_position_embeddings` and have a\n     # `original_max_position_embeddings` field containing the pretrained value. They use the ratio between these two\n     # values to compute the default attention scaling factor, instead of using `factor`.\n-    if original_max_position_embeddings := getattr(config, \"original_max_position_embeddings\", None):\n+    if factor is None:\n         factor = config.max_position_embeddings / original_max_position_embeddings\n-    else:\n-        original_max_position_embeddings = config.max_position_embeddings\n \n     # Sets the attention factor as suggested in the paper\n     if attention_factor is None:\n@@ -587,7 +582,7 @@ class RopeParameters(TypedDict, total=False):\n             most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n             original maximum pre-trained length.\n         original_max_position_embeddings (`int`, *optional*):\n-            Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+            Used with 'yarn', 'longrope' and 'llama3'. The original max position embeddings used during\n             pretraining.\n         attention_factor (`float`, *optional*):\n             Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n@@ -641,6 +636,7 @@ def convert_rope_params_to_dict(self, ignore_keys_at_rope_validation: Optional[s\n \n         # Standardize and validate the correctness of rotary position embeddings parameters\n         self.rope_parameters.setdefault(\"rope_theta\", kwargs.pop(\"rope_theta\", self.default_theta))\n+\n         if \"partial_rotary_factor\" in kwargs:\n             self.rope_parameters.setdefault(\"partial_rotary_factor\", kwargs[\"partial_rotary_factor\"])\n             ignore_keys_at_rope_validation = {\"partial_rotary_factor\"}\n@@ -671,14 +667,30 @@ def standardize_rope_params(self):\n             rope_parameters.setdefault(\"rope_theta\", rope_theta)\n             if partial_rotary_factor is not None:\n                 rope_parameters[\"partial_rotary_factor\"] = partial_rotary_factor\n+\n+            # Move pretraining-time maximum length to rope parameter dict for RoPE types with scaling\n+            if rope_parameters[\"rope_type\"] in [\"llama3\", \"yarn\", \"longrope\"]:\n+                if hasattr(self, \"original_max_position_embeddings\"):\n+                    # NOTE: Phi3 (and potentially other models) save `original_max_position_embeddings` field\n+                    # containing the pretrained value outside rope parameters. This is an exception case where we\n+                    # give priority to `self.original_max_position_embeddings\n+                    self.rope_parameters[\"original_max_position_embeddings\"] = self.original_max_position_embeddings\n+                else:\n+                    self.rope_parameters.setdefault(\"original_max_position_embeddings\", self.max_position_embeddings)\n+\n         # Case 2: different RoPE for each layer -> several params as nested dict\n         else:\n-            for layer_type in layer_types:\n+            for layer_type in set(layer_types):\n                 rope_parameters[layer_type].setdefault(\"rope_type\", rope_parameters[layer_type].get(\"type\", \"default\"))\n                 rope_parameters[layer_type].setdefault(\"rope_theta\", rope_theta)\n                 if partial_rotary_factor is not None:\n                     rope_parameters[layer_type][\"partial_rotary_factor\"] = partial_rotary_factor\n \n+                if rope_parameters[layer_type][\"rope_type\"] in [\"llama3\", \"yarn\", \"longrope\"]:\n+                    self.rope_parameters[layer_type].setdefault(\n+                        \"original_max_position_embeddings\", self.max_position_embeddings\n+                    )\n+\n         self.rope_parameters = rope_parameters\n \n     def validate_rope(self: \"PreTrainedConfig\", ignore_keys: Optional[set] = None):\n@@ -725,26 +737,24 @@ def _validate_linear_rope_parameters(self, rope_parameters: dict, ignore_keys: O\n             logger.warning(f\"`rope_parameters`'s factor field must be a float >= 1, got {factor}\")\n \n     def _validate_dynamic_rope_parameters(self, rope_parameters: dict, ignore_keys: Optional[set] = None):\n-        # TODO (joao): update logic for the inclusion of `original_max_position_embeddings`\n-        optional_keys = {\"original_max_position_embeddings\"}\n         required_keys = {\"rope_type\", \"factor\"}\n         received_keys = set(rope_parameters.keys())\n         rope_type = rope_parameters[\"rope_type\"]\n-        self._check_received_keys(rope_type, received_keys, required_keys, optional_keys, ignore_keys=ignore_keys)\n+        self._check_received_keys(rope_type, received_keys, required_keys, ignore_keys=ignore_keys)\n \n         factor = rope_parameters[\"factor\"]\n         if factor is None or not isinstance(factor, float) or factor < 1.0:\n             logger.warning(f\"`rope_parameters`'s factor field must be a float >= 1, got {factor}\")\n \n     def _validate_yarn_rope_parameters(self, rope_parameters: dict, ignore_keys: Optional[set] = None):\n-        required_keys = {\"rope_type\", \"factor\", \"rope_theta\"}\n+        required_keys = {\"rope_type\", \"factor\", \"rope_theta\", \"original_max_position_embeddings\"}\n         optional_keys = {\n             \"attention_factor\",\n             \"beta_fast\",\n             \"beta_slow\",\n-            \"original_max_position_embeddings\",\n             \"mscale\",\n             \"mscale_all_dim\",\n+            \"truncate\",\n         }\n         received_keys = set(rope_parameters.keys())\n         rope_type = rope_parameters[\"rope_type\"]\n@@ -772,37 +782,24 @@ def _validate_yarn_rope_parameters(self, rope_parameters: dict, ignore_keys: Opt\n                 f\"(defaults to 32 if None) and beta_slow={beta_slow} (defaults to 1 if None)\"\n             )\n \n-        # Models should set `config.rope_parameters[\"original_max_position_embeddings\"]` to their original (pre-yarn) context\n-        # length, with `config.max_position_embeddings` corresponding to their post-yarn context length.\n-        # However, for BC purposes, we allow the former to be unset.\n-        original_max_position_embeddings = self.rope_parameters.get(\"original_max_position_embeddings\")\n-        if original_max_position_embeddings is not None:\n-            # Double-check: `factor` should be the ratio between the pre-yarn and post-yarn context lengths.\n-            implicit_factor = self.max_position_embeddings / original_max_position_embeddings\n-            if implicit_factor != factor:\n-                logger.warning_once(\n-                    f\"The explicitly set RoPE scaling factor (config.rope_parameters['factor'] = {factor}) does not match \"\n-                    \"the ratio implicitly set by other parameters (implicit factor = \"\n-                    \"post-yarn context length / pre-yarn context length = \"\n-                    \"config.max_position_embeddings / config.rope_parameters['original_max_position_embeddings'] = \"\n-                    f\"{implicit_factor}). Using the explicit factor ({factor}) in YaRN. This may cause unexpected \"\n-                    \"behaviour in model usage, please correct the 'max_position_embeddings' fields in the model config.\"\n-                )\n-        # No `config.rope_parameters[\"original_max_position_embeddings\"]`. Is `config.max_position_embeddings` the\n-        # pre-yarn or the post-yarn context length?\n-        # BC: we assume it is the pre-yarn context length.\n-        else:\n+        # Double-check: `factor` should be the ratio between the pre-yarn and post-yarn context lengths.\n+        # NOTE: we might get `implicit_factor == 1` if config's `original_max_position_embeddings` was\n+        # inferred from `max_position_embeddings` during standardization\n+        original_max_position_embeddings = self.rope_parameters[\"original_max_position_embeddings\"]\n+        implicit_factor = self.max_position_embeddings / original_max_position_embeddings\n+        if implicit_factor != factor and implicit_factor != 1:\n             logger.warning_once(\n-                \"config.rope_parameters['original_max_position_embeddings'], the pre-yarn context length, is unset. We will \"\n-                \"**assume** config.max_position_embeddings holds the pre-yarn context length. Some use cases may expect \"\n-                \"config.max_position_embeddings to hold the post-yarn context length (pre-yarn context length * \"\n-                \"factor) -- we recommend updating both fields for optimal downstream model usage.\"\n+                f\"The explicitly set RoPE scaling factor (config.rope_parameters['factor'] = {factor}) does not match \"\n+                \"the ratio implicitly set by other parameters (implicit factor = \"\n+                \"post-yarn context length / pre-yarn context length = \"\n+                \"config.max_position_embeddings / config.rope_parameters['original_max_position_embeddings'] = \"\n+                f\"{implicit_factor}). Using the explicit factor ({factor}) in YaRN. This may cause unexpected \"\n+                \"behaviour in model usage, please correct the 'original_max_position_embeddings' fields in the model config.\"\n             )\n \n     def _validate_longrope_rope_parameters(self, rope_parameters: dict, ignore_keys: Optional[set] = None):\n-        required_keys = {\"rope_type\", \"short_factor\", \"long_factor\", \"rope_theta\"}\n-        # TODO (joao): update logic for the inclusion of `original_max_position_embeddings`\n-        optional_keys = {\"attention_factor\", \"factor\", \"original_max_position_embeddings\"}\n+        required_keys = {\"rope_type\", \"short_factor\", \"long_factor\", \"rope_theta\", \"original_max_position_embeddings\"}\n+        optional_keys = {\"attention_factor\", \"factor\"}\n         received_keys = set(rope_parameters.keys())\n         rope_type = rope_parameters[\"rope_type\"]\n         self._check_received_keys(rope_type, received_keys, required_keys, optional_keys, ignore_keys=ignore_keys)\n@@ -827,29 +824,28 @@ def _validate_longrope_rope_parameters(self, rope_parameters: dict, ignore_keys:\n                 f\"`rope_parameters`'s long_factor field must have length {dim // 2}, got {len(long_factor)}\"\n             )\n \n-        # Handle Phi3 divergence: prefer the use of `attention_factor` and/or `factor` over\n-        # `original_max_position_embeddings` to compute internal variables. The latter lives outside `rope_parameters` and is\n-        # unique to longrope (= undesirable)\n-        if hasattr(self, \"original_max_position_embeddings\"):\n+        factor = rope_parameters.get(\"factor\")\n+        original_max_position_embeddings = rope_parameters[\"original_max_position_embeddings\"]\n+\n+        # Handle Phi3 divergence: we prefer the use of `attention_factor` and/or `factor` over\n+        # `original_max_position_embeddings` to compute internal variables. The latter is undesirable\n+        if factor is None and original_max_position_embeddings is not None:\n             logger.warning_once(\n-                \"This model has set a `original_max_position_embeddings` field, to be used together with \"\n+                \"This model config has set a `rope_parameters['original_max_position_embeddings']` field, to be used together with \"\n                 \"`max_position_embeddings` to determine a scaling factor. Please set the `factor` field of `rope_parameters`\"\n                 \"with this ratio instead -- we recommend the use of this field over `original_max_position_embeddings`, \"\n                 \"as it is compatible with most model architectures.\"\n             )\n-        else:\n-            factor = rope_parameters.get(\"factor\")\n-            if factor is None:\n-                logger.warning(\"Missing required keys in `rope_parameters`: 'factor'\")\n-            elif not isinstance(factor, float) or factor < 1.0:\n-                logger.warning(f\"`rope_parameters`'s factor field must be a float >= 1, got {factor}\")\n-\n-            attention_factor = rope_parameters.get(\"attention_factor\")\n-            if attention_factor is not None:\n-                if not isinstance(attention_factor, float) or attention_factor < 0.0:\n-                    logger.warning(\n-                        f\"`rope_parameters`'s attention_factor field must be a float greater than 0, got {attention_factor}\"\n-                    )\n+        elif factor is None and original_max_position_embeddings is None:\n+            logger.warning(\"Missing required keys in `rope_parameters`: 'factor'\")\n+        elif not isinstance(factor, float) or factor < 1.0:\n+            logger.warning(f\"`rope_parameters`'s factor field must be a float >= 1, got {factor}\")\n+\n+        attention_factor = rope_parameters.get(\"attention_factor\")\n+        if attention_factor is not None and (not isinstance(attention_factor, float) or attention_factor < 0.0):\n+            logger.warning(\n+                f\"`rope_parameters`'s attention_factor field must be a float greater than 0, got {attention_factor}\"\n+            )\n \n     def _validate_llama3_rope_parameters(self, rope_parameters: dict, ignore_keys: Optional[set] = None):\n         required_keys = {\n@@ -906,6 +902,10 @@ def _check_received_keys(\n             received_keys -= {\"type\"}\n             required_keys.add(\"rope_type\")\n \n+        optional_keys = optional_keys or set()\n+        if \"partial_rotary_factor\" not in optional_keys:\n+            optional_keys.add(\"partial_rotary_factor\")\n+\n         # Some models need to store model-specific keys, and we don't want to throw warning at them\n         if ignore_keys is not None:\n             received_keys -= ignore_keys\n@@ -914,10 +914,7 @@ def _check_received_keys(\n         if missing_keys:\n             raise KeyError(f\"Missing required keys in `rope_parameters` for 'rope_type'='{rope_type}': {missing_keys}\")\n \n-        if optional_keys is not None:\n-            unused_keys = received_keys - required_keys - optional_keys\n-        else:\n-            unused_keys = received_keys - required_keys\n+        unused_keys = received_keys - required_keys - optional_keys\n         if unused_keys:\n             logger.warning(f\"Unrecognized keys in `rope_parameters` for 'rope_type'='{rope_type}': {unused_keys}\")\n "
        },
        {
            "sha": "9e703ca1f7a9a3d15a405fc734b7b586041d32c1",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/347328108aa79959eda0e2eae0695c12f0101046/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/347328108aa79959eda0e2eae0695c12f0101046/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=347328108aa79959eda0e2eae0695c12f0101046",
            "patch": "@@ -158,6 +158,7 @@ def test_model_rope_scaling_from_config(self):\n     def test_model_rope_scaling_frequencies(self):\n         \"\"\"Tests the frequency properties of the different RoPE scaling types on the model RoPE layer.\"\"\"\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.layer_types = [\"full_attention\", \"sliding_attention\"]\n \n         # Retrieves the RoPE layer class from the base model class. Uses `.named_modules()` to avoid hardcoding the\n         # named location of the RoPE layer class."
        },
        {
            "sha": "04de261cd6c5b543eadc340c535abf3f0483b1ba",
            "filename": "tests/utils/test_modeling_rope_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/347328108aa79959eda0e2eae0695c12f0101046/tests%2Futils%2Ftest_modeling_rope_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/347328108aa79959eda0e2eae0695c12f0101046/tests%2Futils%2Ftest_modeling_rope_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_rope_utils.py?ref=347328108aa79959eda0e2eae0695c12f0101046",
            "patch": "@@ -101,17 +101,15 @@ def test_yarn_original_original_max_position_embeddings_validation(self):\n             with self.assertLogs(\"transformers.modeling_rope_utils\", level=\"WARNING\") as logs:\n                 config.validate_rope()\n \n-        # bad rope config, no `original_max_position_embeddings` -> warning\n+        # bad rope config, no `original_max_position_embeddings` -> raise error\n         rope_config = {\n             \"rope_type\": \"yarn\",\n             \"rope_theta\": 10000.0,\n             \"factor\": 2.0,\n         }\n         config.rope_parameters = rope_config\n-        with self.assertLogs(\"transformers.modeling_rope_utils\", level=\"WARNING\") as logs:\n+        with self.assertRaises(KeyError):\n             config.validate_rope()\n-            self.assertEqual(len(logs.output), 1)\n-            self.assertIn(\"is unset\", logs.output[0])\n \n         # bad rope config, bad implicit fator -> warning\n         rope_config = {\n@@ -338,9 +336,8 @@ def test_longrope_rope_numerically(self):\n         default_inv_freq, _ = rope_fn(config=config, device=torch_device)\n \n         # Check 1: according to the paper, if `attention_factor` is not specified, then it has a specific default --\n-        # `math.sqrt(1 + math.log(factor) / math.log(max_position_embeddings))`\n+        # `math.sqrt(1 + math.log(factor) / math.log(original_max_position_embeddings))`\n         rope_fn = ROPE_INIT_FUNCTIONS[\"longrope\"]\n-        max_position_embeddings = config.max_position_embeddings\n         for factor in (2.0, 10.0, 20.0):\n             config.rope_parameters = {\n                 \"rope_type\": \"longrope\",\n@@ -350,7 +347,9 @@ def test_longrope_rope_numerically(self):\n                 \"long_factor\": long_factor,\n             }\n             _, attention_scale = rope_fn(config=config, device=torch_device)\n-            self.assertEqual(attention_scale, math.sqrt(1 + math.log(factor) / math.log(max_position_embeddings)))\n+            self.assertEqual(\n+                attention_scale, math.sqrt(1 + math.log(factor) / math.log(config.max_position_embeddings))\n+            )\n \n             config.rope_parameters = {\n                 \"rope_type\": \"longrope\",\n@@ -372,6 +371,7 @@ def test_longrope_rope_numerically(self):\n             }\n             self.assertEqual(config.rope_parameters.get(\"attention_factor\"), None)\n             # Verify that \"TypeError: '<' not supported between instances of 'NoneType' and 'int'\" is not raised.\n+            config.standardize_rope_params()\n             config.validate_rope()\n \n         # Check 2: seq_len == 0 -> short factor is applied to the default frequencies"
        }
    ],
    "stats": {
        "total": 168,
        "additions": 83,
        "deletions": 85
    }
}