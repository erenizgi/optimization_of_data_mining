{
    "author": "cyyever",
    "message": "Use accelerator API to free device memory (#41195)\n\n* Use accelerator API to free device memory\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Use clear_device_cache\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Cleanup\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Cleanup\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "1c5ac899e83c55ff64a020ae98ff2ae149791585",
    "files": [
        {
            "sha": "484f98218bd3619b910cf3c4f4113d21c3dcc3e0",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 21,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c5ac899e83c55ff64a020ae98ff2ae149791585/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c5ac899e83c55ff64a020ae98ff2ae149791585/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=1c5ac899e83c55ff64a020ae98ff2ae149791585",
            "patch": "@@ -162,13 +162,11 @@\n     is_schedulefree_available,\n     is_torch_hpu_available,\n     is_torch_mlu_available,\n-    is_torch_mps_available,\n     is_torch_musa_available,\n     is_torch_neuroncore_available,\n     is_torch_npu_available,\n     is_torch_optimi_available,\n     is_torch_xla_available,\n-    is_torch_xpu_available,\n     is_torchao_available,\n     logging,\n     strtobool,\n@@ -220,9 +218,11 @@\n         DistributedType,\n         load_fsdp_model,\n         load_fsdp_optimizer,\n+        release_memory,\n         save_fsdp_model,\n         save_fsdp_optimizer,\n     )\n+    from accelerate.utils.memory import clear_device_cache\n \n     if is_deepspeed_available():\n         from accelerate.utils import DeepSpeedSchedulerWrapper\n@@ -2229,9 +2229,7 @@ def _inner_training_loop(\n         self._train_batch_size = batch_size\n         if self.args.auto_find_batch_size:\n             if self.state.train_batch_size != self._train_batch_size:\n-                from accelerate.utils import release_memory\n-\n-                (self.model_wrapped,) = release_memory(self.model_wrapped)\n+                release_memory(self.model_wrapped)\n                 self.model_wrapped = self.model\n \n                 # Check for DeepSpeed *after* the initial pass and modify the config\n@@ -3828,22 +3826,7 @@ def training_step(\n                 self.args.torch_empty_cache_steps is not None\n                 and self.state.global_step % self.args.torch_empty_cache_steps == 0\n             ):\n-                if is_torch_xpu_available():\n-                    torch.xpu.empty_cache()\n-                elif is_torch_mlu_available():\n-                    torch.mlu.empty_cache()\n-                elif is_torch_musa_available():\n-                    torch.musa.empty_cache()\n-                elif is_torch_npu_available():\n-                    torch.npu.empty_cache()\n-                elif is_torch_mps_available():\n-                    torch.mps.empty_cache()\n-                elif is_torch_hpu_available():\n-                    logger.warning(\n-                        \"`torch_empty_cache_steps` is set but HPU device/backend does not support empty_cache().\"\n-                    )\n-                else:\n-                    torch.cuda.empty_cache()\n+                clear_device_cache()\n \n             kwargs = {}\n "
        }
    ],
    "stats": {
        "total": 25,
        "additions": 4,
        "deletions": 21
    }
}