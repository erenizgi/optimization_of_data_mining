{
    "author": "cyyever",
    "message": "[3/N] Use pyupgrade --py39-plus to improve code (#36936)\n\nUse pyupgrade --py39-plus to improve code\n\nSigned-off-by: cyy <cyyever@outlook.com>",
    "sha": "32c12aaec3665882d1fa8dd79964a423a0be6e62",
    "files": [
        {
            "sha": "88cf0b3fd008a64966a0bf8268bb524191cc835b",
            "filename": "src/transformers/dynamic_module_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/32c12aaec3665882d1fa8dd79964a423a0be6e62/src%2Ftransformers%2Fdynamic_module_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32c12aaec3665882d1fa8dd79964a423a0be6e62/src%2Ftransformers%2Fdynamic_module_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdynamic_module_utils.py?ref=32c12aaec3665882d1fa8dd79964a423a0be6e62",
            "patch": "@@ -90,7 +90,7 @@ def get_relative_imports(module_file: Union[str, os.PathLike]) -> list[str]:\n         module_file (`str` or `os.PathLike`): The module file to inspect.\n \n     Returns:\n-        `List[str]`: The list of relative imports in the module.\n+        `list[str]`: The list of relative imports in the module.\n     \"\"\"\n     with open(module_file, encoding=\"utf-8\") as f:\n         content = f.read()\n@@ -112,7 +112,7 @@ def get_relative_import_files(module_file: Union[str, os.PathLike]) -> list[str]\n         module_file (`str` or `os.PathLike`): The module file to inspect.\n \n     Returns:\n-        `List[str]`: The list of all relative imports a given module needs (recursively), which will give us the list\n+        `list[str]`: The list of all relative imports a given module needs (recursively), which will give us the list\n         of module files a given module needs.\n     \"\"\"\n     no_change = False\n@@ -144,7 +144,7 @@ def get_imports(filename: Union[str, os.PathLike]) -> list[str]:\n         filename (`str` or `os.PathLike`): The module file to inspect.\n \n     Returns:\n-        `List[str]`: The list of all packages required to use the input module.\n+        `list[str]`: The list of all packages required to use the input module.\n     \"\"\"\n     with open(filename, encoding=\"utf-8\") as f:\n         content = f.read()\n@@ -175,7 +175,7 @@ def check_imports(filename: Union[str, os.PathLike]) -> list[str]:\n         filename (`str` or `os.PathLike`): The module file to check.\n \n     Returns:\n-        `List[str]`: The list of relative imports in the file.\n+        `list[str]`: The list of relative imports in the file.\n     \"\"\"\n     imports = get_imports(filename)\n     missing_packages = []"
        },
        {
            "sha": "0992636a141ccea2a2ad5cad26eb4fd4ecfb1f0b",
            "filename": "src/transformers/model_debugging_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/32c12aaec3665882d1fa8dd79964a423a0be6e62/src%2Ftransformers%2Fmodel_debugging_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32c12aaec3665882d1fa8dd79964a423a0be6e62/src%2Ftransformers%2Fmodel_debugging_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodel_debugging_utils.py?ref=32c12aaec3665882d1fa8dd79964a423a0be6e62",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 The HuggingFace Inc. team.\n # All rights reserved.\n #"
        },
        {
            "sha": "3a1c2e59985dcf99eb88ae9c42d4255242d9672d",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/32c12aaec3665882d1fa8dd79964a423a0be6e62/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32c12aaec3665882d1fa8dd79964a423a0be6e62/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=32c12aaec3665882d1fa8dd79964a423a0be6e62",
            "patch": "@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-# coding=utf-8\n \n # Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n #\n@@ -16,7 +15,6 @@\n # limitations under the License.\n \n from functools import lru_cache\n-from typing import FrozenSet\n \n from huggingface_hub import get_full_repo_name  # for backward compatibility\n from huggingface_hub.constants import HF_HUB_DISABLE_TELEMETRY as DISABLE_TELEMETRY  # for backward compatibility\n@@ -300,8 +298,8 @@ def check_min_version(min_version):\n         )\n \n \n-@lru_cache()\n-def get_available_devices() -> FrozenSet[str]:\n+@lru_cache\n+def get_available_devices() -> frozenset[str]:\n     \"\"\"\n     Returns a frozenset of devices available for the current PyTorch installation.\n     \"\"\""
        },
        {
            "sha": "bad6a7471e54f12a47f854aca51138577bb99e6d",
            "filename": "src/transformers/utils/attention_visualizer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/32c12aaec3665882d1fa8dd79964a423a0be6e62/src%2Ftransformers%2Futils%2Fattention_visualizer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32c12aaec3665882d1fa8dd79964a423a0be6e62/src%2Ftransformers%2Futils%2Fattention_visualizer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fattention_visualizer.py?ref=32c12aaec3665882d1fa8dd79964a423a0be6e62",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "a6ab0046bc5c77d9a18450bd7518df16c31faabd",
            "filename": "src/transformers/utils/backbone_utils.py",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/32c12aaec3665882d1fa8dd79964a423a0be6e62/src%2Ftransformers%2Futils%2Fbackbone_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32c12aaec3665882d1fa8dd79964a423a0be6e62/src%2Ftransformers%2Futils%2Fbackbone_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fbackbone_utils.py?ref=32c12aaec3665882d1fa8dd79964a423a0be6e62",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2023 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -17,7 +16,8 @@\n \n import enum\n import inspect\n-from typing import TYPE_CHECKING, Iterable, List, Optional, Tuple, Union\n+from collections.abc import Iterable\n+from typing import TYPE_CHECKING, Optional, Union\n \n \n if TYPE_CHECKING:\n@@ -75,9 +75,9 @@ def verify_out_features_out_indices(\n \n \n def _align_output_features_output_indices(\n-    out_features: Optional[List[str]],\n-    out_indices: Optional[Union[List[int], Tuple[int]]],\n-    stage_names: List[str],\n+    out_features: Optional[list[str]],\n+    out_indices: Optional[Union[list[int], tuple[int]]],\n+    stage_names: list[str],\n ):\n     \"\"\"\n     Finds the corresponding `out_features` and `out_indices` for the given `stage_names`.\n@@ -106,10 +106,10 @@ def _align_output_features_output_indices(\n \n \n def get_aligned_output_features_output_indices(\n-    out_features: Optional[List[str]],\n-    out_indices: Optional[Union[List[int], Tuple[int]]],\n-    stage_names: List[str],\n-) -> Tuple[List[str], List[int]]:\n+    out_features: Optional[list[str]],\n+    out_indices: Optional[Union[list[int], tuple[int]]],\n+    stage_names: list[str],\n+) -> tuple[list[str], list[int]]:\n     \"\"\"\n     Get the `out_features` and `out_indices` so that they are aligned.\n \n@@ -198,7 +198,7 @@ def out_features(self):\n         return self._out_features\n \n     @out_features.setter\n-    def out_features(self, out_features: List[str]):\n+    def out_features(self, out_features: list[str]):\n         \"\"\"\n         Set the out_features attribute. This will also update the out_indices attribute to match the new out_features.\n         \"\"\"\n@@ -211,7 +211,7 @@ def out_indices(self):\n         return self._out_indices\n \n     @out_indices.setter\n-    def out_indices(self, out_indices: Union[Tuple[int], List[int]]):\n+    def out_indices(self, out_indices: Union[tuple[int], list[int]]):\n         \"\"\"\n         Set the out_indices attribute. This will also update the out_features attribute to match the new out_indices.\n         \"\"\"\n@@ -264,7 +264,7 @@ def out_features(self):\n         return self._out_features\n \n     @out_features.setter\n-    def out_features(self, out_features: List[str]):\n+    def out_features(self, out_features: list[str]):\n         \"\"\"\n         Set the out_features attribute. This will also update the out_indices attribute to match the new out_features.\n         \"\"\"\n@@ -277,7 +277,7 @@ def out_indices(self):\n         return self._out_indices\n \n     @out_indices.setter\n-    def out_indices(self, out_indices: Union[Tuple[int], List[int]]):\n+    def out_indices(self, out_indices: Union[tuple[int], list[int]]):\n         \"\"\"\n         Set the out_indices attribute. This will also update the out_features attribute to match the new out_indices.\n         \"\"\""
        },
        {
            "sha": "f96b5d8ddecab445a6706d9562de3e1c6af43d0c",
            "filename": "src/transformers/utils/chat_template_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/32c12aaec3665882d1fa8dd79964a423a0be6e62/src%2Ftransformers%2Futils%2Fchat_template_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32c12aaec3665882d1fa8dd79964a423a0be6e62/src%2Ftransformers%2Futils%2Fchat_template_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fchat_template_utils.py?ref=32c12aaec3665882d1fa8dd79964a423a0be6e62",
            "patch": "@@ -19,7 +19,7 @@\n from contextlib import contextmanager\n from datetime import datetime\n from functools import lru_cache\n-from typing import Any, Callable, Dict, List, Optional, Tuple, Union, get_args, get_origin, get_type_hints\n+from typing import Any, Callable, Optional, Union, get_args, get_origin, get_type_hints\n \n from packaging import version\n \n@@ -71,7 +71,7 @@ class DocstringParsingException(Exception):\n     pass\n \n \n-def _get_json_schema_type(param_type: str) -> Dict[str, str]:\n+def _get_json_schema_type(param_type: str) -> dict[str, str]:\n     type_mapping = {\n         int: {\"type\": \"integer\"},\n         float: {\"type\": \"number\"},\n@@ -87,7 +87,7 @@ def _get_json_schema_type(param_type: str) -> Dict[str, str]:\n     return type_mapping.get(param_type, {\"type\": \"object\"})\n \n \n-def _parse_type_hint(hint: str) -> Dict:\n+def _parse_type_hint(hint: str) -> dict:\n     origin = get_origin(hint)\n     args = get_args(hint)\n \n@@ -152,7 +152,7 @@ def _parse_type_hint(hint: str) -> Dict:\n     raise TypeHintParsingException(\"Couldn't parse this type hint, likely due to a custom class or object: \", hint)\n \n \n-def _convert_type_hints_to_json_schema(func: Callable) -> Dict:\n+def _convert_type_hints_to_json_schema(func: Callable) -> dict:\n     type_hints = get_type_hints(func)\n     signature = inspect.signature(func)\n     required = []\n@@ -173,7 +173,7 @@ def _convert_type_hints_to_json_schema(func: Callable) -> Dict:\n     return schema\n \n \n-def parse_google_format_docstring(docstring: str) -> Tuple[Optional[str], Optional[Dict], Optional[str]]:\n+def parse_google_format_docstring(docstring: str) -> tuple[Optional[str], Optional[dict], Optional[str]]:\n     \"\"\"\n     Parses a Google-style docstring to extract the function description,\n     argument descriptions, and return description.\n@@ -206,7 +206,7 @@ def parse_google_format_docstring(docstring: str) -> Tuple[Optional[str], Option\n     return description, args_dict, returns\n \n \n-def get_json_schema(func: Callable) -> Dict:\n+def get_json_schema(func: Callable) -> dict:\n     \"\"\"\n     This function generates a JSON schema for a given function, based on its docstring and type hints. This is\n     mostly used for passing lists of tools to a chat template. The JSON schema contains the name and description of\n@@ -398,7 +398,7 @@ def is_active(self) -> bool:\n             return self._rendered_blocks or self._generation_indices\n \n         @contextmanager\n-        def activate_tracker(self, rendered_blocks: List[int], generation_indices: List[int]):\n+        def activate_tracker(self, rendered_blocks: list[int], generation_indices: list[int]):\n             try:\n                 if self.is_active():\n                     raise ValueError(\"AssistantTracker should not be reused before closed\")"
        },
        {
            "sha": "6ab0c45d996ab7374bc9489e2a63cf2a43d3ada6",
            "filename": "src/transformers/utils/fx.py",
            "status": "modified",
            "additions": 19,
            "deletions": 20,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/32c12aaec3665882d1fa8dd79964a423a0be6e62/src%2Ftransformers%2Futils%2Ffx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32c12aaec3665882d1fa8dd79964a423a0be6e62/src%2Ftransformers%2Futils%2Ffx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Ffx.py?ref=32c12aaec3665882d1fa8dd79964a423a0be6e62",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -24,7 +23,7 @@\n import random\n import sys\n import warnings\n-from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Type, Union\n+from typing import Any, Callable, Literal, Optional, Union\n \n import torch\n import torch.utils._pytree as pytree\n@@ -78,9 +77,9 @@\n \n \n def _generate_supported_model_class_names(\n-    model_name: Type[PretrainedConfig],\n-    supported_tasks: Optional[Union[str, List[str]]] = None,\n-) -> List[str]:\n+    model_name: type[PretrainedConfig],\n+    supported_tasks: Optional[Union[str, list[str]]] = None,\n+) -> list[str]:\n     task_mapping = {\n         \"default\": MODEL_MAPPING_NAMES,\n         \"pretraining\": MODEL_FOR_PRETRAINING_MAPPING_NAMES,\n@@ -590,7 +589,7 @@ def to_concrete(t):\n     return operator.getitem(a, b)\n \n \n-_MANUAL_META_OVERRIDES: Dict[Callable, Callable] = {\n+_MANUAL_META_OVERRIDES: dict[Callable, Callable] = {\n     torch.nn.Embedding: torch_nn_embedding,\n     torch.nn.functional.embedding: torch_nn_functional_embedding,\n     torch.nn.LayerNorm: torch_nn_layernorm,\n@@ -716,7 +715,7 @@ class HFCacheProxy(HFProxy):\n     Proxy that represents an instance of `transformers.cache_utils.Cache`.\n     \"\"\"\n \n-    def install_orig_cache_cls(self, orig_cache_cls: Type[Cache]):\n+    def install_orig_cache_cls(self, orig_cache_cls: type[Cache]):\n         self._orig_cache_cls = orig_cache_cls\n \n     @property\n@@ -770,8 +769,8 @@ class HFProxyableClassMeta(type):\n     def __new__(\n         cls,\n         name: str,\n-        bases: Tuple[Type, ...],\n-        attrs: Dict[str, Any],\n+        bases: tuple[type, ...],\n+        attrs: dict[str, Any],\n         proxy_factory_fn: Optional[Callable[[Node], Proxy]] = None,\n     ):\n         cls = super().__new__(cls, name, bases, attrs)\n@@ -794,7 +793,7 @@ def __new__(\n         return cls\n \n \n-def gen_constructor_wrapper(target: Callable) -> Tuple[Callable, Callable]:\n+def gen_constructor_wrapper(target: Callable) -> tuple[Callable, Callable]:\n     \"\"\"\n     Wraps `target` to be proxyable. Used for tensor creators like `torch.ones`, `torch.arange` and so on.\n     \"\"\"\n@@ -813,7 +812,7 @@ def _proxies_to_metas(v):\n     return v\n \n \n-def create_cache_proxy_factory_fn(orig_cache_cls: Type[Cache]) -> Callable[[Node], HFCacheProxy]:\n+def create_cache_proxy_factory_fn(orig_cache_cls: type[Cache]) -> Callable[[Node], HFCacheProxy]:\n     def cache_proxy_factory_fn(n: Node) -> HFCacheProxy:\n         global _CURRENT_TRACER\n         if not isinstance(_CURRENT_TRACER, HFTracer):\n@@ -849,7 +848,7 @@ def cache_proxy_factory_fn(n: Node) -> HFCacheProxy:\n )\n \n \n-def _generate_random_int(low: int = 10, high: int = 20, forbidden_values: Optional[List[int]] = None):\n+def _generate_random_int(low: int = 10, high: int = 20, forbidden_values: Optional[list[int]] = None):\n     if forbidden_values is None:\n         forbidden_values = []\n     value = random.randint(low, high)\n@@ -899,8 +898,8 @@ def __init__(self, autowrap_modules=(math,), autowrap_functions=()):\n             )\n \n     def _generate_dummy_input(\n-        self, model: \"PreTrainedModel\", input_name: str, shape: List[int], input_names: List[str]\n-    ) -> Dict[str, torch.Tensor]:\n+        self, model: \"PreTrainedModel\", input_name: str, shape: list[int], input_names: list[str]\n+    ) -> dict[str, torch.Tensor]:\n         \"\"\"Generates dummy input for model inference recording.\"\"\"\n         # Retrieving the model class, either from the \"class_for_deserialization\" attribute if the model was restored\n         # from pickle, or from the \"__class__\" attribute in the general case.\n@@ -1181,7 +1180,7 @@ def maybe_get_proxy_for_attr(attr_val, collection_to_search, parameter_proxy_cac\n             return attr_val\n \n     # Needed for PyTorch 1.13+\n-    def getattr(self, attr: str, attr_val: Any, parameter_proxy_cache: Dict[str, Any]):\n+    def getattr(self, attr: str, attr_val: Any, parameter_proxy_cache: dict[str, Any]):\n         return self._module_getattr(attr, attr_val, parameter_proxy_cache)\n \n     def call_module(self, m, forward, args, kwargs):\n@@ -1233,8 +1232,8 @@ def patch_for_tracing(self, root: Union[torch.nn.Module, Callable[..., Any]]):\n     def trace(\n         self,\n         root: Union[torch.nn.Module, Callable[..., Any]],\n-        concrete_args: Optional[Dict[str, Any]] = None,\n-        dummy_inputs: Optional[Dict[str, Any]] = None,\n+        concrete_args: Optional[dict[str, Any]] = None,\n+        dummy_inputs: Optional[dict[str, Any]] = None,\n         complete_concrete_args_with_inputs_not_in_dummy_inputs: bool = True,\n     ) -> Graph:\n         \"\"\"\n@@ -1422,7 +1421,7 @@ def keys(self, obj: \"Proxy\") -> Any:\n         return attribute\n \n \n-def get_concrete_args(model: nn.Module, input_names: List[str]):\n+def get_concrete_args(model: nn.Module, input_names: list[str]):\n     sig = inspect.signature(model.forward)\n \n     if not (set(input_names) <= set(sig.parameters.keys())):\n@@ -1450,9 +1449,9 @@ def check_if_model_is_supported(model: \"PreTrainedModel\"):\n \n def symbolic_trace(\n     model: \"PreTrainedModel\",\n-    input_names: Optional[List[str]] = None,\n+    input_names: Optional[list[str]] = None,\n     disable_check: bool = False,\n-    tracer_cls: Type[HFTracer] = HFTracer,\n+    tracer_cls: type[HFTracer] = HFTracer,\n ) -> GraphModule:\n     \"\"\"\n     Performs symbolic tracing on the model."
        },
        {
            "sha": "65a3efaed5ab5cfaf0935638e93c20c23aecef8d",
            "filename": "src/transformers/utils/generic.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/32c12aaec3665882d1fa8dd79964a423a0be6e62/src%2Ftransformers%2Futils%2Fgeneric.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32c12aaec3665882d1fa8dd79964a423a0be6e62/src%2Ftransformers%2Futils%2Fgeneric.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fgeneric.py?ref=32c12aaec3665882d1fa8dd79964a423a0be6e62",
            "patch": "@@ -21,12 +21,12 @@\n import tempfile\n import warnings\n from collections import OrderedDict, UserDict\n-from collections.abc import MutableMapping\n+from collections.abc import Iterable, MutableMapping\n from contextlib import ExitStack, contextmanager\n from dataclasses import fields, is_dataclass\n from enum import Enum\n from functools import partial, wraps\n-from typing import Any, ContextManager, Dict, Iterable, List, Optional, Tuple, TypedDict\n+from typing import Any, ContextManager, Optional, TypedDict\n \n import numpy as np\n from packaging import version\n@@ -465,7 +465,7 @@ def __reduce__(self):\n         args = tuple(getattr(self, field.name) for field in fields(self))\n         return callable, args, *remaining\n \n-    def to_tuple(self) -> Tuple[Any]:\n+    def to_tuple(self) -> tuple[Any]:\n         \"\"\"\n         Convert self to a tuple containing all the attributes/keys that are not `None`.\n         \"\"\"\n@@ -475,7 +475,7 @@ def to_tuple(self) -> Tuple[Any]:\n if is_torch_available():\n     import torch.utils._pytree as _torch_pytree\n \n-    def _model_output_flatten(output: ModelOutput) -> Tuple[List[Any], \"_torch_pytree.Context\"]:\n+    def _model_output_flatten(output: ModelOutput) -> tuple[list[Any], \"_torch_pytree.Context\"]:\n         return list(output.values()), list(output.keys())\n \n     def _model_output_unflatten(\n@@ -542,7 +542,7 @@ class ContextManagers:\n     in the `fastcore` library.\n     \"\"\"\n \n-    def __init__(self, context_managers: List[ContextManager]):\n+    def __init__(self, context_managers: list[ContextManager]):\n         self.context_managers = context_managers\n         self.stack = ExitStack()\n \n@@ -883,7 +883,7 @@ class LossKwargs(TypedDict, total=False):\n     num_items_in_batch: Optional[int]\n \n \n-def is_timm_config_dict(config_dict: Dict[str, Any]) -> bool:\n+def is_timm_config_dict(config_dict: dict[str, Any]) -> bool:\n     \"\"\"Checks whether a config dict is a timm config dict.\"\"\"\n     return \"pretrained_cfg\" in config_dict\n \n@@ -903,13 +903,13 @@ def is_timm_local_checkpoint(pretrained_model_path: str) -> bool:\n \n     # pretrained_model_path is a file\n     if is_file and pretrained_model_path.endswith(\".json\"):\n-        with open(pretrained_model_path, \"r\") as f:\n+        with open(pretrained_model_path) as f:\n             config_dict = json.load(f)\n         return is_timm_config_dict(config_dict)\n \n     # pretrained_model_path is a directory with a config.json\n     if is_dir and os.path.exists(os.path.join(pretrained_model_path, \"config.json\")):\n-        with open(os.path.join(pretrained_model_path, \"config.json\"), \"r\") as f:\n+        with open(os.path.join(pretrained_model_path, \"config.json\")) as f:\n             config_dict = json.load(f)\n         return is_timm_config_dict(config_dict)\n "
        },
        {
            "sha": "57a267b72889d35074a71352d0630dfcefc9f747",
            "filename": "src/transformers/utils/hub.py",
            "status": "modified",
            "additions": 20,
            "deletions": 24,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/32c12aaec3665882d1fa8dd79964a423a0be6e62/src%2Ftransformers%2Futils%2Fhub.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32c12aaec3665882d1fa8dd79964a423a0be6e62/src%2Ftransformers%2Futils%2Fhub.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fhub.py?ref=32c12aaec3665882d1fa8dd79964a423a0be6e62",
            "patch": "@@ -23,7 +23,7 @@\n import warnings\n from concurrent import futures\n from pathlib import Path\n-from typing import Dict, List, Optional, Union\n+from typing import Optional, Union\n from urllib.parse import urlparse\n from uuid import uuid4\n \n@@ -168,7 +168,7 @@ def define_sagemaker_information():\n     return sagemaker_object\n \n \n-def http_user_agent(user_agent: Union[Dict, str, None] = None) -> str:\n+def http_user_agent(user_agent: Union[dict, str, None] = None) -> str:\n     \"\"\"\n     Formats a user-agent string with basic info about a request.\n     \"\"\"\n@@ -270,17 +270,17 @@ def cached_file(\n \n def cached_files(\n     path_or_repo_id: Union[str, os.PathLike],\n-    filenames: List[str],\n+    filenames: list[str],\n     cache_dir: Optional[Union[str, os.PathLike]] = None,\n     force_download: bool = False,\n     resume_download: Optional[bool] = None,\n-    proxies: Optional[Dict[str, str]] = None,\n+    proxies: Optional[dict[str, str]] = None,\n     token: Optional[Union[bool, str]] = None,\n     revision: Optional[str] = None,\n     local_files_only: bool = False,\n     subfolder: str = \"\",\n     repo_type: Optional[str] = None,\n-    user_agent: Optional[Union[str, Dict[str, str]]] = None,\n+    user_agent: Optional[Union[str, dict[str, str]]] = None,\n     _raise_exceptions_for_gated_repo: bool = True,\n     _raise_exceptions_for_missing_entries: bool = True,\n     _raise_exceptions_for_connection_errors: bool = True,\n@@ -378,7 +378,7 @@ def cached_files(\n             if not os.path.isfile(resolved_file):\n                 if _raise_exceptions_for_missing_entries and filename != os.path.join(subfolder, \"config.json\"):\n                     revision_ = \"main\" if revision is None else revision\n-                    raise EnvironmentError(\n+                    raise OSError(\n                         f\"{path_or_repo_id} does not appear to have a file named {filename}. Checkout \"\n                         f\"'https://huggingface.co/{path_or_repo_id}/tree/{revision_}' for available files.\"\n                     )\n@@ -410,7 +410,7 @@ def cached_files(\n                 elif not _raise_exceptions_for_missing_entries:\n                     file_counter += 1\n                 else:\n-                    raise EnvironmentError(f\"Could not locate {filename} inside {path_or_repo_id}.\")\n+                    raise OSError(f\"Could not locate {filename} inside {path_or_repo_id}.\")\n \n     # Either all the files were found, or some were _CACHED_NO_EXIST but we do not raise for missing entries\n     if file_counter == len(full_filenames):\n@@ -453,14 +453,14 @@ def cached_files(\n     except Exception as e:\n         # We cannot recover from them\n         if isinstance(e, RepositoryNotFoundError) and not isinstance(e, GatedRepoError):\n-            raise EnvironmentError(\n+            raise OSError(\n                 f\"{path_or_repo_id} is not a local folder and is not a valid model identifier \"\n                 \"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to pass a token \"\n                 \"having permission to this repo either by logging in with `huggingface-cli login` or by passing \"\n                 \"`token=<your_token>`\"\n             ) from e\n         elif isinstance(e, RevisionNotFoundError):\n-            raise EnvironmentError(\n+            raise OSError(\n                 f\"{revision} is not a valid git identifier (branch name, tag name or commit id) that exists \"\n                 \"for this model name. Check the model page at \"\n                 f\"'https://huggingface.co/{path_or_repo_id}' for available revisions.\"\n@@ -478,7 +478,7 @@ def cached_files(\n         if isinstance(e, GatedRepoError):\n             if not _raise_exceptions_for_gated_repo:\n                 return None\n-            raise EnvironmentError(\n+            raise OSError(\n                 \"You are trying to access a gated repo.\\nMake sure to have access to it at \"\n                 f\"https://huggingface.co/{path_or_repo_id}.\\n{str(e)}\"\n             ) from e\n@@ -488,7 +488,7 @@ def cached_files(\n             # Here we only raise if both flags for missing entry and connection errors are True (because it can be raised\n             # even when `local_files_only` is True, in which case raising for connections errors only would not make sense)\n             elif _raise_exceptions_for_missing_entries:\n-                raise EnvironmentError(\n+                raise OSError(\n                     f\"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load the files, and couldn't find them in the\"\n                     f\" cached files.\\nCheckout your internet connection or see how to run the library in offline mode at\"\n                     \" 'https://huggingface.co/docs/transformers/installation#offline-mode'.\"\n@@ -498,9 +498,7 @@ def cached_files(\n         elif isinstance(e, HTTPError) and not isinstance(e, EntryNotFoundError):\n             if not _raise_exceptions_for_connection_errors:\n                 return None\n-            raise EnvironmentError(\n-                f\"There was a specific connection error when trying to load {path_or_repo_id}:\\n{e}\"\n-            )\n+            raise OSError(f\"There was a specific connection error when trying to load {path_or_repo_id}:\\n{e}\")\n \n     resolved_files = [\n         _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision) for filename in full_filenames\n@@ -632,7 +630,7 @@ def has_file(\n     path_or_repo: Union[str, os.PathLike],\n     filename: str,\n     revision: Optional[str] = None,\n-    proxies: Optional[Dict[str, str]] = None,\n+    proxies: Optional[dict[str, str]] = None,\n     token: Optional[Union[bool, str]] = None,\n     *,\n     local_files_only: bool = False,\n@@ -707,19 +705,17 @@ def has_file(\n         return True\n     except GatedRepoError as e:\n         logger.error(e)\n-        raise EnvironmentError(\n+        raise OSError(\n             f\"{path_or_repo} is a gated repository. Make sure to request access at \"\n             f\"https://huggingface.co/{path_or_repo} and pass a token having permission to this repo either by \"\n             \"logging in with `huggingface-cli login` or by passing `token=<your_token>`.\"\n         ) from e\n     except RepositoryNotFoundError as e:\n         logger.error(e)\n-        raise EnvironmentError(\n-            f\"{path_or_repo} is not a local folder or a valid repository name on 'https://hf.co'.\"\n-        ) from e\n+        raise OSError(f\"{path_or_repo} is not a local folder or a valid repository name on 'https://hf.co'.\") from e\n     except RevisionNotFoundError as e:\n         logger.error(e)\n-        raise EnvironmentError(\n+        raise OSError(\n             f\"{revision} is not a valid git identifier (branch name, tag name or commit id) that exists for this \"\n             f\"model name. Check the model page at 'https://huggingface.co/{path_or_repo}' for available revisions.\"\n         ) from e\n@@ -780,7 +776,7 @@ def _upload_modified_files(\n         self,\n         working_dir: Union[str, os.PathLike],\n         repo_id: str,\n-        files_timestamps: Dict[str, float],\n+        files_timestamps: dict[str, float],\n         commit_message: Optional[str] = None,\n         token: Optional[Union[bool, str]] = None,\n         create_pr: bool = False,\n@@ -867,7 +863,7 @@ def push_to_hub(\n         safe_serialization: bool = True,\n         revision: Optional[str] = None,\n         commit_description: Optional[str] = None,\n-        tags: Optional[List[str]] = None,\n+        tags: Optional[list[str]] = None,\n         **deprecated_kwargs,\n     ) -> str:\n         \"\"\"\n@@ -1101,7 +1097,7 @@ def get_checkpoint_shard_files(\n     if not os.path.isfile(index_filename):\n         raise ValueError(f\"Can't find a checkpoint index ({index_filename}) in {pretrained_model_name_or_path}.\")\n \n-    with open(index_filename, \"r\") as f:\n+    with open(index_filename) as f:\n         index = json.loads(f.read())\n \n     shard_filenames = sorted(set(index[\"weight_map\"].values()))\n@@ -1136,7 +1132,7 @@ def get_checkpoint_shard_files(\n \n def create_and_tag_model_card(\n     repo_id: str,\n-    tags: Optional[List[str]] = None,\n+    tags: Optional[list[str]] = None,\n     token: Optional[str] = None,\n     ignore_metadata_errors: bool = False,\n ):"
        },
        {
            "sha": "a2915e167a0bcf9d61aeaca269608b98320c3c54",
            "filename": "src/transformers/utils/logging.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/32c12aaec3665882d1fa8dd79964a423a0be6e62/src%2Ftransformers%2Futils%2Flogging.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32c12aaec3665882d1fa8dd79964a423a0be6e62/src%2Ftransformers%2Futils%2Flogging.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Flogging.py?ref=32c12aaec3665882d1fa8dd79964a423a0be6e62",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2020 Optuna, Hugging Face\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "7db16b70a75ccdce7bd9c07c5dfc99f42e5c8807",
            "filename": "src/transformers/utils/model_parallel_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/32c12aaec3665882d1fa8dd79964a423a0be6e62/src%2Ftransformers%2Futils%2Fmodel_parallel_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32c12aaec3665882d1fa8dd79964a423a0be6e62/src%2Ftransformers%2Futils%2Fmodel_parallel_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fmodel_parallel_utils.py?ref=32c12aaec3665882d1fa8dd79964a423a0be6e62",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2020 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "22a44d858e731f9bd4067d373165b0b376f5191b",
            "filename": "src/transformers/utils/notebook.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/32c12aaec3665882d1fa8dd79964a423a0be6e62/src%2Ftransformers%2Futils%2Fnotebook.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32c12aaec3665882d1fa8dd79964a423a0be6e62/src%2Ftransformers%2Futils%2Fnotebook.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fnotebook.py?ref=32c12aaec3665882d1fa8dd79964a423a0be6e62",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2020 Hugging Face\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "3eb62a099059d4c1acd9eef75f94ee5ac22fd60f",
            "filename": "src/transformers/utils/peft_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/32c12aaec3665882d1fa8dd79964a423a0be6e62/src%2Ftransformers%2Futils%2Fpeft_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32c12aaec3665882d1fa8dd79964a423a0be6e62/src%2Ftransformers%2Futils%2Fpeft_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fpeft_utils.py?ref=32c12aaec3665882d1fa8dd79964a423a0be6e62",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n import importlib\n import os\n-from typing import Dict, Optional, Union\n+from typing import Optional, Union\n \n from packaging import version\n \n@@ -31,7 +31,7 @@ def find_adapter_config_file(\n     cache_dir: Optional[Union[str, os.PathLike]] = None,\n     force_download: bool = False,\n     resume_download: Optional[bool] = None,\n-    proxies: Optional[Dict[str, str]] = None,\n+    proxies: Optional[dict[str, str]] = None,\n     token: Optional[Union[bool, str]] = None,\n     revision: Optional[str] = None,\n     local_files_only: bool = False,"
        },
        {
            "sha": "2ea4f4d64ce660b3d59d60c0650f5849e4fd2251",
            "filename": "src/transformers/utils/sentencepiece_model_pb2_new.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/32c12aaec3665882d1fa8dd79964a423a0be6e62/src%2Ftransformers%2Futils%2Fsentencepiece_model_pb2_new.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32c12aaec3665882d1fa8dd79964a423a0be6e62/src%2Ftransformers%2Futils%2Fsentencepiece_model_pb2_new.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fsentencepiece_model_pb2_new.py?ref=32c12aaec3665882d1fa8dd79964a423a0be6e62",
            "patch": "@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n # Generated by the protocol buffer compiler.  DO NOT EDIT!\n # source: sentencepiece_model.proto\n \"\"\"Generated protocol buffer code.\"\"\""
        }
    ],
    "stats": {
        "total": 163,
        "additions": 75,
        "deletions": 88
    }
}