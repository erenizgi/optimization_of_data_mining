{
    "author": "devkade",
    "message": "Update blip model card (#38513)\n\n* Update docs/source/en/model_doc/blip.md\n\n* fix(docs/source/en/model_doc/blip.md): fix redundent typo error\n\n* fix (docs/source/en/model_doc/blip.md): modify of review contents\n\n* fix(docs/source/en/model_doc/blip.md): modify code block\n\n* Update blip.md\n\n---------\n\nCo-authored-by: devkade <mouseku@moana-master>\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "2166b6b4ff09f6dd3867ab982f262f66482aa968",
    "files": [
        {
            "sha": "a8d4c5a14bbde991cc8d95af231d2c14f803f021",
            "filename": "docs/source/en/model_doc/blip.md",
            "status": "modified",
            "additions": 58,
            "deletions": 17,
            "changes": 75,
            "blob_url": "https://github.com/huggingface/transformers/blob/2166b6b4ff09f6dd3867ab982f262f66482aa968/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2166b6b4ff09f6dd3867ab982f262f66482aa968/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip.md?ref=2166b6b4ff09f6dd3867ab982f262f66482aa968",
            "patch": "@@ -14,35 +14,76 @@ rendered properly in your Markdown viewer.\n \n -->\n \n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+    </div>\n+</div>\n+\n # BLIP\n \n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n-</div>\n+[BLIP](https://huggingface.co/papers/2201.12086) (Bootstrapped Language-Image Pretraining) is a vision-language pretraining (VLP) framework designed for *both* understanding and generation tasks. Most existing pretrained models are only good at one or the other. It uses a captioner to generate captions and a filter to remove the noisy captions. This increases training data quality and more effectively uses the messy web data.\n+\n+\n+You can find all the original BLIP checkpoints under the [BLIP](https://huggingface.co/collections/Salesforce/blip-models-65242f40f1491fbf6a9e9472) collection.\n+\n+> [!TIP]\n+> This model was contributed by [ybelkada](https://huggingface.co/ybelkada).\n+> \n+> Click on the BLIP models in the right sidebar for more examples of how to apply BLIP to different vision language tasks.\n+\n+The example below demonstrates how to visual question answering with [`Pipeline`] or the [`AutoModel`] class.\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```python\n+import torch\n+from transformers import pipeline\n+\n+pipeline = pipeline(\n+    task=\"visual-question-answering\",\n+    model=\"Salesforce/blip-vqa-base\",\n+    torch_dtype=torch.float16,\n+    device=0\n+)\n+url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n+pipeline(question=\"What is the weather in this image?\", image=url)\n+```\n \n-## Overview\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n \n-The BLIP model was proposed in [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://huggingface.co/papers/2201.12086) by Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi.\n+```python\n+import requests\n+import torch\n+from PIL import Image\n+from transformers import AutoProcessor, AutoModelForVisualQuestionAnswering\n \n-BLIP is a model that is able to perform various multi-modal tasks including:\n-- Visual Question Answering \n-- Image-Text retrieval (Image-text matching)\n-- Image Captioning\n+processor = AutoProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n+model = AutoModelForVisualQuestionAnswering.from_pretrained(\n+    \"Salesforce/blip-vqa-base\", \n+    torch_dtype=torch.float16,\n+    device_map=\"auto\"\n+)\n \n-The abstract from the paper is the following:\n+url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n+image = Image.open(requests.get(url, stream=True).raw)\n \n-*Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. \n-However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to videolanguage tasks in a zero-shot manner. Code, models, and datasets are released.*\n+question = \"What is the weather in this image?\"\n+inputs = processor(images=image, text=question, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n \n-![BLIP.gif](https://cdn-uploads.huggingface.co/production/uploads/1670928184033-62441d1d9fdefb55a0b7d12c.gif)\n+output = model.generate(**inputs)\n+processor.batch_decode(output, skip_special_tokens=True)[0]\n+```\n \n-This model was contributed by [ybelkada](https://huggingface.co/ybelkada).\n-The original code can be found [here](https://github.com/salesforce/BLIP).\n+</hfoption>\n+</hfoptions>\n \n ## Resources\n \n-- [Jupyter notebook](https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_blip.ipynb) on how to fine-tune BLIP for image captioning on a custom dataset\n+Refer to this [notebook](https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_blip.ipynb) to learn how to fine-tune BLIP for image captioning on a custom dataset.\n \n ## BlipConfig\n "
        }
    ],
    "stats": {
        "total": 75,
        "additions": 58,
        "deletions": 17
    }
}