{
    "author": "fabxoe",
    "message": "ğŸŒ [i18n-KO] Translated `model_doc/mistral.md` to Korean (#33648)\n\n* docs: ko: model_doc/mistral.md\r\n\r\n* feat: nmt draft\r\n\r\n* fix: resolve suggestions\r\n\r\nCo-authored-by: Ahnjj_DEV <ahnjj.dev@gmail.com>\r\nCo-authored-by: Chaewon Song <chaewon1019@ewhain.net>\r\nCo-authored-by: HyeokJun SHIN <96534680+jun048098@users.noreply.github.com>\r\n\r\n* fix: resolve suggestions\r\n\r\n* fix: resolve suggestions\r\n\r\nCo-authored-by: HyeokJun SHIN <96534680+jun048098@users.noreply.github.com>\r\n\r\n---------\r\n\r\nCo-authored-by: Ahnjj_DEV <ahnjj.dev@gmail.com>\r\nCo-authored-by: Chaewon Song <chaewon1019@ewhain.net>\r\nCo-authored-by: HyeokJun SHIN <96534680+jun048098@users.noreply.github.com>",
    "sha": "48e80284fa37e2ff8ec3ef83a74c567b3e7824f6",
    "files": [
        {
            "sha": "723677f704f4f3f6dbd9aab2774a3cb6fbbefcfd",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/48e80284fa37e2ff8ec3ef83a74c567b3e7824f6/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/48e80284fa37e2ff8ec3ef83a74c567b3e7824f6/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=48e80284fa37e2ff8ec3ef83a74c567b3e7824f6",
            "patch": "@@ -448,6 +448,8 @@\n         title: (ë²ˆì—­ì¤‘) MegatronBERT\n       - local: in_translation\n         title: (ë²ˆì—­ì¤‘) MegatronGPT2\n+      - local: model_doc/mistral\n+        title: Mistral\n       - local: in_translation\n         title: (ë²ˆì—­ì¤‘) mLUKE\n       - local: in_translation"
        },
        {
            "sha": "4c9f2d7a77d4a2fee499e4db14e88f4c122c10e5",
            "filename": "docs/source/ko/model_doc/mistral.md",
            "status": "added",
            "additions": 233,
            "deletions": 0,
            "changes": 233,
            "blob_url": "https://github.com/huggingface/transformers/blob/48e80284fa37e2ff8ec3ef83a74c567b3e7824f6/docs%2Fsource%2Fko%2Fmodel_doc%2Fmistral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/48e80284fa37e2ff8ec3ef83a74c567b3e7824f6/docs%2Fsource%2Fko%2Fmodel_doc%2Fmistral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fmistral.md?ref=48e80284fa37e2ff8ec3ef83a74c567b3e7824f6",
            "patch": "@@ -0,0 +1,233 @@\n+<!--Copyright 2023 Mistral AI and The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See thze License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Mistral[[mistral]]\n+\n+## ê°œìš”[[overview]]\n+\n+ë¯¸ìŠ¤íŠ¸ë„ì€ Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayedê°€ ì‘ì„±í•œ [ì´ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸](https://mistral.ai/news/announcing-mistral-7b/)ì—ì„œ ì†Œê°œë˜ì—ˆìŠµë‹ˆë‹¤.\n+\n+ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ì˜ ì„œë‘ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n+\n+*ë¯¸ìŠ¤íŠ¸ë„ AIíŒ€ì€ í˜„ì¡´í•˜ëŠ” ì–¸ì–´ ëª¨ë¸ ì¤‘ í¬ê¸° ëŒ€ë¹„ ê°€ì¥ ê°•ë ¥í•œ ë¯¸ìŠ¤íŠ¸ë„7Bë¥¼ ì¶œì‹œí•˜ê²Œ ë˜ì–´ ìë‘ìŠ¤ëŸ½ìŠµë‹ˆë‹¤.*\n+\n+ë¯¸ìŠ¤íŠ¸ë„-7BëŠ” [mistral.ai](https://mistral.ai/)ì—ì„œ ì¶œì‹œí•œ ì²« ë²ˆì§¸ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì…ë‹ˆë‹¤.\n+\n+### ì•„í‚¤í…ì²˜ ì„¸ë¶€ì‚¬í•­[[architectural-details]]\n+\n+ë¯¸ìŠ¤íŠ¸ë„-7BëŠ” ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ì  íŠ¹ì§•ì„ ê°€ì§„ ë””ì½”ë” ì „ìš© íŠ¸ëœìŠ¤í¬ë¨¸ì…ë‹ˆë‹¤:\n+\n+- ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì–´í…ì…˜: 8k ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ì™€ ê³ ì • ìºì‹œ í¬ê¸°ë¡œ í›ˆë ¨ë˜ì—ˆìœ¼ë©°, ì´ë¡ ìƒ 128K í† í°ì˜ ì–´í…ì…˜ ë²”ìœ„ë¥¼ ê°€ì§‘ë‹ˆë‹¤.\n+- GQA(Grouped Query Attention): ë” ë¹ ë¥¸ ì¶”ë¡ ì´ ê°€ëŠ¥í•˜ê³  ë” ì‘ì€ í¬ê¸°ì˜ ìºì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n+- ë°”ì´íŠ¸ í´ë°±(Byte-fallback) BPE í† í¬ë‚˜ì´ì €: ë¬¸ìë“¤ì´ ì ˆëŒ€ ì–´íœ˜ ëª©ë¡ ì™¸ì˜ í† í°ìœ¼ë¡œ ë§¤í•‘ë˜ì§€ ì•Šë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤.\n+\n+ë” ìì„¸í•œ ë‚´ìš©ì€ [ì¶œì‹œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸](https://mistral.ai/news/announcing-mistral-7b/)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n+\n+### ë¼ì´ì„ ìŠ¤[[license]]\n+\n+`ë¯¸ìŠ¤íŠ¸ë„-7B`ëŠ” ì•„íŒŒì¹˜ 2.0 ë¼ì´ì„ ìŠ¤ë¡œ ì¶œì‹œë˜ì—ˆìŠµë‹ˆë‹¤.\n+\n+## ì‚¬ìš© íŒ[[usage-tips]]\n+\n+ë¯¸ìŠ¤íŠ¸ë„ AIíŒ€ì€ ë‹¤ìŒ 3ê°€ì§€ ì²´í¬í¬ì¸íŠ¸ë¥¼ ê³µê°œí–ˆìŠµë‹ˆë‹¤:\n+\n+- ê¸°ë³¸ ëª¨ë¸ì¸ [ë¯¸ìŠ¤íŠ¸ë„-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)ì€ ì¸í„°ë„· ê·œëª¨ì˜ ë°ì´í„°ì—ì„œ ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ë„ë¡ ì‚¬ì „ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤.\n+- ì§€ì‹œ ì¡°ì • ëª¨ë¸ì¸ [ë¯¸ìŠ¤íŠ¸ë„-7B-Instruct-v0.1](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)ì€ ì§€ë„ ë¯¸ì„¸ ì¡°ì •(SFT)ê³¼ ì§ì ‘ ì„ í˜¸ë„ ìµœì í™”(DPO)ë¥¼ ì‚¬ìš©í•œ ì±„íŒ…ì— ìµœì í™”ëœ ê¸°ë³¸ ëª¨ë¸ì…ë‹ˆë‹¤.\n+- ê°œì„ ëœ ì§€ì‹œ ì¡°ì • ëª¨ë¸ì¸ [ë¯¸ìŠ¤íŠ¸ë„-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)ëŠ” v1ì„ ê°œì„ í•œ ë²„ì „ì…ë‹ˆë‹¤.\n+\n+ê¸°ë³¸ ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì´ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n+\n+```python\n+>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+>>> model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", device_map=\"auto\")\n+>>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n+\n+>>> prompt = \"My favourite condiment is\"\n+\n+>>> model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n+>>> model.to(device)\n+\n+>>> generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n+>>> tokenizer.batch_decode(generated_ids)[0]\n+\"My favourite condiment is to ...\"\n+```\n+\n+ì§€ì‹œ ì¡°ì • ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì´ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n+\n+```python\n+>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+>>> model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", device_map=\"auto\")\n+>>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n+\n+>>> messages = [\n+...     {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n+...     {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n+...     {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n+... ]\n+\n+>>> model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n+\n+>>> generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True)\n+>>> tokenizer.batch_decode(generated_ids)[0]\n+\"Mayonnaise can be made as follows: (...)\"\n+```\n+\n+ì§€ì‹œ ì¡°ì • ëª¨ë¸ì€ ì…ë ¥ì´ ì˜¬ë°”ë¥¸ í˜•ì‹ìœ¼ë¡œ ì¤€ë¹„ë˜ë„ë¡ [ì±„íŒ… í…œí”Œë¦¿](../chat_templating)ì„ ì ìš©í•´ì•¼ í•©ë‹ˆë‹¤.\n+\n+## í”Œë˜ì‹œ ì–´í…ì…˜ì„ ì´ìš©í•œ ë¯¸ìŠ¤íŠ¸ë„ ì†ë„í–¥ìƒ[[speeding-up-mistral-by-using-flash-attention]]\n+\n+ìœ„ì˜ ì½”ë“œ ìŠ¤ë‹ˆí«ë“¤ì€ ì–´ë–¤ ìµœì í™” ê¸°ë²•ë„ ì‚¬ìš©í•˜ì§€ ì•Šì€ ì¶”ë¡  ê³¼ì •ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. í•˜ì§€ë§Œ ëª¨ë¸ ë‚´ë¶€ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì˜ ë” ë¹ ë¥¸ êµ¬í˜„ì¸ [í”Œë˜ì‹œ ì–´í…ì…˜2](../perf_train_gpu_one.md#flash-attention-2)ì„ í™œìš©í•˜ë©´ ëª¨ë¸ì˜ ì†ë„ë¥¼ í¬ê²Œ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+ë¨¼ì €, ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì–´í…ì…˜ ê¸°ëŠ¥ì„ í¬í•¨í•˜ëŠ” í”Œë˜ì‹œ ì–´í…ì…˜2ì˜ ìµœì‹  ë²„ì „ì„ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.\n+\n+```bash\n+pip install -U flash-attn --no-build-isolation\n+```\n+\n+í•˜ë“œì›¨ì–´ì™€ í”Œë˜ì‹œ ì–´í…ì…˜2ì˜ í˜¸í™˜ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì„¸ìš”. ì´ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ [í”Œë˜ì‹œ ì–´í…ì…˜ ì €ì¥ì†Œ](https://github.com/Dao-AILab/flash-attention)ì˜ ê³µì‹ ë¬¸ì„œì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ ëª¨ë¸ì„ ë°˜ì •ë°€ë„(ì˜ˆ: `torch.float16`)ë¡œ ë¶ˆëŸ¬ì™€ì•¼í•©ë‹ˆë‹¤.\n+\n+í”Œë˜ì‹œ ì–´í…ì…˜2ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ê³  ì‹¤í–‰í•˜ë ¤ë©´ ì•„ë˜ ì½”ë“œ ìŠ¤ë‹ˆí«ì„ ì°¸ì¡°í•˜ì„¸ìš”:\n+\n+```python\n+>>> import torch\n+>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+>>> model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\", device_map=\"auto\")\n+>>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n+\n+>>> prompt = \"My favourite condiment is\"\n+\n+>>> model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n+>>> model.to(device)\n+\n+>>> generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n+>>> tokenizer.batch_decode(generated_ids)[0]\n+\"My favourite condiment is to (...)\"\n+```\n+\n+### ê¸°ëŒ€í•˜ëŠ” ì†ë„ í–¥ìƒ[[expected-speedups]]\n+\n+ë‹¤ìŒì€ `mistralai/Mistral-7B-v0.1` ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•œ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ê¸°ë³¸ êµ¬í˜„ê³¼ í”Œë˜ì‹œ ì–´í…ì…˜2 ë²„ì „ ëª¨ë¸ ì‚¬ì´ì˜ ìˆœìˆ˜ ì¶”ë¡  ì‹œê°„ì„ ë¹„êµí•œ ì˜ˆìƒ ì†ë„ í–¥ìƒ ë‹¤ì´ì–´ê·¸ë¨ì…ë‹ˆë‹¤.\n+\n+<div style=\"text-align: center\">\n+<img src=\"https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/mistral-7b-inference-large-seqlen.png\">\n+</div>\n+\n+### ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì–´í…ì…˜[[sliding-window-attention]]\n+\n+í˜„ì¬ êµ¬í˜„ì€ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ê³¼ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ìºì‹œ ê´€ë¦¬ ê¸°ëŠ¥ì„ ì§€ì›í•©ë‹ˆë‹¤. ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì–´í…ì…˜ì„ í™œì„±í™”í•˜ë ¤ë©´, ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì–´í…ì…˜ê³¼ í˜¸í™˜ë˜ëŠ”`flash-attn`(`>=2.3.0`)ë²„ì „ì„ ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤. \n+\n+ë˜í•œ í”Œë˜ì‹œ ì–´í…ì…˜2 ëª¨ë¸ì€ ë” ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ìºì‹œ ìŠ¬ë¼ì´ì‹± ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë¯¸ìŠ¤íŠ¸ë„ ëª¨ë¸ì˜ ê³µì‹ êµ¬í˜„ì—ì„œ ê¶Œì¥í•˜ëŠ” ë¡¤ë§ ìºì‹œ ë©”ì»¤ë‹ˆì¦˜ì„ ë”°ë¼, ìºì‹œ í¬ê¸°ë¥¼ ê³ ì •(`self.config.sliding_window`)ìœ¼ë¡œ ìœ ì§€í•˜ê³ , `padding_side=\"left\"`ì¸ ê²½ìš°ì—ë§Œ ë°°ì¹˜ ìƒì„±(batch generation)ì„ ì§€ì›í•˜ë©°, í˜„ì¬ í† í°ì˜ ì ˆëŒ€ ìœ„ì¹˜ë¥¼ ì‚¬ìš©í•´ ìœ„ì¹˜ ì„ë² ë”©ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n+\n+## ì–‘ìí™”ë¡œ ë¯¸ìŠ¤íŠ¸ë„ í¬ê¸° ì¤„ì´ê¸°[[shrinking-down-mistral-using-quantization]]\n+\n+ë¯¸ìŠ¤íŠ¸ë„ ëª¨ë¸ì€ 70ì–µ ê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§€ê³  ìˆì–´, ì ˆë°˜ì˜ ì •ë°€ë„(float16)ë¡œ ì•½ 14GBì˜ GPU RAMì´ í•„ìš”í•©ë‹ˆë‹¤. ê° íŒŒë¼ë¯¸í„°ê°€ 2ë°”ì´íŠ¸ë¡œ ì €ì¥ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ [ì–‘ìí™”](../quantization.md)ë¥¼ ì‚¬ìš©í•˜ë©´ ëª¨ë¸ í¬ê¸°ë¥¼ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ì„ 4ë¹„íŠ¸(ì¦‰, íŒŒë¼ë¯¸í„°ë‹¹ ë°˜ ë°”ì´íŠ¸)ë¡œ ì–‘ìí™”í•˜ë©´ ì•½ 3.5GBì˜ RAMë§Œ í•„ìš”í•©ë‹ˆë‹¤.\n+\n+ëª¨ë¸ì„ ì–‘ìí™”í•˜ëŠ” ê²ƒì€ `quantization_config`ë¥¼ ëª¨ë¸ì— ì „ë‹¬í•˜ëŠ” ê²ƒë§Œí¼ ê°„ë‹¨í•©ë‹ˆë‹¤. ì•„ë˜ì—ì„œëŠ” BitsAndBytes ì–‘ìí™”ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, ë‹¤ë¥¸ ì–‘ìí™” ë°©ë²•ì€ [ì´ í˜ì´ì§€](../quantization.md)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”:\n+\n+```python\n+>>> import torch\n+>>> from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n+\n+>>> # specify how to quantize the model\n+>>> quantization_config = BitsAndBytesConfig(\n+...         load_in_4bit=True,\n+...         bnb_4bit_quant_type=\"nf4\",\n+...         bnb_4bit_compute_dtype=\"torch.float16\",\n+... )\n+\n+>>> model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", quantization_config=True, device_map=\"auto\")\n+>>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n+\n+>>> prompt = \"My favourite condiment is\"\n+\n+>>> messages = [\n+...     {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n+...     {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n+...     {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n+... ]\n+\n+>>> model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n+\n+>>> generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True)\n+>>> tokenizer.batch_decode(generated_ids)[0]\n+\"The expected output\"\n+```\n+\n+ì´ ëª¨ë¸ì€ [Younes Belkada](https://huggingface.co/ybelkada)ì™€ [Arthur Zucker](https://huggingface.co/ArthurZ)ê°€ ê¸°ì—¬í–ˆìŠµë‹ˆë‹¤.\n+ì›ë³¸ ì½”ë“œëŠ” [ì´ê³³](https://github.com/mistralai/mistral-src)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+## ë¦¬ì†ŒìŠ¤[[resources]]\n+\n+ë¯¸ìŠ¤íŠ¸ë„ì„ ì‹œì‘í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” Hugging Faceì™€ community ìë£Œ ëª©ë¡(ğŸŒë¡œ í‘œì‹œë¨) ì…ë‹ˆë‹¤. ì—¬ê¸°ì— í¬í•¨ë  ìë£Œë¥¼ ì œì¶œí•˜ê³  ì‹¶ìœ¼ì‹œë‹¤ë©´ PR(Pull Request)ë¥¼ ì—´ì–´ì£¼ì„¸ìš”. ë¦¬ë·°í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤! ìë£ŒëŠ” ê¸°ì¡´ ìë£Œë¥¼ ë³µì œí•˜ëŠ” ëŒ€ì‹  ìƒˆë¡œìš´ ë‚´ìš©ì„ ë‹´ê³  ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n+\n+<PipelineTag pipeline=\"text-generation\"/>\n+\n+- ë¯¸ìŠ¤íŠ¸ë„-7Bì˜ ì§€ë„í˜• ë¯¸ì„¸ì¡°ì •(SFT)ì„ ìˆ˜í–‰í•˜ëŠ” ë°ëª¨ ë…¸íŠ¸ë¶ì€ [ì´ê³³](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Mistral/Supervised_fine_tuning_(SFT)_of_an_LLM_using_Hugging_Face_tooling.ipynb)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸŒ\n+- 2024ë…„ì— Hugging Face ë„êµ¬ë¥¼ ì‚¬ìš©í•´ LLMì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ [ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl). ğŸŒ\n+- Hugging Faceì˜ [ì •ë ¬(Alignment) í•¸ë“œë¶](https://github.com/huggingface/alignment-handbook)ì—ëŠ” ë¯¸ìŠ¤íŠ¸ë„-7Bë¥¼ ì‚¬ìš©í•œ ì§€ë„í˜• ë¯¸ì„¸ ì¡°ì •(SFT) ë° ì§ì ‘ ì„ í˜¸ ìµœì í™”(DPO)ë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ìŠ¤í¬ë¦½íŠ¸ì™€ ë ˆì‹œí”¼ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì—ëŠ” ë‹¨ì¼ GPUì—ì„œ QLoRa ë° ë‹¤ì¤‘ GPUë¥¼ ì‚¬ìš©í•œ ì „ì²´ ë¯¸ì„¸ ì¡°ì •ì„ ìœ„í•œ ìŠ¤í¬ë¦½íŠ¸ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n+- [ì¸ê³¼ì  ì–¸ì–´ ëª¨ë¸ë§ ì‘ì—… ê°€ì´ë“œ](../tasks/language_modeling)\n+\n+## MistralConfig[[transformers.MistralConfig]]\n+\n+[[autodoc]] MistralConfig\n+\n+## MistralModel[[transformers.MistralModel]]\n+\n+[[autodoc]] MistralModel\n+    - forward\n+\n+## MistralForCausalLM[[transformers.MistralForCausalLM]]\n+\n+[[autodoc]] MistralForCausalLM\n+    - forward\n+\n+## MistralForSequenceClassification[[transformers.MistralForSequenceClassification]]\n+\n+[[autodoc]] MistralForSequenceClassification\n+    - forward\n+\n+## MistralForTokenClassification[[transformers.MistralForTokenClassification]]\n+\n+[[autodoc]] MistralForTokenClassification\n+    - forward\n+\n+## FlaxMistralModel[[transformers.FlaxMistralModel]]\n+\n+[[autodoc]] FlaxMistralModel\n+    - __call__\n+\n+## FlaxMistralForCausalLM[[transformers.FlaxMistralForCausalLM]]\n+\n+[[autodoc]] FlaxMistralForCausalLM\n+    - __call__\n+\n+## TFMistralModel[[transformers.TFMistralModel]]\n+\n+[[autodoc]] TFMistralModel\n+    - call\n+\n+## TFMistralForCausalLM[[transformers.TFMistralForCausalLM]]\n+\n+[[autodoc]] TFMistralForCausalLM\n+    - call\n+\n+## TFMistralForSequenceClassification[[transformers.TFMistralForSequenceClassification]]\n+\n+[[autodoc]] TFMistralForSequenceClassification\n+    - call\n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 235,
        "additions": 235,
        "deletions": 0
    }
}