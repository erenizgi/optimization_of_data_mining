{
    "author": "MekkCyber",
    "message": "Higgs modules_to_not_convert standardization (#39989)\n\nfix higgs",
    "sha": "fe1bf8215906ac22ae7afaf47b877c4415dd7221",
    "files": [
        {
            "sha": "5c7e5bd7ac71edb1ff0d155b6cd780220498a9b0",
            "filename": "src/transformers/integrations/higgs.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fe1bf8215906ac22ae7afaf47b877c4415dd7221/src%2Ftransformers%2Fintegrations%2Fhiggs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fe1bf8215906ac22ae7afaf47b877c4415dd7221/src%2Ftransformers%2Fintegrations%2Fhiggs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhiggs.py?ref=fe1bf8215906ac22ae7afaf47b877c4415dd7221",
            "patch": "@@ -554,6 +554,7 @@ def replace_with_higgs_linear(\n     quantization_config=None,\n     current_key_name=None,\n     has_been_replaced=False,\n+    modules_to_not_convert=None,\n ):\n     \"\"\"\n     Public method that recursively replaces the Linear layers of the given model with HIGGS quantized layers.\n@@ -582,7 +583,7 @@ def replace_with_higgs_linear(\n         if isinstance(module, nn.Linear):\n             # Check if the current key is not in the `quantization_config.modules_to_not_convert`\n             current_key_name_str = \".\".join(current_key_name)\n-            if not any(current_key_name_str.endswith(key) for key in quantization_config.modules_to_not_convert):\n+            if not any(current_key_name_str.endswith(key) for key in modules_to_not_convert):\n                 with init_empty_weights():\n                     in_features = module.in_features\n                     out_features = module.out_features\n@@ -607,6 +608,7 @@ def replace_with_higgs_linear(\n                 quantization_config=quantization_config,\n                 current_key_name=current_key_name,\n                 has_been_replaced=has_been_replaced,\n+                modules_to_not_convert=modules_to_not_convert,\n             )\n         # Remove the last key for recursion\n         current_key_name.pop(-1)"
        },
        {
            "sha": "87c34a87824db46e41ea496cc495c53b02bceb85",
            "filename": "src/transformers/quantizers/quantizer_higgs.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/fe1bf8215906ac22ae7afaf47b877c4415dd7221/src%2Ftransformers%2Fquantizers%2Fquantizer_higgs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fe1bf8215906ac22ae7afaf47b877c4415dd7221/src%2Ftransformers%2Fquantizers%2Fquantizer_higgs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_higgs.py?ref=fe1bf8215906ac22ae7afaf47b877c4415dd7221",
            "patch": "@@ -123,13 +123,19 @@ def create_quantized_param(\n     def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n+        keep_in_fp32_modules: Optional[list[str]] = None,\n         **kwargs,\n     ):\n         from ..integrations import replace_with_higgs_linear\n \n+        self.modules_to_not_convert = self.get_modules_to_not_convert(\n+            model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n+        )\n+\n         replace_with_higgs_linear(\n             model,\n             quantization_config=self.quantization_config,\n+            modules_to_not_convert=self.modules_to_not_convert,\n         )\n         model.config.quantization_config = self.quantization_config\n "
        }
    ],
    "stats": {
        "total": 10,
        "additions": 9,
        "deletions": 1
    }
}