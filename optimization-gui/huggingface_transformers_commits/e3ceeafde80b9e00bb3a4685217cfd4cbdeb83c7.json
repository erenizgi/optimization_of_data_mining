{
    "author": "vasqu",
    "message": "[`mRope`] Fix warnings (#42660)\n\nfix warning",
    "sha": "e3ceeafde80b9e00bb3a4685217cfd4cbdeb83c7",
    "files": [
        {
            "sha": "f707ab291a8ca5f699375dae4a2d05a676d4c255",
            "filename": "src/transformers/models/glm4v/configuration_glm4v.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e3ceeafde80b9e00bb3a4685217cfd4cbdeb83c7/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconfiguration_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e3ceeafde80b9e00bb3a4685217cfd4cbdeb83c7/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconfiguration_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconfiguration_glm4v.py?ref=e3ceeafde80b9e00bb3a4685217cfd4cbdeb83c7",
            "patch": "@@ -234,7 +234,9 @@ def __init__(\n         self.attention_dropout = attention_dropout\n         self.rope_parameters = rope_parameters\n \n-        super().__init__(tie_word_embeddings=tie_word_embeddings, ignore_keys_at_rope_validation={\"mrope\"}, **kwargs)\n+        super().__init__(\n+            tie_word_embeddings=tie_word_embeddings, ignore_keys_at_rope_validation={\"mrope_section\"}, **kwargs\n+        )\n \n \n class Glm4vConfig(PreTrainedConfig):"
        },
        {
            "sha": "7f81d03f8ac9f96a5d7715a56a90c345acbc9186",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e3ceeafde80b9e00bb3a4685217cfd4cbdeb83c7/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e3ceeafde80b9e00bb3a4685217cfd4cbdeb83c7/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=e3ceeafde80b9e00bb3a4685217cfd4cbdeb83c7",
            "patch": "@@ -271,7 +271,9 @@ def __init__(\n         self.attention_dropout = attention_dropout\n         self.rope_parameters = rope_parameters\n \n-        super().__init__(tie_word_embeddings=tie_word_embeddings, ignore_keys_at_rope_validation={\"mrope\"}, **kwargs)\n+        super().__init__(\n+            tie_word_embeddings=tie_word_embeddings, ignore_keys_at_rope_validation={\"mrope_section\"}, **kwargs\n+        )\n \n \n class Glm4vConfig(PreTrainedConfig):"
        },
        {
            "sha": "fdfb96f75294c413dcc98e654b0ab9e2092aacc6",
            "filename": "src/transformers/models/glm4v_moe/configuration_glm4v_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e3ceeafde80b9e00bb3a4685217cfd4cbdeb83c7/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e3ceeafde80b9e00bb3a4685217cfd4cbdeb83c7/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py?ref=e3ceeafde80b9e00bb3a4685217cfd4cbdeb83c7",
            "patch": "@@ -280,7 +280,9 @@ def __init__(\n         self.first_k_dense_replace = first_k_dense_replace\n         self.norm_topk_prob = norm_topk_prob\n         self.router_aux_loss_coef = router_aux_loss_coef\n-        super().__init__(tie_word_embeddings=tie_word_embeddings, ignore_keys_at_rope_validation={\"mrope\"}, **kwargs)\n+        super().__init__(\n+            tie_word_embeddings=tie_word_embeddings, ignore_keys_at_rope_validation={\"mrope_section\"}, **kwargs\n+        )\n \n \n class Glm4vMoeConfig(PreTrainedConfig):"
        },
        {
            "sha": "71c213f940d1982755581b7c09ce80e29ccd2b7a",
            "filename": "src/transformers/models/glm4v_moe/modular_glm4v_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e3ceeafde80b9e00bb3a4685217cfd4cbdeb83c7/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e3ceeafde80b9e00bb3a4685217cfd4cbdeb83c7/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py?ref=e3ceeafde80b9e00bb3a4685217cfd4cbdeb83c7",
            "patch": "@@ -227,7 +227,7 @@ def __init__(\n         self.norm_topk_prob = norm_topk_prob\n         self.router_aux_loss_coef = router_aux_loss_coef\n         PreTrainedConfig.__init__(\n-            self, tie_word_embeddings=tie_word_embeddings, ignore_keys_at_rope_validation={\"mrope\"}, **kwargs\n+            self, tie_word_embeddings=tie_word_embeddings, ignore_keys_at_rope_validation={\"mrope_section\"}, **kwargs\n         )\n \n "
        },
        {
            "sha": "8ae45c5104f36b1eedf68b301475ce4af7d67f91",
            "filename": "src/transformers/models/qwen2_5_omni/configuration_qwen2_5_omni.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e3ceeafde80b9e00bb3a4685217cfd4cbdeb83c7/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e3ceeafde80b9e00bb3a4685217cfd4cbdeb83c7/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py?ref=e3ceeafde80b9e00bb3a4685217cfd4cbdeb83c7",
            "patch": "@@ -365,7 +365,7 @@ def __init__(\n         self.rope_parameters = rope_parameters\n         super().__init__(\n             tie_word_embeddings=tie_word_embeddings,\n-            ignore_keys_at_rope_validation={\"mrope\"},\n+            ignore_keys_at_rope_validation={\"mrope_section\"},\n             **kwargs,\n         )\n \n@@ -713,7 +713,9 @@ def __init__(\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n         self.rope_parameters = rope_parameters\n-        super().__init__(tie_word_embeddings=tie_word_embeddings, ignore_keys_at_rope_validation={\"mrope\"}, **kwargs)\n+        super().__init__(\n+            tie_word_embeddings=tie_word_embeddings, ignore_keys_at_rope_validation={\"mrope_section\"}, **kwargs\n+        )\n \n \n class Qwen2_5OmniDiTConfig(PreTrainedConfig):"
        },
        {
            "sha": "2bad5d01d7bb62e96ae0ff121e6244ced04959c5",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e3ceeafde80b9e00bb3a4685217cfd4cbdeb83c7/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e3ceeafde80b9e00bb3a4685217cfd4cbdeb83c7/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=e3ceeafde80b9e00bb3a4685217cfd4cbdeb83c7",
            "patch": "@@ -399,7 +399,7 @@ def __init__(\n         self.rope_parameters = rope_parameters\n         super().__init__(\n             tie_word_embeddings=tie_word_embeddings,\n-            ignore_keys_at_rope_validation={\"mrope\"},\n+            ignore_keys_at_rope_validation={\"mrope_section\"},\n             **kwargs,\n         )\n \n@@ -747,7 +747,9 @@ def __init__(\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n         self.rope_parameters = rope_parameters\n-        super().__init__(tie_word_embeddings=tie_word_embeddings, ignore_keys_at_rope_validation={\"mrope\"}, **kwargs)\n+        super().__init__(\n+            tie_word_embeddings=tie_word_embeddings, ignore_keys_at_rope_validation={\"mrope_section\"}, **kwargs\n+        )\n \n \n class Qwen2_5OmniDiTConfig(PreTrainedConfig):"
        },
        {
            "sha": "8832400df55d037216c16dbb2463aed957290cc6",
            "filename": "src/transformers/models/qwen2_5_vl/configuration_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e3ceeafde80b9e00bb3a4685217cfd4cbdeb83c7/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e3ceeafde80b9e00bb3a4685217cfd4cbdeb83c7/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py?ref=e3ceeafde80b9e00bb3a4685217cfd4cbdeb83c7",
            "patch": "@@ -230,7 +230,7 @@ def __init__(\n             bos_token_id=bos_token_id,\n             eos_token_id=eos_token_id,\n             pad_token_id=pad_token_id,\n-            ignore_keys_at_rope_validation={\"mrope\"},\n+            ignore_keys_at_rope_validation={\"mrope_section\"},\n             **kwargs,\n         )\n "
        },
        {
            "sha": "8372690ef47137ba1076512a7d1306ab8a4c182b",
            "filename": "src/transformers/models/qwen2_vl/configuration_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e3ceeafde80b9e00bb3a4685217cfd4cbdeb83c7/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e3ceeafde80b9e00bb3a4685217cfd4cbdeb83c7/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py?ref=e3ceeafde80b9e00bb3a4685217cfd4cbdeb83c7",
            "patch": "@@ -218,7 +218,7 @@ def __init__(\n             bos_token_id=bos_token_id,\n             eos_token_id=eos_token_id,\n             pad_token_id=pad_token_id,\n-            ignore_keys_at_rope_validation={\"mrope\"},\n+            ignore_keys_at_rope_validation={\"mrope_section\"},\n             **kwargs,\n         )\n "
        }
    ],
    "stats": {
        "total": 30,
        "additions": 20,
        "deletions": 10
    }
}