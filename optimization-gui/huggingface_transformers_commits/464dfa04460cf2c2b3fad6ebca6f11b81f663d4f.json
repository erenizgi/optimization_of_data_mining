{
    "author": "Cyrilvallez",
    "message": "Raise conversion errors after loading (#42807)\n\n* raise\n\n* comment\n\n* fix\n\n* add test\n\n* fix\n\n* add back return\n\n* small\n\n* raise after report\n\n* typos\n\n* fix\n\n* patch\n\n* switch name\n\n* doc\n\n* oupsi that was commented out",
    "sha": "464dfa04460cf2c2b3fad6ebca6f11b81f663d4f",
    "files": [
        {
            "sha": "8dd46b69b249592630bc8beaf1facd2245c9e2fc",
            "filename": "src/transformers/core_model_loading.py",
            "status": "modified",
            "additions": 61,
            "deletions": 56,
            "changes": 117,
            "blob_url": "https://github.com/huggingface/transformers/blob/464dfa04460cf2c2b3fad6ebca6f11b81f663d4f/src%2Ftransformers%2Fcore_model_loading.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/464dfa04460cf2c2b3fad6ebca6f11b81f663d4f/src%2Ftransformers%2Fcore_model_loading.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcore_model_loading.py?ref=464dfa04460cf2c2b3fad6ebca6f11b81f663d4f",
            "patch": "@@ -409,7 +409,7 @@ def convert(\n         config=None,\n         hf_quantizer=None,\n         missing_keys: Optional[MutableSet[str]] = None,\n-        misc: Optional[MutableMapping[str, str]] = None,\n+        conversion_errors: Optional[MutableMapping[str, str]] = None,\n     ):\n         # Collect the tensors here - we use a new dictionary to avoid keeping them in memory in the internal\n         # attribute during the whole process\n@@ -421,7 +421,9 @@ def convert(\n         collected_tensors = {target_key: collected_tensors[self.source_patterns[0]]}\n \n         if hf_quantizer is not None and self.quantization_operation is not None:\n-            with log_to_misc(layer_name, misc, (len(collected_tensors), layer_name), self.quantization_operation):\n+            with log_conversion_errors(\n+                layer_name, conversion_errors, (len(collected_tensors), layer_name), self.quantization_operation\n+            ):\n                 collected_tensors = self.quantization_operation.convert(\n                     collected_tensors,\n                     source_patterns=self.source_patterns,\n@@ -432,7 +434,7 @@ def convert(\n                     missing_keys=missing_keys,\n                 )\n \n-        return collected_tensors, misc\n+        return collected_tensors, conversion_errors\n \n \n @dataclass(slots=True)\n@@ -455,14 +457,14 @@ def convert(\n         config=None,\n         hf_quantizer=None,\n         missing_keys: Optional[MutableSet[str]] = None,\n-        misc: Optional[MutableMapping[str, str]] = None,\n+        conversion_errors: Optional[MutableMapping[str, str]] = None,\n     ):\n         # Collect the tensors here - we use a new dictionary to avoid keeping them in memory in the internal\n         # attribute during the whole process\n         collected_tensors = self.materialize_tensors()\n \n         for op in self.operations:\n-            with log_to_misc(layer_name, misc, (len(collected_tensors), layer_name), op):\n+            with log_conversion_errors(layer_name, conversion_errors, (len(collected_tensors), layer_name), op):\n                 collected_tensors = op.convert(\n                     collected_tensors,\n                     source_patterns=self.source_patterns,\n@@ -489,7 +491,9 @@ def convert(\n             pass\n \n         if hf_quantizer is not None and self.quantization_operation is not None:\n-            with log_to_misc(layer_name, misc, (len(collected_tensors), layer_name), self.quantization_operation):\n+            with log_conversion_errors(\n+                layer_name, conversion_errors, (len(collected_tensors), layer_name), self.quantization_operation\n+            ):\n                 collected_tensors = self.quantization_operation.convert(\n                     collected_tensors,\n                     source_patterns=self.source_patterns,\n@@ -499,7 +503,7 @@ def convert(\n                     model=model,\n                     missing_keys=missing_keys,\n                 )\n-        return collected_tensors, misc\n+        return collected_tensors, conversion_errors\n \n \n # For I/O bound operations (i.e. here reading files), it is better to have fewer threads, e.g. 4 is a good default.\n@@ -560,13 +564,14 @@ def dot_natural_key(s: str):\n \n \n @contextmanager\n-def log_to_misc(\n+def log_conversion_errors(\n     first_target_key: str,\n-    misc: MutableMapping[str, str],\n+    conversion_errors: MutableMapping[str, str],\n     extras: Any = None,\n     op: Union[list[ConversionOps], ConversionOps, None] = None,\n ):\n-    # A simple helper to handle errors with contextual messages.\n+    \"\"\"Catch all exceptions during `convert` calls, and log the errors for later. Re-raise a `SkipParameters` exception\n+    that will be catched later to skip the parameters that raised the original Exception.\"\"\"\n     try:\n         yield\n     except Exception as e:\n@@ -585,17 +590,19 @@ def _format_op_name(curr_op: Union[list[ConversionOps], ConversionOps, None]) ->\n         if isinstance(extras, tuple) and len(extras) == 2:\n             length, target_keys = extras\n             descriptor = f\"{op_name} \" if op_name else \"\"\n-            misc[first_target_key] = (\n+            conversion_errors[first_target_key] = (\n                 f\"{e}\\nError: {descriptor}on tensors destined for {target_keys}. Ckpt contains: {length}\"\n             )\n         elif isinstance(extras, str):\n             suffix = f\" via {op_name}\" if op_name else \"\"\n-            misc[first_target_key] = f\"{e}\\nError{suffix} when processing parameter {extras}\"\n+            conversion_errors[first_target_key] = f\"{e}\\nError{suffix} when processing parameter {extras}\"\n         elif extras is None and op_name:\n-            misc[first_target_key] = f\"{op_name}: {e}\"\n+            conversion_errors[first_target_key] = f\"{op_name}: {e}\"\n         else:\n-            misc[first_target_key] = f\"{extras} |Error: {e}\"\n-        raise SkipLayer()\n+            conversion_errors[first_target_key] = f\"{extras} |Error: {e}\"\n+\n+        # Raise a specific Exception that we can catch easily\n+        raise SkipParameters()\n \n \n def set_param_for_module(\n@@ -604,44 +611,42 @@ def set_param_for_module(\n     param_value: torch.Tensor,\n     mismatch_keys: MutableSet[tuple[str, torch.Size, torch.Size]],\n     missing_keys: MutableSet[str],\n-    misc: MutableMapping[str, Any],\n     unexpected_keys: MutableSet[str],\n     distributed_operation: Optional[TensorParallelLayer],\n     hf_quantizer: HfQuantizer,\n ):\n-    with log_to_misc(target_name, misc, target_name):\n-        module_path, _, param_name = target_name.rpartition(\".\")\n-        module_obj = model.get_submodule(module_path) if module_path else model\n+    module_path, _, param_name = target_name.rpartition(\".\")\n+    module_obj = model.get_submodule(module_path) if module_path else model\n \n-        ref = getattr(module_obj, param_name)\n-        if ref is None:\n-            unexpected_keys.add(target_name)\n+    ref = getattr(module_obj, param_name)\n+    if ref is None:\n+        unexpected_keys.add(target_name)\n+    else:\n+        use_dtensor = hasattr(distributed_operation, \"use_dtensor\") and distributed_operation.use_dtensor\n+        if not isinstance(param_value, torch.nn.Parameter):\n+            if distributed_operation is not None:\n+                param_value = DTensor.from_local(\n+                    param_value,\n+                    distributed_operation.device_mesh,\n+                    getattr(distributed_operation, \"shard\", Replicate()),\n+                    run_check=False,\n+                    shape=ref.size(),\n+                    stride=ref.stride(),\n+                )\n+                if not use_dtensor:\n+                    # we convert to local\n+                    param_value = param_value.to_local()\n+            if param_name not in module_obj._buffers:\n+                param_value = torch.nn.Parameter(param_value, requires_grad=param_value.is_floating_point())\n+\n+        # Remove from missing keys (it's either mismatched, or all good)\n+        missing_keys.discard(target_name)\n+        if ref is not None and ref.shape != param_value.shape and hf_quantizer is None:\n+            mismatch_keys.add((target_name, param_value.shape, ref.shape))\n         else:\n-            use_dtensor = hasattr(distributed_operation, \"use_dtensor\") and distributed_operation.use_dtensor\n-            if not isinstance(param_value, torch.nn.Parameter):\n-                if distributed_operation is not None:\n-                    param_value = DTensor.from_local(\n-                        param_value,\n-                        distributed_operation.device_mesh,\n-                        getattr(distributed_operation, \"shard\", Replicate()),\n-                        run_check=False,\n-                        shape=ref.size(),\n-                        stride=ref.stride(),\n-                    )\n-                    if not use_dtensor:\n-                        # we convert to local\n-                        param_value = param_value.to_local()\n-                if param_name not in module_obj._buffers:\n-                    param_value = torch.nn.Parameter(param_value, requires_grad=param_value.is_floating_point())\n-\n-            # Remove from missing keys (it's either mismatched, or all good)\n-            missing_keys.discard(target_name)\n-            if ref is not None and ref.shape != param_value.shape and hf_quantizer is None:\n-                mismatch_keys.add((target_name, param_value.shape, ref.shape))\n-            else:\n-                # super important otherwise _init_weight will re-init the param\n-                param_value._is_hf_initialized = True\n-                setattr(module_obj, param_name, param_value)\n+            # super important otherwise _init_weight will re-init the param\n+            param_value._is_hf_initialized = True\n+            setattr(module_obj, param_name, param_value)\n \n \n def offload_and_maybe_resave_param(\n@@ -663,8 +668,9 @@ def offload_and_maybe_resave_param(\n     return disk_offload_index\n \n \n-class SkipLayer(Exception):\n-    \"\"\"Control-flow sentinel: abort processing of the current layer only.\"\"\"\n+class SkipParameters(Exception):\n+    \"\"\"Control-flow sentinel: abort processing of the current parameters only (that were supposed to be created\n+    by a WeightConverter).\"\"\"\n \n     pass\n \n@@ -818,7 +824,7 @@ def convert_and_load_state_dict_in_model(\n     meta_model_state_dict = model.state_dict()\n     missing_keys = set(meta_model_state_dict.keys())\n \n-    misc = {}\n+    conversion_errors = {}\n     mismatch_keys = set()\n     unexpected_keys = set()\n \n@@ -925,13 +931,13 @@ def convert_and_load_state_dict_in_model(\n                 pbar.set_postfix({\"Materializing param\": first_param_name})\n                 pbar.refresh()\n                 try:\n-                    realized_value, misc = mapping.convert(\n+                    realized_value, conversion_errors = mapping.convert(\n                         first_param_name,\n                         model=model,\n                         config=model.config,\n                         hf_quantizer=hf_quantizer,\n                         missing_keys=missing_keys,\n-                        misc=misc,\n+                        conversion_errors=conversion_errors,\n                     )\n                     for target_name, param in realized_value.items():\n                         param = param[0] if isinstance(param, list) else param\n@@ -949,7 +955,6 @@ def convert_and_load_state_dict_in_model(\n                                 param,\n                                 mismatch_keys,\n                                 missing_keys,\n-                                misc,\n                                 unexpected_keys,\n                                 mapping.distributed_operation,\n                                 hf_quantizer,\n@@ -958,7 +963,7 @@ def convert_and_load_state_dict_in_model(\n                     # Cleanup all the tensors that were gathered before next iteration\n                     del realized_value\n \n-                except SkipLayer:\n+                except SkipParameters:\n                     continue\n \n     # Close the pool, independently of whether the code was interrupted or finished successfully\n@@ -969,7 +974,7 @@ def convert_and_load_state_dict_in_model(\n \n     # Keep the current weight conversion mapping for later saving (in case it was coming directly from the user)\n     model._weight_conversions = weight_mapping\n-    return missing_keys, unexpected_keys, mismatch_keys, disk_offload_index, misc\n+    return missing_keys, unexpected_keys, mismatch_keys, disk_offload_index, conversion_errors\n \n \n def revert_weight_conversion(model: PreTrainedModel, state_dict: dict[str, torch.Tensor]):\n@@ -1016,7 +1021,7 @@ def revert_weight_conversion(model: PreTrainedModel, state_dict: dict[str, torch\n     new_state_dict = {}\n     for first_param_name, reversed_converter in conversion_mapping.items():\n         # Apply the reverse converter\n-        realized_value, misc = reversed_converter.convert(first_param_name, model=model, config=model.config)\n+        realized_value, _ = reversed_converter.convert(first_param_name, model=model, config=model.config)\n         for target_name, param in realized_value.items():\n             param = param[0] if isinstance(param, list) else param\n             new_state_dict[target_name] = param"
        },
        {
            "sha": "47430ba4c3b93c9fd4fd6a126b55e69d5f94ef64",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/464dfa04460cf2c2b3fad6ebca6f11b81f663d4f/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/464dfa04460cf2c2b3fad6ebca6f11b81f663d4f/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=464dfa04460cf2c2b3fad6ebca6f11b81f663d4f",
            "patch": "@@ -4102,7 +4102,7 @@ def _load_pretrained_model(\n                 state_dict = merged_state_dict\n             error_msgs += _load_state_dict_into_zero3_model(model, state_dict)\n             # This is not true but for now we assume only best-case scenario with deepspeed, i.e. perfectly matching checkpoints\n-            missing_keys, unexpected_keys, mismatched_keys, misc = set(), set(), set(), set()\n+            missing_keys, unexpected_keys, mismatched_keys, conversion_errors = set(), set(), set(), set()\n         else:\n             all_pointer = set()\n             # Checkpoints are safetensors\n@@ -4124,7 +4124,7 @@ def _load_pretrained_model(\n             else:\n                 raise ValueError(\"Neither a state dict nor checkpoint files were found.\")\n \n-            missing_keys, unexpected_keys, mismatched_keys, disk_offload_index, misc = (\n+            missing_keys, unexpected_keys, mismatched_keys, disk_offload_index, conversion_errors = (\n                 convert_and_load_state_dict_in_model(\n                     model,\n                     merged_state_dict,\n@@ -4198,7 +4198,7 @@ def _load_pretrained_model(\n             missing_keys=missing_keys,\n             mismatched_keys=mismatched_keys,\n             mismatched_shapes=mismatched_keys,\n-            misc=misc,\n+            conversion_errors=conversion_errors,\n             ignore_mismatched_sizes=ignore_mismatched_sizes,\n         )\n "
        },
        {
            "sha": "b156be562a58acdb8e5c098546447df0bcd465eb",
            "filename": "src/transformers/utils/loading_report.py",
            "status": "modified",
            "additions": 19,
            "deletions": 10,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/464dfa04460cf2c2b3fad6ebca6f11b81f663d4f/src%2Ftransformers%2Futils%2Floading_report.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/464dfa04460cf2c2b3fad6ebca6f11b81f663d4f/src%2Ftransformers%2Futils%2Floading_report.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Floading_report.py?ref=464dfa04460cf2c2b3fad6ebca6f11b81f663d4f",
            "patch": "@@ -148,9 +148,8 @@ def log_state_dict_report(\n     mismatched_keys=None,\n     mismatched_shapes=None,\n     ignore_mismatched_sizes=True,\n-    misc=None,\n+    conversion_errors=None,\n     color=True,  # allow disabling for plain logs\n-    min_width_full_table=60,  # terminal min width to attempt full table\n ):\n     \"\"\"Log a readable report about state_dict loading issues.\n \n@@ -165,12 +164,13 @@ def log_state_dict_report(\n     missing_keys = missing_keys or []\n     mismatched_keys = mismatched_keys or []\n     mismatched_shapes = mismatched_shapes or []\n-    misc = misc or {}\n+    conversion_errors = conversion_errors or {}\n \n     # Detect whether the current stdout supports ANSI colors; allow callers to pass `color=False` to force no color\n     color_enabled = bool(color and sys.stdout.isatty())\n     ansi = ANSI(color_enabled)\n \n+    # Re-raise errors early if needed\n     if error_msgs:\n         error_msg = \"\\n\\t\".join(error_msgs)\n         if \"size mismatch\" in error_msg:\n@@ -204,9 +204,9 @@ def log_state_dict_report(\n             )\n             rows.append(data)\n \n-    if misc:\n-        for k, v in update_key_name(misc).items():\n-            status = \"MISC\"\n+    if conversion_errors:\n+        for k, v in update_key_name(conversion_errors).items():\n+            status = \"CONVERSION\"\n             status = _color(status, \"purple\", ansi)\n             _details = v[:term_w]\n             rows.append([k, status, _details])\n@@ -228,16 +228,25 @@ def log_state_dict_report(\n     if unexpected_keys:\n         tips += f\"\\n- {_color('UNEXPECTED', 'orange', ansi) + ansi['italic']}\\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\"\n     if missing_keys:\n-        tips += f\"\\n- {_color('MISSING', 'red', ansi) + ansi['italic']}\\t:those params were newly initialized because missing form the checkpoint. Consider training on your downstream task.\"\n+        tips += f\"\\n- {_color('MISSING', 'red', ansi) + ansi['italic']}\\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\"\n     if mismatched_keys:\n-        tips += f\"\\n- {_color('MISMATCH', 'yellow', ansi) + ansi['italic']}\\t:ckpt weights were loaded, but they did not match the original empty weight.\"\n-    if misc:\n-        tips += f\"\\n- {_color('MISC', 'purple', ansi) + ansi['italic']}\\t:originate from the conversion scheme\"\n+        tips += f\"\\n- {_color('MISMATCH', 'yellow', ansi) + ansi['italic']}\\t:ckpt weights were loaded, but they did not match the original empty weight shapes.\"\n+    if conversion_errors:\n+        tips += f\"\\n- {_color('CONVERSION', 'purple', ansi) + ansi['italic']}\\t:originate from the conversion scheme\"\n     tips += f\"{ansi['reset']}\"\n \n+    # Log the report as warning\n     logger.warning(prelude + table + tips)\n+\n+    # Re-raise in those case, after the report\n+    if conversion_errors:\n+        raise RuntimeError(\n+            \"We encountered some issues during automatic conversion of the weights. For details look at the `CONVERSION` entries of \"\n+            \"the above report!\"\n+        )\n     if not ignore_mismatched_sizes and mismatched_keys:\n         raise RuntimeError(\n             \"You set `ignore_mismatched_sizes` to `False`, thus raising an error. For details look at the above report!\"\n         )\n+\n     return prelude + table + tips"
        },
        {
            "sha": "08f7b46e7b856bdbd71ff6ac65c589adcfde4ac6",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 26,
            "deletions": 4,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/464dfa04460cf2c2b3fad6ebca6f11b81f663d4f/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/464dfa04460cf2c2b3fad6ebca6f11b81f663d4f/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=464dfa04460cf2c2b3fad6ebca6f11b81f663d4f",
            "patch": "@@ -114,22 +114,22 @@\n         BertModel,\n         CLIPTextModel,\n         GenerationMixin,\n+        MixtralConfig,\n+        MixtralModel,\n         MusicgenConfig,\n         MusicgenForConditionalGeneration,\n         PreTrainedModel,\n         T5Config,\n         T5ForConditionalGeneration,\n     )\n+    from transformers.conversion_mapping import MergeModulelist, WeightConverter, get_model_conversion_mapping\n     from transformers.modeling_attn_mask_utils import (\n         AttentionMaskConverter,\n         _create_4d_causal_attention_mask,\n         _prepare_4d_attention_mask,\n         _prepare_4d_causal_attention_mask,\n     )\n-    from transformers.modeling_utils import (\n-        _find_disjoint,\n-        _find_identical,\n-    )\n+    from transformers.modeling_utils import _find_disjoint, _find_identical\n     from transformers.pytorch_utils import isin_mps_friendly\n \n     # Fake pretrained models for tests\n@@ -2221,6 +2221,28 @@ def tracking_init(self, *args, **kwargs):\n         # Reverse monkey patch\n         threading.Thread.__init__ = original_init\n \n+    def test_error_in_weight_conversion_is_raised(self):\n+        \"\"\"Test that errors in `ConversionOps` are correctly re-raised after loading.\"\"\"\n+        small_config = MixtralConfig(num_hidden_layers=2, hidden_size=32, intermediate_size=32, num_attention_heads=8)\n+        model = MixtralModel(small_config)\n+        weight_conversions = get_model_conversion_mapping(model)\n+        converters = [conversion for conversion in weight_conversions if isinstance(conversion, WeightConverter)]\n+        # Just a safeguard\n+        self.assertTrue(\n+            any(isinstance(ops, MergeModulelist) for converter in converters for ops in converter.operations),\n+            \"The test is useless without conversions on the model\",\n+        )\n+\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            model.save_pretrained(tmpdirname)\n+            # Now try to reload while mocking the WeightConversion to raise\n+            with patch.object(MergeModulelist, \"convert\", side_effect=Exception(\"failed\")):\n+                # It should raise the proper error\n+                with self.assertRaisesRegex(\n+                    RuntimeError, \"We encountered some issues during automatic conversion of the weights.\"\n+                ):\n+                    _ = MixtralModel.from_pretrained(tmpdirname)\n+\n \n @slow\n @require_torch"
        }
    ],
    "stats": {
        "total": 182,
        "additions": 109,
        "deletions": 73
    }
}