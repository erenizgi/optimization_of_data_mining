{
    "author": "cyyever",
    "message": "Fix typos in src (#40782)\n\nFix typoes in src\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "895b3ebe418ebcf6a37fc838ff0effdb69d98386",
    "files": [
        {
            "sha": "e858a9813cea4a2b923e17d181c966da09e2b46f",
            "filename": "src/transformers/generation/flax_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fgeneration%2Fflax_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fgeneration%2Fflax_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fflax_utils.py?ref=895b3ebe418ebcf6a37fc838ff0effdb69d98386",
            "patch": "@@ -221,7 +221,7 @@ def _expand_to_num_beams(tensor, num_beams):\n     def _adapt_logits_for_beam_search(self, logits):\n         \"\"\"\n         This function can be overwritten in the specific modeling_flax_<model-name>.py classes to allow for custom beam\n-        search behavior. Note that the only model that overwrites this method is [`~transformes.FlaxMarianMTModel`].\n+        search behavior. Note that the only model that overwrites this method is [`~transformers.FlaxMarianMTModel`].\n         \"\"\"\n         return logits\n "
        },
        {
            "sha": "22944e97044659f896451936c6253d5aadd7a769",
            "filename": "src/transformers/kernels/yoso/fast_lsh_cumulation_cuda.cu",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fkernels%2Fyoso%2Ffast_lsh_cumulation_cuda.cu",
            "raw_url": "https://github.com/huggingface/transformers/raw/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fkernels%2Fyoso%2Ffast_lsh_cumulation_cuda.cu",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Fyoso%2Ffast_lsh_cumulation_cuda.cu?ref=895b3ebe418ebcf6a37fc838ff0effdb69d98386",
            "patch": "@@ -779,12 +779,12 @@ __global__ void lsh_weighted_cumulation_ver4_step2_cuda_kernel(\n \n     __syncthreads();\n \n-    int num_distint_query = query_counter[0];\n+    int num_distinct_query = query_counter[0];\n \n-    if (num_distint_query > 0) {\n-      for (int idx_base = 0; idx_base < num_distint_query; idx_base = idx_base + num_warps) {\n+    if (num_distinct_query > 0) {\n+      for (int idx_base = 0; idx_base < num_distinct_query; idx_base = idx_base + num_warps) {\n         int idx = idx_base + warp_idx;\n-        if (idx < num_distint_query) {\n+        if (idx < num_distinct_query) {\n           int query_idx = inserted_query[idx];\n           int batch_idx__query_idx = batch_idx * num_query + query_idx;\n \n@@ -813,7 +813,7 @@ __global__ void lsh_weighted_cumulation_ver4_step2_cuda_kernel(\n       }\n     } else {\n \n-      // all computation is completed if num_distint_query == 0\n+      // all computation is completed if num_distinct_query == 0\n       break;\n \n     }"
        },
        {
            "sha": "9d81a26581dd35dcfcc06b0f1881640acd47a070",
            "filename": "src/transformers/models/clap/modeling_clap.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py?ref=895b3ebe418ebcf6a37fc838ff0effdb69d98386",
            "patch": "@@ -1717,7 +1717,7 @@ def forward(\n         >>> model = ClapModel.from_pretrained(\"laion/clap-htsat-unfused\")\n         >>> processor = AutoProcessor.from_pretrained(\"laion/clap-htsat-unfused\")\n \n-        >>> input_text = [\"Sound of a dog\", \"Sound of vaccum cleaner\"]\n+        >>> input_text = [\"Sound of a dog\", \"Sound of vacuum cleaner\"]\n \n         >>> inputs = processor(text=input_text, audios=audio_sample, return_tensors=\"pt\", padding=True)\n "
        },
        {
            "sha": "5938aebd1ff5f54c44f02bd73bf517252f291cb7",
            "filename": "src/transformers/models/clipseg/modeling_clipseg.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py?ref=895b3ebe418ebcf6a37fc838ff0effdb69d98386",
            "patch": "@@ -625,7 +625,7 @@ def forward(\n                 input_ids.to(dtype=torch.int, device=last_hidden_state.device).argmax(dim=-1),\n             ]\n         else:\n-            # The config gets updated `eos_token_id` from PR #24773 (so the use of exta new tokens is possible)\n+            # The config gets updated `eos_token_id` from PR #24773 (so the use of extra new tokens is possible)\n             pooled_output = last_hidden_state[\n                 torch.arange(last_hidden_state.shape[0], device=last_hidden_state.device),\n                 # We need to get the first position of `eos_token_id` value (`pad_token_ids` might equal to `eos_token_id`)"
        },
        {
            "sha": "1a9444cbf9db9e4e2872e3f11ec0710bb137e480",
            "filename": "src/transformers/models/deepseek_vl/image_processing_deepseek_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl.py?ref=895b3ebe418ebcf6a37fc838ff0effdb69d98386",
            "patch": "@@ -355,7 +355,7 @@ def pad_to_square(\n             background_color (`int` or `tuple[int, int, int]`, *optional*, defaults to 0):\n                 The color to use for the padding. Can be an integer for single channel or a\n                 tuple of integers representing for multi-channel images. If passed as integer\n-                in mutli-channel mode, it will default to `0` in subsequent channels.\n+                in multi-channel mode, it will default to `0` in subsequent channels.\n             data_format (`str` or `ChannelDimension`, *optional*):\n                 The channel dimension format for the output image. Can be one of:\n                     - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format."
        },
        {
            "sha": "2204606d42113e612f9eeea53ab00222e2371c89",
            "filename": "src/transformers/models/deepseek_vl/image_processing_deepseek_vl_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl_fast.py?ref=895b3ebe418ebcf6a37fc838ff0effdb69d98386",
            "patch": "@@ -113,7 +113,7 @@ def pad_to_square(\n             background_color (`int` or `tuple[int, int, int]`, *optional*, defaults to 0):\n                 The color to use for the padding. Can be an integer for single channel or a\n                 tuple of integers representing for multi-channel images. If passed as integer\n-                in mutli-channel mode, it will default to `0` in subsequent channels.\n+                in multi-channel mode, it will default to `0` in subsequent channels.\n \n         Returns:\n             `torch.Tensor`: The padded images."
        },
        {
            "sha": "45e19da0d14cabc29c565da735aba6f90e1a4140",
            "filename": "src/transformers/models/deepseek_vl_hybrid/image_processing_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py?ref=895b3ebe418ebcf6a37fc838ff0effdb69d98386",
            "patch": "@@ -428,7 +428,7 @@ def pad_to_square(\n             background_color (`int` or `tuple[int, int, int]`, *optional*, defaults to 0):\n                 The color to use for the padding. Can be an integer for single channel or a\n                 tuple of integers representing for multi-channel images. If passed as integer\n-                in mutli-channel mode, it will default to `0` in subsequent channels.\n+                in multi-channel mode, it will default to `0` in subsequent channels.\n             data_format (`str` or `ChannelDimension`, *optional*):\n                 The channel dimension format for the output image. Can be one of:\n                     - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format."
        },
        {
            "sha": "d55610331f3022df67c0b3d9adfd52d543ee35d4",
            "filename": "src/transformers/models/deepseek_vl_hybrid/image_processing_deepseek_vl_hybrid_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py?ref=895b3ebe418ebcf6a37fc838ff0effdb69d98386",
            "patch": "@@ -147,7 +147,7 @@ def pad_to_square(\n             background_color (`int` or `tuple[int, int, int]`, *optional*, defaults to 0):\n                 The color to use for the padding. Can be an integer for single channel or a\n                 tuple of integers representing for multi-channel images. If passed as integer\n-                in mutli-channel mode, it will default to `0` in subsequent channels.\n+                in multi-channel mode, it will default to `0` in subsequent channels.\n \n         Returns:\n             `torch.Tensor`: The padded images."
        },
        {
            "sha": "9300245ca7f61660d5fbb209be322a57dc3fda54",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/modeling_gptsan_japanese.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py?ref=895b3ebe418ebcf6a37fc838ff0effdb69d98386",
            "patch": "@@ -75,9 +75,9 @@ def load_balancing_loss_func(router_probs: torch.Tensor, expert_indices: torch.T\n \n     Args:\n         router_probs (`torch.Tensor`):\n-            Probability assigned to each expert per token. Shape: [batch_size, seqeunce_length, num_experts].\n+            Probability assigned to each expert per token. Shape: [batch_size, sequence_length, num_experts].\n         expert_indices (`torch.Tensor`):\n-            Indices tensor of shape [batch_size, seqeunce_length] identifying the selected expert for a given token.\n+            Indices tensor of shape [batch_size, sequence_length] identifying the selected expert for a given token.\n \n     Returns:\n         The auxiliary loss."
        },
        {
            "sha": "1387127b4cf00b9a2fb3cf1f1d5ae01fac75526d",
            "filename": "src/transformers/models/dpt/image_processing_dpt_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt_fast.py?ref=895b3ebe418ebcf6a37fc838ff0effdb69d98386",
            "patch": "@@ -62,7 +62,7 @@\n class DPTFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     \"\"\"\n     ensure_multiple_of (`int`, *optional*, defaults to 1):\n-        If `do_resize` is `True`, the image is resized to a size that is a multiple of this value. Can be overidden\n+        If `do_resize` is `True`, the image is resized to a size that is a multiple of this value. Can be overridden\n         by `ensure_multiple_of` in `preprocess`.\n     do_pad (`bool`, *optional*, defaults to `False`):\n         Whether to apply center padding. This was introduced in the DINOv2 paper, which uses the model in\n@@ -72,7 +72,7 @@ class DPTFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n         DINOv2 paper, which uses the model in combination with DPT.\n     keep_aspect_ratio (`bool`, *optional*, defaults to `False`):\n         If `True`, the image is resized to the largest possible size such that the aspect ratio is preserved. Can\n-        be overidden by `keep_aspect_ratio` in `preprocess`.\n+        be overridden by `keep_aspect_ratio` in `preprocess`.\n     do_reduce_labels (`bool`, *optional*, defaults to `self.do_reduce_labels`):\n         Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0\n         is used for background, and background itself is not included in all classes of a dataset (e.g."
        },
        {
            "sha": "f86b5601dada077b4b59295fdc5ffab572b2fa57",
            "filename": "src/transformers/models/dpt/modular_dpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodular_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodular_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodular_dpt.py?ref=895b3ebe418ebcf6a37fc838ff0effdb69d98386",
            "patch": "@@ -92,7 +92,7 @@ def constrain_to_multiple_of(val, multiple, min_val=0, max_val=None):\n class DPTFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     \"\"\"\n     ensure_multiple_of (`int`, *optional*, defaults to 1):\n-        If `do_resize` is `True`, the image is resized to a size that is a multiple of this value. Can be overidden\n+        If `do_resize` is `True`, the image is resized to a size that is a multiple of this value. Can be overridden\n         by `ensure_multiple_of` in `preprocess`.\n     do_pad (`bool`, *optional*, defaults to `False`):\n         Whether to apply center padding. This was introduced in the DINOv2 paper, which uses the model in\n@@ -102,7 +102,7 @@ class DPTFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n         DINOv2 paper, which uses the model in combination with DPT.\n     keep_aspect_ratio (`bool`, *optional*, defaults to `False`):\n         If `True`, the image is resized to the largest possible size such that the aspect ratio is preserved. Can\n-        be overidden by `keep_aspect_ratio` in `preprocess`.\n+        be overridden by `keep_aspect_ratio` in `preprocess`.\n     do_reduce_labels (`bool`, *optional*, defaults to `self.do_reduce_labels`):\n         Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0\n         is used for background, and background itself is not included in all classes of a dataset (e.g."
        },
        {
            "sha": "d2dff4de3745aa322357252ffa46b9dbfd13a0d8",
            "filename": "src/transformers/models/efficientloftr/configuration_efficientloftr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fconfiguration_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fconfiguration_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fconfiguration_efficientloftr.py?ref=895b3ebe418ebcf6a37fc838ff0effdb69d98386",
            "patch": "@@ -19,7 +19,7 @@\n \n class EfficientLoFTRConfig(PretrainedConfig):\n     r\"\"\"\n-    This is the configuration class to store the configuration of a [`EffientLoFTRFromKeypointMatching`].\n+    This is the configuration class to store the configuration of a [`EfficientLoFTRFromKeypointMatching`].\n     It is used to instantiate a EfficientLoFTR model according to the specified arguments, defining the model\n     architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the\n     EfficientLoFTR [zju-community/efficientloftr](https://huggingface.co/zju-community/efficientloftr) architecture."
        },
        {
            "sha": "ec1abec38172b67bd656836622e4af0d556d59f1",
            "filename": "src/transformers/models/glm4v/convert_glm4v_mgt_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconvert_glm4v_mgt_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconvert_glm4v_mgt_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconvert_glm4v_mgt_weights_to_hf.py?ref=895b3ebe418ebcf6a37fc838ff0effdb69d98386",
            "patch": "@@ -417,7 +417,7 @@ def merge_tp_weights(model_path, output_path, vllm_config_path=None):\n             )\n             layer_i += 1\n \n-    # Embedd Model, LM Head, and Norm\n+    # Embedded Model, LM Head, and Norm\n     embed_tokens = merge_tensors(\n         tp_sd=mgt_sd[0],\n         keys=[\"model\", \"embedding.word_embeddings.weight\"],"
        },
        {
            "sha": "9ea10095eec8be11b1ad20a69750c1fb1787a85c",
            "filename": "src/transformers/models/groupvit/modeling_groupvit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py?ref=895b3ebe418ebcf6a37fc838ff0effdb69d98386",
            "patch": "@@ -999,7 +999,7 @@ def forward(\n                 input_ids.to(dtype=torch.int, device=last_hidden_state.device).argmax(dim=-1),\n             ]\n         else:\n-            # The config gets updated `eos_token_id` from PR #24773 (so the use of exta new tokens is possible)\n+            # The config gets updated `eos_token_id` from PR #24773 (so the use of extra new tokens is possible)\n             pooled_output = last_hidden_state[\n                 torch.arange(last_hidden_state.shape[0], device=last_hidden_state.device),\n                 # We need to get the first position of `eos_token_id` value (`pad_token_ids` might equal to `eos_token_id`)"
        },
        {
            "sha": "3669e707928b281e0d43fe02bbeea9d836bc03df",
            "filename": "src/transformers/models/janus/image_processing_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py?ref=895b3ebe418ebcf6a37fc838ff0effdb69d98386",
            "patch": "@@ -352,7 +352,7 @@ def pad_to_square(\n             background_color (`int` or `tuple[int, int, int]`, *optional*, defaults to 0):\n                 The color to use for the padding. Can be an integer for single channel or a\n                 tuple of integers representing for multi-channel images. If passed as integer\n-                in mutli-channel mode, it will default to `0` in subsequent channels.\n+                in multi-channel mode, it will default to `0` in subsequent channels.\n             data_format (`str` or `ChannelDimension`, *optional*):\n                 The channel dimension format for the output image. Can be one of:\n                     - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format."
        },
        {
            "sha": "eedf18e2c19fe098f0f709a154a5778a9be70bf8",
            "filename": "src/transformers/models/janus/image_processing_janus_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus_fast.py?ref=895b3ebe418ebcf6a37fc838ff0effdb69d98386",
            "patch": "@@ -119,7 +119,7 @@ def pad_to_square(\n             background_color (`int` or `tuple[int, int, int]`, *optional*, defaults to 0):\n                 The color to use for the padding. Can be an integer for single channel or a\n                 tuple of integers representing for multi-channel images. If passed as integer\n-                in mutli-channel mode, it will default to `0` in subsequent channels.\n+                in multi-channel mode, it will default to `0` in subsequent channels.\n \n         Returns:\n             `torch.Tensor`: The padded images."
        },
        {
            "sha": "261e994262aa7af1ee13b47f111e0a83ca3c1966",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=895b3ebe418ebcf6a37fc838ff0effdb69d98386",
            "patch": "@@ -1368,7 +1368,7 @@ def pad_to_square(\n             background_color (`int` or `tuple[int, int, int]`, *optional*, defaults to 0):\n                 The color to use for the padding. Can be an integer for single channel or a\n                 tuple of integers representing for multi-channel images. If passed as integer\n-                in mutli-channel mode, it will default to `0` in subsequent channels.\n+                in multi-channel mode, it will default to `0` in subsequent channels.\n             data_format (`str` or `ChannelDimension`, *optional*):\n                 The channel dimension format for the output image. Can be one of:\n                     - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format."
        },
        {
            "sha": "c53ce475f9e05f3e5730280677c17466d884aca3",
            "filename": "src/transformers/models/mimi/configuration_mimi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fmimi%2Fconfiguration_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fmimi%2Fconfiguration_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fconfiguration_mimi.py?ref=895b3ebe418ebcf6a37fc838ff0effdb69d98386",
            "patch": "@@ -120,7 +120,7 @@ class MimiConfig(PretrainedConfig):\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n         layer_scale_initial_scale (`float`, *optional*, defaults to 0.01):\n-            Initiale scale of the residual rescaling operation done in the Transformer models.\n+            Initial scale of the residual rescaling operation done in the Transformer models.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n     Example:"
        },
        {
            "sha": "d5a5b2188cf5ca0f33b18d35e3f14b46106bb7fc",
            "filename": "src/transformers/models/mvp/modeling_mvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py?ref=895b3ebe418ebcf6a37fc838ff0effdb69d98386",
            "patch": "@@ -246,7 +246,7 @@ def forward(\n         attn_output = attn_output.transpose(1, 2)\n \n         # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned aross GPUs when using tensor-parallelism.\n+        # partitioned across GPUs when using tensor-parallelism.\n         attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n \n         attn_output = self.out_proj(attn_output)"
        },
        {
            "sha": "81c7a088df9fea103333e2242c597ccd125658fc",
            "filename": "src/transformers/models/owlv2/modeling_owlv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py?ref=895b3ebe418ebcf6a37fc838ff0effdb69d98386",
            "patch": "@@ -461,7 +461,7 @@ def forward(\n         attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n \n         if output_attentions:\n-            # this operation is a bit akward, but it's required to\n+            # this operation is a bit awkward, but it's required to\n             # make sure that attn_weights keeps its gradient.\n             # In order to do so, attn_weights have to reshaped\n             # twice and have to be reused in the following"
        },
        {
            "sha": "107c8a9dab2fd49c676035c7b834be46c7a98320",
            "filename": "src/transformers/models/owlvit/modeling_owlvit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py?ref=895b3ebe418ebcf6a37fc838ff0effdb69d98386",
            "patch": "@@ -449,7 +449,7 @@ def forward(\n         attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n \n         if output_attentions:\n-            # this operation is a bit akward, but it's required to\n+            # this operation is a bit awkward, but it's required to\n             # make sure that attn_weights keeps its gradient.\n             # In order to do so, attn_weights have to reshaped\n             # twice and have to be reused in the following"
        },
        {
            "sha": "6292358575fd06e5594ba721f2c56d0975116cf3",
            "filename": "src/transformers/models/speecht5/modeling_speecht5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py?ref=895b3ebe418ebcf6a37fc838ff0effdb69d98386",
            "patch": "@@ -1000,7 +1000,7 @@ def forward(\n         attn_output = attn_output.transpose(1, 2)\n \n         # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned aross GPUs when using tensor-parallelism.\n+        # partitioned across GPUs when using tensor-parallelism.\n         attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n \n         attn_output = self.out_proj(attn_output)"
        },
        {
            "sha": "45eca357ffb536f1dc8f12a7fcc90e32cab94b96",
            "filename": "src/transformers/models/vits/modeling_vits.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fvits%2Fmodeling_vits.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fvits%2Fmodeling_vits.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvits%2Fmodeling_vits.py?ref=895b3ebe418ebcf6a37fc838ff0effdb69d98386",
            "patch": "@@ -962,7 +962,7 @@ def forward(\n         attn_output = attn_output.transpose(1, 2)\n \n         # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned aross GPUs when using tensor-parallelism.\n+        # partitioned across GPUs when using tensor-parallelism.\n         attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n \n         attn_output = self.out_proj(attn_output)"
        },
        {
            "sha": "bc5a396dcad4384a6f5c9ac7cc28642a02733f90",
            "filename": "src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_flax_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_flax_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_flax_wav2vec2.py?ref=895b3ebe418ebcf6a37fc838ff0effdb69d98386",
            "patch": "@@ -384,7 +384,7 @@ def setup(self):\n                 for i in range(self.config.num_feat_extract_layers)\n             ]\n         elif self.config.feat_extract_norm == \"group\":\n-            raise NotImplementedError(\"At the moment only ``config.feat_extact_norm == 'layer'`` is supported\")\n+            raise NotImplementedError(\"At the moment only ``config.feat_extract_norm == 'layer'`` is supported\")\n         else:\n             raise ValueError(\n                 f\"`config.feat_extract_norm` is {self.config.feat_extract_norm}, but has to be one of ['group',\""
        },
        {
            "sha": "a87ecafb684e271d49c341ee696df5e023b52f35",
            "filename": "src/transformers/pipelines/image_feature_extraction.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fpipelines%2Fimage_feature_extraction.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fpipelines%2Fimage_feature_extraction.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fimage_feature_extraction.py?ref=895b3ebe418ebcf6a37fc838ff0effdb69d98386",
            "patch": "@@ -32,7 +32,7 @@ class ImageFeatureExtractionPipeline(Pipeline):\n \n     >>> extractor = pipeline(model=\"google/vit-base-patch16-224\", task=\"image-feature-extraction\")\n     >>> result = extractor(\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\", return_tensors=True)\n-    >>> result.shape  # This is a tensor of shape [1, sequence_lenth, hidden_dimension] representing the input image.\n+    >>> result.shape  # This is a tensor of shape [1, sequence_length, hidden_dimension] representing the input image.\n     torch.Size([1, 197, 768])\n     ```\n "
        },
        {
            "sha": "7d703ba50117c5f3fce12ab08e38b742d0ce4178",
            "filename": "src/transformers/pipelines/text_generation.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fpipelines%2Ftext_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fpipelines%2Ftext_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftext_generation.py?ref=895b3ebe418ebcf6a37fc838ff0effdb69d98386",
            "patch": "@@ -483,16 +483,16 @@ def postprocess(\n         generated_sequence = generated_sequence.numpy().tolist()\n         records = []\n         other_outputs = model_outputs.get(\"additional_outputs\", {})\n-        splitted_keys = {}\n+        split_keys = {}\n         if other_outputs:\n             if self.framework == \"pt\":\n                 for k, v in other_outputs.items():\n                     if isinstance(v, torch.Tensor) and v.shape[0] == len(generated_sequence):\n-                        splitted_keys[k] = v.numpy().tolist()\n+                        split_keys[k] = v.numpy().tolist()\n             elif self.framework == \"tf\":\n                 for k, v in other_outputs.items():\n                     if isinstance(v, tf.Tensor) and v.shape[0] == len(generated_sequence):\n-                        splitted_keys[k] = v.numpy().tolist()\n+                        split_keys[k] = v.numpy().tolist()\n \n         skip_special_tokens = skip_special_tokens if skip_special_tokens is not None else True\n         for idx, sequence in enumerate(generated_sequence):\n@@ -539,7 +539,7 @@ def postprocess(\n                             # When we're not starting from a prefill, the output is a new assistant message\n                             all_text = list(prompt_text.messages) + [{\"role\": \"assistant\", \"content\": all_text}]\n                 record = {\"generated_text\": all_text}\n-                for key, values in splitted_keys.items():\n+                for key, values in split_keys.items():\n                     record[key] = values[idx]\n             records.append(record)\n "
        },
        {
            "sha": "9c21681a0d8ef1c93eb16bbb1740a509f091a0bb",
            "filename": "src/transformers/pipelines/zero_shot_audio_classification.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fpipelines%2Fzero_shot_audio_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Fpipelines%2Fzero_shot_audio_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fzero_shot_audio_classification.py?ref=895b3ebe418ebcf6a37fc838ff0effdb69d98386",
            "patch": "@@ -50,7 +50,7 @@ class ZeroShotAudioClassificationPipeline(Pipeline):\n     >>> audio = next(iter(dataset[\"train\"][\"audio\"]))[\"array\"]\n     >>> classifier = pipeline(task=\"zero-shot-audio-classification\", model=\"laion/clap-htsat-unfused\")\n     >>> classifier(audio, candidate_labels=[\"Sound of a dog\", \"Sound of vacuum cleaner\"])\n-    [{'score': 0.9996, 'label': 'Sound of a dog'}, {'score': 0.0004, 'label': 'Sound of vaccum cleaner'}]\n+    [{'score': 0.9996, 'label': 'Sound of a dog'}, {'score': 0.0004, 'label': 'Sound of vacuum cleaner'}]\n     ```\n \n "
        },
        {
            "sha": "7dac97831b3a316b9daf76c1753691bb8a18748a",
            "filename": "src/transformers/utils/fx.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Futils%2Ffx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/895b3ebe418ebcf6a37fc838ff0effdb69d98386/src%2Ftransformers%2Futils%2Ffx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Ffx.py?ref=895b3ebe418ebcf6a37fc838ff0effdb69d98386",
            "patch": "@@ -1346,7 +1346,7 @@ def to_meta(value):\n \n         return self.graph\n \n-    def _stateless_mod_instanciation_depends_on_proxies(self, mod: nn.Module) -> bool:\n+    def _stateless_mod_instantiation_depends_on_proxies(self, mod: nn.Module) -> bool:\n         \"\"\"\n         Whether the module was instantiated with Proxies. If that is the case, such module cannot be a leaf module\n         because its attributes are input-dependent.\n@@ -1359,7 +1359,7 @@ def _insert_module_as_submodule(self, mod: nn.Module) -> str:\n         \"\"\"\n         # If one of the module attributes is a Proxy, it means that its instantiation is input-dependent.\n         # It is not possible to insert such modules, those should be traced through.\n-        if self._stateless_mod_instanciation_depends_on_proxies(mod):\n+        if self._stateless_mod_instantiation_depends_on_proxies(mod):\n             return \"\"\n         idx = 0\n         mod_name = mod.__class__.__name__.lower()\n@@ -1395,7 +1395,7 @@ def path_of_module(self, mod: nn.Module) -> str:\n             raise e\n \n     def is_leaf_module(self, m: torch.nn.Module, module_qualified_name: str) -> bool:\n-        return (not self._stateless_mod_instanciation_depends_on_proxies(m)) and super().is_leaf_module(\n+        return (not self._stateless_mod_instantiation_depends_on_proxies(m)) and super().is_leaf_module(\n             m, module_qualified_name\n         )\n "
        }
    ],
    "stats": {
        "total": 80,
        "additions": 40,
        "deletions": 40
    }
}