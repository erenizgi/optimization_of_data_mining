{
    "author": "flukeskywalker",
    "message": "Fix mask handling for flex attention in llama/gemma2/mistral/qwen2 (#37381)\n\n* fix BlockMask handling when using flex_attention for llama/mistral/gemma2\n\n* fix attention_mask types\n\n* revert type hints and fixup\n\n* remove unnecessary assertion",
    "sha": "1efcfa9ca48e014e2261db1f93ca95d801e8a342",
    "files": [
        {
            "sha": "abd7d1b8b0d3c33a7752f3390770fe7da35f03b1",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -781,12 +781,15 @@ def forward(self, x, position_ids):\n             [`PreTrainedTokenizer.__call__`] for details.\n \n             [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n             - 1 for tokens that are **not masked**,\n             - 0 for tokens that are **masked**.\n \n+            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n+            but you can also pass a `BlockMask` object directly here.\n+\n             [What are attention masks?](../glossary#attention-mask)\n \n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n@@ -983,7 +986,7 @@ def forward(\n \n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -996,8 +999,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "f9aee9362db12f61f6a82847801e833cdbb2d160",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -742,7 +742,7 @@ def forward(\n     # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -755,8 +755,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "05e01c37bbd261496f1a3dc8fde6ff9d6c4368a2",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -1376,7 +1376,7 @@ def forward(\n     # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -1389,8 +1389,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "1bc9ceeaa3b38a8a72e7d2a4f6d728f3adbca301",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -588,7 +588,7 @@ def forward(\n     # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -601,8 +601,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "43c52947884b9c5ad371f0afbe0eca268378ed6d",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -430,12 +430,15 @@ def _init_weights(self, module):\n             [`PreTrainedTokenizer.__call__`] for details.\n \n             [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n             - 1 for tokens that are **not masked**,\n             - 0 for tokens that are **masked**.\n \n+            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n+            but you can also pass a `BlockMask` object directly here.\n+\n             [What are attention masks?](../glossary#attention-mask)\n \n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n@@ -632,7 +635,7 @@ def forward(\n \n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -645,8 +648,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "1d0702615e6c8efcc27cff384e98b58ed74d7dcf",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 16,
            "deletions": 2,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -38,13 +38,20 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     can_return_tuple,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_cohere2 import Cohere2Config\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n _CONFIG_FOR_DOC = \"Cohere2Config\"\n@@ -438,12 +445,15 @@ def _init_weights(self, module):\n             [`PreTrainedTokenizer.__call__`] for details.\n \n             [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n             - 1 for tokens that are **not masked**,\n             - 0 for tokens that are **masked**.\n \n+            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n+            but you can also pass a `BlockMask` object directly here.\n+\n             [What are attention masks?](../glossary#attention-mask)\n \n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n@@ -654,7 +664,7 @@ def forward(\n     @torch.no_grad()\n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: HybridCache,\n@@ -666,6 +676,10 @@ def _update_causal_mask(\n         # as it doesn't cause dynamic control issues.\n         if self.config._attn_implementation == \"flash_attention_2\":\n             return attention_mask\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n \n         dtype, device = input_tensor.dtype, input_tensor.device\n         sequence_length = input_tensor.shape[1]"
        },
        {
            "sha": "7da08106455cf5514f4c0b470323cdf4b2121471",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -1114,7 +1114,7 @@ def forward(\n     # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -1127,8 +1127,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "ccddeed663426f889cc98404b49c6312848915cc",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -573,12 +573,15 @@ def _init_weights(self, module):\n             [`PreTrainedTokenizer.__call__`] for details.\n \n             [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n             - 1 for tokens that are **not masked**,\n             - 0 for tokens that are **masked**.\n \n+            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n+            but you can also pass a `BlockMask` object directly here.\n+\n             [What are attention masks?](../glossary#attention-mask)\n \n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n@@ -777,7 +780,7 @@ def forward(\n \n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -790,8 +793,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "f5bed9bf8796be11afb3be3281e2ec3358fce34e",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -678,12 +678,15 @@ def forward(self, x, position_ids):\n             [`PreTrainedTokenizer.__call__`] for details.\n \n             [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n             - 1 for tokens that are **not masked**,\n             - 0 for tokens that are **masked**.\n \n+            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n+            but you can also pass a `BlockMask` object directly here.\n+\n             [What are attention masks?](../glossary#attention-mask)\n \n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n@@ -880,7 +883,7 @@ def forward(\n \n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -893,8 +896,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n@@ -1259,7 +1261,7 @@ def set_input_embeddings(self, value):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,"
        },
        {
            "sha": "3b0a4882dee3efb13303f2d68fa189b76e6c174c",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -1468,7 +1468,7 @@ def forward(\n \n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -1481,8 +1481,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "fedbac3cea545ed6f45be7073df4af349d3bfeff",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -395,12 +395,15 @@ def _init_weights(self, module):\n             [`PreTrainedTokenizer.__call__`] for details.\n \n             [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n             - 1 for tokens that are **not masked**,\n             - 0 for tokens that are **masked**.\n \n+            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n+            but you can also pass a `BlockMask` object directly here.\n+\n             [What are attention masks?](../glossary#attention-mask)\n \n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n@@ -599,7 +602,7 @@ def forward(\n \n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -612,8 +615,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "22c4e599c2820dcd9a25b89ba2733bed4632b49f",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 16,
            "deletions": 2,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -43,13 +43,20 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     can_return_tuple,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_gemma2 import Gemma2Config\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -440,12 +447,15 @@ def _init_weights(self, module):\n             [`PreTrainedTokenizer.__call__`] for details.\n \n             [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n             - 1 for tokens that are **not masked**,\n             - 0 for tokens that are **masked**.\n \n+            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n+            but you can also pass a `BlockMask` object directly here.\n+\n             [What are attention masks?](../glossary#attention-mask)\n \n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n@@ -666,7 +676,7 @@ def forward(\n     @torch.no_grad()\n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: HybridCache,\n@@ -678,6 +688,10 @@ def _update_causal_mask(\n         # as it doesn't cause dynamic control issues.\n         if self.config._attn_implementation == \"flash_attention_2\":\n             return attention_mask\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n \n         dtype, device = input_tensor.dtype, input_tensor.device\n         sequence_length = input_tensor.shape[1]"
        },
        {
            "sha": "e06a701fc527dcc8fe98b1d4f87c20f0dc825fb1",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 13,
            "deletions": 2,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -30,7 +30,7 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n-from ...utils import logging\n+from ...utils import is_torch_flex_attn_available, logging\n from ..gemma.modeling_gemma import (\n     GemmaAttention,\n     GemmaForCausalLM,\n@@ -46,6 +46,13 @@\n \n _CHECKPOINT_FOR_DOC = \"google/gemma2-7b\"\n \n+\n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -535,7 +542,7 @@ def forward(\n     @torch.no_grad()\n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: HybridCache,\n@@ -547,6 +554,10 @@ def _update_causal_mask(\n         # as it doesn't cause dynamic control issues.\n         if self.config._attn_implementation == \"flash_attention_2\":\n             return attention_mask\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n \n         dtype, device = input_tensor.dtype, input_tensor.device\n         sequence_length = input_tensor.shape[1]"
        },
        {
            "sha": "e9215793267c1aa4ede5f9964328c684cff7f571",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 16,
            "deletions": 2,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -40,6 +40,7 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     can_return_tuple,\n+    is_torch_flex_attn_available,\n     is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n@@ -49,6 +50,12 @@\n from .configuration_gemma3 import Gemma3Config, Gemma3TextConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n _CONFIG_FOR_DOC = \"Gemma3Config\"\n \n@@ -512,12 +519,15 @@ def _init_weights(self, module):\n             [`PreTrainedTokenizer.__call__`] for details.\n \n             [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n             - 1 for tokens that are **not masked**,\n             - 0 for tokens that are **masked**.\n \n+            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n+            but you can also pass a `BlockMask` object directly here.\n+\n             [What are attention masks?](../glossary#attention-mask)\n \n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n@@ -751,7 +761,7 @@ def forward(\n     @torch.no_grad()\n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: HybridCache,\n@@ -763,6 +773,10 @@ def _update_causal_mask(\n         # as it doesn't cause dynamic control issues.\n         if self.config._attn_implementation == \"flash_attention_2\":\n             return attention_mask\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n \n         dtype, device = input_tensor.dtype, input_tensor.device\n         sequence_length = input_tensor.shape[1]"
        },
        {
            "sha": "8b0ccd9c9e3118a153e8c5a7af8647dea3f66aa2",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -413,12 +413,15 @@ def _init_weights(self, module):\n             [`PreTrainedTokenizer.__call__`] for details.\n \n             [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n             - 1 for tokens that are **not masked**,\n             - 0 for tokens that are **masked**.\n \n+            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n+            but you can also pass a `BlockMask` object directly here.\n+\n             [What are attention masks?](../glossary#attention-mask)\n \n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n@@ -615,7 +618,7 @@ def forward(\n \n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -628,8 +631,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "45287c025aed08c584765585e7c1b1d7b9ad422f",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -789,7 +789,7 @@ def forward(\n     # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -802,8 +802,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "e72b82eb8b51a8fc927698f95eda5533962c6b5e",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -386,12 +386,15 @@ def _init_weights(self, module):\n             [`PreTrainedTokenizer.__call__`] for details.\n \n             [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n             - 1 for tokens that are **not masked**,\n             - 0 for tokens that are **masked**.\n \n+            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n+            but you can also pass a `BlockMask` object directly here.\n+\n             [What are attention masks?](../glossary#attention-mask)\n \n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n@@ -600,7 +603,7 @@ def forward(\n \n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -613,8 +616,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "949e26d2b2f5bf2159edcb2133923db738a9c08e",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -640,7 +640,7 @@ def forward(\n     # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -653,8 +653,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "50344787bc2a18e92c3d8122c765bcf55abad928",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -890,7 +890,7 @@ def forward(\n     # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -903,8 +903,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "1d77b8b7bae8194e54f2da2857b62a8a34631f3f",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -414,12 +414,15 @@ def forward(self, x, position_ids):\n             [`PreTrainedTokenizer.__call__`] for details.\n \n             [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n             - 1 for tokens that are **not masked**,\n             - 0 for tokens that are **masked**.\n \n+            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n+            but you can also pass a `BlockMask` object directly here.\n+\n             [What are attention masks?](../glossary#attention-mask)\n \n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n@@ -615,7 +618,7 @@ def forward(\n \n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -628,8 +631,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "40bff42cdfc7b88569c9a61f67558f104d6038ab",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -1089,7 +1089,7 @@ def forward(\n     # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -1102,8 +1102,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "fe62089a0058b33a9a92bfbf723c5b8a2adfe465",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -1034,7 +1034,7 @@ def forward(\n \n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -1047,8 +1047,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "251beacfaddc533f180ce7a1f757888d7fcf5f7a",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -398,12 +398,15 @@ def _init_weights(self, module):\n             [`PreTrainedTokenizer.__call__`] for details.\n \n             [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n             - 1 for tokens that are **not masked**,\n             - 0 for tokens that are **masked**.\n \n+            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n+            but you can also pass a `BlockMask` object directly here.\n+\n             [What are attention masks?](../glossary#attention-mask)\n \n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n@@ -600,7 +603,7 @@ def forward(\n \n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -613,8 +616,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "73d3ac4fab6aef60d28ab27a644f09a68b15600e",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -1385,7 +1385,7 @@ def vblock(\n     # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -1398,8 +1398,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "bf59fd074e676f008256c629629fb2a8f3d625c7",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -1093,7 +1093,7 @@ def forward(\n     # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -1106,8 +1106,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "6898f168a243b2f361e4a6d6519e29cbea823462",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -403,12 +403,15 @@ def _init_weights(self, module):\n             [`PreTrainedTokenizer.__call__`] for details.\n \n             [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n             - 1 for tokens that are **not masked**,\n             - 0 for tokens that are **masked**.\n \n+            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n+            but you can also pass a `BlockMask` object directly here.\n+\n             [What are attention masks?](../glossary#attention-mask)\n \n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n@@ -605,7 +608,7 @@ def forward(\n \n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -618,8 +621,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n@@ -985,7 +987,7 @@ def set_input_embeddings(self, value):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,"
        },
        {
            "sha": "f44b28f901728ecb7b51239948cb381990d42d7c",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -1600,7 +1600,7 @@ def forward(\n     # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -1613,8 +1613,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "a1006ed011bd32618821116fe8e3c5fb26396f72",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 12,
            "deletions": 1,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -33,6 +33,7 @@\n     ModelOutput,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n@@ -42,6 +43,12 @@\n if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -1034,7 +1041,7 @@ def forward(\n     # Copied from transformers.models.phi3.modeling_phi3.Phi3Model._update_causal_mask with Phi3->Mimi\n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -1052,6 +1059,10 @@ def _update_causal_mask(\n             if attention_mask is not None and 0.0 in attention_mask:\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "bf2cccb65bbf4ec8e71b5f885d69504cb0372147",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 17,
            "deletions": 3,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -32,13 +32,20 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     can_return_tuple,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_mistral import MistralConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n _CHECKPOINT_FOR_DOC = \"mistralai/Mistral-7B-v0.1\"\n@@ -366,12 +373,15 @@ def forward(self, x, position_ids):\n             [`PreTrainedTokenizer.__call__`] for details.\n \n             [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n             - 1 for tokens that are **not masked**,\n             - 0 for tokens that are **masked**.\n \n+            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n+            but you can also pass a `BlockMask` object directly here.\n+\n             [What are attention masks?](../glossary#attention-mask)\n \n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n@@ -568,7 +578,7 @@ def forward(\n \n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -586,6 +596,10 @@ def _update_causal_mask(\n             if attention_mask is not None and 0.0 in attention_mask:\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n@@ -1053,7 +1067,7 @@ def set_input_embeddings(self, value):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,"
        },
        {
            "sha": "84062fde3e1efd6340a2fd821ed9b0e7fbb52dc3",
            "filename": "src/transformers/models/mistral/modular_mistral.py",
            "status": "modified",
            "additions": 13,
            "deletions": 3,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -10,7 +10,7 @@\n from ...modeling_outputs import BaseModelOutputWithPast, QuestionAnsweringModelOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n-from ...utils import logging\n+from ...utils import is_torch_flex_attn_available, logging\n from ..llama.modeling_llama import (\n     LlamaAttention,\n     LlamaDecoderLayer,\n@@ -27,6 +27,12 @@\n from .configuration_mistral import MistralConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n _CHECKPOINT_FOR_DOC = \"mistralai/Mistral-7B-v0.1\"\n@@ -120,7 +126,7 @@ def __init__(self, config: MistralConfig):\n \n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -138,6 +144,10 @@ def _update_causal_mask(\n             if attention_mask is not None and 0.0 in attention_mask:\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n@@ -300,7 +310,7 @@ def set_input_embeddings(self, value):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,"
        },
        {
            "sha": "1f3b7bd0a679ecf6227015bebf45f5f7ba67f567",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 17,
            "deletions": 3,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -55,13 +55,20 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     can_return_tuple,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_mixtral import MixtralConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n _CHECKPOINT_FOR_DOC = \"mistralai/Mixtral-8x7B-v0.1\"\n@@ -488,12 +495,15 @@ def _init_weights(self, module):\n             [`PreTrainedTokenizer.__call__`] for details.\n \n             [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n             - 1 for tokens that are **not masked**,\n             - 0 for tokens that are **masked**.\n \n+            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n+            but you can also pass a `BlockMask` object directly here.\n+\n             [What are attention masks?](../glossary#attention-mask)\n \n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n@@ -697,7 +707,7 @@ def forward(\n \n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -715,6 +725,10 @@ def _update_causal_mask(\n             if attention_mask is not None and 0.0 in attention_mask:\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n@@ -1287,7 +1301,7 @@ def set_input_embeddings(self, value):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,"
        },
        {
            "sha": "77bff448ff9559c3d0cd8ed5fe02a503bb253924",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -1062,7 +1062,7 @@ def _init_weights(self, module):\n     # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -1075,8 +1075,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "97453d66149487fe93b5929b36fc2bc07d3cf415",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -718,12 +718,15 @@ def forward(\n             [`PreTrainedTokenizer.__call__`] for details.\n \n             [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n             - 1 for tokens that are **not masked**,\n             - 0 for tokens that are **masked**.\n \n+            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n+            but you can also pass a `BlockMask` object directly here.\n+\n             [What are attention masks?](../glossary#attention-mask)\n \n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n@@ -961,7 +964,7 @@ def forward(\n \n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -974,8 +977,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "403c27f0780e18893a113bec9143b81efeab19a7",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 17,
            "deletions": 2,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -43,6 +43,7 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torch_flex_attn_available,\n     is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n@@ -55,6 +56,12 @@\n if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n _CONFIG_FOR_DOC = \"MoshiConfig\"\n@@ -1261,7 +1268,7 @@ def forward(\n     # Copied from transformers.models.phi3.modeling_phi3.Phi3Model._update_causal_mask with Phi3->Moshi\n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -1279,6 +1286,10 @@ def _update_causal_mask(\n             if attention_mask is not None and 0.0 in attention_mask:\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n@@ -1575,7 +1586,7 @@ def forward(\n     # Copied from transformers.models.phi3.modeling_phi3.Phi3Model._update_causal_mask with Phi3->Moshi\n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -1593,6 +1604,10 @@ def _update_causal_mask(\n             if attention_mask is not None and 0.0 in attention_mask:\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "27badbbeeeed8a2bb73085f59c8c3d2d96449036",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -1191,7 +1191,7 @@ def forward(\n     # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -1204,8 +1204,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "4e816c2a5c545eb6a7202a6df940c8373b249907",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -852,7 +852,7 @@ def forward(\n     # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask with LLAMA->NEMOTRON,Llama->Nemotron,llama->nemotron\n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -865,8 +865,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n@@ -1231,7 +1230,7 @@ def set_input_embeddings(self, value):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,"
        },
        {
            "sha": "7ac2dd6ad919a2810d1ea72a144017e28f91b5b6",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -373,12 +373,15 @@ def _init_weights(self, module):\n             [`PreTrainedTokenizer.__call__`] for details.\n \n             [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n             - 1 for tokens that are **not masked**,\n             - 0 for tokens that are **masked**.\n \n+            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n+            but you can also pass a `BlockMask` object directly here.\n+\n             [What are attention masks?](../glossary#attention-mask)\n \n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n@@ -575,7 +578,7 @@ def forward(\n \n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -588,8 +591,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "d09d47c7c7856e0a3bd2fed01680146ad284bc56",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -379,12 +379,15 @@ def _init_weights(self, module):\n             [`PreTrainedTokenizer.__call__`] for details.\n \n             [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n             - 1 for tokens that are **not masked**,\n             - 0 for tokens that are **masked**.\n \n+            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n+            but you can also pass a `BlockMask` object directly here.\n+\n             [What are attention masks?](../glossary#attention-mask)\n \n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n@@ -581,7 +584,7 @@ def forward(\n \n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -594,8 +597,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "173fe89f6ee3409a850c0f0a0f3c545e8162aa28",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -641,7 +641,7 @@ def set_input_embeddings(self, value):\n     # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -654,8 +654,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "3b492ab8517097149680367f3466b8bdc9387546",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -652,7 +652,7 @@ def forward(\n     # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -665,8 +665,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "de1abd9963a88bb741e6e67334518df1da9d7111",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -373,12 +373,15 @@ def _init_weights(self, module):\n             [`PreTrainedTokenizer.__call__`] for details.\n \n             [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n             - 1 for tokens that are **not masked**,\n             - 0 for tokens that are **masked**.\n \n+            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n+            but you can also pass a `BlockMask` object directly here.\n+\n             [What are attention masks?](../glossary#attention-mask)\n \n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n@@ -573,7 +576,7 @@ def forward(\n \n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -586,8 +589,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "2bf32cdcdc1a7faa1a45b4c9baf0ca20663b1b0d",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 16,
            "deletions": 2,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -47,13 +47,20 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     can_return_tuple,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_phi3 import Phi3Config\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n _CHECKPOINT_FOR_DOC = \"microsoft/Phi-3-mini-4k-instruct\"\n@@ -421,12 +428,15 @@ def forward(self, x, position_ids):\n             [`PreTrainedTokenizer.__call__`] for details.\n \n             [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n             - 1 for tokens that are **not masked**,\n             - 0 for tokens that are **masked**.\n \n+            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n+            but you can also pass a `BlockMask` object directly here.\n+\n             [What are attention masks?](../glossary#attention-mask)\n \n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n@@ -623,7 +633,7 @@ def forward(\n \n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -641,6 +651,10 @@ def _update_causal_mask(\n             if attention_mask is not None and 0.0 in attention_mask:\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "706fb6642b84a5f3076b826863dcb8b40ed9fc78",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 12,
            "deletions": 1,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -49,13 +49,20 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     can_return_tuple,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n     torch_int,\n )\n from .configuration_phi4_multimodal import Phi4MultimodalAudioConfig, Phi4MultimodalConfig, Phi4MultimodalVisionConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -1923,7 +1930,7 @@ def forward(\n \n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -1941,6 +1948,10 @@ def _update_causal_mask(\n             if attention_mask is not None and 0.0 in attention_mask:\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "42df5ac9d01a2233190e70bf3f113c88a68a2df8",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 12,
            "deletions": 1,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -38,6 +38,7 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     can_return_tuple,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n@@ -48,6 +49,12 @@\n if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n # This makes `_prepare_4d_causal_attention_mask` a leaf function in the FX graph.\n # It means that the function will not be traced through and simply appear as a node in the graph.\n _prepare_4d_causal_attention_mask = torch.fx.wrap(_prepare_4d_causal_attention_mask)\n@@ -1170,7 +1177,7 @@ def forward(\n     # Copied from transformers.models.phi3.modeling_phi3.Phi3Model._update_causal_mask with Phi3->Phimoe\n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -1188,6 +1195,10 @@ def _update_causal_mask(\n             if attention_mask is not None and 0.0 in attention_mask:\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "192f4a10b12ff81ec3223563b2fb0f58c202ff92",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -1587,7 +1587,7 @@ def forward(\n     # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -1600,8 +1600,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "0a8b479554374f44105f76c060d7f58e99e14eb0",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -1000,7 +1000,7 @@ def forward(\n     # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -1013,8 +1013,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "8c95569a49ff4a179c5946415e5e5e28c5e589e5",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 17,
            "deletions": 3,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -32,13 +32,20 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     can_return_tuple,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_qwen2 import Qwen2Config\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n _CHECKPOINT_FOR_DOC = \"meta-qwen2/Qwen2-2-7b-hf\"\n@@ -379,12 +386,15 @@ def forward(self, x, position_ids):\n             [`PreTrainedTokenizer.__call__`] for details.\n \n             [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n             - 1 for tokens that are **not masked**,\n             - 0 for tokens that are **masked**.\n \n+            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n+            but you can also pass a `BlockMask` object directly here.\n+\n             [What are attention masks?](../glossary#attention-mask)\n \n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n@@ -581,7 +591,7 @@ def forward(\n \n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -599,6 +609,10 @@ def _update_causal_mask(\n             if attention_mask is not None and 0.0 in attention_mask:\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n@@ -1066,7 +1080,7 @@ def set_input_embeddings(self, value):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,"
        },
        {
            "sha": "388b5f905554246d260404e88735a6aab8110894",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 17,
            "deletions": 2,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -41,7 +41,13 @@\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n-from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n+from ...utils import (\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    is_torch_flex_attn_available,\n+    logging,\n+    replace_return_docstrings,\n+)\n from .configuration_qwen2_5_vl import Qwen2_5_VLConfig, Qwen2_5_VLVisionConfig\n \n \n@@ -52,6 +58,11 @@\n if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n \n logger = logging.get_logger(__name__)\n \n@@ -1208,7 +1219,7 @@ def forward(\n \n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -1226,6 +1237,10 @@ def _update_causal_mask(\n             if attention_mask is not None and 0.0 in attention_mask:\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "365b561eb9e7001e409a73985f02dfe10155e8ee",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 12,
            "deletions": 2,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -46,6 +46,7 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     can_return_tuple,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n@@ -56,6 +57,11 @@\n if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n logger = logging.get_logger(__name__)\n \n _CHECKPOINT_FOR_DOC = \"Qwen/Qwen2-57B-A14B\"\n@@ -1032,7 +1038,7 @@ def forward(\n     # Copied from transformers.models.phi3.modeling_phi3.Phi3Model._update_causal_mask with Phi3->Qwen2Moe\n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -1050,6 +1056,10 @@ def _update_causal_mask(\n             if attention_mask is not None and 0.0 in attention_mask:\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n@@ -1539,7 +1549,7 @@ def set_input_embeddings(self, value):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,"
        },
        {
            "sha": "96cbb4d3a5cfaae894851f2eefb1b9986e700556",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 11,
            "deletions": 1,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -40,6 +40,7 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n@@ -49,6 +50,11 @@\n if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward, flash_attn_varlen_func\n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n \n logger = logging.get_logger(__name__)\n \n@@ -1166,7 +1172,7 @@ def forward(\n     # Copied from transformers.models.phi3.modeling_phi3.Phi3Model._update_causal_mask with Phi3->Qwen2VL\n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -1184,6 +1190,10 @@ def _update_causal_mask(\n             if attention_mask is not None and 0.0 in attention_mask:\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "3cc5de2421b8570c780991574f5c773288713354",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 17,
            "deletions": 3,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -47,13 +47,20 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     can_return_tuple,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_qwen3 import Qwen3Config\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n _CHECKPOINT_FOR_DOC = \"Qwen/Qwen3-8B\"\n@@ -406,12 +413,15 @@ def forward(self, x, position_ids):\n             [`PreTrainedTokenizer.__call__`] for details.\n \n             [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n             - 1 for tokens that are **not masked**,\n             - 0 for tokens that are **masked**.\n \n+            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n+            but you can also pass a `BlockMask` object directly here.\n+\n             [What are attention masks?](../glossary#attention-mask)\n \n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n@@ -608,7 +618,7 @@ def forward(\n \n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -626,6 +636,10 @@ def _update_causal_mask(\n             if attention_mask is not None and 0.0 in attention_mask:\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n@@ -1093,7 +1107,7 @@ def set_input_embeddings(self, value):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,"
        },
        {
            "sha": "6a062420f24b6320140f2bdc821ad7aab8d68e8b",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 17,
            "deletions": 3,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -50,13 +50,20 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     can_return_tuple,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_qwen3_moe import Qwen3MoeConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n _CHECKPOINT_FOR_DOC = \"Qwen/Qwen3-MoE-15B-A2B\"\n@@ -502,12 +509,15 @@ def _init_weights(self, module):\n             [`PreTrainedTokenizer.__call__`] for details.\n \n             [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n             - 1 for tokens that are **not masked**,\n             - 0 for tokens that are **masked**.\n \n+            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n+            but you can also pass a `BlockMask` object directly here.\n+\n             [What are attention masks?](../glossary#attention-mask)\n \n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n@@ -711,7 +721,7 @@ def forward(\n \n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -729,6 +739,10 @@ def _update_causal_mask(\n             if attention_mask is not None and 0.0 in attention_mask:\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n@@ -1301,7 +1315,7 @@ def set_input_embeddings(self, value):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,"
        },
        {
            "sha": "685a7c23720fd28c18b5693979e6d14b96eb6a1c",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -906,7 +906,7 @@ def forward(\n     # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -919,8 +919,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "ae16daaa46f5db02023b14fe4a886ce9e44e9dc7",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 16,
            "deletions": 2,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -49,13 +49,20 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     can_return_tuple,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_starcoder2 import Starcoder2Config\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n _CHECKPOINT_FOR_DOC = \"bigcode/starcoder2-7b\"\n \n@@ -369,12 +376,15 @@ def _init_weights(self, module):\n             [`PreTrainedTokenizer.__call__`] for details.\n \n             [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n             - 1 for tokens that are **not masked**,\n             - 0 for tokens that are **masked**.\n \n+            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n+            but you can also pass a `BlockMask` object directly here.\n+\n             [What are attention masks?](../glossary#attention-mask)\n \n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n@@ -558,7 +568,7 @@ def forward(\n \n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -576,6 +586,10 @@ def _update_causal_mask(\n             if attention_mask is not None and 0.0 in attention_mask:\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "3dd8a139875bb180a3e1568b5e0bcd4d85d03d27",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -1134,7 +1134,7 @@ def forward(\n     # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -1147,8 +1147,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "e8085a8f45651e1537d8561d9d5526c7462924ca",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -1205,7 +1205,7 @@ def forward(\n     # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -1218,8 +1218,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "9cd7f984237381ca8ac2d9fa263339fca4a8affb",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -1537,7 +1537,7 @@ def forward(\n     # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -1550,8 +1550,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "44586243c3fa01588f9f2205c3eaaa5470deb11b",
            "filename": "src/transformers/models/umt5/modeling_umt5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -848,7 +848,7 @@ def forward(\n     # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -861,8 +861,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "52ad48cc2d24e7f463eab1dfd7f24ac4b9185884",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1efcfa9ca48e014e2261db1f93ca95d801e8a342/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=1efcfa9ca48e014e2261db1f93ca95d801e8a342",
            "patch": "@@ -1375,7 +1375,7 @@ def forward(\n     # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -1388,8 +1388,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        }
    ],
    "stats": {
        "total": 600,
        "additions": 423,
        "deletions": 177
    }
}