{
    "author": "sbucaille",
    "message": "[lightglue] add support for remote code DISK keypoint detector (#39253)\n\n* feat: add trust_remote_code in LightGlueConfig\n\n* fix: made sure trust_remote_code is provided only when necessary\n\n* fix: make style\n\n* docs: added missing trust_remote_code docstring\n\n* refactor: refactored LightGlue config init\n\n* fix: removed unnecessary argument",
    "sha": "1255480fd226129075e10c20842efd444f5b0e36",
    "files": [
        {
            "sha": "d920727c0fa669d981dbb45454691dd8c4430e9f",
            "filename": "src/transformers/models/lightglue/configuration_lightglue.py",
            "status": "modified",
            "additions": 17,
            "deletions": 3,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/1255480fd226129075e10c20842efd444f5b0e36/src%2Ftransformers%2Fmodels%2Flightglue%2Fconfiguration_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1255480fd226129075e10c20842efd444f5b0e36/src%2Ftransformers%2Fmodels%2Flightglue%2Fconfiguration_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fconfiguration_lightglue.py?ref=1255480fd226129075e10c20842efd444f5b0e36",
            "patch": "@@ -65,6 +65,8 @@ class LightGlueConfig(PretrainedConfig):\n             The dropout ratio for the attention probabilities.\n         attention_bias (`bool`, *optional*, defaults to `True`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        trust_remote_code (`bool`, *optional*, defaults to `False`):\n+            Whether to trust remote code when using other models than SuperPoint as keypoint detector.\n \n     Examples:\n         ```python\n@@ -98,8 +100,14 @@ def __init__(\n         hidden_act: str = \"gelu\",\n         attention_dropout=0.0,\n         attention_bias=True,\n+        trust_remote_code: bool = False,\n         **kwargs,\n     ):\n+        # LightGlue can be used with other models than SuperPoint as keypoint detector\n+        # We provide the trust_remote_code argument to allow the use of other models\n+        # that are not registered in the CONFIG_MAPPING dictionary (for example DISK)\n+        self.trust_remote_code = trust_remote_code\n+\n         if descriptor_dim % num_attention_heads != 0:\n             raise ValueError(\"descriptor_dim % num_heads is different from zero\")\n \n@@ -124,9 +132,15 @@ def __init__(\n             keypoint_detector_config[\"model_type\"] = (\n                 keypoint_detector_config[\"model_type\"] if \"model_type\" in keypoint_detector_config else \"superpoint\"\n             )\n-            keypoint_detector_config = CONFIG_MAPPING[keypoint_detector_config[\"model_type\"]](\n-                **keypoint_detector_config, attn_implementation=\"eager\"\n-            )\n+            if keypoint_detector_config[\"model_type\"] not in CONFIG_MAPPING:\n+                keypoint_detector_config = AutoConfig.from_pretrained(\n+                    keypoint_detector_config[\"_name_or_path\"], trust_remote_code=self.trust_remote_code\n+                )\n+            else:\n+                keypoint_detector_config = CONFIG_MAPPING[keypoint_detector_config[\"model_type\"]](\n+                    **keypoint_detector_config, attn_implementation=\"eager\"\n+                )\n+\n         if keypoint_detector_config is None:\n             keypoint_detector_config = CONFIG_MAPPING[\"superpoint\"](attn_implementation=\"eager\")\n "
        },
        {
            "sha": "baf9e5c83e1e1703eca86eb1cbb1b79546b45f03",
            "filename": "src/transformers/models/lightglue/modeling_lightglue.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1255480fd226129075e10c20842efd444f5b0e36/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1255480fd226129075e10c20842efd444f5b0e36/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py?ref=1255480fd226129075e10c20842efd444f5b0e36",
            "patch": "@@ -513,8 +513,9 @@ class LightGlueForKeypointMatching(LightGluePreTrainedModel):\n \n     def __init__(self, config: LightGlueConfig):\n         super().__init__(config)\n-\n-        self.keypoint_detector = AutoModelForKeypointDetection.from_config(config.keypoint_detector_config)\n+        self.keypoint_detector = AutoModelForKeypointDetection.from_config(\n+            config.keypoint_detector_config, trust_remote_code=config.trust_remote_code\n+        )\n \n         self.keypoint_detector_descriptor_dim = config.keypoint_detector_config.descriptor_decoder_dim\n         self.descriptor_dim = config.descriptor_dim"
        },
        {
            "sha": "f39e28fb137bb2faf687928ba73ce19da064443f",
            "filename": "src/transformers/models/lightglue/modular_lightglue.py",
            "status": "modified",
            "additions": 20,
            "deletions": 5,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/1255480fd226129075e10c20842efd444f5b0e36/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1255480fd226129075e10c20842efd444f5b0e36/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py?ref=1255480fd226129075e10c20842efd444f5b0e36",
            "patch": "@@ -79,6 +79,8 @@ class LightGlueConfig(PretrainedConfig):\n             The dropout ratio for the attention probabilities.\n         attention_bias (`bool`, *optional*, defaults to `True`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        trust_remote_code (`bool`, *optional*, defaults to `False`):\n+            Whether to trust remote code when using other models than SuperPoint as keypoint detector.\n \n     Examples:\n         ```python\n@@ -112,8 +114,14 @@ def __init__(\n         hidden_act: str = \"gelu\",\n         attention_dropout=0.0,\n         attention_bias=True,\n+        trust_remote_code: bool = False,\n         **kwargs,\n     ):\n+        # LightGlue can be used with other models than SuperPoint as keypoint detector\n+        # We provide the trust_remote_code argument to allow the use of other models\n+        # that are not registered in the CONFIG_MAPPING dictionary (for example DISK)\n+        self.trust_remote_code = trust_remote_code\n+\n         if descriptor_dim % num_attention_heads != 0:\n             raise ValueError(\"descriptor_dim % num_heads is different from zero\")\n \n@@ -138,9 +146,15 @@ def __init__(\n             keypoint_detector_config[\"model_type\"] = (\n                 keypoint_detector_config[\"model_type\"] if \"model_type\" in keypoint_detector_config else \"superpoint\"\n             )\n-            keypoint_detector_config = CONFIG_MAPPING[keypoint_detector_config[\"model_type\"]](\n-                **keypoint_detector_config, attn_implementation=\"eager\"\n-            )\n+            if keypoint_detector_config[\"model_type\"] not in CONFIG_MAPPING:\n+                keypoint_detector_config = AutoConfig.from_pretrained(\n+                    keypoint_detector_config[\"_name_or_path\"], trust_remote_code=self.trust_remote_code\n+                )\n+            else:\n+                keypoint_detector_config = CONFIG_MAPPING[keypoint_detector_config[\"model_type\"]](\n+                    **keypoint_detector_config, attn_implementation=\"eager\"\n+                )\n+\n         if keypoint_detector_config is None:\n             keypoint_detector_config = CONFIG_MAPPING[\"superpoint\"](attn_implementation=\"eager\")\n \n@@ -584,8 +598,9 @@ class LightGlueForKeypointMatching(LightGluePreTrainedModel):\n \n     def __init__(self, config: LightGlueConfig):\n         super().__init__(config)\n-\n-        self.keypoint_detector = AutoModelForKeypointDetection.from_config(config.keypoint_detector_config)\n+        self.keypoint_detector = AutoModelForKeypointDetection.from_config(\n+            config.keypoint_detector_config, trust_remote_code=config.trust_remote_code\n+        )\n \n         self.keypoint_detector_descriptor_dim = config.keypoint_detector_config.descriptor_decoder_dim\n         self.descriptor_dim = config.descriptor_dim"
        }
    ],
    "stats": {
        "total": 50,
        "additions": 40,
        "deletions": 10
    }
}