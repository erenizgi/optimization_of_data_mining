{
    "author": "yonigozlan",
    "message": "Improve `add_dates` script (#41167)\n\n* utils/add_dates.py\n\n* put lfm2-vl in correct category",
    "sha": "53838edde77cb10f3a360150aa85a457637e9ac3",
    "files": [
        {
            "sha": "c0258da704be2a4e3bae6d934494b353230a21d2",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/53838edde77cb10f3a360150aa85a457637e9ac3/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/53838edde77cb10f3a360150aa85a457637e9ac3/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=53838edde77cb10f3a360150aa85a457637e9ac3",
            "patch": "@@ -555,8 +555,6 @@\n         title: LED\n       - local: model_doc/lfm2\n         title: LFM2\n-      - local: model_doc/lfm2_vl\n-        title: LFM2-VL\n       - local: model_doc/llama\n         title: LLaMA\n       - local: model_doc/llama2\n@@ -1087,6 +1085,8 @@\n         title: LayoutLMV3\n       - local: model_doc/layoutxlm\n         title: LayoutXLM\n+      - local: model_doc/lfm2_vl\n+        title: LFM2-VL\n       - local: model_doc/lilt\n         title: LiLT\n       - local: model_doc/llama4"
        },
        {
            "sha": "254cf6c0f44afd420ab0be995ed627d3a9a625ed",
            "filename": "docs/source/en/model_doc/blt.md",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/53838edde77cb10f3a360150aa85a457637e9ac3/docs%2Fsource%2Fen%2Fmodel_doc%2Fblt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/53838edde77cb10f3a360150aa85a457637e9ac3/docs%2Fsource%2Fen%2Fmodel_doc%2Fblt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fblt.md?ref=53838edde77cb10f3a360150aa85a457637e9ac3",
            "patch": "@@ -13,6 +13,7 @@ specific language governing permissions and limitations under the License.\n rendered properly in your Markdown viewer.\n \n -->\n+*This model was released on 2024-12-13 and added to Hugging Face Transformers on 2025-09-19.*\n \n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">\n@@ -28,7 +29,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The BLT model was proposed in [Byte Latent Transformer: Patches Scale Better Than Tokens](<https://arxiv.org/pdf/2412.09871>) by Artidoro Pagnoni, Ram Pasunuru, Pedro Rodriguez, John Nguyen, Benjamin Muller, Margaret Li1, Chunting Zhou, Lili Yu, Jason Weston, Luke Zettlemoyer, Gargi Ghosh, Mike Lewis, Ari Holtzman†, Srinivasan Iyer.\n+The BLT model was proposed in [Byte Latent Transformer: Patches Scale Better Than Tokens](https://huggingface.co/papers/2412.09871) by Artidoro Pagnoni, Ram Pasunuru, Pedro Rodriguez, John Nguyen, Benjamin Muller, Margaret Li1, Chunting Zhou, Lili Yu, Jason Weston, Luke Zettlemoyer, Gargi Ghosh, Mike Lewis, Ari Holtzman†, Srinivasan Iyer.\n BLT is a byte-level LLM that achieves tokenization-level performance through entropy-based dynamic patching.\n \n The abstract from the paper is the following:\n@@ -64,8 +65,8 @@ from transformers import AutoTokenizer, AutoModelForCausalLM\n \n tokenizer = AutoTokenizer.from_pretrained(\"itazap/blt-1b-hf\")\n model = AutoModelForCausalLM.from_pretrained(\n-    \"itazap/blt-1b-hf\", \n-    device_map=\"auto\", \n+    \"itazap/blt-1b-hf\",\n+    device_map=\"auto\",\n )\n \n inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)"
        },
        {
            "sha": "911eea26debdd3c9aa31b8dffd6aa138c2158b97",
            "filename": "docs/source/en/model_doc/kosmos2_5.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/53838edde77cb10f3a360150aa85a457637e9ac3/docs%2Fsource%2Fen%2Fmodel_doc%2Fkosmos2_5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/53838edde77cb10f3a360150aa85a457637e9ac3/docs%2Fsource%2Fen%2Fmodel_doc%2Fkosmos2_5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fkosmos2_5.md?ref=53838edde77cb10f3a360150aa85a457637e9ac3",
            "patch": "@@ -9,7 +9,7 @@ Unless required by applicable law or agreed to in writing, software distributed\n an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n specific language governing permissions and limitations under the License.\n -->\n-*This model was released on {release_date} and added to Hugging Face Transformers on 2025-08-19.*\n+*This model was released on 2023-09-20 and added to Hugging Face Transformers on 2025-08-19.*\n \n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">"
        },
        {
            "sha": "c99473ac1a71173ea63bee3eccc166b3f0cce930",
            "filename": "docs/source/en/model_doc/parakeet.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/53838edde77cb10f3a360150aa85a457637e9ac3/docs%2Fsource%2Fen%2Fmodel_doc%2Fparakeet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/53838edde77cb10f3a360150aa85a457637e9ac3/docs%2Fsource%2Fen%2Fmodel_doc%2Fparakeet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fparakeet.md?ref=53838edde77cb10f3a360150aa85a457637e9ac3",
            "patch": "@@ -13,6 +13,7 @@ specific language governing permissions and limitations under the License.\n rendered properly in your Markdown viewer.\n \n -->\n+*This model was released on {release_date} and added to Hugging Face Transformers on 2025-09-25.*\n \n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">"
        },
        {
            "sha": "9b7fa18d3812c871ec21213133a4176d60ca75e6",
            "filename": "docs/source/en/model_doc/qwen3_omni_moe.md",
            "status": "modified",
            "additions": 33,
            "deletions": 33,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/53838edde77cb10f3a360150aa85a457637e9ac3/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_omni_moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/53838edde77cb10f3a360150aa85a457637e9ac3/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_omni_moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_omni_moe.md?ref=53838edde77cb10f3a360150aa85a457637e9ac3",
            "patch": "@@ -13,9 +13,9 @@ specific language governing permissions and limitations under the License.\n rendered properly in your Markdown viewer.\n \n -->\n-*This model was released on 2025-03-26 and added to Hugging Face Transformers on 2025-04-14.*\n+*This model was released on 2025-03-26 and added to Hugging Face Transformers on 2025-09-21.*\n \n-# Qwen2.5-Omni\n+# Qwen3-Omni-MOE\n \n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n@@ -25,37 +25,37 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The [Qwen2.5-Omni](https://qwenlm.github.io/blog/qwen2.5-omni/) model is a unified multiple modalities model proposed in [Qwen2.5-Omni Technical Report](https://huggingface.co/papers/2503.20215) from Qwen team, Alibaba Group.\n+The Qwen3-Omni-MOE model is a unified multiple modalities model proposed in [Qwen3-Omni Technical Report](https://huggingface.co/papers/2509.17765) from Qwen team, Alibaba Group.\n \n The abstract from the technical report is the following:\n \n-*We present Qwen2.5-Omni, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. To enable the streaming of multimodal information inputs, both audio and visual encoders utilize a block-wise processing approach. This strategy effectively decouples the handling of long sequences of multimodal data, assigning the perceptual responsibilities to the multimodal encoder and entrusting the modeling of extended sequences to a large language model. Such a division of labor enhances the fusion of different modalities via the shared attention mechanism. To synchronize the timestamps of video inputs with audio, we organized the audio and video sequentially in an interleaved manner and propose a novel position embedding approach, named TMRoPE (Time-aligned Multimodal RoPE). To concurrently generate text and speech while avoiding interference between the two modalities, we propose Thinker-Talker architecture. In this framework, Thinker functions as a large language model tasked with text generation, while Talker is a dual-track autoregressive model that directly utilizes the hidden representations from the Thinker to produce audio tokens as output. Both the Thinker and Talker models are designed to be trained and inferred in an end-to-end manner. For decoding audio tokens in a streaming manner, we introduce a sliding-window DiT that restricts the receptive field, aiming to reduce the initial package delay. Qwen2.5-Omni outperforms the similarly sized Qwen2-VL and Qwen2-Audio in both image and audio capabilities. Furthermore, Qwen2.5-Omni achieves state-of-the-art performance on multimodal benchmarks like Omni-Bench. Notably, Qwen2.5-Omni is the first open-source model to achieve a level of performance in end-to-end speech instruction following that is comparable to its capabilities with text inputs, as evidenced by benchmarks such as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni's streaming Talker outperform most existing streaming and non-streaming alternatives in robustness and naturalness.*\n+*We present Qwen3-Omni, a single multimodal model that, for the first time, maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages, and speech generation in 10 languages. To reduce first-packet latency in streaming synthesis, Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms. To further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license.\n \n ## Notes\n \n-- Use [`Qwen2_5OmniForConditionalGeneration`] to generate audio and text output. To generate only one output type, use [`Qwen2_5OmniThinkerForConditionalGeneration`] for text-only and [`Qwen2_5OmniTalkersForConditionalGeneration`] for audio-only outputs.\n-- Audio generation with [`Qwen2_5OmniForConditionalGeneration`] supports only single batch size at the moment.\n+- Use [`Qwen3OmniMoeForConditionalGeneration`] to generate audio and text output. To generate only one output type, use [`Qwen3OmniMoeThinkerForConditionalGeneration`] for text-only and [`Qwen3OmniMoeTalkerForConditionalGeneration`] for audio-only outputs.\n+- Audio generation with [`Qwen3OmniMoeForConditionalGeneration`] supports only single batch size at the moment.\n - In case out out-of-memory errors hwen working with video input, decrease `processor.max_pixels`. By default the maximum is set to a very arge value and high resolution visuals will not be resized, unless resolution exceeds `processor.max_pixels`.\n - The processor has its own [`~ProcessorMixin.apply_chat_template`] method to convert chat messages to model inputs.\n \n ## Usage example\n \n-`Qwen2.5-Omni` can be found on the [Huggingface Hub](https://huggingface.co/Qwen).\n+`Qwen3-Omni` can be found on the [Huggingface Hub](https://huggingface.co/Qwen).\n \n ### Single Media inference\n \n The model can accept text, images, audio and videos as input. Here's an example code for inference.\n \n ```python\n import soundfile as sf\n-from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor\n+from transformers import Qwen3OmniMoeForConditionalGeneration, Qwen3OmniMoeProcessor\n \n-model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n-    \"Qwen/Qwen2.5-Omni-7B\",\n+model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n+    \"Qwen/Qwen3-Omni-30B-A3B-Instruct\",\n     dtype=\"auto\",\n     device_map=\"auto\"\n )\n-processor = Qwen2_5OmniProcessor.from_pretrained(\"Qwen/Qwen2.5-Omni-7B\")\n+processor = Qwen3OmniMoeProcessor.from_pretrained(\"Qwen/Qwen3-Omni-30B-A3B-Instruct\")\n \n conversations = [\n     {\n@@ -82,7 +82,7 @@ inputs = processor.apply_chat_template(\n     return_tensors=\"pt\",\n     video_fps=1,\n \n-    # kwargs to be passed to `Qwen2-5-OmniProcessor`\n+    # kwargs to be passed to `Qwen3OmniMoeProcessor`\n     padding=True,\n     use_audio_in_video=True,\n ).to(model.device)\n@@ -101,17 +101,17 @@ print(text)\n \n ### Text-only generation\n \n-To generate only text output and save compute by not loading the audio generation model, we can use `Qwen2_5OmniThinkerForConditionalGeneration` model.  \n+To generate only text output and save compute by not loading the audio generation model, we can use `Qwen3OmniMoeThinkerForConditionalGeneration` model.\n \n ```python\n-from transformers import Qwen2_5OmniThinkerForConditionalGeneration, Qwen2_5OmniProcessor\n+from transformers import Qwen3OmniMoeThinkerForConditionalGeneration, Qwen3OmniMoeProcessor\n \n-model = Qwen2_5OmniThinkerForConditionalGeneration.from_pretrained(\n-    \"Qwen/Qwen2.5-Omni-7B\",\n+model = Qwen3OmniMoeThinkerForConditionalGeneration.from_pretrained(\n+    \"Qwen/Qwen3-Omni-30B-A3B-Instruct\",\n     dtype=\"auto\",\n     device_map=\"auto\",\n )\n-processor = Qwen2_5OmniProcessor.from_pretrained(\"Qwen/Qwen2.5-Omni-7B\")\n+processor = Qwen3OmniMoeProcessor.from_pretrained(\"Qwen/Qwen3-Omni-30B-A3B-Instruct\")\n \n conversations = [\n     {\n@@ -138,7 +138,7 @@ inputs = processor.apply_chat_template(\n     return_tensors=\"pt\",\n     video_fps=1,\n \n-    # kwargs to be passed to `Qwen2-5-OmniProcessor`\n+    # kwargs to be passed to `Qwen3OmniMoeProcessor`\n     padding=True,\n     use_audio_in_video=True,\n ).to(model.device)\n@@ -157,18 +157,18 @@ print(text)\n \n ### Batch Mixed Media Inference\n \n-The model can batch inputs composed of mixed samples of various types such as text, images, audio and videos as input when using `Qwen2_5OmniThinkerForConditionalGeneration` model. Here is an example.\n+The model can batch inputs composed of mixed samples of various types such as text, images, audio and videos as input when using `Qwen3OmniMoeThinkerForConditionalGeneration` model. Here is an example.\n \n ```python\n import soundfile as sf\n-from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor\n+from transformers import Qwen3OmniMoeForConditionalGeneration, Qwen3OmniMoeProcessor\n \n-model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n-    \"Qwen/Qwen2.5-Omni-7B\",\n+model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n+    \"Qwen/Qwen3-Omni-30B-A3B-Instruct\",\n     dtype=\"auto\",\n     device_map=\"auto\"\n )\n-processor = Qwen2_5OmniProcessor.from_pretrained(\"Qwen/Qwen2.5-Omni-7B\")\n+processor = Qwen3OmniMoeProcessor.from_pretrained(\"Qwen/Qwen3-Omni-30B-A3B-Instruct\")\n \n # Conversation with video only\n conversation1 = [\n@@ -247,7 +247,7 @@ inputs = processor.apply_chat_template(\n     return_tensors=\"pt\",\n     video_fps=1,\n \n-    # kwargs to be passed to `Qwen2-5-OmniProcessor`\n+    # kwargs to be passed to `Qwen3OmniMoeProcessor`\n     padding=True,\n     use_audio_in_video=True,\n ).to(model.thinker.device)\n@@ -267,7 +267,7 @@ The model supports a wide range of resolution inputs. By default, it uses the na\n ```python\n min_pixels = 128*28*28\n max_pixels = 768*28*28\n-processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-Omni-7B\", min_pixels=min_pixels, max_pixels=max_pixels)\n+processor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-Omni-30B-A3B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n ```\n \n #### Prompt for audio output\n@@ -285,8 +285,8 @@ If users need audio output, the system prompt must be set as \"You are Qwen, a vi\n The model supports both text and audio outputs, if users do not need audio outputs, they can set `enable_audio_output` in the `from_pretrained` function. This option will save about `~2GB` of GPU memory but the `return_audio` option for `generate` function will only allow to be set at `False`.\n \n ```python\n-model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n-    \"Qwen/Qwen2.5-Omni-7B\",\n+model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n+    \"Qwen/Qwen3-Omni-30B-A3B-Instruct\",\n     dtype=\"auto\",\n     device_map=\"auto\",\n     enable_audio_output=False,\n@@ -296,8 +296,8 @@ model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n In order to obtain a flexible experience, we recommend that users set `enable_audio_output` at `True` when initializing the model through `from_pretrained` function, and then decide whether to return audio when `generate` function is called. When `return_audio` is set to `False`, the model will only return text outputs to get text responses faster.\n \n ```python\n-model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n-    \"Qwen/Qwen2.5-Omni-7B\",\n+model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n+    \"Qwen/Qwen3-Omni-30B-A3B-Instruct\",\n     dtype=\"auto\",\n     device_map=\"auto\",\n     enable_audio_output=True,\n@@ -307,7 +307,7 @@ text_ids = model.generate(**inputs, return_audio=False)\n ```\n \n #### Change voice type of output audio\n-Qwen2.5-Omni supports the ability to change the voice of the output audio. Users can use the `spk` parameter of `generate` function to specify the voice type. The `\"Qwen/Qwen2.5-Omni-7B\"` checkpoint support two voice types: `Chelsie` and `Ethan`, while `Chelsie` is a female voice and `Ethan` is a male voice. By default, if `spk` is not specified, the default voice type is `Chelsie`.\n+Qwen3-Omni-MOE supports the ability to change the voice of the output audio. Users can use the `spk` parameter of `generate` function to specify the voice type. The `\"Qwen/Qwen3-Omni-30B-A3B-Instruct\"` checkpoint support two voice types: `Chelsie` and `Ethan`, while `Chelsie` is a female voice and `Ethan` is a male voice. By default, if `spk` is not specified, the default voice type is `Chelsie`.\n \n ```python\n text_ids, audio = model.generate(**inputs, spk=\"Chelsie\")\n@@ -330,10 +330,10 @@ Also, you should have hardware that is compatible with FlashAttention 2. Read mo\n To load and run a model using FlashAttention-2, add `attn_implementation=\"flash_attention_2\"` when loading the model:\n \n ```python\n-from transformers import Qwen2_5OmniForConditionalGeneration\n+from transformers import Qwen3OmniMoeForConditionalGeneration\n \n-model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n-    \"Qwen/Qwen2.5-Omni-7B\",\n+model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n+    \"Qwen/Qwen3-Omni-30B-A3B-Instruct\",\n     device_map=\"auto\",\n     dtype=torch.bfloat16,\n     attn_implementation=\"flash_attention_2\","
        },
        {
            "sha": "626b4119aa44f2e7d5cbb1c2872fa707658a1f74",
            "filename": "docs/source/en/model_doc/qwen3_vl.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/53838edde77cb10f3a360150aa85a457637e9ac3/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/53838edde77cb10f3a360150aa85a457637e9ac3/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_vl.md?ref=53838edde77cb10f3a360150aa85a457637e9ac3",
            "patch": "@@ -13,7 +13,7 @@ specific language governing permissions and limitations under the License.\n rendered properly in your Markdown viewer.\n \n -->\n-*This model was released on None and added to Hugging Face Transformers on 2025-09-15.*\n+*This model was released on 2025-02-19 and added to Hugging Face Transformers on 2025-09-15.*\n \n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">"
        },
        {
            "sha": "771f6d411cf2cfd60684b0fc34838f5c1fdf195e",
            "filename": "docs/source/en/model_doc/qwen3_vl_moe.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/53838edde77cb10f3a360150aa85a457637e9ac3/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_vl_moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/53838edde77cb10f3a360150aa85a457637e9ac3/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_vl_moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_vl_moe.md?ref=53838edde77cb10f3a360150aa85a457637e9ac3",
            "patch": "@@ -13,7 +13,7 @@ specific language governing permissions and limitations under the License.\n rendered properly in your Markdown viewer.\n \n -->\n-*This model was released on None and added to Hugging Face Transformers on 2025-09-15.*\n+*This model was released on 2025-02-19 and added to Hugging Face Transformers on 2025-09-15.*\n \n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">"
        },
        {
            "sha": "deada15dc0f79ff9e8a8ecd6b27d343a16ba4b7b",
            "filename": "docs/source/en/model_doc/vaultgemma.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/53838edde77cb10f3a360150aa85a457637e9ac3/docs%2Fsource%2Fen%2Fmodel_doc%2Fvaultgemma.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/53838edde77cb10f3a360150aa85a457637e9ac3/docs%2Fsource%2Fen%2Fmodel_doc%2Fvaultgemma.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvaultgemma.md?ref=53838edde77cb10f3a360150aa85a457637e9ac3",
            "patch": "@@ -15,7 +15,7 @@ limitations under the License.\n ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be rendered properly in your Markdown viewer.\n \n -->\n-*This model was released on {release_date} and added to Hugging Face Transformers on 2025-09-12.*\n+*This model was released on 2016-07-01 and added to Hugging Face Transformers on 2025-09-12.*\n \n # VaultGemma\n "
        },
        {
            "sha": "cfeed77818fd90ebd4bdd8b50b15c558161c99af",
            "filename": "utils/add_dates.py",
            "status": "modified",
            "additions": 37,
            "deletions": 24,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/53838edde77cb10f3a360150aa85a457637e9ac3/utils%2Fadd_dates.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/53838edde77cb10f3a360150aa85a457637e9ac3/utils%2Fadd_dates.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fadd_dates.py?ref=53838edde77cb10f3a360150aa85a457637e9ac3",
            "patch": "@@ -2,6 +2,7 @@\n import os\n import re\n import subprocess\n+from datetime import date\n from typing import Optional\n \n from huggingface_hub import paper_info\n@@ -36,15 +37,14 @@\n def get_modified_cards() -> list[str]:\n     \"\"\"Get the list of model names from modified files in docs/source/en/model_doc/\"\"\"\n \n-    result = subprocess.check_output([\"git\", \"status\", \"--porcelain\"], text=True)\n+    result = subprocess.check_output([\"git\", \"diff\", \"--name-only\", \"upstream/main\"], text=True)\n \n     model_names = []\n     for line in result.strip().split(\"\\n\"):\n         if line:\n-            # Split on whitespace and take the last part (filename)\n-            filename = line.split()[-1]\n-            if filename.startswith(\"docs/source/en/model_doc/\") and filename.endswith(\".md\"):\n-                model_name = os.path.splitext(os.path.basename(filename))[0]\n+            # Check if the file is in the model_doc directory\n+            if line.startswith(\"docs/source/en/model_doc/\") and line.endswith(\".md\"):\n+                model_name = os.path.splitext(os.path.basename(line))[0]\n                 if model_name not in [\"auto\", \"timm_wrapper\"]:\n                     model_names.append(model_name)\n \n@@ -61,13 +61,10 @@ def get_paper_link(model_card: Optional[str], path: Optional[str]) -> str:\n     with open(file_path, \"r\", encoding=\"utf-8\") as f:\n         content = f.read()\n \n-    if \"blog\" in content or \"report\" in content or \"post\" in content:\n-        print(f\"Insert the release date of the blog post or technical report at the top of {model_card}\")\n-        return \"blog\"\n-\n     # Find known paper links\n     paper_ids = re.findall(r\"https://huggingface\\.co/papers/\\d+\\.\\d+\", content)\n     paper_ids += re.findall(r\"https://arxiv\\.org/abs/\\d+\\.\\d+\", content)\n+    paper_ids += re.findall(r\"https://arxiv\\.org/pdf/\\d+\\.\\d+\", content)\n \n     # If no known paper links are found, look for other potential paper links\n     if len(paper_ids) == 0:\n@@ -109,10 +106,19 @@ def get_first_commit_date(model_name: Optional[str]) -> str:\n     if not os.path.exists(file_path):\n         file_path = os.path.join(DOCS_PATH, f\"{model_name}.md\")\n \n-    result = subprocess.check_output(\n-        [\"git\", \"log\", \"--reverse\", \"--pretty=format:%ad\", \"--date=iso\", file_path], text=True\n+    # Check if file exists in upstream/main\n+    result_main = subprocess.check_output(\n+        [\"git\", \"ls-tree\", \"upstream/main\", \"--\", file_path], text=True, stderr=subprocess.DEVNULL\n     )\n-    return result.strip().split(\"\\n\")[0][:10]\n+    if not result_main:\n+        # File does not exist in upstream/main (new model), use today's date\n+        final_date = date.today().isoformat()\n+    else:\n+        # File exists in upstream/main, get the first commit date\n+        final_date = subprocess.check_output(\n+            [\"git\", \"log\", \"--reverse\", \"--pretty=format:%ad\", \"--date=iso\", file_path], text=True\n+        )\n+    return final_date.strip().split(\"\\n\")[0][:10]\n \n \n def get_release_date(link: str) -> str:\n@@ -125,7 +131,7 @@ def get_release_date(link: str) -> str:\n         except Exception as e:\n             print(f\"Error fetching release date for the paper https://huggingface.co/papers/{link}: {e}\")\n \n-    elif link.startswith(\"https://arxiv.org/abs/\"):\n+    elif link.startswith(\"https://arxiv.org/abs/\") or link.startswith(\"https://arxiv.org/pdf/\"):\n         print(f\"This paper {link} is not yet available in Hugging Face papers, skipping the release date attachment.\")\n         return r\"{release_date}\"\n \n@@ -144,13 +150,16 @@ def replace_paper_links(file_path: str) -> bool:\n \n     # Find all arxiv links\n     arxiv_links = re.findall(r\"https://arxiv\\.org/abs/(\\d+\\.\\d+)\", content)\n+    arxiv_links += re.findall(r\"https://arxiv\\.org/pdf/(\\d+\\.\\d+)\", content)\n \n     for paper_id in arxiv_links:\n         try:\n             # Check if paper exists on huggingface\n             paper_info(paper_id)\n             # If no exception, replace the link\n             old_link = f\"https://arxiv.org/abs/{paper_id}\"\n+            if old_link not in content:\n+                old_link = f\"https://arxiv.org/pdf/{paper_id}\"\n             new_link = f\"https://huggingface.co/papers/{paper_id}\"\n             content = content.replace(old_link, new_link)\n             print(f\"Replaced {old_link} with {new_link}\")\n@@ -204,13 +213,25 @@ def insert_dates(model_card_list: list[str]):\n \n         hf_commit_date = get_first_commit_date(model_name=model_card)\n \n+        paper_link = get_paper_link(model_card=model_card, path=file_path)\n+        release_date = \"\"\n+        if not (paper_link == \"No_paper\" or paper_link == \"blog\"):\n+            release_date = get_release_date(paper_link)\n+        else:\n+            release_date = r\"{release_date}\"\n+\n         match = re.search(pattern, content)\n \n-        # If the dates info line already exists, only check and update the hf_commit_date, don't modify the existing release date\n+        # If the dates info line already exists, preserve the existing release date unless it's a placeholder, and update the HF commit date if needed\n         if match:\n-            release_date = match.group(1)  # The release date part\n+            existing_release_date = match.group(1)  # The release date part\n             existing_hf_date = match.group(2)  # The existing HF date part\n-            if existing_hf_date != hf_commit_date:\n+            release_date = (\n+                release_date\n+                if (existing_release_date == r\"{release_date}\" or existing_release_date == \"None\")\n+                else existing_release_date\n+            )\n+            if existing_hf_date != hf_commit_date or existing_release_date != release_date:\n                 old_line = match.group(0)  # Full matched line\n                 new_line = f\"\\n*This model was released on {release_date} and added to Hugging Face Transformers on {hf_commit_date}.*\"\n \n@@ -220,14 +241,6 @@ def insert_dates(model_card_list: list[str]):\n \n         # If the dates info line does not exist, add it\n         else:\n-            paper_link = get_paper_link(model_card=model_card, path=file_path)\n-            release_date = \"\"\n-\n-            if not (paper_link == \"No_paper\" or paper_link == \"blog\"):\n-                release_date = get_release_date(paper_link)\n-            else:\n-                release_date = r\"{release_date}\"\n-\n             insert_index = markers[0].end()\n \n             date_info = f\"\\n*This model was released on {release_date} and added to Hugging Face Transformers on {hf_commit_date}.*\""
        }
    ],
    "stats": {
        "total": 147,
        "additions": 81,
        "deletions": 66
    }
}