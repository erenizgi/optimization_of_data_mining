{
    "author": "techkang",
    "message": "enable average tokens across devices (#34373)\n\n* enable average tokens across devices\r\n\r\n* reduce earlier in case model needs it\r\n\r\n* simplify if statement\r\n\r\n* reformat code to make ruff happy\r\n\r\n* add doc for argument: average_tokens_across_devices\r\n\r\n* cannot find world size when pytorch is unavailable\r\n\r\n* format code\r\n\r\n---------\r\n\r\nCo-authored-by: Zach Mueller <muellerzr@gmail.com>\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "d21dbd1520937c993de1409215b1418bd6be74a1",
    "files": [
        {
            "sha": "9176bd72a550327a4f058cf1e534c2b05d3ac977",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/d21dbd1520937c993de1409215b1418bd6be74a1/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d21dbd1520937c993de1409215b1418bd6be74a1/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=d21dbd1520937c993de1409215b1418bd6be74a1",
            "patch": "@@ -3631,7 +3631,12 @@ def training_step(\n             with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n                 scaled_loss.backward()\n         else:\n-            loss *= self.args.gradient_accumulation_steps\n+            if num_items_in_batch is not None:\n+                if self.compute_loss_func or self.model_accepts_loss_kwargs:\n+                    loss *= self.args.gradient_accumulation_steps\n+                # Average tokens across devices is orthogonal to gradient accumulation\n+                if self.args.average_tokens_across_devices:\n+                    loss *= self.args.world_size\n             self.accelerator.backward(loss, **kwargs)\n \n         return loss.detach() / self.args.gradient_accumulation_steps\n@@ -3646,6 +3651,9 @@ def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=N\n             labels = inputs.pop(\"labels\")\n         else:\n             labels = None\n+        if self.args.average_tokens_across_devices and num_items_in_batch is not None:\n+            num_items_in_batch_tensor = torch.tensor(num_items_in_batch, device=self.args.device)\n+            num_items_in_batch = int(self.accelerator.gather(num_items_in_batch_tensor).sum().cpu())\n         if self.model_accepts_loss_kwargs:\n             loss_kwargs = {}\n             if num_items_in_batch is not None:"
        },
        {
            "sha": "3e5c6cc2f37428c04276b6bbfd586deb0f80dd06",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 22,
            "deletions": 0,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/d21dbd1520937c993de1409215b1418bd6be74a1/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d21dbd1520937c993de1409215b1418bd6be74a1/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=d21dbd1520937c993de1409215b1418bd6be74a1",
            "patch": "@@ -1532,6 +1532,15 @@ class TrainingArguments:\n         },\n     )\n \n+    average_tokens_across_devices: Optional[bool] = field(\n+        default=False,\n+        metadata={\n+            \"help\": \"Whether or not to average tokens across devices. If enabled, will use all_reduce to \"\n+            \"synchronize num_tokens_in_batch for precise loss calculation. Reference: \"\n+            \"https://github.com/huggingface/transformers/issues/34242\"\n+        },\n+    )\n+\n     def __post_init__(self):\n         # Parse in args that could be `dict` sent in from the CLI as a string\n         for field in _VALID_DICT_FIELDS:\n@@ -1765,6 +1774,19 @@ def __post_init__(self):\n         if self.framework == \"pt\" and is_torch_available():\n             self.device\n \n+        # Disable average tokens when using single device\n+        if self.average_tokens_across_devices:\n+            try:\n+                if self.world_size == 1:\n+                    logger.warning(\n+                        \"average_tokens_across_devices is set to True but it is invalid when world size is\"\n+                        \"1. Turn it to False automatically.\"\n+                    )\n+                    self.average_tokens_across_devices = False\n+            except ImportError as e:\n+                logger.warning(f\"Can not specify world size due to {e}. Turn average_tokens_across_devices to False.\")\n+                self.average_tokens_across_devices = False\n+\n         if self.torchdynamo is not None:\n             warnings.warn(\n                 \"`torchdynamo` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use\""
        }
    ],
    "stats": {
        "total": 32,
        "additions": 31,
        "deletions": 1
    }
}