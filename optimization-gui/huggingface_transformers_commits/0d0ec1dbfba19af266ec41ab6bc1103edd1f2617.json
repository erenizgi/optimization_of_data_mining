{
    "author": "yijun-lee",
    "message": "ğŸŒ [i18n-KO] Translated `tokenization_utils.md` to Korean (#33813)\n\n* docs: ko: tokenization_utils.md\r\n\r\n* feat: nmt draft\r\n\r\n* fix: manual edits",
    "sha": "0d0ec1dbfba19af266ec41ab6bc1103edd1f2617",
    "files": [
        {
            "sha": "f333e06f463085bbf1fcdefc7a8106dae514d0f8",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d0ec1dbfba19af266ec41ab6bc1103edd1f2617/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d0ec1dbfba19af266ec41ab6bc1103edd1f2617/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=0d0ec1dbfba19af266ec41ab6bc1103edd1f2617",
            "patch": "@@ -773,10 +773,14 @@\n   - sections:\n     - local: in_translation\n       title: (ë²ˆì—­ì¤‘) Custom Layers and Utilities\n+    - local: in_translation\n+      title: (ë²ˆì—­ì¤‘) Utilities for pipelines\n+    - local: internal/tokenization_utils\n+      title: í† í¬ë‚˜ì´ì €ë¥¼ ìœ„í•œ ìœ í‹¸ë¦¬í‹°\n     - local: internal/pipelines_utils\n       title: íŒŒì´í”„ë¼ì¸ì„ ìœ„í•œ ìœ í‹¸ë¦¬í‹°\n-    - local: in_translation\n-      title: (ë²ˆì—­ì¤‘) Utilities for Tokenizers\n+    - local: internal/tokenization_utils\n+      title: í† í¬ë‚˜ì´ì €ë¥¼ ìœ„í•œ ìœ í‹¸ë¦¬í‹°\n     - local: in_translation\n       title: (ë²ˆì—­ì¤‘) Utilities for Trainer\n     - local: in_translation"
        },
        {
            "sha": "b5b69910479a2fb42aeaf4d8bd366967ea9f53e7",
            "filename": "docs/source/ko/internal/tokenization_utils.md",
            "status": "added",
            "additions": 39,
            "deletions": 0,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d0ec1dbfba19af266ec41ab6bc1103edd1f2617/docs%2Fsource%2Fko%2Finternal%2Ftokenization_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d0ec1dbfba19af266ec41ab6bc1103edd1f2617/docs%2Fsource%2Fko%2Finternal%2Ftokenization_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Finternal%2Ftokenization_utils.md?ref=0d0ec1dbfba19af266ec41ab6bc1103edd1f2617",
            "patch": "@@ -0,0 +1,39 @@\n+<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# í† í¬ë‚˜ì´ì €ë¥¼ ìœ„í•œ ìœ í‹¸ë¦¬í‹° [[utilities-for-tokenizers]]\n+\n+ì´ í˜ì´ì§€ëŠ” í† í¬ë‚˜ì´ì €ì—ì„œ ì‚¬ìš©ë˜ëŠ” ëª¨ë“  ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ì„ ë‚˜ì—´í•˜ë©°, ì£¼ë¡œ [`PreTrainedTokenizer`]ì™€ [`PreTrainedTokenizerFast`] ì‚¬ì´ì˜ ê³µí†µ ë©”ì†Œë“œë¥¼ êµ¬í˜„í•˜ëŠ” [`~tokenization_utils_base.PreTrainedTokenizerBase`] í´ë˜ìŠ¤ì™€ [`~tokenization_utils_base.SpecialTokensMixin`]ì„ ë‹¤ë£¹ë‹ˆë‹¤.\n+\n+ì´ í•¨ìˆ˜ë“¤ ëŒ€ë¶€ë¶„ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ í† í¬ë‚˜ì´ì € ì½”ë“œë¥¼ ì—°êµ¬í•  ë•Œë§Œ ìœ ìš©í•©ë‹ˆë‹¤.\n+\n+## PreTrainedTokenizerBase [[transformers.PreTrainedTokenizerBase]]\n+\n+[[autodoc]] tokenization_utils_base.PreTrainedTokenizerBase\n+   - __call__\n+   - all\n+\n+## SpecialTokensMixin [[transformers.SpecialTokensMixin]]\n+\n+[[autodoc]] tokenization_utils_base.SpecialTokensMixin\n+\n+## Enums ë° namedtuples [[transformers.tokenization_utils_base.TruncationStrategy]]\n+\n+[[autodoc]] tokenization_utils_base.TruncationStrategy\n+\n+[[autodoc]] tokenization_utils_base.CharSpan\n+\n+[[autodoc]] tokenization_utils_base.TokenSpan"
        }
    ],
    "stats": {
        "total": 47,
        "additions": 45,
        "deletions": 2
    }
}