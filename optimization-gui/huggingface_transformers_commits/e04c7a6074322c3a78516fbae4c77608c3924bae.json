{
    "author": "yonigozlan",
    "message": "Add Pix2Struct fast image processor (#42020)\n\nAdd pix2struct fast image processor",
    "sha": "e04c7a6074322c3a78516fbae4c77608c3924bae",
    "files": [
        {
            "sha": "6a68b6381a01e3bb487c226c67183bb13298d5e9",
            "filename": "docs/source/en/model_doc/pix2struct.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e04c7a6074322c3a78516fbae4c77608c3924bae/docs%2Fsource%2Fen%2Fmodel_doc%2Fpix2struct.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e04c7a6074322c3a78516fbae4c77608c3924bae/docs%2Fsource%2Fen%2Fmodel_doc%2Fpix2struct.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpix2struct.md?ref=e04c7a6074322c3a78516fbae4c77608c3924bae",
            "patch": "@@ -65,6 +65,11 @@ The original code can be found [here](https://github.com/google-research/pix2str\n [[autodoc]] Pix2StructImageProcessor\n     - preprocess\n \n+## Pix2StructImageProcessorFast\n+\n+[[autodoc]] Pix2StructImageProcessorFast\n+    - preprocess\n+\n ## Pix2StructTextModel\n \n [[autodoc]] Pix2StructTextModel"
        },
        {
            "sha": "8f37c962528c37c0df89a65380551fe1198e4bba",
            "filename": "src/transformers/cli/add_fast_image_processor.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e04c7a6074322c3a78516fbae4c77608c3924bae/src%2Ftransformers%2Fcli%2Fadd_fast_image_processor.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e04c7a6074322c3a78516fbae4c77608c3924bae/src%2Ftransformers%2Fcli%2Fadd_fast_image_processor.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcli%2Fadd_fast_image_processor.py?ref=e04c7a6074322c3a78516fbae4c77608c3924bae",
            "patch": "@@ -59,8 +59,6 @@ def add_fast_image_processor(\n     image_processor_name = re.findall(r\"class (\\w*ImageProcessor)\", content_base_file)\n     if not image_processor_name:\n         raise ValueError(f\"No ImageProcessor class found in {image_processing_module_file}\")\n-    elif len(image_processor_name) > 1:\n-        raise ValueError(f\"Multiple ImageProcessor classes found in {image_processing_module_file}\")\n \n     image_processor_name = image_processor_name[0]\n     fast_image_processor_name = image_processor_name + \"Fast\""
        },
        {
            "sha": "762b7160f155b71c8eaddb2717e8572e49c0e5fc",
            "filename": "src/transformers/image_processing_utils_fast.py",
            "status": "modified",
            "additions": 123,
            "deletions": 0,
            "changes": 123,
            "blob_url": "https://github.com/huggingface/transformers/blob/e04c7a6074322c3a78516fbae4c77608c3924bae/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e04c7a6074322c3a78516fbae4c77608c3924bae/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils_fast.py?ref=e04c7a6074322c3a78516fbae4c77608c3924bae",
            "patch": "@@ -166,6 +166,129 @@ def divide_to_patches(\n \n @auto_docstring\n class BaseImageProcessorFast(BaseImageProcessor):\n+    r\"\"\"\n+    Base class for fast image processors using PyTorch and TorchVision for image transformations.\n+\n+    This class provides a complete implementation for standard image preprocessing operations (resize, crop, rescale,\n+    normalize) with GPU support and batch processing optimizations. Most image processors can be implemented by simply\n+    setting class attributes; only processors requiring custom logic need to override methods.\n+\n+    Basic Implementation\n+    --------------------\n+\n+    For processors that only need standard operations (resize, center crop, rescale, normalize), define class\n+    attributes:\n+\n+        class MyImageProcessorFast(BaseImageProcessorFast):\n+            resample = PILImageResampling.BILINEAR\n+            image_mean = IMAGENET_DEFAULT_MEAN\n+            image_std = IMAGENET_DEFAULT_STD\n+            size = {\"height\": 224, \"width\": 224}\n+            do_resize = True\n+            do_rescale = True\n+            do_normalize = True\n+\n+    Custom Processing\n+    -----------------\n+\n+    Override `_preprocess` (most common):\n+        For custom image processing logic, override `_preprocess`. This method receives a list of torch tensors with\n+        channel dimension first and should return a BatchFeature. Use `group_images_by_shape` and `reorder_images` for\n+        efficient batch processing:\n+\n+            def _preprocess(\n+                self,\n+                images: list[torch.Tensor],\n+                do_resize: bool,\n+                size: SizeDict,\n+                # ... other parameters\n+                **kwargs,\n+            ) -> BatchFeature:\n+                # Group images by shape for batched operations\n+                grouped_images, indices = group_images_by_shape(images)\n+                processed_groups = {}\n+\n+                for shape, stacked_images in grouped_images.items():\n+                    if do_resize:\n+                        stacked_images = self.resize(stacked_images, size)\n+                    # Custom processing here\n+                    processed_groups[shape] = stacked_images\n+\n+                processed_images = reorder_images(processed_groups, indices)\n+                return BatchFeature(data={\"pixel_values\": torch.stack(processed_images)})\n+\n+    Override `_preprocess_image_like_inputs` (for additional inputs):\n+        For processors handling multiple input types (e.g., images + segmentation maps), override this method:\n+\n+            def _preprocess_image_like_inputs(\n+                self,\n+                images: ImageInput,\n+                segmentation_maps: Optional[ImageInput] = None,\n+                do_convert_rgb: bool,\n+                input_data_format: ChannelDimension,\n+                device: Optional[torch.device] = None,\n+                **kwargs,\n+            ) -> BatchFeature:\n+                images = self._prepare_image_like_inputs(images, do_convert_rgb, input_data_format, device)\n+                batch_feature = self._preprocess(images, **kwargs)\n+\n+                if segmentation_maps is not None:\n+                    # Process segmentation maps separately\n+                    maps = self._prepare_image_like_inputs(segmentation_maps, ...)\n+                    batch_feature[\"labels\"] = self._preprocess(maps, ...)\n+\n+                return batch_feature\n+\n+    Override `_further_process_kwargs` (for custom kwargs formatting):\n+        To format custom kwargs before validation:\n+\n+            def _further_process_kwargs(self, custom_param=None, **kwargs):\n+                kwargs = super()._further_process_kwargs(**kwargs)\n+                if custom_param is not None:\n+                    kwargs[\"custom_param\"] = self._format_custom_param(custom_param)\n+                return kwargs\n+\n+    Override `_validate_preprocess_kwargs` (for custom validation):\n+        To add custom validation logic:\n+\n+            def _validate_preprocess_kwargs(self, custom_param=None, **kwargs):\n+                super()._validate_preprocess_kwargs(**kwargs)\n+                if custom_param is not None and custom_param < 0:\n+                    raise ValueError(\"custom_param must be non-negative\")\n+\n+    Override `_prepare_images_structure` (for nested inputs):\n+        By default, nested image lists are flattened. Override to preserve structure:\n+\n+            def _prepare_images_structure(self, images, expected_ndims=3):\n+                # Custom logic to handle nested structure\n+                return images  # Return as-is or with custom processing\n+\n+    Custom Parameters\n+    -----------------\n+\n+    To add parameters beyond `ImagesKwargs`, create a custom kwargs class and set it as `valid_kwargs`:\n+\n+        class MyImageProcessorKwargs(ImagesKwargs):\n+            custom_param: Optional[int] = None\n+            another_param: Optional[bool] = None\n+\n+        class MyImageProcessorFast(BaseImageProcessorFast):\n+            valid_kwargs = MyImageProcessorKwargs\n+            custom_param = 10  # default value\n+\n+            def _preprocess(self, images, custom_param, **kwargs):\n+                # Use custom_param in processing\n+                ...\n+\n+    Key Notes\n+    ---------\n+\n+    - Images in `_preprocess` are always torch tensors with channel dimension first, regardless of input format\n+    - Arguments not provided by users default to class attribute values\n+    - Use batch processing utilities (`group_images_by_shape`, `reorder_images`) for GPU efficiency\n+    - Image loading, format conversion, and argument handling are automatic - focus only on processing logic\n+    \"\"\"\n+\n     resample = None\n     image_mean = None\n     image_std = None"
        },
        {
            "sha": "2644741006dd5cdab0438f08faf16c3d87d04236",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e04c7a6074322c3a78516fbae4c77608c3924bae/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e04c7a6074322c3a78516fbae4c77608c3924bae/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=e04c7a6074322c3a78516fbae4c77608c3924bae",
            "patch": "@@ -157,7 +157,7 @@\n             (\"perceiver\", (\"PerceiverImageProcessor\", \"PerceiverImageProcessorFast\")),\n             (\"perception_lm\", (None, \"PerceptionLMImageProcessorFast\")),\n             (\"phi4_multimodal\", (None, \"Phi4MultimodalImageProcessorFast\")),\n-            (\"pix2struct\", (\"Pix2StructImageProcessor\", None)),\n+            (\"pix2struct\", (\"Pix2StructImageProcessor\", \"Pix2StructImageProcessorFast\")),\n             (\"pixtral\", (\"PixtralImageProcessor\", \"PixtralImageProcessorFast\")),\n             (\"poolformer\", (\"PoolFormerImageProcessor\", \"PoolFormerImageProcessorFast\")),\n             (\"prompt_depth_anything\", (\"PromptDepthAnythingImageProcessor\", \"PromptDepthAnythingImageProcessorFast\")),"
        },
        {
            "sha": "7cf28e634b13d18369818b582d4e82127001586a",
            "filename": "src/transformers/models/pix2struct/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e04c7a6074322c3a78516fbae4c77608c3924bae/src%2Ftransformers%2Fmodels%2Fpix2struct%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e04c7a6074322c3a78516fbae4c77608c3924bae/src%2Ftransformers%2Fmodels%2Fpix2struct%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2F__init__.py?ref=e04c7a6074322c3a78516fbae4c77608c3924bae",
            "patch": "@@ -20,6 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_pix2struct import *\n     from .image_processing_pix2struct import *\n+    from .image_processing_pix2struct_fast import *\n     from .modeling_pix2struct import *\n     from .processing_pix2struct import *\n else:"
        },
        {
            "sha": "8dcea3c9ab27c0e4f46476d931f6d55270976b94",
            "filename": "src/transformers/models/pix2struct/image_processing_pix2struct_fast.py",
            "status": "added",
            "additions": 339,
            "deletions": 0,
            "changes": 339,
            "blob_url": "https://github.com/huggingface/transformers/blob/e04c7a6074322c3a78516fbae4c77608c3924bae/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fimage_processing_pix2struct_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e04c7a6074322c3a78516fbae4c77608c3924bae/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fimage_processing_pix2struct_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fimage_processing_pix2struct_fast.py?ref=e04c7a6074322c3a78516fbae4c77608c3924bae",
            "patch": "@@ -0,0 +1,339 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for Pix2Struct.\"\"\"\n+\n+from typing import Optional, Union\n+\n+import torch\n+from PIL import Image\n+from torchvision.transforms.v2 import functional as F\n+\n+from ...image_processing_utils import BatchFeature, get_size_dict\n+from ...image_processing_utils_fast import BaseImageProcessorFast\n+from ...image_transforms import group_images_by_shape, reorder_images\n+from ...image_utils import ChannelDimension, ImageInput, SizeDict\n+from ...processing_utils import Unpack\n+from ...utils import TensorType, auto_docstring\n+from .image_processing_pix2struct import Pix2StructImageProcessorKwargs, render_text\n+\n+\n+# Disable as it causes issues with torch.compile\n+@torch.compiler.disable\n+def torch_extract_patches(image_tensor, patch_height, patch_width):\n+    \"\"\"\n+    Extract patches from image tensor. Returns tensor of shape (batch, rows, columns, patch_height*patch_width*channels).\n+\n+    Args:\n+        image_tensor (`torch.Tensor`):\n+            Image tensor of shape (batch, channels, height, width).\n+        patch_height (`int`):\n+            Height of patches to extract.\n+        patch_width (`int`):\n+            Width of patches to extract.\n+    \"\"\"\n+    batch_size, channels, height, width = image_tensor.shape\n+    patches = torch.nn.functional.unfold(image_tensor, (patch_height, patch_width), stride=(patch_height, patch_width))\n+    patches = patches.reshape(batch_size, channels, patch_height, patch_width, -1)\n+    patches = patches.permute(0, 4, 2, 3, 1).reshape(\n+        batch_size, height // patch_height, width // patch_width, channels * patch_height * patch_width\n+    )\n+    return patches\n+\n+\n+@auto_docstring\n+class Pix2StructImageProcessorFast(BaseImageProcessorFast):\n+    rescale_factor = None\n+    do_normalize = True\n+    do_convert_rgb = True\n+    patch_size = {\"height\": 16, \"width\": 16}\n+    max_patches = 2048\n+    is_vqa = False\n+    valid_kwargs = Pix2StructImageProcessorKwargs\n+    model_input_names = [\"flattened_patches\", \"attention_mask\"]\n+\n+    def _further_process_kwargs(\n+        self,\n+        patch_size: Optional[dict[str, int]] = None,\n+        **kwargs,\n+    ) -> dict:\n+        \"\"\"\n+        Process custom Pix2Struct kwargs, specifically converting patch_size to SizeDict.\n+        \"\"\"\n+        # Call super to handle standard kwargs processing (like converting patch_size to SizeDict)\n+        kwargs = super()._further_process_kwargs(**kwargs)\n+        kwargs[\"patch_size\"] = SizeDict(**get_size_dict(size=patch_size, param_name=\"patch_size\"))\n+\n+        return kwargs\n+\n+    def _validate_preprocess_kwargs(self, **kwargs):\n+        \"\"\"\n+        Skip standard validation as Pix2Struct uses custom preprocessing.\n+        \"\"\"\n+        # Pix2Struct doesn't use standard resize/rescale/normalize parameters\n+        # so we skip the default validation\n+        pass\n+\n+    def render_header(\n+        self,\n+        image: torch.Tensor,\n+        header: str,\n+        font_bytes: Optional[bytes] = None,\n+        font_path: Optional[str] = None,\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Render header text on image using torch tensors.\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                Image tensor in channel-first format (C, H, W).\n+            header (`str`):\n+                Header text to render.\n+            font_bytes (`bytes`, *optional*):\n+                Font bytes to use for rendering.\n+            font_path (`str`, *optional*):\n+                Path to font file to use for rendering.\n+\n+        Returns:\n+            `torch.Tensor`: Image with header in channel-first format (C, H, W).\n+        \"\"\"\n+        device = image.device\n+        dtype = image.dtype\n+\n+        # Convert tensor to PIL (channel-first to channel-last for PIL)\n+        if image.dtype == torch.uint8:\n+            image_pil = F.to_pil_image(image)\n+        else:\n+            # If float, convert to uint8 first\n+            image_uint8 = (image * 255).clamp(0, 255).to(torch.uint8)\n+            image_pil = F.to_pil_image(image_uint8)\n+\n+        # Render header text as PIL image\n+        header_image = render_text(header, font_bytes=font_bytes, font_path=font_path)\n+\n+        # Calculate new dimensions\n+        new_width = max(header_image.width, image_pil.width)\n+        new_height = int(image_pil.height * (new_width / image_pil.width))\n+        new_header_height = int(header_image.height * (new_width / header_image.width))\n+\n+        # Create new image and paste header and original image\n+        new_image = Image.new(\"RGB\", (new_width, new_height + new_header_height), \"white\")\n+        new_image.paste(header_image.resize((new_width, new_header_height)), (0, 0))\n+        new_image.paste(image_pil.resize((new_width, new_height)), (0, new_header_height))\n+\n+        # Convert back to tensor (channel-first)\n+        result = F.pil_to_tensor(new_image).to(device)\n+\n+        # Convert back to original dtype if needed\n+        if dtype != torch.uint8:\n+            result = result.float() / 255.0\n+\n+        return result\n+\n+    def normalize(self, images: torch.Tensor) -> torch.Tensor:\n+        \"\"\"\n+        Normalize batched images using per-image mean and standard deviation.\n+\n+        Args:\n+            images (`torch.Tensor`):\n+                Batched float image tensor of shape (B, C, H, W).\n+\n+        Returns:\n+            `torch.Tensor`: Normalized images of shape (B, C, H, W).\n+        \"\"\"\n+        # Compute mean and std per image along spatial and channel dimensions\n+        mean = images.mean(dim=(1, 2, 3), keepdim=True)  # Shape: (B, 1, 1, 1)\n+        std = images.std(dim=(1, 2, 3), keepdim=True)  # Shape: (B, 1, 1, 1)\n+\n+        num_elements_per_image = images.shape[1] * images.shape[2] * images.shape[3]\n+        min_std = 1.0 / num_elements_per_image**0.5\n+        adjusted_stddev = torch.maximum(std, torch.tensor(min_std, device=std.device))\n+\n+        return (images - mean) / adjusted_stddev\n+\n+    def extract_flattened_patches(\n+        self,\n+        images: torch.Tensor,\n+        max_patches: int,\n+        patch_size: SizeDict,\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Extract flattened patches from a batch of images.\n+\n+        Args:\n+            images (`torch.Tensor`):\n+                Batched images tensor of shape (batch, channels, height, width).\n+            max_patches (`int`):\n+                Maximum number of patches to extract.\n+            patch_size (`dict[str, int]`):\n+                Dictionary containing patch height and width.\n+\n+        Returns:\n+            `torch.Tensor`: Batched flattened patches with row/column IDs of shape (batch, max_patches, patch_dim).\n+        \"\"\"\n+        patch_height, patch_width = patch_size.height, patch_size.width\n+        batch_size, channels, image_height, image_width = images.shape\n+\n+        # Calculate scale to maximize patches while respecting max_patches\n+        scale = (max_patches * (patch_height / image_height) * (patch_width / image_width)) ** 0.5\n+        num_feasible_rows = max(min(int(scale * image_height / patch_height), max_patches), 1)\n+        num_feasible_cols = max(min(int(scale * image_width / patch_width), max_patches), 1)\n+        resized_height = max(num_feasible_rows * patch_height, 1)\n+        resized_width = max(num_feasible_cols * patch_width, 1)\n+\n+        # Resize images (batched) using parent class method\n+        resize_size = SizeDict(height=resized_height, width=resized_width)\n+        images = self.resize(\n+            image=images, size=resize_size, interpolation=F.InterpolationMode.BILINEAR, antialias=True\n+        )\n+\n+        # Extract patches: [batch, rows, columns, patch_height * patch_width * channels]\n+        patches = torch_extract_patches(images, patch_height, patch_width)\n+\n+        batch_size, rows, columns, depth = patches.shape\n+\n+        # Reshape to [batch, rows * columns, depth]\n+        patches = patches.reshape(batch_size, rows * columns, depth)\n+\n+        # Create row and column IDs\n+        row_ids = (\n+            torch.arange(rows, device=images.device).reshape(rows, 1).repeat(1, columns).reshape(1, rows * columns, 1)\n+        )\n+        col_ids = (\n+            torch.arange(columns, device=images.device)\n+            .reshape(1, columns)\n+            .repeat(rows, 1)\n+            .reshape(1, rows * columns, 1)\n+        )\n+\n+        # Expand to batch size\n+        row_ids = row_ids.expand(batch_size, -1, -1)\n+        col_ids = col_ids.expand(batch_size, -1, -1)\n+\n+        # Offset by 1 so IDs don't contain zeros (which represent padding)\n+        row_ids = (row_ids + 1).float()\n+        col_ids = (col_ids + 1).float()\n+\n+        # Concatenate row_ids, col_ids, and patches: [batch, rows * columns, 2 + depth]\n+        result = torch.cat([row_ids, col_ids, patches], dim=-1)\n+\n+        # Pad to max_patches: [batch, max_patches, 2 + depth]\n+        result = torch.nn.functional.pad(result, [0, 0, 0, max_patches - (rows * columns)]).float()\n+\n+        return result\n+\n+    @auto_docstring\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        header_text: Optional[Union[str, list[str]]] = None,\n+        **kwargs: Unpack[Pix2StructImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        r\"\"\"\n+        header_text (`Union[str, list[str]]`, *optional*):\n+            Text to render as a header. Only has an effect if `image_processor.is_vqa` is `True`.\n+        \"\"\"\n+        return super().preprocess(images, header_text=header_text, **kwargs)\n+\n+    def _preprocess_image_like_inputs(\n+        self,\n+        images: ImageInput,\n+        header_text: Optional[Union[str, list[str]]] = None,\n+        do_convert_rgb: bool = True,\n+        input_data_format: ChannelDimension = ChannelDimension.FIRST,\n+        device: Optional[Union[str, torch.device]] = None,\n+        **kwargs: Unpack[Pix2StructImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess images for Pix2Struct.\n+        \"\"\"\n+        # Prepare images (converts to torch tensors)\n+        images = self._prepare_image_like_inputs(\n+            images=images,\n+            do_convert_rgb=do_convert_rgb,\n+            input_data_format=input_data_format,\n+            device=device,\n+        )\n+\n+        # Handle VQA mode with header rendering\n+        is_vqa = kwargs.get(\"is_vqa\", self.is_vqa)\n+        if is_vqa:\n+            if header_text is None:\n+                raise ValueError(\"A header text must be provided for VQA models.\")\n+\n+            font_bytes = kwargs.pop(\"font_bytes\", None)\n+            font_path = kwargs.pop(\"font_path\", None)\n+\n+            if isinstance(header_text, str):\n+                header_text = [header_text] * len(images)\n+\n+            # Render headers using torch-native method\n+            images = [\n+                self.render_header(image, header_text[i], font_bytes=font_bytes, font_path=font_path)\n+                for i, image in enumerate(images)\n+            ]\n+\n+        return self._preprocess(images, **kwargs)\n+\n+    def _preprocess(\n+        self,\n+        images: list[torch.Tensor],\n+        do_normalize: bool,\n+        max_patches: int,\n+        patch_size: SizeDict,\n+        return_tensors: Optional[Union[str, TensorType]],\n+        disable_grouping: bool,\n+        **kwargs,\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess images to extract flattened patches.\n+        \"\"\"\n+        # Group images by shape first for efficient batch processing\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+\n+        flattened_patches_grouped = {}\n+        attention_masks_grouped = {}\n+\n+        for shape, stacked_images in grouped_images.items():\n+            # Convert to float if needed (for resize and other operations)\n+            if stacked_images.dtype == torch.uint8:\n+                stacked_images = stacked_images.float()\n+\n+            # Normalize batched images with per-image mean and std\n+            if do_normalize:\n+                stacked_images = self.normalize(stacked_images)\n+\n+            patches = self.extract_flattened_patches(\n+                images=stacked_images, max_patches=max_patches, patch_size=patch_size\n+            )\n+            masks = (patches.sum(dim=-1) != 0).float()\n+\n+            flattened_patches_grouped[shape] = patches\n+            attention_masks_grouped[shape] = masks\n+\n+        flattened_patches = reorder_images(flattened_patches_grouped, grouped_images_index)\n+        attention_masks = reorder_images(attention_masks_grouped, grouped_images_index)\n+\n+        # Stack if return_tensors is set\n+        if return_tensors:\n+            flattened_patches = torch.stack(flattened_patches, dim=0)\n+            attention_masks = torch.stack(attention_masks, dim=0)\n+\n+        return BatchFeature(\n+            data={\"flattened_patches\": flattened_patches, \"attention_mask\": attention_masks},\n+            tensor_type=return_tensors,\n+        )\n+\n+\n+__all__ = [\"Pix2StructImageProcessorFast\"]"
        },
        {
            "sha": "0fb3019cf9daa3fe4f886ec7b95036edac88f093",
            "filename": "tests/models/pix2struct/test_image_processing_pix2struct.py",
            "status": "modified",
            "additions": 286,
            "deletions": 200,
            "changes": 486,
            "blob_url": "https://github.com/huggingface/transformers/blob/e04c7a6074322c3a78516fbae4c77608c3924bae/tests%2Fmodels%2Fpix2struct%2Ftest_image_processing_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e04c7a6074322c3a78516fbae4c77608c3924bae/tests%2Fmodels%2Fpix2struct%2Ftest_image_processing_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpix2struct%2Ftest_image_processing_pix2struct.py?ref=e04c7a6074322c3a78516fbae4c77608c3924bae",
            "patch": "@@ -16,10 +16,11 @@\n import unittest\n \n import numpy as np\n+from packaging import version\n \n from transformers.image_utils import load_image\n-from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.testing_utils import require_torch, require_torch_accelerator, require_vision, slow, torch_device\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n from ...test_processing_common import url_to_local_path\n@@ -33,6 +34,9 @@\n \n     from transformers import Pix2StructImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import Pix2StructImageProcessorFast\n+\n \n class Pix2StructImageProcessingTester:\n     def __init__(\n@@ -87,6 +91,7 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n @require_vision\n class Pix2StructImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = Pix2StructImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = Pix2StructImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -97,198 +102,265 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processor = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processor, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processor, \"do_convert_rgb\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processor, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processor, \"do_convert_rgb\"))\n \n     def test_expected_patches(self):\n         dummy_image = self.image_processor_tester.prepare_dummy_image()\n \n-        image_processor = self.image_processing_class(**self.image_processor_dict)\n-        max_patch = 2048\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            max_patch = 2048\n \n-        inputs = image_processor(dummy_image, return_tensors=\"pt\", max_patches=max_patch)\n-        torch.testing.assert_close(inputs.flattened_patches.mean(), torch.tensor(0.0606), rtol=1e-3, atol=1e-3)\n+            inputs = image_processor(dummy_image, return_tensors=\"pt\", max_patches=max_patch)\n+            torch.testing.assert_close(inputs.flattened_patches.mean(), torch.tensor(0.0606), rtol=1e-3, atol=1e-3)\n \n     def test_call_pil(self):\n-        # Initialize image_processor\n-        image_processor = self.image_processing_class(**self.image_processor_dict)\n-        # create random PIL images\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n-        for image in image_inputs:\n-            self.assertIsInstance(image, Image.Image)\n-\n-        # Test not batched input\n-        expected_hidden_dim = (\n-            (self.image_processor_tester.patch_size[\"height\"] * self.image_processor_tester.patch_size[\"width\"])\n-            * self.image_processor_tester.num_channels\n-        ) + 2\n-\n-        for max_patch in self.image_processor_tester.max_patches:\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processor\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            # create random PIL images\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, Image.Image)\n+\n             # Test not batched input\n-            encoded_images = image_processor(\n-                image_inputs[0], return_tensors=\"pt\", max_patches=max_patch\n-            ).flattened_patches\n-            self.assertEqual(\n-                encoded_images.shape,\n-                (1, max_patch, expected_hidden_dim),\n-            )\n-\n-            # Test batched\n-            encoded_images = image_processor(\n-                image_inputs, return_tensors=\"pt\", max_patches=max_patch\n-            ).flattened_patches\n-            self.assertEqual(\n-                encoded_images.shape,\n-                (self.image_processor_tester.batch_size, max_patch, expected_hidden_dim),\n-            )\n+            expected_hidden_dim = (\n+                (self.image_processor_tester.patch_size[\"height\"] * self.image_processor_tester.patch_size[\"width\"])\n+                * self.image_processor_tester.num_channels\n+            ) + 2\n+\n+            for max_patch in self.image_processor_tester.max_patches:\n+                # Test not batched input\n+                encoded_images = image_processor(\n+                    image_inputs[0], return_tensors=\"pt\", max_patches=max_patch\n+                ).flattened_patches\n+                self.assertEqual(\n+                    encoded_images.shape,\n+                    (1, max_patch, expected_hidden_dim),\n+                )\n+\n+                # Test batched\n+                encoded_images = image_processor(\n+                    image_inputs, return_tensors=\"pt\", max_patches=max_patch\n+                ).flattened_patches\n+                self.assertEqual(\n+                    encoded_images.shape,\n+                    (self.image_processor_tester.batch_size, max_patch, expected_hidden_dim),\n+                )\n \n     def test_call_vqa(self):\n-        # Initialize image_processor\n-        image_processor = self.image_processing_class(**self.image_processor_dict)\n-        # create random PIL images\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n-        for image in image_inputs:\n-            self.assertIsInstance(image, Image.Image)\n-\n-        # Test not batched input\n-        expected_hidden_dim = (\n-            (self.image_processor_tester.patch_size[\"height\"] * self.image_processor_tester.patch_size[\"width\"])\n-            * self.image_processor_tester.num_channels\n-        ) + 2\n-\n-        image_processor.is_vqa = True\n-\n-        for max_patch in self.image_processor_tester.max_patches:\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processor\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            # create random PIL images\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, Image.Image)\n+\n             # Test not batched input\n-            with self.assertRaises(ValueError):\n+            expected_hidden_dim = (\n+                (self.image_processor_tester.patch_size[\"height\"] * self.image_processor_tester.patch_size[\"width\"])\n+                * self.image_processor_tester.num_channels\n+            ) + 2\n+\n+            image_processor.is_vqa = True\n+\n+            for max_patch in self.image_processor_tester.max_patches:\n+                # Test not batched input\n+                with self.assertRaises(ValueError):\n+                    encoded_images = image_processor(\n+                        image_inputs[0], return_tensors=\"pt\", max_patches=max_patch\n+                    ).flattened_patches\n+\n+                dummy_text = \"Hello\"\n+\n                 encoded_images = image_processor(\n-                    image_inputs[0], return_tensors=\"pt\", max_patches=max_patch\n+                    image_inputs[0], return_tensors=\"pt\", max_patches=max_patch, header_text=dummy_text\n                 ).flattened_patches\n+                self.assertEqual(\n+                    encoded_images.shape,\n+                    (1, max_patch, expected_hidden_dim),\n+                )\n \n-            dummy_text = \"Hello\"\n-\n-            encoded_images = image_processor(\n-                image_inputs[0], return_tensors=\"pt\", max_patches=max_patch, header_text=dummy_text\n-            ).flattened_patches\n-            self.assertEqual(\n-                encoded_images.shape,\n-                (1, max_patch, expected_hidden_dim),\n-            )\n-\n-            # Test batched\n-            encoded_images = image_processor(\n-                image_inputs, return_tensors=\"pt\", max_patches=max_patch, header_text=dummy_text\n-            ).flattened_patches\n-            self.assertEqual(\n-                encoded_images.shape,\n-                (self.image_processor_tester.batch_size, max_patch, expected_hidden_dim),\n-            )\n+                # Test batched\n+                encoded_images = image_processor(\n+                    image_inputs, return_tensors=\"pt\", max_patches=max_patch, header_text=dummy_text\n+                ).flattened_patches\n+                self.assertEqual(\n+                    encoded_images.shape,\n+                    (self.image_processor_tester.batch_size, max_patch, expected_hidden_dim),\n+                )\n \n     def test_call_numpy(self):\n-        # Initialize image_processor\n-        image_processor = self.image_processing_class(**self.image_processor_dict)\n-        # create random numpy tensors\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n-        for image in image_inputs:\n-            self.assertIsInstance(image, np.ndarray)\n-\n-        expected_hidden_dim = (\n-            (self.image_processor_tester.patch_size[\"height\"] * self.image_processor_tester.patch_size[\"width\"])\n-            * self.image_processor_tester.num_channels\n-        ) + 2\n-\n-        for max_patch in self.image_processor_tester.max_patches:\n-            # Test not batched input\n-            encoded_images = image_processor(\n-                image_inputs[0], return_tensors=\"pt\", max_patches=max_patch\n-            ).flattened_patches\n-            self.assertEqual(\n-                encoded_images.shape,\n-                (1, max_patch, expected_hidden_dim),\n-            )\n-\n-            # Test batched\n-            encoded_images = image_processor(\n-                image_inputs, return_tensors=\"pt\", max_patches=max_patch\n-            ).flattened_patches\n-            self.assertEqual(\n-                encoded_images.shape,\n-                (self.image_processor_tester.batch_size, max_patch, expected_hidden_dim),\n-            )\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processor\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            # create random numpy tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, np.ndarray)\n+\n+            expected_hidden_dim = (\n+                (self.image_processor_tester.patch_size[\"height\"] * self.image_processor_tester.patch_size[\"width\"])\n+                * self.image_processor_tester.num_channels\n+            ) + 2\n+\n+            for max_patch in self.image_processor_tester.max_patches:\n+                # Test not batched input\n+                encoded_images = image_processor(\n+                    image_inputs[0], return_tensors=\"pt\", max_patches=max_patch\n+                ).flattened_patches\n+                self.assertEqual(\n+                    encoded_images.shape,\n+                    (1, max_patch, expected_hidden_dim),\n+                )\n+\n+                # Test batched\n+                encoded_images = image_processor(\n+                    image_inputs, return_tensors=\"pt\", max_patches=max_patch\n+                ).flattened_patches\n+                self.assertEqual(\n+                    encoded_images.shape,\n+                    (self.image_processor_tester.batch_size, max_patch, expected_hidden_dim),\n+                )\n \n     def test_call_numpy_4_channels(self):\n-        # Initialize image_processor\n-        image_processor = self.image_processing_class(**self.image_processor_dict)\n-        # create random numpy tensors\n-        self.image_processor_tester.num_channels = 4\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n-        for image in image_inputs:\n-            self.assertIsInstance(image, np.ndarray)\n-\n-        expected_hidden_dim = (\n-            (self.image_processor_tester.patch_size[\"height\"] * self.image_processor_tester.patch_size[\"width\"])\n-            * self.image_processor_tester.num_channels\n-        ) + 2\n-\n-        for max_patch in self.image_processor_tester.max_patches:\n-            # Test not batched input\n-            encoded_images = image_processor(\n-                image_inputs[0], return_tensors=\"pt\", max_patches=max_patch, input_data_format=\"channels_last\"\n-            ).flattened_patches\n-            self.assertEqual(\n-                encoded_images.shape,\n-                (1, max_patch, expected_hidden_dim),\n-            )\n-\n-            # Test batched\n-            encoded_images = image_processor(\n-                image_inputs, return_tensors=\"pt\", max_patches=max_patch, input_data_format=\"channels_last\"\n-            ).flattened_patches\n-            self.assertEqual(\n-                encoded_images.shape,\n-                (self.image_processor_tester.batch_size, max_patch, expected_hidden_dim),\n-            )\n-        self.image_processor_tester.num_channels = 3\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processor\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            # create random numpy tensors\n+            self.image_processor_tester.num_channels = 4\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, np.ndarray)\n+\n+            expected_hidden_dim = (\n+                (self.image_processor_tester.patch_size[\"height\"] * self.image_processor_tester.patch_size[\"width\"])\n+                * self.image_processor_tester.num_channels\n+            ) + 2\n+\n+            for max_patch in self.image_processor_tester.max_patches:\n+                # Test not batched input\n+                encoded_images = image_processor(\n+                    image_inputs[0], return_tensors=\"pt\", max_patches=max_patch, input_data_format=\"channels_last\"\n+                ).flattened_patches\n+                self.assertEqual(\n+                    encoded_images.shape,\n+                    (1, max_patch, expected_hidden_dim),\n+                )\n+\n+                # Test batched\n+                encoded_images = image_processor(\n+                    image_inputs, return_tensors=\"pt\", max_patches=max_patch, input_data_format=\"channels_last\"\n+                ).flattened_patches\n+                self.assertEqual(\n+                    encoded_images.shape,\n+                    (self.image_processor_tester.batch_size, max_patch, expected_hidden_dim),\n+                )\n+            self.image_processor_tester.num_channels = 3\n \n     def test_call_pytorch(self):\n-        # Initialize image_processor\n-        image_processor = self.image_processing_class(**self.image_processor_dict)\n-        # create random PyTorch tensors\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n-        for image in image_inputs:\n-            self.assertIsInstance(image, torch.Tensor)\n-\n-        # Test not batched input\n-        expected_hidden_dim = (\n-            (self.image_processor_tester.patch_size[\"height\"] * self.image_processor_tester.patch_size[\"width\"])\n-            * self.image_processor_tester.num_channels\n-        ) + 2\n-\n-        for max_patch in self.image_processor_tester.max_patches:\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processor\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            # create random PyTorch tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, torch.Tensor)\n+\n             # Test not batched input\n-            encoded_images = image_processor(\n-                image_inputs[0], return_tensors=\"pt\", max_patches=max_patch\n-            ).flattened_patches\n-            self.assertEqual(\n-                encoded_images.shape,\n-                (1, max_patch, expected_hidden_dim),\n-            )\n-\n-            # Test batched\n-            encoded_images = image_processor(\n-                image_inputs, return_tensors=\"pt\", max_patches=max_patch\n-            ).flattened_patches\n-            self.assertEqual(\n-                encoded_images.shape,\n-                (self.image_processor_tester.batch_size, max_patch, expected_hidden_dim),\n-            )\n+            expected_hidden_dim = (\n+                (self.image_processor_tester.patch_size[\"height\"] * self.image_processor_tester.patch_size[\"width\"])\n+                * self.image_processor_tester.num_channels\n+            ) + 2\n+\n+            for max_patch in self.image_processor_tester.max_patches:\n+                # Test not batched input\n+                encoded_images = image_processor(\n+                    image_inputs[0], return_tensors=\"pt\", max_patches=max_patch\n+                ).flattened_patches\n+                self.assertEqual(\n+                    encoded_images.shape,\n+                    (1, max_patch, expected_hidden_dim),\n+                )\n+\n+                # Test batched\n+                encoded_images = image_processor(\n+                    image_inputs, return_tensors=\"pt\", max_patches=max_patch\n+                ).flattened_patches\n+                self.assertEqual(\n+                    encoded_images.shape,\n+                    (self.image_processor_tester.batch_size, max_patch, expected_hidden_dim),\n+                )\n+\n+    @require_vision\n+    @require_torch\n+    def test_slow_fast_equivalence(self):\n+        dummy_image = self.image_processor_tester.prepare_dummy_image()\n+\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_image, return_tensors=\"pt\", max_patches=2048)\n+        encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\", max_patches=2048)\n+        # Pix2Struct uses flattened_patches instead of pixel_values\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.flattened_patches, encoding_fast.flattened_patches)\n+\n+    @require_vision\n+    @require_torch\n+    def test_slow_fast_equivalence_batched(self):\n+        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_images, return_tensors=\"pt\", max_patches=2048)\n+        encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\", max_patches=2048)\n+        # Pix2Struct uses flattened_patches instead of pixel_values\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.flattened_patches, encoding_fast.flattened_patches)\n+\n+    @slow\n+    @require_torch_accelerator\n+    @require_vision\n+    def test_can_compile_fast_image_processor(self):\n+        if self.fast_image_processing_class is None:\n+            self.skipTest(\"Skipping compilation test as fast image processor is not defined\")\n+        if version.parse(torch.__version__) < version.parse(\"2.3\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.3 to run.\")\n+\n+        torch.compiler.reset()\n+        input_image = torch.randint(0, 255, (3, 224, 224), dtype=torch.uint8)\n+        image_processor = self.fast_image_processing_class(**self.image_processor_dict)\n+        output_eager = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n+\n+        image_processor = torch.compile(image_processor, mode=\"reduce-overhead\")\n+        output_compiled = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n+        # Pix2Struct uses flattened_patches instead of pixel_values\n+        self._assert_slow_fast_tensors_equivalence(\n+            output_eager.flattened_patches, output_compiled.flattened_patches, atol=1e-4, rtol=1e-4, mean_atol=1e-5\n+        )\n \n \n @require_torch\n @require_vision\n class Pix2StructImageProcessingTestFourChannels(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = Pix2StructImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = Pix2StructImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -300,42 +372,44 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processor = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processor, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processor, \"do_convert_rgb\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processor, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processor, \"do_convert_rgb\"))\n \n     def test_call_pil(self):\n-        # Initialize image_processor\n-        image_processor = self.image_processing_class(**self.image_processor_dict)\n-        # create random PIL images\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n-        for image in image_inputs:\n-            self.assertIsInstance(image, Image.Image)\n-\n-        # Test not batched input\n-        expected_hidden_dim = (\n-            (self.image_processor_tester.patch_size[\"height\"] * self.image_processor_tester.patch_size[\"width\"])\n-            * (self.image_processor_tester.num_channels - 1)\n-        ) + 2\n-\n-        for max_patch in self.image_processor_tester.max_patches:\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processor\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            # create random PIL images\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, Image.Image)\n+\n             # Test not batched input\n-            encoded_images = image_processor(\n-                image_inputs[0], return_tensors=\"pt\", max_patches=max_patch\n-            ).flattened_patches\n-            self.assertEqual(\n-                encoded_images.shape,\n-                (1, max_patch, expected_hidden_dim),\n-            )\n-\n-            # Test batched\n-            encoded_images = image_processor(\n-                image_inputs, return_tensors=\"pt\", max_patches=max_patch\n-            ).flattened_patches\n-            self.assertEqual(\n-                encoded_images.shape,\n-                (self.image_processor_tester.batch_size, max_patch, expected_hidden_dim),\n-            )\n+            expected_hidden_dim = (\n+                (self.image_processor_tester.patch_size[\"height\"] * self.image_processor_tester.patch_size[\"width\"])\n+                * (self.image_processor_tester.num_channels - 1)\n+            ) + 2\n+\n+            for max_patch in self.image_processor_tester.max_patches:\n+                # Test not batched input\n+                encoded_images = image_processor(\n+                    image_inputs[0], return_tensors=\"pt\", max_patches=max_patch\n+                ).flattened_patches\n+                self.assertEqual(\n+                    encoded_images.shape,\n+                    (1, max_patch, expected_hidden_dim),\n+                )\n+\n+                # Test batched\n+                encoded_images = image_processor(\n+                    image_inputs, return_tensors=\"pt\", max_patches=max_patch\n+                ).flattened_patches\n+                self.assertEqual(\n+                    encoded_images.shape,\n+                    (self.image_processor_tester.batch_size, max_patch, expected_hidden_dim),\n+                )\n \n     @unittest.skip(reason=\"Pix2StructImageProcessor does not support 4 channels yet\")  # FIXME Amy\n     def test_call_numpy(self):\n@@ -350,3 +424,15 @@ def test_call_pytorch(self):\n     )  # FIXME Amy\n     def test_call_numpy_4_channels(self):\n         return super().test_call_torch()\n+\n+    @unittest.skip(reason=\"Pix2StructImageProcessor does not support 4 channels yet\")\n+    def test_slow_fast_equivalence(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Pix2StructImageProcessor does not support 4 channels yet\")\n+    def test_slow_fast_equivalence_batched(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Pix2StructImageProcessor does not support 4 channels yet\")\n+    def test_can_compile_fast_image_processor(self):\n+        pass"
        }
    ],
    "stats": {
        "total": 958,
        "additions": 755,
        "deletions": 203
    }
}