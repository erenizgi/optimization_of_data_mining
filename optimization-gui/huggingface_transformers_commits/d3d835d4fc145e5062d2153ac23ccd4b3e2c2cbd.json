{
    "author": "zucchini-nlp",
    "message": "[qwen] refactor attentions for vision/audio (#38930)\n\n* refactor attentions in vision/audio\n\n* remove fa2 import\n\n* make config the only args\n\n* pass along kwargs from modality encoders\n\n* style",
    "sha": "d3d835d4fc145e5062d2153ac23ccd4b3e2c2cbd",
    "files": [
        {
            "sha": "3ccebbd3423162a231f219af42a210aaa5aeb5cc",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 126,
            "deletions": 250,
            "changes": 376,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3d835d4fc145e5062d2153ac23ccd4b3e2c2cbd/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3d835d4fc145e5062d2153ac23ccd4b3e2c2cbd/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=d3d835d4fc145e5062d2153ac23ccd4b3e2c2cbd",
            "patch": "@@ -34,11 +34,7 @@\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n-from ...modeling_flash_attention_utils import (\n-    FlashAttentionKwargs,\n-    flash_attn_supports_top_left_mask,\n-    is_flash_attn_available,\n-)\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPast, ModelOutput\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n@@ -59,13 +55,6 @@\n )\n \n \n-if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import apply_rotary_emb, flash_attn_varlen_func\n-else:\n-    flash_attn_varlen_func = None\n-    apply_rotary_emb = None\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -559,6 +548,44 @@ class Qwen2_5OmniThinkerCausalLMOutputWithPast(ModelOutput):\n     rope_deltas: Optional[torch.LongTensor] = None\n \n \n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class Qwen2_5OmniAudioAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -571,6 +598,7 @@ def __init__(\n         self.num_heads = config.encoder_attention_heads\n         self.dropout = config.attention_dropout\n         self.head_dim = self.embed_dim // self.num_heads\n+        self.num_key_value_groups = 1  # needed for eager attention\n         self.config = config\n \n         if (self.head_dim * self.num_heads) != self.embed_dim:\n@@ -591,6 +619,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         cu_seqlens: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n@@ -600,129 +629,51 @@ def forward(\n         key_states = self.k_proj(hidden_states).reshape(seq_length, self.num_heads, -1)\n         value_states = self.v_proj(hidden_states).reshape(seq_length, self.num_heads, -1)\n \n-        query_states = query_states.transpose(0, 1)\n-        key_states = key_states.transpose(0, 1)\n-        value_states = value_states.transpose(0, 1)\n-        attn_weights = torch.matmul(query_states, key_states.transpose(1, 2)) / math.sqrt(self.head_dim)\n+        query_states = query_states.transpose(0, 1).unsqueeze(0)\n+        key_states = key_states.transpose(0, 1).unsqueeze(0)\n+        value_states = value_states.transpose(0, 1).unsqueeze(0)\n+        max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n \n         attention_mask = torch.full(\n-            [1, seq_length, key_states.shape[1]],\n+            [1, 1, seq_length, key_states.shape[-2]],\n             torch.finfo(query_states.dtype).min,\n             device=query_states.device,\n             dtype=query_states.dtype,\n         )\n         for i in range(1, len(cu_seqlens)):\n             attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = 0\n \n-        attn_weights = attn_weights + attention_mask\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1).to(query_states.dtype)\n-\n-        attn_output = torch.matmul(attn_weights, value_states).transpose(0, 1).reshape(seq_length, self.embed_dim)\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output\n-\n-\n-class Qwen2_5OmniAudioFlashAttention2(Qwen2_5OmniAudioAttention):\n-    \"\"\"\n-    Qwen2.5OmniThinker flash attention module. This module inherits from `Qwen2_5OmniAudioAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        cu_seqlens: Optional[torch.Tensor] = None,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n-        seq_length, all_dim = hidden_states.size()\n-        query_states = self.q_proj(hidden_states)\n-        query_states = query_states.reshape(seq_length, self.num_heads, -1)\n-\n-        key_states = self.k_proj(hidden_states)\n-        key_states = key_states.reshape(seq_length, self.num_heads, -1)\n-        value_states = self.v_proj(hidden_states)\n-        value_states = value_states.reshape(seq_length, self.num_heads, -1)\n-\n-        max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n-        attn_output = flash_attn_varlen_func(\n-            query_states, key_states, value_states, cu_seqlens, cu_seqlens, max_seqlen, max_seqlen, dropout_p=0.0\n-        )\n-        attn_output = attn_output.reshape(seq_length, all_dim)\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output\n-\n-\n-class Qwen2_5OmniAudioSdpaAttention(Qwen2_5OmniAudioAttention):\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        cu_seqlens: Optional[torch.Tensor] = None,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n-        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-\n-        seq_length, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states).reshape(seq_length, self.num_heads, -1)\n-        key_states = self.k_proj(hidden_states).reshape(seq_length, self.num_heads, -1)\n-        value_states = self.v_proj(hidden_states).reshape(seq_length, self.num_heads, -1)\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attention_mask = torch.zeros(\n-            [1, seq_length, key_states.shape[0]], device=query_states.device, dtype=torch.bool\n-        )\n-        for i in range(1, len(cu_seqlens)):\n-            attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = True\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case tgt_len == 1.\n-        query_states = query_states.transpose(0, 1)\n-        key_states = key_states.transpose(0, 1)\n-        value_states = value_states.transpose(0, 1)\n-\n-        # NOTE: SDPA with memory-efficient backend is currently (torch==2.1.2) bugged when using non-contiguous inputs and a custom attn_mask,\n-        # but we are fine here as `_shape` do call `.contiguous()`. Reference: https://github.com/pytorch/pytorch/issues/112577\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+        attn_output, _ = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n-            attn_mask=attention_mask,\n-            dropout_p=self.dropout if self.training else 0.0,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scaling,\n+            cu_seqlens_q=cu_seqlens,  # pass cu seq lens for FA2\n+            cu_seqlens_k=cu_seqlens,\n+            max_seqlen_q=max_seqlen,\n+            max_seqlen_k=max_seqlen,\n+            is_causal=False,\n+            **kwargs,\n         )\n-        attn_output = attn_output.transpose(0, 1)\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(seq_length, self.embed_dim)\n-        attn_output = self.out_proj(attn_output)\n-        return attn_output\n \n+        attn_output = attn_output.reshape(seq_length, -1).contiguous()\n+        attn_output = self.out_proj(attn_output)\n \n-QWEN2_5_OMNI_AUDIO_ATTENTION_CLASSES = {\n-    \"eager\": Qwen2_5OmniAudioAttention,\n-    \"flash_attention_2\": Qwen2_5OmniAudioFlashAttention2,\n-    \"sdpa\": Qwen2_5OmniAudioSdpaAttention,\n-}\n+        return attn_output\n \n \n class Qwen2_5OmniAudioEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Qwen2_5OmniAudioEncoderConfig):\n         super().__init__()\n         self.embed_dim = config.d_model\n-        self.self_attn = QWEN2_5_OMNI_AUDIO_ATTENTION_CLASSES[config._attn_implementation](config)\n+        self.self_attn = Qwen2_5OmniAudioAttention(config)\n         self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n         self.dropout = config.dropout\n         self.activation_fn = ACT2FN[config.activation_function]\n@@ -735,6 +686,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         cu_seqlens: torch.Tensor,\n+        **kwargs,\n     ) -> torch.Tensor:\n         \"\"\"\n         Args:\n@@ -752,6 +704,7 @@ def forward(\n         hidden_states = self.self_attn(\n             hidden_states=hidden_states,\n             cu_seqlens=cu_seqlens,\n+            **kwargs,\n         )\n         hidden_states = residual + hidden_states\n         residual = hidden_states\n@@ -838,6 +791,7 @@ def forward(\n         input_features,\n         feature_lens=None,\n         aftercnn_lens=None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         input_features (`torch.LongTensor` of shape `(batch_size, feature_size, sequence_length)`):\n@@ -881,7 +835,7 @@ def forward(\n         ).to(torch.int32)\n \n         for encoder_layer in self.layers:\n-            layer_outputs = encoder_layer(hidden_states, cu_seqlens)\n+            layer_outputs = encoder_layer(hidden_states, cu_seqlens, **kwargs)\n             hidden_states = layer_outputs[0]\n \n         hidden_states_list = hidden_states.split(aftercnn_lens.tolist(), dim=0)\n@@ -962,106 +916,68 @@ def apply_rotary_pos_emb_vision(tensor: torch.Tensor, freqs: torch.Tensor) -> to\n \n \n class Qwen2_5OmniVisionAttention(nn.Module):\n-    def __init__(self, dim: int, num_heads: int = 16) -> None:\n+    def __init__(self, config: Qwen2_5OmniVisionEncoderConfig = None) -> None:\n         super().__init__()\n-        self.num_heads = num_heads\n-        self.head_dim = dim // num_heads\n-        self.q = nn.Linear(dim, dim, bias=True)\n-        self.k = nn.Linear(dim, dim, bias=True)\n-        self.v = nn.Linear(dim, dim, bias=True)\n-        self.proj = nn.Linear(dim, dim)\n+        self.dim = config.hidden_size\n+        self.num_heads = config.num_heads\n+        self.head_dim = self.dim // self.num_heads\n+        self.q = nn.Linear(self.dim, self.dim, bias=True)\n+        self.k = nn.Linear(self.dim, self.dim, bias=True)\n+        self.v = nn.Linear(self.dim, self.dim, bias=True)\n+        self.proj = nn.Linear(self.dim, self.dim)\n+        self.scaling = math.sqrt(self.head_dim)\n+        self.num_key_value_groups = 1  # needed for eager attention\n+        self.config = config\n \n     def forward(\n-        self, hidden_states: torch.Tensor, cu_seqlens: torch.Tensor, rotary_pos_emb: torch.Tensor = None\n+        self,\n+        hidden_states: torch.Tensor,\n+        cu_seqlens: torch.Tensor,\n+        rotary_pos_emb: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> torch.Tensor:\n         seq_length = hidden_states.shape[0]\n-        q = self.q(hidden_states).reshape(seq_length, self.num_heads, -1)\n-        k = self.k(hidden_states).reshape(seq_length, self.num_heads, -1)\n-        v = self.v(hidden_states).reshape(seq_length, self.num_heads, -1)\n-        q = apply_rotary_pos_emb_vision(q.unsqueeze(0), rotary_pos_emb).squeeze(0)\n-        k = apply_rotary_pos_emb_vision(k.unsqueeze(0), rotary_pos_emb).squeeze(0)\n+        query_states = self.q(hidden_states).reshape(seq_length, self.num_heads, -1)\n+        key_states = self.k(hidden_states).reshape(seq_length, self.num_heads, -1)\n+        value_states = self.v(hidden_states).reshape(seq_length, self.num_heads, -1)\n+        query_states = apply_rotary_pos_emb_vision(query_states.unsqueeze(0), rotary_pos_emb).squeeze(0)\n+        key_states = apply_rotary_pos_emb_vision(key_states.unsqueeze(0), rotary_pos_emb).squeeze(0)\n \n         attention_mask = torch.full(\n-            [1, seq_length, seq_length], torch.finfo(q.dtype).min, device=q.device, dtype=q.dtype\n+            [1, 1, seq_length, seq_length],\n+            torch.finfo(query_states.dtype).min,\n+            device=query_states.device,\n+            dtype=query_states.dtype,\n         )\n         for i in range(1, len(cu_seqlens)):\n             attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = 0\n \n-        q = q.transpose(0, 1)\n-        k = k.transpose(0, 1)\n-        v = v.transpose(0, 1)\n-        attn_weights = torch.matmul(q, k.transpose(1, 2)) / math.sqrt(self.head_dim)\n-        attn_weights = attn_weights + attention_mask\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(q.dtype)\n-        attn_output = torch.matmul(attn_weights, v)\n-        attn_output = attn_output.transpose(0, 1)\n-        attn_output = attn_output.reshape(seq_length, -1)\n-        attn_output = self.proj(attn_output)\n-        return attn_output\n-\n-\n-class Qwen2_5OmniVisionFlashAttention2(nn.Module):\n-    def __init__(self, dim: int, num_heads: int = 16) -> None:\n-        super().__init__()\n-        self.num_heads = num_heads\n-        self.q = nn.Linear(dim, dim, bias=True)\n-        self.k = nn.Linear(dim, dim, bias=True)\n-        self.v = nn.Linear(dim, dim, bias=True)\n-        self.proj = nn.Linear(dim, dim)\n-\n-    def _apply_rotary_pos_emb_flashatt(self, tensor: torch.Tensor, freqs: torch.Tensor) -> torch.Tensor:\n-        tensor_ = tensor.float()\n-        cos = freqs.cos().type_as(tensor_)\n-        sin = freqs.sin().type_as(tensor_)\n-        output = apply_rotary_emb(tensor_, cos, sin).type_as(tensor)\n-        return output\n-\n-    def forward(\n-        self, hidden_states: torch.Tensor, cu_seqlens: torch.Tensor, rotary_pos_emb: torch.Tensor = None\n-    ) -> torch.Tensor:\n-        seq_length = hidden_states.shape[0]\n-        q = self.q(hidden_states).reshape(seq_length, self.num_heads, -1)\n-        k = self.k(hidden_states).reshape(seq_length, self.num_heads, -1)\n-        v = self.v(hidden_states).reshape(seq_length, self.num_heads, -1)\n-        q = self._apply_rotary_pos_emb_flashatt(q.unsqueeze(0), rotary_pos_emb).squeeze(0)\n-        k = self._apply_rotary_pos_emb_flashatt(k.unsqueeze(0), rotary_pos_emb).squeeze(0)\n-\n+        query_states = query_states.transpose(0, 1).unsqueeze(0)  # unsqueeze batch_dim\n+        key_states = key_states.transpose(0, 1).unsqueeze(0)  # unsqueeze batch_dim\n+        value_states = value_states.transpose(0, 1).unsqueeze(0)  # unsqueeze batch_dim\n         max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n-        attn_output = flash_attn_varlen_func(q, k, v, cu_seqlens, cu_seqlens, max_seqlen, max_seqlen).reshape(\n-            seq_length, -1\n-        )\n-        attn_output = self.proj(attn_output)\n-        return attn_output\n \n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-class Qwen2_5OmniVisionSdpaAttention(nn.Module):\n-    def __init__(self, dim: int, num_heads: int = 16) -> None:\n-        super().__init__()\n-        self.num_heads = num_heads\n-        self.q = nn.Linear(dim, dim, bias=True)\n-        self.k = nn.Linear(dim, dim, bias=True)\n-        self.v = nn.Linear(dim, dim, bias=True)\n-        self.proj = nn.Linear(dim, dim)\n-\n-    def forward(\n-        self, hidden_states: torch.Tensor, cu_seqlens: torch.Tensor, rotary_pos_emb: torch.Tensor = None\n-    ) -> torch.Tensor:\n-        seq_length = hidden_states.shape[0]\n-        q = self.q(hidden_states).reshape(seq_length, self.num_heads, -1)\n-        k = self.k(hidden_states).reshape(seq_length, self.num_heads, -1)\n-        v = self.v(hidden_states).reshape(seq_length, self.num_heads, -1)\n-        q = apply_rotary_pos_emb_vision(q.unsqueeze(0), rotary_pos_emb).squeeze(0)\n-        k = apply_rotary_pos_emb_vision(k.unsqueeze(0), rotary_pos_emb).squeeze(0)\n+        attn_output, _ = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0,\n+            scaling=self.scaling,\n+            cu_seqlens_q=cu_seqlens,  # pass cu seq lens for FA2\n+            cu_seqlens_k=cu_seqlens,\n+            max_seqlen_q=max_seqlen,\n+            max_seqlen_k=max_seqlen,\n+            is_causal=False,\n+            **kwargs,\n+        )\n \n-        attention_mask = torch.zeros([1, seq_length, seq_length], device=q.device, dtype=torch.bool)\n-        for i in range(1, len(cu_seqlens)):\n-            attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = True\n-        q = q.transpose(0, 1)\n-        k = k.transpose(0, 1)\n-        v = v.transpose(0, 1)\n-        attn_output = F.scaled_dot_product_attention(q, k, v, attention_mask, dropout_p=0.0)\n-        attn_output = attn_output.transpose(0, 1)\n-        attn_output = attn_output.reshape(seq_length, -1)\n+        attn_output = attn_output.reshape(seq_length, -1).contiguous()\n         attn_output = self.proj(attn_output)\n         return attn_output\n \n@@ -1080,26 +996,23 @@ def forward(self, hidden_state):\n         return self.down_proj(self.act_fn(self.gate_proj(hidden_state)) * self.up_proj(hidden_state))\n \n \n-QWEN2_5_OMNI_VISION_ATTENTION_CLASSES = {\n-    \"eager\": Qwen2_5OmniVisionAttention,\n-    \"flash_attention_2\": Qwen2_5OmniVisionFlashAttention2,\n-    \"sdpa\": Qwen2_5OmniVisionSdpaAttention,\n-}\n-\n-\n class Qwen2_5OmniVisionBlock(GradientCheckpointingLayer):\n     def __init__(self, config: Qwen2_5OmniVisionEncoderConfig) -> None:\n         super().__init__()\n         self.norm1 = Qwen2RMSNorm(config.hidden_size, eps=1e-6)\n         self.norm2 = Qwen2RMSNorm(config.hidden_size, eps=1e-6)\n-        self.attn = QWEN2_5_OMNI_VISION_ATTENTION_CLASSES[config._attn_implementation](\n-            config.hidden_size, num_heads=config.num_heads\n-        )\n+        self.attn = Qwen2_5OmniVisionAttention(config=config)\n         self.mlp = Qwen2_5OmniMLP(config, bias=True)\n \n-    def forward(self, hidden_states, cu_seqlens, rotary_pos_emb) -> torch.Tensor:\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        cu_seqlens: torch.Tensor,\n+        rotary_pos_emb: Optional[torch.Tensor] = None,\n+        **kwargs,\n+    ) -> torch.Tensor:\n         hidden_states = hidden_states + self.attn(\n-            self.norm1(hidden_states), cu_seqlens=cu_seqlens, rotary_pos_emb=rotary_pos_emb\n+            self.norm1(hidden_states), cu_seqlens=cu_seqlens, rotary_pos_emb=rotary_pos_emb, **kwargs\n         )\n         hidden_states = hidden_states + self.mlp(self.norm2(hidden_states))\n         return hidden_states\n@@ -1258,7 +1171,7 @@ def get_window_index(self, grid_thw):\n \n         return window_index, cu_window_seqlens\n \n-    def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.Tensor:\n+    def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs) -> torch.Tensor:\n         \"\"\"\n         Args:\n             hidden_states (`torch.Tensor` of shape `(seq_len, hidden_size)`):\n@@ -1308,6 +1221,7 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.\n                 hidden_states,\n                 cu_seqlens=cu_seqlens_now,\n                 rotary_pos_emb=rotary_pos_emb,\n+                **kwargs,\n             )\n         hidden_states = self.merger(hidden_states)\n         reverse_indices = torch.argsort(window_index)\n@@ -1397,44 +1311,6 @@ def apply_multimodal_rotary_pos_emb(q, k, cos, sin, mrope_section, unsqueeze_dim\n     return q_embed, k_embed\n \n \n-def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n-    \"\"\"\n-    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n-    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n-    \"\"\"\n-    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n-    if n_rep == 1:\n-        return hidden_states\n-    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n-    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n-\n-\n-def eager_attention_forward(\n-    module: nn.Module,\n-    query: torch.Tensor,\n-    key: torch.Tensor,\n-    value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n-    dropout: float = 0.0,\n-    **kwargs,\n-):\n-    key_states = repeat_kv(key, module.num_key_value_groups)\n-    value_states = repeat_kv(value, module.num_key_value_groups)\n-\n-    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n-    if attention_mask is not None:\n-        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-        attn_weights = attn_weights + causal_mask\n-\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n-    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n-    attn_output = torch.matmul(attn_weights, value_states)\n-    attn_output = attn_output.transpose(1, 2).contiguous()\n-\n-    return attn_output, attn_weights\n-\n-\n class Qwen2_5OmniAttention(nn.Module):\n     \"\"\"\n     Multi-headed attention from 'Attention Is All You Need' paper. Modified to use sliding window attention: Longformer"
        },
        {
            "sha": "ac134bd483726aebec25ebc0a8ca6c011af9e42c",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 90,
            "deletions": 202,
            "changes": 292,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3d835d4fc145e5062d2153ac23ccd4b3e2c2cbd/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3d835d4fc145e5062d2153ac23ccd4b3e2c2cbd/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=d3d835d4fc145e5062d2153ac23ccd4b3e2c2cbd",
            "patch": "@@ -17,7 +17,7 @@\n \n import math\n from dataclasses import dataclass\n-from typing import Any, Optional, Union\n+from typing import Any, Callable, Optional, Union\n \n import numpy as np\n import torch\n@@ -36,14 +36,15 @@\n     Qwen2_5_VLTextModel,\n     Qwen2_5_VLVisionBlock,\n     Qwen2RMSNorm,\n+    eager_attention_forward,\n )\n from transformers.models.qwen2_audio.configuration_qwen2_audio import Qwen2AudioEncoderConfig\n from transformers.models.qwen2_audio.modeling_qwen2_audio import Qwen2AudioEncoderLayer\n from transformers.models.qwen2_vl.modeling_qwen2_vl import Qwen2VLRotaryEmbedding\n \n from ...configuration_utils import PretrainedConfig, layer_type_validation\n from ...generation import GenerationMixin\n-from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n+from ...modeling_flash_attention_utils import is_flash_attn_available\n from ...modeling_outputs import BaseModelOutput, ModelOutput\n from ...modeling_rope_utils import rope_config_validation\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n@@ -1601,6 +1602,7 @@ def __init__(\n         self.num_heads = config.encoder_attention_heads\n         self.dropout = config.attention_dropout\n         self.head_dim = self.embed_dim // self.num_heads\n+        self.num_key_value_groups = 1  # needed for eager attention\n         self.config = config\n \n         if (self.head_dim * self.num_heads) != self.embed_dim:\n@@ -1621,6 +1623,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         cu_seqlens: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n@@ -1630,139 +1633,63 @@ def forward(\n         key_states = self.k_proj(hidden_states).reshape(seq_length, self.num_heads, -1)\n         value_states = self.v_proj(hidden_states).reshape(seq_length, self.num_heads, -1)\n \n-        query_states = query_states.transpose(0, 1)\n-        key_states = key_states.transpose(0, 1)\n-        value_states = value_states.transpose(0, 1)\n-        attn_weights = torch.matmul(query_states, key_states.transpose(1, 2)) / math.sqrt(self.head_dim)\n+        query_states = query_states.transpose(0, 1).unsqueeze(0)\n+        key_states = key_states.transpose(0, 1).unsqueeze(0)\n+        value_states = value_states.transpose(0, 1).unsqueeze(0)\n+        max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n \n         attention_mask = torch.full(\n-            [1, seq_length, key_states.shape[1]],\n+            [1, 1, seq_length, key_states.shape[-2]],\n             torch.finfo(query_states.dtype).min,\n             device=query_states.device,\n             dtype=query_states.dtype,\n         )\n         for i in range(1, len(cu_seqlens)):\n             attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = 0\n \n-        attn_weights = attn_weights + attention_mask\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1).to(query_states.dtype)\n-\n-        attn_output = torch.matmul(attn_weights, value_states).transpose(0, 1).reshape(seq_length, self.embed_dim)\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output\n-\n-\n-class Qwen2_5OmniAudioFlashAttention2(Qwen2_5OmniAudioAttention):\n-    \"\"\"\n-    Qwen2.5OmniThinker flash attention module. This module inherits from `Qwen2_5OmniAudioAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        cu_seqlens: Optional[torch.Tensor] = None,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n-        seq_length, all_dim = hidden_states.size()\n-        query_states = self.q_proj(hidden_states)\n-        query_states = query_states.reshape(seq_length, self.num_heads, -1)\n-\n-        key_states = self.k_proj(hidden_states)\n-        key_states = key_states.reshape(seq_length, self.num_heads, -1)\n-        value_states = self.v_proj(hidden_states)\n-        value_states = value_states.reshape(seq_length, self.num_heads, -1)\n-\n-        max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n-        attn_output = flash_attn_varlen_func(\n-            query_states, key_states, value_states, cu_seqlens, cu_seqlens, max_seqlen, max_seqlen, dropout_p=0.0\n-        )\n-        attn_output = attn_output.reshape(seq_length, all_dim)\n-        attn_output = self.out_proj(attn_output)\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        return attn_output\n-\n-\n-class Qwen2_5OmniAudioSdpaAttention(Qwen2_5OmniAudioAttention):\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        cu_seqlens: Optional[torch.Tensor] = None,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n-        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-\n-        seq_length, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states).reshape(seq_length, self.num_heads, -1)\n-        key_states = self.k_proj(hidden_states).reshape(seq_length, self.num_heads, -1)\n-        value_states = self.v_proj(hidden_states).reshape(seq_length, self.num_heads, -1)\n-\n-        attention_mask = torch.zeros(\n-            [1, seq_length, key_states.shape[0]], device=query_states.device, dtype=torch.bool\n-        )\n-        for i in range(1, len(cu_seqlens)):\n-            attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = True\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case tgt_len == 1.\n-        query_states = query_states.transpose(0, 1)\n-        key_states = key_states.transpose(0, 1)\n-        value_states = value_states.transpose(0, 1)\n-\n-        # NOTE: SDPA with memory-efficient backend is currently (torch==2.1.2) bugged when using non-contiguous inputs and a custom attn_mask,\n-        # but we are fine here as `_shape` do call `.contiguous()`. Reference: https://github.com/pytorch/pytorch/issues/112577\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+        attn_output, _ = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n-            attn_mask=attention_mask,\n-            dropout_p=self.dropout if self.training else 0.0,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scaling,\n+            cu_seqlens_q=cu_seqlens,  # pass cu seq lens for FA2\n+            cu_seqlens_k=cu_seqlens,\n+            max_seqlen_q=max_seqlen,\n+            max_seqlen_k=max_seqlen,\n+            is_causal=False,\n+            **kwargs,\n         )\n-        attn_output = attn_output.transpose(0, 1)\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(seq_length, self.embed_dim)\n-        attn_output = self.out_proj(attn_output)\n-        return attn_output\n \n+        attn_output = attn_output.reshape(seq_length, -1).contiguous()\n+        attn_output = self.out_proj(attn_output)\n \n-QWEN2_5_OMNI_AUDIO_ATTENTION_CLASSES = {\n-    \"eager\": Qwen2_5OmniAudioAttention,\n-    \"flash_attention_2\": Qwen2_5OmniAudioFlashAttention2,\n-    \"sdpa\": Qwen2_5OmniAudioSdpaAttention,\n-}\n+        return attn_output\n \n \n class Qwen2_5OmniAudioEncoderLayer(Qwen2AudioEncoderLayer):\n     def __init__(self, config: Qwen2_5OmniAudioEncoderConfig):\n         super().__init__(config)\n-        self.self_attn = QWEN2_5_OMNI_AUDIO_ATTENTION_CLASSES[config._attn_implementation](config)\n+        self.self_attn = Qwen2_5OmniAudioAttention(config)\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         cu_seqlens: torch.Tensor,\n+        **kwargs,\n     ) -> torch.Tensor:\n         residual = hidden_states\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n         hidden_states = self.self_attn(\n             hidden_states=hidden_states,\n             cu_seqlens=cu_seqlens,\n+            **kwargs,\n         )\n         hidden_states = residual + hidden_states\n         residual = hidden_states\n@@ -1849,6 +1776,7 @@ def forward(\n         input_features,\n         feature_lens=None,\n         aftercnn_lens=None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         input_features (`torch.LongTensor` of shape `(batch_size, feature_size, sequence_length)`):\n@@ -1892,7 +1820,7 @@ def forward(\n         ).to(torch.int32)\n \n         for encoder_layer in self.layers:\n-            layer_outputs = encoder_layer(hidden_states, cu_seqlens)\n+            layer_outputs = encoder_layer(hidden_states, cu_seqlens, **kwargs)\n             hidden_states = layer_outputs[0]\n \n         hidden_states_list = hidden_states.split(aftercnn_lens.tolist(), dim=0)\n@@ -1966,127 +1894,86 @@ def apply_rotary_pos_emb_vision(tensor: torch.Tensor, freqs: torch.Tensor) -> to\n \n \n class Qwen2_5OmniVisionAttention(nn.Module):\n-    def __init__(self, dim: int, num_heads: int = 16) -> None:\n+    def __init__(self, config: Qwen2_5OmniVisionEncoderConfig = None) -> None:\n         super().__init__()\n-        self.num_heads = num_heads\n-        self.head_dim = dim // num_heads\n-        self.q = nn.Linear(dim, dim, bias=True)\n-        self.k = nn.Linear(dim, dim, bias=True)\n-        self.v = nn.Linear(dim, dim, bias=True)\n-        self.proj = nn.Linear(dim, dim)\n+        self.dim = config.hidden_size\n+        self.num_heads = config.num_heads\n+        self.head_dim = self.dim // self.num_heads\n+        self.q = nn.Linear(self.dim, self.dim, bias=True)\n+        self.k = nn.Linear(self.dim, self.dim, bias=True)\n+        self.v = nn.Linear(self.dim, self.dim, bias=True)\n+        self.proj = nn.Linear(self.dim, self.dim)\n+        self.scaling = math.sqrt(self.head_dim)\n+        self.num_key_value_groups = 1  # needed for eager attention\n+        self.config = config\n \n     def forward(\n-        self, hidden_states: torch.Tensor, cu_seqlens: torch.Tensor, rotary_pos_emb: torch.Tensor = None\n+        self,\n+        hidden_states: torch.Tensor,\n+        cu_seqlens: torch.Tensor,\n+        rotary_pos_emb: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> torch.Tensor:\n         seq_length = hidden_states.shape[0]\n-        q = self.q(hidden_states).reshape(seq_length, self.num_heads, -1)\n-        k = self.k(hidden_states).reshape(seq_length, self.num_heads, -1)\n-        v = self.v(hidden_states).reshape(seq_length, self.num_heads, -1)\n-        q = apply_rotary_pos_emb_vision(q.unsqueeze(0), rotary_pos_emb).squeeze(0)\n-        k = apply_rotary_pos_emb_vision(k.unsqueeze(0), rotary_pos_emb).squeeze(0)\n+        query_states = self.q(hidden_states).reshape(seq_length, self.num_heads, -1)\n+        key_states = self.k(hidden_states).reshape(seq_length, self.num_heads, -1)\n+        value_states = self.v(hidden_states).reshape(seq_length, self.num_heads, -1)\n+        query_states = apply_rotary_pos_emb_vision(query_states.unsqueeze(0), rotary_pos_emb).squeeze(0)\n+        key_states = apply_rotary_pos_emb_vision(key_states.unsqueeze(0), rotary_pos_emb).squeeze(0)\n \n         attention_mask = torch.full(\n-            [1, seq_length, seq_length], torch.finfo(q.dtype).min, device=q.device, dtype=q.dtype\n+            [1, 1, seq_length, seq_length],\n+            torch.finfo(query_states.dtype).min,\n+            device=query_states.device,\n+            dtype=query_states.dtype,\n         )\n         for i in range(1, len(cu_seqlens)):\n             attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = 0\n \n-        q = q.transpose(0, 1)\n-        k = k.transpose(0, 1)\n-        v = v.transpose(0, 1)\n-        attn_weights = torch.matmul(q, k.transpose(1, 2)) / math.sqrt(self.head_dim)\n-        attn_weights = attn_weights + attention_mask\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(q.dtype)\n-        attn_output = torch.matmul(attn_weights, v)\n-        attn_output = attn_output.transpose(0, 1)\n-        attn_output = attn_output.reshape(seq_length, -1)\n-        attn_output = self.proj(attn_output)\n-        return attn_output\n-\n-\n-class Qwen2_5OmniVisionFlashAttention2(nn.Module):\n-    def __init__(self, dim: int, num_heads: int = 16) -> None:\n-        super().__init__()\n-        self.num_heads = num_heads\n-        self.q = nn.Linear(dim, dim, bias=True)\n-        self.k = nn.Linear(dim, dim, bias=True)\n-        self.v = nn.Linear(dim, dim, bias=True)\n-        self.proj = nn.Linear(dim, dim)\n-\n-    def _apply_rotary_pos_emb_flashatt(self, tensor: torch.Tensor, freqs: torch.Tensor) -> torch.Tensor:\n-        tensor_ = tensor.float()\n-        cos = freqs.cos().type_as(tensor_)\n-        sin = freqs.sin().type_as(tensor_)\n-        output = apply_rotary_emb(tensor_, cos, sin).type_as(tensor)\n-        return output\n-\n-    def forward(\n-        self, hidden_states: torch.Tensor, cu_seqlens: torch.Tensor, rotary_pos_emb: torch.Tensor = None\n-    ) -> torch.Tensor:\n-        seq_length = hidden_states.shape[0]\n-        q = self.q(hidden_states).reshape(seq_length, self.num_heads, -1)\n-        k = self.k(hidden_states).reshape(seq_length, self.num_heads, -1)\n-        v = self.v(hidden_states).reshape(seq_length, self.num_heads, -1)\n-        q = self._apply_rotary_pos_emb_flashatt(q.unsqueeze(0), rotary_pos_emb).squeeze(0)\n-        k = self._apply_rotary_pos_emb_flashatt(k.unsqueeze(0), rotary_pos_emb).squeeze(0)\n-\n+        query_states = query_states.transpose(0, 1).unsqueeze(0)  # unsqueeze batch_dim\n+        key_states = key_states.transpose(0, 1).unsqueeze(0)  # unsqueeze batch_dim\n+        value_states = value_states.transpose(0, 1).unsqueeze(0)  # unsqueeze batch_dim\n         max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n-        attn_output = flash_attn_varlen_func(q, k, v, cu_seqlens, cu_seqlens, max_seqlen, max_seqlen).reshape(\n-            seq_length, -1\n-        )\n-        attn_output = self.proj(attn_output)\n-        return attn_output\n \n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-class Qwen2_5OmniVisionSdpaAttention(nn.Module):\n-    def __init__(self, dim: int, num_heads: int = 16) -> None:\n-        super().__init__()\n-        self.num_heads = num_heads\n-        self.q = nn.Linear(dim, dim, bias=True)\n-        self.k = nn.Linear(dim, dim, bias=True)\n-        self.v = nn.Linear(dim, dim, bias=True)\n-        self.proj = nn.Linear(dim, dim)\n-\n-    def forward(\n-        self, hidden_states: torch.Tensor, cu_seqlens: torch.Tensor, rotary_pos_emb: torch.Tensor = None\n-    ) -> torch.Tensor:\n-        seq_length = hidden_states.shape[0]\n-        q = self.q(hidden_states).reshape(seq_length, self.num_heads, -1)\n-        k = self.k(hidden_states).reshape(seq_length, self.num_heads, -1)\n-        v = self.v(hidden_states).reshape(seq_length, self.num_heads, -1)\n-        q = apply_rotary_pos_emb_vision(q.unsqueeze(0), rotary_pos_emb).squeeze(0)\n-        k = apply_rotary_pos_emb_vision(k.unsqueeze(0), rotary_pos_emb).squeeze(0)\n+        attn_output, _ = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0,\n+            scaling=self.scaling,\n+            cu_seqlens_q=cu_seqlens,  # pass cu seq lens for FA2\n+            cu_seqlens_k=cu_seqlens,\n+            max_seqlen_q=max_seqlen,\n+            max_seqlen_k=max_seqlen,\n+            is_causal=False,\n+            **kwargs,\n+        )\n \n-        attention_mask = torch.zeros([1, seq_length, seq_length], device=q.device, dtype=torch.bool)\n-        for i in range(1, len(cu_seqlens)):\n-            attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = True\n-        q = q.transpose(0, 1)\n-        k = k.transpose(0, 1)\n-        v = v.transpose(0, 1)\n-        attn_output = F.scaled_dot_product_attention(q, k, v, attention_mask, dropout_p=0.0)\n-        attn_output = attn_output.transpose(0, 1)\n-        attn_output = attn_output.reshape(seq_length, -1)\n+        attn_output = attn_output.reshape(seq_length, -1).contiguous()\n         attn_output = self.proj(attn_output)\n         return attn_output\n \n \n-QWEN2_5_OMNI_VISION_ATTENTION_CLASSES = {\n-    \"eager\": Qwen2_5OmniVisionAttention,\n-    \"flash_attention_2\": Qwen2_5OmniVisionFlashAttention2,\n-    \"sdpa\": Qwen2_5OmniVisionSdpaAttention,\n-}\n-\n-\n class Qwen2_5OmniVisionBlock(Qwen2_5_VLVisionBlock):\n     def __init__(self, config: Qwen2_5OmniVisionEncoderConfig) -> None:\n         super().__init__(config, config._attn_implementation)\n-        self.attn = QWEN2_5_OMNI_VISION_ATTENTION_CLASSES[config._attn_implementation](\n-            config.hidden_size, num_heads=config.num_heads\n-        )\n+        self.attn = Qwen2_5OmniVisionAttention(config=config)\n \n-    def forward(self, hidden_states, cu_seqlens, rotary_pos_emb) -> torch.Tensor:\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        cu_seqlens: torch.Tensor,\n+        rotary_pos_emb: Optional[torch.Tensor] = None,\n+        **kwargs,\n+    ) -> torch.Tensor:\n         hidden_states = hidden_states + self.attn(\n-            self.norm1(hidden_states), cu_seqlens=cu_seqlens, rotary_pos_emb=rotary_pos_emb\n+            self.norm1(hidden_states), cu_seqlens=cu_seqlens, rotary_pos_emb=rotary_pos_emb, **kwargs\n         )\n         hidden_states = hidden_states + self.mlp(self.norm2(hidden_states))\n         return hidden_states\n@@ -2100,7 +1987,7 @@ def __init__(self, config: Qwen2_5OmniVisionEncoderConfig, *inputs, **kwargs) ->\n         super().__init__(config, *inputs, **kwargs)\n         self.blocks = nn.ModuleList([Qwen2_5OmniVisionBlock(config) for _ in range(config.depth)])\n \n-    def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.Tensor:\n+    def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs) -> torch.Tensor:\n         \"\"\"\n         Args:\n             hidden_states (`torch.Tensor` of shape `(seq_len, hidden_size)`):\n@@ -2150,6 +2037,7 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.\n                 hidden_states,\n                 cu_seqlens=cu_seqlens_now,\n                 rotary_pos_emb=rotary_pos_emb,\n+                **kwargs,\n             )\n         hidden_states = self.merger(hidden_states)\n         reverse_indices = torch.argsort(window_index)"
        },
        {
            "sha": "0122aa37e02523d10b46f3bafbd2ba5d7fbd68c2",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 88,
            "deletions": 167,
            "changes": 255,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3d835d4fc145e5062d2153ac23ccd4b3e2c2cbd/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3d835d4fc145e5062d2153ac23ccd4b3e2c2cbd/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=d3d835d4fc145e5062d2153ac23ccd4b3e2c2cbd",
            "patch": "@@ -36,7 +36,7 @@\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs, is_flash_attn_available\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n@@ -46,10 +46,6 @@\n from .configuration_qwen2_5_vl import Qwen2_5_VLConfig, Qwen2_5_VLTextConfig, Qwen2_5_VLVisionConfig\n \n \n-if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import apply_rotary_emb, flash_attn_varlen_func\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -141,56 +137,6 @@ def forward(self, x: torch.Tensor) -> torch.Tensor:\n         return x\n \n \n-def apply_rotary_pos_emb_flashatt(\n-    q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor\n-) -> tuple[torch.Tensor, torch.Tensor]:\n-    cos = cos.chunk(2, dim=-1)[0].contiguous()\n-    sin = sin.chunk(2, dim=-1)[0].contiguous()\n-    q_embed = apply_rotary_emb(q.float(), cos.float(), sin.float()).type_as(q)\n-    k_embed = apply_rotary_emb(k.float(), cos.float(), sin.float()).type_as(k)\n-    return q_embed, k_embed\n-\n-\n-class Qwen2_5_VLVisionFlashAttention2(nn.Module):\n-    def __init__(self, dim: int, num_heads: int = 16) -> None:\n-        super().__init__()\n-        self.num_heads = num_heads\n-        self.qkv = nn.Linear(dim, dim * 3, bias=True)\n-        self.proj = nn.Linear(dim, dim)\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        cu_seqlens: torch.Tensor,\n-        rotary_pos_emb: Optional[torch.Tensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-    ) -> torch.Tensor:\n-        seq_length = hidden_states.shape[0]\n-        q, k, v = self.qkv(hidden_states).reshape(seq_length, 3, self.num_heads, -1).permute(1, 0, 2, 3).unbind(0)\n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `rotary_pos_emb` (2D tensor of RoPE theta values), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.54 `rotary_pos_emb` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n-        else:\n-            cos, sin = position_embeddings\n-        q, k = apply_rotary_pos_emb_flashatt(q.unsqueeze(0), k.unsqueeze(0), cos, sin)\n-        q = q.squeeze(0)\n-        k = k.squeeze(0)\n-\n-        max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n-        attn_output = flash_attn_varlen_func(q, k, v, cu_seqlens, cu_seqlens, max_seqlen, max_seqlen).reshape(\n-            seq_length, -1\n-        )\n-        attn_output = self.proj(attn_output)\n-        return attn_output\n-\n-\n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n     x1 = x[..., : x.shape[-1] // 2]\n@@ -212,23 +158,68 @@ def apply_rotary_pos_emb_vision(\n     return q_embed, k_embed\n \n \n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class Qwen2_5_VLVisionAttention(nn.Module):\n-    def __init__(self, dim: int, num_heads: int = 16) -> None:\n+    def __init__(self, config: Qwen2_5_VLVisionConfig) -> None:\n         super().__init__()\n-        self.num_heads = num_heads\n-        self.head_dim = dim // num_heads\n-        self.qkv = nn.Linear(dim, dim * 3, bias=True)\n-        self.proj = nn.Linear(dim, dim)\n+        self.dim = config.hidden_size\n+        self.num_heads = config.num_heads\n+        self.head_dim = self.dim // self.num_heads\n+        self.num_key_value_groups = 1  # needed for eager attention\n+        self.qkv = nn.Linear(self.dim, self.dim * 3, bias=True)\n+        self.proj = nn.Linear(self.dim, self.dim)\n+        self.scaling = math.sqrt(self.head_dim)\n+        self.config = config\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         cu_seqlens: torch.Tensor,\n         rotary_pos_emb: Optional[torch.Tensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        **kwargs,\n     ) -> torch.Tensor:\n         seq_length = hidden_states.shape[0]\n-        q, k, v = self.qkv(hidden_states).reshape(seq_length, 3, self.num_heads, -1).permute(1, 0, 2, 3).unbind(0)\n+        query_states, key_states, value_states = (\n+            self.qkv(hidden_states).reshape(seq_length, 3, self.num_heads, -1).permute(1, 0, 2, 3).unbind(0)\n+        )\n         if position_embeddings is None:\n             logger.warning_once(\n                 \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n@@ -241,87 +232,53 @@ def forward(\n             sin = emb.sin()\n         else:\n             cos, sin = position_embeddings\n-        q, k = apply_rotary_pos_emb_vision(q, k, cos, sin)\n+        query_states, key_states = apply_rotary_pos_emb_vision(query_states, key_states, cos, sin)\n \n         attention_mask = torch.full(\n-            [1, seq_length, seq_length], torch.finfo(q.dtype).min, device=q.device, dtype=q.dtype\n+            [1, 1, seq_length, seq_length],\n+            torch.finfo(value_states.dtype).min,\n+            device=value_states.device,\n+            dtype=value_states.dtype,\n         )\n         for i in range(1, len(cu_seqlens)):\n             attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = 0\n \n-        q = q.transpose(0, 1)\n-        k = k.transpose(0, 1)\n-        v = v.transpose(0, 1)\n-        attn_weights = torch.matmul(q, k.transpose(1, 2)) / math.sqrt(self.head_dim)\n-        attn_weights = attn_weights + attention_mask\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(q.dtype)\n-        attn_output = torch.matmul(attn_weights, v)\n-        attn_output = attn_output.transpose(0, 1)\n-        attn_output = attn_output.reshape(seq_length, -1)\n-        attn_output = self.proj(attn_output)\n-        return attn_output\n-\n-\n-class Qwen2_5_VLVisionSdpaAttention(nn.Module):\n-    def __init__(self, dim: int, num_heads: int = 16) -> None:\n-        super().__init__()\n-        self.num_heads = num_heads\n-        self.qkv = nn.Linear(dim, dim * 3, bias=True)\n-        self.proj = nn.Linear(dim, dim)\n+        query_states = query_states.transpose(0, 1).unsqueeze(0)  # unsqueeze batch_dim\n+        key_states = key_states.transpose(0, 1).unsqueeze(0)  # unsqueeze batch_dim\n+        value_states = value_states.transpose(0, 1).unsqueeze(0)  # unsqueeze batch_dim\n+        max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n \n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        cu_seqlens: torch.Tensor,\n-        rotary_pos_emb: Optional[torch.Tensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-    ) -> torch.Tensor:\n-        seq_length = hidden_states.shape[0]\n-        q, k, v = self.qkv(hidden_states).reshape(seq_length, 3, self.num_heads, -1).permute(1, 0, 2, 3).unbind(0)\n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `rotary_pos_emb` (2D tensor of RoPE theta values), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.54 `rotary_pos_emb` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n-        else:\n-            cos, sin = position_embeddings\n-        q, k = apply_rotary_pos_emb_vision(q, k, cos, sin)\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attention_mask = torch.zeros([1, seq_length, seq_length], device=q.device, dtype=torch.bool)\n-        for i in range(1, len(cu_seqlens)):\n-            attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = True\n-        q = q.transpose(0, 1)\n-        k = k.transpose(0, 1)\n-        v = v.transpose(0, 1)\n-        attn_output = F.scaled_dot_product_attention(\n-            q.unsqueeze(0), k.unsqueeze(0), v.unsqueeze(0), attention_mask, dropout_p=0.0\n+        attn_output, _ = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0,\n+            scaling=self.scaling,\n+            cu_seqlens_q=cu_seqlens,  # pass cu seq lens for FA2\n+            cu_seqlens_k=cu_seqlens,\n+            max_seqlen_q=max_seqlen,\n+            max_seqlen_k=max_seqlen,\n+            is_causal=False,\n+            **kwargs,\n         )\n-        attn_output = attn_output.squeeze(0).transpose(0, 1)\n-        attn_output = attn_output.reshape(seq_length, -1)\n+\n+        attn_output = attn_output.reshape(seq_length, -1).contiguous()\n         attn_output = self.proj(attn_output)\n         return attn_output\n \n \n-QWEN2_5_VL_VISION_ATTENTION_CLASSES = {\n-    \"eager\": Qwen2_5_VLVisionAttention,\n-    \"flash_attention_2\": Qwen2_5_VLVisionFlashAttention2,\n-    \"sdpa\": Qwen2_5_VLVisionSdpaAttention,\n-}\n-\n-\n class Qwen2_5_VLVisionBlock(GradientCheckpointingLayer):\n     def __init__(self, config, attn_implementation: str = \"sdpa\") -> None:\n         super().__init__()\n         self.norm1 = Qwen2RMSNorm(config.hidden_size, eps=1e-6)\n         self.norm2 = Qwen2RMSNorm(config.hidden_size, eps=1e-6)\n-        self.attn = QWEN2_5_VL_VISION_ATTENTION_CLASSES[attn_implementation](\n-            config.hidden_size, num_heads=config.num_heads\n-        )\n+        self.attn = Qwen2_5_VLVisionAttention(config=config)\n         self.mlp = Qwen2_5_VLMLP(config, bias=True)\n \n     def forward(\n@@ -330,12 +287,14 @@ def forward(\n         cu_seqlens: torch.Tensor,\n         rotary_pos_emb: Optional[torch.Tensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        **kwargs,\n     ) -> torch.Tensor:\n         hidden_states = hidden_states + self.attn(\n             self.norm1(hidden_states),\n             cu_seqlens=cu_seqlens,\n             rotary_pos_emb=rotary_pos_emb,\n             position_embeddings=position_embeddings,\n+            **kwargs,\n         )\n         hidden_states = hidden_states + self.mlp(self.norm2(hidden_states))\n         return hidden_states\n@@ -390,9 +349,7 @@ def __init__(self, config, *inputs, **kwargs) -> None:\n         head_dim = config.hidden_size // config.num_heads\n         self.rotary_pos_emb = Qwen2_5_VisionRotaryEmbedding(head_dim // 2)\n \n-        self.blocks = nn.ModuleList(\n-            [Qwen2_5_VLVisionBlock(config, config._attn_implementation) for _ in range(config.depth)]\n-        )\n+        self.blocks = nn.ModuleList([Qwen2_5_VLVisionBlock(config) for _ in range(config.depth)])\n         self.merger = Qwen2_5_VLPatchMerger(\n             dim=config.out_hidden_size,\n             context_dim=config.hidden_size,\n@@ -470,7 +427,7 @@ def get_window_index(self, grid_thw):\n \n         return window_index, cu_window_seqlens\n \n-    def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.Tensor:\n+    def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs) -> torch.Tensor:\n         \"\"\"\n         Args:\n             hidden_states (`torch.Tensor` of shape `(seq_len, hidden_size)`):\n@@ -516,7 +473,9 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.\n                 cu_seqlens_now = cu_seqlens\n             else:\n                 cu_seqlens_now = cu_window_seqlens\n-            hidden_states = blk(hidden_states, cu_seqlens=cu_seqlens_now, position_embeddings=position_embeddings)\n+            hidden_states = blk(\n+                hidden_states, cu_seqlens=cu_seqlens_now, position_embeddings=position_embeddings, **kwargs\n+            )\n \n         hidden_states = self.merger(hidden_states)\n         reverse_indices = torch.argsort(window_index)\n@@ -647,44 +606,6 @@ def apply_multimodal_rotary_pos_emb(q, k, cos, sin, mrope_section, unsqueeze_dim\n     return q_embed, k_embed\n \n \n-def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n-    \"\"\"\n-    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n-    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n-    \"\"\"\n-    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n-    if n_rep == 1:\n-        return hidden_states\n-    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n-    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n-\n-\n-def eager_attention_forward(\n-    module: nn.Module,\n-    query: torch.Tensor,\n-    key: torch.Tensor,\n-    value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n-    dropout: float = 0.0,\n-    **kwargs,\n-):\n-    key_states = repeat_kv(key, module.num_key_value_groups)\n-    value_states = repeat_kv(value, module.num_key_value_groups)\n-\n-    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n-    if attention_mask is not None:\n-        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-        attn_weights = attn_weights + causal_mask\n-\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n-    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n-    attn_output = torch.matmul(attn_weights, value_states)\n-    attn_output = attn_output.transpose(1, 2).contiguous()\n-\n-    return attn_output, attn_weights\n-\n-\n class Qwen2_5_VLAttention(nn.Module):\n     \"\"\"\n     Multi-headed attention from 'Attention Is All You Need' paper. Modified to use sliding window attention: Longformer"
        },
        {
            "sha": "84a7a69ac81eabbfc0f2cbd03ed80cc1d40c0b4a",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 12,
            "deletions": 72,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3d835d4fc145e5062d2153ac23ccd4b3e2c2cbd/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3d835d4fc145e5062d2153ac23ccd4b3e2c2cbd/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=d3d835d4fc145e5062d2153ac23ccd4b3e2c2cbd",
            "patch": "@@ -40,7 +40,6 @@\n     Qwen2VLPreTrainedModel,\n     VisionAttention,\n     VisionRotaryEmbedding,\n-    VisionSdpaAttention,\n )\n from transformers.models.qwen2_vl.processing_qwen2_vl import Qwen2VLImagesKwargs, Qwen2VLProcessor\n \n@@ -57,22 +56,12 @@\n \n \n if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import apply_rotary_emb, flash_attn_varlen_func\n+    pass\n \n \n logger = logging.get_logger(__name__)\n \n \n-def apply_rotary_pos_emb_flashatt(\n-    q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor\n-) -> tuple[torch.Tensor, torch.Tensor]:\n-    cos = cos.chunk(2, dim=-1)[0].contiguous()\n-    sin = sin.chunk(2, dim=-1)[0].contiguous()\n-    q_embed = apply_rotary_emb(q.float(), cos.float(), sin.float()).type_as(q)\n-    k_embed = apply_rotary_emb(k.float(), cos.float(), sin.float()).type_as(k)\n-    return q_embed, k_embed\n-\n-\n class Qwen2_5_VLVisionConfig(PretrainedConfig):\n     model_type = \"qwen2_5_vl\"\n     base_config_key = \"vision_config\"\n@@ -150,69 +139,18 @@ def __init__(self, dim: int, context_dim: int, spatial_merge_size: int = 2) -> N\n         self.ln_q = Qwen2RMSNorm(context_dim, eps=1e-6)\n \n \n-class Qwen2_5_VLVisionFlashAttention2(nn.Module):\n-    def __init__(self, dim: int, num_heads: int = 16) -> None:\n-        super().__init__()\n-        self.num_heads = num_heads\n-        self.qkv = nn.Linear(dim, dim * 3, bias=True)\n-        self.proj = nn.Linear(dim, dim)\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        cu_seqlens: torch.Tensor,\n-        rotary_pos_emb: Optional[torch.Tensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-    ) -> torch.Tensor:\n-        seq_length = hidden_states.shape[0]\n-        q, k, v = self.qkv(hidden_states).reshape(seq_length, 3, self.num_heads, -1).permute(1, 0, 2, 3).unbind(0)\n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `rotary_pos_emb` (2D tensor of RoPE theta values), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.54 `rotary_pos_emb` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n-        else:\n-            cos, sin = position_embeddings\n-        q, k = apply_rotary_pos_emb_flashatt(q.unsqueeze(0), k.unsqueeze(0), cos, sin)\n-        q = q.squeeze(0)\n-        k = k.squeeze(0)\n-\n-        max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n-        attn_output = flash_attn_varlen_func(q, k, v, cu_seqlens, cu_seqlens, max_seqlen, max_seqlen).reshape(\n-            seq_length, -1\n-        )\n-        attn_output = self.proj(attn_output)\n-        return attn_output\n-\n-\n class Qwen2_5_VLVisionAttention(VisionAttention):\n-    pass\n-\n-\n-class Qwen2_5_VLVisionSdpaAttention(VisionSdpaAttention):\n-    pass\n-\n-\n-QWEN2_5_VL_VISION_ATTENTION_CLASSES = {\n-    \"eager\": Qwen2_5_VLVisionAttention,\n-    \"flash_attention_2\": Qwen2_5_VLVisionFlashAttention2,\n-    \"sdpa\": Qwen2_5_VLVisionSdpaAttention,\n-}\n+    def __init__(self, config: Qwen2_5_VLVisionConfig) -> None:\n+        super().__init__()\n+        self.dim = config.hidden_size\n \n \n class Qwen2_5_VLVisionBlock(GradientCheckpointingLayer):\n     def __init__(self, config, attn_implementation: str = \"sdpa\") -> None:\n         super().__init__()\n         self.norm1 = Qwen2RMSNorm(config.hidden_size, eps=1e-6)\n         self.norm2 = Qwen2RMSNorm(config.hidden_size, eps=1e-6)\n-        self.attn = QWEN2_5_VL_VISION_ATTENTION_CLASSES[attn_implementation](\n-            config.hidden_size, num_heads=config.num_heads\n-        )\n+        self.attn = Qwen2_5_VLVisionAttention(config=config)\n         self.mlp = Qwen2_5_VLMLP(config, bias=True)\n \n     def forward(\n@@ -221,12 +159,14 @@ def forward(\n         cu_seqlens: torch.Tensor,\n         rotary_pos_emb: Optional[torch.Tensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        **kwargs,\n     ) -> torch.Tensor:\n         hidden_states = hidden_states + self.attn(\n             self.norm1(hidden_states),\n             cu_seqlens=cu_seqlens,\n             rotary_pos_emb=rotary_pos_emb,\n             position_embeddings=position_embeddings,\n+            **kwargs,\n         )\n         hidden_states = hidden_states + self.mlp(self.norm2(hidden_states))\n         return hidden_states\n@@ -269,9 +209,7 @@ def __init__(self, config, *inputs, **kwargs) -> None:\n         head_dim = config.hidden_size // config.num_heads\n         self.rotary_pos_emb = Qwen2_5_VisionRotaryEmbedding(head_dim // 2)\n \n-        self.blocks = nn.ModuleList(\n-            [Qwen2_5_VLVisionBlock(config, config._attn_implementation) for _ in range(config.depth)]\n-        )\n+        self.blocks = nn.ModuleList([Qwen2_5_VLVisionBlock(config) for _ in range(config.depth)])\n         self.merger = Qwen2_5_VLPatchMerger(\n             dim=config.out_hidden_size,\n             context_dim=config.hidden_size,\n@@ -349,7 +287,7 @@ def get_window_index(self, grid_thw):\n \n         return window_index, cu_window_seqlens\n \n-    def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.Tensor:\n+    def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs) -> torch.Tensor:\n         \"\"\"\n         Args:\n             hidden_states (`torch.Tensor` of shape `(seq_len, hidden_size)`):\n@@ -395,7 +333,9 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.\n                 cu_seqlens_now = cu_seqlens\n             else:\n                 cu_seqlens_now = cu_window_seqlens\n-            hidden_states = blk(hidden_states, cu_seqlens=cu_seqlens_now, position_embeddings=position_embeddings)\n+            hidden_states = blk(\n+                hidden_states, cu_seqlens=cu_seqlens_now, position_embeddings=position_embeddings, **kwargs\n+            )\n \n         hidden_states = self.merger(hidden_states)\n         reverse_indices = torch.argsort(window_index)"
        },
        {
            "sha": "3b3c460c0c6dd4346d924450169e14bfbf3ff963",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 93,
            "deletions": 155,
            "changes": 248,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3d835d4fc145e5062d2153ac23ccd4b3e2c2cbd/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3d835d4fc145e5062d2153ac23ccd4b3e2c2cbd/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=d3d835d4fc145e5062d2153ac23ccd4b3e2c2cbd",
            "patch": "@@ -33,7 +33,7 @@\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs, is_flash_attn_available\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n@@ -49,10 +49,6 @@\n from .configuration_qwen2_vl import Qwen2VLConfig, Qwen2VLTextConfig, Qwen2VLVisionConfig\n \n \n-if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import flash_attn_varlen_func\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -279,72 +275,69 @@ def forward(self, x) -> torch.Tensor:\n         return self.fc2(self.act(self.fc1(x)))\n \n \n-class VisionAttention(nn.Module):\n-    def __init__(self, dim: int, num_heads: int = 16) -> None:\n-        super().__init__()\n-        self.num_heads = num_heads\n-        self.head_dim = dim // num_heads\n-        self.qkv = nn.Linear(dim, dim * 3, bias=True)\n-        self.proj = nn.Linear(dim, dim)\n+# Copied from transformers.models.llama.modeling_llama.repeat_kv\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        cu_seqlens: torch.Tensor,\n-        rotary_pos_emb: Optional[torch.Tensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-    ) -> torch.Tensor:\n-        seq_length = hidden_states.shape[0]\n-        q, k, v = self.qkv(hidden_states).reshape(seq_length, 3, self.num_heads, -1).permute(1, 0, 2, 3).unbind(0)\n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `rotary_pos_emb` (2D tensor of RoPE theta values), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.54 `rotary_pos_emb` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n-        else:\n-            cos, sin = position_embeddings\n-        q, k = apply_rotary_pos_emb_vision(q, k, cos, sin)\n \n-        attention_mask = torch.full(\n-            [1, seq_length, seq_length], torch.finfo(q.dtype).min, device=q.device, dtype=q.dtype\n-        )\n-        for i in range(1, len(cu_seqlens)):\n-            attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = 0\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n \n-        q = q.transpose(0, 1)\n-        k = k.transpose(0, 1)\n-        v = v.transpose(0, 1)\n-        attn_weights = torch.matmul(q, k.transpose(1, 2)) / math.sqrt(self.head_dim)\n-        attn_weights = attn_weights + attention_mask\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(q.dtype)\n-        attn_output = torch.matmul(attn_weights, v)\n-        attn_output = attn_output.transpose(0, 1)\n-        attn_output = attn_output.reshape(seq_length, -1)\n-        attn_output = self.proj(attn_output)\n-        return attn_output\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n \n+    return attn_output, attn_weights\n \n-class VisionFlashAttention2(nn.Module):\n-    def __init__(self, dim: int, num_heads: int = 16) -> None:\n+\n+class VisionAttention(nn.Module):\n+    def __init__(self, config: Qwen2VLVisionConfig) -> None:\n         super().__init__()\n-        self.num_heads = num_heads\n-        self.qkv = nn.Linear(dim, dim * 3, bias=True)\n-        self.proj = nn.Linear(dim, dim)\n+        self.dim = config.embed_dim\n+        self.num_heads = config.num_heads\n+        self.head_dim = self.dim // self.num_heads\n+        self.num_key_value_groups = 1  # needed for eager attention\n+        self.qkv = nn.Linear(self.dim, self.dim * 3, bias=True)\n+        self.proj = nn.Linear(self.dim, self.dim)\n+        self.scaling = math.sqrt(self.head_dim)\n+        self.config = config\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         cu_seqlens: torch.Tensor,\n         rotary_pos_emb: Optional[torch.Tensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        **kwargs,\n     ) -> torch.Tensor:\n         seq_length = hidden_states.shape[0]\n-        q, k, v = self.qkv(hidden_states).reshape(seq_length, 3, self.num_heads, -1).permute(1, 0, 2, 3).unbind(0)\n+        query_states, key_states, value_states = (\n+            self.qkv(hidden_states).reshape(seq_length, 3, self.num_heads, -1).permute(1, 0, 2, 3).unbind(0)\n+        )\n         if position_embeddings is None:\n             logger.warning_once(\n                 \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n@@ -357,78 +350,55 @@ def forward(\n             sin = emb.sin()\n         else:\n             cos, sin = position_embeddings\n-        q, k = apply_rotary_pos_emb_vision(q, k, cos, sin)\n+        query_states, key_states = apply_rotary_pos_emb_vision(query_states, key_states, cos, sin)\n \n-        max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n-        attn_output = flash_attn_varlen_func(q, k, v, cu_seqlens, cu_seqlens, max_seqlen, max_seqlen).reshape(\n-            seq_length, -1\n+        attention_mask = torch.full(\n+            [1, 1, seq_length, seq_length],\n+            torch.finfo(value_states.dtype).min,\n+            device=value_states.device,\n+            dtype=value_states.dtype,\n         )\n-        attn_output = self.proj(attn_output)\n-        return attn_output\n-\n+        for i in range(1, len(cu_seqlens)):\n+            attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = 0\n \n-class VisionSdpaAttention(nn.Module):\n-    def __init__(self, dim: int, num_heads: int = 16) -> None:\n-        super().__init__()\n-        self.num_heads = num_heads\n-        self.qkv = nn.Linear(dim, dim * 3, bias=True)\n-        self.proj = nn.Linear(dim, dim)\n+        query_states = query_states.transpose(0, 1).unsqueeze(0)  # unsqueeze batch_dim\n+        key_states = key_states.transpose(0, 1).unsqueeze(0)  # unsqueeze batch_dim\n+        value_states = value_states.transpose(0, 1).unsqueeze(0)  # unsqueeze batch_dim\n+        max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n \n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        cu_seqlens: torch.Tensor,\n-        rotary_pos_emb: Optional[torch.Tensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-    ) -> torch.Tensor:\n-        seq_length = hidden_states.shape[0]\n-        q, k, v = self.qkv(hidden_states).reshape(seq_length, 3, self.num_heads, -1).permute(1, 0, 2, 3).unbind(0)\n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `rotary_pos_emb` (2D tensor of RoPE theta values), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.54 `rotary_pos_emb` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n-        else:\n-            cos, sin = position_embeddings\n-        q, k = apply_rotary_pos_emb_vision(q, k, cos, sin)\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attention_mask = torch.zeros([1, seq_length, seq_length], device=q.device, dtype=torch.bool)\n-        for i in range(1, len(cu_seqlens)):\n-            attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = True\n-        q = q.transpose(0, 1)\n-        k = k.transpose(0, 1)\n-        v = v.transpose(0, 1)\n-        attn_output = F.scaled_dot_product_attention(\n-            q.unsqueeze(0), k.unsqueeze(0), v.unsqueeze(0), attention_mask, dropout_p=0.0\n+        attn_output, _ = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0,\n+            scaling=self.scaling,\n+            cu_seqlens_q=cu_seqlens,  # pass cu seq lens for FA2\n+            cu_seqlens_k=cu_seqlens,\n+            max_seqlen_q=max_seqlen,\n+            max_seqlen_k=max_seqlen,\n+            is_causal=False,\n+            **kwargs,\n         )\n-        attn_output = attn_output.squeeze(0).transpose(0, 1)\n-        attn_output = attn_output.reshape(seq_length, -1)\n+\n+        attn_output = attn_output.reshape(seq_length, -1).contiguous()\n         attn_output = self.proj(attn_output)\n         return attn_output\n \n \n-QWEN2_VL_VISION_ATTENTION_CLASSES = {\n-    \"eager\": VisionAttention,\n-    \"flash_attention_2\": VisionFlashAttention2,\n-    \"sdpa\": VisionSdpaAttention,\n-}\n-\n-\n class Qwen2VLVisionBlock(GradientCheckpointingLayer):\n     def __init__(self, config, attn_implementation: str = \"sdpa\") -> None:\n         super().__init__()\n         self.norm1 = LayerNorm(config.embed_dim, eps=1e-6)\n         self.norm2 = LayerNorm(config.embed_dim, eps=1e-6)\n         mlp_hidden_dim = int(config.embed_dim * config.mlp_ratio)\n \n-        self.attn = QWEN2_VL_VISION_ATTENTION_CLASSES[attn_implementation](\n-            config.embed_dim, num_heads=config.num_heads\n-        )\n+        self.attn = VisionAttention(config=config)\n         self.mlp = VisionMlp(dim=config.embed_dim, hidden_dim=mlp_hidden_dim, hidden_act=config.hidden_act)\n \n     def forward(\n@@ -437,12 +407,14 @@ def forward(\n         cu_seqlens: torch.Tensor,\n         rotary_pos_emb: Optional[torch.Tensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        **kwargs,\n     ) -> torch.Tensor:\n         hidden_states = hidden_states + self.attn(\n             self.norm1(hidden_states),\n             cu_seqlens=cu_seqlens,\n             rotary_pos_emb=rotary_pos_emb,\n             position_embeddings=position_embeddings,\n+            **kwargs,\n         )\n         hidden_states = hidden_states + self.mlp(self.norm2(hidden_states))\n         return hidden_states\n@@ -486,45 +458,6 @@ def forward(self, x):\n         return down_proj\n \n \n-# Copied from transformers.models.llama.modeling_llama.repeat_kv\n-def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n-    \"\"\"\n-    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n-    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n-    \"\"\"\n-    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n-    if n_rep == 1:\n-        return hidden_states\n-    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n-    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n-\n-\n-def eager_attention_forward(\n-    module: nn.Module,\n-    query: torch.Tensor,\n-    key: torch.Tensor,\n-    value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n-    dropout: float = 0.0,\n-    **kwargs,\n-):\n-    key_states = repeat_kv(key, module.num_key_value_groups)\n-    value_states = repeat_kv(value, module.num_key_value_groups)\n-\n-    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n-    if attention_mask is not None:\n-        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-        attn_weights = attn_weights + causal_mask\n-\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n-    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n-    attn_output = torch.matmul(attn_weights, value_states)\n-    attn_output = attn_output.transpose(1, 2).contiguous()\n-\n-    return attn_output, attn_weights\n-\n-\n class Qwen2VLAttention(nn.Module):\n     \"\"\"\n     Multi-headed attention from 'Attention Is All You Need' paper. Modified to use sliding window attention: Longformer\n@@ -752,9 +685,7 @@ def __init__(self, config) -> None:\n         head_dim = config.embed_dim // config.num_heads\n         self.rotary_pos_emb = VisionRotaryEmbedding(head_dim // 2)\n \n-        self.blocks = nn.ModuleList(\n-            [Qwen2VLVisionBlock(config, config._attn_implementation) for _ in range(config.depth)]\n-        )\n+        self.blocks = nn.ModuleList([Qwen2VLVisionBlock(config) for _ in range(config.depth)])\n         self.merger = PatchMerger(\n             dim=config.hidden_size, context_dim=config.embed_dim, spatial_merge_size=config.spatial_merge_size\n         )\n@@ -796,7 +727,12 @@ def rot_pos_emb(self, grid_thw):\n         return rotary_pos_emb\n \n     @auto_docstring\n-    def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.Tensor:\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        grid_thw: torch.Tensor,\n+        **kwargs,\n+    ) -> torch.Tensor:\n         r\"\"\"\n         grid_thw (`torch.LongTensor` of shape `(num_images, 3)`):\n             The temporal, height and width dimensions of feature shape for each image. Each row contains [t, h, w] values.\n@@ -817,7 +753,9 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.\n         cu_seqlens = F.pad(cu_seqlens, (1, 0), value=0)\n \n         for blk in self.blocks:\n-            hidden_states = blk(hidden_states, cu_seqlens=cu_seqlens, position_embeddings=position_embeddings)\n+            hidden_states = blk(\n+                hidden_states, cu_seqlens=cu_seqlens, position_embeddings=position_embeddings, **kwargs\n+            )\n \n         return self.merger(hidden_states)\n "
        }
    ],
    "stats": {
        "total": 1255,
        "additions": 409,
        "deletions": 846
    }
}