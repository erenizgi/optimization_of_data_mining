{
    "author": "zucchini-nlp",
    "message": "fix qwen text config (#41158)\n\n* fix qwen text config\n\n* fix tests\n\n* fix one more test\n\n* address comments",
    "sha": "f22cb1e8689905091acb07e238b670aadde1e4ee",
    "files": [
        {
            "sha": "4c417020fa84666613e09bac4f4d4d428865a279",
            "filename": "src/transformers/models/glm4v/configuration_glm4v.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f22cb1e8689905091acb07e238b670aadde1e4ee/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconfiguration_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f22cb1e8689905091acb07e238b670aadde1e4ee/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconfiguration_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconfiguration_glm4v.py?ref=f22cb1e8689905091acb07e238b670aadde1e4ee",
            "patch": "@@ -330,7 +330,6 @@ def __init__(\n         video_end_token_id=151342,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n         if isinstance(vision_config, dict):\n             self.vision_config = self.sub_configs[\"vision_config\"](**vision_config)\n         elif vision_config is None:\n@@ -339,7 +338,6 @@ def __init__(\n         if isinstance(text_config, dict):\n             self.text_config = self.sub_configs[\"text_config\"](**text_config)\n         elif text_config is None:\n-            # For BC use all kwargs to init `TextConfig`\n             self.text_config = self.sub_configs[\"text_config\"](**kwargs)\n \n         self.image_token_id = image_token_id\n@@ -349,5 +347,7 @@ def __init__(\n         self.image_start_token_id = image_start_token_id\n         self.image_end_token_id = image_end_token_id\n \n+        super().__init__(**kwargs)\n+\n \n __all__ = [\"Glm4vConfig\", \"Glm4vTextConfig\"]"
        },
        {
            "sha": "d15057a0218b34e1d64b4dbbe870bea6e7671677",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 19,
            "deletions": 3,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/f22cb1e8689905091acb07e238b670aadde1e4ee/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f22cb1e8689905091acb07e238b670aadde1e4ee/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=f22cb1e8689905091acb07e238b670aadde1e4ee",
            "patch": "@@ -38,7 +38,6 @@\n from ...utils.generic import check_model_inputs\n from ...video_utils import VideoInput\n from ..glm4.modeling_glm4 import Glm4MLP, Glm4RMSNorm, eager_attention_forward\n-from ..qwen2_5_vl.configuration_qwen2_5_vl import Qwen2_5_VLConfig\n from ..qwen2_5_vl.modeling_qwen2_5_vl import (\n     Qwen2_5_VisionPatchEmbed,\n     Qwen2_5_VisionRotaryEmbedding,\n@@ -313,7 +312,7 @@ def __init__(\n         super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n \n \n-class Glm4vConfig(Qwen2_5_VLConfig):\n+class Glm4vConfig(PretrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Glm4vModel`]. It is used to instantiate a\n     GLM-4.1V model according to the specified arguments, defining the model architecture. Instantiating a\n@@ -355,6 +354,10 @@ class Glm4vConfig(Qwen2_5_VLConfig):\n     >>> configuration = model.config\n     ```\"\"\"\n \n+    model_type = \"glm4v\"\n+    sub_configs = {\"vision_config\": Glm4vVisionConfig, \"text_config\": Glm4vTextConfig}\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+\n     def __init__(\n         self,\n         text_config=None,\n@@ -367,12 +370,25 @@ def __init__(\n         video_end_token_id=151342,\n         **kwargs,\n     ):\n-        super().__init__()\n+        if isinstance(vision_config, dict):\n+            self.vision_config = self.sub_configs[\"vision_config\"](**vision_config)\n+        elif vision_config is None:\n+            self.vision_config = self.sub_configs[\"vision_config\"]()\n+\n+        if isinstance(text_config, dict):\n+            self.text_config = self.sub_configs[\"text_config\"](**text_config)\n+        elif text_config is None:\n+            self.text_config = self.sub_configs[\"text_config\"](**kwargs)\n+\n+        self.image_token_id = image_token_id\n+        self.video_token_id = video_token_id\n         self.video_start_token_id = video_start_token_id\n         self.video_end_token_id = video_end_token_id\n         self.image_start_token_id = image_start_token_id\n         self.image_end_token_id = image_end_token_id\n \n+        super().__init__(**kwargs)\n+\n \n # Will be used for both Text and Vision modalities\n class Glm4vRMSNorm(Glm4RMSNorm):"
        },
        {
            "sha": "b06642e250bcfedd82dcb7f47e2aae6d3f249dcb",
            "filename": "src/transformers/models/glm4v_moe/configuration_glm4v_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f22cb1e8689905091acb07e238b670aadde1e4ee/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f22cb1e8689905091acb07e238b670aadde1e4ee/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py?ref=f22cb1e8689905091acb07e238b670aadde1e4ee",
            "patch": "@@ -371,7 +371,6 @@ def __init__(\n         if isinstance(text_config, dict):\n             self.text_config = self.sub_configs[\"text_config\"](**text_config)\n         elif text_config is None:\n-            # For BC use all kwargs to init `TextConfig`\n             self.text_config = self.sub_configs[\"text_config\"](**kwargs)\n \n         self.image_token_id = image_token_id"
        },
        {
            "sha": "fcd17cb5811fc565c29b6a4d1878893a17cf77f1",
            "filename": "src/transformers/models/qwen2_5_vl/configuration_qwen2_5_vl.py",
            "status": "modified",
            "additions": 37,
            "deletions": 11,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/f22cb1e8689905091acb07e238b670aadde1e4ee/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f22cb1e8689905091acb07e238b670aadde1e4ee/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py?ref=f22cb1e8689905091acb07e238b670aadde1e4ee",
            "patch": "@@ -159,10 +159,6 @@ class Qwen2_5_VLTextConfig(PretrainedConfig):\n                     Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n                 `high_freq_factor` (`float`, *optional*):\n                     Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n-        image_token_id (`int`, *optional*):\n-            Token index used as placeholder for image embeddings.\n-        video_token_id (`int`, *optional*):\n-            Token index used as placeholder for video embeddings.\n \n     ```python\n     >>> from transformers import Qwen2_5_VLTextModel, Qwen2_5_VLConfig\n@@ -217,8 +213,6 @@ def __init__(\n         layer_types=None,\n         attention_dropout=0.0,\n         rope_scaling=None,\n-        image_token_id=None,\n-        video_token_id=None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -264,9 +258,6 @@ def __init__(\n                 self.rope_scaling[\"type\"] = \"default\"\n             self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n         rope_config_validation(self, ignore_keys={\"mrope_section\"})\n-        self.image_token_id = image_token_id\n-        self.video_token_id = video_token_id\n-\n         super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n \n \n@@ -290,6 +281,10 @@ class Qwen2_5_VLConfig(PretrainedConfig):\n             The image token index to encode the image prompt.\n         video_token_id (`int`, *optional*, defaults to 151656):\n             The video token index to encode the image prompt.\n+        vision_start_token_id (`int`, *optional*, defaults to 151652):\n+            The token index to denote start of vision input.\n+        vision_end_token_id (`int`, *optional*, defaults to 151653):\n+            The token index to denote end of vision input.\n \n     ```python\n     >>> from transformers import Qwen2_5_VLForConditionalGeneration, Qwen2_5_VLConfig\n@@ -314,8 +309,15 @@ def __init__(\n         vision_config=None,\n         image_token_id=151655,\n         video_token_id=151656,\n+        vision_start_token_id=151652,\n+        vision_end_token_id=151653,\n         **kwargs,\n     ):\n+        # We need to init super() here so that it does not reset values\n+        # that are in text config to the BaseClass defaults. The Base\n+        # config has many text related defaults and not all defaults are same as for `Qwen2_5_VLTextConfig`\n+        super().__init__(**kwargs)\n+\n         if isinstance(vision_config, dict):\n             self.vision_config = self.sub_configs[\"vision_config\"](**vision_config)\n         elif vision_config is None:\n@@ -329,8 +331,32 @@ def __init__(\n \n         self.image_token_id = image_token_id\n         self.video_token_id = video_token_id\n-\n-        super().__init__(**kwargs)\n+        self.vision_start_token_id = vision_start_token_id\n+        self.vision_end_token_id = vision_end_token_id\n+\n+        # Attention implementation to use. It sets it recursively on sub-configs so we call it again in the end\n+        self._attn_implementation = kwargs.pop(\"attn_implementation\", None)\n+\n+    def __setattr__(self, key, value):\n+        if (\n+            (text_config := super().__getattribute__(\"__dict__\").get(\"text_config\")) is not None\n+            and key not in [\"dtype\", \"_attn_implementation_internal\"]\n+            and key in text_config.__dict__\n+        ):\n+            setattr(text_config, key, value)\n+        else:\n+            super().__setattr__(key, value)\n+\n+    def __getattribute__(self, key):\n+        if \"text_config\" in super().__getattribute__(\"__dict__\") and key not in [\n+            \"dtype\",\n+            \"_attn_implementation_internal\",\n+        ]:\n+            text_config = super().__getattribute__(\"text_config\")\n+            if key in text_config.__dict__:\n+                return getattr(text_config, key)\n+\n+        return super().__getattribute__(key)\n \n \n __all__ = [\"Qwen2_5_VLConfig\", \"Qwen2_5_VLTextConfig\"]"
        },
        {
            "sha": "774e35d30bb25f6d68c9368b16f304339ef56a08",
            "filename": "src/transformers/models/qwen2_vl/configuration_qwen2_vl.py",
            "status": "modified",
            "additions": 43,
            "deletions": 17,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/f22cb1e8689905091acb07e238b670aadde1e4ee/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f22cb1e8689905091acb07e238b670aadde1e4ee/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py?ref=f22cb1e8689905091acb07e238b670aadde1e4ee",
            "patch": "@@ -148,10 +148,6 @@ class Qwen2VLTextConfig(PretrainedConfig):\n                     Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n                 `high_freq_factor` (`float`, *optional*):\n                     Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n-        image_token_id (`int`, *optional*):\n-            Token index used as placeholder for image embeddings.\n-        video_token_id (`int`, *optional*):\n-            Token index used as placeholder for video embeddings.\n \n     ```python\n     >>> from transformers import Qwen2VLTextModel, Qwen2VLConfig\n@@ -206,8 +202,6 @@ def __init__(\n         layer_types=None,\n         attention_dropout=0.0,\n         rope_scaling=None,\n-        image_token_id=None,\n-        video_token_id=None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -253,9 +247,6 @@ def __init__(\n                 self.rope_scaling[\"type\"] = \"default\"\n             self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n         rope_config_validation(self, ignore_keys={\"mrope_section\"})\n-        self.image_token_id = image_token_id\n-        self.video_token_id = video_token_id\n-\n         super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n \n \n@@ -271,23 +262,27 @@ class Qwen2VLConfig(PretrainedConfig):\n \n \n     Args:\n-        text_config (`Union[PreTrainedConfig, dict]`, *optional*, defaults to `Qwen2_5_VLTextConfig`):\n+        text_config (`Union[PreTrainedConfig, dict]`, *optional*, defaults to `Qwen2VLTextConfig`):\n             The config object or dictionary of the text backbone.\n-        vision_config (`Union[PreTrainedConfig, dict]`,  *optional*, defaults to `Qwen2_5_VLVisionConfig`):\n+        vision_config (`Union[PreTrainedConfig, dict]`,  *optional*, defaults to `Qwen2VLVisionConfig`):\n             The config object or dictionary of the vision backbone.\n         image_token_id (`int`, *optional*, defaults to 151655):\n             The image token index to encode the image prompt.\n         video_token_id (`int`, *optional*, defaults to 151656):\n             The video token index to encode the image prompt.\n+        vision_start_token_id (`int`, *optional*, defaults to 151652):\n+            The token index to denote start of vision input.\n+        vision_end_token_id (`int`, *optional*, defaults to 151653):\n+            The token index to denote end of vision input.\n \n     ```python\n-    >>> from transformers import Qwen2_5_VLForConditionalGeneration, Qwen2_5_VLConfig\n+    >>> from transformers import Qwen2VLForConditionalGeneration, Qwen2VLConfig\n \n-    >>> # Initializing a Qwen2_5_VL style configuration\n-    >>> configuration = Qwen2_5_VLConfig()\n+    >>> # Initializing a Qwen2VL style configuration\n+    >>> configuration = Qwen2VLConfig()\n \n     >>> # Initializing a model from the Qwen2-VL-7B style configuration\n-    >>> model = Qwen2_5_VLForConditionalGeneration(configuration)\n+    >>> model = Qwen2VLForConditionalGeneration(configuration)\n \n     >>> # Accessing the model configuration\n     >>> configuration = model.config\n@@ -303,8 +298,15 @@ def __init__(\n         vision_config=None,\n         image_token_id=151655,\n         video_token_id=151656,\n+        vision_start_token_id=151652,\n+        vision_end_token_id=151653,\n         **kwargs,\n     ):\n+        # We need to init super() here so that it does not reset values\n+        # that are in text config to the BaseClass defaults. The Base\n+        # config has many text related defaults and not all defaults are same as for `Qwen2VLTextConfig`\n+        super().__init__(**kwargs)\n+\n         if isinstance(vision_config, dict):\n             self.vision_config = self.sub_configs[\"vision_config\"](**vision_config)\n         elif vision_config is None:\n@@ -318,8 +320,32 @@ def __init__(\n \n         self.image_token_id = image_token_id\n         self.video_token_id = video_token_id\n-\n-        super().__init__(**kwargs)\n+        self.vision_start_token_id = vision_start_token_id\n+        self.vision_end_token_id = vision_end_token_id\n+\n+        # Attention implementation to use. It sets it recursively on sub-configs so we call it again in the end\n+        self._attn_implementation = kwargs.pop(\"attn_implementation\", None)\n+\n+    def __setattr__(self, key, value):\n+        if (\n+            (text_config := super().__getattribute__(\"__dict__\").get(\"text_config\")) is not None\n+            and key not in [\"dtype\", \"_attn_implementation_internal\"]\n+            and key in text_config.__dict__\n+        ):\n+            setattr(text_config, key, value)\n+        else:\n+            super().__setattr__(key, value)\n+\n+    def __getattribute__(self, key):\n+        if \"text_config\" in super().__getattribute__(\"__dict__\") and key not in [\n+            \"dtype\",\n+            \"_attn_implementation_internal\",\n+        ]:\n+            text_config = super().__getattribute__(\"text_config\")\n+            if key in text_config.__dict__:\n+                return getattr(text_config, key)\n+\n+        return super().__getattribute__(key)\n \n \n __all__ = [\"Qwen2VLConfig\", \"Qwen2VLTextConfig\"]"
        },
        {
            "sha": "f3f4fa2ffe4fc7842ee714a5859ff6661e44a1ae",
            "filename": "tests/models/qwen2_5_vl/test_modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 51,
            "deletions": 35,
            "changes": 86,
            "blob_url": "https://github.com/huggingface/transformers/blob/f22cb1e8689905091acb07e238b670aadde1e4ee/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f22cb1e8689905091acb07e238b670aadde1e4ee/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py?ref=f22cb1e8689905091acb07e238b670aadde1e4ee",
            "patch": "@@ -74,44 +74,34 @@ def __init__(\n         bos_token_id=0,\n         eos_token_id=1,\n         pad_token_id=2,\n-        vision_start_token_id=3,\n-        image_token_id=4,\n-        video_token_id=5,\n         hidden_act=\"silu\",\n         hidden_size=32,\n         vocab_size=99,\n         intermediate_size=37,\n         max_position_embeddings=512,\n         max_window_layers=3,\n-        model_type=\"qwen2_5_vl\",\n         num_attention_heads=4,\n         num_hidden_layers=2,\n         num_key_value_heads=2,\n         rope_theta=10000,\n         tie_word_embeddings=True,\n         is_training=True,\n         vision_config=None,\n-        rope_scaling=None,\n+        vision_start_token_id=3,\n+        image_token_id=4,\n+        video_token_id=5,\n     ):\n         self.parent = parent\n         self.ignore_index = ignore_index\n         self.bos_token_id = bos_token_id\n         self.eos_token_id = eos_token_id\n         self.pad_token_id = pad_token_id\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.hidden_size = hidden_size\n         self.vision_start_token_id = vision_start_token_id\n         self.image_token_id = image_token_id\n         self.video_token_id = video_token_id\n-        self.hidden_act = hidden_act\n-        self.hidden_size = hidden_size\n-        self.intermediate_size = intermediate_size\n-        self.max_position_embeddings = max_position_embeddings\n-        self.max_window_layers = max_window_layers\n-        self.model_type = model_type\n-        self.num_attention_heads = num_attention_heads\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_key_value_heads = num_key_value_heads\n-        self.rope_theta = rope_theta\n-        self.tie_word_embeddings = tie_word_embeddings\n         self.batch_size = batch_size\n         self.num_channels = num_channels\n         self.image_size = image_size\n@@ -135,32 +125,31 @@ def __init__(\n                 \"temporal_patch_size\": 2,\n             }\n         self.vision_config = vision_config\n-        # Same goes for rope scaling\n-        if rope_scaling is None:\n-            rope_scaling = {\"type\": \"mrope\", \"mrope_section\": [2, 1, 1]}\n-        self.rope_scaling = rope_scaling\n+        self.text_config = {\n+            \"bos_token_id\": bos_token_id,\n+            \"eos_token_id\": eos_token_id,\n+            \"pad_token_id\": pad_token_id,\n+            \"hidden_act\": hidden_act,\n+            \"hidden_size\": hidden_size,\n+            \"intermediate_size\": intermediate_size,\n+            \"max_position_embeddings\": max_position_embeddings,\n+            \"max_window_layers\": max_window_layers,\n+            \"num_attention_heads\": num_attention_heads,\n+            \"num_hidden_layers\": num_hidden_layers,\n+            \"num_key_value_heads\": num_key_value_heads,\n+            \"rope_theta\": rope_theta,\n+            \"tie_word_embeddings\": tie_word_embeddings,\n+            \"vocab_size\": vocab_size,\n+            \"rope_scaling\": {\"type\": \"mrope\", \"mrope_section\": [2, 1, 1]},\n+        }\n \n     def get_config(self):\n         return Qwen2_5_VLConfig(\n-            hidden_size=self.hidden_size,\n-            intermediate_size=self.intermediate_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            num_key_value_heads=self.num_key_value_heads,\n-            hidden_act=self.hidden_act,\n-            max_position_embeddings=self.max_position_embeddings,\n+            text_config=self.text_config,\n             vision_config=self.vision_config,\n-            model_type=self.model_type,\n-            max_window_layers=self.max_window_layers,\n-            rope_scaling=self.rope_scaling,\n-            tie_word_embeddings=self.tie_word_embeddings,\n-            bos_token_id=self.bos_token_id,\n-            eos_token_id=self.eos_token_id,\n-            pad_token_id=self.pad_token_id,\n             vision_start_token_id=self.vision_start_token_id,\n             image_token_id=self.image_token_id,\n             video_token_id=self.video_token_id,\n-            vocab_size=self.vocab_size,\n         )\n \n     def prepare_config_and_inputs(self):\n@@ -220,6 +209,33 @@ def setUp(self):\n     def test_config(self):\n         self.config_tester.run_common_tests()\n \n+    def test_text_config(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+        base_config_dict = config.to_dict()\n+        base_config = Qwen2_5_VLConfig(**base_config_dict)\n+\n+        # Trying to get or set text related attributes happens via text config\n+        vocab_size = base_config.vocab_size\n+        text_vocab_size = base_config.text_config.vocab_size\n+        self.assertEqual(vocab_size, text_vocab_size)\n+\n+        base_config.vocab_size = 55\n+        self.assertEqual(base_config.vocab_size, 55)\n+        self.assertEqual(base_config.text_config.vocab_size, 55)\n+\n+        # We can still initialize config from old-format json, i.e. flat structure\n+        text_config_dict = base_config_dict.pop(\"text_config\")\n+        flat_config_dict = {**text_config_dict, **base_config_dict}\n+        config_from_flat_dict = Qwen2_5_VLConfig(**flat_config_dict)\n+        config_from_flat_dict.vocab_size = 78\n+        self.assertEqual(config_from_flat_dict.vocab_size, 78)\n+        self.assertEqual(config_from_flat_dict.text_config.vocab_size, 78)\n+\n+        # Vision config attributes are NOT force-set via vision config\n+        base_config.patch_size = 8\n+        self.assertEqual(base_config.patch_size, 8)\n+        self.assertNotEqual(base_config.vision_config.patch_size, 8)\n+\n     def test_initialization(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n "
        },
        {
            "sha": "9cb028eb3a731f05de0523b331972dd87638ce8e",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 53,
            "deletions": 47,
            "changes": 100,
            "blob_url": "https://github.com/huggingface/transformers/blob/f22cb1e8689905091acb07e238b670aadde1e4ee/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f22cb1e8689905091acb07e238b670aadde1e4ee/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=f22cb1e8689905091acb07e238b670aadde1e4ee",
            "patch": "@@ -65,24 +65,26 @@ def __init__(\n         num_channels=3,\n         ignore_index=-100,\n         image_size=14,\n-        bos_token_id=0,\n-        eos_token_id=1,\n-        pad_token_id=2,\n+        text_config={\n+            \"bos_token_id\": 0,\n+            \"eos_token_id\": 1,\n+            \"pad_token_id\": 2,\n+            \"hidden_act\": \"silu\",\n+            \"hidden_size\": 32,\n+            \"vocab_size\": 99,\n+            \"intermediate_size\": 37,\n+            \"max_position_embeddings\": 512,\n+            \"max_window_layers\": 3,\n+            \"num_attention_heads\": 4,\n+            \"num_hidden_layers\": 2,\n+            \"num_key_value_heads\": 2,\n+            \"rope_theta\": 10000,\n+            \"tie_word_embeddings\": True,\n+            \"rope_scaling\": {\"type\": \"mrope\", \"mrope_section\": [2, 1, 1]},\n+        },\n         vision_start_token_id=3,\n         image_token_id=4,\n         video_token_id=5,\n-        hidden_act=\"silu\",\n-        hidden_size=32,\n-        vocab_size=99,\n-        intermediate_size=37,\n-        max_position_embeddings=512,\n-        max_window_layers=3,\n-        model_type=\"qwen2_vl\",\n-        num_attention_heads=4,\n-        num_hidden_layers=2,\n-        num_key_value_heads=2,\n-        rope_theta=10000,\n-        tie_word_embeddings=True,\n         is_training=True,\n         vision_config={\n             \"depth\": 2,\n@@ -95,58 +97,35 @@ def __init__(\n             \"spatial_merge_size\": 1,\n             \"temporal_patch_size\": 2,\n         },\n-        rope_scaling={\"type\": \"mrope\", \"mrope_section\": [2, 1, 1]},\n     ):\n         self.parent = parent\n         self.ignore_index = ignore_index\n-        self.bos_token_id = bos_token_id\n-        self.eos_token_id = eos_token_id\n-        self.pad_token_id = pad_token_id\n+        self.bos_token_id = text_config[\"bos_token_id\"]\n+        self.eos_token_id = text_config[\"eos_token_id\"]\n+        self.pad_token_id = text_config[\"pad_token_id\"]\n+        self.num_hidden_layers = text_config[\"num_hidden_layers\"]\n+        self.num_attention_heads = text_config[\"num_attention_heads\"]\n+        self.hidden_size = text_config[\"hidden_size\"]\n         self.vision_start_token_id = vision_start_token_id\n         self.image_token_id = image_token_id\n         self.video_token_id = video_token_id\n-        self.hidden_act = hidden_act\n-        self.hidden_size = hidden_size\n-        self.intermediate_size = intermediate_size\n-        self.max_position_embeddings = max_position_embeddings\n-        self.max_window_layers = max_window_layers\n-        self.model_type = model_type\n-        self.num_attention_heads = num_attention_heads\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_key_value_heads = num_key_value_heads\n-        self.rope_theta = rope_theta\n-        self.tie_word_embeddings = tie_word_embeddings\n+        self.text_config = text_config\n         self.vision_config = vision_config\n-        self.rope_scaling = rope_scaling\n         self.batch_size = batch_size\n         self.num_channels = num_channels\n         self.image_size = image_size\n         self.is_training = is_training\n-        self.vocab_size = vocab_size\n+        self.vocab_size = text_config[\"vocab_size\"]\n         self.num_image_tokens = 32\n         self.seq_length = seq_length + self.num_image_tokens\n \n     def get_config(self):\n         return Qwen2VLConfig(\n-            hidden_size=self.hidden_size,\n-            intermediate_size=self.intermediate_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            num_key_value_heads=self.num_key_value_heads,\n-            hidden_act=self.hidden_act,\n-            max_position_embeddings=self.max_position_embeddings,\n+            text_config=self.text_config,\n             vision_config=self.vision_config,\n-            model_type=self.model_type,\n-            max_window_layers=self.max_window_layers,\n-            rope_scaling=self.rope_scaling,\n-            tie_word_embeddings=self.tie_word_embeddings,\n-            bos_token_id=self.bos_token_id,\n-            eos_token_id=self.eos_token_id,\n-            pad_token_id=self.pad_token_id,\n             vision_start_token_id=self.vision_start_token_id,\n             image_token_id=self.image_token_id,\n             video_token_id=self.video_token_id,\n-            vocab_size=self.vocab_size,\n         )\n \n     def prepare_config_and_inputs(self):\n@@ -210,6 +189,33 @@ def setUp(self):\n     def test_config(self):\n         self.config_tester.run_common_tests()\n \n+    def test_text_config(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+        base_config_dict = config.to_dict()\n+        base_config = Qwen2VLConfig(**base_config_dict)\n+\n+        # Trying to get or set text related attributes happens via text config\n+        vocab_size = base_config.vocab_size\n+        text_vocab_size = base_config.text_config.vocab_size\n+        self.assertEqual(vocab_size, text_vocab_size)\n+\n+        base_config.vocab_size = 55\n+        self.assertEqual(base_config.vocab_size, 55)\n+        self.assertEqual(base_config.text_config.vocab_size, 55)\n+\n+        # We can still initialize config from old-format json, i.e. flat structure\n+        text_config_dict = base_config_dict.pop(\"text_config\")\n+        flat_config_dict = {**text_config_dict, **base_config_dict}\n+        config_from_flat_dict = Qwen2VLConfig(**flat_config_dict)\n+        config_from_flat_dict.vocab_size = 78\n+        self.assertEqual(config_from_flat_dict.vocab_size, 78)\n+        self.assertEqual(config_from_flat_dict.text_config.vocab_size, 78)\n+\n+        # Vision config attributes are NOT force-set via vision config\n+        base_config.patch_size = 8\n+        self.assertEqual(base_config.patch_size, 8)\n+        self.assertNotEqual(base_config.vision_config.patch_size, 8)\n+\n     def test_initialization(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n "
        },
        {
            "sha": "ebccb1078a43370b46fce59fe09126bae546754a",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f22cb1e8689905091acb07e238b670aadde1e4ee/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f22cb1e8689905091acb07e238b670aadde1e4ee/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=f22cb1e8689905091acb07e238b670aadde1e4ee",
            "patch": "@@ -3899,8 +3899,7 @@ def test_sliding_window_mask(self):\n             # Set sliding window to `True` and check that all tokens beyond window size are masked\n             config.use_sliding_window = True\n             config_dict = config.to_diff_dict()\n-            if hasattr(config, \"layer_types\"):\n-                del config_dict[\"layer_types\"]\n+            config_dict.pop(\"layer_types\", None)\n             new_config = config.__class__(**config_dict)\n             # We need to set eager as otherwise `output_attentions` is not supported\n             model = model_class._from_config(new_config, attn_implementation=\"eager\").to(torch_device)\n@@ -3917,8 +3916,7 @@ def test_sliding_window_mask(self):\n             # Check that all tokens beyond window size are not masked\n             config.use_sliding_window = False\n             config_dict = config.to_diff_dict()\n-            if hasattr(config, \"layer_types\"):\n-                del config_dict[\"layer_types\"]\n+            config_dict.pop(\"layer_types\", None)\n             new_config = config.__class__(**config_dict)\n             # We need to set eager as otherwise `output_attentions` is not supported\n             model = model_class._from_config(new_config, attn_implementation=\"eager\").to(torch_device)"
        }
    ],
    "stats": {
        "total": 327,
        "additions": 207,
        "deletions": 120
    }
}