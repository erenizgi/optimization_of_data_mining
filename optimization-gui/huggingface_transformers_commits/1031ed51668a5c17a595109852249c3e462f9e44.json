{
    "author": "ahadnagy",
    "message": "Disable custom MRA kernels for ROCm (#38738)\n\n* Disable custom MRA kernels for ROCm\n\n* Move platform check code to utils\n\n* Ruff\n\n* Ruff again\n\n* Fix querying HIP version\n\n* Revert some changes\n\n* Add missing return statement\n\n---------\n\nCo-authored-by: ivarflakstad <69173633+ivarflakstad@users.noreply.github.com>",
    "sha": "1031ed51668a5c17a595109852249c3e462f9e44",
    "files": [
        {
            "sha": "ec793d62c161ce47ce1d9201c12658645024c787",
            "filename": "src/transformers/models/mra/modeling_mra.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1031ed51668a5c17a595109852249c3e462f9e44/src%2Ftransformers%2Fmodels%2Fmra%2Fmodeling_mra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1031ed51668a5c17a595109852249c3e462f9e44/src%2Ftransformers%2Fmodels%2Fmra%2Fmodeling_mra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmra%2Fmodeling_mra.py?ref=1031ed51668a5c17a595109852249c3e462f9e44",
            "patch": "@@ -35,7 +35,7 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import auto_docstring, is_ninja_available, is_torch_cuda_available, logging\n+from ...utils import auto_docstring, is_cuda_platform, is_ninja_available, is_torch_cuda_available, logging\n from .configuration_mra import MraConfig\n \n \n@@ -528,7 +528,7 @@ def __init__(self, config, position_embedding_type=None):\n             )\n \n         kernel_loaded = mra_cuda_kernel is not None\n-        if is_torch_cuda_available() and is_ninja_available() and not kernel_loaded:\n+        if is_torch_cuda_available() and is_cuda_platform() and is_ninja_available() and not kernel_loaded:\n             try:\n                 load_cuda_kernels()\n             except Exception as e:"
        },
        {
            "sha": "564213d720e6d096a1ce968523b89acc895b032c",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1031ed51668a5c17a595109852249c3e462f9e44/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1031ed51668a5c17a595109852249c3e462f9e44/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=1031ed51668a5c17a595109852249c3e462f9e44",
            "patch": "@@ -141,6 +141,7 @@\n     is_ccl_available,\n     is_coloredlogs_available,\n     is_compressed_tensors_available,\n+    is_cuda_platform,\n     is_cv2_available,\n     is_cython_available,\n     is_datasets_available,\n@@ -201,6 +202,7 @@\n     is_quark_available,\n     is_rich_available,\n     is_rjieba_available,\n+    is_rocm_platform,\n     is_sacremoses_available,\n     is_safetensors_available,\n     is_sagemaker_dp_enabled,"
        },
        {
            "sha": "ebd1ae9ef19e6d26ee18b382b9b6fe7346de45b4",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/1031ed51668a5c17a595109852249c3e462f9e44/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1031ed51668a5c17a595109852249c3e462f9e44/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=1031ed51668a5c17a595109852249c3e462f9e44",
            "patch": "@@ -484,6 +484,24 @@ def is_torch_cuda_available():\n         return False\n \n \n+def is_cuda_platform():\n+    if is_torch_available():\n+        import torch\n+\n+        return torch.version.cuda is not None\n+    else:\n+        return False\n+\n+\n+def is_rocm_platform():\n+    if is_torch_available():\n+        import torch\n+\n+        return torch.version.hip is not None\n+    else:\n+        return False\n+\n+\n def is_mamba_ssm_available():\n     if is_torch_available():\n         import torch"
        }
    ],
    "stats": {
        "total": 24,
        "additions": 22,
        "deletions": 2
    }
}