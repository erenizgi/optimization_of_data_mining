{
    "author": "zucchini-nlp",
    "message": "[vlm] fix loading of retrieval VLMs (#39242)\n\n* fix vlm with retrieval\n\n* we can't use AutoModel because new ColQwen was released after refactor\n\n* no need for colqwen\n\n* tied weight keys are necessary, if using IMageTextToText\n\n* need to apply renaming in tied weights, only for ColPali\n\n* overwrite tied keys in ColPali\n\n* fix copies, modular can't handle if-statements",
    "sha": "9f41f67135b0656c428ff2c2b446d8eb15f5a7c5",
    "files": [
        {
            "sha": "3202ef47c19d859fb11639c3172c72de7d56b0a4",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f41f67135b0656c428ff2c2b446d8eb15f5a7c5/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f41f67135b0656c428ff2c2b446d8eb15f5a7c5/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=9f41f67135b0656c428ff2c2b446d8eb15f5a7c5",
            "patch": "@@ -231,6 +231,7 @@ def is_local_dist_rank_0():\n VLMS = [\n     \"aria\",\n     \"ayavision\",\n+    \"colpali\",\n     \"emu3\",\n     \"fuyu\",\n     \"gotocr2\","
        },
        {
            "sha": "1f38ea407a4dbd565e732d1126292c96e37d9094",
            "filename": "src/transformers/models/colpali/modeling_colpali.py",
            "status": "modified",
            "additions": 11,
            "deletions": 12,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f41f67135b0656c428ff2c2b446d8eb15f5a7c5/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f41f67135b0656c428ff2c2b446d8eb15f5a7c5/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py?ref=9f41f67135b0656c428ff2c2b446d8eb15f5a7c5",
            "patch": "@@ -97,15 +97,20 @@ class ColPaliForRetrievalOutput(ModelOutput):\n     \"\"\"\n )\n class ColPaliForRetrieval(ColPaliPreTrainedModel):\n+    _checkpoint_conversion_mapping = {\n+        \"vlm.language_model.model\": \"vlm.model.language_model\",\n+        \"vlm.vision_tower\": \"vlm.model.vision_tower\",\n+        \"vlm.multi_modal_projector\": \"vlm.model.multi_modal_projector\",\n+        \"vlm.language_model.lm_head\": \"vlm.lm_head\",\n+    }\n+\n     def __init__(self, config: ColPaliConfig):\n         super().__init__(config)\n         self.config = config\n         self.vocab_size = config.vlm_config.text_config.vocab_size\n \n-        vlm = AutoModelForImageTextToText.from_config(config.vlm_config)\n-        if vlm._tied_weights_keys is not None:\n-            self._tied_weights_keys = [f\"vlm.{k}\" for k in vlm._tied_weights_keys]\n-        self.vlm = vlm\n+        self.vlm = AutoModelForImageTextToText.from_config(config.vlm_config)\n+        self._tied_weights_keys = [f\"vlm.language_model.{k}\" for k in (self.vlm._tied_weights_keys or [])]\n \n         self.embedding_dim = self.config.embedding_dim\n         self.embedding_proj_layer = nn.Linear(\n@@ -136,7 +141,7 @@ def forward(\n         )\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        vlm_output = self.vlm(\n+        vlm_output = self.vlm.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             pixel_values=pixel_values,\n@@ -148,7 +153,7 @@ def forward(\n         vlm_hidden_states = vlm_output.hidden_states if output_hidden_states else None\n         vlm_image_hidden_states = vlm_output.image_hidden_states if pixel_values is not None else None\n \n-        last_hidden_states = vlm_output.hidden_states[-1]  # (batch_size, sequence_length, hidden_size)\n+        last_hidden_states = vlm_output[0]  # (batch_size, sequence_length, hidden_size)\n         embeddings = self.embedding_proj_layer(last_hidden_states)  # (batch_size, sequence_length, dim)\n \n         # L2 normalization\n@@ -177,12 +182,6 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.vlm.set_output_embeddings(new_embeddings)\n \n-    def set_decoder(self, decoder):\n-        self.vlm.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.vlm.get_decoder()\n-\n     def tie_weights(self):\n         return self.vlm.tie_weights()\n "
        },
        {
            "sha": "b0703f665e9b70ccbdba5f1f481656bfdfd42fb5",
            "filename": "src/transformers/models/colqwen2/modeling_colqwen2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 11,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f41f67135b0656c428ff2c2b446d8eb15f5a7c5/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f41f67135b0656c428ff2c2b446d8eb15f5a7c5/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py?ref=9f41f67135b0656c428ff2c2b446d8eb15f5a7c5",
            "patch": "@@ -104,21 +104,21 @@ class ColQwen2ForRetrievalOutput(ModelOutput):\n     \"\"\"\n )\n class ColQwen2ForRetrieval(ColQwen2PreTrainedModel):\n+    _checkpoint_conversion_mapping = {}\n+\n     def __init__(self, config: ColQwen2Config):\n         super().__init__(config)\n         self.config = config\n         self.vocab_size = config.vlm_config.text_config.vocab_size\n \n-        vlm = AutoModelForImageTextToText.from_config(config.vlm_config)\n-        if vlm._tied_weights_keys is not None:\n-            self._tied_weights_keys = [f\"vlm.{k}\" for k in vlm._tied_weights_keys]\n-        self.vlm = vlm\n+        self.vlm = AutoModelForImageTextToText.from_config(config.vlm_config)\n \n         self.embedding_dim = self.config.embedding_dim\n         self.embedding_proj_layer = nn.Linear(\n             self.config.vlm_config.text_config.hidden_size,\n             self.embedding_dim,\n         )\n+        self._tied_weights_keys = [f\"vlm.{k}\" for k in (self.vlm._tied_weights_keys or [])]\n \n         self.post_init()\n \n@@ -172,7 +172,7 @@ def forward(\n \n         # Custom data preparation to fix an issue with the gradient flow when training with multiple GPUs.\n         if inputs_embeds is None:\n-            inputs_embeds = self.vlm.model.language_model.embed_tokens(input_ids)\n+            inputs_embeds = self.vlm.language_model.embed_tokens(input_ids)\n \n             if pixel_values is not None:\n                 pixel_values = pixel_values.type(self.vlm.visual.get_dtype())\n@@ -228,12 +228,6 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.vlm.set_output_embeddings(new_embeddings)\n \n-    def set_decoder(self, decoder):\n-        self.vlm.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.vlm.get_decoder()\n-\n     def tie_weights(self):\n         return self.vlm.tie_weights()\n "
        },
        {
            "sha": "f63e865a7142a21d87532c462b3f21a9b9cf377b",
            "filename": "src/transformers/models/colqwen2/modular_colqwen2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f41f67135b0656c428ff2c2b446d8eb15f5a7c5/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f41f67135b0656c428ff2c2b446d8eb15f5a7c5/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py?ref=9f41f67135b0656c428ff2c2b446d8eb15f5a7c5",
            "patch": "@@ -25,6 +25,7 @@\n from ...processing_utils import ProcessingKwargs, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import ModelOutput, auto_docstring, can_return_tuple, is_torch_available, logging\n+from .configuration_colqwen2 import ColQwen2Config\n \n \n if is_torch_available():\n@@ -272,6 +273,13 @@ class ColQwen2ForRetrievalOutput(ModelOutput):\n     \"\"\"\n )\n class ColQwen2ForRetrieval(ColPaliForRetrieval):\n+    _checkpoint_conversion_mapping = {}\n+\n+    def __init__(self, config: ColQwen2Config):\n+        super().__init__(config)\n+        del self._tied_weights_keys\n+        self._tied_weights_keys = [f\"vlm.{k}\" for k in (self.vlm._tied_weights_keys or [])]\n+\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -322,7 +330,7 @@ def forward(\n \n         # Custom data preparation to fix an issue with the gradient flow when training with multiple GPUs.\n         if inputs_embeds is None:\n-            inputs_embeds = self.vlm.model.language_model.embed_tokens(input_ids)\n+            inputs_embeds = self.vlm.language_model.embed_tokens(input_ids)\n \n             if pixel_values is not None:\n                 pixel_values = pixel_values.type(self.vlm.visual.get_dtype())"
        },
        {
            "sha": "c6c9892d1737c99b9767e19a886324d92de46965",
            "filename": "tests/models/colpali/test_modeling_colpali.py",
            "status": "modified",
            "additions": 41,
            "deletions": 0,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f41f67135b0656c428ff2c2b446d8eb15f5a7c5/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f41f67135b0656c428ff2c2b446d8eb15f5a7c5/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py?ref=9f41f67135b0656c428ff2c2b446d8eb15f5a7c5",
            "patch": "@@ -13,7 +13,9 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch ColPali model.\"\"\"\n \n+import collections\n import gc\n+import re\n import unittest\n from typing import ClassVar\n \n@@ -40,6 +42,8 @@\n if is_torch_available():\n     import torch\n \n+    from transformers.pytorch_utils import id_tensor_storage\n+\n \n class ColPaliForRetrievalModelTester:\n     def __init__(\n@@ -206,6 +210,43 @@ def test_colpali_forward_inputs(self):\n \n             self.assertIsInstance(outputs, ColPaliForRetrievalOutput)\n \n+    # ColPali uses a VLM internally which has its state dict keys renames with `conversion_mapping`\n+    # This test is written assuming that `_tied_weights_keys` are not going to be renamed, thus we\n+    # overwrite it. NOTE: ColPali inference/save/load works without issues, it is the testcase\n+    # that makes general assumptions\n+    def test_tied_weights_keys(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.get_text_config().tie_word_embeddings = True\n+        for model_class in self.all_model_classes:\n+            model_tied = model_class(config)\n+\n+            ptrs = collections.defaultdict(list)\n+            for name, tensor in model_tied.state_dict().items():\n+                ptrs[id_tensor_storage(tensor)].append(name)\n+\n+            # These are all the pointers of shared tensors.\n+            tied_params = [names for _, names in ptrs.items() if len(names) > 1]\n+\n+            tied_weight_keys = model_tied._tied_weights_keys if model_tied._tied_weights_keys is not None else []\n+            # Detect we get a hit for each key\n+            for key in tied_weight_keys:\n+                key = key.replace(\".language_model\", \"\")  # remove 'language_model' prefix\n+                is_tied_key = any(re.search(key, p) for group in tied_params for p in group)\n+                self.assertTrue(is_tied_key, f\"{key} is not a tied weight key for {model_class}.\")\n+\n+            # Removed tied weights found from tied params -> there should only be one left after\n+            for key in tied_weight_keys:\n+                key = key.replace(\".language_model\", \"\")  # remove 'language_model' prefix\n+                for i in range(len(tied_params)):\n+                    tied_params[i] = [p for p in tied_params[i] if re.search(key, p) is None]\n+\n+            tied_params = [group for group in tied_params if len(group) > 1]\n+            self.assertListEqual(\n+                tied_params,\n+                [],\n+                f\"Missing `_tied_weights_keys` for {model_class}: add all of {tied_params} except one.\",\n+            )\n+\n     @unittest.skip(\n         reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )"
        }
    ],
    "stats": {
        "total": 91,
        "additions": 67,
        "deletions": 24
    }
}