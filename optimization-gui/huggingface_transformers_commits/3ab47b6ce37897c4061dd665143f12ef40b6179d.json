{
    "author": "zucchini-nlp",
    "message": "[VLMs] add helpers to get multimodal encodings (#37743)\n\n* add helpers in VLMs\n\n* fix tests and copies\n\n* fix blip tests\n\n* make fix-copies\n\n* fix copies\n\n* fixup",
    "sha": "3ab47b6ce37897c4061dd665143f12ef40b6179d",
    "files": [
        {
            "sha": "0d516c5f1c016d2d17a6dd6b344752e9e68e6125",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=3ab47b6ce37897c4061dd665143f12ef40b6179d",
            "patch": "@@ -1197,7 +1197,7 @@ def set_input_embeddings(self, value):\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n-        pixel_mask: torch.FloatTensor = None,\n+        pixel_mask: Optional[torch.FloatTensor] = None,\n         vision_feature_layer: int = -1,\n     ):\n         \"\"\"\n@@ -1208,13 +1208,16 @@ def get_image_features(\n                The tensors corresponding to the input images.\n             pixel_mask (`torch.FloatTensor]`, *optional*):\n                 The tensors corresponding to the input image mask.\n-            vision_feature_layer (`Union[int, List[int]]`):\n+            vision_feature_layer (`Union[int, List[int]]`, *optional*):\n                 The index of the layer to select the vision feature. If multiple indices are provided,\n                 the vision feature of the corresponding indices will be concatenated to form the\n                 vision features.\n         Returns:\n             image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).\n         \"\"\"\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n         patch_attention_mask = self._create_patch_attention_mask(pixel_mask)\n         image_outputs = self.vision_tower(\n             pixel_values, patch_attention_mask=patch_attention_mask, output_hidden_states=True"
        },
        {
            "sha": "738b269b0bb23dffffc82c54ba973237476c46d2",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=3ab47b6ce37897c4061dd665143f12ef40b6179d",
            "patch": "@@ -1325,7 +1325,7 @@ def _create_patch_attention_mask(self, pixel_mask):\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n-        pixel_mask: torch.FloatTensor = None,\n+        pixel_mask: Optional[torch.FloatTensor] = None,\n         vision_feature_layer: int = -1,\n     ):\n         \"\"\"\n@@ -1336,13 +1336,16 @@ def get_image_features(\n                The tensors corresponding to the input images.\n             pixel_mask (`torch.FloatTensor]`, *optional*):\n                 The tensors corresponding to the input image mask.\n-            vision_feature_layer (`Union[int, List[int]]`):\n+            vision_feature_layer (`Union[int, List[int]]`, *optional*):\n                 The index of the layer to select the vision feature. If multiple indices are provided,\n                 the vision feature of the corresponding indices will be concatenated to form the\n                 vision features.\n         Returns:\n             image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).\n         \"\"\"\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n         patch_attention_mask = self._create_patch_attention_mask(pixel_mask)\n         image_outputs = self.vision_tower(\n             pixel_values, patch_attention_mask=patch_attention_mask, output_hidden_states=True"
        },
        {
            "sha": "b7d7521ef691e19990fc3137030850427ffe8a88",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 13,
            "deletions": 4,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=3ab47b6ce37897c4061dd665143f12ef40b6179d",
            "patch": "@@ -213,8 +213,8 @@ def set_input_embeddings(self, value):\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n-        vision_feature_layer: Union[int, List[int]],\n-        vision_feature_select_strategy: str,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n         **kwargs,\n     ):\n         \"\"\"\n@@ -223,16 +223,25 @@ def get_image_features(\n         Args:\n             pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`):\n                The tensors corresponding to the input images.\n-            vision_feature_layer (`Union[int, List[int]]`):\n+            vision_feature_layer (`Union[int, List[int]]`, *optional*):\n                 The index of the layer to select the vision feature. If multiple indices are provided,\n                 the vision feature of the corresponding indices will be concatenated to form the\n                 vision features.\n-            vision_feature_select_strategy (`str`):\n+            vision_feature_select_strategy (`str`, *optional*):\n                 The feature selection strategy used to select the vision feature from the vision backbone.\n                 Can be one of `\"default\"` or `\"full\"`\n         Returns:\n             image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).\n         \"\"\"\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+        vision_feature_select_strategy = (\n+            vision_feature_select_strategy\n+            if vision_feature_select_strategy is not None\n+            else self.config.vision_feature_select_strategy\n+        )\n+\n         if vision_feature_select_strategy not in [\"default\", \"full\"]:\n             raise ValueError(f\"Unexpected select feature strategy: {self.config.vision_feature_select_strategy}\")\n "
        },
        {
            "sha": "3ca38af6addccd85afb99d0bd92387c95da5510f",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 48,
            "deletions": 30,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=3ab47b6ce37897c4061dd665143f12ef40b6179d",
            "patch": "@@ -1956,6 +1956,50 @@ def _preprocess_accelerate(self):\n         if hasattr(self.language_model, \"_hf_hook\"):\n             self.language_model._hf_hook.io_same_device = True  # For `generate` compatibility\n \n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        interpolate_pos_encoding: Optional[bool] = False,\n+        return_dict: Optional[bool] = False,\n+    ):\n+        \"\"\"\n+        Encodes images into continuous embeddings that can be forwarded to the language model.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                The tensors corresponding to the input images.\n+        \"\"\"\n+        # step 1: forward the images through the vision encoder,\n+        # to get image embeddings of shape (batch_size, seq_len, hidden_size)\n+        vision_outputs = self.vision_model(\n+            pixel_values=pixel_values,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n+            return_dict=True,\n+        )\n+        image_embeds = vision_outputs[0]\n+\n+        # step 2: forward the query tokens through the QFormer, using the image embeddings for cross-attention\n+        image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)\n+\n+        query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n+        query_outputs = self.qformer(\n+            query_embeds=query_tokens,\n+            encoder_hidden_states=image_embeds,\n+            encoder_attention_mask=image_attention_mask,\n+            return_dict=True,\n+        )\n+        query_output = query_outputs[0]\n+\n+        # Qformer is kept in fp32, we downcast the output back if needed\n+        if query_output.dtype != image_embeds.dtype:\n+            query_output = query_output.to(image_embeds.dtype)\n+\n+        # step 3: use the language model, conditioned on the query outputs and the prompt\n+        language_model_inputs = self.language_projection(query_output)\n+        if return_dict:\n+            return language_model_inputs, vision_outputs, query_outputs\n+        return language_model_inputs\n+\n     @auto_docstring\n     def forward(\n         self,\n@@ -2047,37 +2091,11 @@ def forward(\n         ```\"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        # step 1: forward the images through the vision encoder,\n-        # to get image embeddings of shape (batch_size, seq_len, hidden_size)\n-        vision_outputs = self.vision_model(\n-            pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-            interpolate_pos_encoding=interpolate_pos_encoding,\n+        language_model_inputs, vision_outputs, query_outputs = self.get_image_features(\n+            pixel_values, interpolate_pos_encoding=interpolate_pos_encoding, return_dict=True\n         )\n-        image_embeds = vision_outputs[0]\n-\n-        # step 2: forward the query tokens through the QFormer, using the image embeddings for cross-attention\n-        image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)\n-\n-        query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n-        query_outputs = self.qformer(\n-            query_embeds=query_tokens,\n-            encoder_hidden_states=image_embeds,\n-            encoder_attention_mask=image_attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-        query_output = query_outputs[0]\n-\n-        # Qformer is kept in fp32, we downcast the output back if needed\n-        if query_output.dtype != image_embeds.dtype:\n-            query_output = query_output.to(image_embeds.dtype)\n-\n-        # step 3: use the language model, conditioned on the query outputs and the prompt\n-        language_model_inputs = self.language_projection(query_output)\n+        vision_outputs = vision_outputs.to_tuple() if not return_dict else vision_outputs\n+        query_outputs = query_outputs.to_tuple() if not return_dict else query_outputs\n         language_model_attention_mask = torch.ones(\n             language_model_inputs.size()[:-1], dtype=torch.long, device=language_model_inputs.device\n         )"
        },
        {
            "sha": "8b5d2a46067a2220fac26c104e4d11908c2c4bf3",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=3ab47b6ce37897c4061dd665143f12ef40b6179d",
            "patch": "@@ -904,6 +904,12 @@ def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n     def get_image_tokens(self, pixel_values: torch.FloatTensor):\n+        logger.warning(\n+            \"`model.get_image_tokens()` is deprecated and will be removed in v4.58. To obtain discrete token use `model.get_image_features()`\"\n+        )\n+        return self.get_image_featues(pixel_values)\n+\n+    def get_image_features(self, pixel_values: torch.FloatTensor):\n         \"\"\"\n         Tokenizes images into discrete tokens with VQGAN module. Converts\n         obtained image tokens into BPE tokens and wraps with \"boi\" and \"eoi\"\n@@ -957,7 +963,7 @@ def forward(\n             )\n \n         if pixel_values is not None:\n-            image_tokens = self.get_image_tokens(pixel_values)\n+            image_tokens = self.get_image_features(pixel_values)\n             special_image_mask = input_ids == self.vocabulary_mapping.image_token_id\n             if not is_torchdynamo_compiling() and input_ids[special_image_mask].numel() != image_tokens.numel():\n                 n_image_tokens_in_text = (input_ids == self.vocabulary_mapping.image_token_id).sum()"
        },
        {
            "sha": "61ccecd6501bd12a1ddee705e8f94f43c6a6b13a",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=3ab47b6ce37897c4061dd665143f12ef40b6179d",
            "patch": "@@ -1586,6 +1586,12 @@ def set_input_embeddings(self, value):\n         self.text_model.set_input_embeddings(value)\n \n     def get_image_tokens(self, pixel_values: torch.FloatTensor, image_sizes: torch.LongTensor):\n+        logger.warning(\n+            \"`model.get_image_tokens()` is deprecated and will be removed in v4.58. To obtain discrete token use `model.get_image_features()`\"\n+        )\n+        return self.get_image_featues(pixel_values)\n+\n+    def get_image_features(self, pixel_values: torch.FloatTensor, image_sizes: torch.LongTensor):\n         \"\"\"\n         Tokenizes images into discrete tokens with VQGAN module. Converts\n         obtained image tokens into BPE tokens and wraps with \"boi\" and \"eoi\"\n@@ -1662,7 +1668,7 @@ def forward(\n             )\n \n         if pixel_values is not None:\n-            image_tokens = self.get_image_tokens(pixel_values, image_sizes)\n+            image_tokens = self.get_image_features(pixel_values, image_sizes)\n             special_image_mask = input_ids == self.vocabulary_mapping.image_token_id\n             image_tokens = image_tokens.to(input_ids.device, input_ids.dtype)\n             input_ids = input_ids.masked_scatter(special_image_mask, image_tokens)"
        },
        {
            "sha": "ade55f93a167db03cbd9db6bbe171322b8525b15",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=3ab47b6ce37897c4061dd665143f12ef40b6179d",
            "patch": "@@ -941,6 +941,12 @@ def set_input_embeddings(self, value):\n         self.text_model.set_input_embeddings(value)\n \n     def get_image_tokens(self, pixel_values: torch.FloatTensor, image_sizes: torch.LongTensor):\n+        logger.warning(\n+            \"`model.get_image_tokens()` is deprecated and will be removed in v4.58. To obtain discrete token use `model.get_image_features()`\"\n+        )\n+        return self.get_image_featues(pixel_values)\n+\n+    def get_image_features(self, pixel_values: torch.FloatTensor, image_sizes: torch.LongTensor):\n         \"\"\"\n         Tokenizes images into discrete tokens with VQGAN module. Converts\n         obtained image tokens into BPE tokens and wraps with \"boi\" and \"eoi\"\n@@ -1017,7 +1023,7 @@ def forward(\n             )\n \n         if pixel_values is not None:\n-            image_tokens = self.get_image_tokens(pixel_values, image_sizes)\n+            image_tokens = self.get_image_features(pixel_values, image_sizes)\n             special_image_mask = input_ids == self.vocabulary_mapping.image_token_id\n             image_tokens = image_tokens.to(input_ids.device, input_ids.dtype)\n             input_ids = input_ids.masked_scatter(special_image_mask, image_tokens)"
        },
        {
            "sha": "4f120afa1ba52e5f1a37d50b0dbc26b978123c57",
            "filename": "src/transformers/models/fuyu/modeling_fuyu.py",
            "status": "modified",
            "additions": 18,
            "deletions": 7,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py?ref=3ab47b6ce37897c4061dd665143f12ef40b6179d",
            "patch": "@@ -125,9 +125,25 @@ def gather_continuous_embeddings(\n                     f\"Number of continuous embeddings {continuous_embeddings[batch_idx].shape=} does not match \"\n                     f\"number of continuous token ids {src_indices.shape=} in batch element {batch_idx}.\"\n                 )\n-            output_embeddings[batch_idx, dst_indices] = continuous_embeddings[batch_idx][src_indices]\n+            output_embeddings[batch_idx, dst_indices] = continuous_embeddings[batch_idx][src_indices].to(\n+                output_embeddings.device\n+            )\n         return output_embeddings\n \n+    def get_image_features(self, pixel_values: torch.FloatTensor):\n+        \"\"\"\n+        Encodes images into continuous embeddings that can be forwarded to the language model.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                The tensors corresponding to the input images.\n+        \"\"\"\n+        patch_embeddings = [\n+            self.vision_embed_tokens(patch.to(self.vision_embed_tokens.weight.dtype)).squeeze(0)\n+            for patch in pixel_values\n+        ]\n+        return patch_embeddings\n+\n     @auto_docstring\n     def forward(\n         self,\n@@ -185,12 +201,7 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.language_model.get_input_embeddings()(input_ids)\n             if image_patches is not None and past_key_values is None:\n-                patch_embeddings = [\n-                    self.vision_embed_tokens(patch.to(self.vision_embed_tokens.weight.dtype))\n-                    .squeeze(0)\n-                    .to(inputs_embeds.device)\n-                    for patch in image_patches\n-                ]\n+                patch_embeddings = self.get_image_features(image_patches)\n                 inputs_embeds = self.gather_continuous_embeddings(\n                     word_embeddings=inputs_embeds,\n                     continuous_embeddings=patch_embeddings,"
        },
        {
            "sha": "f7008ad33e83e5231b8a9bc348300d2a837af7c6",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 48,
            "deletions": 42,
            "changes": 90,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=3ab47b6ce37897c4061dd665143f12ef40b6179d",
            "patch": "@@ -961,13 +961,57 @@ def inputs_merger(\n         - The merging happens so that we obtain the following sequence: `vector_tok_1 vector_tok_2 vector_tok_3 vector_fake_tok_around_image {sequence of image_seq_len image hidden states} vector_fake_toke_around_image vector_tok_4`. That sequence is fed to the LM.\n         - To fit the format of that sequence, `input_ids`, `input_embeds`, `attention_mask` are all 3 adapted to insert the image hidden states.\n         \"\"\"\n-        num_images, _, vision_hidden_size = image_hidden_states.shape\n         special_image_token_mask = input_ids == self.image_token_id\n         new_inputs_embeds = inputs_embeds.clone()\n-        reshaped_image_hidden_states = image_hidden_states.view(-1, vision_hidden_size)\n-        new_inputs_embeds[special_image_token_mask] = reshaped_image_hidden_states.to(new_inputs_embeds.device)\n+        new_inputs_embeds[special_image_token_mask] = image_hidden_states.to(new_inputs_embeds.device)\n         return new_inputs_embeds\n \n+    def get_image_features(self, pixel_values: torch.FloatTensor, pixel_attention_mask: torch.LongTensor = None):\n+        \"\"\"\n+        Encodes images into continuous embeddings that can be forwarded to the language model.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                The tensors corresponding to the input images.\n+            pixel_attention_mask (`torch.LongTensor`, *optional*):\n+                The attention mask indicating padded regions in the image.\n+        \"\"\"\n+        batch_size, num_images, num_channels, height, width = pixel_values.shape\n+        pixel_values = pixel_values.to(dtype=self.dtype)  # fp16 compatibility\n+        pixel_values = pixel_values.view(batch_size * num_images, *pixel_values.shape[2:])\n+\n+        # Remove padding images - padding images are full 0.\n+        nb_values_per_image = pixel_values.shape[1:].numel()\n+        real_images_inds = (pixel_values == 0.0).sum(dim=(-1, -2, -3)) != nb_values_per_image\n+        pixel_values = pixel_values[real_images_inds].contiguous()\n+\n+        # Handle the vision attention mask\n+        if pixel_attention_mask is None:\n+            pixel_attention_mask = torch.ones(\n+                size=(pixel_values.size(0), pixel_values.size(2), pixel_values.size(3)),\n+                dtype=torch.bool,\n+                device=pixel_values.device,\n+            )\n+        else:\n+            # Remove padding images from the mask/pP p\n+            pixel_attention_mask = pixel_attention_mask.view(batch_size * num_images, *pixel_attention_mask.shape[2:])\n+            pixel_attention_mask = pixel_attention_mask[real_images_inds].contiguous()\n+\n+        patch_size = self.config.vision_config.patch_size\n+        patches_subgrid = pixel_attention_mask.unfold(dimension=1, size=patch_size, step=patch_size)\n+        patches_subgrid = patches_subgrid.unfold(dimension=2, size=patch_size, step=patch_size)\n+        patch_attention_mask = (patches_subgrid.sum(dim=(-1, -2)) == patch_size * patch_size).bool()\n+        # Get sequence from the vision encoder\n+        image_hidden_states = self.vision_model(pixel_values=pixel_values, patch_attention_mask=patch_attention_mask)\n+        image_hidden_states = image_hidden_states.last_hidden_state\n+\n+        # Modality projection & resampling\n+        image_hidden_states = self.connector(\n+            image_hidden_states, attention_mask=patch_attention_mask.view(pixel_values.size(0), -1)\n+        )\n+        image_hidden_states = image_hidden_states.view(-1, image_hidden_states.shape[-1])\n+        return image_hidden_states\n+\n     @can_return_tuple\n     @auto_docstring(\n         custom_intro=\"\"\"\n@@ -1052,45 +1096,7 @@ def forward(\n         if pixel_values is not None and image_hidden_states is not None:\n             raise ValueError(\"You cannot specify both pixel_values and image_hidden_states at the same time\")\n         elif pixel_values is not None:\n-            batch_size, num_images, num_channels, height, width = pixel_values.shape\n-            pixel_values = pixel_values.to(dtype=self.dtype)  # fp16 compatibility\n-            pixel_values = pixel_values.view(batch_size * num_images, *pixel_values.shape[2:])\n-\n-            # Remove padding images - padding images are full 0.\n-            nb_values_per_image = pixel_values.shape[1:].numel()\n-            real_images_inds = (pixel_values == 0.0).sum(dim=(-1, -2, -3)) != nb_values_per_image\n-            pixel_values = pixel_values[real_images_inds].contiguous()\n-\n-            # Handle the vision attention mask\n-            if pixel_attention_mask is None:\n-                pixel_attention_mask = torch.ones(\n-                    size=(pixel_values.size(0), pixel_values.size(2), pixel_values.size(3)),\n-                    dtype=torch.bool,\n-                    device=pixel_values.device,\n-                )\n-            else:\n-                # Remove padding images from the mask/pP p\n-                pixel_attention_mask = pixel_attention_mask.view(\n-                    batch_size * num_images, *pixel_attention_mask.shape[2:]\n-                )\n-                pixel_attention_mask = pixel_attention_mask[real_images_inds].contiguous()\n-\n-            patch_size = self.config.vision_config.patch_size\n-            patches_subgrid = pixel_attention_mask.unfold(dimension=1, size=patch_size, step=patch_size)\n-            patches_subgrid = patches_subgrid.unfold(dimension=2, size=patch_size, step=patch_size)\n-            patch_attention_mask = (patches_subgrid.sum(dim=(-1, -2)) == patch_size * patch_size).bool()\n-\n-            # Get sequence from the vision encoder\n-            image_hidden_states = self.vision_model(\n-                pixel_values=pixel_values,\n-                patch_attention_mask=patch_attention_mask,\n-            ).last_hidden_state\n-\n-            # Modality projection & resampling\n-            image_hidden_states = self.connector(\n-                image_hidden_states, attention_mask=patch_attention_mask.view(pixel_values.size(0), -1)\n-            )\n-\n+            image_hidden_states = self.get_image_features(pixel_values, pixel_attention_mask)\n         elif image_hidden_states is not None:\n             image_hidden_states = image_hidden_states.to(dtype=self.dtype, device=input_ids.device)\n "
        },
        {
            "sha": "6bcdd3594e49917af3b967df44dc0ad7882814a9",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 48,
            "deletions": 41,
            "changes": 89,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=3ab47b6ce37897c4061dd665143f12ef40b6179d",
            "patch": "@@ -692,16 +692,59 @@ def inputs_merger(\n         - The merging happens so that we obtain the following sequence: `vector_tok_1 vector_tok_2 vector_tok_3 vector_fake_tok_around_image {sequence of image_seq_len image hidden states} vector_fake_toke_around_image vector_tok_4`. That sequence is fed to the LM.\n         - To fit the format of that sequence, `input_ids`, `input_embeds`, `attention_mask` are all 3 adapted to insert the image hidden states.\n         \"\"\"\n-        num_images, _, vision_hidden_size = image_hidden_states.shape\n         special_image_token_mask = input_ids == self.image_token_id\n         #  Fixes RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.\n         new_inputs_embeds = inputs_embeds.clone()\n-        reshaped_image_hidden_states = image_hidden_states.view(-1, vision_hidden_size)\n         # cast to the dtype of the input_embeds to support quantized models\n-        reshaped_image_hidden_states = reshaped_image_hidden_states.to(inputs_embeds.device, inputs_embeds.dtype)\n-        new_inputs_embeds[special_image_token_mask] = reshaped_image_hidden_states\n+        image_hidden_states = image_hidden_states.to(inputs_embeds.device, inputs_embeds.dtype)\n+        new_inputs_embeds[special_image_token_mask] = image_hidden_states\n         return new_inputs_embeds\n \n+    def get_image_features(self, pixel_values: torch.FloatTensor, pixel_attention_mask: torch.LongTensor = None):\n+        \"\"\"\n+        Encodes images into continuous embeddings that can be forwarded to the language model.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                The tensors corresponding to the input images.\n+            pixel_attention_mask (`torch.LongTensor`, *optional*):\n+                The attention mask indicating padded regions in the image.\n+        \"\"\"\n+        batch_size, num_images, num_channels, height, width = pixel_values.shape\n+        pixel_values = pixel_values.to(dtype=self.dtype)  # fp16 compatibility\n+        pixel_values = pixel_values.view(batch_size * num_images, *pixel_values.shape[2:])\n+\n+        # Remove padding images - padding images are full 0.\n+        nb_values_per_image = pixel_values.shape[1:].numel()\n+        real_images_inds = (pixel_values == 0.0).sum(dim=(-1, -2, -3)) != nb_values_per_image\n+        pixel_values = pixel_values[real_images_inds].contiguous()\n+\n+        # Handle the vision attention mask\n+        if pixel_attention_mask is None:\n+            pixel_attention_mask = torch.ones(\n+                size=(pixel_values.size(0), pixel_values.size(2), pixel_values.size(3)),\n+                dtype=torch.bool,\n+                device=pixel_values.device,\n+            )\n+        else:\n+            # Remove padding images from the mask\n+            pixel_attention_mask = pixel_attention_mask.view(batch_size * num_images, *pixel_attention_mask.shape[2:])\n+            pixel_attention_mask = pixel_attention_mask[real_images_inds].contiguous()\n+\n+        patch_size = self.config.vision_config.patch_size\n+        patches_subgrid = pixel_attention_mask.unfold(dimension=1, size=patch_size, step=patch_size)\n+        patches_subgrid = patches_subgrid.unfold(dimension=2, size=patch_size, step=patch_size)\n+        patch_attention_mask = (patches_subgrid.sum(dim=(-1, -2)) > 0).bool()\n+\n+        # Get sequence from the vision encoder\n+        image_hidden_states = self.vision_model(pixel_values=pixel_values, patch_attention_mask=patch_attention_mask)\n+        image_hidden_states.last_hidden_state\n+\n+        # Modality projection & resampling\n+        image_hidden_states = self.connector(image_hidden_states.last_hidden_state)\n+        image_hidden_states = image_hidden_states.view(-1, image_hidden_states.shape[-1])\n+        return image_hidden_states\n+\n     @can_return_tuple\n     @auto_docstring(\n         custom_intro=\"\"\"\n@@ -774,43 +817,7 @@ def forward(\n         if pixel_values is not None and image_hidden_states is not None:\n             raise ValueError(\"You cannot specify both pixel_values and image_hidden_states at the same time\")\n         elif pixel_values is not None:\n-            batch_size, num_images, num_channels, height, width = pixel_values.shape\n-            pixel_values = pixel_values.to(dtype=self.dtype)  # fp16 compatibility\n-            pixel_values = pixel_values.view(batch_size * num_images, *pixel_values.shape[2:])\n-\n-            # Remove padding images - padding images are full 0.\n-            nb_values_per_image = pixel_values.shape[1:].numel()\n-            real_images_inds = (pixel_values == 0.0).sum(dim=(-1, -2, -3)) != nb_values_per_image\n-            pixel_values = pixel_values[real_images_inds].contiguous()\n-\n-            # Handle the vision attention mask\n-            if pixel_attention_mask is None:\n-                pixel_attention_mask = torch.ones(\n-                    size=(pixel_values.size(0), pixel_values.size(2), pixel_values.size(3)),\n-                    dtype=torch.bool,\n-                    device=pixel_values.device,\n-                )\n-            else:\n-                # Remove padding images from the mask\n-                pixel_attention_mask = pixel_attention_mask.view(\n-                    batch_size * num_images, *pixel_attention_mask.shape[2:]\n-                )\n-                pixel_attention_mask = pixel_attention_mask[real_images_inds].contiguous()\n-\n-            patch_size = self.config.vision_config.patch_size\n-            patches_subgrid = pixel_attention_mask.unfold(dimension=1, size=patch_size, step=patch_size)\n-            patches_subgrid = patches_subgrid.unfold(dimension=2, size=patch_size, step=patch_size)\n-            patch_attention_mask = (patches_subgrid.sum(dim=(-1, -2)) > 0).bool()\n-\n-            # Get sequence from the vision encoder\n-            image_hidden_states = self.vision_model(\n-                pixel_values=pixel_values,\n-                patch_attention_mask=patch_attention_mask,\n-            ).last_hidden_state\n-\n-            # Modality projection & resampling\n-            image_hidden_states = self.connector(image_hidden_states)\n-\n+            image_hidden_states = self.get_image_features(pixel_values, pixel_attention_mask)\n         elif image_hidden_states is not None:\n             image_hidden_states = image_hidden_states.to(dtype=self.dtype, device=input_ids.device)\n "
        },
        {
            "sha": "c90d22f012da72bb1b6c4be5b02ad99c1bf11445",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 60,
            "deletions": 53,
            "changes": 113,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=3ab47b6ce37897c4061dd665143f12ef40b6179d",
            "patch": "@@ -1466,6 +1466,55 @@ def _preprocess_accelerate(self):\n         if hasattr(self.language_model, \"_hf_hook\"):\n             self.language_model._hf_hook.io_same_device = True  # For `generate` compatibility\n \n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        qformer_input_ids: torch.LongTensor,\n+        qformer_attention_mask: Optional[torch.LongTensor] = None,\n+        interpolate_pos_encoding: Optional[bool] = False,\n+        return_dict: Optional[bool] = False,\n+    ):\n+        \"\"\"\n+        Encodes images into continuous embeddings that can be forwarded to the language model.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                The tensors corresponding to the input images.\n+        \"\"\"\n+        # step 1: forward the images through the vision encoder,\n+        # to get image embeddings of shape (batch_size, seq_len, hidden_size)\n+        vision_outputs = self.vision_model(\n+            pixel_values=pixel_values,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n+            return_dict=True,\n+        )\n+        image_embeds = vision_outputs[0]\n+\n+        # step 2: forward the query tokens through the QFormer, using the image embeddings for cross-attention\n+        image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)\n+\n+        # difference with BLIP-2 here: we also feed the instruction prompt to the Q-Former\n+        query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n+        query_attention_mask = torch.ones(query_tokens.size()[:-1], dtype=torch.long, device=image_embeds.device)\n+        if qformer_attention_mask is None:\n+            qformer_attention_mask = torch.ones_like(qformer_input_ids)\n+        qformer_attention_mask = torch.cat([query_attention_mask, qformer_attention_mask], dim=1)\n+        query_outputs = self.qformer(\n+            input_ids=qformer_input_ids,\n+            attention_mask=qformer_attention_mask,\n+            query_embeds=query_tokens,\n+            encoder_hidden_states=image_embeds,\n+            encoder_attention_mask=image_attention_mask,\n+            return_dict=True,\n+        )\n+        query_output = query_outputs[0][:, : query_tokens.size(1), :]\n+\n+        # step 3: use the language model, conditioned on the query outputs and the prompt\n+        language_model_inputs = self.language_projection(query_output)\n+        if return_dict:\n+            return language_model_inputs, vision_outputs, query_outputs\n+        return language_model_inputs\n+\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -1555,40 +1604,15 @@ def forward(\n         ```\"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        # step 1: forward the images through the vision encoder,\n-        # to get image embeddings of shape (batch_size, seq_len, hidden_size)\n-        vision_outputs = self.vision_model(\n-            pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+        language_model_inputs, vision_outputs, query_outputs = self.get_image_features(\n+            pixel_values,\n+            qformer_input_ids=qformer_input_ids,\n+            qformer_attention_mask=qformer_attention_mask,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n+            return_dict=True,\n         )\n-        image_embeds = vision_outputs[0]\n-\n-        # step 2: forward the query tokens through the QFormer, using the image embeddings for cross-attention\n-        image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)\n-\n-        # difference with BLIP-2 here: we also feed the instruction prompt to the Q-Former\n-        query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n-        query_attention_mask = torch.ones(query_tokens.size()[:-1], dtype=torch.long, device=image_embeds.device)\n-        if qformer_attention_mask is None:\n-            qformer_attention_mask = torch.ones_like(qformer_input_ids)\n-        qformer_attention_mask = torch.cat([query_attention_mask, qformer_attention_mask], dim=1)\n-        query_outputs = self.qformer(\n-            input_ids=qformer_input_ids,\n-            attention_mask=qformer_attention_mask,\n-            query_embeds=query_tokens,\n-            encoder_hidden_states=image_embeds,\n-            encoder_attention_mask=image_attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-        query_output = query_outputs[0][:, : query_tokens.size(1), :]\n-\n-        # step 3: use the language model, conditioned on the query outputs and the prompt\n-        language_model_inputs = self.language_projection(query_output)\n+        vision_outputs = vision_outputs.to_tuple() if not return_dict else vision_outputs\n+        query_outputs = query_outputs.to_tuple() if not return_dict else query_outputs\n         language_model_attention_mask = torch.ones(\n             language_model_inputs.size()[:-1], dtype=torch.long, device=language_model_inputs.device\n         )\n@@ -1690,30 +1714,13 @@ def generate(\n             self._preprocess_accelerate()\n \n         batch_size = pixel_values.shape[0]\n-        image_embeds = self.vision_model(\n+        language_model_inputs, vision_outputs, query_outputs = self.get_image_features(\n             pixel_values,\n-            return_dict=True,\n+            qformer_input_ids=qformer_input_ids,\n+            qformer_attention_mask=qformer_attention_mask,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n-        ).last_hidden_state\n-\n-        image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)\n-\n-        query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n-        query_attention_mask = torch.ones(query_tokens.size()[:-1], dtype=torch.long, device=image_embeds.device)\n-        if qformer_attention_mask is None:\n-            qformer_attention_mask = torch.ones_like(qformer_input_ids)\n-        qformer_attention_mask = torch.cat([query_attention_mask, qformer_attention_mask], dim=1)\n-        query_outputs = self.qformer(\n-            input_ids=qformer_input_ids,\n-            attention_mask=qformer_attention_mask,\n-            query_embeds=query_tokens,\n-            encoder_hidden_states=image_embeds,\n-            encoder_attention_mask=image_attention_mask,\n             return_dict=True,\n         )\n-        query_output = query_outputs.last_hidden_state[:, : query_tokens.size(1), :]\n-\n-        language_model_inputs = self.language_projection(query_output)\n         language_attention_mask = torch.ones(\n             language_model_inputs.size()[:-1], dtype=torch.long, device=language_model_inputs.device\n         )\n@@ -1722,7 +1729,7 @@ def generate(\n             start_tokens = [self.config.text_config.bos_token_id]\n             if getattr(self.config, \"image_token_id\", None) is not None:\n                 start_tokens = [self.config.image_token_id] * self.config.num_query_tokens + start_tokens\n-            input_ids = torch.tensor([start_tokens], dtype=torch.long, device=image_embeds.device)\n+            input_ids = torch.tensor([start_tokens], dtype=torch.long, device=pixel_values.device)\n             input_ids = input_ids.repeat(batch_size, 1)\n \n         if attention_mask is None:"
        },
        {
            "sha": "b9f40deffef8d1ec9dd9827dbea38e51d23806fa",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 88,
            "deletions": 71,
            "changes": 159,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=3ab47b6ce37897c4061dd665143f12ef40b6179d",
            "patch": "@@ -1470,6 +1470,23 @@ def _preprocess_accelerate(self):\n         if hasattr(self.language_model, \"_hf_hook\"):\n             self.language_model._hf_hook.io_same_device = True  # For `generate` compatibility\n \n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        qformer_input_ids: torch.LongTensor,\n+        qformer_attention_mask: Optional[torch.LongTensor] = None,\n+        interpolate_pos_encoding: Optional[bool] = False,\n+        return_dict: Optional[bool] = False,\n+    ):\n+        \"\"\"\n+        Encodes images into continuous embeddings that can be forwarded to the language model.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                The tensors corresponding to the input images.\n+        \"\"\"\n+        pass\n+\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -1582,50 +1599,15 @@ def forward(\n         ```\"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        # step 1: forward the images through the vision encoder,\n-        # we process in a batched way, later unbatch it back (video has frames=4 always)\n-        batch_size, frames, channel, height, width = pixel_values.shape\n-        pixel_values = pixel_values.reshape(batch_size * frames, channel, height, width)\n-\n-        vision_outputs = self.vision_model(\n-            pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+        language_model_inputs, vision_outputs, query_outputs = self.get_video_features(\n+            pixel_values,\n+            qformer_input_ids=qformer_input_ids,\n+            qformer_attention_mask=qformer_attention_mask,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n+            return_dict=True,\n         )\n-        image_embeds = vision_outputs[0]\n-\n-        # step 2: forward the query tokens through the QFormer, using the image embeddings for cross-attention\n-        image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)\n-\n-        # difference with BLIP-2 here: we also feed the instruction prompt to the Q-Former\n-        query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n-        query_attention_mask = torch.ones(query_tokens.size()[:-1], dtype=torch.long, device=image_embeds.device)\n-\n-        if qformer_attention_mask is None:\n-            qformer_attention_mask = torch.ones_like(qformer_input_ids)\n-\n-        qformer_input_ids = qformer_input_ids.repeat_interleave(frames, dim=0)\n-        qformer_attention_mask = qformer_attention_mask.repeat_interleave(frames, dim=0)\n-        qformer_attention_mask = torch.cat([query_attention_mask, qformer_attention_mask], dim=1)\n-        query_outputs = self.qformer(\n-            input_ids=qformer_input_ids,\n-            attention_mask=qformer_attention_mask,\n-            query_embeds=query_tokens,\n-            encoder_hidden_states=image_embeds,\n-            encoder_attention_mask=image_attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-        query_output = query_outputs[0][:, : query_tokens.size(1), :]\n-\n-        # step 3: use the language model, conditioned on the query outputs and the prompt\n-        language_model_inputs = self.language_projection(query_output)\n-\n-        # unbatch inputs back, each video-frame gets `num_query_tokens` seq length\n-        language_model_inputs = language_model_inputs.reshape(batch_size, self.config.num_query_tokens * frames, -1)\n+        vision_outputs = vision_outputs.to_tuple() if not return_dict else vision_outputs\n+        query_outputs = query_outputs.to_tuple() if not return_dict else query_outputs\n         language_model_attention_mask = torch.ones(\n             language_model_inputs.size()[:-1], dtype=torch.long, device=language_model_inputs.device\n         )\n@@ -1726,39 +1708,15 @@ def generate(\n             # preprocess for `accelerate`\n             self._preprocess_accelerate()\n \n-        # we process in a batched way, later unbatch it back (video has frames=4)\n-        batch_size, frames, channel, height, width = pixel_values.shape\n-        pixel_values = pixel_values.reshape(batch_size * frames, channel, height, width)\n-\n-        image_embeds = self.vision_model(\n+        batch_size = pixel_values.shape[0]\n+        language_model_inputs, vision_outputs, query_outputs = self.get_video_features(\n             pixel_values,\n-            return_dict=True,\n+            qformer_input_ids=qformer_input_ids,\n+            qformer_attention_mask=qformer_attention_mask,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n-        ).last_hidden_state\n-        image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)\n-\n-        query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n-        query_attention_mask = torch.ones(query_tokens.size()[:-1], dtype=torch.long, device=image_embeds.device)\n-        if qformer_attention_mask is None:\n-            qformer_attention_mask = torch.ones_like(qformer_input_ids)\n-\n-        qformer_input_ids = qformer_input_ids.repeat_interleave(frames, dim=0)\n-        qformer_attention_mask = qformer_attention_mask.repeat_interleave(frames, dim=0)\n-        qformer_attention_mask = torch.cat([query_attention_mask, qformer_attention_mask], dim=1)\n-        query_outputs = self.qformer(\n-            input_ids=qformer_input_ids,\n-            attention_mask=qformer_attention_mask,\n-            query_embeds=query_tokens,\n-            encoder_hidden_states=image_embeds,\n-            encoder_attention_mask=image_attention_mask,\n             return_dict=True,\n         )\n-        query_output = query_outputs.last_hidden_state[:, : query_tokens.size(1), :]\n-\n-        language_model_inputs = self.language_projection(query_output)\n \n-        # unbatch the embeddings back by moving frames to seq-len\n-        language_model_inputs = language_model_inputs.reshape(batch_size, self.config.num_query_tokens * frames, -1)\n         language_attention_mask = torch.ones(\n             language_model_inputs.size()[:-1], dtype=torch.long, device=language_model_inputs.device\n         )\n@@ -1767,7 +1725,7 @@ def generate(\n             start_tokens = [self.config.text_config.bos_token_id]\n             if getattr(self.config, \"video_token_id\", None) is not None:\n                 start_tokens = [self.config.video_token_id] * self.config.num_query_tokens * 4 + start_tokens\n-            input_ids = torch.tensor([start_tokens], dtype=torch.long, device=image_embeds.device)\n+            input_ids = torch.tensor([start_tokens], dtype=torch.long, device=pixel_values.device)\n             input_ids = input_ids.repeat(batch_size, 1)\n \n         if attention_mask is None:\n@@ -1807,6 +1765,65 @@ def generate(\n \n         return outputs\n \n+    def get_video_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        qformer_input_ids: torch.LongTensor,\n+        qformer_attention_mask: Optional[torch.LongTensor] = None,\n+        interpolate_pos_encoding: Optional[bool] = False,\n+        return_dict: Optional[bool] = False,\n+    ):\n+        \"\"\"\n+        Encodes images into continuous embeddings that can be forwarded to the language model.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                The tensors corresponding to the input images.\n+        \"\"\"\n+        # step 1: forward the images through the vision encoder,\n+        # we process in a batched way, later unbatch it back (video has frames=4 always)\n+        batch_size, frames, channel, height, width = pixel_values.shape\n+        pixel_values = pixel_values.reshape(batch_size * frames, channel, height, width)\n+\n+        vision_outputs = self.vision_model(\n+            pixel_values=pixel_values,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n+            return_dict=True,\n+        )\n+        image_embeds = vision_outputs[0]\n+\n+        # step 2: forward the query tokens through the QFormer, using the image embeddings for cross-attention\n+        image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)\n+\n+        # difference with BLIP-2 here: we also feed the instruction prompt to the Q-Former\n+        query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n+        query_attention_mask = torch.ones(query_tokens.size()[:-1], dtype=torch.long, device=image_embeds.device)\n+\n+        if qformer_attention_mask is None:\n+            qformer_attention_mask = torch.ones_like(qformer_input_ids)\n+\n+        qformer_input_ids = qformer_input_ids.repeat_interleave(frames, dim=0)\n+        qformer_attention_mask = qformer_attention_mask.repeat_interleave(frames, dim=0)\n+        qformer_attention_mask = torch.cat([query_attention_mask, qformer_attention_mask], dim=1)\n+        query_outputs = self.qformer(\n+            input_ids=qformer_input_ids,\n+            attention_mask=qformer_attention_mask,\n+            query_embeds=query_tokens,\n+            encoder_hidden_states=image_embeds,\n+            encoder_attention_mask=image_attention_mask,\n+            return_dict=True,\n+        )\n+        query_output = query_outputs[0][:, : query_tokens.size(1), :]\n+\n+        # step 3: use the language model, conditioned on the query outputs and the prompt\n+        language_model_inputs = self.language_projection(query_output)\n+\n+        # unbatch inputs back, each video-frame gets `num_query_tokens` seq length\n+        language_model_inputs = language_model_inputs.reshape(batch_size, self.config.num_query_tokens * frames, -1)\n+        if return_dict:\n+            return language_model_inputs, vision_outputs, query_outputs\n+        return language_model_inputs\n+\n \n __all__ = [\n     \"InstructBlipVideoVisionModel\","
        },
        {
            "sha": "079abe579edbe63e031814657d5fef7e540e87ae",
            "filename": "src/transformers/models/instructblipvideo/modular_instructblipvideo.py",
            "status": "modified",
            "additions": 82,
            "deletions": 71,
            "changes": 153,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py?ref=3ab47b6ce37897c4061dd665143f12ef40b6179d",
            "patch": "@@ -295,6 +295,76 @@ def forward(\n \n \n class InstructBlipVideoForConditionalGeneration(InstructBlipForConditionalGeneration):\n+    def get_video_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        qformer_input_ids: torch.LongTensor,\n+        qformer_attention_mask: Optional[torch.LongTensor] = None,\n+        interpolate_pos_encoding: Optional[bool] = False,\n+        return_dict: Optional[bool] = False,\n+    ):\n+        \"\"\"\n+        Encodes images into continuous embeddings that can be forwarded to the language model.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                The tensors corresponding to the input images.\n+        \"\"\"\n+        # step 1: forward the images through the vision encoder,\n+        # we process in a batched way, later unbatch it back (video has frames=4 always)\n+        batch_size, frames, channel, height, width = pixel_values.shape\n+        pixel_values = pixel_values.reshape(batch_size * frames, channel, height, width)\n+\n+        vision_outputs = self.vision_model(\n+            pixel_values=pixel_values,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n+            return_dict=True,\n+        )\n+        image_embeds = vision_outputs[0]\n+\n+        # step 2: forward the query tokens through the QFormer, using the image embeddings for cross-attention\n+        image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)\n+\n+        # difference with BLIP-2 here: we also feed the instruction prompt to the Q-Former\n+        query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n+        query_attention_mask = torch.ones(query_tokens.size()[:-1], dtype=torch.long, device=image_embeds.device)\n+\n+        if qformer_attention_mask is None:\n+            qformer_attention_mask = torch.ones_like(qformer_input_ids)\n+\n+        qformer_input_ids = qformer_input_ids.repeat_interleave(frames, dim=0)\n+        qformer_attention_mask = qformer_attention_mask.repeat_interleave(frames, dim=0)\n+        qformer_attention_mask = torch.cat([query_attention_mask, qformer_attention_mask], dim=1)\n+        query_outputs = self.qformer(\n+            input_ids=qformer_input_ids,\n+            attention_mask=qformer_attention_mask,\n+            query_embeds=query_tokens,\n+            encoder_hidden_states=image_embeds,\n+            encoder_attention_mask=image_attention_mask,\n+            return_dict=True,\n+        )\n+        query_output = query_outputs[0][:, : query_tokens.size(1), :]\n+\n+        # step 3: use the language model, conditioned on the query outputs and the prompt\n+        language_model_inputs = self.language_projection(query_output)\n+\n+        # unbatch inputs back, each video-frame gets `num_query_tokens` seq length\n+        language_model_inputs = language_model_inputs.reshape(batch_size, self.config.num_query_tokens * frames, -1)\n+        if return_dict:\n+            return language_model_inputs, vision_outputs, query_outputs\n+        return language_model_inputs\n+\n+    # Model supports only videos\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        qformer_input_ids: torch.LongTensor,\n+        qformer_attention_mask: Optional[torch.LongTensor] = None,\n+        interpolate_pos_encoding: Optional[bool] = False,\n+        return_dict: Optional[bool] = False,\n+    ):\n+        pass\n+\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -370,50 +440,15 @@ def forward(\n         ```\"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        # step 1: forward the images through the vision encoder,\n-        # we process in a batched way, later unbatch it back (video has frames=4 always)\n-        batch_size, frames, channel, height, width = pixel_values.shape\n-        pixel_values = pixel_values.reshape(batch_size * frames, channel, height, width)\n-\n-        vision_outputs = self.vision_model(\n-            pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+        language_model_inputs, vision_outputs, query_outputs = self.get_video_features(\n+            pixel_values,\n+            qformer_input_ids=qformer_input_ids,\n+            qformer_attention_mask=qformer_attention_mask,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n+            return_dict=True,\n         )\n-        image_embeds = vision_outputs[0]\n-\n-        # step 2: forward the query tokens through the QFormer, using the image embeddings for cross-attention\n-        image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)\n-\n-        # difference with BLIP-2 here: we also feed the instruction prompt to the Q-Former\n-        query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n-        query_attention_mask = torch.ones(query_tokens.size()[:-1], dtype=torch.long, device=image_embeds.device)\n-\n-        if qformer_attention_mask is None:\n-            qformer_attention_mask = torch.ones_like(qformer_input_ids)\n-\n-        qformer_input_ids = qformer_input_ids.repeat_interleave(frames, dim=0)\n-        qformer_attention_mask = qformer_attention_mask.repeat_interleave(frames, dim=0)\n-        qformer_attention_mask = torch.cat([query_attention_mask, qformer_attention_mask], dim=1)\n-        query_outputs = self.qformer(\n-            input_ids=qformer_input_ids,\n-            attention_mask=qformer_attention_mask,\n-            query_embeds=query_tokens,\n-            encoder_hidden_states=image_embeds,\n-            encoder_attention_mask=image_attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-        query_output = query_outputs[0][:, : query_tokens.size(1), :]\n-\n-        # step 3: use the language model, conditioned on the query outputs and the prompt\n-        language_model_inputs = self.language_projection(query_output)\n-\n-        # unbatch inputs back, each video-frame gets `num_query_tokens` seq length\n-        language_model_inputs = language_model_inputs.reshape(batch_size, self.config.num_query_tokens * frames, -1)\n+        vision_outputs = vision_outputs.to_tuple() if not return_dict else vision_outputs\n+        query_outputs = query_outputs.to_tuple() if not return_dict else query_outputs\n         language_model_attention_mask = torch.ones(\n             language_model_inputs.size()[:-1], dtype=torch.long, device=language_model_inputs.device\n         )\n@@ -514,39 +549,15 @@ def generate(\n             # preprocess for `accelerate`\n             self._preprocess_accelerate()\n \n-        # we process in a batched way, later unbatch it back (video has frames=4)\n-        batch_size, frames, channel, height, width = pixel_values.shape\n-        pixel_values = pixel_values.reshape(batch_size * frames, channel, height, width)\n-\n-        image_embeds = self.vision_model(\n+        batch_size = pixel_values.shape[0]\n+        language_model_inputs, vision_outputs, query_outputs = self.get_video_features(\n             pixel_values,\n-            return_dict=True,\n+            qformer_input_ids=qformer_input_ids,\n+            qformer_attention_mask=qformer_attention_mask,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n-        ).last_hidden_state\n-        image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)\n-\n-        query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n-        query_attention_mask = torch.ones(query_tokens.size()[:-1], dtype=torch.long, device=image_embeds.device)\n-        if qformer_attention_mask is None:\n-            qformer_attention_mask = torch.ones_like(qformer_input_ids)\n-\n-        qformer_input_ids = qformer_input_ids.repeat_interleave(frames, dim=0)\n-        qformer_attention_mask = qformer_attention_mask.repeat_interleave(frames, dim=0)\n-        qformer_attention_mask = torch.cat([query_attention_mask, qformer_attention_mask], dim=1)\n-        query_outputs = self.qformer(\n-            input_ids=qformer_input_ids,\n-            attention_mask=qformer_attention_mask,\n-            query_embeds=query_tokens,\n-            encoder_hidden_states=image_embeds,\n-            encoder_attention_mask=image_attention_mask,\n             return_dict=True,\n         )\n-        query_output = query_outputs.last_hidden_state[:, : query_tokens.size(1), :]\n \n-        language_model_inputs = self.language_projection(query_output)\n-\n-        # unbatch the embeddings back by moving frames to seq-len\n-        language_model_inputs = language_model_inputs.reshape(batch_size, self.config.num_query_tokens * frames, -1)\n         language_attention_mask = torch.ones(\n             language_model_inputs.size()[:-1], dtype=torch.long, device=language_model_inputs.device\n         )\n@@ -555,7 +566,7 @@ def generate(\n             start_tokens = [self.config.text_config.bos_token_id]\n             if getattr(self.config, \"video_token_id\", None) is not None:\n                 start_tokens = [self.config.video_token_id] * self.config.num_query_tokens * 4 + start_tokens\n-            input_ids = torch.tensor([start_tokens], dtype=torch.long, device=image_embeds.device)\n+            input_ids = torch.tensor([start_tokens], dtype=torch.long, device=pixel_values.device)\n             input_ids = input_ids.repeat(batch_size, 1)\n \n         if attention_mask is None:"
        },
        {
            "sha": "d25f100a2348c2d4c2d8ee84ab5a2bdedc402e42",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 33,
            "deletions": 12,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=3ab47b6ce37897c4061dd665143f12ef40b6179d",
            "patch": "@@ -1625,6 +1625,37 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value):\n         self.text_model.model.embed_tokens = value\n \n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        return_attentions: Optional[bool] = False,\n+        interpolate_pos_encoding: Optional[bool] = False,\n+    ):\n+        \"\"\"\n+        Encodes images into continuous embeddings that can be forwarded to the language model.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                The tensors corresponding to the input images.\n+            return_attentions (`bool`, *optional*, defaults to `False`):\n+                Whether to return `projection_attentions` or not.\n+            interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):\n+                Whether to interpolate positional embeddings or not.\n+        \"\"\"\n+        vision_model_output = self.vision_model(\n+            pixel_values=pixel_values,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n+        )\n+        # The whole `last_hidden_state` through `post_layernorm` instead of just `pooled_output`.\n+        image_embeds = self.vision_model.model.post_layernorm(vision_model_output[0])\n+        # normalized features\n+        image_embeds = nn.functional.normalize(image_embeds, dim=-1)\n+        image_embeds, projection_attentions = self.image_to_text_projection(image_embeds)\n+\n+        if return_attentions:\n+            return image_embeds, projection_attentions\n+        return image_embeds\n+\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -1696,19 +1727,9 @@ def forward(\n         if image_embeds is None:\n             if pixel_values is None:\n                 raise ValueError(\"You have to specify either `pixel_values` or `image_embeds`.\")\n-\n-            vision_model_output = self.vision_model(\n-                pixel_values=pixel_values,\n-                output_attentions=output_attentions,\n-                output_hidden_states=output_hidden_states,\n-                interpolate_pos_encoding=interpolate_pos_encoding,\n-                return_dict=return_dict,\n+            image_embeds, projection_attentions = self.get_image_features(\n+                pixel_values, return_attentions=True, interpolate_pos_encoding=interpolate_pos_encoding\n             )\n-            # The whole `last_hidden_state` through `post_layernorm` instead of just `pooled_output`.\n-            image_embeds = self.vision_model.model.post_layernorm(vision_model_output[0])\n-            # normalized features\n-            image_embeds = nn.functional.normalize(image_embeds, dim=-1)\n-            image_embeds, projection_attentions = self.image_to_text_projection(image_embeds)\n \n         outputs = self.text_model(\n             input_ids=input_ids,"
        },
        {
            "sha": "b27624241ac1b56c2c6c657ca9532e7b005b3886",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 13,
            "deletions": 4,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=3ab47b6ce37897c4061dd665143f12ef40b6179d",
            "patch": "@@ -183,8 +183,8 @@ def set_input_embeddings(self, value):\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n-        vision_feature_layer: Union[int, List[int]],\n-        vision_feature_select_strategy: str,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n         **kwargs,\n     ):\n         \"\"\"\n@@ -193,16 +193,25 @@ def get_image_features(\n         Args:\n             pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`):\n                The tensors corresponding to the input images.\n-            vision_feature_layer (`Union[int, List[int]]`):\n+            vision_feature_layer (`Union[int, List[int]]`, *optional*):\n                 The index of the layer to select the vision feature. If multiple indices are provided,\n                 the vision feature of the corresponding indices will be concatenated to form the\n                 vision features.\n-            vision_feature_select_strategy (`str`):\n+            vision_feature_select_strategy (`str`, *optional*):\n                 The feature selection strategy used to select the vision feature from the vision backbone.\n                 Can be one of `\"default\"` or `\"full\"`\n         Returns:\n             image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).\n         \"\"\"\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+        vision_feature_select_strategy = (\n+            vision_feature_select_strategy\n+            if vision_feature_select_strategy is not None\n+            else self.config.vision_feature_select_strategy\n+        )\n+\n         if vision_feature_select_strategy not in [\"default\", \"full\"]:\n             raise ValueError(f\"Unexpected select feature strategy: {self.config.vision_feature_select_strategy}\")\n "
        },
        {
            "sha": "fa92bc622363e6b4dbfdff5cd710af26b8d24d4a",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 13,
            "deletions": 4,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=3ab47b6ce37897c4061dd665143f12ef40b6179d",
            "patch": "@@ -365,8 +365,8 @@ def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n         image_sizes: torch.Tensor,\n-        vision_feature_layer: Union[int, List[int]],\n-        vision_feature_select_strategy: str,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n     ):\n         \"\"\"\n         Obtains image last hidden states from the vision tower and apply multimodal projection.\n@@ -376,17 +376,26 @@ def get_image_features(\n                The tensors corresponding to the input images.\n             image_sizes (`torch.Tensor` of shape `(num_images, 2)`)\n                 Actual image size of each images (H, W).\n-            vision_feature_layer (`Union[int, List[int]]`):\n+            vision_feature_layer (`Union[int, List[int]]`, *optional*):\n                 The index of the layer to select the vision feature. If multiple indices are provided,\n                 the vision feature of the corresponding indices will be concatenated to form the\n                 vision features.\n-            vision_feature_select_strategy (`str`):\n+            vision_feature_select_strategy (`str`, *optional*):\n                 The feature selection strategy used to select the vision feature from the vision backbone.\n                 Can be one of `\"default\"` or `\"full\"`\n         Returns:\n             image_features (List[`torch.Tensor`]): List of image feature tensor, each contains all the visual feature of all patches\n             and are of shape `(num_patches, image_length, embed_dim)`).\n         \"\"\"\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+        vision_feature_select_strategy = (\n+            vision_feature_select_strategy\n+            if vision_feature_select_strategy is not None\n+            else self.config.vision_feature_select_strategy\n+        )\n+\n         # ! infer image_num_patches from image_sizes\n         image_num_patches = [\n             image_size_to_num_patches("
        },
        {
            "sha": "3cb81ada8ac1b2c322d4269e1b82060b1f047dd8",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 26,
            "deletions": 8,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=3ab47b6ce37897c4061dd665143f12ef40b6179d",
            "patch": "@@ -419,8 +419,8 @@ def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n         image_sizes: torch.Tensor,\n-        vision_feature_layer: Union[int, List[int]],\n-        vision_feature_select_strategy: str,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n     ):\n         \"\"\"\n         Obtains image last hidden states from the vision tower and apply multimodal projection.\n@@ -430,17 +430,26 @@ def get_image_features(\n                The tensors corresponding to the input images.\n             image_sizes (`torch.Tensor` of shape `(num_images, 2)`)\n                 Actual image size of each images (H, W).\n-            vision_feature_layer (`Union[int, List[int]]`):\n+            vision_feature_layer (`Union[int, List[int]]`, *optional*):\n                 The index of the layer to select the vision feature. If multiple indices are provided,\n                 the vision feature of the corresponding indices will be concatenated to form the\n                 vision features.\n-            vision_feature_select_strategy (`str`):\n+            vision_feature_select_strategy (`str`, *optional*):\n                 The feature selection strategy used to select the vision feature from the vision backbone.\n                 Can be one of `\"default\"` or `\"full\"`\n         Returns:\n             image_features (List[`torch.Tensor`]): List of image feature tensor, each contains all the visual feature of all patches\n             and are of shape `(num_patches, image_length, embed_dim)`).\n         \"\"\"\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+        vision_feature_select_strategy = (\n+            vision_feature_select_strategy\n+            if vision_feature_select_strategy is not None\n+            else self.config.vision_feature_select_strategy\n+        )\n+\n         # ! infer image_num_patches from image_sizes\n         image_num_patches = [\n             image_size_to_num_patches(\n@@ -600,26 +609,35 @@ def forward(\n     def get_video_features(\n         self,\n         pixel_values: torch.FloatTensor,\n-        vision_feature_layer: Union[int, List[int]],\n-        vision_feature_select_strategy: str,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n     ):\n         \"\"\"\n         Obtains video last hidden states from the vision tower and apply multimodal projection.\n \n         Args:\n             pixel_values (`torch.FloatTensor]` of shape `(batch_size, num_frames, channels, height, width)`)\n                The tensors corresponding to the input video.\n-            vision_feature_layer (`Union[int, List[int]]`):\n+            vision_feature_layer (`Union[int, List[int]]`, *optiona;*):\n                 The index of the layer to select the vision feature. If multiple indices are provided,\n                 the vision feature of the corresponding indices will be concatenated to form the\n                 vision features.\n-            vision_feature_select_strategy (`str`):\n+            vision_feature_select_strategy (`str`, *optional*):\n                 The feature selection strategy used to select the vision feature from the vision backbone.\n                 Can be one of `\"default\"` or `\"full\"`\n         Returns:\n             video_features (List[`torch.Tensor`]): List of video feature tensor, each contains all the visual feature of all patches\n             and are of shape `(num_videos, video_length, embed_dim)`).\n         \"\"\"\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+        vision_feature_select_strategy = (\n+            vision_feature_select_strategy\n+            if vision_feature_select_strategy is not None\n+            else self.config.vision_feature_select_strategy\n+        )\n+\n         batch_size, frames, channels, height, width = pixel_values.shape\n         pixel_values = pixel_values.reshape(batch_size * frames, channels, height, width)\n         video_features = self.vision_tower(pixel_values, output_hidden_states=True)"
        },
        {
            "sha": "91d16caab6bf1aa3c93863166a0eec9665f56db0",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 26,
            "deletions": 8,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=3ab47b6ce37897c4061dd665143f12ef40b6179d",
            "patch": "@@ -252,8 +252,8 @@ def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n         image_sizes: torch.Tensor,\n-        vision_feature_layer: Union[int, List[int]],\n-        vision_feature_select_strategy: str,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n     ):\n         \"\"\"\n         Obtains image last hidden states from the vision tower and apply multimodal projection.\n@@ -263,17 +263,26 @@ def get_image_features(\n                The tensors corresponding to the input images.\n             image_sizes (`torch.Tensor` of shape `(num_images, 2)`)\n                 Actual image size of each images (H, W).\n-            vision_feature_layer (`Union[int, List[int]]`):\n+            vision_feature_layer (`Union[int, List[int]]`, *optional*):\n                 The index of the layer to select the vision feature. If multiple indices are provided,\n                 the vision feature of the corresponding indices will be concatenated to form the\n                 vision features.\n-            vision_feature_select_strategy (`str`):\n+            vision_feature_select_strategy (`str`, *optional*):\n                 The feature selection strategy used to select the vision feature from the vision backbone.\n                 Can be one of `\"default\"` or `\"full\"`\n         Returns:\n             image_features (List[`torch.Tensor`]): List of image feature tensor, each contains all the visual feature of all patches\n             and are of shape `(num_patches, image_length, embed_dim)`).\n         \"\"\"\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+        vision_feature_select_strategy = (\n+            vision_feature_select_strategy\n+            if vision_feature_select_strategy is not None\n+            else self.config.vision_feature_select_strategy\n+        )\n+\n         # ! infer image_num_patches from image_sizes\n         image_num_patches = [\n             image_size_to_num_patches(\n@@ -311,26 +320,35 @@ def get_image_features(\n     def get_video_features(\n         self,\n         pixel_values: torch.FloatTensor,\n-        vision_feature_layer: Union[int, List[int]],\n-        vision_feature_select_strategy: str,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n     ):\n         \"\"\"\n         Obtains video last hidden states from the vision tower and apply multimodal projection.\n \n         Args:\n             pixel_values (`torch.FloatTensor]` of shape `(batch_size, num_frames, channels, height, width)`)\n                The tensors corresponding to the input video.\n-            vision_feature_layer (`Union[int, List[int]]`):\n+            vision_feature_layer (`Union[int, List[int]]`, *optiona;*):\n                 The index of the layer to select the vision feature. If multiple indices are provided,\n                 the vision feature of the corresponding indices will be concatenated to form the\n                 vision features.\n-            vision_feature_select_strategy (`str`):\n+            vision_feature_select_strategy (`str`, *optional*):\n                 The feature selection strategy used to select the vision feature from the vision backbone.\n                 Can be one of `\"default\"` or `\"full\"`\n         Returns:\n             video_features (List[`torch.Tensor`]): List of video feature tensor, each contains all the visual feature of all patches\n             and are of shape `(num_videos, video_length, embed_dim)`).\n         \"\"\"\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+        vision_feature_select_strategy = (\n+            vision_feature_select_strategy\n+            if vision_feature_select_strategy is not None\n+            else self.config.vision_feature_select_strategy\n+        )\n+\n         batch_size, frames, channels, height, width = pixel_values.shape\n         pixel_values = pixel_values.reshape(batch_size * frames, channels, height, width)\n         video_features = self.vision_tower(pixel_values, output_hidden_states=True)"
        },
        {
            "sha": "be600e8a96e6a32e8422e8324ee48ccfc237cfde",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 13,
            "deletions": 4,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=3ab47b6ce37897c4061dd665143f12ef40b6179d",
            "patch": "@@ -419,8 +419,8 @@ def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n         image_sizes: torch.Tensor,\n-        vision_feature_layer: Union[int, List[int]],\n-        vision_feature_select_strategy: str,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n     ):\n         \"\"\"\n         Obtains image last hidden states from the vision tower and apply multimodal projection.\n@@ -430,17 +430,26 @@ def get_image_features(\n                The tensors corresponding to the input images.\n             image_sizes (`torch.Tensor` of shape `(num_images, 2)`)\n                 Actual image size of each images (H, W).\n-            vision_feature_layer (`Union[int, List[int]]`):\n+            vision_feature_layer (`Union[int, List[int]]`, *optional*):\n                 The index of the layer to select the vision feature. If multiple indices are provided,\n                 the vision feature of the corresponding indices will be concatenated to form the\n                 vision features.\n-            vision_feature_select_strategy (`str`):\n+            vision_feature_select_strategy (`str`, *optional*):\n                 The feature selection strategy used to select the vision feature from the vision backbone.\n                 Can be one of `\"default\"` or `\"full\"`\n         Returns:\n             image_features (List[`torch.Tensor`]): List of image feature tensor, each contains all the visual feature of all patches\n             and are of shape `(num_patches, image_length, embed_dim)`).\n         \"\"\"\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+        vision_feature_select_strategy = (\n+            vision_feature_select_strategy\n+            if vision_feature_select_strategy is not None\n+            else self.config.vision_feature_select_strategy\n+        )\n+\n         # ! infer image_num_patches from image_sizes\n         image_num_patches = [\n             image_size_to_num_patches("
        },
        {
            "sha": "5f0c960476037a01d148f208d21b43ab0db7653d",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=3ab47b6ce37897c4061dd665143f12ef40b6179d",
            "patch": "@@ -255,8 +255,8 @@ def set_input_embeddings(self, value):\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n-        vision_feature_layer: Union[int, List[int]],\n         image_sizes: torch.Tensor,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n         **kwargs,\n     ):\n         \"\"\"\n@@ -265,15 +265,19 @@ def get_image_features(\n         Args:\n             pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`):\n                The tensors corresponding to the input images.\n-            vision_feature_layer (`Union[int, List[int]]`):\n+            vision_feature_layer (`Union[int, List[int]]`, *optional*):\n                 The index of the layer to select the vision feature. If multiple indices are provided,\n                 the vision feature of the corresponding indices will be concatenated to form the\n                 vision features.\n-            image_sizes (`torch.Tensor`):\n+            image_sizes (`torch.Tensor`, *optional*):\n                 Tensor containing the image sizes as returned by the processor.\n         Returns:\n             image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).\n         \"\"\"\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+\n         kwargs = {k: v for k, v in kwargs.items() if v is not None}\n         # this is not memory efficient at all (output_hidden_states=True) will save all the hidden states.\n         image_outputs = self.vision_tower(pixel_values, image_sizes=image_sizes, output_hidden_states=True, **kwargs)"
        },
        {
            "sha": "a3b5fa5ecab582edf6c308c3c47832b3d5cdf9b9",
            "filename": "src/transformers/models/mistral3/modular_mistral3.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py?ref=3ab47b6ce37897c4061dd665143f12ef40b6179d",
            "patch": "@@ -135,8 +135,8 @@ class Mistral3Model(LlavaModel):\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n-        vision_feature_layer: Union[int, List[int]],\n         image_sizes: torch.Tensor,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n         **kwargs,\n     ):\n         \"\"\"\n@@ -145,15 +145,19 @@ def get_image_features(\n         Args:\n             pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`):\n                The tensors corresponding to the input images.\n-            vision_feature_layer (`Union[int, List[int]]`):\n+            vision_feature_layer (`Union[int, List[int]]`, *optional*):\n                 The index of the layer to select the vision feature. If multiple indices are provided,\n                 the vision feature of the corresponding indices will be concatenated to form the\n                 vision features.\n-            image_sizes (`torch.Tensor`):\n+            image_sizes (`torch.Tensor`, *optional*):\n                 Tensor containing the image sizes as returned by the processor.\n         Returns:\n             image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).\n         \"\"\"\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+\n         kwargs = {k: v for k, v in kwargs.items() if v is not None}\n         # this is not memory efficient at all (output_hidden_states=True) will save all the hidden states.\n         image_outputs = self.vision_tower(pixel_values, image_sizes=image_sizes, output_hidden_states=True, **kwargs)"
        },
        {
            "sha": "ce785d7a7b5446b1e8b8ff5c680a4b1fcdc213e8",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 105,
            "deletions": 47,
            "changes": 152,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=3ab47b6ce37897c4061dd665143f12ef40b6179d",
            "patch": "@@ -2187,6 +2187,75 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.set_input_embeddings(value)\n \n+    def get_video_features(\n+        self, pixel_values_videos: torch.FloatTensor, video_grid_thw: Optional[torch.LongTensor] = None\n+    ):\n+        \"\"\"\n+        Encodes videos into continuous embeddings that can be forwarded to the language model.\n+\n+        Args:\n+            pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                The tensors corresponding to the input videos.\n+            video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):\n+                The temporal, height and width of feature shape of each video in LLM.\n+        \"\"\"\n+        pixel_values_videos = pixel_values_videos.type(self.visual.dtype)\n+        video_embeds = self.visual(pixel_values_videos, grid_thw=video_grid_thw)\n+        return video_embeds\n+\n+    def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = None):\n+        \"\"\"\n+        Encodes images into continuous embeddings that can be forwarded to the language model.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                The tensors corresponding to the input images.\n+            image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n+                The temporal, height and width of feature shape of each image in LLM.\n+        \"\"\"\n+        pixel_values = pixel_values.type(self.visual.dtype)\n+        image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n+        return image_embeds\n+\n+    def get_audio_features(\n+        self,\n+        input_features: torch.FloatTensor,\n+        feature_attention_mask: Optional[torch.LongTensor] = None,\n+        audio_feature_lengths: Optional[torch.LongTensor] = None,\n+    ):\n+        \"\"\"\n+        Encodes audios into continuous embeddings that can be forwarded to the language model.\n+\n+        Args:\n+            input_features (`torch.FloatTensor`):\n+                The tensors corresponding to the input audios.\n+            feature_attention_mask (`torch.LongTensor`, *optional*):\n+                Mask to avoid performing attention on padding feature indices. Mask values selected in `[0, 1]`:\n+            audio_feature_lengths (`torch.LongTensor` of shape `(num_audios)`, *optional*):\n+                The length of feature shape of each audio in LLM.\n+        \"\"\"\n+        if feature_attention_mask is not None:\n+            audio_feature_lengths = torch.sum(feature_attention_mask, dim=1)\n+            input_features = input_features.permute(0, 2, 1)[feature_attention_mask.bool()].permute(1, 0)\n+        else:\n+            audio_feature_lengths = None\n+\n+        audio_feat_lengths, audio_output_lengths = self.audio_tower._get_feat_extract_output_lengths(\n+            audio_feature_lengths if audio_feature_lengths is not None else feature_attention_mask.sum(-1)\n+        )\n+        feature_lens = audio_feature_lengths if audio_feature_lengths is not None else feature_attention_mask.sum(-1)\n+        audio_outputs = self.audio_tower(\n+            input_features,\n+            feature_lens=feature_lens,\n+            aftercnn_lens=audio_feat_lengths,\n+        )\n+        audio_features = audio_outputs.last_hidden_state\n+\n+        if audio_features.shape[0] != sum(audio_output_lengths.tolist()):\n+            raise ValueError(\"length of audio_features should match audio_output_lengths\")\n+\n+        return audio_features\n+\n     @auto_docstring\n     def forward(\n         self,\n@@ -2284,58 +2353,18 @@ def forward(\n         )\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        if feature_attention_mask is not None:\n-            audio_feature_lengths = torch.sum(feature_attention_mask, dim=1)\n-            input_features = input_features.permute(0, 2, 1)[feature_attention_mask.bool()].permute(1, 0)\n-        else:\n-            audio_feature_lengths = None\n-        if attention_mask is not None and position_ids is None:\n-            if (\n-                cache_position is None\n-                or (cache_position is not None and cache_position[0] == 0)\n-                or self.rope_deltas is None\n-            ):\n-                delta0 = (1 - attention_mask).sum(dim=-1).unsqueeze(1)\n-                position_ids, rope_deltas = self.get_rope_index(\n-                    input_ids,\n-                    image_grid_thw,\n-                    video_grid_thw,\n-                    attention_mask,\n-                    use_audio_in_video,\n-                    audio_feature_lengths,\n-                    video_second_per_grid,\n-                )\n-                rope_deltas = rope_deltas - delta0\n-                self.rope_deltas = rope_deltas\n-            else:\n-                batch_size, seq_length = input_ids.shape\n-                delta = cache_position[0] + self.rope_deltas if cache_position is not None else 0\n-                position_ids = torch.arange(seq_length, device=input_ids.device)\n-                position_ids = position_ids.view(1, -1).expand(batch_size, -1)\n-                position_ids = position_ids.add(delta)\n-                position_ids = position_ids.unsqueeze(0).expand(3, -1, -1)\n-\n         if inputs_embeds is None:\n             # 1. Extract the input embeddings\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n         # 2. Merge text , audios , image and video\n         if input_ids is not None and input_ids.shape[1] != 1:  # Prefill stage\n             if input_features is not None:\n-                audio_feat_lengths, audio_output_lengths = self.audio_tower._get_feat_extract_output_lengths(\n-                    audio_feature_lengths if audio_feature_lengths is not None else feature_attention_mask.sum(-1)\n-                )\n-                feature_lens = (\n-                    audio_feature_lengths if audio_feature_lengths is not None else feature_attention_mask.sum(-1)\n-                )\n-                audio_outputs = self.audio_tower(\n+                audio_features = self.get_audio_features(\n                     input_features,\n-                    feature_lens=feature_lens,\n-                    aftercnn_lens=audio_feat_lengths,\n+                    feature_attention_mask=feature_attention_mask,\n+                    audio_feature_lengths=audio_feature_lengths,\n                 )\n-                audio_features = audio_outputs.last_hidden_state\n-                if audio_features.shape[0] != sum(audio_output_lengths.tolist()):\n-                    raise ValueError(\"length of audio_features should match audio_output_lengths\")\n                 audio_mask = (\n                     (input_ids == self.config.audio_token_id)\n                     .unsqueeze(-1)\n@@ -2346,8 +2375,7 @@ def forward(\n                 inputs_embeds = inputs_embeds.masked_scatter(audio_mask, audio_features)\n \n             if pixel_values is not None:\n-                pixel_values = pixel_values.type(self.visual.dtype)\n-                image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n+                image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n                 image_mask = (\n                     (input_ids == self.config.image_token_id)\n                     .unsqueeze(-1)\n@@ -2358,8 +2386,7 @@ def forward(\n                 inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n \n             if pixel_values_videos is not None:\n-                pixel_values_videos = pixel_values_videos.type(self.visual.dtype)\n-                video_embeds = self.visual(pixel_values_videos, grid_thw=video_grid_thw)\n+                video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)\n                 video_mask = (\n                     (input_ids == self.config.video_token_id)\n                     .unsqueeze(-1)\n@@ -2372,6 +2399,37 @@ def forward(\n             if attention_mask is not None:\n                 attention_mask = attention_mask.to(inputs_embeds.device)\n \n+        if feature_attention_mask is not None:\n+            audio_feature_lengths = torch.sum(feature_attention_mask, dim=1)\n+        else:\n+            audio_feature_lengths = None\n+\n+        if attention_mask is not None and position_ids is None:\n+            if (\n+                cache_position is None\n+                or (cache_position is not None and cache_position[0] == 0)\n+                or self.rope_deltas is None\n+            ):\n+                delta0 = (1 - attention_mask).sum(dim=-1).unsqueeze(1)\n+                position_ids, rope_deltas = self.get_rope_index(\n+                    input_ids,\n+                    image_grid_thw,\n+                    video_grid_thw,\n+                    attention_mask,\n+                    use_audio_in_video,\n+                    audio_feature_lengths,\n+                    video_second_per_grid,\n+                )\n+                rope_deltas = rope_deltas - delta0\n+                self.rope_deltas = rope_deltas\n+            else:\n+                batch_size, seq_length = input_ids.shape\n+                delta = cache_position[0] + self.rope_deltas if cache_position is not None else 0\n+                position_ids = torch.arange(seq_length, device=input_ids.device)\n+                position_ids = position_ids.view(1, -1).expand(batch_size, -1)\n+                position_ids = position_ids.add(delta)\n+                position_ids = position_ids.unsqueeze(0).expand(3, -1, -1)\n+\n         outputs = self.model(\n             attention_mask=attention_mask,\n             position_ids=position_ids,"
        },
        {
            "sha": "3d55bf10f40d2f324da59fa338f8b0657d495bea",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 105,
            "deletions": 47,
            "changes": 152,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=3ab47b6ce37897c4061dd665143f12ef40b6179d",
            "patch": "@@ -2181,6 +2181,75 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.set_input_embeddings(value)\n \n+    def get_video_features(\n+        self, pixel_values_videos: torch.FloatTensor, video_grid_thw: Optional[torch.LongTensor] = None\n+    ):\n+        \"\"\"\n+        Encodes videos into continuous embeddings that can be forwarded to the language model.\n+\n+        Args:\n+            pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                The tensors corresponding to the input videos.\n+            video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):\n+                The temporal, height and width of feature shape of each video in LLM.\n+        \"\"\"\n+        pixel_values_videos = pixel_values_videos.type(self.visual.dtype)\n+        video_embeds = self.visual(pixel_values_videos, grid_thw=video_grid_thw)\n+        return video_embeds\n+\n+    def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = None):\n+        \"\"\"\n+        Encodes images into continuous embeddings that can be forwarded to the language model.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                The tensors corresponding to the input images.\n+            image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n+                The temporal, height and width of feature shape of each image in LLM.\n+        \"\"\"\n+        pixel_values = pixel_values.type(self.visual.dtype)\n+        image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n+        return image_embeds\n+\n+    def get_audio_features(\n+        self,\n+        input_features: torch.FloatTensor,\n+        feature_attention_mask: Optional[torch.LongTensor] = None,\n+        audio_feature_lengths: Optional[torch.LongTensor] = None,\n+    ):\n+        \"\"\"\n+        Encodes audios into continuous embeddings that can be forwarded to the language model.\n+\n+        Args:\n+            input_features (`torch.FloatTensor`):\n+                The tensors corresponding to the input audios.\n+            feature_attention_mask (`torch.LongTensor`, *optional*):\n+                Mask to avoid performing attention on padding feature indices. Mask values selected in `[0, 1]`:\n+            audio_feature_lengths (`torch.LongTensor` of shape `(num_audios)`, *optional*):\n+                The length of feature shape of each audio in LLM.\n+        \"\"\"\n+        if feature_attention_mask is not None:\n+            audio_feature_lengths = torch.sum(feature_attention_mask, dim=1)\n+            input_features = input_features.permute(0, 2, 1)[feature_attention_mask.bool()].permute(1, 0)\n+        else:\n+            audio_feature_lengths = None\n+\n+        audio_feat_lengths, audio_output_lengths = self.audio_tower._get_feat_extract_output_lengths(\n+            audio_feature_lengths if audio_feature_lengths is not None else feature_attention_mask.sum(-1)\n+        )\n+        feature_lens = audio_feature_lengths if audio_feature_lengths is not None else feature_attention_mask.sum(-1)\n+        audio_outputs = self.audio_tower(\n+            input_features,\n+            feature_lens=feature_lens,\n+            aftercnn_lens=audio_feat_lengths,\n+        )\n+        audio_features = audio_outputs.last_hidden_state\n+\n+        if audio_features.shape[0] != sum(audio_output_lengths.tolist()):\n+            raise ValueError(\"length of audio_features should match audio_output_lengths\")\n+\n+        return audio_features\n+\n     @auto_docstring\n     def forward(\n         self,\n@@ -2278,58 +2347,18 @@ def forward(\n         )\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        if feature_attention_mask is not None:\n-            audio_feature_lengths = torch.sum(feature_attention_mask, dim=1)\n-            input_features = input_features.permute(0, 2, 1)[feature_attention_mask.bool()].permute(1, 0)\n-        else:\n-            audio_feature_lengths = None\n-        if attention_mask is not None and position_ids is None:\n-            if (\n-                cache_position is None\n-                or (cache_position is not None and cache_position[0] == 0)\n-                or self.rope_deltas is None\n-            ):\n-                delta0 = (1 - attention_mask).sum(dim=-1).unsqueeze(1)\n-                position_ids, rope_deltas = self.get_rope_index(\n-                    input_ids,\n-                    image_grid_thw,\n-                    video_grid_thw,\n-                    attention_mask,\n-                    use_audio_in_video,\n-                    audio_feature_lengths,\n-                    video_second_per_grid,\n-                )\n-                rope_deltas = rope_deltas - delta0\n-                self.rope_deltas = rope_deltas\n-            else:\n-                batch_size, seq_length = input_ids.shape\n-                delta = cache_position[0] + self.rope_deltas if cache_position is not None else 0\n-                position_ids = torch.arange(seq_length, device=input_ids.device)\n-                position_ids = position_ids.view(1, -1).expand(batch_size, -1)\n-                position_ids = position_ids.add(delta)\n-                position_ids = position_ids.unsqueeze(0).expand(3, -1, -1)\n-\n         if inputs_embeds is None:\n             # 1. Extract the input embeddings\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n         # 2. Merge text , audios , image and video\n         if input_ids is not None and input_ids.shape[1] != 1:  # Prefill stage\n             if input_features is not None:\n-                audio_feat_lengths, audio_output_lengths = self.audio_tower._get_feat_extract_output_lengths(\n-                    audio_feature_lengths if audio_feature_lengths is not None else feature_attention_mask.sum(-1)\n-                )\n-                feature_lens = (\n-                    audio_feature_lengths if audio_feature_lengths is not None else feature_attention_mask.sum(-1)\n-                )\n-                audio_outputs = self.audio_tower(\n+                audio_features = self.get_audio_features(\n                     input_features,\n-                    feature_lens=feature_lens,\n-                    aftercnn_lens=audio_feat_lengths,\n+                    feature_attention_mask=feature_attention_mask,\n+                    audio_feature_lengths=audio_feature_lengths,\n                 )\n-                audio_features = audio_outputs.last_hidden_state\n-                if audio_features.shape[0] != sum(audio_output_lengths.tolist()):\n-                    raise ValueError(\"length of audio_features should match audio_output_lengths\")\n                 audio_mask = (\n                     (input_ids == self.config.audio_token_id)\n                     .unsqueeze(-1)\n@@ -2340,8 +2369,7 @@ def forward(\n                 inputs_embeds = inputs_embeds.masked_scatter(audio_mask, audio_features)\n \n             if pixel_values is not None:\n-                pixel_values = pixel_values.type(self.visual.dtype)\n-                image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n+                image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n                 image_mask = (\n                     (input_ids == self.config.image_token_id)\n                     .unsqueeze(-1)\n@@ -2352,8 +2380,7 @@ def forward(\n                 inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n \n             if pixel_values_videos is not None:\n-                pixel_values_videos = pixel_values_videos.type(self.visual.dtype)\n-                video_embeds = self.visual(pixel_values_videos, grid_thw=video_grid_thw)\n+                video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)\n                 video_mask = (\n                     (input_ids == self.config.video_token_id)\n                     .unsqueeze(-1)\n@@ -2366,6 +2393,37 @@ def forward(\n             if attention_mask is not None:\n                 attention_mask = attention_mask.to(inputs_embeds.device)\n \n+        if feature_attention_mask is not None:\n+            audio_feature_lengths = torch.sum(feature_attention_mask, dim=1)\n+        else:\n+            audio_feature_lengths = None\n+\n+        if attention_mask is not None and position_ids is None:\n+            if (\n+                cache_position is None\n+                or (cache_position is not None and cache_position[0] == 0)\n+                or self.rope_deltas is None\n+            ):\n+                delta0 = (1 - attention_mask).sum(dim=-1).unsqueeze(1)\n+                position_ids, rope_deltas = self.get_rope_index(\n+                    input_ids,\n+                    image_grid_thw,\n+                    video_grid_thw,\n+                    attention_mask,\n+                    use_audio_in_video,\n+                    audio_feature_lengths,\n+                    video_second_per_grid,\n+                )\n+                rope_deltas = rope_deltas - delta0\n+                self.rope_deltas = rope_deltas\n+            else:\n+                batch_size, seq_length = input_ids.shape\n+                delta = cache_position[0] + self.rope_deltas if cache_position is not None else 0\n+                position_ids = torch.arange(seq_length, device=input_ids.device)\n+                position_ids = position_ids.view(1, -1).expand(batch_size, -1)\n+                position_ids = position_ids.add(delta)\n+                position_ids = position_ids.unsqueeze(0).expand(3, -1, -1)\n+\n         outputs = self.model(\n             attention_mask=attention_mask,\n             position_ids=position_ids,"
        },
        {
            "sha": "12d988353381b3dd47676cf7ddc95a0587e236d3",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 32,
            "deletions": 4,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=3ab47b6ce37897c4061dd665143f12ef40b6179d",
            "patch": "@@ -1583,6 +1583,36 @@ def get_rope_index(\n \n             return position_ids, mrope_position_deltas\n \n+    def get_video_features(\n+        self, pixel_values_videos: torch.FloatTensor, video_grid_thw: Optional[torch.LongTensor] = None\n+    ):\n+        \"\"\"\n+        Encodes videos into continuous embeddings that can be forwarded to the language model.\n+\n+        Args:\n+            pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                The tensors corresponding to the input videos.\n+            video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):\n+                The temporal, height and width of feature shape of each video in LLM.\n+        \"\"\"\n+        pixel_values_videos = pixel_values_videos.type(self.visual.dtype)\n+        video_embeds = self.visual(pixel_values_videos, grid_thw=video_grid_thw)\n+        return video_embeds\n+\n+    def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = None):\n+        \"\"\"\n+        Encodes images into continuous embeddings that can be forwarded to the language model.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                The tensors corresponding to the input images.\n+            image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n+                The temporal, height and width of feature shape of each image in LLM.\n+        \"\"\"\n+        pixel_values = pixel_values.type(self.visual.dtype)\n+        image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n+        return image_embeds\n+\n     @auto_docstring\n     def forward(\n         self,\n@@ -1627,8 +1657,7 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n             if pixel_values is not None:\n-                pixel_values = pixel_values.type(self.visual.dtype)\n-                image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n+                image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n                 n_image_tokens = (input_ids == self.config.image_token_id).sum().item()\n                 n_image_features = image_embeds.shape[0]\n                 if n_image_tokens != n_image_features:\n@@ -1645,8 +1674,7 @@ def forward(\n                 inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n \n             if pixel_values_videos is not None:\n-                pixel_values_videos = pixel_values_videos.type(self.visual.dtype)\n-                video_embeds = self.visual(pixel_values_videos, grid_thw=video_grid_thw)\n+                video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)\n                 n_video_tokens = (input_ids == self.config.video_token_id).sum().item()\n                 n_video_features = video_embeds.shape[0]\n                 if n_video_tokens != n_video_features:"
        },
        {
            "sha": "0b3fd6ea0bc6ec783187a8a3d1b44c88dd189be4",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=3ab47b6ce37897c4061dd665143f12ef40b6179d",
            "patch": "@@ -646,8 +646,7 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n             if pixel_values is not None:\n-                pixel_values = pixel_values.type(self.visual.dtype)\n-                image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n+                image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n                 n_image_tokens = (input_ids == self.config.image_token_id).sum().item()\n                 n_image_features = image_embeds.shape[0]\n                 if n_image_tokens != n_image_features:\n@@ -664,8 +663,7 @@ def forward(\n                 inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n \n             if pixel_values_videos is not None:\n-                pixel_values_videos = pixel_values_videos.type(self.visual.dtype)\n-                video_embeds = self.visual(pixel_values_videos, grid_thw=video_grid_thw)\n+                video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)\n                 n_video_tokens = (input_ids == self.config.video_token_id).sum().item()\n                 n_video_features = video_embeds.shape[0]\n                 if n_video_tokens != n_video_features:"
        },
        {
            "sha": "47e54e76ae855e4d2d88c801e520fbbd0e6c4da9",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 34,
            "deletions": 6,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=3ab47b6ce37897c4061dd665143f12ef40b6179d",
            "patch": "@@ -1508,6 +1508,36 @@ def get_rope_index(\n \n             return position_ids, mrope_position_deltas\n \n+    def get_video_features(\n+        self, pixel_values_videos: torch.FloatTensor, video_grid_thw: Optional[torch.LongTensor] = None\n+    ):\n+        \"\"\"\n+        Encodes videos into continuous embeddings that can be forwarded to the language model.\n+\n+        Args:\n+            pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                The tensors corresponding to the input videos.\n+            video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):\n+                The temporal, height and width of feature shape of each video in LLM.\n+        \"\"\"\n+        pixel_values_videos = pixel_values_videos.type(self.visual.dtype)\n+        video_embeds = self.visual(pixel_values_videos, grid_thw=video_grid_thw)\n+        return video_embeds\n+\n+    def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = None):\n+        \"\"\"\n+        Encodes images into continuous embeddings that can be forwarded to the language model.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                The tensors corresponding to the input images.\n+            image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n+                The temporal, height and width of feature shape of each image in LLM.\n+        \"\"\"\n+        pixel_values = pixel_values.type(self.visual.dtype)\n+        image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n+        return image_embeds\n+\n     @auto_docstring\n     def forward(\n         self,\n@@ -1549,9 +1579,8 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n             if pixel_values is not None:\n-                pixel_values = pixel_values.type(self.visual.get_dtype())\n-                image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n+                image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n+                n_image_tokens = (input_ids == self.config.image_token_id).sum().item()\n                 n_image_features = image_embeds.shape[0]\n                 if not is_torchdynamo_compiling() and n_image_tokens != n_image_features:\n                     raise ValueError(\n@@ -1567,9 +1596,8 @@ def forward(\n                 inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n \n             if pixel_values_videos is not None:\n-                pixel_values_videos = pixel_values_videos.type(self.visual.get_dtype())\n-                video_embeds = self.visual(pixel_values_videos, grid_thw=video_grid_thw)\n-                n_video_tokens = (input_ids == self.config.video_token_id).sum()\n+                video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)\n+                n_video_tokens = (input_ids == self.config.video_token_id).sum().item()\n                 n_video_features = video_embeds.shape[0]\n                 if not is_torchdynamo_compiling() and n_video_tokens != n_video_features:\n                     raise ValueError("
        },
        {
            "sha": "bbf05404ac1290649176c260ca2be477eb236be6",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 47,
            "deletions": 42,
            "changes": 89,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=3ab47b6ce37897c4061dd665143f12ef40b6179d",
            "patch": "@@ -632,6 +632,52 @@ def inputs_merger(\n         merged_embeds = torch.where(image_mask.unsqueeze(-1), image_embeds, inputs_embeds)\n         return merged_embeds\n \n+    def get_image_features(self, pixel_values: torch.FloatTensor, pixel_attention_mask: torch.LongTensor = None):\n+        \"\"\"\n+        Encodes images into continuous embeddings that can be forwarded to the language model.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                The tensors corresponding to the input images.\n+            pixel_attention_mask (`torch.LongTensor`, *optional*):\n+                The attention mask indicating padded regions in the image.\n+        \"\"\"\n+        batch_size, num_images, num_channels, height, width = pixel_values.shape\n+        pixel_values = pixel_values.view(batch_size * num_images, *pixel_values.shape[2:])\n+\n+        # Remove padding images - padding images are full 0.\n+        nb_values_per_image = pixel_values.shape[1:].numel()\n+        real_images_inds = (pixel_values == 0.0).sum(dim=(-1, -2, -3)) != nb_values_per_image\n+\n+        if not any(real_images_inds):\n+            # no images, leave one empty image.\n+            real_images_inds[0] = True\n+\n+        pixel_values = pixel_values[real_images_inds].contiguous()\n+        # Handle the vision attention mask\n+        if pixel_attention_mask is None:\n+            pixel_attention_mask = torch.ones(\n+                size=[pixel_values.shape[i] for i in (0, 2, 3)],\n+                dtype=torch.bool,\n+                device=pixel_values.device,\n+            )\n+        else:\n+            # Remove padding images from the mask\n+            pixel_attention_mask = pixel_attention_mask.view(batch_size * num_images, *pixel_attention_mask.shape[2:])\n+            pixel_attention_mask = pixel_attention_mask[real_images_inds].contiguous()\n+        patch_size = self.config.vision_config.patch_size\n+        patches_subgrid = pixel_attention_mask.unfold(dimension=1, size=patch_size, step=patch_size)\n+        patches_subgrid = patches_subgrid.unfold(dimension=2, size=patch_size, step=patch_size)\n+        patch_attention_mask = (patches_subgrid.sum(dim=(-1, -2)) > 0).bool()\n+\n+        # Get sequence from the vision encoder\n+        image_hidden_states = self.vision_model(pixel_values=pixel_values, patch_attention_mask=patch_attention_mask)\n+        image_hidden_states = image_hidden_states.last_hidden_state\n+\n+        # Modality projection & resampling\n+        image_hidden_states = self.connector(image_hidden_states)\n+        return image_hidden_states\n+\n     @can_return_tuple\n     @auto_docstring(\n         custom_intro=\"\"\"\n@@ -704,48 +750,7 @@ def forward(\n         if pixel_values is not None and image_hidden_states is not None:\n             raise ValueError(\"You cannot specify both pixel_values and image_hidden_states at the same time\")\n         elif pixel_values is not None:\n-            batch_size, num_images, num_channels, height, width = pixel_values.shape\n-            pixel_values = pixel_values\n-            pixel_values = pixel_values.view(batch_size * num_images, *pixel_values.shape[2:])\n-\n-            # Remove padding images - padding images are full 0.\n-            nb_values_per_image = pixel_values.shape[1:].numel()\n-            real_images_inds = (pixel_values == 0.0).sum(dim=(-1, -2, -3)) != nb_values_per_image\n-\n-            if not any(real_images_inds):\n-                # no images, leave one empty image.\n-                real_images_inds[0] = True\n-\n-            pixel_values = pixel_values[real_images_inds].contiguous()\n-\n-            # Handle the vision attention mask\n-            if pixel_attention_mask is None:\n-                pixel_attention_mask = torch.ones(\n-                    size=[pixel_values.shape[i] for i in (0, 2, 3)],\n-                    dtype=torch.bool,\n-                    device=pixel_values.device,\n-                )\n-            else:\n-                # Remove padding images from the mask\n-                pixel_attention_mask = pixel_attention_mask.view(\n-                    batch_size * num_images, *pixel_attention_mask.shape[2:]\n-                )\n-                pixel_attention_mask = pixel_attention_mask[real_images_inds].contiguous()\n-\n-            patch_size = self.config.vision_config.patch_size\n-            patches_subgrid = pixel_attention_mask.unfold(dimension=1, size=patch_size, step=patch_size)\n-            patches_subgrid = patches_subgrid.unfold(dimension=2, size=patch_size, step=patch_size)\n-            patch_attention_mask = (patches_subgrid.sum(dim=(-1, -2)) > 0).bool()\n-\n-            # Get sequence from the vision encoder\n-            image_hidden_states = self.vision_model(\n-                pixel_values=pixel_values,\n-                patch_attention_mask=patch_attention_mask,\n-            ).last_hidden_state\n-\n-            # Modality projection & resampling\n-            image_hidden_states = self.connector(image_hidden_states)\n-\n+            image_hidden_states = self.get_image_features(pixel_values, pixel_attention_mask)\n         elif image_hidden_states is not None:\n             image_hidden_states = image_hidden_states.to(dtype=self.dtype, device=input_ids.device)\n "
        },
        {
            "sha": "b2263db30e4d69505391376731314a6238d6c02e",
            "filename": "src/transformers/models/smolvlm/modular_smolvlm.py",
            "status": "modified",
            "additions": 60,
            "deletions": 43,
            "changes": 103,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py?ref=3ab47b6ce37897c4061dd665143f12ef40b6179d",
            "patch": "@@ -22,7 +22,7 @@\n from ...cache_utils import DynamicCache\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...processing_utils import Unpack\n-from ...utils import logging\n+from ...utils import auto_docstring, can_return_tuple, logging\n from ..idefics3.configuration_idefics3 import Idefics3Config, Idefics3VisionConfig\n from ..idefics3.image_processing_idefics3 import Idefics3ImageProcessor\n from ..idefics3.modeling_idefics3 import (\n@@ -195,6 +195,64 @@ def inputs_merger(\n         merged_embeds = torch.where(image_mask.unsqueeze(-1), image_embeds, inputs_embeds)\n         return merged_embeds\n \n+    def get_image_features(self, pixel_values: torch.FloatTensor, pixel_attention_mask: torch.LongTensor = None):\n+        \"\"\"\n+        Encodes images into continuous embeddings that can be forwarded to the language model.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                The tensors corresponding to the input images.\n+            pixel_attention_mask (`torch.LongTensor`, *optional*):\n+                The attention mask indicating padded regions in the image.\n+        \"\"\"\n+        batch_size, num_images, num_channels, height, width = pixel_values.shape\n+        pixel_values = pixel_values.view(batch_size * num_images, *pixel_values.shape[2:])\n+\n+        # Remove padding images - padding images are full 0.\n+        nb_values_per_image = pixel_values.shape[1:].numel()\n+        real_images_inds = (pixel_values == 0.0).sum(dim=(-1, -2, -3)) != nb_values_per_image\n+\n+        if not any(real_images_inds):\n+            # no images, leave one empty image.\n+            real_images_inds[0] = True\n+\n+        pixel_values = pixel_values[real_images_inds].contiguous()\n+        # Handle the vision attention mask\n+        if pixel_attention_mask is None:\n+            pixel_attention_mask = torch.ones(\n+                size=[pixel_values.shape[i] for i in (0, 2, 3)],\n+                dtype=torch.bool,\n+                device=pixel_values.device,\n+            )\n+        else:\n+            # Remove padding images from the mask\n+            pixel_attention_mask = pixel_attention_mask.view(batch_size * num_images, *pixel_attention_mask.shape[2:])\n+            pixel_attention_mask = pixel_attention_mask[real_images_inds].contiguous()\n+        patch_size = self.config.vision_config.patch_size\n+        patches_subgrid = pixel_attention_mask.unfold(dimension=1, size=patch_size, step=patch_size)\n+        patches_subgrid = patches_subgrid.unfold(dimension=2, size=patch_size, step=patch_size)\n+        patch_attention_mask = (patches_subgrid.sum(dim=(-1, -2)) > 0).bool()\n+\n+        # Get sequence from the vision encoder\n+        image_hidden_states = self.vision_model(pixel_values=pixel_values, patch_attention_mask=patch_attention_mask)\n+        image_hidden_states = image_hidden_states.last_hidden_state\n+\n+        # Modality projection & resampling\n+        image_hidden_states = self.connector(image_hidden_states)\n+        return image_hidden_states\n+\n+    @can_return_tuple\n+    @auto_docstring(\n+        custom_intro=\"\"\"\n+        Inputs fed to the model can have an arbitrary number of images. To account for this, pixel_values fed to\n+        the model have image padding -> (batch_size, max_num_images, 3, max_heights, max_widths) where\n+        max_num_images is the maximum number of images among the batch_size samples in the batch.\n+        Padding images are not needed beyond padding the pixel_values at the entrance of the model.\n+        For efficiency, we only pass through the vision_model's forward the real images by\n+        discarding the padding images i.e. pixel_values of size (image_batch_size, 3, height, width) where\n+        image_batch_size would be 7 when num_images_per_sample=[1, 3, 1, 2] and max_num_images would be 3.\n+        \"\"\"\n+    )\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -249,48 +307,7 @@ def forward(\n         if pixel_values is not None and image_hidden_states is not None:\n             raise ValueError(\"You cannot specify both pixel_values and image_hidden_states at the same time\")\n         elif pixel_values is not None:\n-            batch_size, num_images, num_channels, height, width = pixel_values.shape\n-            pixel_values = pixel_values\n-            pixel_values = pixel_values.view(batch_size * num_images, *pixel_values.shape[2:])\n-\n-            # Remove padding images - padding images are full 0.\n-            nb_values_per_image = pixel_values.shape[1:].numel()\n-            real_images_inds = (pixel_values == 0.0).sum(dim=(-1, -2, -3)) != nb_values_per_image\n-\n-            if not any(real_images_inds):\n-                # no images, leave one empty image.\n-                real_images_inds[0] = True\n-\n-            pixel_values = pixel_values[real_images_inds].contiguous()\n-\n-            # Handle the vision attention mask\n-            if pixel_attention_mask is None:\n-                pixel_attention_mask = torch.ones(\n-                    size=[pixel_values.shape[i] for i in (0, 2, 3)],\n-                    dtype=torch.bool,\n-                    device=pixel_values.device,\n-                )\n-            else:\n-                # Remove padding images from the mask\n-                pixel_attention_mask = pixel_attention_mask.view(\n-                    batch_size * num_images, *pixel_attention_mask.shape[2:]\n-                )\n-                pixel_attention_mask = pixel_attention_mask[real_images_inds].contiguous()\n-\n-            patch_size = self.config.vision_config.patch_size\n-            patches_subgrid = pixel_attention_mask.unfold(dimension=1, size=patch_size, step=patch_size)\n-            patches_subgrid = patches_subgrid.unfold(dimension=2, size=patch_size, step=patch_size)\n-            patch_attention_mask = (patches_subgrid.sum(dim=(-1, -2)) > 0).bool()\n-\n-            # Get sequence from the vision encoder\n-            image_hidden_states = self.vision_model(\n-                pixel_values=pixel_values,\n-                patch_attention_mask=patch_attention_mask,\n-            ).last_hidden_state\n-\n-            # Modality projection & resampling\n-            image_hidden_states = self.connector(image_hidden_states)\n-\n+            image_hidden_states = self.get_image_features(pixel_values, pixel_attention_mask)\n         elif image_hidden_states is not None:\n             image_hidden_states = image_hidden_states.to(dtype=self.dtype, device=input_ids.device)\n "
        },
        {
            "sha": "535c796e8ab9f4244660bcd0dc266d1434ea9132",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 19,
            "deletions": 6,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ab47b6ce37897c4061dd665143f12ef40b6179d/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=3ab47b6ce37897c4061dd665143f12ef40b6179d",
            "patch": "@@ -205,25 +205,34 @@ def set_input_embeddings(self, value):\n     def get_image_features(\n         self,\n         pixel_values_images: torch.FloatTensor,\n-        vision_feature_layer: Union[int, List[int]],\n-        vision_feature_select_strategy: str,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n     ):\n         \"\"\"\n         Obtains image last hidden states from the vision tower and apply multimodal projection.\n \n         Args:\n             pixel_values_images (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`)\n                The tensors corresponding to the input images.\n-            vision_feature_layer (`Union[int, List[int]]`):\n+            vision_feature_layer (`Union[int, List[int]]`, *optional*):\n                 The index of the layer to select the vision feature. If multiple indices are provided,\n                 the vision feature of the corresponding indices will be concatenated to form the\n                 vision features.\n-            vision_feature_select_strategy (`str`):\n+            vision_feature_select_strategy (`str`, *optional*):\n                 The feature selection strategy used to select the vision feature from the vision backbone.\n                 Can be one of `\"default\"` or `\"full\"`\n         Returns:\n             image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).\n         \"\"\"\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+        vision_feature_select_strategy = (\n+            vision_feature_select_strategy\n+            if vision_feature_select_strategy is not None\n+            else self.config.vision_feature_select_strategy\n+        )\n+\n         if vision_feature_select_strategy not in [\"default\", \"full\"]:\n             raise ValueError(f\"Unexpected select feature strategy: {self.config.vision_feature_select_strategy}\")\n \n@@ -249,22 +258,26 @@ def get_image_features(\n     def get_video_features(\n         self,\n         pixel_values_videos: torch.FloatTensor,\n-        vision_feature_layer: Union[int, List[int]],\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n     ):\n         \"\"\"\n         Obtains video last hidden states from the vision tower and apply multimodal projection.\n \n         Args:\n             pixel_values_videos (`torch.FloatTensor]` of shape `(batch_size, num_frames, channels, height, width)`)\n                The tensors corresponding to the input videos.\n-            vision_feature_layer (`Union[int, List[int]]`):\n+            vision_feature_layer (`Union[int, List[int]]`, *optional*):\n                 The index of the layer to select the vision feature. If multiple indices are provided,\n                 the vision feature of the corresponding indices will be concatenated to form the\n                 vision features.\n         Returns:\n             video_features (`torch.Tensor`): Video feature tensor of shape `(num_videos * num_frames, image_length, embed_dim)`).\n             frames (`int`): Number of frames the videos have.\n         \"\"\"\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+\n         batch_size_vid, num_frames, channels, height, width = pixel_values_videos.shape\n \n         pixel_values = pixel_values_videos.reshape(batch_size_vid * num_frames, channels, height, width)"
        }
    ],
    "stats": {
        "total": 1549,
        "additions": 978,
        "deletions": 571
    }
}