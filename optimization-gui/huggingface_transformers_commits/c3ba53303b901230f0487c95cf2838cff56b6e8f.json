{
    "author": "kmehant",
    "message": "feat: add support for tensor parallel training workflow with accelerate (#34194)\n\n* feat: add support for tensor parallel flow using accelerate\r\n\r\nSigned-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>\r\n\r\n* fix: add tp degree to env variable\r\n\r\nSigned-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>\r\n\r\n* fix: add version check for accelerate to allow TP\r\n\r\nSigned-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>\r\n\r\n* docs: tensor parallelism\r\n\r\nSigned-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>\r\n\r\n* nit: rename plugin name\r\n\r\nSigned-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>\r\n\r\n* fix: guard accelerate version before allow tp\r\n\r\nSigned-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>\r\n\r\n* docs: add more docs and updates related to TP\r\n\r\nSigned-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>\r\n\r\n---------\r\n\r\nSigned-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "c3ba53303b901230f0487c95cf2838cff56b6e8f",
    "files": [
        {
            "sha": "e70dbb255eacbe1f37bab701ee9374ca6f5cd217",
            "filename": "docs/source/ar/trainer.md",
            "status": "modified",
            "additions": 23,
            "deletions": 0,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/c3ba53303b901230f0487c95cf2838cff56b6e8f/docs%2Fsource%2Far%2Ftrainer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c3ba53303b901230f0487c95cf2838cff56b6e8f/docs%2Fsource%2Far%2Ftrainer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Ftrainer.md?ref=c3ba53303b901230f0487c95cf2838cff56b6e8f",
            "patch": "@@ -673,6 +673,29 @@ tpu_use_sudo: false\n use_cpu: false\n ```\n \n+</hfoption>\n+<hfoption id=\"Tensor Parallelism with PyTorch 2\">\n+\n+```yml\n+compute_environment: LOCAL_MACHINE\n+tp_config:\n+  tp_size: 4\n+distributed_type: TP\n+downcast_bf16: 'no'\n+machine_rank: 0\n+main_training_function: main\n+mixed_precision: 'no'\n+num_machines: 1\n+num_processes: 4\n+rdzv_backend: static\n+same_network: true\n+tpu_env: []\n+tpu_use_cluster: false\n+tpu_use_sudo: false\n+use_cpu: false\n+\n+```\n+\n </hfoption>\n </hfoptions>\n ÙŠÙØ¹Ø¯ Ø£Ù…Ø±  [`accelerate_launch`](https://huggingface.co/docs/accelerate/package_reference/cli#accelerate-launch) Ù‡Ùˆ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù…ÙÙˆØµÙ‰ Ø¨Ù‡Ø§ Ù„ØªØ´ØºÙŠÙ„ Ù†Øµ Ø§Ù„Ø¨Ø±Ù…Ø¬Ù‰ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ Ù†Ø¸Ø§Ù… Ù…ÙˆØ²Ø¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Accelerate Ùˆ [`Trainer`] Ù…Ø¹ Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© ÙÙŠ `config_file.yaml`. ÙŠØªÙ… Ø­ÙØ¸ Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ù ÙÙŠ Ù…Ø¬Ù„Ø¯ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù€ Accelerate ÙˆÙŠØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¹Ù†Ø¯ ØªØ´ØºÙŠÙ„ `accelerate_launch`."
        },
        {
            "sha": "1fd458d430f65ab1d1d9d07769c3de4beb49e9eb",
            "filename": "docs/source/en/llm_tutorial_optimization.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c3ba53303b901230f0487c95cf2838cff56b6e8f/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c3ba53303b901230f0487c95cf2838cff56b6e8f/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md?ref=c3ba53303b901230f0487c95cf2838cff56b6e8f",
            "patch": "@@ -55,7 +55,7 @@ To give some examples of how much VRAM it roughly takes to load a model in bfloa\n \n As of writing this document, the largest GPU chip on the market is the A100 & H100 offering 80GB of VRAM. Most of the models listed before require more than 80GB just to be loaded and therefore necessarily require [tensor parallelism](https://huggingface.co/docs/transformers/perf_train_gpu_many#tensor-parallelism) and/or [pipeline parallelism](https://huggingface.co/docs/transformers/perf_train_gpu_many#naive-model-parallelism-vertical-and-pipeline-parallelism).\n \n-ğŸ¤— Transformers does not support tensor parallelism out of the box as it requires the model architecture to be written in a specific way. If you're interested in writing models in a tensor-parallelism-friendly way, feel free to have a look at [the text-generation-inference library](https://github.com/huggingface/text-generation-inference/tree/main/server/text_generation_server/models/custom_modeling).\n+ğŸ¤— Transformers now supports tensor parallelism for supported models having `base_tp_plan` in their respecitve config classes. Learn more about Tensor Parallelism [here](perf_train_gpu_many#tensor-parallelism). Furthermore, if you're interested in writing models in a tensor-parallelism-friendly way, feel free to have a look at [the text-generation-inference library](https://github.com/huggingface/text-generation-inference/tree/main/server/text_generation_server/models/custom_modeling).\n \n Naive pipeline parallelism is supported out of the box. For this, simply load the model with `device=\"auto\"` which will automatically place the different layers on the available GPUs as explained [here](https://huggingface.co/docs/accelerate/v0.22.0/en/concept_guides/big_model_inference).\n Note, however that while very effective, this naive pipeline parallelism does not tackle the issues of GPU idling. For this more advanced pipeline parallelism is required as explained [here](https://huggingface.co/docs/transformers/en/perf_train_gpu_many#naive-model-parallelism-vertical-and-pipeline-parallelism)."
        },
        {
            "sha": "bf9467d19d2249939308dc45ce6f485990a63002",
            "filename": "docs/source/en/perf_train_gpu_many.md",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c3ba53303b901230f0487c95cf2838cff56b6e8f/docs%2Fsource%2Fen%2Fperf_train_gpu_many.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c3ba53303b901230f0487c95cf2838cff56b6e8f/docs%2Fsource%2Fen%2Fperf_train_gpu_many.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_train_gpu_many.md?ref=c3ba53303b901230f0487c95cf2838cff56b6e8f",
            "patch": "@@ -450,12 +450,13 @@ Implementations:\n - [parallelformers](https://github.com/tunib-ai/parallelformers) (only inference at the moment)\n - [SageMaker](https://arxiv.org/abs/2111.05972) - this is a proprietary solution that can only be used on AWS.\n - [OSLO](https://github.com/tunib-ai/oslo) has the tensor parallelism implementation based on the Transformers.\n+- [`transformers` integration](main_classes/trainer) tensor parallelism is available through tp_size attribute for models having `base_tp_plan`. Further you can look at [example usage](perf_infer_gpu_multi)\n \n SageMaker combines TP with DP for a more efficient processing.\n \n ğŸ¤— Transformers status:\n-- core: not yet implemented in the core\n-- but if you want inference [parallelformers](https://github.com/tunib-ai/parallelformers) provides this support for most of our models. So until this is implemented in the core you can use theirs. And hopefully training mode will be supported too.\n+- core: uses PyTorch 2 APIs to support tensor parallelism to models having base_tp_plan in their respective config classes.\n+- Alternatively, you can as well try [parallelformers](https://github.com/tunib-ai/parallelformers) that provides this support for most of our models. Training mode with TP is as well supported natively in transformers.\n - Deepspeed-Inference also supports our BERT, GPT-2, and GPT-Neo models in their super-fast CUDA-kernel-based inference mode, see more [here](https://www.deepspeed.ai/tutorials/inference-tutorial/)\n \n ğŸ¤— Accelerate integrates with [TP from Megatron-LM](https://huggingface.co/docs/accelerate/v0.23.0/en/usage_guides/megatron_lm).\n@@ -535,7 +536,7 @@ Important papers:\n - [Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model](\n https://arxiv.org/abs/2201.11990)\n \n-ğŸ¤— Transformers status: not yet implemented, since we have no PP and TP.\n+ğŸ¤— Transformers status: not yet implemented, since we have no PP.\n \n ## FlexFlow\n "
        },
        {
            "sha": "8cfe5dfc6afd3f4bf7ce3d5d27cf3a8681d20b5d",
            "filename": "docs/source/en/trainer.md",
            "status": "modified",
            "additions": 23,
            "deletions": 0,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/c3ba53303b901230f0487c95cf2838cff56b6e8f/docs%2Fsource%2Fen%2Ftrainer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c3ba53303b901230f0487c95cf2838cff56b6e8f/docs%2Fsource%2Fen%2Ftrainer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftrainer.md?ref=c3ba53303b901230f0487c95cf2838cff56b6e8f",
            "patch": "@@ -799,6 +799,29 @@ tpu_use_sudo: false\n use_cpu: false\n ```\n \n+</hfoption>\n+<hfoption id=\"Tensor Parallelism with PyTorch 2\">\n+\n+```yml\n+compute_environment: LOCAL_MACHINE\n+tp_config:\n+  tp_size: 4\n+distributed_type: TP\n+downcast_bf16: 'no'\n+machine_rank: 0\n+main_training_function: main\n+mixed_precision: 'no'\n+num_machines: 1\n+num_processes: 4\n+rdzv_backend: static\n+same_network: true\n+tpu_env: []\n+tpu_use_cluster: false\n+tpu_use_sudo: false\n+use_cpu: false\n+\n+```\n+\n </hfoption>\n </hfoptions>\n "
        },
        {
            "sha": "0362fe1d7d20f7effe8a59f56f2c0532f266a2b4",
            "filename": "docs/source/es/trainer.md",
            "status": "modified",
            "additions": 24,
            "deletions": 0,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/c3ba53303b901230f0487c95cf2838cff56b6e8f/docs%2Fsource%2Fes%2Ftrainer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c3ba53303b901230f0487c95cf2838cff56b6e8f/docs%2Fsource%2Fes%2Ftrainer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Ftrainer.md?ref=c3ba53303b901230f0487c95cf2838cff56b6e8f",
            "patch": "@@ -361,6 +361,30 @@ use_cpu: false\n \n ```\n \n+</hfoption>\n+\n+<hfoption id=\"Tensor Parallelism with PyTorch 2\">\n+\n+```yml\n+compute_environment: LOCAL_MACHINE\n+tp_config:\n+  tp_size: 4\n+distributed_type: TP\n+downcast_bf16: 'no'\n+machine_rank: 0\n+main_training_function: main\n+mixed_precision: 'no'\n+num_machines: 1\n+num_processes: 4\n+rdzv_backend: static\n+same_network: true\n+tpu_env: []\n+tpu_use_cluster: false\n+tpu_use_sudo: false\n+use_cpu: false\n+\n+```\n+\n </hfoption>\n </hfoptions>\n "
        },
        {
            "sha": "976072730c869888bd99b9b3e9cc974a4190cfd5",
            "filename": "docs/source/ko/trainer.md",
            "status": "modified",
            "additions": 23,
            "deletions": 0,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/c3ba53303b901230f0487c95cf2838cff56b6e8f/docs%2Fsource%2Fko%2Ftrainer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c3ba53303b901230f0487c95cf2838cff56b6e8f/docs%2Fsource%2Fko%2Ftrainer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftrainer.md?ref=c3ba53303b901230f0487c95cf2838cff56b6e8f",
            "patch": "@@ -548,6 +548,29 @@ tpu_use_sudo: false\n use_cpu: false\n ```\n \n+</hfoption>\n+<hfoption id=\"Tensor Parallelism with PyTorch 2\">\n+\n+```yml\n+compute_environment: LOCAL_MACHINE\n+tp_config:\n+  tp_size: 4\n+distributed_type: TP\n+downcast_bf16: 'no'\n+machine_rank: 0\n+main_training_function: main\n+mixed_precision: 'no'\n+num_machines: 1\n+num_processes: 4\n+rdzv_backend: static\n+same_network: true\n+tpu_env: []\n+tpu_use_cluster: false\n+tpu_use_sudo: false\n+use_cpu: false\n+\n+```\n+\n </hfoption>\n </hfoptions>\n "
        },
        {
            "sha": "94659d9884145f6176edfedc414cea18865af49c",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 11,
            "deletions": 1,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/c3ba53303b901230f0487c95cf2838cff56b6e8f/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c3ba53303b901230f0487c95cf2838cff56b6e8f/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=c3ba53303b901230f0487c95cf2838cff56b6e8f",
            "patch": "@@ -241,6 +241,8 @@\n     )\n \n     DATA_SAMPLERS = [RandomSampler]\n+    if version.parse(accelerate_version) > version.parse(\"1.3.0\"):\n+        from accelerate.utils import TorchTensorParallelPlugin\n     if version.parse(accelerate_version) > version.parse(\"0.23.0\"):\n         from accelerate.data_loader import SeedableRandomSampler\n \n@@ -5094,6 +5096,14 @@ def create_accelerator_and_postprocess(self):\n             args[\"dataloader_config\"] = dataloader_config\n         else:\n             args.update(accelerator_config)\n+        # tp is initialized at Accelerator init phase so\n+        # args should be prepared here\n+        if self.args.tp_size > 1:\n+            self.is_tp_enabled = True\n+            if version.parse(accelerate_version) > version.parse(\"1.3.0\"):\n+                args[\"torch_tp_plugin\"] = TorchTensorParallelPlugin(tp_size=self.args.tp_size)\n+            else:\n+                raise ValueError(\"Requires accelerate>1.3.0 to use Tensor Parallelism.\")\n \n         # create accelerator object\n         self.accelerator = Accelerator(**args)\n@@ -5108,7 +5118,7 @@ def create_accelerator_and_postprocess(self):\n         # deepspeed and accelerate flags covering both trainer args and accelerate launcher\n         self.is_deepspeed_enabled = getattr(self.accelerator.state, \"deepspeed_plugin\", None) is not None\n         self.is_fsdp_enabled = getattr(self.accelerator.state, \"fsdp_plugin\", None) is not None\n-\n+        self.is_tp_enabled = getattr(self.accelerator.state, \"torch_tp_plugin\", None) is not None\n         # post accelerator creation setup\n         if self.is_fsdp_enabled:\n             fsdp_plugin = self.accelerator.state.fsdp_plugin"
        },
        {
            "sha": "005c035ca6226163005e9f7ffecec710ba88bc17",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 24,
            "deletions": 1,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/c3ba53303b901230f0487c95cf2838cff56b6e8f/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c3ba53303b901230f0487c95cf2838cff56b6e8f/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=c3ba53303b901230f0487c95cf2838cff56b6e8f",
            "patch": "@@ -569,7 +569,10 @@ class TrainingArguments:\n                     Will use gradient checkpointing over each nested XLA FSDP wrapped layer. This setting can only be\n                     used when the xla flag is set to true, and an auto wrapping policy is specified through\n                     fsdp_min_num_params or fsdp_transformer_layer_cls_to_wrap.\n-\n+        tp_size (`int`, *optional*):\n+            Use tp_size to enable PyTorch tensor parallelism. Tensor parallelism support is only available to models having `base_tp_plan`\n+            in their respective config classes.\n+            Set a value greater than 1 to activate TP. The same is used to prepare device mesh internally. Requires accelerate>1.3.0.\n         deepspeed (`str` or `dict`, *optional*):\n             Use [Deepspeed](https://github.com/deepspeedai/DeepSpeed). This is an experimental feature and its API may\n             evolve in the future. The value is either the location of DeepSpeed json config file (e.g.,\n@@ -1250,6 +1253,18 @@ class TrainingArguments:\n             )\n         },\n     )\n+    tp_size: Optional[int] = field(\n+        default=0,\n+        metadata={\n+            \"help\": (\n+                \"Use tp_size to enable pytorch tensor parallelism.\"\n+                \"Tensor parallelism support is only available to models having `base_tp_plan` in their respective config classes.\"\n+                \"Set a value greater than 1 to activate TP.\"\n+                \"The same is used to prepare device mesh internally.\"\n+                \"Requires accelerate>1.3.0.\"\n+            )\n+        },\n+    )\n     fsdp_transformer_layer_cls_to_wrap: Optional[str] = field(\n         default=None,\n         metadata={\n@@ -1975,6 +1990,14 @@ def __post_init__(self):\n             if self.fsdp_config[\"xla_fsdp_grad_ckpt\"]:\n                 warnings.warn(\"`--xla_fsdp_grad_ckpt` is useful only when `--xla` is set to true.\")\n \n+        if self.tp_size > 1:\n+            if not is_accelerate_available(\"1.3.1\"):\n+                raise NotImplementedError(\n+                    \"TP using PyTorch requires Accelerate version `accelerate` >= 1.3.1. \"\n+                    \"This is not supported and we recommend you to update your version.\"\n+                )\n+            os.environ[\"ACCELERATE_USE_TP\"] = \"true\"\n+            os.environ[\"TP_SIZE\"] = str(self.tp_size)\n         # accelerate integration for FSDP\n         if len(self.fsdp) > 0 and not self.fsdp_config[\"xla\"]:\n             os.environ[\"ACCELERATE_USE_FSDP\"] = \"true\""
        }
    ],
    "stats": {
        "total": 139,
        "additions": 133,
        "deletions": 6
    }
}