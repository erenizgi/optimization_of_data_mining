{
    "author": "vasqu",
    "message": "[`Mistral Tokenizers`] Fix tokenizer detection (#42389)\n\n* fix\n\n* sanity check\n\n* style\n\n* comments\n\n* make it v5 explicit\n\n* make explicit fixes possible in local tokenizers\n\n* remove hub usage on local\n\n* fix\n\n* extend test for no config case\n\n* move mistral patch outside to separate fn\n\n* fix local path only\n\n* add a tes\n\n* make sure test does not pass before this PR\n\n* styling\n\n* make sure it exists\n\n* fix\n\n* fix\n\n* rename\n\n* up\n\n* last nit i hope lord\n\n---------\n\nCo-authored-by: Arthur <arthur.zucker@gmail.com>",
    "sha": "b6055550a15a8fab367cf983b743ff68cc58d81a",
    "files": [
        {
            "sha": "f65e474767b301196c8e8249ce4f73a43c94e179",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 83,
            "deletions": 32,
            "changes": 115,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6055550a15a8fab367cf983b743ff68cc58d81a/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6055550a15a8fab367cf983b743ff68cc58d81a/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=b6055550a15a8fab367cf983b743ff68cc58d81a",
            "patch": "@@ -2099,12 +2099,13 @@ def from_pretrained(\n                             template = template.removesuffix(\".jinja\")\n                             vocab_files[f\"chat_template_{template}\"] = f\"{CHAT_TEMPLATE_DIR}/{template}.jinja\"\n \n+        remote_files = []\n         if not is_local and not local_files_only:\n             try:\n                 remote_files = list_repo_files(pretrained_model_name_or_path)\n             except Exception:\n                 remote_files = []\n-        else:\n+        elif pretrained_model_name_or_path and os.path.isdir(pretrained_model_name_or_path):\n             remote_files = os.listdir(pretrained_model_name_or_path)\n \n         if \"tokenizer_file\" in vocab_files and not re.search(vocab_files[\"tokenizer_file\"], \"\".join(remote_files)):\n@@ -2437,57 +2438,108 @@ def _from_pretrained(\n         except NotImplementedError:\n             vocab_size = 0\n \n+        # Optionally patches mistral tokenizers with wrong regex\n         if (\n             vocab_size > 100000\n             and hasattr(tokenizer, \"_tokenizer\")\n             and getattr(tokenizer._tokenizer, \"pre_tokenizer\", None) is not None\n         ):\n-            from huggingface_hub import model_info\n+            tokenizer = cls._patch_mistral_regex(\n+                tokenizer,\n+                pretrained_model_name_or_path,\n+                token=token,\n+                cache_dir=cache_dir,\n+                local_files_only=local_files_only,\n+                _commit_hash=_commit_hash,\n+                _is_local=_is_local,\n+                init_kwargs=init_kwargs,\n+                fix_mistral_regex=kwargs.get(\"fix_mistral_regex\"),\n+            )\n \n-            def is_base_mistral(model_id: str) -> bool:\n-                model = model_info(model_id)\n-                if model.tags is not None:\n-                    if re.search(\"base_model:.*mistralai\", \"\".join(model.tags)):\n-                        return True\n-                return False\n+        return tokenizer\n \n-            if _is_local or is_base_mistral(pretrained_model_name_or_path):\n-                _config_file = cached_file(\n-                    pretrained_model_name_or_path,\n-                    \"config.json\",\n-                    cache_dir=cache_dir,\n-                    token=token,\n-                    local_files_only=local_files_only,\n-                    _raise_exceptions_for_missing_entries=False,\n-                    _raise_exceptions_for_connection_errors=False,\n-                    _commit_hash=_commit_hash,\n-                )\n-                if _config_file is not None:\n-                    with open(_config_file, encoding=\"utf-8\") as f:\n-                        _config = json.load(f)\n-                    transformers_version = _config.get(\"transformers_version\")\n+    @classmethod\n+    def _patch_mistral_regex(\n+        cls,\n+        tokenizer,\n+        pretrained_model_name_or_path,\n+        token=None,\n+        cache_dir=None,\n+        local_files_only=False,\n+        _commit_hash=None,\n+        _is_local=False,\n+        init_kwargs=None,\n+        fix_mistral_regex=None,\n+    ):\n+        \"\"\"\n+        Patches mistral related tokenizers with incorrect regex if detected\n+            1) Local file with an associated config saved next to it\n+                >> Model type one of the mistral models (on older versions)\n+            2) Remote models on the hub from official mistral models\n+                >> Tags including `base_model:.*mistralai`\n+        \"\"\"\n+        from huggingface_hub import model_info\n \n-                    if transformers_version and version.parse(transformers_version) <= version.parse(\"4.57.2\"):\n-                        if _is_local and _config.model_type not in [\n+        def is_base_mistral(model_id: str) -> bool:\n+            model = model_info(model_id)\n+            if model.tags is not None:\n+                if re.search(\"base_model:.*mistralai\", \"\".join(model.tags)):\n+                    return True\n+            return False\n+\n+        if _is_local or is_base_mistral(pretrained_model_name_or_path):\n+            _config_file = cached_file(\n+                pretrained_model_name_or_path,\n+                \"config.json\",\n+                cache_dir=cache_dir,\n+                token=token,\n+                local_files_only=local_files_only,\n+                _raise_exceptions_for_missing_entries=False,\n+                _raise_exceptions_for_connection_errors=False,\n+                _commit_hash=_commit_hash,\n+            )\n+\n+            # Detected using a (local) mistral tokenizer\n+            mistral_config_detected = False\n+            if _config_file is not None:\n+                with open(_config_file, encoding=\"utf-8\") as f:\n+                    _config = json.load(f)\n+                transformers_version = _config.get(\"transformers_version\")\n+                transformers_model_type = _config.get(\"model_type\")\n+\n+                # Detect if we can skip the mistral fix by\n+                #   a) having a non-mistral tokenizer\n+                #   b) fixed version of transformers\n+                if transformers_version and version.parse(transformers_version) <= version.parse(\"4.57.2\"):\n+                    if (\n+                        _is_local\n+                        and transformers_model_type is not None\n+                        and transformers_model_type\n+                        not in [\n                             \"mistral\",\n                             \"mistral3\",\n-                            \"voxstral\",\n+                            \"voxtral\",\n                             \"ministral\",\n                             \"pixtral\",\n-                        ]:\n-                            return tokenizer\n+                        ]\n+                    ):\n+                        return tokenizer\n+                elif transformers_version and version.parse(transformers_version) >= version.parse(\"5.0.0\"):\n+                    return tokenizer\n \n+                mistral_config_detected = True\n+\n+            if mistral_config_detected or (not _is_local and is_base_mistral(pretrained_model_name_or_path)):\n                 # Expose the `fix_mistral_regex` flag on the tokenizer when provided, even if no correction is applied.\n-                if \"fix_mistral_regex\" in init_kwargs:\n+                if init_kwargs and \"fix_mistral_regex\" in init_kwargs:\n                     setattr(tokenizer, \"fix_mistral_regex\", init_kwargs[\"fix_mistral_regex\"])\n \n-                fix_mistral_regex = kwargs.get(\"fix_mistral_regex\")  # not init kwargs\n                 # only warn if its not explicitly passed\n                 if fix_mistral_regex is None and not getattr(tokenizer, \"fix_mistral_regex\", False):\n                     setattr(tokenizer, \"fix_mistral_regex\", False)\n                     logger.warning(\n                         f\"The tokenizer you are loading from '{pretrained_model_name_or_path}'\"\n-                        f\" with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. \"\n+                        f\" with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e.\"\n                         \" This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\"\n                     )\n                 elif fix_mistral_regex is True or getattr(tokenizer, \"fix_mistral_regex\", False):\n@@ -2500,7 +2552,6 @@ def is_base_mistral(model_id: str) -> bool:\n                         ),\n                         behavior=\"isolated\",\n                     )\n-\n         return tokenizer\n \n     @staticmethod"
        },
        {
            "sha": "ed5c38a03e73fedd44470061f38715f0b1bf1ab0",
            "filename": "tests/models/auto/test_tokenization_auto.py",
            "status": "modified",
            "additions": 39,
            "deletions": 0,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6055550a15a8fab367cf983b743ff68cc58d81a/tests%2Fmodels%2Fauto%2Ftest_tokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6055550a15a8fab367cf983b743ff68cc58d81a/tests%2Fmodels%2Fauto%2Ftest_tokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fauto%2Ftest_tokenization_auto.py?ref=b6055550a15a8fab367cf983b743ff68cc58d81a",
            "patch": "@@ -34,9 +34,13 @@\n     GPT2Tokenizer,\n     GPT2TokenizerFast,\n     PreTrainedTokenizerFast,\n+    Qwen2Tokenizer,\n+    Qwen2TokenizerFast,\n+    Qwen3MoeConfig,\n     RobertaTokenizer,\n     RobertaTokenizerFast,\n     is_tokenizers_available,\n+    logging,\n )\n from transformers.models.auto.configuration_auto import CONFIG_MAPPING, AutoConfig\n from transformers.models.auto.tokenization_auto import (\n@@ -49,6 +53,7 @@\n     DUMMY_DIFF_TOKENIZER_IDENTIFIER,\n     DUMMY_UNKNOWN_IDENTIFIER,\n     SMALL_MODEL_IDENTIFIER,\n+    CaptureLogger,\n     RequestCounter,\n     is_flaky,\n     require_tokenizers,\n@@ -229,6 +234,40 @@ def test_auto_tokenizer_from_local_folder(self):\n         self.assertIsInstance(tokenizer2, tokenizer.__class__)\n         self.assertEqual(tokenizer2.vocab_size, 12)\n \n+    def test_auto_tokenizer_from_local_folder_mistral_detection(self):\n+        \"\"\"See #42374 for reference, ensuring proper mistral detection on local tokenizers\"\"\"\n+        tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-235B-A22B-Thinking-2507\")\n+        config = Qwen3MoeConfig.from_pretrained(\"Qwen/Qwen3-235B-A22B-Thinking-2507\")\n+        self.assertIsInstance(tokenizer, (Qwen2Tokenizer, Qwen2TokenizerFast))\n+\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            tokenizer.save_pretrained(tmp_dir)\n+\n+            # Case 1: Tokenizer with no config associated\n+            logger = logging.get_logger(\"transformers.tokenization_utils_base\")\n+            with CaptureLogger(logger) as cl:\n+                AutoTokenizer.from_pretrained(tmp_dir)\n+            self.assertNotIn(\n+                \"with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e\",\n+                cl.out,\n+            )\n+\n+            # Case 2: Tokenizer with config associated\n+            # Needed to be saved along the tokenizer to detect (non)mistral\n+            # for a version where the regex bug occurs\n+            config_dict = config.to_diff_dict()\n+            config_dict[\"transformers_version\"] = \"4.57.2\"\n+\n+            # Manually saving to avoid versioning clashes\n+            config_path = os.path.join(tmp_dir, \"config.json\")\n+            with open(config_path, \"w\", encoding=\"utf-8\") as f:\n+                json.dump(config_dict, f, indent=2, sort_keys=True)\n+\n+            tokenizer2 = AutoTokenizer.from_pretrained(tmp_dir)\n+\n+        self.assertIsInstance(tokenizer2, tokenizer.__class__)\n+        self.assertTrue(tokenizer2.vocab_size > 100_000)\n+\n     def test_auto_tokenizer_fast_no_slow(self):\n         tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/ctrl\")\n         # There is no fast CTRL so this always gives us a slow tokenizer."
        },
        {
            "sha": "0a5bbcf2edf9ce561e9f176dde5b32bf190f7fb9",
            "filename": "tests/models/llama/test_tokenization_llama.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6055550a15a8fab367cf983b743ff68cc58d81a/tests%2Fmodels%2Fllama%2Ftest_tokenization_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6055550a15a8fab367cf983b743ff68cc58d81a/tests%2Fmodels%2Fllama%2Ftest_tokenization_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_tokenization_llama.py?ref=b6055550a15a8fab367cf983b743ff68cc58d81a",
            "patch": "@@ -49,7 +49,11 @@\n @require_sentencepiece\n @require_tokenizers\n class LlamaTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n-    from_pretrained_id = [\"hf-internal-testing/llama-tokenizer\", \"meta-llama/Llama-2-7b-hf\"]\n+    from_pretrained_id = [\n+        \"hf-internal-testing/llama-tokenizer\",\n+        \"meta-llama/Llama-2-7b-hf\",\n+        \"meta-llama/Meta-Llama-3-8B\",\n+    ]\n     tokenizer_class = LlamaTokenizer\n     rust_tokenizer_class = LlamaTokenizerFast\n "
        },
        {
            "sha": "7c8aede5876396b5b875cea13fc5d00474bc9ead",
            "filename": "tests/test_tokenization_common.py",
            "status": "modified",
            "additions": 22,
            "deletions": 0,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6055550a15a8fab367cf983b743ff68cc58d81a/tests%2Ftest_tokenization_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6055550a15a8fab367cf983b743ff68cc58d81a/tests%2Ftest_tokenization_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_tokenization_common.py?ref=b6055550a15a8fab367cf983b743ff68cc58d81a",
            "patch": "@@ -4670,3 +4670,25 @@ def test_empty_input_string(self):\n                 for return_type, target_type in zip(tokenizer_return_type, output_tensor_type):\n                     output = tokenizer(empty_input_string, return_tensors=return_type)\n                     self.assertEqual(output.input_ids.dtype, target_type)\n+\n+    def test_local_files_only(self):\n+        from transformers import AutoTokenizer\n+\n+        pretrained_list = getattr(self, \"from_pretrained_id\", []) or []\n+        for pretrained_name in pretrained_list:\n+            with self.subTest(f\"AutoTokenizer ({pretrained_name})\"):\n+                # First cache the tokenizer files\n+                try:\n+                    tokenizer_cached = AutoTokenizer.from_pretrained(pretrained_name)\n+\n+                    # Now load with local_files_only=True\n+                    tokenizer_local = AutoTokenizer.from_pretrained(pretrained_name, local_files_only=True)\n+\n+                    # Check that the two tokenizers are identical\n+                    self.assertEqual(tokenizer_cached.get_vocab(), tokenizer_local.get_vocab())\n+                    self.assertEqual(\n+                        tokenizer_cached.all_special_tokens_extended,\n+                        tokenizer_local.all_special_tokens_extended,\n+                    )\n+                except Exception as _:\n+                    pass  # if the pretrained model is not loadable how could it pass locally :)"
        }
    ],
    "stats": {
        "total": 182,
        "additions": 149,
        "deletions": 33
    }
}