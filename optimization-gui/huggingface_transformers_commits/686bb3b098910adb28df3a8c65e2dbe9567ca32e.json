{
    "author": "Cyrilvallez",
    "message": "Remove all expired deprecation cycles (#39725)\n\n* remove all deprecation cycles\n\n* style\n\n* fix\n\n* remove\n\n* remove\n\n* fix\n\n* Update modular_dpt.py\n\n* back\n\n* typo\n\n* typo\n\n* final fix\n\n* remove all args",
    "sha": "686bb3b098910adb28df3a8c65e2dbe9567ca32e",
    "files": [
        {
            "sha": "b62d6e7ed10fefe343720939dbeb36f19e23c7d6",
            "filename": "src/transformers/generation/candidate_generator.py",
            "status": "modified",
            "additions": 1,
            "deletions": 13,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -37,8 +37,6 @@\n     from ..tokenization_utils_base import PreTrainedTokenizerBase\n     from .configuration_utils import GenerationConfig\n \n-from ..utils.deprecation import deprecate_kwarg\n-\n \n class CandidateGenerator:\n     \"\"\"Abstract base class for all candidate generators that can be applied during assisted generation.\"\"\"\n@@ -685,9 +683,6 @@ class AssistantToTargetTranslator:\n             The tokenizer used by the assistant model.\n         target_vocab_size (`int`):\n             The size of the target model's vocabulary. If not provided, will be inferred from the target tokenizer.\n-        assistant_model_device (str, optional): The device on which the assistant model is loaded.\n-                Defaults to \"cpu\".\n-        assistant_model_device (`str`, defaults to \"cpu\"): The device where the assistant model is located. Used for placing tensors.\n         assistant_model (Optional[PreTrainedModel], optional): The assistant model to be used. Defaults to None for backward compatibility.\n         assistant_prune_lm_head (bool): Whether to prune the assistant model's language model\n             head to match the target vocabulary. This is only applicable if `assistant_model` is provided.\n@@ -697,21 +692,17 @@ class AssistantToTargetTranslator:\n     FILTER_VALUE: float = -float(\"Inf\")  # The value used to filter out unmapped tokens in the logits.\n     SUPPRESS_TOKEN_ID: int = -1  # The ID used to mark suppressed tokens in the mapping.\n \n-    @deprecate_kwarg(\"assistant_model_device\", version=\"4.53\")\n     def __init__(\n         self,\n         target_tokenizer: \"PreTrainedTokenizerBase\",\n         assistant_tokenizer: \"PreTrainedTokenizerBase\",\n         target_vocab_size: int,  # required since target_vocab_size can be different from the length of target_tokenizer.get_vocab()\n-        assistant_model_device: str = \"cpu\",\n         assistant_model: Optional[\"PreTrainedModel\"] = None,\n         assistant_prune_lm_head: bool = False,\n     ):\n         self._target_tokenizer: PreTrainedTokenizerBase = target_tokenizer\n         self._assistant_tokenizer: PreTrainedTokenizerBase = assistant_tokenizer\n-        self._assistant_model_device: str = (\n-            assistant_model_device if assistant_model is None else assistant_model.device\n-        )\n+        self._assistant_model_device = assistant_model.device if assistant_model is not None else \"cpu\"\n         self.target_vocab_size: int = target_vocab_size\n         self._assistant_to_target_input_ids, self.target_to_assistant_input_ids = (\n             self._get_assistant_to_target_input_ids()\n@@ -845,13 +836,11 @@ class AssistantVocabTranslatorCache:\n     _cache = weakref.WeakKeyDictionary()\n \n     @classmethod\n-    @deprecate_kwarg(\"assistant_model_device\", version=\"4.53\")\n     def get_translator(\n         cls,\n         target_tokenizer: \"PreTrainedTokenizerBase\",\n         assistant_tokenizer: \"PreTrainedTokenizerBase\",\n         target_vocab_size: int,\n-        assistant_model_device: str = \"cpu\",\n         assistant_model: Optional[\"PreTrainedModel\"] = None,\n         assistant_prune_lm_head: bool = False,\n     ) -> AssistantToTargetTranslator:\n@@ -866,7 +855,6 @@ def get_translator(\n                 target_tokenizer,\n                 assistant_tokenizer,\n                 target_vocab_size,\n-                assistant_model_device,\n                 assistant_model,\n                 assistant_prune_lm_head,\n             )"
        },
        {
            "sha": "697046b3ed9751c13f4c8ed0bafc0d63b126fc10",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -35,7 +35,6 @@\n     is_torch_available,\n     logging,\n )\n-from ..utils.deprecation import deprecate_kwarg\n \n \n if TYPE_CHECKING:\n@@ -576,7 +575,6 @@ def get_generation_mode(self, assistant_model: Optional[\"PreTrainedModel\"] = Non\n                 )\n         return generation_mode\n \n-    @deprecate_kwarg(\"is_init\", version=\"4.54.0\")\n     def validate(self, strict=False):\n         \"\"\"\n         Validates the values of the attributes of the [`GenerationConfig`] instance. Raises exceptions in the presence"
        },
        {
            "sha": "24cc3639ce4cd003aedaae1cfdc936db223a7ea7",
            "filename": "src/transformers/models/align/modeling_align.py",
            "status": "modified",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -33,7 +33,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, auto_docstring, can_return_tuple, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_align import AlignConfig, AlignTextConfig, AlignVisionConfig\n \n \n@@ -621,17 +620,11 @@ def __init__(self, config):\n         self.attention_dropout = config.attention_probs_dropout_prob\n         self.scaling = self.attention_head_size**-0.5\n \n-    @deprecate_kwarg(\"encoder_hidden_states\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n@@ -703,17 +696,11 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"encoder_hidden_states\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n@@ -769,17 +756,11 @@ def __init__(self, config):\n         self.intermediate = AlignTextIntermediate(config)\n         self.output = AlignTextOutput(config)\n \n-    @deprecate_kwarg(\"encoder_hidden_states\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n@@ -813,20 +794,12 @@ def __init__(self, config):\n         self.layer = nn.ModuleList([AlignTextLayer(config) for i in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n \n-    @deprecate_kwarg(\"encoder_hidden_states\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"past_key_values\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"use_cache\", version=\"4.54.0\")\n     @can_return_tuple\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,"
        },
        {
            "sha": "6581e2f18c4450a000a84341b9aa5823b3936d9a",
            "filename": "src/transformers/models/altclip/modeling_altclip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 39,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -33,7 +33,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, auto_docstring, can_return_tuple, logging, torch_int\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_altclip import AltCLIPConfig, AltCLIPTextConfig, AltCLIPVisionConfig\n \n \n@@ -205,17 +204,11 @@ def __init__(self, config, position_embedding_type=None):\n             self.max_position_embeddings = config.max_position_embeddings\n             self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n-    @deprecate_kwarg(\"encoder_hidden_states\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         input_shape = hidden_states.shape[:-1]\n@@ -319,17 +312,11 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"encoder_hidden_states\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n@@ -384,17 +371,11 @@ def __init__(self, config):\n         self.intermediate = AltRobertaIntermediate(config)\n         self.output = AltRobertaOutput(config)\n \n-    @deprecate_kwarg(\"encoder_hidden_states\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n@@ -429,20 +410,12 @@ def __init__(self, config):\n         self.layer = nn.ModuleList([AltRobertaLayer(config) for i in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n \n-    @deprecate_kwarg(\"encoder_hidden_states\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"past_key_values\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"use_cache\", version=\"4.54.0\")\n     @can_return_tuple\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n@@ -1036,10 +1009,6 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n-    @deprecate_kwarg(\"encoder_hidden_states\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"past_key_values\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"use_cache\", version=\"4.54.0\")\n     @auto_docstring\n     # Copied from transformers.models.clap.modeling_clap.ClapTextModel.forward\n     def forward(\n@@ -1050,10 +1019,6 @@ def forward(\n         position_ids: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n-        encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n-        use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n@@ -1139,8 +1104,6 @@ def set_input_embeddings(self, value: nn.Embedding) -> None:\n     def resize_token_embeddings(self, new_num_tokens: Optional[int] = None) -> nn.Embedding:\n         return super().resize_token_embeddings(new_num_tokens)\n \n-    @deprecate_kwarg(\"encoder_hidden_states\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.54.0\")\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -1151,8 +1114,6 @@ def forward(\n         position_ids: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n-        encoder_attention_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,"
        },
        {
            "sha": "4f549bcff6d00f87ce8ea36c8664b9828cd3617b",
            "filename": "src/transformers/models/beit/image_processing_beit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 14,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"Image processor class for Beit.\"\"\"\n \n-from typing import Any, Optional, Union\n+from typing import Optional, Union\n \n import numpy as np\n \n@@ -41,7 +41,6 @@\n     is_vision_available,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import requires\n \n \n@@ -101,7 +100,6 @@ class BeitImageProcessor(BaseImageProcessor):\n \n     model_input_names = [\"pixel_values\"]\n \n-    @deprecate_kwarg(\"reduce_labels\", new_name=\"do_reduce_labels\", version=\"4.41.0\")\n     @filter_out_non_signature_kwargs(extra=INIT_SERVICE_KWARGS)\n     def __init__(\n         self,\n@@ -135,16 +133,6 @@ def __init__(\n         self.image_std = image_std if image_std is not None else IMAGENET_STANDARD_STD\n         self.do_reduce_labels = do_reduce_labels\n \n-    @classmethod\n-    def from_dict(cls, image_processor_dict: dict[str, Any], **kwargs):\n-        \"\"\"\n-        Overrides the `from_dict` method from the base class to save support of deprecated `reduce_labels` in old configs\n-        \"\"\"\n-        image_processor_dict = image_processor_dict.copy()\n-        if \"reduce_labels\" in image_processor_dict:\n-            image_processor_dict[\"do_reduce_labels\"] = image_processor_dict.pop(\"reduce_labels\")\n-        return super().from_dict(image_processor_dict, **kwargs)\n-\n     def resize(\n         self,\n         image: np.ndarray,\n@@ -313,7 +301,6 @@ def __call__(self, images, segmentation_maps=None, **kwargs):\n         # be passed in as positional arguments.\n         return super().__call__(images, segmentation_maps=segmentation_maps, **kwargs)\n \n-    @deprecate_kwarg(\"reduce_labels\", new_name=\"do_reduce_labels\", version=\"4.41.0\")\n     @filter_out_non_signature_kwargs()\n     def preprocess(\n         self,"
        },
        {
            "sha": "3b7b7efd7a943770d617306006c1d7ed67ad4976",
            "filename": "src/transformers/models/beit/image_processing_beit_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 11,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for Beit.\"\"\"\n \n-from typing import Any, Optional, Union\n+from typing import Optional, Union\n \n import torch\n from torchvision.transforms import functional as F\n@@ -68,16 +68,6 @@ class BeitImageProcessorFast(BaseImageProcessorFast):\n     def __init__(self, **kwargs: Unpack[BeitFastImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n-    @classmethod\n-    def from_dict(cls, image_processor_dict: dict[str, Any], **kwargs):\n-        \"\"\"\n-        Overrides the `from_dict` method from the base class to save support of deprecated `reduce_labels` in old configs\n-        \"\"\"\n-        image_processor_dict = image_processor_dict.copy()\n-        if \"reduce_labels\" in image_processor_dict:\n-            image_processor_dict[\"do_reduce_labels\"] = image_processor_dict.pop(\"reduce_labels\")\n-        return super().from_dict(image_processor_dict, **kwargs)\n-\n     def reduce_label(self, labels: list[\"torch.Tensor\"]):\n         for idx in range(len(labels)):\n             label = labels[idx]"
        },
        {
            "sha": "97387cdfc0707ac39826fc5c579da569054e1ef9",
            "filename": "src/transformers/models/bert/modeling_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -46,7 +46,6 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, auto_docstring, get_torch_version, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_bert import BertConfig\n \n \n@@ -218,14 +217,12 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -236,13 +233,7 @@ def forward(\n             1, 2\n         )\n \n-        # If this is instantiated as a cross-attention module, the keys\n-        # and values come from an encoder; the attention mask needs to be\n-        # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n-        if is_cross_attention and encoder_attention_mask is not None:\n-            attention_mask = encoder_attention_mask\n-\n         if past_key_value is not None:\n             if isinstance(past_key_value, EncoderDecoderCache):\n                 is_updated = past_key_value.is_updated.get(self.layer_idx)\n@@ -336,14 +327,12 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.require_contiguous_qkv = version.parse(get_torch_version()) < version.parse(\"2.2.0\")\n \n     # Adapted from BertSelfAttention\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -362,7 +351,6 @@ def forward(\n                 attention_mask,\n                 head_mask,\n                 encoder_hidden_states,\n-                encoder_attention_mask,\n                 past_key_value,\n                 output_attentions,\n                 cache_position,\n@@ -374,12 +362,7 @@ def forward(\n             self.query(hidden_states).view(bsz, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n         )\n \n-        # If this is instantiated as a cross-attention module, the keys and values come from an encoder; the attention\n-        # mask needs to be such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n-        if is_cross_attention and encoder_attention_mask is not None:\n-            attention_mask = encoder_attention_mask\n-\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if past_key_value is not None:\n             if isinstance(past_key_value, EncoderDecoderCache):\n@@ -499,14 +482,12 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -516,7 +497,6 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n             past_key_value=past_key_value,\n             output_attentions=output_attentions,\n             cache_position=cache_position,"
        },
        {
            "sha": "41f4b972213cce556bc71d2ddd4d2202c9a9cee1",
            "filename": "src/transformers/models/bert_generation/modeling_bert_generation.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -29,7 +29,6 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_bert_generation import BertGenerationConfig\n \n \n@@ -80,14 +79,12 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -98,13 +95,7 @@ def forward(\n             1, 2\n         )\n \n-        # If this is instantiated as a cross-attention module, the keys\n-        # and values come from an encoder; the attention mask needs to be\n-        # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n-        if is_cross_attention and encoder_attention_mask is not None:\n-            attention_mask = encoder_attention_mask\n-\n         if past_key_value is not None:\n             if isinstance(past_key_value, EncoderDecoderCache):\n                 is_updated = past_key_value.is_updated.get(self.layer_idx)\n@@ -226,14 +217,12 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -243,7 +232,6 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n             past_key_value=past_key_value,\n             output_attentions=output_attentions,\n             cache_position=cache_position,"
        },
        {
            "sha": "d4f3f2373ba2c82cdc4faf703862bf666b61c0f0",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -24,7 +24,6 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -37,7 +36,6 @@\n from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring, logging, torch_int\n-from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModelForCausalLM, AutoModelForSeq2SeqLM\n from .configuration_blip_2 import Blip2Config, Blip2QFormerConfig, Blip2VisionConfig\n \n@@ -642,15 +640,13 @@ def transpose_for_scores(self, x):\n         x = x.view(*new_x_shape)\n         return x.permute(0, 2, 1, 3)\n \n-    @deprecate_kwarg(\"past_key_value\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        past_key_value=None,\n         output_attentions=False,\n     ):\n         # If this is instantiated as a cross-attention module, the keys\n@@ -767,15 +763,13 @@ def prune_heads(self, heads):\n         self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"past_key_value\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.attention(\n@@ -844,15 +838,13 @@ def __init__(self, config, layer_idx):\n         self.intermediate_query = Blip2QFormerIntermediate(config)\n         self.output_query = Blip2QFormerOutput(config)\n \n-    @deprecate_kwarg(\"past_key_value\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        past_key_value=None,\n         output_attentions=False,\n         query_length=0,\n     ):\n@@ -929,17 +921,13 @@ def __init__(self, config):\n         )\n         self.gradient_checkpointing = False\n \n-    @deprecate_kwarg(\"past_key_value\", version=\"4.55.0\")\n-    @deprecate_kwarg(\"use_cache\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        past_key_values=None,\n-        use_cache=None,\n         output_attentions=False,\n         output_hidden_states=False,\n         return_dict=True,\n@@ -1119,8 +1107,6 @@ def get_extended_attention_mask(\n         extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n         return extended_attention_mask\n \n-    @deprecate_kwarg(\"past_key_value\", version=\"4.55.0\")\n-    @deprecate_kwarg(\"use_cache\", version=\"4.55.0\")\n     @auto_docstring\n     def forward(\n         self,\n@@ -1130,8 +1116,6 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,"
        },
        {
            "sha": "644b8b0a3b6fa51493419f96be15bc23906d6e04",
            "filename": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -37,7 +37,6 @@\n from ...modeling_utils import PreTrainedModel, apply_chunking_to_forward\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, logging, torch_int\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_bridgetower import BridgeTowerConfig, BridgeTowerTextConfig, BridgeTowerVisionConfig\n \n \n@@ -430,14 +429,12 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -448,13 +445,7 @@ def forward(\n             1, 2\n         )\n \n-        # If this is instantiated as a cross-attention module, the keys\n-        # and values come from an encoder; the attention mask needs to be\n-        # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n-        if is_cross_attention and encoder_attention_mask is not None:\n-            attention_mask = encoder_attention_mask\n-\n         if past_key_value is not None:\n             if isinstance(past_key_value, EncoderDecoderCache):\n                 is_updated = past_key_value.is_updated.get(self.layer_idx)\n@@ -576,14 +567,12 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -593,7 +582,6 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n             past_key_value=past_key_value,\n             output_attentions=output_attentions,\n             cache_position=cache_position,"
        },
        {
            "sha": "f12b47081d6a433b01b8423268e5ee83d5f18d5b",
            "filename": "src/transformers/models/bros/modeling_bros.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -33,7 +33,6 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, auto_docstring, can_return_tuple, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_bros import BrosConfig\n \n \n@@ -208,7 +207,6 @@ def __init__(self, config):\n \n         self.is_decoder = config.is_decoder\n \n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -217,7 +215,6 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[torch.Tensor] = False,\n     ) -> tuple[torch.Tensor]:\n         hidden_shape = (hidden_states.shape[0], -1, self.num_attention_heads, self.attention_head_size)\n@@ -336,7 +333,6 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -345,7 +341,6 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n@@ -407,7 +402,6 @@ def __init__(self, config):\n         self.intermediate = BrosIntermediate(config)\n         self.output = BrosOutput(config)\n \n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -416,7 +410,6 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         self_attention_outputs = self.attention(\n@@ -477,8 +470,6 @@ def __init__(self, config):\n         self.config = config\n         self.layer = nn.ModuleList([BrosLayer(config) for _ in range(config.num_hidden_layers)])\n \n-    @deprecate_kwarg(\"past_key_values\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"use_cache\", version=\"4.54.0\")\n     @can_return_tuple\n     def forward(\n         self,\n@@ -488,8 +479,6 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n@@ -638,8 +627,6 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n-    @deprecate_kwarg(\"past_key_values\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"use_cache\", version=\"4.54.0\")\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -653,8 +640,6 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n-        use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,"
        },
        {
            "sha": "e7295051860180844f30697a99b812177c9dbe3f",
            "filename": "src/transformers/models/camembert/modeling_camembert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -42,7 +42,6 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, get_torch_version, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_camembert import CamembertConfig\n \n \n@@ -168,14 +167,12 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -186,13 +183,7 @@ def forward(\n             1, 2\n         )\n \n-        # If this is instantiated as a cross-attention module, the keys\n-        # and values come from an encoder; the attention mask needs to be\n-        # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n-        if is_cross_attention and encoder_attention_mask is not None:\n-            attention_mask = encoder_attention_mask\n-\n         if past_key_value is not None:\n             if isinstance(past_key_value, EncoderDecoderCache):\n                 is_updated = past_key_value.is_updated.get(self.layer_idx)\n@@ -287,14 +278,12 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.require_contiguous_qkv = version.parse(get_torch_version()) < version.parse(\"2.2.0\")\n \n     # Adapted from CamembertSelfAttention\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -313,7 +302,6 @@ def forward(\n                 attention_mask,\n                 head_mask,\n                 encoder_hidden_states,\n-                encoder_attention_mask,\n                 past_key_value,\n                 output_attentions,\n                 cache_position,\n@@ -325,12 +313,7 @@ def forward(\n             self.query(hidden_states).view(bsz, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n         )\n \n-        # If this is instantiated as a cross-attention module, the keys and values come from an encoder; the attention\n-        # mask needs to be such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n-        if is_cross_attention and encoder_attention_mask is not None:\n-            attention_mask = encoder_attention_mask\n-\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if past_key_value is not None:\n             if isinstance(past_key_value, EncoderDecoderCache):\n@@ -452,14 +435,12 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -469,7 +450,6 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n             past_key_value=past_key_value,\n             output_attentions=output_attentions,\n             cache_position=cache_position,"
        },
        {
            "sha": "6d2452242d52e2cbe1a64d56179f7be9c8137191",
            "filename": "src/transformers/models/chinese_clip/modeling_chinese_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -31,7 +31,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, auto_docstring, can_return_tuple, logging, torch_int\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_chinese_clip import ChineseCLIPConfig, ChineseCLIPTextConfig, ChineseCLIPVisionConfig\n \n \n@@ -288,17 +287,11 @@ def __init__(self, config):\n         self.attention_dropout = config.attention_probs_dropout_prob\n         self.scaling = self.attention_head_size**-0.5\n \n-    @deprecate_kwarg(\"encoder_hidden_states\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n@@ -371,17 +364,11 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"encoder_hidden_states\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n@@ -509,17 +496,11 @@ def __init__(self, config):\n         self.intermediate = ChineseCLIPTextIntermediate(config)\n         self.output = ChineseCLIPTextOutput(config)\n \n-    @deprecate_kwarg(\"encoder_hidden_states\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n@@ -667,20 +648,12 @@ def __init__(self, config):\n         self.layer = nn.ModuleList([ChineseCLIPTextLayer(config) for i in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n \n-    @deprecate_kwarg(\"encoder_hidden_states\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"past_key_values\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"use_cache\", version=\"4.54.0\")\n     @can_return_tuple\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,"
        },
        {
            "sha": "cf52cff8064ae77dbd4bfdd1caa5ab6d97e7c167",
            "filename": "src/transformers/models/clap/modeling_clap.py",
            "status": "modified",
            "additions": 0,
            "deletions": 35,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -33,7 +33,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, meshgrid, prune_linear_layer\n from ...utils import ModelOutput, auto_docstring, can_return_tuple, logging, torch_int\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_clap import ClapAudioConfig, ClapConfig, ClapTextConfig\n \n \n@@ -1137,17 +1136,11 @@ def __init__(self, config):\n         self.attention_dropout = config.attention_probs_dropout_prob\n         self.scaling = self.attention_head_size**-0.5\n \n-    @deprecate_kwarg(\"encoder_hidden_states\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n@@ -1220,17 +1213,11 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"encoder_hidden_states\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n@@ -1287,17 +1274,11 @@ def __init__(self, config):\n         self.intermediate = ClapTextIntermediate(config)\n         self.output = ClapTextOutput(config)\n \n-    @deprecate_kwarg(\"encoder_hidden_states\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n@@ -1332,20 +1313,12 @@ def __init__(self, config):\n         self.layer = nn.ModuleList([ClapTextLayer(config) for i in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n \n-    @deprecate_kwarg(\"encoder_hidden_states\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"past_key_values\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"use_cache\", version=\"4.54.0\")\n     @can_return_tuple\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n@@ -1526,10 +1499,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    @deprecate_kwarg(\"encoder_hidden_states\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"past_key_values\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"use_cache\", version=\"4.54.0\")\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -1540,10 +1509,6 @@ def forward(\n         position_ids: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n-        encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n-        use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,"
        },
        {
            "sha": "4472251a9d05d0a21672d63fdb7f33bdae569497",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -35,7 +35,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_cohere2 import Cohere2Config\n \n@@ -264,7 +263,6 @@ def __init__(self, config: Cohere2Config, layer_idx: int):\n         self.input_layernorm = Cohere2LayerNorm(hidden_size=(config.hidden_size), eps=config.layer_norm_eps)\n         self.attention_type = config.layer_types[layer_idx]\n \n-    @deprecate_kwarg(\"last_cache_position\", version=\"4.53.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "f0fa8f12ac5beca0b5ac52b6cbfddb3d837e402c",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -28,7 +28,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ..cohere.modeling_cohere import (\n     CohereAttention,\n     CohereDecoderLayer,\n@@ -348,7 +347,6 @@ def __init__(self, config: Cohere2Config, layer_idx: int):\n         super().__init__(config, layer_idx)\n         self.attention_type = config.layer_types[layer_idx]\n \n-    @deprecate_kwarg(\"last_cache_position\", version=\"4.53.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "c29bf8f6c1ddaae8538fee3ef3a903f731987b28",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -45,7 +45,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, is_peft_available, is_torch_flex_attn_available\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_data2vec_audio import Data2VecAudioConfig\n \n \n@@ -241,7 +240,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "8e0f308faf9d98b7eebde6468e733dd9d489928b",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -39,7 +39,6 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_data2vec_text import Data2VecTextConfig\n \n \n@@ -168,14 +167,12 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -186,13 +183,7 @@ def forward(\n             1, 2\n         )\n \n-        # If this is instantiated as a cross-attention module, the keys\n-        # and values come from an encoder; the attention mask needs to be\n-        # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n-        if is_cross_attention and encoder_attention_mask is not None:\n-            attention_mask = encoder_attention_mask\n-\n         if past_key_value is not None:\n             if isinstance(past_key_value, EncoderDecoderCache):\n                 is_updated = past_key_value.is_updated.get(self.layer_idx)\n@@ -329,14 +320,12 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -346,7 +335,6 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n             past_key_value=past_key_value,\n             output_attentions=output_attentions,\n             cache_position=cache_position,"
        },
        {
            "sha": "540b6136bb6390af1012270010c37c6217d432b5",
            "filename": "src/transformers/models/decision_transformer/modeling_decision_transformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -34,7 +34,6 @@\n     auto_docstring,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_decision_transformer import DecisionTransformerConfig\n \n \n@@ -256,7 +255,6 @@ def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None, hea\n \n         return attn_output, attn_weights\n \n-    @deprecate_kwarg(\"layer_past\", new_name=\"past_key_value\", version=\"4.53.0\", raise_if_both_names=True)\n     def forward(\n         self,\n         hidden_states: Optional[tuple[torch.FloatTensor]],\n@@ -380,7 +378,6 @@ def __init__(self, config, layer_idx=None):\n \n         self.mlp = DecisionTransformerGPT2MLP(inner_dim, config)\n \n-    @deprecate_kwarg(\"layer_past\", new_name=\"past_key_value\", version=\"4.53.0\", raise_if_both_names=True)\n     def forward(\n         self,\n         hidden_states: Optional[tuple[torch.FloatTensor]],"
        },
        {
            "sha": "584181701b65e261eb5fd803ed7a784fc8ff35c6",
            "filename": "src/transformers/models/dpt/modular_dpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodular_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodular_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodular_dpt.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -139,9 +139,6 @@ class DPTImageProcessorFast(BeitImageProcessorFast):\n \n     valid_kwargs = DPTFastImageProcessorKwargs\n \n-    def from_dict():\n-        raise NotImplementedError(\"No need to override this method\")\n-\n     def resize(\n         self,\n         image: \"torch.Tensor\","
        },
        {
            "sha": "67dbe02c2074a1a93a0a1045b1d568e519ea9c41",
            "filename": "src/transformers/models/electra/modeling_electra.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -41,7 +41,6 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_electra import ElectraConfig\n \n \n@@ -225,14 +224,12 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -243,13 +240,7 @@ def forward(\n             1, 2\n         )\n \n-        # If this is instantiated as a cross-attention module, the keys\n-        # and values come from an encoder; the attention mask needs to be\n-        # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n-        if is_cross_attention and encoder_attention_mask is not None:\n-            attention_mask = encoder_attention_mask\n-\n         if past_key_value is not None:\n             if isinstance(past_key_value, EncoderDecoderCache):\n                 is_updated = past_key_value.is_updated.get(self.layer_idx)\n@@ -386,14 +377,12 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -403,7 +392,6 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n             past_key_value=past_key_value,\n             output_attentions=output_attentions,\n             cache_position=cache_position,"
        },
        {
            "sha": "f2aaa897237e0c04fb1d11520600c87b8819e23c",
            "filename": "src/transformers/models/ernie/modeling_ernie.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -42,7 +42,6 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_ernie import ErnieConfig\n \n \n@@ -154,14 +153,12 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -172,13 +169,7 @@ def forward(\n             1, 2\n         )\n \n-        # If this is instantiated as a cross-attention module, the keys\n-        # and values come from an encoder; the attention mask needs to be\n-        # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n-        if is_cross_attention and encoder_attention_mask is not None:\n-            attention_mask = encoder_attention_mask\n-\n         if past_key_value is not None:\n             if isinstance(past_key_value, EncoderDecoderCache):\n                 is_updated = past_key_value.is_updated.get(self.layer_idx)\n@@ -315,14 +306,12 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -332,7 +321,6 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n             past_key_value=past_key_value,\n             output_attentions=output_attentions,\n             cache_position=cache_position,"
        },
        {
            "sha": "9acc3625bd69371f8db549cb2ccf15dbe64a976b",
            "filename": "src/transformers/models/esm/modeling_esm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 17,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -34,7 +34,6 @@\n )\n from ...modeling_utils import PreTrainedModel, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, can_return_tuple, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_esm import EsmConfig\n \n \n@@ -287,15 +286,13 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         hidden_shape = (hidden_states.shape[0], -1, self.num_attention_heads, self.attention_head_size)\n@@ -400,15 +397,13 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n         self.dropout_prob = config.attention_probs_dropout_prob\n \n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         # Flash attention doesn't support output_attentions or cross attention\n@@ -530,15 +525,13 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        past_key_value=None,\n         output_attentions=False,\n         cache_position=None,\n     ):\n@@ -596,15 +589,13 @@ def __init__(self, config):\n         self.output = EsmOutput(config)\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        past_key_value=None,\n         output_attentions=False,\n         cache_position=None,\n     ):\n@@ -664,8 +655,6 @@ def __init__(self, config):\n         self.emb_layer_norm_after = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.gradient_checkpointing = False\n \n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"use_cache\", version=\"4.54.0\")\n     @can_return_tuple\n     def forward(\n         self,\n@@ -674,8 +663,6 @@ def forward(\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        past_key_values=None,\n-        use_cache=None,\n         output_attentions=False,\n         output_hidden_states=False,\n         return_dict=True,\n@@ -818,8 +805,6 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n-    @deprecate_kwarg(\"past_key_values\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"use_cache\", version=\"4.54.0\")\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -831,8 +816,6 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n-        use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,"
        },
        {
            "sha": "d65653adbc2d3d9873816d552ea7c4b8663cc84a",
            "filename": "src/transformers/models/evolla/modeling_evolla.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -52,7 +52,6 @@\n )\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_evolla import EvollaConfig, SaProtConfig\n \n@@ -265,15 +264,13 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         hidden_shape = (hidden_states.shape[0], -1, self.num_attention_heads, self.attention_head_size)\n@@ -378,15 +375,13 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n         self.dropout_prob = config.attention_probs_dropout_prob\n \n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         # Flash attention doesn't support output_attentions or cross attention\n@@ -508,15 +503,13 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        past_key_value=None,\n         output_attentions=False,\n         cache_position=None,\n     ):\n@@ -581,15 +574,13 @@ def __init__(self, config):\n         self.output = EvollaSaProtOutput(config)\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        past_key_value=None,\n         output_attentions=False,\n         cache_position=None,\n     ):\n@@ -649,8 +640,6 @@ def __init__(self, config):\n         self.emb_layer_norm_after = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.gradient_checkpointing = False\n \n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"use_cache\", version=\"4.54.0\")\n     @can_return_tuple\n     def forward(\n         self,\n@@ -659,8 +648,6 @@ def forward(\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        past_key_values=None,\n-        use_cache=None,\n         output_attentions=False,\n         output_hidden_states=False,\n         return_dict=True,"
        },
        {
            "sha": "a76e390012b518290e68d0b2e6136153246db052",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -39,7 +39,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_gemma2 import Gemma2Config\n \n@@ -252,7 +251,6 @@ def __init__(self, config: Gemma2Config, layer_idx: int):\n         self.pre_feedforward_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_feedforward_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"last_cache_position\", version=\"4.53.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "1f22987e67af3db40c46baae79ff0ddf49e5fee3",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -29,7 +29,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ..gemma.modeling_gemma import (\n     GemmaAttention,\n     GemmaForCausalLM,\n@@ -317,7 +316,6 @@ def __init__(self, config: Gemma2Config, layer_idx: int):\n         self.pre_feedforward_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_feedforward_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"last_cache_position\", version=\"4.53.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "76eb0b697bda2fc33dfab1b71be587bcac2ecc21",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -46,7 +46,6 @@\n     is_torchdynamo_compiling,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from ..auto import AutoModel\n from .configuration_gemma3 import Gemma3Config, Gemma3TextConfig\n@@ -366,7 +365,6 @@ def __init__(self, config: Gemma3TextConfig, layer_idx: int):\n         self.pre_feedforward_layernorm = Gemma3RMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n         self.post_feedforward_layernorm = Gemma3RMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"last_cache_position\", version=\"4.53.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "c2ad52f108095fb478bd07804714bea8336110be",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -32,7 +32,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ..gemma2.configuration_gemma2 import Gemma2Config\n from ..gemma2.modeling_gemma2 import (\n     Gemma2Attention,\n@@ -464,7 +463,6 @@ def __init__(self, config: Gemma3TextConfig, layer_idx: int):\n         self.pre_feedforward_layernorm = Gemma3RMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n         self.post_feedforward_layernorm = Gemma3RMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"last_cache_position\", version=\"4.53.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "8a0067548767ce41645f4c771c421723873c9573",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -47,7 +47,6 @@\n     is_torchdynamo_compiling,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel\n from .configuration_gemma3n import Gemma3nAudioConfig, Gemma3nConfig, Gemma3nTextConfig, Gemma3nVisionConfig\n \n@@ -1408,7 +1407,6 @@ def __init__(self, config: Gemma3nTextConfig, layer_idx: int):\n         self.per_layer_projection = nn.Linear(self.hidden_size_per_layer_input, self.hidden_size, bias=False)\n         self.post_per_layer_input_norm = Gemma3nRMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"last_cache_position\", version=\"4.53.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "c3155b17eae38fe4f606fb4be548e38b9d4a08df",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -46,7 +46,6 @@\n     auto_docstring,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.model_parallel_utils import assert_device_map, get_device_map\n from .configuration_gpt2 import GPT2Config\n \n@@ -266,7 +265,6 @@ def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None, hea\n \n         return attn_output, attn_weights\n \n-    @deprecate_kwarg(\"layer_past\", new_name=\"past_key_value\", version=\"4.53.0\", raise_if_both_names=True)\n     def forward(\n         self,\n         hidden_states: Optional[tuple[torch.FloatTensor]],\n@@ -385,7 +383,6 @@ def __init__(self, config, layer_idx=None):\n \n         self.mlp = GPT2MLP(inner_dim, config)\n \n-    @deprecate_kwarg(\"layer_past\", new_name=\"past_key_value\", version=\"4.53.0\", raise_if_both_names=True)\n     def forward(\n         self,\n         hidden_states: Optional[tuple[torch.FloatTensor]],"
        },
        {
            "sha": "3bd0b988069f3cafff7875717b71ce888f528132",
            "filename": "src/transformers/models/grounding_dino/processing_grounding_dino.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fprocessing_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fprocessing_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fprocessing_grounding_dino.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -26,7 +26,6 @@\n from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import BatchEncoding, PreTokenizedInput, TextInput\n from ...utils import TensorType, is_torch_available\n-from ...utils.deprecation import deprecate_kwarg\n \n \n if is_torch_available():\n@@ -239,7 +238,6 @@ def model_input_names(self):\n         image_processor_input_names = self.image_processor.model_input_names\n         return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n \n-    @deprecate_kwarg(\"box_threshold\", new_name=\"threshold\", version=\"4.51.0\")\n     def post_process_grounded_object_detection(\n         self,\n         outputs: \"GroundingDinoObjectDetectionOutput\","
        },
        {
            "sha": "19f3302c5a8117ecabea2064c42191c9adef954f",
            "filename": "src/transformers/models/hubert/modeling_hubert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -37,7 +37,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_hubert import HubertConfig\n \n \n@@ -301,7 +300,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "55d7d32f0ff0065ee3c94313869f15d6418ea995",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -23,7 +23,6 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -37,7 +36,6 @@\n from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple, logging, torch_int\n-from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n from .configuration_instructblip import InstructBlipConfig, InstructBlipQFormerConfig, InstructBlipVisionConfig\n \n@@ -558,15 +556,13 @@ def transpose_for_scores(self, x):\n         x = x.view(*new_x_shape)\n         return x.permute(0, 2, 1, 3)\n \n-    @deprecate_kwarg(\"past_key_value\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        past_key_value=None,\n         output_attentions=False,\n     ):\n         # If this is instantiated as a cross-attention module, the keys\n@@ -679,15 +675,13 @@ def prune_heads(self, heads):\n         self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"past_key_value\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.attention(\n@@ -755,15 +749,13 @@ def __init__(self, config, layer_idx):\n         self.intermediate_query = InstructBlipQFormerIntermediate(config)\n         self.output_query = InstructBlipQFormerOutput(config)\n \n-    @deprecate_kwarg(\"past_key_value\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        past_key_value=None,\n         output_attentions=False,\n         query_length=0,\n     ):\n@@ -841,17 +833,13 @@ def __init__(self, config):\n         )\n         self.gradient_checkpointing = False\n \n-    @deprecate_kwarg(\"past_key_value\", version=\"4.55.0\")\n-    @deprecate_kwarg(\"use_cache\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        past_key_values=None,\n-        use_cache=None,\n         output_attentions=False,\n         output_hidden_states=False,\n         return_dict=True,\n@@ -1035,8 +1023,6 @@ def get_extended_attention_mask(\n         extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n         return extended_attention_mask\n \n-    @deprecate_kwarg(\"past_key_value\", version=\"4.55.0\")\n-    @deprecate_kwarg(\"use_cache\", version=\"4.55.0\")\n     def forward(\n         self,\n         input_ids: torch.LongTensor,\n@@ -1046,8 +1032,6 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,"
        },
        {
            "sha": "6f5b0f8f0493826a9e9e604aa60062a8221f821f",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -27,7 +27,6 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -41,7 +40,6 @@\n from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple, logging, torch_int\n-from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n from .configuration_instructblipvideo import (\n     InstructBlipVideoConfig,\n@@ -423,15 +421,13 @@ def transpose_for_scores(self, x):\n         x = x.view(*new_x_shape)\n         return x.permute(0, 2, 1, 3)\n \n-    @deprecate_kwarg(\"past_key_value\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        past_key_value=None,\n         output_attentions=False,\n     ):\n         # If this is instantiated as a cross-attention module, the keys\n@@ -542,15 +538,13 @@ def prune_heads(self, heads):\n         self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"past_key_value\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.attention(\n@@ -616,15 +610,13 @@ def __init__(self, config, layer_idx):\n         self.intermediate_query = InstructBlipVideoQFormerIntermediate(config)\n         self.output_query = InstructBlipVideoQFormerOutput(config)\n \n-    @deprecate_kwarg(\"past_key_value\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        past_key_value=None,\n         output_attentions=False,\n         query_length=0,\n     ):\n@@ -701,17 +693,13 @@ def __init__(self, config):\n         )\n         self.gradient_checkpointing = False\n \n-    @deprecate_kwarg(\"past_key_value\", version=\"4.55.0\")\n-    @deprecate_kwarg(\"use_cache\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        past_key_values=None,\n-        use_cache=None,\n         output_attentions=False,\n         output_hidden_states=False,\n         return_dict=True,\n@@ -996,8 +984,6 @@ def get_extended_attention_mask(\n         extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n         return extended_attention_mask\n \n-    @deprecate_kwarg(\"past_key_value\", version=\"4.55.0\")\n-    @deprecate_kwarg(\"use_cache\", version=\"4.55.0\")\n     def forward(\n         self,\n         input_ids: torch.LongTensor,\n@@ -1007,8 +993,6 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,"
        },
        {
            "sha": "b3b79ef99d38cc1e2932e4d759f8719fd739cfc3",
            "filename": "src/transformers/models/layoutlm/modeling_layoutlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 35,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_layoutlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_layoutlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_layoutlm.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -34,7 +34,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, can_return_tuple, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_layoutlm import LayoutLMConfig\n \n \n@@ -171,17 +170,11 @@ def __init__(self, config):\n         self.attention_dropout = config.attention_probs_dropout_prob\n         self.scaling = self.attention_head_size**-0.5\n \n-    @deprecate_kwarg(\"encoder_hidden_states\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n@@ -254,17 +247,11 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"encoder_hidden_states\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n@@ -321,17 +308,11 @@ def __init__(self, config):\n         self.intermediate = LayoutLMIntermediate(config)\n         self.output = LayoutLMOutput(config)\n \n-    @deprecate_kwarg(\"encoder_hidden_states\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n@@ -366,20 +347,12 @@ def __init__(self, config):\n         self.layer = nn.ModuleList([LayoutLMLayer(config) for i in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n \n-    @deprecate_kwarg(\"encoder_hidden_states\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"past_key_values\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"use_cache\", version=\"4.54.0\")\n     @can_return_tuple\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n@@ -537,8 +510,6 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n-    @deprecate_kwarg(\"encoder_hidden_states\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.54.0\")\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -550,8 +521,6 @@ def forward(\n         position_ids: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n@@ -684,8 +653,6 @@ def set_output_embeddings(self, new_embeddings):\n         self.cls.predictions.decoder = new_embeddings\n         self.cls.predictions.bias = new_embeddings.bias\n \n-    @deprecate_kwarg(\"encoder_hidden_states\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.54.0\")\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -698,8 +665,6 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,"
        },
        {
            "sha": "0dd845ecff3cff62f6e83dbd2dcaa398916359ab",
            "filename": "src/transformers/models/markuplm/modeling_markuplm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -40,7 +40,6 @@\n     prune_linear_layer,\n )\n from ...utils import auto_docstring, can_return_tuple, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_markuplm import MarkupLMConfig\n \n \n@@ -378,17 +377,11 @@ def __init__(self, config):\n         self.attention_dropout = config.attention_probs_dropout_prob\n         self.scaling = self.attention_head_size**-0.5\n \n-    @deprecate_kwarg(\"encoder_hidden_states\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n@@ -446,17 +439,11 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"encoder_hidden_states\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n@@ -482,17 +469,11 @@ def __init__(self, config):\n         self.intermediate = MarkupLMIntermediate(config)\n         self.output = MarkupLMOutput(config)\n \n-    @deprecate_kwarg(\"encoder_hidden_states\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n@@ -527,20 +508,12 @@ def __init__(self, config):\n         self.layer = nn.ModuleList([MarkupLMLayer(config) for i in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n \n-    @deprecate_kwarg(\"encoder_hidden_states\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"past_key_values\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"use_cache\", version=\"4.54.0\")\n     @can_return_tuple\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,"
        },
        {
            "sha": "2ccc3bb67bf71905b037282872e1f089c1083121",
            "filename": "src/transformers/models/mask2former/image_processing_mask2former.py",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -50,7 +50,6 @@\n     is_torch_tensor,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n \n \n logger = logging.get_logger(__name__)\n@@ -442,9 +441,6 @@ class Mask2FormerImageProcessor(BaseImageProcessor):\n \n     model_input_names = [\"pixel_values\", \"pixel_mask\"]\n \n-    @deprecate_kwarg(\"reduce_labels\", new_name=\"do_reduce_labels\", version=\"4.44.0\")\n-    @deprecate_kwarg(\"size_divisibility\", new_name=\"size_divisor\", version=\"4.41.0\")\n-    @deprecate_kwarg(\"max_size\", version=\"4.27.0\", warn_if_greater_or_equal_version=True)\n     @filter_out_non_signature_kwargs(extra=[\"max_size\", *INIT_SERVICE_KWARGS])\n     def __init__(\n         self,\n@@ -486,21 +482,6 @@ def __init__(\n         self.num_labels = num_labels\n         self.pad_size = pad_size\n \n-    @classmethod\n-    def from_dict(cls, image_processor_dict: dict[str, Any], **kwargs):\n-        \"\"\"\n-        Overrides the `from_dict` method from the base class to make sure parameters are updated if image processor is\n-        created using from_dict and kwargs e.g. `Mask2FormerImageProcessor.from_pretrained(checkpoint, max_size=800)`\n-        \"\"\"\n-        image_processor_dict = image_processor_dict.copy()\n-        if \"max_size\" in kwargs:\n-            image_processor_dict[\"max_size\"] = kwargs.pop(\"max_size\")\n-        if \"size_divisibility\" in kwargs:\n-            image_processor_dict[\"size_divisor\"] = kwargs.pop(\"size_divisibility\")\n-        if \"reduce_labels\" in image_processor_dict:\n-            image_processor_dict[\"do_reduce_labels\"] = image_processor_dict.pop(\"reduce_labels\")\n-        return super().from_dict(image_processor_dict, **kwargs)\n-\n     # Copied from transformers.models.maskformer.image_processing_maskformer.MaskFormerImageProcessor.to_dict\n     def to_dict(self) -> dict[str, Any]:\n         \"\"\"\n@@ -511,7 +492,6 @@ def to_dict(self) -> dict[str, Any]:\n         image_processor_dict.pop(\"_max_size\", None)\n         return image_processor_dict\n \n-    @deprecate_kwarg(\"max_size\", version=\"4.27.0\", warn_if_greater_or_equal_version=True)\n     # Copied from transformers.models.maskformer.image_processing_maskformer.MaskFormerImageProcessor.resize with get_maskformer_resize_output_image_size->get_mask2former_resize_output_image_size\n     def resize(\n         self,\n@@ -722,7 +702,6 @@ def _preprocess_mask(\n             segmentation_map = segmentation_map.squeeze(0)\n         return segmentation_map\n \n-    @deprecate_kwarg(\"reduce_labels\", new_name=\"do_reduce_labels\", version=\"4.44.0\")\n     @filter_out_non_signature_kwargs()\n     def preprocess(\n         self,"
        },
        {
            "sha": "0b942a326423b9b0609399731b018c509987fc05",
            "filename": "src/transformers/models/mask2former/image_processing_mask2former_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 19,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former_fast.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -47,7 +47,6 @@\n     is_torchvision_v2_available,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .image_processing_mask2former import (\n     compute_segments,\n     convert_segmentation_to_rle,\n@@ -155,9 +154,6 @@ class Mask2FormerImageProcessorFast(BaseImageProcessorFast):\n     do_reduce_labels = False\n     valid_kwargs = Mask2FormerFastImageProcessorKwargs\n \n-    @deprecate_kwarg(\"reduce_labels\", new_name=\"do_reduce_labels\", version=\"4.44.0\")\n-    @deprecate_kwarg(\"size_divisibility\", new_name=\"size_divisor\", version=\"4.41.0\")\n-    @deprecate_kwarg(\"max_size\", version=\"4.27.0\", warn_if_greater_or_equal_version=True)\n     def __init__(self, **kwargs: Unpack[Mask2FormerFastImageProcessorKwargs]) -> None:\n         if \"pad_and_return_pixel_mask\" in kwargs:\n             kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n@@ -175,21 +171,6 @@ def __init__(self, **kwargs: Unpack[Mask2FormerFastImageProcessorKwargs]) -> Non\n \n         super().__init__(**kwargs)\n \n-    @classmethod\n-    def from_dict(cls, image_processor_dict: dict[str, Any], **kwargs):\n-        \"\"\"\n-        Overrides the `from_dict` method from the base class to make sure parameters are updated if image processor is\n-        created using from_dict and kwargs e.g. `Mask2FormerImageProcessor.from_pretrained(checkpoint, max_size=800)`\n-        \"\"\"\n-        image_processor_dict = image_processor_dict.copy()\n-        if \"max_size\" in kwargs:\n-            image_processor_dict[\"max_size\"] = kwargs.pop(\"max_size\")\n-        if \"size_divisibility\" in kwargs:\n-            image_processor_dict[\"size_divisor\"] = kwargs.pop(\"size_divisibility\")\n-        if \"reduce_labels\" in image_processor_dict:\n-            image_processor_dict[\"do_reduce_labels\"] = image_processor_dict.pop(\"reduce_labels\")\n-        return super().from_dict(image_processor_dict, **kwargs)\n-\n     def to_dict(self) -> dict[str, Any]:\n         \"\"\"\n         Serializes this instance to a Python dictionary. This method calls the superclass method and then removes the"
        },
        {
            "sha": "8896dad734425fda01f7736e7b028fe30ff1b5e2",
            "filename": "src/transformers/models/maskformer/image_processing_maskformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -51,7 +51,6 @@\n     is_torch_tensor,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import requires\n \n \n@@ -448,9 +447,6 @@ class MaskFormerImageProcessor(BaseImageProcessor):\n \n     model_input_names = [\"pixel_values\", \"pixel_mask\"]\n \n-    @deprecate_kwarg(\"reduce_labels\", new_name=\"do_reduce_labels\", version=\"4.44.0\")\n-    @deprecate_kwarg(\"size_divisibility\", new_name=\"size_divisor\", version=\"4.41.0\")\n-    @deprecate_kwarg(\"max_size\", version=\"4.27.0\", warn_if_greater_or_equal_version=True)\n     @filter_out_non_signature_kwargs(extra=[\"max_size\", *INIT_SERVICE_KWARGS])\n     def __init__(\n         self,\n@@ -492,21 +488,6 @@ def __init__(\n         self.num_labels = num_labels\n         self.pad_size = pad_size\n \n-    @classmethod\n-    def from_dict(cls, image_processor_dict: dict[str, Any], **kwargs):\n-        \"\"\"\n-        Overrides the `from_dict` method from the base class to make sure parameters are updated if image processor is\n-        created using from_dict and kwargs e.g. `MaskFormerImageProcessor.from_pretrained(checkpoint, max_size=800)`\n-        \"\"\"\n-        image_processor_dict = image_processor_dict.copy()\n-        if \"max_size\" in kwargs:\n-            image_processor_dict[\"max_size\"] = kwargs.pop(\"max_size\")\n-        if \"size_divisibility\" in kwargs:\n-            image_processor_dict[\"size_divisor\"] = kwargs.pop(\"size_divisibility\")\n-        if \"reduce_labels\" in image_processor_dict:\n-            image_processor_dict[\"do_reduce_labels\"] = image_processor_dict.pop(\"reduce_labels\")\n-        return super().from_dict(image_processor_dict, **kwargs)\n-\n     def to_dict(self) -> dict[str, Any]:\n         \"\"\"\n         Serializes this instance to a Python dictionary. This method calls the superclass method and then removes the\n@@ -516,7 +497,6 @@ def to_dict(self) -> dict[str, Any]:\n         image_processor_dict.pop(\"_max_size\", None)\n         return image_processor_dict\n \n-    @deprecate_kwarg(\"max_size\", version=\"4.27.0\", warn_if_greater_or_equal_version=True)\n     def resize(\n         self,\n         image: np.ndarray,\n@@ -725,7 +705,6 @@ def _preprocess_mask(\n             segmentation_map = segmentation_map.squeeze(0)\n         return segmentation_map\n \n-    @deprecate_kwarg(\"reduce_labels\", new_name=\"do_reduce_labels\", version=\"4.44.0\")\n     @filter_out_non_signature_kwargs()\n     def preprocess(\n         self,"
        },
        {
            "sha": "5a9961fa657d3758e4bcfd271c4a45f559790fa1",
            "filename": "src/transformers/models/maskformer/image_processing_maskformer_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 19,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -44,7 +44,6 @@\n     is_torchvision_v2_available,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .image_processing_maskformer import (\n     compute_segments,\n     convert_segmentation_to_rle,\n@@ -156,9 +155,6 @@ class MaskFormerImageProcessorFast(BaseImageProcessorFast):\n     do_reduce_labels = False\n     valid_kwargs = MaskFormerFastImageProcessorKwargs\n \n-    @deprecate_kwarg(\"reduce_labels\", new_name=\"do_reduce_labels\", version=\"4.44.0\")\n-    @deprecate_kwarg(\"size_divisibility\", new_name=\"size_divisor\", version=\"4.41.0\")\n-    @deprecate_kwarg(\"max_size\", version=\"4.27.0\", warn_if_greater_or_equal_version=True)\n     def __init__(self, **kwargs: Unpack[MaskFormerFastImageProcessorKwargs]) -> None:\n         if \"pad_and_return_pixel_mask\" in kwargs:\n             kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n@@ -176,21 +172,6 @@ def __init__(self, **kwargs: Unpack[MaskFormerFastImageProcessorKwargs]) -> None\n \n         super().__init__(**kwargs)\n \n-    @classmethod\n-    def from_dict(cls, image_processor_dict: dict[str, Any], **kwargs):\n-        \"\"\"\n-        Overrides the `from_dict` method from the base class to make sure parameters are updated if image processor is\n-        created using from_dict and kwargs e.g. `MaskFormerImageProcessor.from_pretrained(checkpoint, max_size=800)`\n-        \"\"\"\n-        image_processor_dict = image_processor_dict.copy()\n-        if \"max_size\" in kwargs:\n-            image_processor_dict[\"max_size\"] = kwargs.pop(\"max_size\")\n-        if \"size_divisibility\" in kwargs:\n-            image_processor_dict[\"size_divisor\"] = kwargs.pop(\"size_divisibility\")\n-        if \"reduce_labels\" in image_processor_dict:\n-            image_processor_dict[\"do_reduce_labels\"] = image_processor_dict.pop(\"reduce_labels\")\n-        return super().from_dict(image_processor_dict, **kwargs)\n-\n     def to_dict(self) -> dict[str, Any]:\n         \"\"\"\n         Serializes this instance to a Python dictionary. This method calls the superclass method and then removes the"
        },
        {
            "sha": "0f90ec3f6aeed8205abd9d0db5f02f2583ada3a5",
            "filename": "src/transformers/models/megatron_bert/modeling_megatron_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -44,7 +44,6 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_megatron_bert import MegatronBertConfig\n \n \n@@ -207,14 +206,12 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -225,13 +222,7 @@ def forward(\n             1, 2\n         )\n \n-        # If this is instantiated as a cross-attention module, the keys\n-        # and values come from an encoder; the attention mask needs to be\n-        # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n-        if is_cross_attention and encoder_attention_mask is not None:\n-            attention_mask = encoder_attention_mask\n-\n         if past_key_value is not None:\n             if isinstance(past_key_value, EncoderDecoderCache):\n                 is_updated = past_key_value.is_updated.get(self.layer_idx)"
        },
        {
            "sha": "5185bee97c44ef2049f7cc86e580e6b4a932a409",
            "filename": "src/transformers/models/omdet_turbo/processing_omdet_turbo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fprocessing_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fprocessing_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fprocessing_omdet_turbo.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -29,7 +29,6 @@\n     is_torch_available,\n     is_torchvision_available,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import requires\n \n \n@@ -319,8 +318,6 @@ def _get_default_image_size(self) -> tuple[int, int]:\n         )\n         return height, width\n \n-    @deprecate_kwarg(\"score_threshold\", new_name=\"threshold\", version=\"4.51.0\")\n-    @deprecate_kwarg(\"classes\", new_name=\"text_labels\", version=\"4.51.0\")\n     def post_process_grounded_object_detection(\n         self,\n         outputs: \"OmDetTurboObjectDetectionOutput\","
        },
        {
            "sha": "5a66918af17e31b635ed86212a94a81d9c608573",
            "filename": "src/transformers/models/oneformer/image_processing_oneformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -53,7 +53,6 @@\n     is_torch_tensor,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n \n \n logger = logging.get_logger(__name__)\n@@ -425,8 +424,6 @@ class OneFormerImageProcessor(BaseImageProcessor):\n \n     model_input_names = [\"pixel_values\", \"pixel_mask\", \"task_inputs\"]\n \n-    @deprecate_kwarg(\"reduce_labels\", new_name=\"do_reduce_labels\", version=\"4.44.0\")\n-    @deprecate_kwarg(\"max_size\", version=\"4.27.0\", warn_if_greater_or_equal_version=True)\n     @filter_out_non_signature_kwargs(extra=[\"max_size\", \"metadata\", *INIT_SERVICE_KWARGS])\n     def __init__(\n         self,\n@@ -473,16 +470,6 @@ def __init__(\n         self.num_text = num_text\n         self.num_labels = num_labels\n \n-    @classmethod\n-    def from_dict(cls, image_processor_dict: dict[str, Any], **kwargs):\n-        \"\"\"\n-        Overrides the `from_dict` method from the base class to save support of deprecated `reduce_labels` in old configs\n-        \"\"\"\n-        image_processor_dict = image_processor_dict.copy()\n-        if \"reduce_labels\" in image_processor_dict:\n-            image_processor_dict[\"do_reduce_labels\"] = image_processor_dict.pop(\"reduce_labels\")\n-        return super().from_dict(image_processor_dict, **kwargs)\n-\n     # Copied from transformers.models.maskformer.image_processing_maskformer.MaskFormerImageProcessor.to_dict\n     def to_dict(self) -> dict[str, Any]:\n         \"\"\"\n@@ -493,7 +480,6 @@ def to_dict(self) -> dict[str, Any]:\n         image_processor_dict.pop(\"_max_size\", None)\n         return image_processor_dict\n \n-    @deprecate_kwarg(\"max_size\", version=\"4.27.0\", warn_if_greater_or_equal_version=True)\n     @filter_out_non_signature_kwargs(extra=[\"max_size\"])\n     def resize(\n         self,"
        },
        {
            "sha": "fcafe3ebac5e3080497b50661b3950ab4e7f0a37",
            "filename": "src/transformers/models/patchtsmixer/modeling_patchtsmixer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -29,7 +29,6 @@\n from ...processing_utils import Unpack\n from ...time_series_utils import NegativeBinomialOutput, NormalOutput, StudentTOutput\n from ...utils import auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_patchtsmixer import PatchTSMixerConfig\n \n \n@@ -304,7 +303,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "9a22657cbffa21fe3be3308b9b984a6ffb9a85a8",
            "filename": "src/transformers/models/patchtst/modeling_patchtst.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -28,7 +28,6 @@\n from ...processing_utils import Unpack\n from ...time_series_utils import NegativeBinomialOutput, NormalOutput, StudentTOutput\n from ...utils import ModelOutput, auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_patchtst import PatchTSTConfig\n \n \n@@ -101,7 +100,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "9a892e6838c4f82095210e06f0c06a4ecce1b2c2",
            "filename": "src/transformers/models/qwen2_audio/processing_qwen2_audio.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fprocessing_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fprocessing_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fprocessing_qwen2_audio.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -16,15 +16,13 @@\n Processor class for Qwen2Audio.\n \"\"\"\n \n-import warnings\n from typing import Union\n \n import numpy as np\n \n from ...feature_extraction_utils import BatchFeature\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils.deprecation import deprecate_kwarg\n \n \n class Qwen2AudioProcessorKwargs(ProcessingKwargs, total=False):\n@@ -80,12 +78,10 @@ def __init__(\n         self.audio_eos_token = tokenizer.audio_eos_token if hasattr(tokenizer, \"audio_eos_token\") else audio_eos_token\n         super().__init__(feature_extractor, tokenizer, chat_template=chat_template)\n \n-    @deprecate_kwarg(\"audios\", version=\"4.54.0\", new_name=\"audio\")\n     def __call__(\n         self,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         audio: Union[np.ndarray, list[np.ndarray]] = None,\n-        audios=None,  # kept for BC\n         **kwargs: Unpack[Qwen2AudioProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n@@ -103,16 +99,6 @@ def __call__(\n             audio (`np.ndarray`, `list[np.ndarray]`):\n                 The audio or batch of audios to be prepared. Each audio can be a NumPy array.\n         \"\"\"\n-\n-        # Handle BC when user passes deprecated keyword argument\n-        if audios is not None and audio is None:\n-            audio = audios\n-            warnings.warn(\n-                \"You may have used the keyword argument for the `audio` inputs. It is strongly recommended to pass inputs with keyword arguments \"\n-                \"with keys `audio` and `text`. From transformers v4.55 `audio` will be the only acceptable keyword argument.\",\n-                FutureWarning,\n-            )\n-\n         if text is None:\n             raise ValueError(\"You need to specify `text` input to process.\")\n         elif isinstance(text, str):"
        },
        {
            "sha": "bd27f3eeecfd5b3121d4a92e96a2ce5582b3a87d",
            "filename": "src/transformers/models/rembert/modeling_rembert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -40,7 +40,6 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_rembert import RemBertConfig\n \n \n@@ -222,14 +221,12 @@ def __init__(self, config, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -241,13 +238,7 @@ def forward(\n             .transpose(1, 2)\n         )\n \n-        # If this is instantiated as a cross-attention module, the keys\n-        # and values come from an encoder; the attention mask needs to be\n-        # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n-        if is_cross_attention and encoder_attention_mask is not None:\n-            attention_mask = encoder_attention_mask\n-\n         if past_key_value is not None:\n             if isinstance(past_key_value, EncoderDecoderCache):\n                 is_updated = past_key_value.is_updated.get(self.layer_idx)\n@@ -355,15 +346,13 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     # Copied from transformers.models.bert.modeling_bert.BertAttention.forward\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -373,7 +362,6 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n             past_key_value=past_key_value,\n             output_attentions=output_attentions,\n             cache_position=cache_position,"
        },
        {
            "sha": "f9fc9c3b58ae49464a3ce5d74a9a0b3f5628a9e0",
            "filename": "src/transformers/models/roberta/modeling_roberta.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -42,7 +42,6 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, get_torch_version, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_roberta import RobertaConfig\n \n \n@@ -167,14 +166,12 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -185,13 +182,7 @@ def forward(\n             1, 2\n         )\n \n-        # If this is instantiated as a cross-attention module, the keys\n-        # and values come from an encoder; the attention mask needs to be\n-        # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n-        if is_cross_attention and encoder_attention_mask is not None:\n-            attention_mask = encoder_attention_mask\n-\n         if past_key_value is not None:\n             if isinstance(past_key_value, EncoderDecoderCache):\n                 is_updated = past_key_value.is_updated.get(self.layer_idx)\n@@ -286,14 +277,12 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.require_contiguous_qkv = version.parse(get_torch_version()) < version.parse(\"2.2.0\")\n \n     # Adapted from RobertaSelfAttention\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -312,7 +301,6 @@ def forward(\n                 attention_mask,\n                 head_mask,\n                 encoder_hidden_states,\n-                encoder_attention_mask,\n                 past_key_value,\n                 output_attentions,\n                 cache_position,\n@@ -324,12 +312,7 @@ def forward(\n             self.query(hidden_states).view(bsz, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n         )\n \n-        # If this is instantiated as a cross-attention module, the keys and values come from an encoder; the attention\n-        # mask needs to be such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n-        if is_cross_attention and encoder_attention_mask is not None:\n-            attention_mask = encoder_attention_mask\n-\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if past_key_value is not None:\n             if isinstance(past_key_value, EncoderDecoderCache):\n@@ -451,14 +434,12 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -468,7 +449,6 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n             past_key_value=past_key_value,\n             output_attentions=output_attentions,\n             cache_position=cache_position,"
        },
        {
            "sha": "a809d199051eb6fe6838d6bb2e26a951d6ed3b53",
            "filename": "src/transformers/models/roberta_prelayernorm/modeling_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -40,7 +40,6 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_roberta_prelayernorm import RobertaPreLayerNormConfig\n \n \n@@ -166,14 +165,12 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -184,13 +181,7 @@ def forward(\n             1, 2\n         )\n \n-        # If this is instantiated as a cross-attention module, the keys\n-        # and values come from an encoder; the attention mask needs to be\n-        # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n-        if is_cross_attention and encoder_attention_mask is not None:\n-            attention_mask = encoder_attention_mask\n-\n         if past_key_value is not None:\n             if isinstance(past_key_value, EncoderDecoderCache):\n                 is_updated = past_key_value.is_updated.get(self.layer_idx)\n@@ -319,14 +310,12 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -337,7 +326,6 @@ def forward(\n             attention_mask,\n             head_mask,\n             encoder_hidden_states,\n-            encoder_attention_mask,\n             past_key_value,\n             output_attentions,\n             cache_position,"
        },
        {
            "sha": "be5c72526983788d967c18ca5a743c3fda9bf45f",
            "filename": "src/transformers/models/roc_bert/modeling_roc_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -40,7 +40,6 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_roc_bert import RoCBertConfig\n \n \n@@ -281,14 +280,12 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -299,13 +296,7 @@ def forward(\n             1, 2\n         )\n \n-        # If this is instantiated as a cross-attention module, the keys\n-        # and values come from an encoder; the attention mask needs to be\n-        # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n-        if is_cross_attention and encoder_attention_mask is not None:\n-            attention_mask = encoder_attention_mask\n-\n         if past_key_value is not None:\n             if isinstance(past_key_value, EncoderDecoderCache):\n                 is_updated = past_key_value.is_updated.get(self.layer_idx)\n@@ -442,14 +433,12 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -459,7 +448,6 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n             past_key_value=past_key_value,\n             output_attentions=output_attentions,\n             cache_position=cache_position,"
        },
        {
            "sha": "9cb05f81185778c3ce588fd14c244eb937c4c0de",
            "filename": "src/transformers/models/segformer/image_processing_segformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 14,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"Image processor class for Segformer.\"\"\"\n \n-from typing import Any, Optional, Union\n+from typing import Optional, Union\n \n import numpy as np\n \n@@ -41,7 +41,6 @@\n     is_vision_available,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import requires\n \n \n@@ -94,7 +93,6 @@ class SegformerImageProcessor(BaseImageProcessor):\n \n     model_input_names = [\"pixel_values\"]\n \n-    @deprecate_kwarg(\"reduce_labels\", new_name=\"do_reduce_labels\", version=\"4.41.0\")\n     @filter_out_non_signature_kwargs(extra=INIT_SERVICE_KWARGS)\n     def __init__(\n         self,\n@@ -122,16 +120,6 @@ def __init__(\n         self.image_std = image_std if image_std is not None else IMAGENET_DEFAULT_STD\n         self.do_reduce_labels = do_reduce_labels\n \n-    @classmethod\n-    def from_dict(cls, image_processor_dict: dict[str, Any], **kwargs):\n-        \"\"\"\n-        Overrides the `from_dict` method from the base class to save support of deprecated `reduce_labels` in old configs\n-        \"\"\"\n-        image_processor_dict = image_processor_dict.copy()\n-        if \"reduce_labels\" in image_processor_dict:\n-            image_processor_dict[\"do_reduce_labels\"] = image_processor_dict.pop(\"reduce_labels\")\n-        return super().from_dict(image_processor_dict, **kwargs)\n-\n     # Copied from transformers.models.vit.image_processing_vit.ViTImageProcessor.resize\n     def resize(\n         self,\n@@ -304,7 +292,6 @@ def __call__(self, images, segmentation_maps=None, **kwargs):\n         \"\"\"\n         return super().__call__(images, segmentation_maps=segmentation_maps, **kwargs)\n \n-    @deprecate_kwarg(\"reduce_labels\", new_name=\"do_reduce_labels\", version=\"4.41.0\")\n     @filter_out_non_signature_kwargs()\n     def preprocess(\n         self,"
        },
        {
            "sha": "3b985bce20b4f244cc2676327b0c57843bc12fa1",
            "filename": "src/transformers/models/sew/modeling_sew.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -37,7 +37,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_sew import SEWConfig\n \n \n@@ -294,7 +293,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "905c7a27ad3d166df988d2d12787be06310d4e87",
            "filename": "src/transformers/models/splinter/modeling_splinter.py",
            "status": "modified",
            "additions": 0,
            "deletions": 35,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fsplinter%2Fmodeling_splinter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fsplinter%2Fmodeling_splinter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsplinter%2Fmodeling_splinter.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -32,7 +32,6 @@\n     can_return_tuple,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_splinter import SplinterConfig\n \n \n@@ -143,17 +142,11 @@ def __init__(self, config):\n         self.attention_dropout = config.attention_probs_dropout_prob\n         self.scaling = self.attention_head_size**-0.5\n \n-    @deprecate_kwarg(\"encoder_hidden_states\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n@@ -226,17 +219,11 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"encoder_hidden_states\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n@@ -293,17 +280,11 @@ def __init__(self, config):\n         self.intermediate = SplinterIntermediate(config)\n         self.output = SplinterOutput(config)\n \n-    @deprecate_kwarg(\"encoder_hidden_states\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n@@ -338,20 +319,12 @@ def __init__(self, config):\n         self.layer = nn.ModuleList([SplinterLayer(config) for i in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n \n-    @deprecate_kwarg(\"encoder_hidden_states\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"past_key_values\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"use_cache\", version=\"4.54.0\")\n     @can_return_tuple\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n@@ -443,10 +416,6 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n-    @deprecate_kwarg(\"encoder_hidden_states\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"past_key_values\", version=\"4.54.0\")\n-    @deprecate_kwarg(\"use_cache\", version=\"4.54.0\")\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -457,10 +426,6 @@ def forward(\n         position_ids: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n-        encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n-        use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,"
        },
        {
            "sha": "e04daf76b2db745770a2461d5e972b8f929cd52d",
            "filename": "src/transformers/models/tapas/modeling_tapas.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -32,7 +32,6 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_tapas import TapasConfig\n \n \n@@ -301,14 +300,12 @@ def __init__(self, config, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n         head_mask=None,\n         encoder_hidden_states=None,\n-        encoder_attention_mask=None,\n         past_key_value=None,\n         output_attentions=False,\n         cache_position=None,\n@@ -320,13 +317,7 @@ def forward(\n             .transpose(1, 2)\n         )\n \n-        # If this is instantiated as a cross-attention module, the keys\n-        # and values come from an encoder; the attention mask needs to be\n-        # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n-        if is_cross_attention and encoder_attention_mask is not None:\n-            attention_mask = encoder_attention_mask\n-\n         if past_key_value is not None:\n             if isinstance(past_key_value, EncoderDecoderCache):\n                 is_updated = past_key_value.is_updated.get(self.layer_idx)\n@@ -436,15 +427,13 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     # Copied from transformers.models.bert.modeling_bert.BertAttention.forward\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -454,7 +443,6 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n             past_key_value=past_key_value,\n             output_attentions=output_attentions,\n             cache_position=cache_position,"
        },
        {
            "sha": "233de0d14ce8004af95900d9bd3fba1c29a274f8",
            "filename": "src/transformers/models/unispeech/modeling_unispeech.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -45,7 +45,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_unispeech import UniSpeechConfig\n \n \n@@ -333,7 +332,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "c7fc63849c7c9e54ce6f6157f39d9c3a8a46a580",
            "filename": "src/transformers/models/unispeech_sat/modeling_unispeech_sat.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -47,7 +47,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, is_peft_available, is_torch_flex_attn_available, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_unispeech_sat import UniSpeechSatConfig\n \n \n@@ -338,7 +337,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "2c352da1804c883f5afa146929c65e8a741671f3",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -55,7 +55,6 @@\n     is_torch_flex_attn_available,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_wav2vec2 import Wav2Vec2Config\n \n \n@@ -525,7 +524,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "4ff6ea8cf2fa5b980838a15d0dbb46b42516060b",
            "filename": "src/transformers/models/xlm_roberta/modeling_xlm_roberta.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -42,7 +42,6 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, get_torch_version, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_xlm_roberta import XLMRobertaConfig\n \n \n@@ -168,14 +167,12 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -186,13 +183,7 @@ def forward(\n             1, 2\n         )\n \n-        # If this is instantiated as a cross-attention module, the keys\n-        # and values come from an encoder; the attention mask needs to be\n-        # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n-        if is_cross_attention and encoder_attention_mask is not None:\n-            attention_mask = encoder_attention_mask\n-\n         if past_key_value is not None:\n             if isinstance(past_key_value, EncoderDecoderCache):\n                 is_updated = past_key_value.is_updated.get(self.layer_idx)\n@@ -287,14 +278,12 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.require_contiguous_qkv = version.parse(get_torch_version()) < version.parse(\"2.2.0\")\n \n     # Adapted from XLMRobertaSelfAttention\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -313,7 +302,6 @@ def forward(\n                 attention_mask,\n                 head_mask,\n                 encoder_hidden_states,\n-                encoder_attention_mask,\n                 past_key_value,\n                 output_attentions,\n                 cache_position,\n@@ -325,12 +313,7 @@ def forward(\n             self.query(hidden_states).view(bsz, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n         )\n \n-        # If this is instantiated as a cross-attention module, the keys and values come from an encoder; the attention\n-        # mask needs to be such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n-        if is_cross_attention and encoder_attention_mask is not None:\n-            attention_mask = encoder_attention_mask\n-\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if past_key_value is not None:\n             if isinstance(past_key_value, EncoderDecoderCache):\n@@ -452,14 +435,12 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -469,7 +450,6 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n             past_key_value=past_key_value,\n             output_attentions=output_attentions,\n             cache_position=cache_position,"
        },
        {
            "sha": "69c623934e9d245d2a9e2b8fbae3383827d55974",
            "filename": "src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -41,7 +41,6 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, get_torch_version, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_xlm_roberta_xl import XLMRobertaXLConfig\n \n \n@@ -165,14 +164,12 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -183,13 +180,7 @@ def forward(\n             1, 2\n         )\n \n-        # If this is instantiated as a cross-attention module, the keys\n-        # and values come from an encoder; the attention mask needs to be\n-        # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n-        if is_cross_attention and encoder_attention_mask is not None:\n-            attention_mask = encoder_attention_mask\n-\n         if past_key_value is not None:\n             if isinstance(past_key_value, EncoderDecoderCache):\n                 is_updated = past_key_value.is_updated.get(self.layer_idx)\n@@ -284,14 +275,12 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.require_contiguous_qkv = version.parse(get_torch_version()) < version.parse(\"2.2.0\")\n \n     # Adapted from XLMRobertaXLSelfAttention\n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -310,7 +299,6 @@ def forward(\n                 attention_mask,\n                 head_mask,\n                 encoder_hidden_states,\n-                encoder_attention_mask,\n                 past_key_value,\n                 output_attentions,\n                 cache_position,\n@@ -322,12 +310,7 @@ def forward(\n             self.query(hidden_states).view(bsz, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n         )\n \n-        # If this is instantiated as a cross-attention module, the keys and values come from an encoder; the attention\n-        # mask needs to be such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n-        if is_cross_attention and encoder_attention_mask is not None:\n-            attention_mask = encoder_attention_mask\n-\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if past_key_value is not None:\n             if isinstance(past_key_value, EncoderDecoderCache):\n@@ -447,14 +430,12 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n         head_mask=None,\n         encoder_hidden_states=None,\n-        encoder_attention_mask=None,\n         past_key_value=None,\n         output_attentions=False,\n         cache_position=None,\n@@ -465,7 +446,6 @@ def forward(\n             attention_mask,\n             head_mask,\n             encoder_hidden_states,\n-            encoder_attention_mask,\n             past_key_value,\n             output_attentions,\n             cache_position,"
        },
        {
            "sha": "11566a99d249c14e54c173a2e71272c8f1f58a8c",
            "filename": "src/transformers/models/xmod/modeling_xmod.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -39,7 +39,6 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_xmod import XmodConfig\n \n \n@@ -165,14 +164,12 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -183,13 +180,7 @@ def forward(\n             1, 2\n         )\n \n-        # If this is instantiated as a cross-attention module, the keys\n-        # and values come from an encoder; the attention mask needs to be\n-        # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n-        if is_cross_attention and encoder_attention_mask is not None:\n-            attention_mask = encoder_attention_mask\n-\n         if past_key_value is not None:\n             if isinstance(past_key_value, EncoderDecoderCache):\n                 is_updated = past_key_value.is_updated.get(self.layer_idx)\n@@ -318,14 +309,12 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n@@ -338,7 +327,6 @@ def forward(\n             attention_mask,\n             head_mask,\n             encoder_hidden_states,\n-            encoder_attention_mask,\n             past_key_value,\n             output_attentions,\n             cache_position,"
        },
        {
            "sha": "526f481eb93b2e171acc75872f9ebf8eacebe3f2",
            "filename": "tests/models/mask2former/test_image_processing_mask2former.py",
            "status": "modified",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/tests%2Fmodels%2Fmask2former%2Ftest_image_processing_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/tests%2Fmodels%2Fmask2former%2Ftest_image_processing_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmask2former%2Ftest_image_processing_mask2former.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -191,18 +191,6 @@ def test_image_processor_properties(self):\n             self.assertTrue(hasattr(image_processing, \"ignore_index\"))\n             self.assertTrue(hasattr(image_processing, \"num_labels\"))\n \n-    def test_image_processor_from_dict_with_kwargs(self):\n-        for image_processing_class in self.image_processor_list:\n-            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n-            self.assertEqual(image_processor.size, {\"shortest_edge\": 32, \"longest_edge\": 1333})\n-            self.assertEqual(image_processor.size_divisor, 0)\n-\n-            image_processor = image_processing_class.from_dict(\n-                self.image_processor_dict, size=42, max_size=84, size_divisibility=8\n-            )\n-            self.assertEqual(image_processor.size, {\"shortest_edge\": 42, \"longest_edge\": 84})\n-            self.assertEqual(image_processor.size_divisor, 8)\n-\n     def comm_get_image_processing_inputs(\n         self,\n         image_processor_tester,\n@@ -568,20 +556,6 @@ def test_post_process_label_fusing(self):\n                 num_segments_fused = max([el[\"id\"] for el in el_fused])\n                 self.assertEqual(num_segments_fused, expected_num_segments)\n \n-    def test_removed_deprecated_kwargs(self):\n-        image_processor_dict = dict(self.image_processor_dict)\n-        image_processor_dict.pop(\"do_reduce_labels\", None)\n-        image_processor_dict[\"reduce_labels\"] = True\n-\n-        # test we are able to create the image processor with the deprecated kwargs\n-        for image_processing_class in self.image_processor_list:\n-            image_processor = image_processing_class(**image_processor_dict)\n-            self.assertEqual(image_processor.do_reduce_labels, True)\n-\n-            # test we still support reduce_labels with config\n-            image_processor = image_processing_class.from_dict(image_processor_dict)\n-            self.assertEqual(image_processor.do_reduce_labels, True)\n-\n     def test_slow_fast_equivalence(self):\n         if not self.test_slow_image_processor or not self.test_fast_image_processor:\n             self.skipTest(reason=\"Skipping slow/fast equivalence test\")"
        },
        {
            "sha": "44797837233cd9c361e0574953c860dddea78e48",
            "filename": "tests/models/maskformer/test_image_processing_maskformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/tests%2Fmodels%2Fmaskformer%2Ftest_image_processing_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/tests%2Fmodels%2Fmaskformer%2Ftest_image_processing_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmaskformer%2Ftest_image_processing_maskformer.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -187,18 +187,6 @@ def test_image_processor_properties(self):\n             self.assertTrue(hasattr(image_processing, \"ignore_index\"))\n             self.assertTrue(hasattr(image_processing, \"num_labels\"))\n \n-    def test_image_processor_from_dict_with_kwargs(self):\n-        for image_processing_class in self.image_processor_list:\n-            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n-            self.assertEqual(image_processor.size, {\"shortest_edge\": 32, \"longest_edge\": 1333})\n-            self.assertEqual(image_processor.size_divisor, 0)\n-\n-            image_processor = image_processing_class.from_dict(\n-                self.image_processor_dict, size=42, max_size=84, size_divisibility=8\n-            )\n-            self.assertEqual(image_processor.size, {\"shortest_edge\": 42, \"longest_edge\": 84})\n-            self.assertEqual(image_processor.size_divisor, 8)\n-\n     def comm_get_image_processing_inputs(\n         self, with_segmentation_maps=False, is_instance_map=False, segmentation_type=\"np\"\n     ):\n@@ -556,19 +544,6 @@ def test_post_process_label_fusing(self):\n                 num_segments_fused = max([el[\"id\"] for el in el_fused])\n                 self.assertEqual(num_segments_fused, expected_num_segments)\n \n-    def test_removed_deprecated_kwargs(self):\n-        image_processor_dict = dict(self.image_processor_dict)\n-        image_processor_dict.pop(\"do_reduce_labels\", None)\n-        image_processor_dict[\"reduce_labels\"] = True\n-\n-        # test we are able to create the image processor with the deprecated kwargs\n-        image_processor = self.image_processing_class(**image_processor_dict)\n-        self.assertEqual(image_processor.do_reduce_labels, True)\n-\n-        # test we still support reduce_labels with config\n-        image_processor = self.image_processing_class.from_dict(image_processor_dict)\n-        self.assertEqual(image_processor.do_reduce_labels, True)\n-\n     def test_slow_fast_equivalence(self):\n         if not self.test_slow_image_processor or not self.test_fast_image_processor:\n             self.skipTest(reason=\"Skipping slow/fast equivalence test\")"
        },
        {
            "sha": "d201c704091cebb2daa926466615d2625b90288a",
            "filename": "tests/models/oneformer/test_image_processing_oneformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/tests%2Fmodels%2Foneformer%2Ftest_image_processing_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/tests%2Fmodels%2Foneformer%2Ftest_image_processing_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Foneformer%2Ftest_image_processing_oneformer.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -377,20 +377,6 @@ def test_can_load_with_local_metadata(self):\n \n             self.assertEqual(image_processor.metadata, metadata)\n \n-    def test_removed_deprecated_kwargs(self):\n-        image_processor_dict = dict(self.image_processor_dict)\n-        image_processor_dict.pop(\"do_reduce_labels\", None)\n-        image_processor_dict[\"reduce_labels\"] = True\n-        # Only test for OneFormerImageProcessor\n-        image_processing_class = self.image_processing_class\n-        # test we are able to create the image processor with the deprecated kwargs\n-        image_processor = image_processing_class(**image_processor_dict)\n-        self.assertEqual(image_processor.do_reduce_labels, True)\n-\n-        # test we still support reduce_labels with config\n-        image_processor = image_processing_class.from_dict(image_processor_dict)\n-        self.assertEqual(image_processor.do_reduce_labels, True)\n-\n     def test_slow_fast_equivalence(self):\n         if not self.test_slow_image_processor or not self.test_fast_image_processor:\n             self.skipTest(reason=\"Skipping slow/fast equivalence test\")"
        },
        {
            "sha": "3bf1eb1f6c60873694beee7f0b1ef4751390be28",
            "filename": "tests/models/segformer/test_image_processing_segformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/686bb3b098910adb28df3a8c65e2dbe9567ca32e/tests%2Fmodels%2Fsegformer%2Ftest_image_processing_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/686bb3b098910adb28df3a8c65e2dbe9567ca32e/tests%2Fmodels%2Fsegformer%2Ftest_image_processing_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsegformer%2Ftest_image_processing_segformer.py?ref=686bb3b098910adb28df3a8c65e2dbe9567ca32e",
            "patch": "@@ -247,16 +247,3 @@ def test_reduce_labels(self):\n         encoding = image_processing(image, map, return_tensors=\"pt\")\n         self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n         self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n-\n-    def test_removed_deprecated_kwargs(self):\n-        image_processor_dict = dict(self.image_processor_dict)\n-        image_processor_dict.pop(\"do_reduce_labels\", None)\n-        image_processor_dict[\"reduce_labels\"] = True\n-\n-        # test we are able to create the image processor with the deprecated kwargs\n-        image_processor = self.image_processing_class(**image_processor_dict)\n-        self.assertEqual(image_processor.do_reduce_labels, True)\n-\n-        # test we still support reduce_labels with config\n-        image_processor = self.image_processing_class.from_dict(image_processor_dict)\n-        self.assertEqual(image_processor.do_reduce_labels, True)"
        }
    ],
    "stats": {
        "total": 835,
        "additions": 4,
        "deletions": 831
    }
}