{
    "author": "hiyouga",
    "message": "Support batch size > 1 image-text inference (#36682)\n\n* update make nested image list\n\n* fix make flat list of images\n\n* update type anno\n\n* fix image_processing_smolvlm\n\n* use first image\n\n* add verbose comment\n\n* fix images\n\n* rollback\n\n* fix ut\n\n* Update image_processing_smolvlm.py\n\n* Update image_processing_idefics3.py\n\n* add tests and fix some processors\n\n* fix copies\n\n* fix after rebase\n\n* make the test cover chat templates\n\n* sjip udop, no point in fixing it\n\n* fix after rebase\n\n* fix a few more tests\n\n---------\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\nCo-authored-by: raushan <raushan@huggingface.co>",
    "sha": "564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
    "files": [
        {
            "sha": "2a5778b2c752c301c4bcdb2fad609e4b02e68bd7",
            "filename": "src/transformers/image_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fimage_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fimage_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_utils.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -233,7 +233,7 @@ def make_flat_list_of_images(\n     if (\n         isinstance(images, (list, tuple))\n         and all(isinstance(images_i, (list, tuple)) for images_i in images)\n-        and all(is_valid_list_of_images(images_i) for images_i in images)\n+        and all(is_valid_list_of_images(images_i) or not images_i for images_i in images)\n     ):\n         return [img for img_list in images for img in img_list]\n \n@@ -255,7 +255,7 @@ def make_flat_list_of_images(\n def make_nested_list_of_images(\n     images: Union[list[ImageInput], ImageInput],\n     expected_ndims: int = 3,\n-) -> ImageInput:\n+) -> list[ImageInput]:\n     \"\"\"\n     Ensure that the output is a nested list of images.\n     Args:\n@@ -270,7 +270,7 @@ def make_nested_list_of_images(\n     if (\n         isinstance(images, (list, tuple))\n         and all(isinstance(images_i, (list, tuple)) for images_i in images)\n-        and all(is_valid_list_of_images(images_i) for images_i in images)\n+        and all(is_valid_list_of_images(images_i) or not images_i for images_i in images)\n     ):\n         return images\n "
        },
        {
            "sha": "7fde9c8a8c901731e184f6cc7320bfba810a073f",
            "filename": "src/transformers/models/beit/image_processing_beit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -28,7 +28,7 @@\n     PILImageResampling,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -390,10 +390,10 @@ def preprocess(\n         image_std = image_std if image_std is not None else self.image_std\n         do_reduce_labels = do_reduce_labels if do_reduce_labels is not None else self.do_reduce_labels\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if segmentation_maps is not None:\n-            segmentation_maps = make_list_of_images(segmentation_maps, expected_ndims=2)\n+            segmentation_maps = make_flat_list_of_images(segmentation_maps, expected_ndims=2)\n \n         if segmentation_maps is not None and not valid_images(segmentation_maps):\n             raise ValueError("
        },
        {
            "sha": "6ebd9267b6fbe385dc06a4d54b5cbc227c5b6711",
            "filename": "src/transformers/models/bit/image_processing_bit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fbit%2Fimage_processing_bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fbit%2Fimage_processing_bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbit%2Fimage_processing_bit.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -33,7 +33,7 @@\n     PILImageResampling,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -256,7 +256,7 @@ def preprocess(\n         image_std = image_std if image_std is not None else self.image_std\n         do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "3b2464e9371dd21e00504e909ce26d2b218cb42c",
            "filename": "src/transformers/models/chinese_clip/image_processing_chinese_clip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -33,7 +33,7 @@\n     PILImageResampling,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -250,7 +250,7 @@ def preprocess(\n         image_std = image_std if image_std is not None else self.image_std\n         do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "9221c463fe8536fd1e3a6523962192ef15515520",
            "filename": "src/transformers/models/conditional_detr/image_processing_conditional_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -46,7 +46,7 @@\n     get_image_size,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_annotations,\n@@ -1390,7 +1390,7 @@ def preprocess(\n         pad_size = self.pad_size if pad_size is None else pad_size\n         format = self.format if format is None else format\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "299cabd5d4d99872d67d0105c1ed6775ae7a0df1",
            "filename": "src/transformers/models/convnext/image_processing_convnext.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -33,7 +33,7 @@\n     PILImageResampling,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -262,7 +262,7 @@ def preprocess(\n         size = size if size is not None else self.size\n         size = get_size_dict(size, default_to_square=False)\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "c6875eb9b8f80e152c0e310e9bdd051e6b2200e6",
            "filename": "src/transformers/models/deformable_detr/image_processing_deformable_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -46,7 +46,7 @@\n     get_image_size,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_annotations,\n@@ -1388,7 +1388,7 @@ def preprocess(\n         pad_size = self.pad_size if pad_size is None else pad_size\n         format = self.format if format is None else format\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "1e2f6c3b5ae5f0f1cf2eb1727d2e3235443b81b9",
            "filename": "src/transformers/models/deit/image_processing_deit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fdeit%2Fimage_processing_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fdeit%2Fimage_processing_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fimage_processing_deit.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -28,7 +28,7 @@\n     PILImageResampling,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -240,7 +240,7 @@ def preprocess(\n         crop_size = crop_size if crop_size is not None else self.crop_size\n         crop_size = get_size_dict(crop_size, param_name=\"crop_size\")\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "f274d8c058f960934ccfe7c221045df536867d03",
            "filename": "src/transformers/models/deprecated/vit_hybrid/image_processing_vit_hybrid.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fimage_processing_vit_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fimage_processing_vit_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fimage_processing_vit_hybrid.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -33,7 +33,7 @@\n     PILImageResampling,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_kwargs,\n@@ -273,7 +273,7 @@ def preprocess(\n         image_std = image_std if image_std is not None else self.image_std\n         do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self._valid_processor_keys)\n "
        },
        {
            "sha": "47f224f248bd1c222823c9ba2159999b68c641e1",
            "filename": "src/transformers/models/depth_pro/image_processing_depth_pro.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -35,7 +35,7 @@\n     infer_channel_dimension_format,\n     is_scaled_image,\n     is_torch_available,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n )\n@@ -258,7 +258,7 @@ def preprocess(\n \n         size = size if size is not None else self.size\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "f29bd48a5934cca8224a91d0cc25ce54f79e169f",
            "filename": "src/transformers/models/detr/image_processing_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -45,7 +45,7 @@\n     get_image_size,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_annotations,\n@@ -1363,7 +1363,7 @@ def preprocess(\n         pad_size = self.pad_size if pad_size is None else pad_size\n         format = self.format if format is None else format\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "d6c963803743a820ee49a3dc247f551f6f5ff3fc",
            "filename": "src/transformers/models/donut/image_processing_donut.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -35,7 +35,7 @@\n     get_image_size,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -399,7 +399,7 @@ def preprocess(\n         image_mean = image_mean if image_mean is not None else self.image_mean\n         image_std = image_std if image_std is not None else self.image_std\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "4a312e3005f310360f3654e56745bbcd19dfcedc",
            "filename": "src/transformers/models/dpt/image_processing_dpt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -39,7 +39,7 @@\n     is_scaled_image,\n     is_torch_available,\n     is_torch_tensor,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -525,10 +525,10 @@ def preprocess(\n         size_divisor = size_divisor if size_divisor is not None else self.size_divisor\n         do_reduce_labels = do_reduce_labels if do_reduce_labels is not None else self.do_reduce_labels\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if segmentation_maps is not None:\n-            segmentation_maps = make_list_of_images(segmentation_maps, expected_ndims=2)\n+            segmentation_maps = make_flat_list_of_images(segmentation_maps, expected_ndims=2)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "ea822d75ca27a8acff86a6049bbc1c79a1c3f8ca",
            "filename": "src/transformers/models/efficientnet/image_processing_efficientnet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -28,7 +28,7 @@\n     PILImageResampling,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -295,7 +295,7 @@ def preprocess(\n         crop_size = crop_size if crop_size is not None else self.crop_size\n         crop_size = get_size_dict(crop_size, param_name=\"crop_size\")\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "05431d796a30e3e7153c35201479d68cf5c5a3a9",
            "filename": "src/transformers/models/emu3/image_processing_emu3.py",
            "status": "modified",
            "additions": 20,
            "deletions": 42,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -31,8 +31,8 @@\n     get_image_size,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    is_valid_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n+    make_nested_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -46,29 +46,6 @@\n logger = logging.get_logger(__name__)\n \n \n-def make_batched_images(images) -> list[list[ImageInput]]:\n-    \"\"\"\n-    Accepts images in list or nested list format, and makes a list of images for preprocessing.\n-\n-    Args:\n-        images (`Union[list[list[ImageInput]], list[ImageInput], ImageInput]`):\n-            The input image.\n-\n-    Returns:\n-        list: A list of images.\n-    \"\"\"\n-    if isinstance(images, (list, tuple)) and isinstance(images[0], (list, tuple)) and is_valid_image(images[0][0]):\n-        return [img for img_list in images for img in img_list]\n-\n-    elif isinstance(images, (list, tuple)) and is_valid_image(images[0]):\n-        return images\n-\n-    elif is_valid_image(images):\n-        return [images]\n-\n-    raise ValueError(f\"Could not make batched images from {images}\")\n-\n-\n def smart_resize(\n     height: int, width: int, factor: int = 28, min_pixels: int = 56 * 56, max_pixels: int = 14 * 14 * 4 * 1280\n ):\n@@ -211,7 +188,7 @@ def _preprocess(\n                 - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n                 - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.   - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n         \"\"\"\n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if do_convert_rgb:\n             images = [convert_to_rgb(image) for image in images]\n@@ -382,7 +359,7 @@ def preprocess(\n \n         if images is not None:\n             images = self.fetch_images(images)\n-            images = make_batched_images(images)\n+            images = make_nested_list_of_images(images)\n \n         if images is not None and not valid_images(images):\n             raise ValueError(\n@@ -402,20 +379,21 @@ def preprocess(\n \n         pixel_values = []\n         for image in images:\n-            image = self._preprocess(\n-                image,\n-                do_resize=do_resize,\n-                resample=resample,\n-                do_rescale=do_rescale,\n-                rescale_factor=rescale_factor,\n-                do_normalize=do_normalize,\n-                image_mean=image_mean,\n-                image_std=image_std,\n-                data_format=data_format,\n-                do_convert_rgb=do_convert_rgb,\n-                input_data_format=input_data_format,\n-            )\n-            pixel_values.extend(image)\n+            if image:\n+                image = self._preprocess(\n+                    image,\n+                    do_resize=do_resize,\n+                    resample=resample,\n+                    do_rescale=do_rescale,\n+                    rescale_factor=rescale_factor,\n+                    do_normalize=do_normalize,\n+                    image_mean=image_mean,\n+                    image_std=image_std,\n+                    data_format=data_format,\n+                    do_convert_rgb=do_convert_rgb,\n+                    input_data_format=input_data_format,\n+                )\n+                pixel_values.extend(image)\n \n         image_sizes = [image.shape[-2:] for image in pixel_values]\n         if do_pad:\n@@ -471,7 +449,7 @@ def postprocess(\n         image_mean = image_mean if image_mean is not None else self.image_mean\n         image_std = image_std if image_std is not None else self.image_std\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n         if isinstance(images[0], Image.Image):\n             return images if len(images) > 1 else images[0]\n "
        },
        {
            "sha": "a5f482ef9b4043c5af86823a85451757299ff88d",
            "filename": "src/transformers/models/eomt/image_processing_eomt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -32,7 +32,6 @@\n     get_image_size,\n     infer_channel_dimension_format,\n     make_flat_list_of_images,\n-    make_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -615,7 +614,7 @@ def preprocess(\n         )\n \n         if segmentation_maps is not None:\n-            segmentation_maps = make_list_of_images(segmentation_maps, expected_ndims=2)\n+            segmentation_maps = make_flat_list_of_images(segmentation_maps, expected_ndims=2)\n             segmentation_maps = [to_numpy_array(mask) for mask in segmentation_maps]\n \n             segmentation_maps = ["
        },
        {
            "sha": "9dcc2c85d479404d78dbb0699dbcbb3970752fb9",
            "filename": "src/transformers/models/flava/image_processing_flava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -32,7 +32,7 @@\n     PILImageResampling,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -637,7 +637,7 @@ def preprocess(\n         codebook_image_mean = codebook_image_mean if codebook_image_mean is not None else self.codebook_image_mean\n         codebook_image_std = codebook_image_std if codebook_image_std is not None else self.codebook_image_std\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "29af98ed507210d983ad618c004b8b4a4a2fdc4d",
            "filename": "src/transformers/models/fuyu/image_processing_fuyu.py",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Ffuyu%2Fimage_processing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Ffuyu%2Fimage_processing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fimage_processing_fuyu.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -464,17 +464,23 @@ def preprocess(\n         # All transformations expect numpy arrays.\n         batch_images = [[to_numpy_array(image) for image in images] for images in batch_images]\n \n-        if do_rescale and is_scaled_image(batch_images[0][0]):\n+        # Search for the first image in the image list.\n+        # NOTE: we can't slice the first image with images_list[0][0] if the first batch contains no images. See #36682\n+        first_image_in_list = [images for images in batch_images if images][0][0]\n+\n+        if do_rescale and is_scaled_image(first_image_in_list):\n             logger.warning_once(\n                 \"It looks like you are trying to rescale already rescaled images. If the input\"\n                 \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n             )\n \n         if input_data_format is None:\n             # We assume that all images have the same channel dimension format.\n-            input_data_format = infer_channel_dimension_format(batch_images[0][0])\n+            input_data_format = infer_channel_dimension_format(first_image_in_list)\n \n-        original_image_sizes = [get_image_size(images[0], channel_dim=input_data_format) for images in batch_images]\n+        original_image_sizes = [\n+            get_image_size(images[0], channel_dim=input_data_format) for images in batch_images if images\n+        ]\n         size = get_size_dict(size)  # for BC\n \n         if do_resize:\n@@ -483,7 +489,7 @@ def preprocess(\n                 for images in batch_images\n             ]\n \n-        image_sizes = [get_image_size(images[0], channel_dim=input_data_format) for images in batch_images]\n+        image_sizes = [get_image_size(images[0], channel_dim=input_data_format) for images in batch_images if images]\n         image_unpadded_heights = [[image_size[0]] for image_size in image_sizes]\n         image_unpadded_widths = [[image_size[1]] for image_size in image_sizes]\n "
        },
        {
            "sha": "59b8738076fc06fb9b6915fce1a5abc84ebb984b",
            "filename": "src/transformers/models/fuyu/processing_fuyu.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -561,7 +561,7 @@ def __call__(\n \n         # --- Use self.tokenizer to get the ids of special tokens to insert into image ids ---\n \n-        tensor_batch_images = torch.stack([img[0] for img in batch_images]).unsqueeze(1)\n+        tensor_batch_images = torch.stack([img[0] for img in batch_images if img]).unsqueeze(1)\n \n         # --- Use self.image_processor again to obtain the full token ids and batch inputs ---\n         all_encodings = []"
        },
        {
            "sha": "2e2f246bc333a4a2e688d2a66283481298220cad",
            "filename": "src/transformers/models/glm4v/image_processing_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -35,7 +35,6 @@\n     infer_channel_dimension_format,\n     is_scaled_image,\n     make_flat_list_of_images,\n-    make_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -219,7 +218,7 @@ def _preprocess(\n                 - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n                 - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.   - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n         \"\"\"\n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if do_convert_rgb:\n             images = [convert_to_rgb(image) for image in images]"
        },
        {
            "sha": "e3e0255e2b47c3fcb1a617a5793502d6babb5537",
            "filename": "src/transformers/models/glpn/image_processing_glpn.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fglpn%2Fimage_processing_glpn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fglpn%2Fimage_processing_glpn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglpn%2Fimage_processing_glpn.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -34,7 +34,7 @@\n     infer_channel_dimension_format,\n     is_scaled_image,\n     is_torch_available,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -186,7 +186,7 @@ def preprocess(\n         size_divisor = size_divisor if size_divisor is not None else self.size_divisor\n         resample = resample if resample is not None else self.resample\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "24e3c8b3f9873d59a596433492a6f8fef8b9054f",
            "filename": "src/transformers/models/grounding_dino/image_processing_grounding_dino.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -44,7 +44,7 @@\n     get_image_size,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_annotations,\n@@ -1426,7 +1426,7 @@ def preprocess(\n         pad_size = self.pad_size if pad_size is None else pad_size\n         format = self.format if format is None else format\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "fe9085331cdeda9eeb3b94a4df00fd72f05c7776",
            "filename": "src/transformers/models/idefics/image_processing_idefics.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fidefics%2Fimage_processing_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fidefics%2Fimage_processing_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fimage_processing_idefics.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -24,7 +24,7 @@\n     ChannelDimension,\n     ImageInput,\n     PILImageResampling,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n )\n@@ -152,7 +152,7 @@ def preprocess(\n             return []\n \n         images = self.fetch_images(images)\n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "5f20669c8a8fe81ca08cde7c96df70cb20d13a12",
            "filename": "src/transformers/models/idefics/processing_idefics.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -358,9 +358,10 @@ def __call__(\n             if not all(isinstance(i, str) for i in text):\n                 raise ValueError(\"When using the image-text-to-text behavior, the prompts should only contain text.\")\n             if isinstance(images[0], (list, tuple)):\n-                # if nested images, nest text as well\n-                text = [[i] for i in text]\n-            prompts = list(zip(images, text))\n+                # if nested images, un-nest each sublist and create `prompts`\n+                prompts = [[sample, *image_list] for image_list, sample in zip(images, text)]\n+            else:\n+                prompts = list(zip(images, text))\n \n         output_kwargs = self._merge_kwargs(\n             IdeficsProcessorKwargs,"
        },
        {
            "sha": "51ed8f13a6cea852138f3ab212f7023db51a7084",
            "filename": "src/transformers/models/idefics2/image_processing_idefics2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -497,16 +497,19 @@ def preprocess(\n \n         # All transformations expect numpy arrays.\n         images_list = [[to_numpy_array(image) for image in images] for images in images_list]\n+        # Search for the first image in the image list.\n+        # NOTE: we can't slice the first image with images_list[0][0] if the first batch contains no images. See #36682\n+        first_image_in_list = [images for images in images_list if images][0][0]\n \n-        if do_rescale and is_scaled_image(images_list[0][0]):\n+        if do_rescale and is_scaled_image(first_image_in_list):\n             logger.warning_once(\n                 \"It looks like you are trying to rescale already rescaled images. If the input\"\n                 \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n             )\n \n         if input_data_format is None:\n             # We assume that all images have the same channel dimension format.\n-            input_data_format = infer_channel_dimension_format(images_list[0][0])\n+            input_data_format = infer_channel_dimension_format(first_image_in_list)\n \n         if do_image_splitting:\n             new_images_list = []"
        },
        {
            "sha": "24afcdd6e02d8e2fd3977657172f1a08622cea0d",
            "filename": "src/transformers/models/idefics3/image_processing_idefics3.py",
            "status": "modified",
            "additions": 11,
            "deletions": 6,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -526,7 +526,7 @@ def _pad_image(\n \n     def pad(\n         self,\n-        images: list[np.ndarray],\n+        images: list[list[np.ndarray]],\n         constant_values: Union[float, Iterable[float]] = 0,\n         return_pixel_mask: bool = True,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n@@ -537,7 +537,7 @@ def pad(\n         For a list of images, for each images, pads a batch of images to the bottom and right of the image with zeros to the size of largest height and width.\n         For each sample in the batch, pads the sample with empty images to the max_number of images per sample in the batch. Optionally returns a pixel mask.\n         Args:\n-            images (`list[np.ndarray]`):\n+            images (`list[list[np.ndarray]]`):\n                 List of list of images to pad. Pads to the largest height and width in the batch.\n             constant_values (`float` or `Iterable[float]`, *optional*):\n                 The value to use for the padding if `mode` is `\"constant\"`.\n@@ -565,11 +565,13 @@ def pad(\n             else input_data_format\n         )\n         data_format = input_data_format if data_format is None else data_format\n+        # filter out empty image lists, then take first image of the first sample\n+        first_image_in_list = [sample_images for sample_images in images if sample_images][0][0]\n \n         if input_data_format == ChannelDimension.FIRST:\n-            n_channels = images[0][0].shape[0]\n+            n_channels = first_image_in_list.shape[0]\n         elif input_data_format == ChannelDimension.LAST:\n-            n_channels = images[0][0].shape[-1]\n+            n_channels = first_image_in_list.shape[-1]\n         else:\n             raise ValueError(\"Invalid channel dimension format.\")\n \n@@ -715,6 +717,9 @@ def preprocess(\n \n         # All transformations expect numpy arrays.\n         images_list = [[to_numpy_array(image) for image in images] for images in images_list]\n+        # Search for the first image in the image list.\n+        # NOTE: we can't slice the first image with images_list[0][0] if the first batch contains no images. See #36682\n+        first_image_in_list = [images for images in images_list if images][0][0]\n \n         # Extra channel dimension for grayscale images\n         if input_data_format in [ChannelDimension.LAST, None]:\n@@ -726,15 +731,15 @@ def preprocess(\n                 [np.expand_dims(img, axis=0) if img.ndim == 2 else img for img in images] for images in images_list\n             ]\n \n-        if do_rescale and is_scaled_image(images_list[0][0]):\n+        if do_rescale and is_scaled_image(first_image_in_list):\n             logger.warning_once(\n                 \"It looks like you are trying to rescale already rescaled images. If the input\"\n                 \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n             )\n \n         # We assume that all images have the same channel dimension format.\n         if input_data_format is None:\n-            input_data_format = infer_channel_dimension_format(images_list[0][0], num_channels=(1, 3, 4))\n+            input_data_format = infer_channel_dimension_format(first_image_in_list, num_channels=(1, 3, 4))\n \n         if do_resize:\n             images_list = ["
        },
        {
            "sha": "7d55a9a9beb8f42c3831f57bb8d9826233541fdf",
            "filename": "src/transformers/models/imagegpt/image_processing_imagegpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -26,7 +26,7 @@\n     PILImageResampling,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -238,7 +238,7 @@ def preprocess(\n         clusters = clusters if clusters is not None else self.clusters\n         clusters = np.array(clusters)\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "3e045d0aa264646728a79a2e479ff736cffb9301",
            "filename": "src/transformers/models/janus/image_processing_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -36,7 +36,6 @@\n     infer_channel_dimension_format,\n     is_scaled_image,\n     make_flat_list_of_images,\n-    make_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -434,7 +433,7 @@ def postprocess(\n         image_mean = image_mean if image_mean is not None else self.image_mean\n         image_std = image_std if image_std is not None else self.image_std\n \n-        images = make_list_of_images(images)  # Ensures input is a list\n+        images = make_flat_list_of_images(images)  # Ensures input is a list\n \n         if isinstance(images[0], PIL.Image.Image):\n             return images if len(images) > 1 else images[0]"
        },
        {
            "sha": "9fa52bf78d9dfe131828737e53a575da09d45363",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -36,7 +36,7 @@\n     PILImageResampling,\n     get_image_size,\n     infer_channel_dimension_format,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n )\n from ...modeling_outputs import ModelOutput\n@@ -1519,7 +1519,7 @@ def postprocess(\n         image_mean = image_mean if image_mean is not None else self.image_mean\n         image_std = image_std if image_std is not None else self.image_std\n \n-        images = make_list_of_images(images)  # Ensures input is a list\n+        images = make_flat_list_of_images(images)  # Ensures input is a list\n \n         if isinstance(images[0], PIL.Image.Image):\n             return images if len(images) > 1 else images[0]"
        },
        {
            "sha": "0289f82b650d2ea109142b5767974034a81551ca",
            "filename": "src/transformers/models/kosmos2/processing_kosmos2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -20,7 +20,7 @@\n from typing import Optional, Union\n \n from ...image_processing_utils import BatchFeature\n-from ...image_utils import ImageInput, is_batched\n+from ...image_utils import ImageInput\n from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, TextKwargs, Unpack\n from ...tokenization_utils import AddedToken\n from ...tokenization_utils_base import BatchEncoding, TextInput\n@@ -372,7 +372,7 @@ def preprocess_examples(\n \n         if images is None:\n             images = [None] * len(texts)\n-        elif not is_batched(images):\n+        elif not isinstance(images, list):\n             images = [images]\n         if len(texts) != len(images):\n             raise ValueError("
        },
        {
            "sha": "e68ad2462d071a9a95a72755c2c94a39aaabc41f",
            "filename": "src/transformers/models/kosmos2_5/image_processing_kosmos2_5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -30,7 +30,7 @@\n     ImageInput,\n     get_image_size,\n     infer_channel_dimension_format,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n )\n@@ -296,7 +296,7 @@ def preprocess(\n         if kwargs.get(\"data_format\") is not None:\n             raise ValueError(\"data_format is not an accepted input as the outputs are \")\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "42b00b2f5c3e3d20a1cceb522fc8b2327e70915d",
            "filename": "src/transformers/models/layoutlmv2/image_processing_layoutlmv2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -25,7 +25,7 @@\n     ImageInput,\n     PILImageResampling,\n     infer_channel_dimension_format,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -251,7 +251,7 @@ def preprocess(\n         ocr_lang = ocr_lang if ocr_lang is not None else self.ocr_lang\n         tesseract_config = tesseract_config if tesseract_config is not None else self.tesseract_config\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "8189abf673119cccbf87387f65e15f8dd43bde80",
            "filename": "src/transformers/models/layoutlmv3/image_processing_layoutlmv3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -29,7 +29,7 @@\n     PILImageResampling,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -306,7 +306,7 @@ def preprocess(\n         apply_ocr = apply_ocr if apply_ocr is not None else self.apply_ocr\n         ocr_lang = ocr_lang if ocr_lang is not None else self.ocr_lang\n         tesseract_config = tesseract_config if tesseract_config is not None else self.tesseract_config\n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "bece4f1a00c2dc764dcf0b8aa6ee8a482ae333ac",
            "filename": "src/transformers/models/levit/image_processing_levit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Flevit%2Fimage_processing_levit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Flevit%2Fimage_processing_levit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flevit%2Fimage_processing_levit.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -33,7 +33,7 @@\n     PILImageResampling,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -255,7 +255,7 @@ def preprocess(\n         size = get_size_dict(size, default_to_square=False)\n         crop_size = crop_size if crop_size is not None else self.crop_size\n         crop_size = get_size_dict(crop_size, param_name=\"crop_size\")\n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "bbf8ab1421f843fd43e52d015bf6a020e01106b0",
            "filename": "src/transformers/models/llava/image_processing_llava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -34,7 +34,7 @@\n     get_image_size,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_kwargs,\n@@ -368,7 +368,7 @@ def preprocess(\n         validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self._valid_processor_keys)\n \n         images = self.fetch_images(images)\n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "5a8b0bdde6e1e3db58b2918fd49e7b7a5e907bf3",
            "filename": "src/transformers/models/llava_next/image_processing_llava_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -44,7 +44,6 @@\n     infer_channel_dimension_format,\n     is_scaled_image,\n     make_flat_list_of_images,\n-    make_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -373,7 +372,7 @@ def _preprocess(\n                 - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n                 - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n         \"\"\"\n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         all_images = []\n         for image in images:"
        },
        {
            "sha": "787691682999f797ff22012d8c87e14f2c9f9785",
            "filename": "src/transformers/models/llava_next_video/image_processing_llava_next_video.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fimage_processing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fimage_processing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fimage_processing_llava_next_video.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -33,7 +33,7 @@\n     PILImageResampling,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     validate_preprocess_arguments,\n )\n@@ -234,7 +234,7 @@ def _preprocess(\n                 - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n                 - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n         \"\"\"\n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if do_convert_rgb:\n             images = [convert_to_rgb(image) for image in images]"
        },
        {
            "sha": "5f9639d0f9db981d0a684be0297c4e391106fc03",
            "filename": "src/transformers/models/mask2former/image_processing_mask2former.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -36,7 +36,7 @@\n     get_image_size,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -761,9 +761,9 @@ def preprocess(\n                 \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n             )\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n         if segmentation_maps is not None:\n-            segmentation_maps = make_list_of_images(segmentation_maps, expected_ndims=2)\n+            segmentation_maps = make_flat_list_of_images(segmentation_maps, expected_ndims=2)\n \n         if segmentation_maps is not None and len(images) != len(segmentation_maps):\n             raise ValueError(\"Images and segmentation maps must have the same length.\")"
        },
        {
            "sha": "5437af6df9ecdfcf1f1031c05a611a471e1cda48",
            "filename": "src/transformers/models/maskformer/image_processing_maskformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -37,7 +37,7 @@\n     get_image_size,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -764,9 +764,9 @@ def preprocess(\n                 \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n             )\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n         if segmentation_maps is not None:\n-            segmentation_maps = make_list_of_images(segmentation_maps, expected_ndims=2)\n+            segmentation_maps = make_flat_list_of_images(segmentation_maps, expected_ndims=2)\n \n         if segmentation_maps is not None and len(images) != len(segmentation_maps):\n             raise ValueError(\"Images and segmentation maps must have the same length.\")"
        },
        {
            "sha": "8b07157875450ff8fc55b450ce84752728c2e42b",
            "filename": "src/transformers/models/mobilenet_v1/image_processing_mobilenet_v1.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fimage_processing_mobilenet_v1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fimage_processing_mobilenet_v1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fimage_processing_mobilenet_v1.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -32,7 +32,7 @@\n     PILImageResampling,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -245,7 +245,7 @@ def preprocess(\n         image_mean = image_mean if image_mean is not None else self.image_mean\n         image_std = image_std if image_std is not None else self.image_std\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "7f261c5c45c36152b86531836dd2766c32c53c09",
            "filename": "src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -32,7 +32,7 @@\n     PILImageResampling,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -407,10 +407,10 @@ def preprocess(\n         image_mean = image_mean if image_mean is not None else self.image_mean\n         image_std = image_std if image_std is not None else self.image_std\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if segmentation_maps is not None:\n-            segmentation_maps = make_list_of_images(segmentation_maps, expected_ndims=2)\n+            segmentation_maps = make_flat_list_of_images(segmentation_maps, expected_ndims=2)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "1207ec0da399a7ef47a9ed9c88cb96738faae580",
            "filename": "src/transformers/models/mobilevit/image_processing_mobilevit.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -26,7 +26,7 @@\n     PILImageResampling,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -404,12 +404,12 @@ def preprocess(\n \n         do_reduce_labels = do_reduce_labels if do_reduce_labels is not None else self.do_reduce_labels\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if segmentation_maps is not None:\n-            segmentation_maps = make_list_of_images(segmentation_maps, expected_ndims=2)\n+            segmentation_maps = make_flat_list_of_images(segmentation_maps, expected_ndims=2)\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "79aa421d38644f100af9cbd5c0694c30a1621570",
            "filename": "src/transformers/models/nougat/image_processing_nougat.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -35,7 +35,7 @@\n     get_image_size,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -448,7 +448,7 @@ def preprocess(\n         image_mean = image_mean if image_mean is not None else self.image_mean\n         image_std = image_std if image_std is not None else self.image_std\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "d88b373418c6282ffefc56bf6b3257313e0d3a0c",
            "filename": "src/transformers/models/oneformer/image_processing_oneformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -39,7 +39,7 @@\n     get_image_size,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -722,9 +722,9 @@ def preprocess(\n                 \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n             )\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n         if segmentation_maps is not None:\n-            segmentation_maps = make_list_of_images(segmentation_maps, expected_ndims=2)\n+            segmentation_maps = make_flat_list_of_images(segmentation_maps, expected_ndims=2)\n \n         if segmentation_maps is not None and len(images) != len(segmentation_maps):\n             raise ValueError(\"Images and segmentation maps must have the same length.\")"
        },
        {
            "sha": "c228dc4a062577a3ceebc43c5cf9affd0e309bd1",
            "filename": "src/transformers/models/owlv2/image_processing_owlv2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -34,7 +34,7 @@\n     get_image_size,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -434,7 +434,7 @@ def preprocess(\n         size = size if size is not None else self.size\n         size = get_size_dict(size)  # for BC\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "8040248039d4491ae1143b12a003c59e6fcc5bb9",
            "filename": "src/transformers/models/owlvit/image_processing_owlvit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fowlvit%2Fimage_processing_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fowlvit%2Fimage_processing_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fimage_processing_owlvit.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -35,7 +35,7 @@\n     PILImageResampling,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -382,7 +382,7 @@ def preprocess(\n         image_mean = image_mean if image_mean is not None else self.image_mean\n         image_std = image_std if image_std is not None else self.image_std\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "8506713247128b6999632bbfcafe4ed470055b45",
            "filename": "src/transformers/models/perceiver/image_processing_perceiver.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fperceiver%2Fimage_processing_perceiver.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fperceiver%2Fimage_processing_perceiver.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperceiver%2Fimage_processing_perceiver.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -29,7 +29,7 @@\n     get_image_size,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -286,7 +286,7 @@ def preprocess(\n         image_mean = image_mean if image_mean is not None else self.image_mean\n         image_std = image_std if image_std is not None else self.image_std\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "111179796d8a1fdb9595c5b20f37b29818ff9431",
            "filename": "src/transformers/models/pix2struct/image_processing_pix2struct.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fimage_processing_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fimage_processing_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fimage_processing_pix2struct.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -28,7 +28,7 @@\n     ImageInput,\n     get_image_size,\n     infer_channel_dimension_format,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n )\n@@ -407,7 +407,7 @@ def preprocess(\n         if kwargs.get(\"data_format\") is not None:\n             raise ValueError(\"data_format is not an accepted input as the outputs are \")\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "c11a3fddf4a423773a312628e8d7c95631a0645a",
            "filename": "src/transformers/models/poolformer/image_processing_poolformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -32,7 +32,7 @@\n     PILImageResampling,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -291,7 +291,7 @@ def preprocess(\n         crop_size = crop_size if crop_size is not None else self.crop_size\n         crop_size = get_size_dict(crop_size, param_name=\"crop_size\")\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "b5323c308ca4d7d453803e2f88e905df65709ae9",
            "filename": "src/transformers/models/prompt_depth_anything/image_processing_prompt_depth_anything.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -34,7 +34,7 @@\n     infer_channel_dimension_format,\n     is_scaled_image,\n     is_torch_available,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -368,7 +368,7 @@ def preprocess(\n         do_pad = do_pad if do_pad is not None else self.do_pad\n         size_divisor = size_divisor if size_divisor is not None else self.size_divisor\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError(\n@@ -432,7 +432,7 @@ def preprocess(\n         if prompt_depth is not None:\n             # prompt_depth is a list of images with shape (height, width)\n             # we need to convert it to a list of images with shape (1, height, width)\n-            prompt_depths = make_list_of_images(prompt_depth, expected_ndims=2)\n+            prompt_depths = make_flat_list_of_images(prompt_depth, expected_ndims=2)\n \n             # Validate prompt_depths has same length as images\n             if len(prompt_depths) != len(images):"
        },
        {
            "sha": "d0c594e47e3676670042b4ed25ccc4c11bcd10e4",
            "filename": "src/transformers/models/pvt/image_processing_pvt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fpvt%2Fimage_processing_pvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fpvt%2Fimage_processing_pvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpvt%2Fimage_processing_pvt.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -28,7 +28,7 @@\n     PILImageResampling,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -216,7 +216,7 @@ def preprocess(\n         size = size if size is not None else self.size\n         size_dict = get_size_dict(size)\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "cfc76f495f8316b5874b6a2f7d33b8b913bb2a2a",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -40,7 +40,6 @@\n     infer_channel_dimension_format,\n     is_scaled_image,\n     make_flat_list_of_images,\n-    make_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -224,7 +223,7 @@ def _preprocess(\n                 - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n                 - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.   - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n         \"\"\"\n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if do_convert_rgb:\n             images = [convert_to_rgb(image) for image in images]"
        },
        {
            "sha": "de61a8019047dbc97761c151d345d06d16cdca4d",
            "filename": "src/transformers/models/rt_detr/image_processing_rt_detr.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -42,7 +42,7 @@\n     get_image_size,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_annotations,\n@@ -893,7 +893,7 @@ def preprocess(\n         pad_size = self.pad_size if pad_size is None else pad_size\n         format = self.format if format is None else format\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError(\n@@ -926,7 +926,7 @@ def preprocess(\n         if annotations is not None:\n             validate_annotations(format, SUPPORTED_ANNOTATION_FORMATS, annotations)\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n         if not valid_images(images):\n             raise ValueError(\n                 \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \""
        },
        {
            "sha": "ea149b256f7be1f479889df1a55c787b762ad7f1",
            "filename": "src/transformers/models/sam/image_processing_sam.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -32,7 +32,7 @@\n     get_image_size,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -494,7 +494,7 @@ def preprocess(\n         mask_pad_size = get_size_dict(mask_pad_size, default_to_square=True)\n         do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError(\n@@ -503,7 +503,7 @@ def preprocess(\n             )\n \n         if segmentation_maps is not None:\n-            segmentation_maps = make_list_of_images(segmentation_maps, expected_ndims=2)\n+            segmentation_maps = make_flat_list_of_images(segmentation_maps, expected_ndims=2)\n \n             if not valid_images(segmentation_maps):\n                 raise ValueError("
        },
        {
            "sha": "a651795ca0cff2fa0266afae3979b877f4fde487",
            "filename": "src/transformers/models/segformer/image_processing_segformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -28,7 +28,7 @@\n     PILImageResampling,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -368,10 +368,10 @@ def preprocess(\n         image_mean = image_mean if image_mean is not None else self.image_mean\n         image_std = image_std if image_std is not None else self.image_std\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if segmentation_maps is not None:\n-            segmentation_maps = make_list_of_images(segmentation_maps, expected_ndims=2)\n+            segmentation_maps = make_flat_list_of_images(segmentation_maps, expected_ndims=2)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "c60cad80d7fd2f30fabc33105eeffd2b678bea3d",
            "filename": "src/transformers/models/seggpt/image_processing_seggpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fseggpt%2Fimage_processing_seggpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fseggpt%2Fimage_processing_seggpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseggpt%2Fimage_processing_seggpt.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -28,7 +28,7 @@\n     PILImageResampling,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n )\n@@ -323,7 +323,7 @@ def _preprocess_step(\n         size_dict = get_size_dict(size)\n \n         # If segmentation map is passed we expect 2D images\n-        images = make_list_of_images(images, expected_ndims=2 if do_convert_rgb else 3)\n+        images = make_flat_list_of_images(images, expected_ndims=2 if do_convert_rgb else 3)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "412f231bef9165bbd5a2870ad39e16db22565e63",
            "filename": "src/transformers/models/shieldgemma2/processing_shieldgemma2.py",
            "status": "modified",
            "additions": 29,
            "deletions": 12,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fprocessing_shieldgemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fprocessing_shieldgemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fprocessing_shieldgemma2.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -154,19 +154,36 @@ def __call__(\n         messages = []\n         expanded_images = []\n         for img in images:\n+            if not isinstance(img, list):\n+                img = [img]\n+            elif len(img) > 1:\n+                raise ValueError(f\"SheildGemma can process at most one image per sample, but got {len(img)} images\")\n+\n             for policy in policies:\n-                messages.append(\n-                    [\n-                        {\n-                            \"role\": \"user\",\n-                            \"content\": [\n-                                {\"type\": \"image\"},\n-                                {\"type\": \"text\", \"text\": policy_definitions[policy]},\n-                            ],\n-                        }\n-                    ]\n-                )\n-                expanded_images.append([img])\n+                if img:\n+                    messages.append(\n+                        [\n+                            {\n+                                \"role\": \"user\",\n+                                \"content\": [\n+                                    {\"type\": \"image\"},\n+                                    {\"type\": \"text\", \"text\": policy_definitions[policy]},\n+                                ],\n+                            }\n+                        ]\n+                    )\n+                else:\n+                    messages.append(\n+                        [\n+                            {\n+                                \"role\": \"user\",\n+                                \"content\": [\n+                                    {\"type\": \"text\", \"text\": policy_definitions[policy]},\n+                                ],\n+                            }\n+                        ]\n+                    )\n+                expanded_images.append(img)\n \n         text = self.apply_chat_template(messages, tokenize=False)\n         return super().__call__(images=expanded_images, text=text, **kwargs)"
        },
        {
            "sha": "12ae2cfb8e82623a32189f780007d5a140eae6ba",
            "filename": "src/transformers/models/smolvlm/image_processing_smolvlm.py",
            "status": "modified",
            "additions": 11,
            "deletions": 6,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -523,7 +523,7 @@ def _pad_image(\n \n     def pad(\n         self,\n-        images: list[np.ndarray],\n+        images: list[list[np.ndarray]],\n         constant_values: Union[float, Iterable[float]] = 0,\n         return_pixel_mask: bool = True,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n@@ -534,7 +534,7 @@ def pad(\n         For a list of images, for each images, pads a batch of images to the bottom and right of the image with zeros to the size of largest height and width.\n         For each sample in the batch, pads the sample with empty images to the max_number of images per sample in the batch. Optionally returns a pixel mask.\n         Args:\n-            images (`list[np.ndarray]`):\n+            images (`list[list[np.ndarray]]`):\n                 List of list of images to pad. Pads to the largest height and width in the batch.\n             constant_values (`float` or `Iterable[float]`, *optional*):\n                 The value to use for the padding if `mode` is `\"constant\"`.\n@@ -562,11 +562,13 @@ def pad(\n             else input_data_format\n         )\n         data_format = input_data_format if data_format is None else data_format\n+        # filter out empty image lists, then take first image of the first sample\n+        first_image_in_list = [sample_images for sample_images in images if sample_images][0][0]\n \n         if input_data_format == ChannelDimension.FIRST:\n-            n_channels = images[0][0].shape[0]\n+            n_channels = first_image_in_list.shape[0]\n         elif input_data_format == ChannelDimension.LAST:\n-            n_channels = images[0][0].shape[-1]\n+            n_channels = first_image_in_list.shape[-1]\n         else:\n             raise ValueError(\"Invalid channel dimension format.\")\n \n@@ -712,6 +714,9 @@ def preprocess(\n \n         # All transformations expect numpy arrays.\n         images_list = [[to_numpy_array(image) for image in images] for images in images_list]\n+        # Search for the first image in the image list.\n+        # NOTE: we can't slice the first image with images_list[0][0] if the first batch contains no images. See #36682\n+        first_image_in_list = [images for images in images_list if images][0][0]\n \n         # Extra channel dimension for grayscale images\n         if input_data_format in [ChannelDimension.LAST, None]:\n@@ -723,15 +728,15 @@ def preprocess(\n                 [np.expand_dims(img, axis=0) if img.ndim == 2 else img for img in images] for images in images_list\n             ]\n \n-        if do_rescale and is_scaled_image(images_list[0][0]):\n+        if do_rescale and is_scaled_image(first_image_in_list):\n             logger.warning_once(\n                 \"It looks like you are trying to rescale already rescaled images. If the input\"\n                 \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n             )\n \n         # We assume that all images have the same channel dimension format.\n         if input_data_format is None:\n-            input_data_format = infer_channel_dimension_format(images_list[0][0], num_channels=(1, 3, 4))\n+            input_data_format = infer_channel_dimension_format(first_image_in_list, num_channels=(1, 3, 4))\n \n         if do_resize:\n             images_list = ["
        },
        {
            "sha": "7894e34e227df24dd7362b378f6fdf44dbe35450",
            "filename": "src/transformers/models/superpoint/image_processing_superpoint.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -26,7 +26,7 @@\n     PILImageResampling,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n )\n@@ -245,7 +245,7 @@ def preprocess(\n         size = size if size is not None else self.size\n         size = get_size_dict(size, default_to_square=False)\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "d36ce936c2f18f7a60768bcd1ccf292d5a51b8d5",
            "filename": "src/transformers/models/swin2sr/image_processing_swin2sr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -25,7 +25,7 @@\n     ImageInput,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -159,7 +159,7 @@ def preprocess(\n         do_pad = do_pad if do_pad is not None else self.do_pad\n         pad_size = pad_size if pad_size is not None else self.pad_size\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "cff4b991d8f37e5b654c2e6c998c250eec73975b",
            "filename": "src/transformers/models/textnet/image_processing_textnet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -33,7 +33,7 @@\n     PILImageResampling,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_kwargs,\n@@ -290,7 +290,7 @@ def preprocess(\n \n         validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self._valid_processor_keys)\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "89c8883cbd6a1680ab85a23e1059e82e9b0a143d",
            "filename": "src/transformers/models/timm_wrapper/image_processing_timm_wrapper.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fimage_processing_timm_wrapper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fimage_processing_timm_wrapper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fimage_processing_timm_wrapper.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -20,7 +20,7 @@\n \n from ...image_processing_utils import BaseImageProcessor, BatchFeature\n from ...image_transforms import to_pil_image\n-from ...image_utils import ImageInput, make_list_of_images\n+from ...image_utils import ImageInput, make_flat_list_of_images\n from ...utils import TensorType, logging, requires_backends\n from ...utils.import_utils import is_timm_available, is_torch_available, requires\n \n@@ -121,7 +121,7 @@ def preprocess(\n             # Add batch dimension if a single image\n             images = images.unsqueeze(0) if images.ndim == 3 else images\n         else:\n-            images = make_list_of_images(images)\n+            images = make_flat_list_of_images(images)\n             images = [to_pil_image(image) for image in images]\n             images = torch.stack([self.val_transforms(image) for image in images])\n "
        },
        {
            "sha": "90c39ea49fb03992fb3715a2f96be8d6182af9ff",
            "filename": "src/transformers/models/tvp/image_processing_tvp_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp_fast.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -101,9 +101,6 @@ def preprocess(\n         videos: Union[ImageInput, list[ImageInput], list[list[ImageInput]]],\n         **kwargs: Unpack[TvpFastImageProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Preprocess videos using the fast image processor.\n-        \"\"\"\n         return super().preprocess(videos, **kwargs)\n \n     def _further_process_kwargs("
        },
        {
            "sha": "eb9fbfa178888e1bef038deb72e50d7793f2a137",
            "filename": "src/transformers/models/video_llava/image_processing_video_llava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fimage_processing_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fimage_processing_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fimage_processing_video_llava.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -33,7 +33,7 @@\n     PILImageResampling,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -258,7 +258,7 @@ def preprocess(\n \n         if images is not None:\n             images = self.fetch_images(images)\n-            images = make_list_of_images(images)\n+            images = make_flat_list_of_images(images)\n \n         if images is not None and not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "a624d957013f51015817dd278d09e6b525e3ce66",
            "filename": "src/transformers/models/vilt/image_processing_vilt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -30,7 +30,7 @@\n     get_image_size,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -417,7 +417,7 @@ def preprocess(\n         size = size if size is not None else self.size\n         size = get_size_dict(size, default_to_square=False)\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "27de1076cf54dada35490582044e0c5970da4037",
            "filename": "src/transformers/models/vit/image_processing_vit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fvit%2Fimage_processing_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fvit%2Fimage_processing_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit%2Fimage_processing_vit.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -28,7 +28,7 @@\n     PILImageResampling,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -225,7 +225,7 @@ def preprocess(\n         size = size if size is not None else self.size\n         size_dict = get_size_dict(size)\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "891fdb45735998ecef9e7452490b82b1d728e613",
            "filename": "src/transformers/models/vitmatte/image_processing_vitmatte.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -28,7 +28,7 @@\n     get_image_size,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -195,8 +195,8 @@ def preprocess(\n         image_std = image_std if image_std is not None else self.image_std\n         size_divisibility = size_divisibility if size_divisibility is not None else self.size_divisibility\n \n-        images = make_list_of_images(images)\n-        trimaps = make_list_of_images(trimaps, expected_ndims=2)\n+        images = make_flat_list_of_images(images)\n+        trimaps = make_flat_list_of_images(trimaps, expected_ndims=2)\n \n         if not valid_images(trimaps):\n             raise ValueError("
        },
        {
            "sha": "6466ccc1d22f46fc37946e6786c353de4d68ee0e",
            "filename": "src/transformers/models/vitpose/image_processing_vitpose.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -29,7 +29,7 @@\n     ImageInput,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n )\n@@ -484,7 +484,7 @@ def preprocess(\n         image_mean = image_mean if image_mean is not None else self.image_mean\n         image_std = image_std if image_std is not None else self.image_std\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "48a1300191af8ac0bff994235a950014842cc1b3",
            "filename": "src/transformers/models/yolos/image_processing_yolos.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -44,7 +44,7 @@\n     get_image_size,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_annotations,\n@@ -1301,7 +1301,7 @@ def preprocess(\n         format = self.format if format is None else format\n         validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self._valid_processor_keys)\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "90e69105be01203d58ef4c81f67414ce450da0b3",
            "filename": "src/transformers/models/zoedepth/image_processing_zoedepth.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -35,7 +35,7 @@\n     get_image_size,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -385,7 +385,7 @@ def preprocess(\n         image_std = image_std if image_std is not None else self.image_std\n         do_pad = do_pad if do_pad is not None else self.do_pad\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "abb8d4476af732f7d87ed20837d6dafb410d50dc",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -1603,10 +1603,8 @@ def apply_chat_template(\n \n                     # Currently all processors can accept nested list of batches, but not flat list of visuals\n                     # So we'll make a batched list of images and let the processor handle it\n-                    if image_fnames:\n-                        batch_images.append(image_fnames)\n-                    if video_fnames:\n-                        batch_videos.append(video_fnames)\n+                    batch_images.append(image_fnames)\n+                    batch_videos.append(video_fnames)\n \n         prompt, generation_indices = render_jinja_template(\n             conversations=conversations,\n@@ -1636,10 +1634,12 @@ def apply_chat_template(\n             ):\n                 kwargs[\"do_sample_frames\"] = True\n \n+            images_exist = any(len(im) > 0 for im_list in batch_images for im in im_list)\n+            videos_exist = any(len(vid) > 0 for vid_list in batch_videos for vid in vid_list)\n             out = self(\n                 text=prompt,\n-                images=batch_images if batch_images else None,\n-                videos=batch_videos if batch_videos else None,\n+                images=batch_images if images_exist else None,\n+                videos=batch_videos if videos_exist else None,\n                 audio=batch_audios if batch_audios else None,\n                 **kwargs,\n             )"
        },
        {
            "sha": "e03cda179d3cc73df10f74617b4b698cdb97e9e4",
            "filename": "src/transformers/video_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fvideo_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/src%2Ftransformers%2Fvideo_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_utils.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -115,7 +115,7 @@ def is_valid_video_frame(frame):\n def is_valid_video(video):\n     if not isinstance(video, (list, tuple)):\n         return (is_numpy_array(video) or is_torch_tensor(video)) and video.ndim == 4\n-    return all(is_valid_video_frame(frame) for frame in video)\n+    return video and all(is_valid_video_frame(frame) for frame in video)\n \n \n def valid_videos(videos):\n@@ -181,7 +181,7 @@ def make_batched_videos(videos) -> list[Union[\"np.ndarray\", \"torch.Tensor\", \"URL\n     \"\"\"\n     # Early exit for deeply nested list of image frame paths. We shouldn't flatten them\n     try:\n-        if isinstance(videos[0][0][0], str):\n+        if isinstance(videos[0][0], list) and isinstance(videos[0][0][0], str):\n             return [image_paths for sublist in videos for image_paths in sublist]\n     except (IndexError, TypeError):\n         pass\n@@ -202,7 +202,7 @@ def make_batched_videos(videos) -> list[Union[\"np.ndarray\", \"torch.Tensor\", \"URL\n     for item in videos:\n         if isinstance(item, str) or is_valid_video(item):\n             flat_videos_list.append(item)\n-        elif isinstance(item, list):\n+        elif isinstance(item, list) and item:\n             flat_videos_list.extend(make_batched_videos(item))\n \n     flat_videos_list = convert_pil_frames_to_video(flat_videos_list)"
        },
        {
            "sha": "221836db84230a8fa7e3f5fc67288a0ad887e9e6",
            "filename": "tests/models/colpali/test_processing_colpali.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/tests%2Fmodels%2Fcolpali%2Ftest_processing_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/tests%2Fmodels%2Fcolpali%2Ftest_processing_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolpali%2Ftest_processing_colpali.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -282,3 +282,7 @@ def test_model_input_names(self):\n         inputs = processor(images=image_input)\n \n         self.assertSetEqual(set(inputs.keys()), set(processor.model_input_names))\n+\n+    @unittest.skip(\"ColPali can't process text+image inputs at the same time\")\n+    def test_processor_text_has_no_visual(self):\n+        pass"
        },
        {
            "sha": "7346c0d5079c1f74f781628c48d795efb211aafb",
            "filename": "tests/models/colqwen2/test_processing_colqwen2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/tests%2Fmodels%2Fcolqwen2%2Ftest_processing_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/tests%2Fmodels%2Fcolqwen2%2Ftest_processing_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolqwen2%2Ftest_processing_colqwen2.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -281,3 +281,7 @@ def test_model_input_names(self):\n         inputs = processor(images=image_input)\n \n         self.assertSetEqual(set(inputs.keys()), set(processor.model_input_names))\n+\n+    @unittest.skip(\"ColPali can't process text+image inputs at the same time\")\n+    def test_processor_text_has_no_visual(self):\n+        pass"
        },
        {
            "sha": "8ed9b20d168fef143da3dbc45c3c174819be4897",
            "filename": "tests/models/fuyu/test_processing_fuyu.py",
            "status": "modified",
            "additions": 22,
            "deletions": 0,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/tests%2Fmodels%2Ffuyu%2Ftest_processing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/tests%2Fmodels%2Ffuyu%2Ftest_processing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffuyu%2Ftest_processing_fuyu.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -347,6 +347,28 @@ def test_unstructured_kwargs_batched(self):\n \n         self.assertEqual(len(inputs[\"input_ids\"][0]), 7)\n \n+    def test_processor_text_has_no_visual(self):\n+        # Overwritten: Fuyu has a complicated processing so we don't check id values\n+        processor = self.get_processor()\n+\n+        text = self.prepare_text_inputs(batch_size=3, modalities=\"image\")\n+        image_inputs = self.prepare_image_inputs(batch_size=3)\n+        processing_kwargs = {\"return_tensors\": \"pt\", \"padding\": True, \"multi_page\": True}\n+\n+        # Call with nested list of vision inputs\n+        image_inputs_nested = [[image] if not isinstance(image, list) else image for image in image_inputs]\n+        inputs_dict_nested = {\"text\": text, \"images\": image_inputs_nested}\n+        inputs = processor(**inputs_dict_nested, **processing_kwargs)\n+        self.assertTrue(self.text_input_name in inputs)\n+\n+        # Call with one of the samples with no associated vision input\n+        plain_text = \"lower newer\"\n+        image_inputs_nested[0] = []\n+        text[0] = plain_text\n+        inputs_dict_no_vision = {\"text\": text, \"images\": image_inputs_nested}\n+        inputs_nested = processor(**inputs_dict_no_vision, **processing_kwargs)\n+        self.assertTrue(self.text_input_name in inputs_nested)\n+\n \n @require_torch\n class TestImageTextProcessingUtils(unittest.TestCase):"
        },
        {
            "sha": "ffa0f97cd4e42dbf5b56862559d6abc1db2689ec",
            "filename": "tests/models/got_ocr2/test_processing_got_ocr2.py",
            "status": "modified",
            "additions": 24,
            "deletions": 0,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/tests%2Fmodels%2Fgot_ocr2%2Ftest_processing_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/tests%2Fmodels%2Fgot_ocr2%2Ftest_processing_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgot_ocr2%2Ftest_processing_got_ocr2.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -78,3 +78,27 @@ def test_ocr_queries(self):\n         inputs = processor(image_input, return_tensors=\"pt\", crop_to_patches=True, max_patches=6)\n         self.assertEqual(inputs[\"input_ids\"].shape, (1, 1826))\n         self.assertEqual(inputs[\"pixel_values\"].shape, (7, 3, 384, 384))\n+\n+    def test_processor_text_has_no_visual(self):\n+        # Overwritten: requires `multi_page` kwarg to process nested vision inputs\n+        processor = self.get_processor()\n+\n+        text = self.prepare_text_inputs(batch_size=3, modalities=\"image\")\n+        image_inputs = self.prepare_image_inputs(batch_size=3)\n+        processing_kwargs = {\"return_tensors\": \"pt\", \"padding\": True, \"multi_page\": True}\n+\n+        # Call with nested list of vision inputs\n+        image_inputs_nested = [[image] if not isinstance(image, list) else image for image in image_inputs]\n+        inputs_dict_nested = {\"text\": text, \"images\": image_inputs_nested}\n+        inputs = processor(**inputs_dict_nested, **processing_kwargs)\n+        self.assertTrue(self.text_input_name in inputs)\n+\n+        # Call with one of the samples with no associated vision input\n+        plain_text = \"lower newer\"\n+        image_inputs_nested[0] = []\n+        text[0] = plain_text\n+        inputs_dict_no_vision = {\"text\": text, \"images\": image_inputs_nested}\n+        inputs_nested = processor(**inputs_dict_no_vision, **processing_kwargs)\n+        self.assertListEqual(\n+            inputs[self.text_input_name][1:].tolist(), inputs_nested[self.text_input_name][1:].tolist()\n+        )"
        },
        {
            "sha": "1c007dc2d0b4bf57069ca6dcae5d7ab472e977ed",
            "filename": "tests/models/grounding_dino/test_processing_grounding_dino.py",
            "status": "modified",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/tests%2Fmodels%2Fgrounding_dino%2Ftest_processing_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/tests%2Fmodels%2Fgrounding_dino%2Ftest_processing_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgrounding_dino%2Ftest_processing_grounding_dino.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -280,3 +280,30 @@ def test_text_preprocessing_equivalence(self):\n             torch.allclose(inputs1[\"input_ids\"], inputs2[\"input_ids\"]),\n             f\"Input ids are not equal for batched input: {inputs1['input_ids']} != {inputs2['input_ids']}\",\n         )\n+\n+    def test_processor_text_has_no_visual(self):\n+        # Overwritten: text inputs have to be nested as well\n+        processor = self.get_processor()\n+\n+        text = self.prepare_text_inputs(batch_size=3, modalities=\"image\")\n+        image_inputs = self.prepare_image_inputs(batch_size=3)\n+        processing_kwargs = {\"return_tensors\": \"pt\", \"padding\": True}\n+\n+        # Call with nested list of vision inputs\n+        image_inputs_nested = [[image] if not isinstance(image, list) else image for image in image_inputs]\n+        inputs_dict_nested = {\"text\": text, \"images\": image_inputs_nested}\n+        inputs = processor(**inputs_dict_nested, **processing_kwargs)\n+        self.assertTrue(self.text_input_name in inputs)\n+\n+        # Call with one of the samples with no associated vision input\n+        plain_text = [\"lower newer\"]\n+        image_inputs_nested[0] = []\n+        text[0] = plain_text\n+        inputs_dict_no_vision = {\"text\": text, \"images\": image_inputs_nested}\n+        inputs_nested = processor(**inputs_dict_no_vision, **processing_kwargs)\n+\n+        # Check that text samples are same and are expanded with placeholder tokens correctly. First sample\n+        # has no vision input associated, so we skip it and check it has no vision\n+        self.assertListEqual(\n+            inputs[self.text_input_name][1:].tolist(), inputs_nested[self.text_input_name][1:].tolist()\n+        )"
        },
        {
            "sha": "84a1f8229201813a18d8c6e8875382afda90cee9",
            "filename": "tests/models/idefics2/test_processing_idefics2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/tests%2Fmodels%2Fidefics2%2Ftest_processing_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/tests%2Fmodels%2Fidefics2%2Ftest_processing_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics2%2Ftest_processing_idefics2.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -285,17 +285,11 @@ def test_process_interleaved_images_prompts_image_error(self):\n             \"In this other sentence we try some good things<image>\",\n         ]\n         images = [[self.image1], []]\n-        with self.assertRaises(ValueError):\n-            processor(text=text, images=images, padding=True)\n-        images = [[], [self.image2]]\n         with self.assertRaises(ValueError):\n             processor(text=text, images=images, padding=True)\n         images = [self.image1, self.image2]\n         with self.assertRaises(ValueError):\n             processor(text=text, images=images, padding=True)\n-        images = [self.image1]\n-        with self.assertRaises(ValueError):\n-            processor(text=text, images=images, padding=True)\n \n     def test_apply_chat_template(self):\n         # Message contains content which a mix of lists with images and image urls and string"
        },
        {
            "sha": "1940ab1a9171d667bca86a3fd47e52b72b4d1243",
            "filename": "tests/models/idefics3/test_processing_idefics3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/tests%2Fmodels%2Fidefics3%2Ftest_processing_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/tests%2Fmodels%2Fidefics3%2Ftest_processing_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics3%2Ftest_processing_idefics3.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -321,17 +321,11 @@ def test_process_interleaved_images_prompts_image_error(self):\n             \"In this other sentence we try some good things<image>\",\n         ]\n         images = [[self.image1], []]\n-        with self.assertRaises(ValueError):\n-            processor(text=text, images=images, padding=True)\n-        images = [[], [self.image2]]\n         with self.assertRaises(ValueError):\n             processor(text=text, images=images, padding=True)\n         images = [self.image1, self.image2]\n         with self.assertRaises(ValueError):\n             processor(text=text, images=images, padding=True)\n-        images = [self.image1]\n-        with self.assertRaises(ValueError):\n-            processor(text=text, images=images, padding=True)\n \n     def test_apply_chat_template(self):\n         # Message contains content which a mix of lists with images and image urls and string"
        },
        {
            "sha": "b75cf8e5cff75158049181485e3eb2ea5efd01d7",
            "filename": "tests/models/mllama/test_processing_mllama.py",
            "status": "modified",
            "additions": 13,
            "deletions": 2,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/tests%2Fmodels%2Fmllama%2Ftest_processing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/tests%2Fmodels%2Fmllama%2Ftest_processing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_processing_mllama.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -16,6 +16,7 @@\n import shutil\n import tempfile\n import unittest\n+from typing import Optional\n \n import numpy as np\n \n@@ -56,6 +57,14 @@ def tearDownClass(cls):\n     def prepare_processor_dict(self):\n         return {\"chat_template\": \"{% for message in messages %}{% if loop.index0 == 0 %}{{ bos_token }}{% endif %}{{ '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' }}{% if message['content'] is string %}{{ message['content'] }}{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' %}{{ '<|image|>' }}{% elif content['type'] == 'text' %}{{ content['text'] }}{% endif %}{% endfor %}{% endif %}{{ '<|eot_id|>' }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\"}  # fmt: skip\n \n+    # Override as Mllama needs images to be an explicitly nested batch\n+    def prepare_image_inputs(self, batch_size: Optional[int] = None):\n+        \"\"\"This function prepares a list of PIL images for testing\"\"\"\n+        images = super().prepare_image_inputs(batch_size)\n+        if isinstance(images, (list, tuple)):\n+            images = [[image] for image in images]\n+        return images\n+\n     def test_chat_template_is_saved(self):\n         processor_loaded = self.processor_class.from_pretrained(self.tmpdirname)\n         processor_dict_loaded = json.loads(processor_loaded.to_json_string())\n@@ -344,7 +353,6 @@ def test_unstructured_kwargs_batched(self):\n \n         input_str = self.prepare_text_inputs(batch_size=2, modalities=\"image\")\n         image_input = self.prepare_image_inputs(batch_size=2)\n-        image_input = [[image_input[0]], [image_input[1]]]\n         inputs = processor(\n             text=input_str,\n             images=image_input,\n@@ -368,7 +376,6 @@ def test_special_mm_token_truncation(self):\n \n         input_str = self.prepare_text_inputs(batch_size=2, modalities=\"image\")\n         image_input = self.prepare_image_inputs(batch_size=2)\n-        image_input = [[image_input[0]], [image_input[1]]]\n         _ = processor(\n             text=input_str,\n             images=image_input,\n@@ -386,3 +393,7 @@ def test_special_mm_token_truncation(self):\n                 padding=True,\n                 max_length=3,\n             )\n+\n+    @unittest.skip(\"Mllama can't process inouts with no image ttogether with multimodal inputs\")\n+    def test_processor_text_has_no_visual(self):\n+        pass"
        },
        {
            "sha": "b9f8b58ad3f6eafc5a94ec57720b212a355078b9",
            "filename": "tests/models/shieldgemma2/test_processing_shieldgemma2.py",
            "status": "modified",
            "additions": 22,
            "deletions": 0,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/tests%2Fmodels%2Fshieldgemma2%2Ftest_processing_shieldgemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/tests%2Fmodels%2Fshieldgemma2%2Ftest_processing_shieldgemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fshieldgemma2%2Ftest_processing_shieldgemma2.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -210,3 +210,25 @@ def test_kwargs_overrides_default_image_processor_kwargs(self):\n     @unittest.skip(\"ShieldGemma requires images in input, and fails in text-only processing\")\n     def test_apply_chat_template_assistant_mask(self):\n         pass\n+\n+    def test_processor_text_has_no_visual(self):\n+        # Overwritten: Shieldgemma has a complicated processing so we don't check id values\n+        processor = self.get_processor()\n+\n+        text = self.prepare_text_inputs(batch_size=3, modalities=\"image\")\n+        image_inputs = self.prepare_image_inputs(batch_size=3)\n+        processing_kwargs = {\"return_tensors\": \"pt\", \"padding\": True, \"multi_page\": True}\n+\n+        # Call with nested list of vision inputs\n+        image_inputs_nested = [[image] if not isinstance(image, list) else image for image in image_inputs]\n+        inputs_dict_nested = {\"text\": text, \"images\": image_inputs_nested}\n+        inputs = processor(**inputs_dict_nested, **processing_kwargs)\n+        self.assertTrue(self.text_input_name in inputs)\n+\n+        # Call with one of the samples with no associated vision input\n+        plain_text = \"lower newer\"\n+        image_inputs_nested[0] = []\n+        text[0] = plain_text\n+        inputs_dict_no_vision = {\"text\": text, \"images\": image_inputs_nested}\n+        inputs_nested = processor(**inputs_dict_no_vision, **processing_kwargs)\n+        self.assertTrue(self.text_input_name in inputs_nested)"
        },
        {
            "sha": "1634c53f132a8c020c632e76e01ca997ca332790",
            "filename": "tests/models/smolvlm/test_processing_smolvlm.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/tests%2Fmodels%2Fsmolvlm%2Ftest_processing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/tests%2Fmodels%2Fsmolvlm%2Ftest_processing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmolvlm%2Ftest_processing_smolvlm.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -94,6 +94,14 @@ def prepare_processor_dict():\n             \"chat_template\": \"<|im_start|>{% for message in messages %}{{message['role'] | capitalize}}{% if message['content'][0]['type'] == 'image' %}{{':'}}{% else %}{{': '}}{% endif %}{% for line in message['content'] %}{% if line['type'] == 'text' %}{{line['text']}}{% elif line['type'] == 'image' %}{{ '<image>' }}{% endif %}{% endfor %}<end_of_utterance>\\n{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}\",\n         }\n \n+    # Override as SmolVLM needs images/video to be an explicitly nested batch\n+    def prepare_image_inputs(self, batch_size: Optional[int] = None):\n+        \"\"\"This function prepares a list of PIL images for testing\"\"\"\n+        images = super().prepare_image_inputs(batch_size)\n+        if isinstance(images, (list, tuple)):\n+            images = [[image] for image in images]\n+        return images\n+\n     def prepare_video_inputs(self, batch_size: Optional[int] = None):\n         \"\"\"This function prepares a list of numpy videos.\"\"\"\n         video_input = [np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)] * 8\n@@ -334,17 +342,11 @@ def test_process_interleaved_images_prompts_image_error(self):\n             \"In this other sentence we try some good things<image>\",\n         ]\n         images = [[self.image1], []]\n-        with self.assertRaises(ValueError):\n-            processor(text=text, images=images, padding=True)\n-        images = [[], [self.image2]]\n         with self.assertRaises(ValueError):\n             processor(text=text, images=images, padding=True)\n         images = [self.image1, self.image2]\n         with self.assertRaises(ValueError):\n             processor(text=text, images=images, padding=True)\n-        images = [self.image1]\n-        with self.assertRaises(ValueError):\n-            processor(text=text, images=images, padding=True)\n \n     def test_apply_chat_template(self):\n         # Message contains content which a mix of lists with images and image urls and string\n@@ -452,7 +454,6 @@ def test_unstructured_kwargs_batched(self):\n \n         input_str = self.prepare_text_inputs(batch_size=2, modalities=\"image\")\n         image_input = self.prepare_image_inputs(batch_size=2)\n-        image_input = [[image_input[0]], [image_input[1]]]\n         inputs = processor(\n             text=input_str,\n             images=image_input,\n@@ -574,7 +575,6 @@ def test_special_mm_token_truncation(self):\n \n         input_str = self.prepare_text_inputs(batch_size=2, modalities=\"image\")\n         image_input = self.prepare_image_inputs(batch_size=2)\n-        image_input = [[image_input[0]], [image_input[1]]]\n         _ = processor(\n             text=input_str,\n             images=image_input,"
        },
        {
            "sha": "78cb2c36b9f3964b965172f54bb464af48a46cee",
            "filename": "tests/models/udop/test_processing_udop.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/tests%2Fmodels%2Fudop%2Ftest_processing_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/tests%2Fmodels%2Fudop%2Ftest_processing_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fudop%2Ftest_processing_udop.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -196,6 +196,10 @@ def preprocess_data(examples):\n \n         self.assertEqual(len(train_data[\"pixel_values\"]), len(train_data[\"input_ids\"]))\n \n+    @unittest.skip(\"We will not support batch input with and without images for UDOP!\")\n+    def test_processor_text_has_no_visual(self):\n+        pass\n+\n \n # different use cases tests\n @require_sentencepiece"
        },
        {
            "sha": "e1b36cd2445fc27e90c053fb7e1c57be033bed52",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 85,
            "deletions": 0,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/564be6d8950ae781c1b0e93435a4fe7d80e59fc9/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=564be6d8950ae781c1b0e93435a4fe7d80e59fc9",
            "patch": "@@ -170,6 +170,7 @@ def prepare_image_inputs(self, batch_size: Optional[int] = None):\n     def prepare_video_inputs(self, batch_size: Optional[int] = None):\n         \"\"\"This function prepares a list of numpy videos.\"\"\"\n         video_input = [np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)] * 8\n+        video_input = np.array(video_input)\n         if batch_size is None:\n             return video_input\n         return [video_input] * batch_size\n@@ -264,6 +265,90 @@ def test_model_input_names(self):\n \n         self.assertSetEqual(set(inputs.keys()), set(processor.model_input_names))\n \n+    def test_processor_text_has_no_visual(self):\n+        \"\"\"\n+        Tests that multimodal models can process batch of inputs where samples can\n+        be with images/videos or without. See https://github.com/huggingface/transformers/issues/40263\n+        \"\"\"\n+        processor = self.get_processor()\n+        call_signature = inspect.signature(processor.__call__)\n+        input_args = [param.name for param in call_signature.parameters.values() if param.annotation != param.empty]\n+\n+        if not (\"text\" in input_args and (\"images\" in input_args and \"videos\" in input_args)):\n+            self.skipTest(f\"{self.processor_class} doesn't support several vision modalities with text.\")\n+\n+        # Prepare inputs and filter by input signature. Make sure to use a high batch size, we'll set some\n+        # samples to text-only later\n+        text = self.prepare_text_inputs(batch_size=3, modalities=[\"image\", \"video\"])\n+        image_inputs = self.prepare_image_inputs(batch_size=3)\n+        video_inputs = self.prepare_video_inputs(batch_size=3)\n+        inputs_dict = {\"text\": text, \"images\": image_inputs, \"videos\": video_inputs}\n+        inputs_dict = {k: v for k, v in inputs_dict.items() if k in input_args}\n+\n+        processing_kwargs = {\"return_tensors\": \"pt\", \"padding\": True}\n+        if \"videos\" in inputs_dict:\n+            processing_kwargs[\"do_sample_frames\"] = False\n+\n+        # Firts call processor with all inputs and use nested input type, which is the format supported by all multimodal processors\n+        image_inputs_nested = [[image] if not isinstance(image, list) else image for image in image_inputs]\n+        video_inputs_nested = [[video] for video in video_inputs]\n+        inputs_dict_nested = {\"text\": text, \"images\": image_inputs_nested, \"videos\": video_inputs_nested}\n+        inputs_dict_nested = {k: v for k, v in inputs_dict_nested.items() if k in input_args}\n+        inputs = processor(**inputs_dict_nested, **processing_kwargs)\n+        self.assertTrue(self.text_input_name in inputs)\n+\n+        # Now call with one of the samples with no associated vision input. Let's set the first input to be a plain text\n+        # with no placeholder tokens and no images/videos. The final format would be `images = [[], [image2], [image3]]`\n+        plain_text = \"lower newer\"\n+        image_inputs_nested[0] = []\n+        video_inputs_nested[0] = []\n+        text[0] = plain_text\n+        inputs_dict_no_vision = {\"text\": text, \"images\": image_inputs_nested, \"videos\": video_inputs_nested}\n+        inputs_dict_no_vision = {k: v for k, v in inputs_dict_no_vision.items() if k in input_args}\n+        inputs_nested = processor(**inputs_dict_no_vision, **processing_kwargs)\n+\n+        # Check that text samples are same and are expanded with placeholder tokens correctly. First sample\n+        # has no vision input associated, so we skip it and check it has no vision\n+        self.assertListEqual(\n+            inputs[self.text_input_name][1:].tolist(), inputs_nested[self.text_input_name][1:].tolist()\n+        )\n+\n+        # Now test if we can apply chat templates with no vision inputs in one of the samples\n+        # NOTE: we don't skip the test as we want the above to be checked even if process has to chat template\n+        if processor.chat_template is not None:\n+            messages = [\n+                [\n+                    {\n+                        \"role\": \"user\",\n+                        \"content\": [\n+                            {\"type\": \"text\", \"text\": \"What is the capital of France?\"},\n+                        ],\n+                    },\n+                ],\n+                [\n+                    {\n+                        \"role\": \"user\",\n+                        \"content\": [\n+                            {\"type\": \"text\", \"text\": \"What is the capital of France?\"},\n+                            {\n+                                \"type\": \"image\",\n+                                \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png\",\n+                            },\n+                        ],\n+                    },\n+                ],\n+            ]\n+\n+            inputs_chat_template = processor.apply_chat_template(\n+                messages,\n+                add_generation_prompt=False,\n+                tokenize=True,\n+                return_dict=True,\n+                return_tensors=\"pt\",\n+                padding=True,\n+            )\n+            self.assertTrue(self.text_input_name in inputs_chat_template)\n+\n     # These kwargs-related tests ensure that processors are correctly instantiated.\n     # they need to be applied only if an image_processor exists.\n "
        }
    ],
    "stats": {
        "total": 690,
        "additions": 444,
        "deletions": 246
    }
}