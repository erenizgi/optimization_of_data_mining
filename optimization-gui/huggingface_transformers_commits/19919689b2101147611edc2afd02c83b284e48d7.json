{
    "author": "gau-nernst",
    "message": "Fix Gemma3 embedding scaling (#37109)\n\nfix gemma3 embedding",
    "sha": "19919689b2101147611edc2afd02c83b284e48d7",
    "files": [
        {
            "sha": "7c5cd254ffd7da9af8e4153563744146c666977e",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/19919689b2101147611edc2afd02c83b284e48d7/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19919689b2101147611edc2afd02c83b284e48d7/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=19919689b2101147611edc2afd02c83b284e48d7",
            "patch": "@@ -97,12 +97,12 @@ class Gemma3TextScaledWordEmbedding(nn.Embedding):\n     This module overrides nn.Embeddings' forward by multiplying with embeddings scale.\n     \"\"\"\n \n-    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int, embed_scale: Optional[float] = 1.0):\n+    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int, embed_scale: float = 1.0):\n         super().__init__(num_embeddings, embedding_dim, padding_idx)\n-        self.embed_scale = embed_scale\n+        self.register_buffer(\"embed_scale\", torch.tensor(embed_scale), persistent=False)\n \n     def forward(self, input_ids: torch.Tensor):\n-        return super().forward(input_ids) * self.embed_scale\n+        return super().forward(input_ids) * self.embed_scale.to(self.weight.dtype)\n \n \n class Gemma3MLP(nn.Module):\n@@ -616,7 +616,7 @@ def __init__(self, config: Gemma3TextConfig):\n         self.padding_idx = config.pad_token_id\n         self.vocab_size = config.vocab_size\n \n-        # Gemma3 downcasts the below to float16, causing sqrt(3072)=55.4256 to become 55.5. See https://github.com/huggingface/transformers/pull/29402\n+        # Gemma3 downcasts the below to bfloat16, causing sqrt(3072)=55.4256 to become 55.5. See https://github.com/huggingface/transformers/pull/29402\n         self.embed_tokens = Gemma3TextScaledWordEmbedding(\n             config.vocab_size, config.hidden_size, self.padding_idx, embed_scale=self.config.hidden_size**0.5\n         )"
        },
        {
            "sha": "4a16c52d50d8df4be88d7ef66e2925bb092f67d6",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 12,
            "deletions": 4,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/19919689b2101147611edc2afd02c83b284e48d7/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19919689b2101147611edc2afd02c83b284e48d7/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=19919689b2101147611edc2afd02c83b284e48d7",
            "patch": "@@ -40,7 +40,6 @@\n     replace_return_docstrings,\n )\n from ...utils.deprecation import deprecate_kwarg\n-from ..bart.modeling_bart import BartScaledWordEmbedding\n from ..gemma2.configuration_gemma2 import Gemma2Config\n from ..gemma2.modeling_gemma2 import (\n     Gemma2Attention,\n@@ -339,8 +338,17 @@ class Gemma3CausalLMOutputWithPast(ModelOutput):\n     image_hidden_states: Optional[torch.FloatTensor] = None\n \n \n-class Gemma3TextScaledWordEmbedding(BartScaledWordEmbedding):\n-    pass\n+class Gemma3TextScaledWordEmbedding(nn.Embedding):\n+    \"\"\"\n+    This module overrides nn.Embeddings' forward by multiplying with embeddings scale.\n+    \"\"\"\n+\n+    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int, embed_scale: float = 1.0):\n+        super().__init__(num_embeddings, embedding_dim, padding_idx)\n+        self.register_buffer(\"embed_scale\", torch.tensor(embed_scale), persistent=False)\n+\n+    def forward(self, input_ids: torch.Tensor):\n+        return super().forward(input_ids) * self.embed_scale.to(self.weight.dtype)\n \n \n class Gemma3MLP(Gemma2MLP):\n@@ -562,7 +570,7 @@ class Gemma3TextModel(Gemma2Model):\n     def __init__(self, config: Gemma3TextConfig):\n         super().__init__(config)\n \n-        # Gemma3 downcasts the below to float16, causing sqrt(3072)=55.4256 to become 55.5. See https://github.com/huggingface/transformers/pull/29402\n+        # Gemma3 downcasts the below to bfloat16, causing sqrt(3072)=55.4256 to become 55.5. See https://github.com/huggingface/transformers/pull/29402\n         self.embed_tokens = Gemma3TextScaledWordEmbedding(\n             config.vocab_size, config.hidden_size, self.padding_idx, embed_scale=self.config.hidden_size**0.5\n         )"
        }
    ],
    "stats": {
        "total": 24,
        "additions": 16,
        "deletions": 8
    }
}