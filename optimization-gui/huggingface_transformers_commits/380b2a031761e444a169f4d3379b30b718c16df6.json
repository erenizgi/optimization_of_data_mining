{
    "author": "Cyrilvallez",
    "message": "Rework add-new-model-like with modular and make test filenames coherent (#39612)\n\n* remove tf/flax\n\n* fix\n\n* style\n\n* Update add_new_model_like.py\n\n* work in progress\n\n* continue\n\n* more cleanup\n\n* simplify and first final version\n\n* fixes -> it works\n\n* add linter checks\n\n* Update add_new_model_like.py\n\n* fix\n\n* add modular conversion at the end\n\n* Update add_new_model_like.py\n\n* add video processor\n\n* Update add_new_model_like.py\n\n* Update add_new_model_like.py\n\n* Update add_new_model_like.py\n\n* fix\n\n* Update image_processing_auto.py\n\n* Update image_processing_auto.py\n\n* fix post rebase\n\n* start test filenames replacement\n\n* rename all test_processor -> test_processing\n\n* fix copied from\n\n* add docstrings\n\n* Update add_new_model_like.py\n\n* fix regex\n\n* improve wording\n\n* Update add_new_model_like.py\n\n* Update add_new_model_like.py\n\n* Update add_new_model_like.py\n\n* start adding test\n\n* fix\n\n* fix\n\n* proper first test\n\n* tests\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* modular can be used from anywhere\n\n* protect import\n\n* fix\n\n* Update add_new_model_like.py\n\n* fix",
    "sha": "380b2a031761e444a169f4d3379b30b718c16df6",
    "files": [
        {
            "sha": "e07103e4c366029f0aeead9932467b4f7ed9e798",
            "filename": "conftest.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/conftest.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/conftest.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/conftest.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "patch": "@@ -29,7 +29,6 @@\n NOT_DEVICE_TESTS = {\n     \"test_tokenization\",\n     \"test_tokenization_mistral_common\",\n-    \"test_processor\",\n     \"test_processing\",\n     \"test_beam_constraints\",\n     \"test_configuration_utils\","
        },
        {
            "sha": "74330b8d3cea4061b402d1590cef18cf93593c63",
            "filename": "src/transformers/commands/add_new_model_like.py",
            "status": "modified",
            "additions": 550,
            "deletions": 1570,
            "changes": 2120,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/src%2Ftransformers%2Fcommands%2Fadd_new_model_like.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/src%2Ftransformers%2Fcommands%2Fadd_new_model_like.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fadd_new_model_like.py?ref=380b2a031761e444a169f4d3379b30b718c16df6"
        },
        {
            "sha": "d00bfad332a8e2b1b882c22f6f3b4ce41297b112",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 33,
            "deletions": 45,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "patch": "@@ -62,7 +62,7 @@\n             (\"aimv2\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"aimv2_vision_model\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"align\", (\"EfficientNetImageProcessor\", \"EfficientNetImageProcessorFast\")),\n-            (\"aria\", (\"AriaImageProcessor\")),\n+            (\"aria\", (\"AriaImageProcessor\", None)),\n             (\"beit\", (\"BeitImageProcessor\", \"BeitImageProcessorFast\")),\n             (\"bit\", (\"BitImageProcessor\", \"BitImageProcessorFast\")),\n             (\"blip\", (\"BlipImageProcessor\", \"BlipImageProcessorFast\")),\n@@ -72,7 +72,7 @@\n             (\"chinese_clip\", (\"ChineseCLIPImageProcessor\", \"ChineseCLIPImageProcessorFast\")),\n             (\"clip\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"clipseg\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n-            (\"cohere2_vision\", (\"Cohere2VisionImageProcessorFast\",)),\n+            (\"cohere2_vision\", (None, \"Cohere2VisionImageProcessorFast\")),\n             (\"conditional_detr\", (\"ConditionalDetrImageProcessor\", \"ConditionalDetrImageProcessorFast\")),\n             (\"convnext\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),\n             (\"convnextv2\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),\n@@ -84,52 +84,52 @@\n             (\"deit\", (\"DeiTImageProcessor\", \"DeiTImageProcessorFast\")),\n             (\"depth_anything\", (\"DPTImageProcessor\", \"DPTImageProcessorFast\")),\n             (\"depth_pro\", (\"DepthProImageProcessor\", \"DepthProImageProcessorFast\")),\n-            (\"deta\", (\"DetaImageProcessor\",)),\n+            (\"deta\", (\"DetaImageProcessor\", None)),\n             (\"detr\", (\"DetrImageProcessor\", \"DetrImageProcessorFast\")),\n             (\"dinat\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"dinov2\", (\"BitImageProcessor\", \"BitImageProcessorFast\")),\n             (\"donut-swin\", (\"DonutImageProcessor\", \"DonutImageProcessorFast\")),\n             (\"dpt\", (\"DPTImageProcessor\", \"DPTImageProcessorFast\")),\n-            (\"efficientformer\", (\"EfficientFormerImageProcessor\",)),\n-            (\"efficientloftr\", (\"EfficientLoFTRImageProcessor\",)),\n+            (\"efficientformer\", (\"EfficientFormerImageProcessor\", None)),\n+            (\"efficientloftr\", (\"EfficientLoFTRImageProcessor\", None)),\n             (\"efficientnet\", (\"EfficientNetImageProcessor\", \"EfficientNetImageProcessorFast\")),\n             (\"eomt\", (\"EomtImageProcessor\", \"EomtImageProcessorFast\")),\n             (\"flava\", (\"FlavaImageProcessor\", \"FlavaImageProcessorFast\")),\n             (\"focalnet\", (\"BitImageProcessor\", \"BitImageProcessorFast\")),\n-            (\"fuyu\", (\"FuyuImageProcessor\",)),\n+            (\"fuyu\", (\"FuyuImageProcessor\", None)),\n             (\"gemma3\", (\"Gemma3ImageProcessor\", \"Gemma3ImageProcessorFast\")),\n             (\"gemma3n\", (\"SiglipImageProcessor\", \"SiglipImageProcessorFast\")),\n             (\"git\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"glm4v\", (\"Glm4vImageProcessor\", \"Glm4vImageProcessorFast\")),\n-            (\"glpn\", (\"GLPNImageProcessor\",)),\n+            (\"glpn\", (\"GLPNImageProcessor\", None)),\n             (\"got_ocr2\", (\"GotOcr2ImageProcessor\", \"GotOcr2ImageProcessorFast\")),\n             (\"grounding-dino\", (\"GroundingDinoImageProcessor\", \"GroundingDinoImageProcessorFast\")),\n             (\"groupvit\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"hiera\", (\"BitImageProcessor\", \"BitImageProcessorFast\")),\n-            (\"idefics\", (\"IdeficsImageProcessor\",)),\n+            (\"idefics\", (\"IdeficsImageProcessor\", None)),\n             (\"idefics2\", (\"Idefics2ImageProcessor\", \"Idefics2ImageProcessorFast\")),\n             (\"idefics3\", (\"Idefics3ImageProcessor\", \"Idefics3ImageProcessorFast\")),\n             (\"ijepa\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n-            (\"imagegpt\", (\"ImageGPTImageProcessor\",)),\n+            (\"imagegpt\", (\"ImageGPTImageProcessor\", None)),\n             (\"instructblip\", (\"BlipImageProcessor\", \"BlipImageProcessorFast\")),\n-            (\"instructblipvideo\", (\"InstructBlipVideoImageProcessor\",)),\n+            (\"instructblipvideo\", (\"InstructBlipVideoImageProcessor\", None)),\n             (\"janus\", (\"JanusImageProcessor\", \"JanusImageProcessorFast\")),\n             (\"kosmos-2\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"layoutlmv2\", (\"LayoutLMv2ImageProcessor\", \"LayoutLMv2ImageProcessorFast\")),\n             (\"layoutlmv3\", (\"LayoutLMv3ImageProcessor\", \"LayoutLMv3ImageProcessorFast\")),\n             (\"levit\", (\"LevitImageProcessor\", \"LevitImageProcessorFast\")),\n-            (\"lightglue\", (\"LightGlueImageProcessor\",)),\n+            (\"lightglue\", (\"LightGlueImageProcessor\", None)),\n             (\"llama4\", (\"Llama4ImageProcessor\", \"Llama4ImageProcessorFast\")),\n             (\"llava\", (\"LlavaImageProcessor\", \"LlavaImageProcessorFast\")),\n             (\"llava_next\", (\"LlavaNextImageProcessor\", \"LlavaNextImageProcessorFast\")),\n-            (\"llava_next_video\", (\"LlavaNextVideoImageProcessor\",)),\n+            (\"llava_next_video\", (\"LlavaNextVideoImageProcessor\", None)),\n             (\"llava_onevision\", (\"LlavaOnevisionImageProcessor\", \"LlavaOnevisionImageProcessorFast\")),\n             (\"mask2former\", (\"Mask2FormerImageProcessor\", \"Mask2FormerImageProcessorFast\")),\n             (\"maskformer\", (\"MaskFormerImageProcessor\", \"MaskFormerImageProcessorFast\")),\n             (\"mgp-str\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"mistral3\", (\"PixtralImageProcessor\", \"PixtralImageProcessorFast\")),\n             (\"mlcd\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n-            (\"mllama\", (\"MllamaImageProcessor\",)),\n+            (\"mllama\", (\"MllamaImageProcessor\", None)),\n             (\"mm-grounding-dino\", (\"GroundingDinoImageProcessor\", \"GroundingDinoImageProcessorFast\")),\n             (\"mobilenet_v1\", (\"MobileNetV1ImageProcessor\", \"MobileNetV1ImageProcessorFast\")),\n             (\"mobilenet_v2\", (\"MobileNetV2ImageProcessor\", \"MobileNetV2ImageProcessorFast\")),\n@@ -142,12 +142,12 @@\n             (\"owlvit\", (\"OwlViTImageProcessor\", \"OwlViTImageProcessorFast\")),\n             (\"paligemma\", (\"SiglipImageProcessor\", \"SiglipImageProcessorFast\")),\n             (\"perceiver\", (\"PerceiverImageProcessor\", \"PerceiverImageProcessorFast\")),\n-            (\"perception_lm\", (\"PerceptionLMImageProcessorFast\",)),\n-            (\"phi4_multimodal\", (\"Phi4MultimodalImageProcessorFast\",)),\n-            (\"pix2struct\", (\"Pix2StructImageProcessor\",)),\n+            (\"perception_lm\", (None, \"PerceptionLMImageProcessorFast\")),\n+            (\"phi4_multimodal\", (None, \"Phi4MultimodalImageProcessorFast\")),\n+            (\"pix2struct\", (\"Pix2StructImageProcessor\", None)),\n             (\"pixtral\", (\"PixtralImageProcessor\", \"PixtralImageProcessorFast\")),\n             (\"poolformer\", (\"PoolFormerImageProcessor\", \"PoolFormerImageProcessorFast\")),\n-            (\"prompt_depth_anything\", (\"PromptDepthAnythingImageProcessor\",)),\n+            (\"prompt_depth_anything\", (\"PromptDepthAnythingImageProcessor\", None)),\n             (\"pvt\", (\"PvtImageProcessor\", \"PvtImageProcessorFast\")),\n             (\"pvt_v2\", (\"PvtImageProcessor\", \"PvtImageProcessorFast\")),\n             (\"qwen2_5_vl\", (\"Qwen2VLImageProcessor\", \"Qwen2VLImageProcessorFast\")),\n@@ -157,39 +157,31 @@\n             (\"rt_detr\", (\"RTDetrImageProcessor\", \"RTDetrImageProcessorFast\")),\n             (\"sam\", (\"SamImageProcessor\", \"SamImageProcessorFast\")),\n             (\"sam_hq\", (\"SamImageProcessor\", \"SamImageProcessorFast\")),\n-            (\"segformer\", (\"SegformerImageProcessor\",)),\n             (\"segformer\", (\"SegformerImageProcessor\", \"SegformerImageProcessorFast\")),\n-            (\"seggpt\", (\"SegGptImageProcessor\",)),\n+            (\"seggpt\", (\"SegGptImageProcessor\", None)),\n             (\"shieldgemma2\", (\"Gemma3ImageProcessor\", \"Gemma3ImageProcessorFast\")),\n             (\"siglip\", (\"SiglipImageProcessor\", \"SiglipImageProcessorFast\")),\n             (\"siglip2\", (\"Siglip2ImageProcessor\", \"Siglip2ImageProcessorFast\")),\n             (\"smolvlm\", (\"SmolVLMImageProcessor\", \"SmolVLMImageProcessorFast\")),\n-            (\"superglue\", (\"SuperGlueImageProcessor\",)),\n-            (\n-                \"superpoint\",\n-                (\n-                    \"SuperPointImageProcessor\",\n-                    \"SuperPointImageProcessorFast\",\n-                ),\n-            ),\n+            (\"superglue\", (\"SuperGlueImageProcessor\", None)),\n+            (\"superpoint\", (\"SuperPointImageProcessor\", \"SuperPointImageProcessorFast\")),\n             (\"swiftformer\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"swin\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"swin2sr\", (\"Swin2SRImageProcessor\", \"Swin2SRImageProcessorFast\")),\n             (\"swinv2\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n-            (\"table-transformer\", (\"DetrImageProcessor\",)),\n-            (\"timesformer\", (\"VideoMAEImageProcessor\",)),\n-            (\"timm_wrapper\", (\"TimmWrapperImageProcessor\",)),\n-            (\"tvlt\", (\"TvltImageProcessor\",)),\n-            (\"tvp\", (\"TvpImageProcessor\",)),\n+            (\"table-transformer\", (\"DetrImageProcessor\", None)),\n+            (\"timesformer\", (\"VideoMAEImageProcessor\", None)),\n+            (\"timm_wrapper\", (\"TimmWrapperImageProcessor\", None)),\n+            (\"tvlt\", (\"TvltImageProcessor\", None)),\n+            (\"tvp\", (\"TvpImageProcessor\", None)),\n             (\"udop\", (\"LayoutLMv3ImageProcessor\", \"LayoutLMv3ImageProcessorFast\")),\n-            (\"udop\", (\"LayoutLMv3ImageProcessor\",)),\n             (\"upernet\", (\"SegformerImageProcessor\", \"SegformerImageProcessorFast\")),\n             (\"van\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),\n-            (\"videomae\", (\"VideoMAEImageProcessor\",)),\n+            (\"videomae\", (\"VideoMAEImageProcessor\", None)),\n             (\"vilt\", (\"ViltImageProcessor\", \"ViltImageProcessorFast\")),\n             (\"vipllava\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"vit\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n-            (\"vit_hybrid\", (\"ViTHybridImageProcessor\",)),\n+            (\"vit_hybrid\", (\"ViTHybridImageProcessor\", None)),\n             (\"vit_mae\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"vit_msn\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"vitmatte\", (\"VitMatteImageProcessor\", \"VitMatteImageProcessorFast\")),\n@@ -199,18 +191,14 @@\n         ]\n     )\n \n-for model_type, image_processors in IMAGE_PROCESSOR_MAPPING_NAMES.items():\n-    slow_image_processor_class, *fast_image_processor_class = image_processors\n+# Override to None if the packages are not available\n+for model_type, (slow_class, fast_class) in IMAGE_PROCESSOR_MAPPING_NAMES.items():\n     if not is_vision_available():\n-        slow_image_processor_class = None\n+        slow_class = None\n+    if not is_torchvision_available():\n+        fast_class = None\n \n-    # If the fast image processor is not defined, or torchvision is not available, we set it to None\n-    if not fast_image_processor_class or fast_image_processor_class[0] is None or not is_torchvision_available():\n-        fast_image_processor_class = None\n-    else:\n-        fast_image_processor_class = fast_image_processor_class[0]\n-\n-    IMAGE_PROCESSOR_MAPPING_NAMES[model_type] = (slow_image_processor_class, fast_image_processor_class)\n+    IMAGE_PROCESSOR_MAPPING_NAMES[model_type] = (slow_class, fast_class)\n \n IMAGE_PROCESSOR_MAPPING = _LazyAutoMapping(CONFIG_MAPPING_NAMES, IMAGE_PROCESSOR_MAPPING_NAMES)\n "
        },
        {
            "sha": "bb2920b40335a38c4900d76d91925499fc538d49",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "patch": "@@ -179,6 +179,7 @@\n     is_keras_nlp_available,\n     is_kernels_available,\n     is_levenshtein_available,\n+    is_libcst_available,\n     is_librosa_available,\n     is_liger_kernel_available,\n     is_lomo_available,"
        },
        {
            "sha": "9fa7c5253d9c1446bb5c92595179261e568a96f4",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "patch": "@@ -120,6 +120,7 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n _av_available = importlib.util.find_spec(\"av\") is not None\n _decord_available = importlib.util.find_spec(\"decord\") is not None\n _torchcodec_available = importlib.util.find_spec(\"torchcodec\") is not None\n+_libcst_available = _is_package_available(\"libcst\")\n _bitsandbytes_available = _is_package_available(\"bitsandbytes\")\n _eetq_available = _is_package_available(\"eetq\")\n _fbgemm_gpu_available = _is_package_available(\"fbgemm_gpu\")\n@@ -379,6 +380,10 @@ def is_torch_available():\n     return _torch_available\n \n \n+def is_libcst_available():\n+    return _libcst_available\n+\n+\n def is_accelerate_available(min_version: str = ACCELERATE_MIN_VERSION):\n     return _accelerate_available and version.parse(_accelerate_version) >= version.parse(min_version)\n "
        },
        {
            "sha": "9205e5f4f21c7f386c18a5c6cb8ceaedd39481d3",
            "filename": "tests/models/align/test_processing_align.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Falign%2Ftest_processing_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Falign%2Ftest_processing_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Falign%2Ftest_processing_align.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/align/test_processor_align.py"
        },
        {
            "sha": "f498ce4aa87c23e950c46ade5ddd5c56717f67cb",
            "filename": "tests/models/altclip/test_processing_altclip.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Faltclip%2Ftest_processing_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Faltclip%2Ftest_processing_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faltclip%2Ftest_processing_altclip.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/altclip/test_processor_altclip.py"
        },
        {
            "sha": "a78c372f0edcddb19b11eac789d5e4444bb6c00f",
            "filename": "tests/models/aria/test_processing_aria.py",
            "status": "renamed",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Faria%2Ftest_processing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Faria%2Ftest_processing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_processing_aria.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "patch": "@@ -95,7 +95,7 @@ def get_processor(self, **kwargs):\n     def tearDownClass(cls):\n         shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n \n-    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_get_num_vision_tokens\n+    # Copied from tests.models.llava.test_processing_llava.LlavaProcessorTest.test_get_num_vision_tokens\n     def test_get_num_vision_tokens(self):\n         \"Tests general functionality of the helper used internally in vLLM\"\n ",
            "previous_filename": "tests/models/aria/test_processor_aria.py"
        },
        {
            "sha": "33b7da48f2f896204c23eb5ad4471d69eea729b5",
            "filename": "tests/models/aya_vision/test_processing_aya_vision.py",
            "status": "renamed",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Faya_vision%2Ftest_processing_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Faya_vision%2Ftest_processing_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faya_vision%2Ftest_processing_aya_vision.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "patch": "@@ -80,7 +80,7 @@ def get_processor(self, **kwargs):\n     def tearDownClass(cls):\n         shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n \n-    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_get_num_vision_tokens\n+    # Copied from tests.models.llava.test_processing_llava.LlavaProcessorTest.test_get_num_vision_tokens\n     def test_get_num_vision_tokens(self):\n         \"Tests general functionality of the helper used internally in vLLM\"\n ",
            "previous_filename": "tests/models/aya_vision/test_processor_aya_vision.py"
        },
        {
            "sha": "447d38b956545904ea6e1eb7f090fa0c23dfd227",
            "filename": "tests/models/bark/test_processing_bark.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fbark%2Ftest_processing_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fbark%2Ftest_processing_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbark%2Ftest_processing_bark.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/bark/test_processor_bark.py"
        },
        {
            "sha": "254eede275ba3e12ae09584cf088553089798879",
            "filename": "tests/models/blip/test_processing_blip.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fblip%2Ftest_processing_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fblip%2Ftest_processing_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip%2Ftest_processing_blip.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/blip/test_processor_blip.py"
        },
        {
            "sha": "a24ccc4aeb50ed48d52376e4e5672151ab592132",
            "filename": "tests/models/blip_2/test_processing_blip_2.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fblip_2%2Ftest_processing_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fblip_2%2Ftest_processing_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_processing_blip_2.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/blip_2/test_processor_blip_2.py"
        },
        {
            "sha": "60989eb91d9a826753eeef9f495d758c0a18b135",
            "filename": "tests/models/bridgetower/test_processing_bridgetower.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fbridgetower%2Ftest_processing_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fbridgetower%2Ftest_processing_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbridgetower%2Ftest_processing_bridgetower.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/bridgetower/test_processor_bridgetower.py"
        },
        {
            "sha": "71da4431e74469c93535a075283bd200547cec2d",
            "filename": "tests/models/chameleon/test_processing_chameleon.py",
            "status": "renamed",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fchameleon%2Ftest_processing_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fchameleon%2Ftest_processing_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchameleon%2Ftest_processing_chameleon.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "patch": "@@ -75,7 +75,7 @@ def test_special_mm_token_truncation(self):\n     def prepare_processor_dict():\n         return {\"image_seq_length\": 2}  # fmt: skip\n \n-    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_get_num_vision_tokens\n+    # Copied from tests.models.llava.test_processing_llava.LlavaProcessorTest.test_get_num_vision_tokens\n     def test_get_num_vision_tokens(self):\n         \"Tests general functionality of the helper used internally in vLLM\"\n ",
            "previous_filename": "tests/models/chameleon/test_processor_chameleon.py"
        },
        {
            "sha": "83f68c0361bedf946c3ca8e275de6548d352b05e",
            "filename": "tests/models/chinese_clip/test_processing_chinese_clip.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fchinese_clip%2Ftest_processing_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fchinese_clip%2Ftest_processing_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchinese_clip%2Ftest_processing_chinese_clip.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/chinese_clip/test_processor_chinese_clip.py"
        },
        {
            "sha": "43192cee2efd3ae8bf9311165f75a7f452cbbf41",
            "filename": "tests/models/clap/test_processing_clap.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fclap%2Ftest_processing_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fclap%2Ftest_processing_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclap%2Ftest_processing_clap.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/clap/test_processor_clap.py"
        },
        {
            "sha": "bb7fae4a861def9d59ba310381db0523d9118e67",
            "filename": "tests/models/clip/test_processing_clip.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fclip%2Ftest_processing_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fclip%2Ftest_processing_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclip%2Ftest_processing_clip.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/clip/test_processor_clip.py"
        },
        {
            "sha": "f7255838caa8504806dac45d8a70503d3e787cfe",
            "filename": "tests/models/clipseg/test_processing_clipseg.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fclipseg%2Ftest_processing_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fclipseg%2Ftest_processing_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclipseg%2Ftest_processing_clipseg.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/clipseg/test_processor_clipseg.py"
        },
        {
            "sha": "d03f7f75ed370a7ad44221b4a5a058e70b3551e5",
            "filename": "tests/models/clvp/test_processing_clvp.py",
            "status": "renamed",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fclvp%2Ftest_processing_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fclvp%2Ftest_processing_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclvp%2Ftest_processing_clvp.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "patch": "@@ -35,15 +35,15 @@ def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n         gc.collect()\n \n-    # Copied from transformers.tests.models.whisper.test_processor_whisper.WhisperProcessorTest.get_tokenizer with Whisper->Clvp\n+    # Copied from transformers.tests.models.whisper.test_processing_whisper.WhisperProcessorTest.get_tokenizer with Whisper->Clvp\n     def get_tokenizer(self, **kwargs):\n         return ClvpTokenizer.from_pretrained(self.checkpoint, **kwargs)\n \n-    # Copied from transformers.tests.models.whisper.test_processor_whisper.WhisperProcessorTest.get_feature_extractor with Whisper->Clvp\n+    # Copied from transformers.tests.models.whisper.test_processing_whisper.WhisperProcessorTest.get_feature_extractor with Whisper->Clvp\n     def get_feature_extractor(self, **kwargs):\n         return ClvpFeatureExtractor.from_pretrained(self.checkpoint, **kwargs)\n \n-    # Copied from transformers.tests.models.whisper.test_processor_whisper.WhisperProcessorTest.test_save_load_pretrained_default with Whisper->Clvp\n+    # Copied from transformers.tests.models.whisper.test_processing_whisper.WhisperProcessorTest.test_save_load_pretrained_default with Whisper->Clvp\n     def test_save_load_pretrained_default(self):\n         tokenizer = self.get_tokenizer()\n         feature_extractor = self.get_feature_extractor()\n@@ -59,7 +59,7 @@ def test_save_load_pretrained_default(self):\n         self.assertEqual(processor.feature_extractor.to_json_string(), feature_extractor.to_json_string())\n         self.assertIsInstance(processor.feature_extractor, ClvpFeatureExtractor)\n \n-    # Copied from transformers.tests.models.whisper.test_processor_whisper.WhisperProcessorTest.test_feature_extractor with Whisper->Clvp,processor(raw_speech->processor(raw_speech=raw_speech\n+    # Copied from transformers.tests.models.whisper.test_processing_whisper.WhisperProcessorTest.test_feature_extractor with Whisper->Clvp,processor(raw_speech->processor(raw_speech=raw_speech\n     def test_feature_extractor(self):\n         feature_extractor = self.get_feature_extractor()\n         tokenizer = self.get_tokenizer()\n@@ -74,7 +74,7 @@ def test_feature_extractor(self):\n         for key in input_feat_extract:\n             self.assertAlmostEqual(input_feat_extract[key].sum(), input_processor[key].sum(), delta=1e-2)\n \n-    # Copied from transformers.tests.models.whisper.test_processor_whisper.WhisperProcessorTest.test_tokenizer with Whisper->Clvp\n+    # Copied from transformers.tests.models.whisper.test_processing_whisper.WhisperProcessorTest.test_tokenizer with Whisper->Clvp\n     def test_tokenizer(self):\n         feature_extractor = self.get_feature_extractor()\n         tokenizer = self.get_tokenizer()\n@@ -90,7 +90,7 @@ def test_tokenizer(self):\n         for key in encoded_tok:\n             self.assertListEqual(encoded_tok[key], encoded_processor[key])\n \n-    # Copied from transformers.tests.models.whisper.test_processor_whisper.WhisperProcessorTest.test_tokenizer_decode with Whisper->Clvp\n+    # Copied from transformers.tests.models.whisper.test_processing_whisper.WhisperProcessorTest.test_tokenizer_decode with Whisper->Clvp\n     def test_tokenizer_decode(self):\n         feature_extractor = self.get_feature_extractor()\n         tokenizer = self.get_tokenizer()",
            "previous_filename": "tests/models/clvp/test_processor_clvp.py"
        },
        {
            "sha": "aa4fad517bbad91c6428eeefdfab70c00d26c98a",
            "filename": "tests/models/colpali/test_processing_colpali.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fcolpali%2Ftest_processing_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fcolpali%2Ftest_processing_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolpali%2Ftest_processing_colpali.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "patch": "@@ -54,7 +54,7 @@ def setUpClass(cls):\n     def tearDownClass(cls):\n         shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n \n-    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_get_num_vision_tokens\n+    # Copied from tests.models.llava.test_processing_llava.LlavaProcessorTest.test_get_num_vision_tokens\n     def test_get_num_vision_tokens(self):\n         \"Tests general functionality of the helper used internally in vLLM\"\n "
        },
        {
            "sha": "300cc680fe4fd63c4e871b0edefda64ebebbd1e2",
            "filename": "tests/models/colqwen2/test_processing_colqwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fcolqwen2%2Ftest_processing_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fcolqwen2%2Ftest_processing_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolqwen2%2Ftest_processing_colqwen2.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "patch": "@@ -57,7 +57,7 @@ def get_image_processor(self, **kwargs):\n     def tearDownClass(cls):\n         shutil.rmtree(cls.tmpdirname)\n \n-    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_get_num_vision_tokens\n+    # Copied from tests.models.llava.test_processing_llava.LlavaProcessorTest.test_get_num_vision_tokens\n     def test_get_num_vision_tokens(self):\n         \"Tests general functionality of the helper used internally in vLLM\"\n "
        },
        {
            "sha": "910a0e7868fc20b91d7b80a29e8f1461743e5ced",
            "filename": "tests/models/csm/test_processing_csm.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fcsm%2Ftest_processing_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fcsm%2Ftest_processing_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcsm%2Ftest_processing_csm.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/csm/test_processor_csm.py"
        },
        {
            "sha": "3c61f377e26424c763ca064fbb6394ce49a1f779",
            "filename": "tests/models/deepseek_vl/test_processing_deepseek_vl.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fdeepseek_vl%2Ftest_processing_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fdeepseek_vl%2Ftest_processing_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_vl%2Ftest_processing_deepseek_vl.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/deepseek_vl/test_processor_deepseek_vl.py"
        },
        {
            "sha": "10608d8bdba764d509194f1f71d47b8dc976b895",
            "filename": "tests/models/deepseek_vl_hybrid/test_processing_deepseek_vl_hybrid.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_processing_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_processing_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_processing_deepseek_vl_hybrid.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/deepseek_vl_hybrid/test_processor_deepseek_vl_hybrid.py"
        },
        {
            "sha": "b015e3c197bf5e1beaab235f893a1e803afd7e50",
            "filename": "tests/models/dia/test_processing_dia.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fdia%2Ftest_processing_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fdia%2Ftest_processing_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdia%2Ftest_processing_dia.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/dia/test_processor_dia.py"
        },
        {
            "sha": "272f1fd823414885ecf1e3fdfb7a8f63c476a08b",
            "filename": "tests/models/donut/test_processing_donut.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fdonut%2Ftest_processing_donut.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fdonut%2Ftest_processing_donut.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdonut%2Ftest_processing_donut.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/donut/test_processor_donut.py"
        },
        {
            "sha": "a62107892ae29563491b83f733da5130bce4a2f7",
            "filename": "tests/models/emu3/test_processing_emu3.py",
            "status": "renamed",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Femu3%2Ftest_processing_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Femu3%2Ftest_processing_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Femu3%2Ftest_processing_emu3.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "patch": "@@ -91,7 +91,7 @@ def test_processor_postprocess(self):\n         # For an image where pixels go from 0 to 255 the diff can be 1 due to some numerical precision errors when scaling and unscaling\n         self.assertTrue(np.abs(orig_image - unnormalized_images).max() >= 1)\n \n-    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_get_num_vision_tokens\n+    # Copied from tests.models.llava.test_processing_llava.LlavaProcessorTest.test_get_num_vision_tokens\n     def test_get_num_vision_tokens(self):\n         \"Tests general functionality of the helper used internally in vLLM\"\n ",
            "previous_filename": "tests/models/emu3/test_processor_emu3.py"
        },
        {
            "sha": "0a1f1f3cd2d507043f36f05822a598cb0693b79e",
            "filename": "tests/models/evolla/test_processing_evolla.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fevolla%2Ftest_processing_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fevolla%2Ftest_processing_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fevolla%2Ftest_processing_evolla.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/evolla/test_processor_evolla.py"
        },
        {
            "sha": "afba82b3833a69d4c15470ecf638e311a709c1f6",
            "filename": "tests/models/flava/test_processing_flava.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fflava%2Ftest_processing_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fflava%2Ftest_processing_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fflava%2Ftest_processing_flava.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/flava/test_processor_flava.py"
        },
        {
            "sha": "6f38e9a0e81b937dfefbaaff177cd934415f05c3",
            "filename": "tests/models/fuyu/test_processing_fuyu.py",
            "status": "renamed",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Ffuyu%2Ftest_processing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Ffuyu%2Ftest_processing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffuyu%2Ftest_processing_fuyu.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "patch": "@@ -64,7 +64,7 @@ def get_tokenizer(self, **kwargs):\n     def get_image_processor(self, **kwargs):\n         return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n \n-    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_get_num_vision_tokens\n+    # Copied from tests.models.llava.test_processing_llava.LlavaProcessorTest.test_get_num_vision_tokens\n     def test_get_num_vision_tokens(self):\n         \"Tests general functionality of the helper used internally in vLLM\"\n ",
            "previous_filename": "tests/models/fuyu/test_processor_fuyu.py"
        },
        {
            "sha": "16789e8096842ad80e7a0ad8cc4b346fe26a9844",
            "filename": "tests/models/gemma3/test_processing_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fgemma3%2Ftest_processing_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fgemma3%2Ftest_processing_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_processing_gemma3.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "patch": "@@ -58,7 +58,7 @@ def setUpClass(cls):\n         processor.save_pretrained(cls.tmpdirname)\n         cls.image_token = processor.boi_token\n \n-    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_get_num_vision_tokens\n+    # Copied from tests.models.llava.test_processing_llava.LlavaProcessorTest.test_get_num_vision_tokens\n     def test_get_num_vision_tokens(self):\n         \"Tests general functionality of the helper used internally in vLLM\"\n "
        },
        {
            "sha": "65b7819319a3d7f498c3b51fcdcf21ebf02a88d6",
            "filename": "tests/models/git/test_processing_git.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fgit%2Ftest_processing_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fgit%2Ftest_processing_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgit%2Ftest_processing_git.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/git/test_processor_git.py"
        },
        {
            "sha": "0719d211ddad3e33b6304b75bbfa244794bd9e6f",
            "filename": "tests/models/got_ocr2/test_processing_got_ocr2.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fgot_ocr2%2Ftest_processing_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fgot_ocr2%2Ftest_processing_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgot_ocr2%2Ftest_processing_got_ocr2.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/got_ocr2/test_processor_got_ocr2.py"
        },
        {
            "sha": "569ac9cfbc191c7afa36b073bf1b6d6df69d0e59",
            "filename": "tests/models/granite_speech/test_processing_granite_speech.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fgranite_speech%2Ftest_processing_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fgranite_speech%2Ftest_processing_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranite_speech%2Ftest_processing_granite_speech.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/granite_speech/test_processor_granite_speech.py"
        },
        {
            "sha": "5aaa4f8b31695284adcc23f031f82c6adde6f997",
            "filename": "tests/models/grounding_dino/test_processing_grounding_dino.py",
            "status": "renamed",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fgrounding_dino%2Ftest_processing_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fgrounding_dino%2Ftest_processing_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgrounding_dino%2Ftest_processing_grounding_dino.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "patch": "@@ -94,17 +94,17 @@ def prepare_text_inputs(self, batch_size: Optional[int] = None, modality: Option\n         return [labels, labels_longer] + [labels] * (batch_size - 2)\n \n     @classmethod\n-    # Copied from tests.models.clip.test_processor_clip.CLIPProcessorTest.get_tokenizer with CLIP->Bert\n+    # Copied from tests.models.clip.test_processing_clip.CLIPProcessorTest.get_tokenizer with CLIP->Bert\n     def get_tokenizer(cls, **kwargs):\n         return BertTokenizer.from_pretrained(cls.tmpdirname, **kwargs)\n \n     @classmethod\n-    # Copied from tests.models.clip.test_processor_clip.CLIPProcessorTest.get_rust_tokenizer with CLIP->Bert\n+    # Copied from tests.models.clip.test_processing_clip.CLIPProcessorTest.get_rust_tokenizer with CLIP->Bert\n     def get_rust_tokenizer(cls, **kwargs):\n         return BertTokenizerFast.from_pretrained(cls.tmpdirname, **kwargs)\n \n     @classmethod\n-    # Copied from tests.models.clip.test_processor_clip.CLIPProcessorTest.get_image_processor with CLIP->GroundingDino\n+    # Copied from tests.models.clip.test_processing_clip.CLIPProcessorTest.get_image_processor with CLIP->GroundingDino\n     def get_image_processor(cls, **kwargs):\n         return GroundingDinoImageProcessor.from_pretrained(cls.tmpdirname, **kwargs)\n \n@@ -145,7 +145,7 @@ def test_post_process_grounded_object_detection(self):\n         expected_box_slice = torch.tensor([0.6908, 0.4354, 1.0737, 1.3947])\n         torch.testing.assert_close(post_processed[0][\"boxes\"][0], expected_box_slice, rtol=1e-4, atol=1e-4)\n \n-    # Copied from tests.models.clip.test_processor_clip.CLIPProcessorTest.test_save_load_pretrained_default with CLIP->GroundingDino,GroundingDinoTokenizer->BertTokenizer\n+    # Copied from tests.models.clip.test_processing_clip.CLIPProcessorTest.test_save_load_pretrained_default with CLIP->GroundingDino,GroundingDinoTokenizer->BertTokenizer\n     def test_save_load_pretrained_default(self):\n         tokenizer_slow = self.get_tokenizer()\n         tokenizer_fast = self.get_rust_tokenizer()\n@@ -171,7 +171,7 @@ def test_save_load_pretrained_default(self):\n         self.assertIsInstance(processor_slow.image_processor, GroundingDinoImageProcessor)\n         self.assertIsInstance(processor_fast.image_processor, GroundingDinoImageProcessor)\n \n-    # Copied from tests.models.clip.test_processor_clip.CLIPProcessorTest.test_save_load_pretrained_additional_features with CLIP->GroundingDino,GroundingDinoTokenizer->BertTokenizer\n+    # Copied from tests.models.clip.test_processing_clip.CLIPProcessorTest.test_save_load_pretrained_additional_features with CLIP->GroundingDino,GroundingDinoTokenizer->BertTokenizer\n     def test_save_load_pretrained_additional_features(self):\n         with tempfile.TemporaryDirectory() as tmpdir:\n             processor = GroundingDinoProcessor(\n@@ -194,7 +194,7 @@ def test_save_load_pretrained_additional_features(self):\n         self.assertEqual(processor.image_processor.to_json_string(), image_processor_add_kwargs.to_json_string())\n         self.assertIsInstance(processor.image_processor, GroundingDinoImageProcessor)\n \n-    # Copied from tests.models.clip.test_processor_clip.CLIPProcessorTest.test_image_processor with CLIP->GroundingDino\n+    # Copied from tests.models.clip.test_processing_clip.CLIPProcessorTest.test_image_processor with CLIP->GroundingDino\n     def test_image_processor(self):\n         image_processor = self.get_image_processor()\n         tokenizer = self.get_tokenizer()\n@@ -209,7 +209,7 @@ def test_image_processor(self):\n         for key in input_image_proc:\n             self.assertAlmostEqual(input_image_proc[key].sum(), input_processor[key].sum(), delta=1e-2)\n \n-    # Copied from tests.models.clip.test_processor_clip.CLIPProcessorTest.test_tokenizer with CLIP->GroundingDino\n+    # Copied from tests.models.clip.test_processing_clip.CLIPProcessorTest.test_tokenizer with CLIP->GroundingDino\n     def test_tokenizer(self):\n         image_processor = self.get_image_processor()\n         tokenizer = self.get_tokenizer()\n@@ -244,7 +244,7 @@ def test_processor(self):\n         with pytest.raises(ValueError):\n             processor()\n \n-    # Copied from tests.models.clip.test_processor_clip.CLIPProcessorTest.test_tokenizer_decode with CLIP->GroundingDino\n+    # Copied from tests.models.clip.test_processing_clip.CLIPProcessorTest.test_tokenizer_decode with CLIP->GroundingDino\n     def test_tokenizer_decode(self):\n         image_processor = self.get_image_processor()\n         tokenizer = self.get_tokenizer()\n@@ -258,7 +258,7 @@ def test_tokenizer_decode(self):\n \n         self.assertListEqual(decoded_tok, decoded_processor)\n \n-    # Copied from tests.models.clip.test_processor_clip.CLIPProcessorTest.test_model_input_names with CLIP->GroundingDino\n+    # Copied from tests.models.clip.test_processing_clip.CLIPProcessorTest.test_model_input_names with CLIP->GroundingDino\n     def test_model_input_names(self):\n         image_processor = self.get_image_processor()\n         tokenizer = self.get_tokenizer()",
            "previous_filename": "tests/models/grounding_dino/test_processor_grounding_dino.py"
        },
        {
            "sha": "483d1ad1e90c607ebe20fd790542f6f4e435cc6c",
            "filename": "tests/models/idefics/test_processing_idefics.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fidefics%2Ftest_processing_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fidefics%2Ftest_processing_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_processing_idefics.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/idefics/test_processor_idefics.py"
        },
        {
            "sha": "a39d14d4f17ee6d9613662873c220060e7787685",
            "filename": "tests/models/idefics2/test_processing_idefics2.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fidefics2%2Ftest_processing_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fidefics2%2Ftest_processing_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics2%2Ftest_processing_idefics2.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/idefics2/test_processor_idefics2.py"
        },
        {
            "sha": "780a6899d540186d03c768a04be75ce28caba889",
            "filename": "tests/models/idefics3/test_processing_idefics3.py",
            "status": "renamed",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fidefics3%2Ftest_processing_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fidefics3%2Ftest_processing_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics3%2Ftest_processing_idefics3.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "patch": "@@ -84,7 +84,7 @@ def get_processor(self, **kwargs):\n     def prepare_processor_dict():\n         return {\"image_seq_len\": 2}\n \n-    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_get_num_vision_tokens\n+    # Copied from tests.models.llava.test_processing_llava.LlavaProcessorTest.test_get_num_vision_tokens\n     def test_get_num_vision_tokens(self):\n         \"Tests general functionality of the helper used internally in vLLM\"\n \n@@ -284,7 +284,7 @@ def test_non_nested_images_with_batched_text(self):\n         self.assertEqual(np.array(inputs[\"pixel_values\"]).shape, (2, 2, 3, 364, 364))\n         self.assertEqual(np.array(inputs[\"pixel_attention_mask\"]).shape, (2, 2, 364, 364))\n \n-    # Copied from tests.models.idefics2.test_processor_idefics2.Idefics2ProcessorTest.test_process_interleaved_images_prompts_image_error\n+    # Copied from tests.models.idefics2.test_processing_idefics2.Idefics2ProcessorTest.test_process_interleaved_images_prompts_image_error\n     def test_process_interleaved_images_prompts_image_error(self):\n         processor = self.get_processor()\n ",
            "previous_filename": "tests/models/idefics3/test_processor_idefics3.py"
        },
        {
            "sha": "82da35ce79ec6ac49ed21e2e36b29ff91aabf559",
            "filename": "tests/models/instructblip/test_processing_instructblip.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Finstructblip%2Ftest_processing_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Finstructblip%2Ftest_processing_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_processing_instructblip.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/instructblip/test_processor_instructblip.py"
        },
        {
            "sha": "2c1b897dd1ca3ac38d8204079bb0d44fae918e4e",
            "filename": "tests/models/instructblipvideo/test_processing_instructblipvideo.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Finstructblipvideo%2Ftest_processing_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Finstructblipvideo%2Ftest_processing_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblipvideo%2Ftest_processing_instructblipvideo.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/instructblipvideo/test_processor_instructblipvideo.py"
        },
        {
            "sha": "b5cf23c8d2250b9367cda7e782faea74c967c5a8",
            "filename": "tests/models/internvl/test_processing_internvl.py",
            "status": "renamed",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Finternvl%2Ftest_processing_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Finternvl%2Ftest_processing_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finternvl%2Ftest_processing_internvl.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "patch": "@@ -97,7 +97,7 @@ def get_processor(self, **kwargs):\n     def tearDownClass(cls):\n         shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n \n-    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_get_num_vision_tokens\n+    # Copied from tests.models.llava.test_processing_llava.LlavaProcessorTest.test_get_num_vision_tokens\n     def test_get_num_vision_tokens(self):\n         \"Tests general functionality of the helper used internally in vLLM\"\n ",
            "previous_filename": "tests/models/internvl/test_processor_internvl.py"
        },
        {
            "sha": "f385ab4830e94f4ee318aac449f2356cd7ccc0a1",
            "filename": "tests/models/internvl/test_video_processing_internvl.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Finternvl%2Ftest_video_processing_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Finternvl%2Ftest_video_processing_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finternvl%2Ftest_video_processing_internvl.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/internvl/test_video_processor_internvl.py"
        },
        {
            "sha": "88fcc8888dbd72494273f285a57814ccd34d2de7",
            "filename": "tests/models/janus/test_processing_janus.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fjanus%2Ftest_processing_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fjanus%2Ftest_processing_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjanus%2Ftest_processing_janus.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/janus/test_processor_janus.py"
        },
        {
            "sha": "6f539e866b8bbe3d538e55a11a54ae1040733a5d",
            "filename": "tests/models/kosmos2/test_processing_kosmos2.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fkosmos2%2Ftest_processing_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fkosmos2%2Ftest_processing_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2%2Ftest_processing_kosmos2.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/kosmos2/test_processor_kosmos2.py"
        },
        {
            "sha": "e9f76cfbd7167e3c3ed0d25c3debbec867ad83a1",
            "filename": "tests/models/layoutlmv2/test_processing_layoutlmv2.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Flayoutlmv2%2Ftest_processing_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Flayoutlmv2%2Ftest_processing_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv2%2Ftest_processing_layoutlmv2.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/layoutlmv2/test_processor_layoutlmv2.py"
        },
        {
            "sha": "cf367c615ea0a88d818ee5aad64c4e6b38b839d5",
            "filename": "tests/models/layoutlmv3/test_processing_layoutlmv3.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Flayoutlmv3%2Ftest_processing_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Flayoutlmv3%2Ftest_processing_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv3%2Ftest_processing_layoutlmv3.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/layoutlmv3/test_processor_layoutlmv3.py"
        },
        {
            "sha": "2fc7a273b96cea66f869d9cc95deb42fe027a16f",
            "filename": "tests/models/layoutxlm/test_processing_layoutxlm.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Flayoutxlm%2Ftest_processing_layoutxlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Flayoutxlm%2Ftest_processing_layoutxlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutxlm%2Ftest_processing_layoutxlm.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/layoutxlm/test_processor_layoutxlm.py"
        },
        {
            "sha": "aef3539a37ea0ff3f4ea9620a926c7d5a80ef704",
            "filename": "tests/models/llama4/test_processing_llama4.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fllama4%2Ftest_processing_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fllama4%2Ftest_processing_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama4%2Ftest_processing_llama4.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/llama4/test_processor_llama4.py"
        },
        {
            "sha": "7e3be530d4ec0d573b5ee2851a2a70f98c079b6e",
            "filename": "tests/models/llava/test_processing_llava.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fllava%2Ftest_processing_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fllava%2Ftest_processing_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_processing_llava.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/llava/test_processor_llava.py"
        },
        {
            "sha": "915ea238b25582a61f39d6a95f180373ea958513",
            "filename": "tests/models/llava_next/test_processing_llava_next.py",
            "status": "renamed",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fllava_next%2Ftest_processing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fllava_next%2Ftest_processing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_processing_llava_next.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "patch": "@@ -66,7 +66,7 @@ def prepare_processor_dict():\n             \"vision_feature_select_strategy\": \"default\"\n         }  # fmt: skip\n \n-    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_get_num_vision_tokens\n+    # Copied from tests.models.llava.test_processing_llava.LlavaProcessorTest.test_get_num_vision_tokens\n     def test_get_num_vision_tokens(self):\n         \"Tests general functionality of the helper used internally in vLLM\"\n \n@@ -79,7 +79,7 @@ def test_get_num_vision_tokens(self):\n         self.assertTrue(\"num_image_patches\" in output)\n         self.assertEqual(len(output[\"num_image_patches\"]), 3)\n \n-    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_chat_template_is_saved\n+    # Copied from tests.models.llava.test_processing_llava.LlavaProcessorTest.test_chat_template_is_saved\n     def test_chat_template_is_saved(self):\n         processor_loaded = self.processor_class.from_pretrained(self.tmpdirname)\n         processor_dict_loaded = json.loads(processor_loaded.to_json_string())",
            "previous_filename": "tests/models/llava_next/test_processor_llava_next.py"
        },
        {
            "sha": "bf5d6082059d7089eccd79922bf2c43358dca35f",
            "filename": "tests/models/llava_next_video/test_processing_llava_next_video.py",
            "status": "renamed",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fllava_next_video%2Ftest_processing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fllava_next_video%2Ftest_processing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_processing_llava_next_video.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "patch": "@@ -75,7 +75,7 @@ def prepare_processor_dict(cls):\n             \"vision_feature_select_strategy\": \"default\",\n         }\n \n-    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_get_num_vision_tokens\n+    # Copied from tests.models.llava.test_processing_llava.LlavaProcessorTest.test_get_num_vision_tokens\n     def test_get_num_vision_tokens(self):\n         \"Tests general functionality of the helper used internally in vLLM\"\n \n@@ -88,7 +88,7 @@ def test_get_num_vision_tokens(self):\n         self.assertTrue(\"num_image_patches\" in output)\n         self.assertEqual(len(output[\"num_image_patches\"]), 3)\n \n-    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_chat_template_is_saved\n+    # Copied from tests.models.llava.test_processing_llava.LlavaProcessorTest.test_chat_template_is_saved\n     def test_chat_template_is_saved(self):\n         processor_loaded = self.processor_class.from_pretrained(self.tmpdirname)\n         processor_dict_loaded = json.loads(processor_loaded.to_json_string())",
            "previous_filename": "tests/models/llava_next_video/test_processor_llava_next_video.py"
        },
        {
            "sha": "d7e03443abba39a2ae378f407d16ee30f0f9d373",
            "filename": "tests/models/llava_onevision/test_processing_llava_onevision.py",
            "status": "renamed",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fllava_onevision%2Ftest_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fllava_onevision%2Ftest_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_processing_llava_onevision.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "patch": "@@ -79,7 +79,7 @@ def prepare_processor_dict():\n             \"vision_feature_select_strategy\": \"default\"\n         }  # fmt: skip\n \n-    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_get_num_vision_tokens\n+    # Copied from tests.models.llava.test_processing_llava.LlavaProcessorTest.test_get_num_vision_tokens\n     def test_get_num_vision_tokens(self):\n         \"Tests general functionality of the helper used internally in vLLM\"\n \n@@ -92,7 +92,7 @@ def test_get_num_vision_tokens(self):\n         self.assertTrue(\"num_image_patches\" in output)\n         self.assertEqual(len(output[\"num_image_patches\"]), 3)\n \n-    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_chat_template_is_saved\n+    # Copied from tests.models.llava.test_processing_llava.LlavaProcessorTest.test_chat_template_is_saved\n     def test_chat_template_is_saved(self):\n         processor_loaded = self.processor_class.from_pretrained(self.tmpdirname)\n         processor_dict_loaded = json.loads(processor_loaded.to_json_string())",
            "previous_filename": "tests/models/llava_onevision/test_processor_llava_onevision.py"
        },
        {
            "sha": "444651dc3d06988fd04cfc7d41ee24844da39fdc",
            "filename": "tests/models/markuplm/test_processing_markuplm.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fmarkuplm%2Ftest_processing_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fmarkuplm%2Ftest_processing_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmarkuplm%2Ftest_processing_markuplm.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/markuplm/test_processor_markuplm.py"
        },
        {
            "sha": "7157655abe23d4b4bd04d39c3da9ffd6cbb5e2c9",
            "filename": "tests/models/mgp_str/test_processing_mgp_str.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fmgp_str%2Ftest_processing_mgp_str.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fmgp_str%2Ftest_processing_mgp_str.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmgp_str%2Ftest_processing_mgp_str.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/mgp_str/test_processor_mgp_str.py"
        },
        {
            "sha": "d000b7854cb346cf8082088822d19ae4c5730158",
            "filename": "tests/models/mistral3/test_processing_mistral3.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fmistral3%2Ftest_processing_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fmistral3%2Ftest_processing_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral3%2Ftest_processing_mistral3.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/mistral3/test_processor_mistral3.py"
        },
        {
            "sha": "f20dd5d277fa26646767b4ce3e40df50c6bb504d",
            "filename": "tests/models/mllama/test_processing_mllama.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fmllama%2Ftest_processing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fmllama%2Ftest_processing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_processing_mllama.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/mllama/test_processor_mllama.py"
        },
        {
            "sha": "9070b753e5b193a11aa3dd5847ee808a8317963c",
            "filename": "tests/models/musicgen/test_processing_musicgen.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fmusicgen%2Ftest_processing_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fmusicgen%2Ftest_processing_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen%2Ftest_processing_musicgen.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/musicgen/test_processor_musicgen.py"
        },
        {
            "sha": "f6bf6ea17cd100e91071012f8b3a28c63b3725a3",
            "filename": "tests/models/musicgen_melody/test_processing_musicgen_melody.py",
            "status": "renamed",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fmusicgen_melody%2Ftest_processing_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fmusicgen_melody%2Ftest_processing_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen_melody%2Ftest_processing_musicgen_melody.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "patch": "@@ -50,7 +50,7 @@ def floats_list(shape, scale=1.0, rng=None, name=None):\n @require_torch\n @require_sentencepiece\n @require_torchaudio\n-# Copied from tests.models.musicgen.test_processor_musicgen.MusicgenProcessorTest with Musicgen->MusicgenMelody, Encodec->MusicgenMelody, padding_mask->attention_mask, input_values->input_features\n+# Copied from tests.models.musicgen.test_processing_musicgen.MusicgenProcessorTest with Musicgen->MusicgenMelody, Encodec->MusicgenMelody, padding_mask->attention_mask, input_values->input_features\n class MusicgenMelodyProcessorTest(unittest.TestCase):\n     def setUp(self):\n         # Ignore copy",
            "previous_filename": "tests/models/musicgen_melody/test_processor_musicgen_melody.py"
        },
        {
            "sha": "262cff8947c2a79f99e7f2a51cac518948fb221b",
            "filename": "tests/models/omdet_turbo/test_processing_omdet_turbo.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fomdet_turbo%2Ftest_processing_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fomdet_turbo%2Ftest_processing_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fomdet_turbo%2Ftest_processing_omdet_turbo.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/omdet_turbo/test_processor_omdet_turbo.py"
        },
        {
            "sha": "5e5a26e3a796a4b0f26f06b2d40b7264d30bb2d2",
            "filename": "tests/models/oneformer/test_processing_oneformer.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Foneformer%2Ftest_processing_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Foneformer%2Ftest_processing_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Foneformer%2Ftest_processing_oneformer.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/oneformer/test_processor_oneformer.py"
        },
        {
            "sha": "55dbe51e2a5c9e0887e69c34e81c50473947d4ea",
            "filename": "tests/models/owlv2/test_processing_owlv2.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fowlv2%2Ftest_processing_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fowlv2%2Ftest_processing_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fowlv2%2Ftest_processing_owlv2.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/owlv2/test_processor_owlv2.py"
        },
        {
            "sha": "069fa7b776e23277995738d3ba2425d2d7536ccd",
            "filename": "tests/models/owlvit/test_processing_owlvit.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fowlvit%2Ftest_processing_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fowlvit%2Ftest_processing_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fowlvit%2Ftest_processing_owlvit.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/owlvit/test_processor_owlvit.py"
        },
        {
            "sha": "25f33bfadaa1baf5724b2d127537af16a9ffd39c",
            "filename": "tests/models/paligemma/test_processing_paligemma.py",
            "status": "renamed",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fpaligemma%2Ftest_processing_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fpaligemma%2Ftest_processing_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_processing_paligemma.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "patch": "@@ -48,7 +48,7 @@ def setUpClass(cls):\n     def tearDownClass(cls):\n         shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n \n-    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_get_num_vision_tokens\n+    # Copied from tests.models.llava.test_processing_llava.LlavaProcessorTest.test_get_num_vision_tokens\n     def test_get_num_vision_tokens(self):\n         \"Tests general functionality of the helper used internally in vLLM\"\n ",
            "previous_filename": "tests/models/paligemma/test_processor_paligemma.py"
        },
        {
            "sha": "c6384e4b456cb61308a29d42a752bc311a378029",
            "filename": "tests/models/perception_lm/test_processing_perception_lm.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fperception_lm%2Ftest_processing_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fperception_lm%2Ftest_processing_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fperception_lm%2Ftest_processing_perception_lm.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/perception_lm/test_processor_perception_lm.py"
        },
        {
            "sha": "8d235b51990ba5c789e39b71c5acd341d74ae0d7",
            "filename": "tests/models/phi4_multimodal/test_feature_extraction_phi4_multimodal.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fphi4_multimodal%2Ftest_feature_extraction_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fphi4_multimodal%2Ftest_feature_extraction_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi4_multimodal%2Ftest_feature_extraction_phi4_multimodal.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/phi4_multimodal/test_feature_extractor_phi4_multimodal.py"
        },
        {
            "sha": "5bc3a10336343ad767129782dffc12287603a982",
            "filename": "tests/models/pix2struct/test_processing_pix2struct.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fpix2struct%2Ftest_processing_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fpix2struct%2Ftest_processing_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpix2struct%2Ftest_processing_pix2struct.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/pix2struct/test_processor_pix2struct.py"
        },
        {
            "sha": "2dc44d0d063bd568a0dee71e7741ff393dee5d50",
            "filename": "tests/models/pixtral/test_processing_pixtral.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fpixtral%2Ftest_processing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fpixtral%2Ftest_processing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpixtral%2Ftest_processing_pixtral.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/pixtral/test_processor_pixtral.py"
        },
        {
            "sha": "41b37621ec26b00a3d9380b3e77cd5d98587018e",
            "filename": "tests/models/pop2piano/test_processing_pop2piano.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fpop2piano%2Ftest_processing_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fpop2piano%2Ftest_processing_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpop2piano%2Ftest_processing_pop2piano.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/pop2piano/test_processor_pop2piano.py"
        },
        {
            "sha": "8295084b78246627091390934c44e7f0056cd33e",
            "filename": "tests/models/qwen2_5_omni/test_processing_qwen2_5_omni.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_processing_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_processing_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_processing_qwen2_5_omni.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/qwen2_5_omni/test_processor_qwen2_5_omni.py"
        },
        {
            "sha": "e05238d58810589ebfdfe95db454e8f3462b4509",
            "filename": "tests/models/qwen2_5_vl/test_processing_qwen2_5_vl.py",
            "status": "renamed",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processing_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processing_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processing_qwen2_5_vl.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "patch": "@@ -65,7 +65,7 @@ def get_processor(self, **kwargs):\n     def tearDownClass(cls):\n         shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n \n-    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_get_num_vision_tokens\n+    # Copied from tests.models.llava.test_processing_llava.LlavaProcessorTest.test_get_num_vision_tokens\n     def test_get_num_vision_tokens(self):\n         \"Tests general functionality of the helper used internally in vLLM\"\n ",
            "previous_filename": "tests/models/qwen2_5_vl/test_processor_qwen2_5_vl.py"
        },
        {
            "sha": "95832c72c2f275434e8cfd926209056cb4fec3e7",
            "filename": "tests/models/qwen2_audio/test_processing_qwen2_audio.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fqwen2_audio%2Ftest_processing_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fqwen2_audio%2Ftest_processing_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_audio%2Ftest_processing_qwen2_audio.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/qwen2_audio/test_processor_qwen2_audio.py"
        },
        {
            "sha": "05a82f5fc9cbca0c91bcba77d3ba9533501ea96d",
            "filename": "tests/models/qwen2_vl/test_processing_qwen2_vl.py",
            "status": "renamed",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fqwen2_vl%2Ftest_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fqwen2_vl%2Ftest_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_processing_qwen2_vl.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "patch": "@@ -68,7 +68,7 @@ def get_processor(self, **kwargs):\n     def tearDownClass(cls):\n         shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n \n-    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_get_num_vision_tokens\n+    # Copied from tests.models.llava.test_processing_llava.LlavaProcessorTest.test_get_num_vision_tokens\n     def test_get_num_vision_tokens(self):\n         \"Tests general functionality of the helper used internally in vLLM\"\n ",
            "previous_filename": "tests/models/qwen2_vl/test_processor_qwen2_vl.py"
        },
        {
            "sha": "b6095152051bd02f65d0ee45b0d0da86cc0884d3",
            "filename": "tests/models/sam/test_processing_sam.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fsam%2Ftest_processing_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fsam%2Ftest_processing_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam%2Ftest_processing_sam.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/sam/test_processor_sam.py"
        },
        {
            "sha": "36e00a9ce0bd4f3ae9232107f975a0c90e06f240",
            "filename": "tests/models/sam_hq/test_processing_samhq.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fsam_hq%2Ftest_processing_samhq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fsam_hq%2Ftest_processing_samhq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam_hq%2Ftest_processing_samhq.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/sam_hq/test_processor_samhq.py"
        },
        {
            "sha": "929255a82fbd09b8c1cd45f536448c5d4732095e",
            "filename": "tests/models/seamless_m4t/test_processing_seamless_m4t.py",
            "status": "renamed",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fseamless_m4t%2Ftest_processing_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fseamless_m4t%2Ftest_processing_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseamless_m4t%2Ftest_processing_seamless_m4t.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "patch": "@@ -76,7 +76,7 @@ def test_save_load_pretrained_additional_features(self):\n         tokenizer_instance = isinstance(processor.tokenizer, (SeamlessM4TTokenizerFast, SeamlessM4TTokenizer))\n         self.assertTrue(tokenizer_instance)\n \n-    # Copied from test.models.whisper.test_processor_whisper.WhisperProcessorTest.test_feature_extractor with Whisper->SeamlessM4T\n+    # Copied from test.models.whisper.test_processing_whisper.WhisperProcessorTest.test_feature_extractor with Whisper->SeamlessM4T\n     def test_feature_extractor(self):\n         feature_extractor = self.get_feature_extractor()\n         tokenizer = self.get_tokenizer()\n@@ -91,7 +91,7 @@ def test_feature_extractor(self):\n         for key in input_feat_extract:\n             self.assertAlmostEqual(input_feat_extract[key].sum(), input_processor[key].sum(), delta=1e-2)\n \n-    # Copied from test.models.whisper.test_processor_whisper.WhisperProcessorTest.test_tokenizer with Whisper->SeamlessM4T\n+    # Copied from test.models.whisper.test_processing_whisper.WhisperProcessorTest.test_tokenizer with Whisper->SeamlessM4T\n     def test_tokenizer(self):\n         feature_extractor = self.get_feature_extractor()\n         tokenizer = self.get_tokenizer()\n@@ -107,7 +107,7 @@ def test_tokenizer(self):\n         for key in encoded_tok:\n             self.assertListEqual(encoded_tok[key], encoded_processor[key])\n \n-    # Copied from test.models.whisper.test_processor_whisper.WhisperProcessorTest.test_tokenizer_decode with Whisper->SeamlessM4T\n+    # Copied from test.models.whisper.test_processing_whisper.WhisperProcessorTest.test_tokenizer_decode with Whisper->SeamlessM4T\n     def test_tokenizer_decode(self):\n         feature_extractor = self.get_feature_extractor()\n         tokenizer = self.get_tokenizer()",
            "previous_filename": "tests/models/seamless_m4t/test_processor_seamless_m4t.py"
        },
        {
            "sha": "88f1c7990283b18648c0685bf2a66372426f3eaf",
            "filename": "tests/models/smolvlm/test_processing_smolvlm.py",
            "status": "renamed",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fsmolvlm%2Ftest_processing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fsmolvlm%2Ftest_processing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmolvlm%2Ftest_processing_smolvlm.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "patch": "@@ -297,7 +297,7 @@ def test_non_nested_images_with_batched_text(self):\n         self.assertEqual(np.array(inputs[\"pixel_values\"]).shape, (2, 2, 3, 512, 512))\n         self.assertEqual(np.array(inputs[\"pixel_attention_mask\"]).shape, (2, 2, 512, 512))\n \n-    # Copied from tests.models.idefics2.test_processor_idefics2.Idefics2ProcessorTest.test_process_interleaved_images_prompts_image_error\n+    # Copied from tests.models.idefics2.test_processing_idefics2.Idefics2ProcessorTest.test_process_interleaved_images_prompts_image_error\n     def test_process_interleaved_images_prompts_image_error(self):\n         processor = self.get_processor()\n ",
            "previous_filename": "tests/models/smolvlm/test_processor_smolvlm.py"
        },
        {
            "sha": "f4794cca538eb1b10d2a3602d28fd45101082c01",
            "filename": "tests/models/speech_to_text/test_processing_speech_to_text.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fspeech_to_text%2Ftest_processing_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fspeech_to_text%2Ftest_processing_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeech_to_text%2Ftest_processing_speech_to_text.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/speech_to_text/test_processor_speech_to_text.py"
        },
        {
            "sha": "ea84104ac71b7618c504826a5a146a153c88a721",
            "filename": "tests/models/speecht5/test_processing_speecht5.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fspeecht5%2Ftest_processing_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fspeecht5%2Ftest_processing_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeecht5%2Ftest_processing_speecht5.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/speecht5/test_processor_speecht5.py"
        },
        {
            "sha": "205f268fe6a5fccc2226c2ec1be3dca30f197d67",
            "filename": "tests/models/trocr/test_processing_trocr.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Ftrocr%2Ftest_processing_trocr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Ftrocr%2Ftest_processing_trocr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftrocr%2Ftest_processing_trocr.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/trocr/test_processor_trocr.py"
        },
        {
            "sha": "2fc3f59d2dbaef60e3a903216a0c146a922241e0",
            "filename": "tests/models/udop/test_processing_udop.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fudop%2Ftest_processing_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fudop%2Ftest_processing_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fudop%2Ftest_processing_udop.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/udop/test_processor_udop.py"
        },
        {
            "sha": "814c202fdbefed3990abb38bfb3ddfcb35bdae3f",
            "filename": "tests/models/vision_text_dual_encoder/test_processing_vision_text_dual_encoder.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_processing_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_processing_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_processing_vision_text_dual_encoder.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/vision_text_dual_encoder/test_processor_vision_text_dual_encoder.py"
        },
        {
            "sha": "1ed5fa2982dd8af6104ae73138bb2f783d6c9ee2",
            "filename": "tests/models/wav2vec2/test_processing_wav2vec2.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fwav2vec2%2Ftest_processing_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fwav2vec2%2Ftest_processing_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2%2Ftest_processing_wav2vec2.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/wav2vec2/test_processor_wav2vec2.py"
        },
        {
            "sha": "5c4f062de777c0b7a9f010d76c55df23306fbd92",
            "filename": "tests/models/wav2vec2_bert/test_processing_wav2vec2_bert.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fwav2vec2_bert%2Ftest_processing_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fwav2vec2_bert%2Ftest_processing_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2_bert%2Ftest_processing_wav2vec2_bert.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/wav2vec2_bert/test_processor_wav2vec2_bert.py"
        },
        {
            "sha": "97e83ff3677e2da8830d6cb1d605530d0c0c7966",
            "filename": "tests/models/wav2vec2_with_lm/test_processing_wav2vec2_with_lm.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fwav2vec2_with_lm%2Ftest_processing_wav2vec2_with_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fwav2vec2_with_lm%2Ftest_processing_wav2vec2_with_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2_with_lm%2Ftest_processing_wav2vec2_with_lm.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/wav2vec2_with_lm/test_processor_wav2vec2_with_lm.py"
        },
        {
            "sha": "a80e7950b226ba34f41856eb5c445df3d481052c",
            "filename": "tests/models/whisper/test_processing_whisper.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fwhisper%2Ftest_processing_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Fmodels%2Fwhisper%2Ftest_processing_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_processing_whisper.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "previous_filename": "tests/models/whisper/test_processor_whisper.py"
        },
        {
            "sha": "dffe7189780607e3c767005a4288384a27aacdcd",
            "filename": "tests/utils/test_add_new_model_like.py",
            "status": "modified",
            "additions": 712,
            "deletions": 1126,
            "changes": 1838,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Futils%2Ftest_add_new_model_like.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/tests%2Futils%2Ftest_add_new_model_like.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_add_new_model_like.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "patch": "@@ -12,1229 +12,815 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import os\n-import re\n+import shutil\n import tempfile\n+import textwrap\n import unittest\n+from datetime import date\n+from pathlib import Path\n \n-from transformers.commands.add_new_model_like import (\n-    ModelPatterns,\n-    _re_class_func,\n-    add_content_to_file,\n-    add_content_to_text,\n-    clean_frameworks_in_init,\n-    duplicate_doc_file,\n-    duplicate_module,\n-    filter_framework_files,\n-    find_base_model_checkpoint,\n-    get_model_files,\n-    get_module_from_file,\n-    parse_module_content,\n-    replace_model_patterns,\n-    retrieve_info_for_model,\n-    retrieve_model_classes,\n-    simplify_replacements,\n-)\n+import transformers.commands.add_new_model_like\n+from transformers.commands.add_new_model_like import ModelInfos, create_new_model_like\n from transformers.testing_utils import require_torch\n \n \n-BERT_MODEL_FILES = {\n-    \"transformers/models/bert/__init__.py\",\n-    \"transformers/models/bert/configuration_bert.py\",\n-    \"transformers/models/bert/tokenization_bert.py\",\n-    \"transformers/models/bert/tokenization_bert_fast.py\",\n-    \"transformers/models/bert/tokenization_bert_tf.py\",\n-    \"transformers/models/bert/modeling_bert.py\",\n-    \"transformers/models/bert/modeling_flax_bert.py\",\n-    \"transformers/models/bert/modeling_tf_bert.py\",\n-    \"transformers/models/bert/convert_bert_original_tf_checkpoint_to_pytorch.py\",\n-    \"transformers/models/bert/convert_bert_original_tf2_checkpoint_to_pytorch.py\",\n-    \"transformers/models/bert/convert_bert_pytorch_checkpoint_to_original_tf.py\",\n-    \"transformers/models/bert/convert_bert_token_dropping_original_tf2_checkpoint_to_pytorch.py\",\n-}\n-\n-VIT_MODEL_FILES = {\n-    \"transformers/models/vit/__init__.py\",\n-    \"transformers/models/vit/configuration_vit.py\",\n-    \"transformers/models/vit/convert_dino_to_pytorch.py\",\n-    \"transformers/models/vit/convert_vit_timm_to_pytorch.py\",\n-    \"transformers/models/vit/feature_extraction_vit.py\",\n-    \"transformers/models/vit/image_processing_vit.py\",\n-    \"transformers/models/vit/image_processing_vit_fast.py\",\n-    \"transformers/models/vit/modeling_vit.py\",\n-    \"transformers/models/vit/modeling_tf_vit.py\",\n-    \"transformers/models/vit/modeling_flax_vit.py\",\n-}\n-\n-WAV2VEC2_MODEL_FILES = {\n-    \"transformers/models/wav2vec2/__init__.py\",\n-    \"transformers/models/wav2vec2/configuration_wav2vec2.py\",\n-    \"transformers/models/wav2vec2/convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py\",\n-    \"transformers/models/wav2vec2/convert_wav2vec2_original_s3prl_checkpoint_to_pytorch.py\",\n-    \"transformers/models/wav2vec2/feature_extraction_wav2vec2.py\",\n-    \"transformers/models/wav2vec2/modeling_wav2vec2.py\",\n-    \"transformers/models/wav2vec2/modeling_tf_wav2vec2.py\",\n-    \"transformers/models/wav2vec2/modeling_flax_wav2vec2.py\",\n-    \"transformers/models/wav2vec2/processing_wav2vec2.py\",\n-    \"transformers/models/wav2vec2/tokenization_wav2vec2.py\",\n-}\n-\n-\n-def get_last_n_components_of_path(path, n):\n-    \"\"\"\n-    Get the last `components` of the path. E.g. `get_last_n_components_of_path(\"/foo/bar/baz\", 2)` returns `bar/baz`\n-    \"\"\"\n-    return os.path.sep.join(os.path.normpath(path).split(os.path.sep)[-n:])\n+REPO_PATH = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))\n+MODELS_TO_COPY = (\"auto\", \"llama\", \"phi4_multimodal\")\n+CURRENT_YEAR = date.today().year\n \n \n @require_torch\n class TestAddNewModelLike(unittest.TestCase):\n-    def init_file(self, file_name, content):\n-        with open(file_name, \"w\", encoding=\"utf-8\") as f:\n-            f.write(content)\n-\n-    def check_result(self, file_name, expected_result):\n-        with open(file_name, encoding=\"utf-8\") as f:\n-            result = f.read()\n-            self.assertEqual(result, expected_result)\n-\n-    def test_re_class_func(self):\n-        self.assertEqual(_re_class_func.search(\"def my_function(x, y):\").groups()[0], \"my_function\")\n-        self.assertEqual(_re_class_func.search(\"class MyClass:\").groups()[0], \"MyClass\")\n-        self.assertEqual(_re_class_func.search(\"class MyClass(SuperClass):\").groups()[0], \"MyClass\")\n-\n-    def test_model_patterns_defaults(self):\n-        model_patterns = ModelPatterns(\"GPT-New new\", \"huggingface/gpt-new-base\")\n-\n-        self.assertEqual(model_patterns.model_type, \"gpt-new-new\")\n-        self.assertEqual(model_patterns.model_lower_cased, \"gpt_new_new\")\n-        self.assertEqual(model_patterns.model_camel_cased, \"GPTNewNew\")\n-        self.assertEqual(model_patterns.model_upper_cased, \"GPT_NEW_NEW\")\n-        self.assertEqual(model_patterns.config_class, \"GPTNewNewConfig\")\n-        self.assertIsNone(model_patterns.tokenizer_class)\n-        self.assertIsNone(model_patterns.feature_extractor_class)\n-        self.assertIsNone(model_patterns.processor_class)\n-\n-    def test_parse_module_content(self):\n-        test_code = \"\"\"SOME_CONSTANT = a constant\n-\n-CONSTANT_DEFINED_ON_SEVERAL_LINES = [\n-    first_item,\n-    second_item\n-]\n-\n-def function(args):\n-    some code\n-\n-# Copied from transformers.some_module\n-class SomeClass:\n-    some code\n-\"\"\"\n-\n-        expected_parts = [\n-            \"SOME_CONSTANT = a constant\\n\",\n-            \"CONSTANT_DEFINED_ON_SEVERAL_LINES = [\\n    first_item,\\n    second_item\\n]\",\n-            \"\",\n-            \"def function(args):\\n    some code\\n\",\n-            \"# Copied from transformers.some_module\\nclass SomeClass:\\n    some code\\n\",\n-        ]\n-        self.assertEqual(parse_module_content(test_code), expected_parts)\n-\n-    def test_add_content_to_text(self):\n-        test_text = \"\"\"all_configs = {\n-    \"gpt\": \"GPTConfig\",\n-    \"bert\": \"BertConfig\",\n-    \"t5\": \"T5Config\",\n-}\"\"\"\n-\n-        expected = \"\"\"all_configs = {\n-    \"gpt\": \"GPTConfig\",\n-    \"gpt2\": \"GPT2Config\",\n-    \"bert\": \"BertConfig\",\n-    \"t5\": \"T5Config\",\n-}\"\"\"\n-        line = '    \"gpt2\": \"GPT2Config\",'\n-\n-        self.assertEqual(add_content_to_text(test_text, line, add_before=\"bert\"), expected)\n-        self.assertEqual(add_content_to_text(test_text, line, add_before=\"bert\", exact_match=True), test_text)\n-        self.assertEqual(\n-            add_content_to_text(test_text, line, add_before='    \"bert\": \"BertConfig\",', exact_match=True), expected\n+    @classmethod\n+    def setUpClass(cls):\n+        \"\"\"\n+        Create a temporary repo with the same structure as Transformers, with just 2 models.\n+        \"\"\"\n+        cls.FAKE_REPO = tempfile.TemporaryDirectory().name\n+        os.makedirs(os.path.join(cls.FAKE_REPO, \"src\", \"transformers\", \"models\"), exist_ok=True)\n+        os.makedirs(os.path.join(cls.FAKE_REPO, \"tests\", \"models\"), exist_ok=True)\n+        os.makedirs(os.path.join(cls.FAKE_REPO, \"docs\", \"source\", \"en\", \"model_doc\"), exist_ok=True)\n+\n+        # We need to copy the utils to run the cleanup commands\n+        utils_src = os.path.join(REPO_PATH, \"utils\")\n+        shutil.copytree(utils_src, utils_src.replace(REPO_PATH, cls.FAKE_REPO))\n+        # Copy the __init__ files\n+        model_init = os.path.join(REPO_PATH, \"src\", \"transformers\", \"models\", \"__init__.py\")\n+        shutil.copy(model_init, model_init.replace(REPO_PATH, cls.FAKE_REPO))\n+        doc_toc = os.path.join(REPO_PATH, \"docs\", \"source\", \"en\", \"_toctree.yml\")\n+        shutil.copy(doc_toc, doc_toc.replace(REPO_PATH, cls.FAKE_REPO))\n+        # We need the pyproject for ruff as well\n+        pyproject = os.path.join(REPO_PATH, \"pyproject.toml\")\n+        shutil.copy(pyproject, pyproject.replace(REPO_PATH, cls.FAKE_REPO))\n+        # Copy over all the specific model files\n+        for model in MODELS_TO_COPY:\n+            model_src = os.path.join(REPO_PATH, \"src\", \"transformers\", \"models\", model)\n+            shutil.copytree(model_src, model_src.replace(REPO_PATH, cls.FAKE_REPO))\n+\n+            test_src = os.path.join(REPO_PATH, \"tests\", \"models\", model)\n+            shutil.copytree(test_src, test_src.replace(REPO_PATH, cls.FAKE_REPO))\n+\n+            if model != \"auto\":\n+                doc_src = os.path.join(REPO_PATH, \"docs\", \"source\", \"en\", \"model_doc\", f\"{model}.md\")\n+                shutil.copy(doc_src, doc_src.replace(REPO_PATH, cls.FAKE_REPO))\n+\n+        # Replace the globals\n+        cls.ORIGINAL_REPO = transformers.commands.add_new_model_like.REPO_PATH\n+        cls.ORIGINAL_TRANSFORMERS_REPO = transformers.commands.add_new_model_like.TRANSFORMERS_PATH\n+        transformers.commands.add_new_model_like.REPO_PATH = Path(cls.FAKE_REPO)\n+        transformers.commands.add_new_model_like.TRANSFORMERS_PATH = Path(cls.FAKE_REPO) / \"src\" / \"transformers\"\n+\n+        # For convenience\n+        cls.MODEL_PATH = os.path.join(cls.FAKE_REPO, \"src\", \"transformers\", \"models\")\n+        cls.TESTS_MODEL_PATH = os.path.join(cls.FAKE_REPO, \"tests\", \"models\")\n+        cls.DOC_PATH = os.path.join(cls.FAKE_REPO, \"docs\", \"source\", \"en\")\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        transformers.commands.add_new_model_like.REPO_PATH = cls.ORIGINAL_REPO\n+        transformers.commands.add_new_model_like.TRANSFORMERS_PATH = cls.ORIGINAL_TRANSFORMERS_REPO\n+        del cls.FAKE_REPO\n+\n+    def assertFileIsEqual(self, text: str, filepath: str):\n+        with open(filepath, \"r\") as f:\n+            file_text = f.read()\n+        self.assertEqual(file_text.strip(), text.strip())\n+\n+    def assertInFile(self, text: str, filepath: str):\n+        with open(filepath, \"r\") as f:\n+            file_text = f.read()\n+        self.assertTrue(text in file_text)\n+\n+    def test_llama_without_tokenizers(self):\n+        # This is the structure without adding the tokenizers\n+        filenames_to_add = (\n+            (\"configuration_llama.py\", True),\n+            (\"modeling_llama.py\", True),\n+            (\"tokenization_llama.py\", False),\n+            (\"tokenization_llama_fast.py\", False),\n+            (\"image_processing_llama.py\", False),\n+            (\"image_processing_llama_fast.py\", False),\n+            (\"video_processing_llama.py\", False),\n+            (\"feature_extraction_llama.py\", False),\n+            (\"processing_llama.py\", False),\n+        )\n+        # Run the command\n+        create_new_model_like(\n+            old_model_infos=ModelInfos(\"llama\"),\n+            new_lowercase_name=\"my_test\",\n+            new_model_paper_name=\"MyTest\",\n+            filenames_to_add=filenames_to_add,\n+            create_fast_image_processor=False,\n         )\n-        self.assertEqual(add_content_to_text(test_text, line, add_before=re.compile(r'^\\s*\"bert\":')), expected)\n \n-        self.assertEqual(add_content_to_text(test_text, line, add_after=\"gpt\"), expected)\n-        self.assertEqual(add_content_to_text(test_text, line, add_after=\"gpt\", exact_match=True), test_text)\n-        self.assertEqual(\n-            add_content_to_text(test_text, line, add_after='    \"gpt\": \"GPTConfig\",', exact_match=True), expected\n+        # First assert that all files were created correctly\n+        model_repo = os.path.join(self.MODEL_PATH, \"my_test\")\n+        tests_repo = os.path.join(self.TESTS_MODEL_PATH, \"my_test\")\n+        self.assertTrue(os.path.isfile(os.path.join(model_repo, \"modular_my_test.py\")))\n+        self.assertTrue(os.path.isfile(os.path.join(model_repo, \"modeling_my_test.py\")))\n+        self.assertTrue(os.path.isfile(os.path.join(model_repo, \"configuration_my_test.py\")))\n+        self.assertTrue(os.path.isfile(os.path.join(model_repo, \"__init__.py\")))\n+        self.assertTrue(os.path.isfile(os.path.join(self.DOC_PATH, \"model_doc\", \"my_test.md\")))\n+        self.assertTrue(os.path.isfile(os.path.join(tests_repo, \"__init__.py\")))\n+        self.assertTrue(os.path.isfile(os.path.join(tests_repo, \"test_modeling_my_test.py\")))\n+\n+        # Now assert the correct imports/auto mappings/toctree were added\n+        self.assertInFile(\n+            \"from .my_test import *\\n\",\n+            os.path.join(self.MODEL_PATH, \"__init__.py\"),\n         )\n-        self.assertEqual(add_content_to_text(test_text, line, add_after=re.compile(r'^\\s*\"gpt\":')), expected)\n-\n-    def test_add_content_to_file(self):\n-        test_text = \"\"\"all_configs = {\n-    \"gpt\": \"GPTConfig\",\n-    \"bert\": \"BertConfig\",\n-    \"t5\": \"T5Config\",\n-}\"\"\"\n-\n-        expected = \"\"\"all_configs = {\n-    \"gpt\": \"GPTConfig\",\n-    \"gpt2\": \"GPT2Config\",\n-    \"bert\": \"BertConfig\",\n-    \"t5\": \"T5Config\",\n-}\"\"\"\n-        line = '    \"gpt2\": \"GPT2Config\",'\n-\n-        with tempfile.TemporaryDirectory() as tmp_dir:\n-            file_name = os.path.join(tmp_dir, \"code.py\")\n-\n-            self.init_file(file_name, test_text)\n-            add_content_to_file(file_name, line, add_before=\"bert\")\n-            self.check_result(file_name, expected)\n-\n-            self.init_file(file_name, test_text)\n-            add_content_to_file(file_name, line, add_before=\"bert\", exact_match=True)\n-            self.check_result(file_name, test_text)\n-\n-            self.init_file(file_name, test_text)\n-            add_content_to_file(file_name, line, add_before='    \"bert\": \"BertConfig\",', exact_match=True)\n-            self.check_result(file_name, expected)\n-\n-            self.init_file(file_name, test_text)\n-            add_content_to_file(file_name, line, add_before=re.compile(r'^\\s*\"bert\":'))\n-            self.check_result(file_name, expected)\n-\n-            self.init_file(file_name, test_text)\n-            add_content_to_file(file_name, line, add_after=\"gpt\")\n-            self.check_result(file_name, expected)\n-\n-            self.init_file(file_name, test_text)\n-            add_content_to_file(file_name, line, add_after=\"gpt\", exact_match=True)\n-            self.check_result(file_name, test_text)\n-\n-            self.init_file(file_name, test_text)\n-            add_content_to_file(file_name, line, add_after='    \"gpt\": \"GPTConfig\",', exact_match=True)\n-            self.check_result(file_name, expected)\n-\n-            self.init_file(file_name, test_text)\n-            add_content_to_file(file_name, line, add_after=re.compile(r'^\\s*\"gpt\":'))\n-            self.check_result(file_name, expected)\n-\n-    def test_simplify_replacements(self):\n-        self.assertEqual(simplify_replacements([(\"Bert\", \"NewBert\")]), [(\"Bert\", \"NewBert\")])\n-        self.assertEqual(\n-            simplify_replacements([(\"Bert\", \"NewBert\"), (\"bert\", \"new-bert\")]),\n-            [(\"Bert\", \"NewBert\"), (\"bert\", \"new-bert\")],\n+        self.assertInFile(\n+            '(\"my_test\", \"MyTestConfig\"),\\n',\n+            os.path.join(self.MODEL_PATH, \"auto\", \"configuration_auto.py\"),\n         )\n-        self.assertEqual(\n-            simplify_replacements([(\"BertConfig\", \"NewBertConfig\"), (\"Bert\", \"NewBert\"), (\"bert\", \"new-bert\")]),\n-            [(\"Bert\", \"NewBert\"), (\"bert\", \"new-bert\")],\n+        self.assertInFile(\n+            '(\"my_test\", \"MyTest\"),\\n',\n+            os.path.join(self.MODEL_PATH, \"auto\", \"configuration_auto.py\"),\n         )\n-\n-    def test_replace_model_patterns(self):\n-        bert_model_patterns = ModelPatterns(\"Bert\", \"google-bert/bert-base-cased\")\n-        new_bert_model_patterns = ModelPatterns(\"New Bert\", \"huggingface/bert-new-base\")\n-        bert_test = '''class TFBertPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n-    config_class = BertConfig\n-    load_tf_weights = load_tf_weights_in_bert\n-    base_model_prefix = \"bert\"\n-    is_parallelizable = True\n-    supports_gradient_checkpointing = True\n-    model_type = \"bert\"\n-\n-BERT_CONSTANT = \"value\"\n-'''\n-        bert_expected = '''class TFNewBertPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n-    config_class = NewBertConfig\n-    load_tf_weights = load_tf_weights_in_new_bert\n-    base_model_prefix = \"new_bert\"\n-    is_parallelizable = True\n-    supports_gradient_checkpointing = True\n-    model_type = \"new-bert\"\n-\n-NEW_BERT_CONSTANT = \"value\"\n-'''\n-\n-        bert_converted, replacements = replace_model_patterns(bert_test, bert_model_patterns, new_bert_model_patterns)\n-        self.assertEqual(bert_converted, bert_expected)\n-        # Replacements are empty here since bert as been replaced by bert_new in some instances and bert-new\n-        # in others.\n-        self.assertEqual(replacements, \"\")\n-\n-        # If we remove the model type, we will get replacements\n-        bert_test = bert_test.replace('    model_type = \"bert\"\\n', \"\")\n-        bert_expected = bert_expected.replace('    model_type = \"new-bert\"\\n', \"\")\n-        bert_converted, replacements = replace_model_patterns(bert_test, bert_model_patterns, new_bert_model_patterns)\n-        self.assertEqual(bert_converted, bert_expected)\n-        self.assertEqual(replacements, \"BERT->NEW_BERT,Bert->NewBert,bert->new_bert\")\n-\n-        gpt_model_patterns = ModelPatterns(\"GPT2\", \"gpt2\")\n-        new_gpt_model_patterns = ModelPatterns(\"GPT-New new\", \"huggingface/gpt-new-base\")\n-        gpt_test = '''class GPT2PreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n-    config_class = GPT2Config\n-    load_tf_weights = load_tf_weights_in_gpt2\n-    base_model_prefix = \"transformer\"\n-    is_parallelizable = True\n-    supports_gradient_checkpointing = True\n-\n-GPT2_CONSTANT = \"value\"\n-'''\n-\n-        gpt_expected = '''class GPTNewNewPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n-    config_class = GPTNewNewConfig\n-    load_tf_weights = load_tf_weights_in_gpt_new_new\n-    base_model_prefix = \"transformer\"\n-    is_parallelizable = True\n-    supports_gradient_checkpointing = True\n-\n-GPT_NEW_NEW_CONSTANT = \"value\"\n-'''\n-\n-        gpt_converted, replacements = replace_model_patterns(gpt_test, gpt_model_patterns, new_gpt_model_patterns)\n-        self.assertEqual(gpt_converted, gpt_expected)\n-        # Replacements are empty here since GPT2 as been replaced by GPTNewNew in some instances and GPT_NEW_NEW\n-        # in others.\n-        self.assertEqual(replacements, \"\")\n-\n-        roberta_model_patterns = ModelPatterns(\"RoBERTa\", \"FacebookAI/roberta-base\", model_camel_cased=\"Roberta\")\n-        new_roberta_model_patterns = ModelPatterns(\n-            \"RoBERTa-New\", \"huggingface/roberta-new-base\", model_camel_cased=\"RobertaNew\"\n+        self.assertInFile(\n+            '(\"my_test\", \"MyTestModel\"),\\n',\n+            os.path.join(self.MODEL_PATH, \"auto\", \"modeling_auto.py\"),\n+        )\n+        self.assertInFile(\n+            '(\"my_test\", \"MyTestForCausalLM\"),\\n',\n+            os.path.join(self.MODEL_PATH, \"auto\", \"modeling_auto.py\"),\n+        )\n+        self.assertInFile(\n+            '(\"my_test\", \"MyTestForSequenceClassification\"),\\n',\n+            os.path.join(self.MODEL_PATH, \"auto\", \"modeling_auto.py\"),\n+        )\n+        self.assertInFile(\n+            '(\"my_test\", \"MyTestForQuestionAnswering\"),\\n',\n+            os.path.join(self.MODEL_PATH, \"auto\", \"modeling_auto.py\"),\n         )\n-        roberta_test = '''# Copied from transformers.models.bert.BertModel with Bert->Roberta\n-class RobertaModel(RobertaPreTrainedModel):\n-    \"\"\" The base RoBERTa model. \"\"\"\n-    checkpoint = FacebookAI/roberta-base\n-    base_model_prefix = \"roberta\"\n-        '''\n-        roberta_expected = '''# Copied from transformers.models.bert.BertModel with Bert->RobertaNew\n-class RobertaNewModel(RobertaNewPreTrainedModel):\n-    \"\"\" The base RoBERTa-New model. \"\"\"\n-    checkpoint = huggingface/roberta-new-base\n-    base_model_prefix = \"roberta_new\"\n-        '''\n-        roberta_converted, replacements = replace_model_patterns(\n-            roberta_test, roberta_model_patterns, new_roberta_model_patterns\n+        self.assertInFile(\n+            '(\"my_test\", \"MyTestForTokenClassification\"),\\n',\n+            os.path.join(self.MODEL_PATH, \"auto\", \"modeling_auto.py\"),\n         )\n-        self.assertEqual(roberta_converted, roberta_expected)\n+        self.assertInFile(\n+            \"- local: model_doc/my_test\\n        title: MyTest\\n\",\n+            os.path.join(self.DOC_PATH, \"_toctree.yml\"),\n+        )\n+\n+        # Check some exact file creation. For model definition, only check modular as modeling/config/etc... are created\n+        # directly from it\n+        EXPECTED_MODULAR = textwrap.dedent(\n+            f\"\"\"\n+            # coding=utf-8\n+            # Copyright {CURRENT_YEAR} the HuggingFace Team. All rights reserved.\n+            #\n+            # Licensed under the Apache License, Version 2.0 (the \"License\");\n+            # you may not use this file except in compliance with the License.\n+            # You may obtain a copy of the License at\n+            #\n+            #     http://www.apache.org/licenses/LICENSE-2.0\n+            #\n+            # Unless required by applicable law or agreed to in writing, software\n+            # distributed under the License is distributed on an \"AS IS\" BASIS,\n+            # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+            # See the License for the specific language governing permissions and\n+            # limitations under the License.\n+\n+            from ..llama.configuration_llama import LlamaConfig\n+            from ..llama.modeling_llama import (\n+                LlamaAttention,\n+                LlamaDecoderLayer,\n+                LlamaForCausalLM,\n+                LlamaForQuestionAnswering,\n+                LlamaForSequenceClassification,\n+                LlamaForTokenClassification,\n+                LlamaMLP,\n+                LlamaModel,\n+                LlamaPreTrainedModel,\n+                LlamaRMSNorm,\n+                LlamaRotaryEmbedding,\n+            )\n+\n+\n+            class MyTestConfig(LlamaConfig):\n+                pass\n+\n+\n+            class MyTestRMSNorm(LlamaRMSNorm):\n+                pass\n+\n+\n+            class MyTestRotaryEmbedding(LlamaRotaryEmbedding):\n+                pass\n+\n+\n+            class MyTestMLP(LlamaMLP):\n+                pass\n+\n+\n+            class MyTestAttention(LlamaAttention):\n+                pass\n+\n+\n+            class MyTestDecoderLayer(LlamaDecoderLayer):\n+                pass\n+\n+\n+            class MyTestPreTrainedModel(LlamaPreTrainedModel):\n+                pass\n+\n+\n+            class MyTestModel(LlamaModel):\n+                pass\n+\n+\n+            class MyTestForCausalLM(LlamaForCausalLM):\n+                pass\n+\n+\n+            class MyTestForSequenceClassification(LlamaForSequenceClassification):\n+                pass\n+\n+\n+            class MyTestForQuestionAnswering(LlamaForQuestionAnswering):\n+                pass\n+\n+\n+            class MyTestForTokenClassification(LlamaForTokenClassification):\n+                pass\n+\n \n-    def test_get_module_from_file(self):\n-        self.assertEqual(\n-            get_module_from_file(\"/git/transformers/src/transformers/models/bert/modeling_tf_bert.py\"),\n-            \"transformers.models.bert.modeling_tf_bert\",\n+            __all__ = [\n+                \"MyTestConfig\",\n+                \"MyTestForCausalLM\",\n+                \"MyTestModel\",\n+                \"MyTestPreTrainedModel\",\n+                \"MyTestForSequenceClassification\",\n+                \"MyTestForQuestionAnswering\",\n+                \"MyTestForTokenClassification\",\n+            ]\n+            \"\"\"\n         )\n-        self.assertEqual(\n-            get_module_from_file(\"/transformers/models/gpt2/modeling_gpt2.py\"),\n-            \"transformers.models.gpt2.modeling_gpt2\",\n+        self.assertFileIsEqual(EXPECTED_MODULAR, os.path.join(model_repo, \"modular_my_test.py\"))\n+\n+        EXPECTED_INIT = textwrap.dedent(\n+            f\"\"\"\n+            # coding=utf-8\n+            # Copyright {CURRENT_YEAR} the HuggingFace Team. All rights reserved.\n+            #\n+            # Licensed under the Apache License, Version 2.0 (the \"License\");\n+            # you may not use this file except in compliance with the License.\n+            # You may obtain a copy of the License at\n+            #\n+            #     http://www.apache.org/licenses/LICENSE-2.0\n+            #\n+            # Unless required by applicable law or agreed to in writing, software\n+            # distributed under the License is distributed on an \"AS IS\" BASIS,\n+            # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+            # See the License for the specific language governing permissions and\n+            # limitations under the License.\n+\n+            from typing import TYPE_CHECKING\n+\n+            from ...utils import _LazyModule\n+            from ...utils.import_utils import define_import_structure\n+\n+\n+            if TYPE_CHECKING:\n+                from .configuration_my_test import *\n+                from .modeling_my_test import *\n+            else:\n+                import sys\n+\n+                _file = globals()[\"__file__\"]\n+                sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)\n+\n+            \"\"\"\n+        )\n+        self.assertFileIsEqual(EXPECTED_INIT, os.path.join(model_repo, \"__init__.py\"))\n+\n+        EXPECTED_DOC = textwrap.dedent(\n+            f\"\"\"\n+            <!--Copyright {CURRENT_YEAR} the HuggingFace Team. All rights reserved.\n+\n+            Licensed under the Apache License, Version 2.0 (the \"License\");\n+            you may not use this file except in compliance with the License.\n+            You may obtain a copy of the License at\n+\n+                http://www.apache.org/licenses/LICENSE-2.0\n+\n+            Unless required by applicable law or agreed to in writing, software\n+            distributed under the License is distributed on an \"AS IS\" BASIS,\n+            WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+            See the License for the specific language governing permissions and\n+            limitations under the License.\n+\n+\n+             Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be rendered properly in your Markdown viewer.\n+\n+            -->\n+\n+\n+            # MyTest\n+\n+            ## Overview\n+\n+            The MyTest model was proposed in [<INSERT PAPER NAME HERE>](<INSERT PAPER LINK HERE>) by <INSERT AUTHORS HERE>.\n+            <INSERT SHORT SUMMARY HERE>\n+\n+            The abstract from the paper is the following:\n+\n+            <INSERT PAPER ABSTRACT HERE>\n+\n+            Tips:\n+\n+            <INSERT TIPS ABOUT MODEL HERE>\n+\n+            This model was contributed by [INSERT YOUR HF USERNAME HERE](https://huggingface.co/<INSERT YOUR HF USERNAME HERE>).\n+            The original code can be found [here](<INSERT LINK TO GITHUB REPO HERE>).\n+\n+            ## Usage examples\n+\n+            <INSERT SOME NICE EXAMPLES HERE>\n+\n+            ## MyTestConfig\n+\n+            [[autodoc]] MyTestConfig\n+\n+            ## MyTestForCausalLM\n+\n+            [[autodoc]] MyTestForCausalLM\n+\n+            ## MyTestModel\n+\n+            [[autodoc]] MyTestModel\n+                - forward\n+\n+            ## MyTestPreTrainedModel\n+\n+            [[autodoc]] MyTestPreTrainedModel\n+                - forward\n+\n+            ## MyTestForSequenceClassification\n+\n+            [[autodoc]] MyTestForSequenceClassification\n+\n+            ## MyTestForQuestionAnswering\n+\n+            [[autodoc]] MyTestForQuestionAnswering\n+\n+            ## MyTestForTokenClassification\n+\n+            [[autodoc]] MyTestForTokenClassification\n+            \"\"\"\n         )\n-        with self.assertRaises(ValueError):\n-            get_module_from_file(\"/models/gpt2/modeling_gpt2.py\")\n-\n-    def test_duplicate_module(self):\n-        bert_model_patterns = ModelPatterns(\"Bert\", \"google-bert/bert-base-cased\")\n-        new_bert_model_patterns = ModelPatterns(\"New Bert\", \"huggingface/bert-new-base\")\n-        bert_test = '''class TFBertPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n-    config_class = BertConfig\n-    load_tf_weights = load_tf_weights_in_bert\n-    base_model_prefix = \"bert\"\n-    is_parallelizable = True\n-    supports_gradient_checkpointing = True\n-\n-BERT_CONSTANT = \"value\"\n-'''\n-        bert_expected = '''class TFNewBertPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n-    config_class = NewBertConfig\n-    load_tf_weights = load_tf_weights_in_new_bert\n-    base_model_prefix = \"new_bert\"\n-    is_parallelizable = True\n-    supports_gradient_checkpointing = True\n-\n-NEW_BERT_CONSTANT = \"value\"\n-'''\n-        bert_expected_with_copied_from = (\n-            \"# Copied from transformers.bert_module.TFBertPreTrainedModel with Bert->NewBert,bert->new_bert\\n\"\n-            + bert_expected\n+        self.assertFileIsEqual(EXPECTED_DOC, os.path.join(self.DOC_PATH, \"model_doc\", \"my_test.md\"))\n+\n+    def test_phi4_with_all_processors(self):\n+        # This is the structure without adding the tokenizers\n+        filenames_to_add = (\n+            (\"configuration_phi4_multimodal.py\", True),\n+            (\"modeling_phi4_multimodal.py\", True),\n+            (\"tokenization_phi4_multimodal.py\", False),\n+            (\"tokenization_phi4_multimodal_fast.py\", False),\n+            (\"image_processing_phi4_multimodal.py\", False),\n+            (\"image_processing_phi4_multimodal_fast.py\", True),\n+            (\"video_processing_phi4_multimodal.py\", False),\n+            (\"feature_extraction_phi4_multimodal.py\", True),\n+            (\"processing_phi4_multimodal.py\", True),\n         )\n-        with tempfile.TemporaryDirectory() as tmp_dir:\n-            work_dir = os.path.join(tmp_dir, \"transformers\")\n-            os.makedirs(work_dir)\n-            file_name = os.path.join(work_dir, \"bert_module.py\")\n-            dest_file_name = os.path.join(work_dir, \"new_bert_module.py\")\n-\n-            self.init_file(file_name, bert_test)\n-            duplicate_module(file_name, bert_model_patterns, new_bert_model_patterns)\n-            self.check_result(dest_file_name, bert_expected_with_copied_from)\n-\n-            self.init_file(file_name, bert_test)\n-            duplicate_module(file_name, bert_model_patterns, new_bert_model_patterns, add_copied_from=False)\n-            self.check_result(dest_file_name, bert_expected)\n-\n-    def test_duplicate_module_with_copied_from(self):\n-        bert_model_patterns = ModelPatterns(\"Bert\", \"google-bert/bert-base-cased\")\n-        new_bert_model_patterns = ModelPatterns(\"New Bert\", \"huggingface/bert-new-base\")\n-        bert_test = '''# Copied from transformers.models.xxx.XxxModel with Xxx->Bert\n-class TFBertPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n-    config_class = BertConfig\n-    load_tf_weights = load_tf_weights_in_bert\n-    base_model_prefix = \"bert\"\n-    is_parallelizable = True\n-    supports_gradient_checkpointing = True\n-\n-BERT_CONSTANT = \"value\"\n-'''\n-        bert_expected = '''# Copied from transformers.models.xxx.XxxModel with Xxx->NewBert\n-class TFNewBertPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n-    config_class = NewBertConfig\n-    load_tf_weights = load_tf_weights_in_new_bert\n-    base_model_prefix = \"new_bert\"\n-    is_parallelizable = True\n-    supports_gradient_checkpointing = True\n-\n-NEW_BERT_CONSTANT = \"value\"\n-'''\n-        with tempfile.TemporaryDirectory() as tmp_dir:\n-            work_dir = os.path.join(tmp_dir, \"transformers\")\n-            os.makedirs(work_dir)\n-            file_name = os.path.join(work_dir, \"bert_module.py\")\n-            dest_file_name = os.path.join(work_dir, \"new_bert_module.py\")\n-\n-            self.init_file(file_name, bert_test)\n-            duplicate_module(file_name, bert_model_patterns, new_bert_model_patterns)\n-            # There should not be a new Copied from statement, the old one should be adapted.\n-            self.check_result(dest_file_name, bert_expected)\n-\n-            self.init_file(file_name, bert_test)\n-            duplicate_module(file_name, bert_model_patterns, new_bert_model_patterns, add_copied_from=False)\n-            self.check_result(dest_file_name, bert_expected)\n-\n-    def test_filter_framework_files(self):\n-        files = [\"modeling_bert.py\", \"modeling_tf_bert.py\", \"modeling_flax_bert.py\", \"configuration_bert.py\"]\n-        self.assertEqual(set(filter_framework_files(files, [\"pt\", \"tf\", \"flax\"])), set(files))\n-\n-        self.assertEqual(set(filter_framework_files(files, [\"pt\"])), {\"modeling_bert.py\", \"configuration_bert.py\"})\n-        self.assertEqual(set(filter_framework_files(files, [\"tf\"])), {\"modeling_tf_bert.py\", \"configuration_bert.py\"})\n-        self.assertEqual(\n-            set(filter_framework_files(files, [\"flax\"])), {\"modeling_flax_bert.py\", \"configuration_bert.py\"}\n+        # Run the command\n+        create_new_model_like(\n+            old_model_infos=ModelInfos(\"phi4_multimodal\"),\n+            new_lowercase_name=\"my_test2\",\n+            new_model_paper_name=\"MyTest2\",\n+            filenames_to_add=filenames_to_add,\n+            create_fast_image_processor=False,\n         )\n \n-        self.assertEqual(\n-            set(filter_framework_files(files, [\"pt\", \"tf\"])),\n-            {\"modeling_tf_bert.py\", \"modeling_bert.py\", \"configuration_bert.py\"},\n+        # First assert that all files were created correctly\n+        model_repo = os.path.join(self.MODEL_PATH, \"my_test2\")\n+        tests_repo = os.path.join(self.TESTS_MODEL_PATH, \"my_test2\")\n+        self.assertTrue(os.path.isfile(os.path.join(model_repo, \"modular_my_test2.py\")))\n+        self.assertTrue(os.path.isfile(os.path.join(model_repo, \"modeling_my_test2.py\")))\n+        self.assertTrue(os.path.isfile(os.path.join(model_repo, \"configuration_my_test2.py\")))\n+        self.assertTrue(os.path.isfile(os.path.join(model_repo, \"image_processing_my_test2_fast.py\")))\n+        self.assertTrue(os.path.isfile(os.path.join(model_repo, \"feature_extraction_my_test2.py\")))\n+        self.assertTrue(os.path.isfile(os.path.join(model_repo, \"processing_my_test2.py\")))\n+        self.assertTrue(os.path.isfile(os.path.join(model_repo, \"__init__.py\")))\n+        self.assertTrue(os.path.isfile(os.path.join(self.DOC_PATH, \"model_doc\", \"my_test2.md\")))\n+        self.assertTrue(os.path.isfile(os.path.join(tests_repo, \"__init__.py\")))\n+        self.assertTrue(os.path.isfile(os.path.join(tests_repo, \"test_modeling_my_test2.py\")))\n+        self.assertTrue(os.path.isfile(os.path.join(tests_repo, \"test_feature_extraction_my_test2.py\")))\n+        self.assertTrue(os.path.isfile(os.path.join(tests_repo, \"test_image_processing_my_test2.py\")))\n+\n+        # Now assert the correct imports/auto mappings/toctree were added\n+        self.assertInFile(\n+            \"from .my_test2 import *\\n\",\n+            os.path.join(self.MODEL_PATH, \"__init__.py\"),\n         )\n-        self.assertEqual(\n-            set(filter_framework_files(files, [\"tf\", \"flax\"])),\n-            {\"modeling_tf_bert.py\", \"modeling_flax_bert.py\", \"configuration_bert.py\"},\n+        self.assertInFile(\n+            '(\"my_test2\", \"MyTest2Config\"),\\n',\n+            os.path.join(self.MODEL_PATH, \"auto\", \"configuration_auto.py\"),\n         )\n-        self.assertEqual(\n-            set(filter_framework_files(files, [\"pt\", \"flax\"])),\n-            {\"modeling_bert.py\", \"modeling_flax_bert.py\", \"configuration_bert.py\"},\n+        self.assertInFile(\n+            '(\"my_test2\", \"MyTest2\"),\\n',\n+            os.path.join(self.MODEL_PATH, \"auto\", \"configuration_auto.py\"),\n+        )\n+        self.assertInFile(\n+            '(\"my_test2\", \"MyTest2Model\"),\\n',\n+            os.path.join(self.MODEL_PATH, \"auto\", \"modeling_auto.py\"),\n+        )\n+        self.assertInFile(\n+            '(\"my_test2\", \"MyTest2ForCausalLM\"),\\n',\n+            os.path.join(self.MODEL_PATH, \"auto\", \"modeling_auto.py\"),\n+        )\n+        self.assertInFile(\n+            '(\"my_test2\", (None, \"MyTest2ImageProcessorFast\")),\\n',\n+            os.path.join(self.MODEL_PATH, \"auto\", \"image_processing_auto.py\"),\n+        )\n+        self.assertInFile(\n+            '(\"my_test2\", \"MyTest2FeatureExtractor\"),\\n',\n+            os.path.join(self.MODEL_PATH, \"auto\", \"feature_extraction_auto.py\"),\n+        )\n+        self.assertInFile(\n+            '(\"my_test2\", \"MyTest2Processor\"),\\n',\n+            os.path.join(self.MODEL_PATH, \"auto\", \"processing_auto.py\"),\n+        )\n+        self.assertInFile(\n+            \"- local: model_doc/my_test2\\n        title: MyTest2\\n\",\n+            os.path.join(self.DOC_PATH, \"_toctree.yml\"),\n         )\n \n-    def test_get_model_files_only_pt(self):\n-        # BERT\n-        bert_files = get_model_files(\"bert\", frameworks=[\"pt\"])\n-\n-        doc_file = get_last_n_components_of_path(bert_files[\"doc_file\"], n=5)\n-        self.assertEqual(doc_file, \"docs/source/en/model_doc/bert.md\")\n-\n-        model_files = {get_last_n_components_of_path(f, n=4) for f in bert_files[\"model_files\"]}\n-        bert_model_files = BERT_MODEL_FILES - {\n-            \"transformers/models/bert/modeling_tf_bert.py\",\n-            \"transformers/models/bert/modeling_flax_bert.py\",\n-        }\n-        self.assertEqual(model_files, bert_model_files)\n-\n-        self.assertEqual(bert_files[\"module_name\"], \"bert\")\n-\n-        # TODO: failing in CI, fix me\n-        # test_files = {get_last_n_components_of_path(f, n=4) for f in bert_files[\"test_files\"]}\n-        # bert_test_files = {\n-        #     \"tests/models/bert/test_tokenization_bert.py\",\n-        #     \"tests/models/bert/test_modeling_bert.py\",\n-        # }\n-        # self.assertEqual(test_files, bert_test_files)\n-\n-        # VIT\n-        vit_files = get_model_files(\"vit\", frameworks=[\"pt\"])\n-        doc_file = get_last_n_components_of_path(vit_files[\"doc_file\"], n=5)\n-        self.assertEqual(doc_file, \"docs/source/en/model_doc/vit.md\")\n-\n-        model_files = {get_last_n_components_of_path(f, n=4) for f in vit_files[\"model_files\"]}\n-        vit_model_files = VIT_MODEL_FILES - {\n-            \"transformers/models/vit/modeling_tf_vit.py\",\n-            \"transformers/models/vit/modeling_flax_vit.py\",\n-        }\n-        self.assertEqual(model_files, vit_model_files)\n-\n-        self.assertEqual(vit_files[\"module_name\"], \"vit\")\n-\n-        # TODO: failing in CI, fix me\n-        # test_files = {get_last_n_components_of_path(f, n=4) for f in vit_files[\"test_files\"]}\n-        # vit_test_files = {\n-        #     \"tests/models/vit/test_image_processing_vit.py\",\n-        #     \"tests/models/vit/test_modeling_vit.py\",\n-        # }\n-        # self.assertEqual(test_files, vit_test_files)\n-\n-        # Wav2Vec2\n-        wav2vec2_files = get_model_files(\"wav2vec2\", frameworks=[\"pt\"])\n-        doc_file = get_last_n_components_of_path(wav2vec2_files[\"doc_file\"], n=5)\n-        self.assertEqual(doc_file, \"docs/source/en/model_doc/wav2vec2.md\")\n-\n-        model_files = {get_last_n_components_of_path(f, n=4) for f in wav2vec2_files[\"model_files\"]}\n-        wav2vec2_model_files = WAV2VEC2_MODEL_FILES - {\n-            \"transformers/models/wav2vec2/modeling_tf_wav2vec2.py\",\n-            \"transformers/models/wav2vec2/modeling_flax_wav2vec2.py\",\n-        }\n-        self.assertEqual(model_files, wav2vec2_model_files)\n-\n-        self.assertEqual(wav2vec2_files[\"module_name\"], \"wav2vec2\")\n-\n-        # TODO: failing in CI, fix me\n-        # test_files = {get_last_n_components_of_path(f, n=4) for f in wav2vec2_files[\"test_files\"]}\n-        # wav2vec2_test_files = {\n-        #     \"tests/models/wav2vec2/test_feature_extraction_wav2vec2.py\",\n-        #     \"tests/models/wav2vec2/test_modeling_wav2vec2.py\",\n-        #     \"tests/models/wav2vec2/test_processor_wav2vec2.py\",\n-        #     \"tests/models/wav2vec2/test_tokenization_wav2vec2.py\",\n-        # }\n-        # self.assertEqual(test_files, wav2vec2_test_files)\n-\n-    def test_find_base_model_checkpoint(self):\n-        self.assertEqual(find_base_model_checkpoint(\"bert\"), \"google-bert/bert-base-uncased\")\n-        self.assertEqual(find_base_model_checkpoint(\"gpt2\"), \"openai-community/gpt2\")\n-\n-    def test_retrieve_model_classes(self):\n-        gpt_classes = {k: set(v) for k, v in retrieve_model_classes(\"gpt2\", frameworks=[\"pt\"]).items()}\n-        expected_gpt_classes = {\n-            \"pt\": {\n-                \"GPT2ForTokenClassification\",\n-                \"GPT2Model\",\n-                \"GPT2LMHeadModel\",\n-                \"GPT2ForSequenceClassification\",\n-                \"GPT2ForQuestionAnswering\",\n-            },\n-        }\n-        self.assertEqual(gpt_classes, expected_gpt_classes)\n-\n-    def test_retrieve_info_for_model_with_bert(self):\n-        bert_info = retrieve_info_for_model(\"bert\", frameworks=[\"pt\"])\n-        bert_classes = [\n-            \"BertForTokenClassification\",\n-            \"BertForQuestionAnswering\",\n-            \"BertForNextSentencePrediction\",\n-            \"BertForSequenceClassification\",\n-            \"BertForMaskedLM\",\n-            \"BertForMultipleChoice\",\n-            \"BertModel\",\n-            \"BertForPreTraining\",\n-            \"BertLMHeadModel\",\n-        ]\n-        expected_model_classes = {\n-            \"pt\": set(bert_classes),\n-        }\n-\n-        self.assertEqual(set(bert_info[\"frameworks\"]), {\"pt\"})\n-        model_classes = {k: set(v) for k, v in bert_info[\"model_classes\"].items()}\n-        self.assertEqual(model_classes, expected_model_classes)\n-\n-        all_bert_files = bert_info[\"model_files\"]\n-        model_files = {get_last_n_components_of_path(f, 4) for f in all_bert_files[\"model_files\"]}\n-        bert_model_files = BERT_MODEL_FILES - {\n-            \"transformers/models/bert/modeling_tf_bert.py\",\n-            \"transformers/models/bert/modeling_flax_bert.py\",\n-        }\n-        self.assertEqual(model_files, bert_model_files)\n-\n-        # TODO: failing in CI, fix me\n-        # test_files = {get_last_n_components_of_path(f, n=4) for f in all_bert_files[\"test_files\"]}\n-        # bert_test_files = {\n-        #     \"tests/models/bert/test_tokenization_bert.py\",\n-        #     \"tests/models/bert/test_modeling_bert.py\",\n-        # }\n-        # self.assertEqual(test_files, bert_test_files)\n-\n-        doc_file = get_last_n_components_of_path(all_bert_files[\"doc_file\"], n=5)\n-        self.assertEqual(doc_file, \"docs/source/en/model_doc/bert.md\")\n-\n-        self.assertEqual(all_bert_files[\"module_name\"], \"bert\")\n-\n-        bert_model_patterns = bert_info[\"model_patterns\"]\n-        self.assertEqual(bert_model_patterns.model_name, \"BERT\")\n-        self.assertEqual(bert_model_patterns.checkpoint, \"google-bert/bert-base-uncased\")\n-        self.assertEqual(bert_model_patterns.model_type, \"bert\")\n-        self.assertEqual(bert_model_patterns.model_lower_cased, \"bert\")\n-        self.assertEqual(bert_model_patterns.model_camel_cased, \"Bert\")\n-        self.assertEqual(bert_model_patterns.model_upper_cased, \"BERT\")\n-        self.assertEqual(bert_model_patterns.config_class, \"BertConfig\")\n-        self.assertEqual(bert_model_patterns.tokenizer_class, \"BertTokenizer\")\n-        self.assertIsNone(bert_model_patterns.feature_extractor_class)\n-        self.assertIsNone(bert_model_patterns.processor_class)\n-\n-    def test_retrieve_info_for_model_with_vit(self):\n-        vit_info = retrieve_info_for_model(\"vit\", frameworks=[\"pt\"])\n-        vit_classes = [\"ViTForImageClassification\", \"ViTModel\"]\n-        pt_only_classes = [\"ViTForMaskedImageModeling\"]\n-        expected_model_classes = {\n-            \"pt\": set(vit_classes + pt_only_classes),\n-        }\n-\n-        self.assertEqual(set(vit_info[\"frameworks\"]), {\"pt\"})\n-        model_classes = {k: set(v) for k, v in vit_info[\"model_classes\"].items()}\n-        self.assertEqual(model_classes, expected_model_classes)\n-\n-        all_vit_files = vit_info[\"model_files\"]\n-        model_files = {get_last_n_components_of_path(f, 4) for f in all_vit_files[\"model_files\"]}\n-        vit_model_files = VIT_MODEL_FILES - {\n-            \"transformers/models/vit/modeling_tf_vit.py\",\n-            \"transformers/models/vit/modeling_flax_vit.py\",\n-        }\n-        self.assertEqual(model_files, vit_model_files)\n-\n-        # TODO: failing in CI, fix me\n-        # test_files = {get_last_n_components_of_path(f, n=4) for f in all_vit_files[\"test_files\"]}\n-        # vit_test_files = {\n-        #     \"tests/models/vit/test_image_processing_vit.py\",\n-        #     \"tests/models/vit/test_modeling_vit.py\",\n-        # }\n-        # self.assertEqual(test_files, vit_test_files)\n-\n-        doc_file = get_last_n_components_of_path(all_vit_files[\"doc_file\"], n=5)\n-        self.assertEqual(doc_file, \"docs/source/en/model_doc/vit.md\")\n-\n-        self.assertEqual(all_vit_files[\"module_name\"], \"vit\")\n-\n-        vit_model_patterns = vit_info[\"model_patterns\"]\n-        self.assertEqual(vit_model_patterns.model_name, \"ViT\")\n-        self.assertEqual(vit_model_patterns.checkpoint, \"google/vit-base-patch16-224\")\n-        self.assertEqual(vit_model_patterns.model_type, \"vit\")\n-        self.assertEqual(vit_model_patterns.model_lower_cased, \"vit\")\n-        self.assertEqual(vit_model_patterns.model_camel_cased, \"ViT\")\n-        self.assertEqual(vit_model_patterns.model_upper_cased, \"VIT\")\n-        self.assertEqual(vit_model_patterns.config_class, \"ViTConfig\")\n-        self.assertEqual(vit_model_patterns.feature_extractor_class, \"ViTFeatureExtractor\")\n-        self.assertEqual(vit_model_patterns.image_processor_class, \"ViTImageProcessor\")\n-        self.assertIsNone(vit_model_patterns.tokenizer_class)\n-        self.assertIsNone(vit_model_patterns.processor_class)\n-\n-    def test_retrieve_info_for_model_with_wav2vec2(self):\n-        wav2vec2_info = retrieve_info_for_model(\"wav2vec2\", frameworks=[\"pt\"])\n-        wav2vec2_classes = [\n-            \"Wav2Vec2Model\",\n-            \"Wav2Vec2ForPreTraining\",\n-            \"Wav2Vec2ForAudioFrameClassification\",\n-            \"Wav2Vec2ForCTC\",\n-            \"Wav2Vec2ForMaskedLM\",\n-            \"Wav2Vec2ForSequenceClassification\",\n-            \"Wav2Vec2ForXVector\",\n-        ]\n-        expected_model_classes = {\n-            \"pt\": set(wav2vec2_classes),\n-        }\n-\n-        self.assertEqual(set(wav2vec2_info[\"frameworks\"]), {\"pt\"})\n-        model_classes = {k: set(v) for k, v in wav2vec2_info[\"model_classes\"].items()}\n-        self.assertEqual(model_classes, expected_model_classes)\n-\n-        all_wav2vec2_files = wav2vec2_info[\"model_files\"]\n-        model_files = {get_last_n_components_of_path(f, 4) for f in all_wav2vec2_files[\"model_files\"]}\n-        wav2vec2_model_files = WAV2VEC2_MODEL_FILES - {\n-            \"transformers/models/wav2vec2/modeling_tf_wav2vec2.py\",\n-            \"transformers/models/wav2vec2/modeling_flax_wav2vec2.py\",\n-        }\n-        self.assertEqual(model_files, wav2vec2_model_files)\n-\n-        # TODO: failing in CI, fix me\n-        # test_files = {get_last_n_components_of_path(f, n=4) for f in all_wav2vec2_files[\"test_files\"]}\n-        # wav2vec2_test_files = {\n-        #     \"tests/models/wav2vec2/test_feature_extraction_wav2vec2.py\",\n-        #     \"tests/models/wav2vec2/test_modeling_wav2vec2.py\",\n-        #     \"tests/models/wav2vec2/test_processor_wav2vec2.py\",\n-        #     \"tests/models/wav2vec2/test_tokenization_wav2vec2.py\",\n-        # }\n-        # self.assertEqual(test_files, wav2vec2_test_files)\n-\n-        doc_file = get_last_n_components_of_path(all_wav2vec2_files[\"doc_file\"], n=5)\n-        self.assertEqual(doc_file, \"docs/source/en/model_doc/wav2vec2.md\")\n-\n-        self.assertEqual(all_wav2vec2_files[\"module_name\"], \"wav2vec2\")\n-\n-        wav2vec2_model_patterns = wav2vec2_info[\"model_patterns\"]\n-        self.assertEqual(wav2vec2_model_patterns.model_name, \"Wav2Vec2\")\n-        self.assertEqual(wav2vec2_model_patterns.checkpoint, \"facebook/wav2vec2-base-960h\")\n-        self.assertEqual(wav2vec2_model_patterns.model_type, \"wav2vec2\")\n-        self.assertEqual(wav2vec2_model_patterns.model_lower_cased, \"wav2vec2\")\n-        self.assertEqual(wav2vec2_model_patterns.model_camel_cased, \"Wav2Vec2\")\n-        self.assertEqual(wav2vec2_model_patterns.model_upper_cased, \"WAV2VEC2\")\n-        self.assertEqual(wav2vec2_model_patterns.config_class, \"Wav2Vec2Config\")\n-        self.assertEqual(wav2vec2_model_patterns.feature_extractor_class, \"Wav2Vec2FeatureExtractor\")\n-        self.assertEqual(wav2vec2_model_patterns.processor_class, \"Wav2Vec2Processor\")\n-        self.assertEqual(wav2vec2_model_patterns.tokenizer_class, \"Wav2Vec2CTCTokenizer\")\n-\n-    def test_clean_frameworks_in_init_with_gpt(self):\n-        test_init = \"\"\"\n-from typing import TYPE_CHECKING\n-\n-from ...utils import _LazyModule, is_flax_available, is_tf_available, is_tokenizers_available, is_torch_available\n-\n-_import_structure = {\n-    \"configuration_gpt2\": [\"GPT2Config\", \"GPT2OnnxConfig\"],\n-    \"tokenization_gpt2\": [\"GPT2Tokenizer\"],\n-}\n-\n-try:\n-    if not is_tokenizers_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_gpt2_fast\"] = [\"GPT2TokenizerFast\"]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_gpt2\"] = [\"GPT2Model\"]\n-\n-try:\n-    if not is_tf_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_tf_gpt2\"] = [\"TFGPT2Model\"]\n-\n-try:\n-    if not is_flax_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_flax_gpt2\"] = [\"FlaxGPT2Model\"]\n-\n-if TYPE_CHECKING:\n-    from .configuration_gpt2 import GPT2Config, GPT2OnnxConfig\n-    from .tokenization_gpt2 import GPT2Tokenizer\n-\n-    try:\n-        if not is_tokenizers_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_gpt2_fast import GPT2TokenizerFast\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_gpt2 import GPT2Model\n-\n-    try:\n-        if not is_tf_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_tf_gpt2 import TFGPT2Model\n-\n-    try:\n-        if not is_flax_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_flax_gpt2 import FlaxGPT2Model\n-\n-else:\n-    import sys\n-\n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure)\n-\"\"\"\n-\n-        init_pt_only = \"\"\"\n-from typing import TYPE_CHECKING\n-\n-from ...utils import _LazyModule, is_tokenizers_available, is_torch_available\n-\n-_import_structure = {\n-    \"configuration_gpt2\": [\"GPT2Config\", \"GPT2OnnxConfig\"],\n-    \"tokenization_gpt2\": [\"GPT2Tokenizer\"],\n-}\n-\n-try:\n-    if not is_tokenizers_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_gpt2_fast\"] = [\"GPT2TokenizerFast\"]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_gpt2\"] = [\"GPT2Model\"]\n-\n-if TYPE_CHECKING:\n-    from .configuration_gpt2 import GPT2Config, GPT2OnnxConfig\n-    from .tokenization_gpt2 import GPT2Tokenizer\n-\n-    try:\n-        if not is_tokenizers_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_gpt2_fast import GPT2TokenizerFast\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_gpt2 import GPT2Model\n-\n-else:\n-    import sys\n-\n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure)\n-\"\"\"\n-\n-        init_pt_only_no_tokenizer = \"\"\"\n-from typing import TYPE_CHECKING\n-\n-from ...utils import _LazyModule, is_torch_available\n-\n-_import_structure = {\n-    \"configuration_gpt2\": [\"GPT2Config\", \"GPT2OnnxConfig\"],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_gpt2\"] = [\"GPT2Model\"]\n-\n-if TYPE_CHECKING:\n-    from .configuration_gpt2 import GPT2Config, GPT2OnnxConfig\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_gpt2 import GPT2Model\n-\n-else:\n-    import sys\n-\n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure)\n-\"\"\"\n-\n-        with tempfile.TemporaryDirectory() as tmp_dir:\n-            file_name = os.path.join(tmp_dir, \"../__init__.py\")\n-\n-            self.init_file(file_name, test_init)\n-            clean_frameworks_in_init(file_name, frameworks=[\"pt\"])\n-            self.check_result(file_name, init_pt_only)\n-\n-            self.init_file(file_name, test_init)\n-            clean_frameworks_in_init(file_name, frameworks=[\"pt\"], keep_processing=False)\n-            self.check_result(file_name, init_pt_only_no_tokenizer)\n-\n-    def test_clean_frameworks_in_init_with_vit(self):\n-        test_init = \"\"\"\n-from typing import TYPE_CHECKING\n-\n-from ...utils import _LazyModule, is_flax_available, is_tf_available, is_torch_available, is_vision_available\n-\n-_import_structure = {\n-    \"configuration_vit\": [\"ViTConfig\"],\n-}\n-\n-try:\n-    if not is_vision_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"image_processing_vit\"] = [\"ViTImageProcessor\"]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_vit\"] = [\"ViTModel\"]\n-\n-try:\n-    if not is_tf_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_tf_vit\"] = [\"TFViTModel\"]\n-\n-try:\n-    if not is_flax_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_flax_vit\"] = [\"FlaxViTModel\"]\n-\n-if TYPE_CHECKING:\n-    from .configuration_vit import ViTConfig\n-\n-    try:\n-        if not is_vision_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .image_processing_vit import ViTImageProcessor\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_vit import ViTModel\n-\n-    try:\n-        if not is_tf_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_tf_vit import TFViTModel\n-\n-    try:\n-        if not is_flax_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_flax_vit import FlaxViTModel\n-\n-else:\n-    import sys\n-\n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure)\n-\"\"\"\n-\n-        init_pt_only = \"\"\"\n-from typing import TYPE_CHECKING\n-\n-from ...utils import _LazyModule, is_torch_available, is_vision_available\n-\n-_import_structure = {\n-    \"configuration_vit\": [\"ViTConfig\"],\n-}\n-\n-try:\n-    if not is_vision_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"image_processing_vit\"] = [\"ViTImageProcessor\"]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_vit\"] = [\"ViTModel\"]\n-\n-if TYPE_CHECKING:\n-    from .configuration_vit import ViTConfig\n-\n-    try:\n-        if not is_vision_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .image_processing_vit import ViTImageProcessor\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_vit import ViTModel\n+        # Check some exact file creation. For model definition, only check modular as modeling/config/etc... are created\n+        # directly from it\n+        EXPECTED_MODULAR = textwrap.dedent(\n+            f\"\"\"\n+            # coding=utf-8\n+            # Copyright {CURRENT_YEAR} the HuggingFace Team. All rights reserved.\n+            #\n+            # Licensed under the Apache License, Version 2.0 (the \"License\");\n+            # you may not use this file except in compliance with the License.\n+            # You may obtain a copy of the License at\n+            #\n+            #     http://www.apache.org/licenses/LICENSE-2.0\n+            #\n+            # Unless required by applicable law or agreed to in writing, software\n+            # distributed under the License is distributed on an \"AS IS\" BASIS,\n+            # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+            # See the License for the specific language governing permissions and\n+            # limitations under the License.\n+\n+            from ..phi4_multimodal.configuration_phi4_multimodal import (\n+                Phi4MultimodalAudioConfig,\n+                Phi4MultimodalConfig,\n+                Phi4MultimodalVisionConfig,\n+            )\n+            from ..phi4_multimodal.feature_extraction_phi4_multimodal import Phi4MultimodalFeatureExtractor\n+            from ..phi4_multimodal.image_processing_phi4_multimodal_fast import (\n+                Phi4MultimodalFastImageProcessorKwargs,\n+                Phi4MultimodalImageProcessorFast,\n+            )\n+            from ..phi4_multimodal.modeling_phi4_multimodal import (\n+                Phi4MultimodalAttention,\n+                Phi4MultimodalAudioAttention,\n+                Phi4MultimodalAudioConformerEncoderLayer,\n+                Phi4MultimodalAudioConvModule,\n+                Phi4MultimodalAudioDepthWiseSeperableConv1d,\n+                Phi4MultimodalAudioEmbedding,\n+                Phi4MultimodalAudioGluPointWiseConv,\n+                Phi4MultimodalAudioMeanVarianceNormLayer,\n+                Phi4MultimodalAudioMLP,\n+                Phi4MultimodalAudioModel,\n+                Phi4MultimodalAudioNemoConvSubsampling,\n+                Phi4MultimodalAudioPreTrainedModel,\n+                Phi4MultimodalAudioRelativeAttentionBias,\n+                Phi4MultimodalDecoderLayer,\n+                Phi4MultimodalFeatureEmbedding,\n+                Phi4MultimodalForCausalLM,\n+                Phi4MultimodalImageEmbedding,\n+                Phi4MultimodalMLP,\n+                Phi4MultimodalModel,\n+                Phi4MultimodalPreTrainedModel,\n+                Phi4MultimodalRMSNorm,\n+                Phi4MultimodalRotaryEmbedding,\n+                Phi4MultimodalVisionAttention,\n+                Phi4MultimodalVisionEmbeddings,\n+                Phi4MultimodalVisionEncoder,\n+                Phi4MultimodalVisionEncoderLayer,\n+                Phi4MultimodalVisionMLP,\n+                Phi4MultimodalVisionModel,\n+                Phi4MultimodalVisionMultiheadAttentionPoolingHead,\n+                Phi4MultimodalVisionPreTrainedModel,\n+            )\n+            from ..phi4_multimodal.processing_phi4_multimodal import Phi4MultimodalProcessor, Phi4MultimodalProcessorKwargs\n \n-else:\n-    import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure)\n-\"\"\"\n+            class MyTest2VisionConfig(Phi4MultimodalVisionConfig):\n+                pass\n \n-        init_pt_only_no_feature_extractor = \"\"\"\n-from typing import TYPE_CHECKING\n \n-from ...utils import _LazyModule, is_torch_available\n+            class MyTest2AudioConfig(Phi4MultimodalAudioConfig):\n+                pass\n \n-_import_structure = {\n-    \"configuration_vit\": [\"ViTConfig\"],\n-}\n \n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_vit\"] = [\"ViTModel\"]\n+            class MyTest2Config(Phi4MultimodalConfig):\n+                pass\n \n-if TYPE_CHECKING:\n-    from .configuration_vit import ViTConfig\n \n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_vit import ViTModel\n+            class MyTest2VisionMLP(Phi4MultimodalVisionMLP):\n+                pass\n \n-else:\n-    import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure)\n-\"\"\"\n+            class MyTest2VisionAttention(Phi4MultimodalVisionAttention):\n+                pass\n \n-        with tempfile.TemporaryDirectory() as tmp_dir:\n-            file_name = os.path.join(tmp_dir, \"../__init__.py\")\n \n-            self.init_file(file_name, test_init)\n-            clean_frameworks_in_init(file_name, frameworks=[\"pt\"])\n-            self.check_result(file_name, init_pt_only)\n+            class MyTest2VisionEncoderLayer(Phi4MultimodalVisionEncoderLayer):\n+                pass\n \n-            self.init_file(file_name, test_init)\n-            clean_frameworks_in_init(file_name, frameworks=[\"pt\"], keep_processing=False)\n-            self.check_result(file_name, init_pt_only_no_feature_extractor)\n \n-    def test_duplicate_doc_file(self):\n-        test_doc = \"\"\"\n-# GPT2\n+            class MyTest2VisionEncoder(Phi4MultimodalVisionEncoder):\n+                pass\n \n-## Overview\n \n-Overview of the model.\n+            class MyTest2VisionPreTrainedModel(Phi4MultimodalVisionPreTrainedModel):\n+                pass\n \n-## GPT2Config\n \n-[[autodoc]] GPT2Config\n+            class MyTest2VisionEmbeddings(Phi4MultimodalVisionEmbeddings):\n+                pass\n \n-## GPT2Tokenizer\n \n-[[autodoc]] GPT2Tokenizer\n-    - save_vocabulary\n+            class MyTest2VisionMultiheadAttentionPoolingHead(Phi4MultimodalVisionMultiheadAttentionPoolingHead):\n+                pass\n \n-## GPT2TokenizerFast\n \n-[[autodoc]] GPT2TokenizerFast\n+            class MyTest2VisionModel(Phi4MultimodalVisionModel):\n+                pass\n \n-## GPT2 specific outputs\n \n-[[autodoc]] models.gpt2.modeling_gpt2.GPT2DoubleHeadsModelOutput\n+            class MyTest2ImageEmbedding(Phi4MultimodalImageEmbedding):\n+                pass\n \n-[[autodoc]] models.gpt2.modeling_tf_gpt2.TFGPT2DoubleHeadsModelOutput\n \n-## GPT2Model\n+            class MyTest2AudioMLP(Phi4MultimodalAudioMLP):\n+                pass\n \n-[[autodoc]] GPT2Model\n-    - forward\n \n-## TFGPT2Model\n+            class MyTest2AudioAttention(Phi4MultimodalAudioAttention):\n+                pass\n \n-[[autodoc]] TFGPT2Model\n-    - call\n \n-## FlaxGPT2Model\n+            class MyTest2AudioDepthWiseSeperableConv1d(Phi4MultimodalAudioDepthWiseSeperableConv1d):\n+                pass\n \n-[[autodoc]] FlaxGPT2Model\n-    - __call__\n \n-\"\"\"\n-        test_new_doc = \"\"\"\n-# GPT-New New\n+            class MyTest2AudioGluPointWiseConv(Phi4MultimodalAudioGluPointWiseConv):\n+                pass\n \n-## Overview\n \n-The GPT-New New model was proposed in [<INSERT PAPER NAME HERE>](<INSERT PAPER LINK HERE>) by <INSERT AUTHORS HERE>.\n-<INSERT SHORT SUMMARY HERE>\n+            class MyTest2AudioConvModule(Phi4MultimodalAudioConvModule):\n+                pass\n \n-The abstract from the paper is the following:\n \n-*<INSERT PAPER ABSTRACT HERE>*\n+            class MyTest2AudioConformerEncoderLayer(Phi4MultimodalAudioConformerEncoderLayer):\n+                pass\n \n-Tips:\n \n-<INSERT TIPS ABOUT MODEL HERE>\n+            class MyTest2AudioNemoConvSubsampling(Phi4MultimodalAudioNemoConvSubsampling):\n+                pass\n \n-This model was contributed by [INSERT YOUR HF USERNAME HERE](https://huggingface.co/<INSERT YOUR HF USERNAME HERE>).\n-The original code can be found [here](<INSERT LINK TO GITHUB REPO HERE>).\n \n+            class MyTest2AudioRelativeAttentionBias(Phi4MultimodalAudioRelativeAttentionBias):\n+                pass\n \n-## GPTNewNewConfig\n \n-[[autodoc]] GPTNewNewConfig\n+            class MyTest2AudioMeanVarianceNormLayer(Phi4MultimodalAudioMeanVarianceNormLayer):\n+                pass\n \n-## GPTNewNewTokenizer\n \n-[[autodoc]] GPTNewNewTokenizer\n-    - save_vocabulary\n+            class MyTest2AudioPreTrainedModel(Phi4MultimodalAudioPreTrainedModel):\n+                pass\n \n-## GPTNewNewTokenizerFast\n \n-[[autodoc]] GPTNewNewTokenizerFast\n+            class MyTest2AudioModel(Phi4MultimodalAudioModel):\n+                pass\n \n-## GPTNewNew specific outputs\n \n-[[autodoc]] models.gpt_new_new.modeling_gpt_new_new.GPTNewNewDoubleHeadsModelOutput\n+            class MyTest2AudioEmbedding(Phi4MultimodalAudioEmbedding):\n+                pass\n \n-[[autodoc]] models.gpt_new_new.modeling_tf_gpt_new_new.TFGPTNewNewDoubleHeadsModelOutput\n \n-## GPTNewNewModel\n+            class MyTest2RMSNorm(Phi4MultimodalRMSNorm):\n+                pass\n \n-[[autodoc]] GPTNewNewModel\n-    - forward\n \n-## TFGPTNewNewModel\n+            class MyTest2MLP(Phi4MultimodalMLP):\n+                pass\n \n-[[autodoc]] TFGPTNewNewModel\n-    - call\n \n-## FlaxGPTNewNewModel\n+            class MyTest2Attention(Phi4MultimodalAttention):\n+                pass\n \n-[[autodoc]] FlaxGPTNewNewModel\n-    - __call__\n \n-\"\"\"\n+            class MyTest2DecoderLayer(Phi4MultimodalDecoderLayer):\n+                pass\n \n-        with tempfile.TemporaryDirectory() as tmp_dir:\n-            doc_file = os.path.join(tmp_dir, \"gpt2.md\")\n-            new_doc_file = os.path.join(tmp_dir, \"gpt-new-new.md\")\n \n-            gpt2_model_patterns = ModelPatterns(\"GPT2\", \"gpt2\", tokenizer_class=\"GPT2Tokenizer\")\n-            new_model_patterns = ModelPatterns(\n-                \"GPT-New New\", \"huggingface/gpt-new-new\", tokenizer_class=\"GPTNewNewTokenizer\"\n-            )\n+            class MyTest2FeatureEmbedding(Phi4MultimodalFeatureEmbedding):\n+                pass\n \n-            self.init_file(doc_file, test_doc)\n-            duplicate_doc_file(doc_file, gpt2_model_patterns, new_model_patterns, frameworks=[\"pt\", \"tf\", \"flax\"])\n-            self.check_result(new_doc_file, test_new_doc)\n \n-            test_new_doc_pt_only = test_new_doc.replace(\n-                \"\"\"\n-## TFGPTNewNewModel\n+            class MyTest2RotaryEmbedding(Phi4MultimodalRotaryEmbedding):\n+                pass\n \n-[[autodoc]] TFGPTNewNewModel\n-    - call\n \n-## FlaxGPTNewNewModel\n+            class MyTest2PreTrainedModel(Phi4MultimodalPreTrainedModel):\n+                pass\n \n-[[autodoc]] FlaxGPTNewNewModel\n-    - __call__\n \n-\"\"\",\n-                \"\",\n-            )\n-            self.init_file(doc_file, test_doc)\n-            duplicate_doc_file(doc_file, gpt2_model_patterns, new_model_patterns, frameworks=[\"pt\"])\n-            self.check_result(new_doc_file, test_new_doc_pt_only)\n+            class MyTest2Model(Phi4MultimodalModel):\n+                pass\n \n-            test_new_doc_no_tok = test_new_doc.replace(\n-                \"\"\"\n-## GPTNewNewTokenizer\n \n-[[autodoc]] GPTNewNewTokenizer\n-    - save_vocabulary\n+            class MyTest2ForCausalLM(Phi4MultimodalForCausalLM):\n+                pass\n \n-## GPTNewNewTokenizerFast\n \n-[[autodoc]] GPTNewNewTokenizerFast\n-\"\"\",\n-                \"\",\n-            )\n-            new_model_patterns = ModelPatterns(\n-                \"GPT-New New\", \"huggingface/gpt-new-new\", tokenizer_class=\"GPT2Tokenizer\"\n-            )\n-            self.init_file(doc_file, test_doc)\n-            duplicate_doc_file(doc_file, gpt2_model_patterns, new_model_patterns, frameworks=[\"pt\", \"tf\", \"flax\"])\n-            print(test_new_doc_no_tok)\n-            self.check_result(new_doc_file, test_new_doc_no_tok)\n+            class MyTest2FastImageProcessorKwargs(Phi4MultimodalFastImageProcessorKwargs):\n+                pass\n \n-            test_new_doc_pt_only_no_tok = test_new_doc_no_tok.replace(\n-                \"\"\"\n-## TFGPTNewNewModel\n \n-[[autodoc]] TFGPTNewNewModel\n-    - call\n+            class MyTest2ImageProcessorFast(Phi4MultimodalImageProcessorFast):\n+                pass\n \n-## FlaxGPTNewNewModel\n \n-[[autodoc]] FlaxGPTNewNewModel\n-    - __call__\n+            class MyTest2FeatureExtractor(Phi4MultimodalFeatureExtractor):\n+                pass\n \n-\"\"\",\n-                \"\",\n-            )\n-            self.init_file(doc_file, test_doc)\n-            duplicate_doc_file(doc_file, gpt2_model_patterns, new_model_patterns, frameworks=[\"pt\"])\n-            self.check_result(new_doc_file, test_new_doc_pt_only_no_tok)\n+\n+            class MyTest2ProcessorKwargs(Phi4MultimodalProcessorKwargs):\n+                pass\n+\n+\n+            class MyTest2Processor(Phi4MultimodalProcessor):\n+                pass\n+\n+\n+            __all__ = [\n+                \"MyTest2VisionConfig\",\n+                \"MyTest2AudioConfig\",\n+                \"MyTest2Config\",\n+                \"MyTest2AudioPreTrainedModel\",\n+                \"MyTest2AudioModel\",\n+                \"MyTest2VisionPreTrainedModel\",\n+                \"MyTest2VisionModel\",\n+                \"MyTest2PreTrainedModel\",\n+                \"MyTest2Model\",\n+                \"MyTest2ForCausalLM\",\n+                \"MyTest2ImageProcessorFast\",\n+                \"MyTest2FeatureExtractor\",\n+                \"MyTest2Processor\",\n+            ]\n+            \"\"\"\n+        )\n+        self.assertFileIsEqual(EXPECTED_MODULAR, os.path.join(model_repo, \"modular_my_test2.py\"))\n+\n+        EXPECTED_INIT = textwrap.dedent(\n+            f\"\"\"\n+            # coding=utf-8\n+            # Copyright {CURRENT_YEAR} the HuggingFace Team. All rights reserved.\n+            #\n+            # Licensed under the Apache License, Version 2.0 (the \"License\");\n+            # you may not use this file except in compliance with the License.\n+            # You may obtain a copy of the License at\n+            #\n+            #     http://www.apache.org/licenses/LICENSE-2.0\n+            #\n+            # Unless required by applicable law or agreed to in writing, software\n+            # distributed under the License is distributed on an \"AS IS\" BASIS,\n+            # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+            # See the License for the specific language governing permissions and\n+            # limitations under the License.\n+\n+            from typing import TYPE_CHECKING\n+\n+            from ...utils import _LazyModule\n+            from ...utils.import_utils import define_import_structure\n+\n+\n+            if TYPE_CHECKING:\n+                from .configuration_my_test2 import *\n+                from .feature_extraction_my_test2 import *\n+                from .image_processing_my_test2_fast import *\n+                from .modeling_my_test2 import *\n+                from .processing_my_test2 import *\n+            else:\n+                import sys\n+\n+                _file = globals()[\"__file__\"]\n+                sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)\n+            \"\"\"\n+        )\n+        self.assertFileIsEqual(EXPECTED_INIT, os.path.join(model_repo, \"__init__.py\"))\n+\n+        EXPECTED_DOC = textwrap.dedent(\n+            f\"\"\"\n+            <!--Copyright {CURRENT_YEAR} the HuggingFace Team. All rights reserved.\n+\n+            Licensed under the Apache License, Version 2.0 (the \"License\");\n+            you may not use this file except in compliance with the License.\n+            You may obtain a copy of the License at\n+\n+                http://www.apache.org/licenses/LICENSE-2.0\n+\n+            Unless required by applicable law or agreed to in writing, software\n+            distributed under the License is distributed on an \"AS IS\" BASIS,\n+            WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+            See the License for the specific language governing permissions and\n+            limitations under the License.\n+\n+\n+             Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be rendered properly in your Markdown viewer.\n+\n+            -->\n+\n+\n+            # MyTest2\n+\n+            ## Overview\n+\n+            The MyTest2 model was proposed in [<INSERT PAPER NAME HERE>](<INSERT PAPER LINK HERE>) by <INSERT AUTHORS HERE>.\n+            <INSERT SHORT SUMMARY HERE>\n+\n+            The abstract from the paper is the following:\n+\n+            <INSERT PAPER ABSTRACT HERE>\n+\n+            Tips:\n+\n+            <INSERT TIPS ABOUT MODEL HERE>\n+\n+            This model was contributed by [INSERT YOUR HF USERNAME HERE](https://huggingface.co/<INSERT YOUR HF USERNAME HERE>).\n+            The original code can be found [here](<INSERT LINK TO GITHUB REPO HERE>).\n+\n+            ## Usage examples\n+\n+            <INSERT SOME NICE EXAMPLES HERE>\n+\n+            ## MyTest2VisionConfig\n+\n+            [[autodoc]] MyTest2VisionConfig\n+\n+            ## MyTest2AudioConfig\n+\n+            [[autodoc]] MyTest2AudioConfig\n+\n+            ## MyTest2Config\n+\n+            [[autodoc]] MyTest2Config\n+\n+            ## MyTest2AudioPreTrainedModel\n+\n+            [[autodoc]] MyTest2AudioPreTrainedModel\n+                - forward\n+\n+            ## MyTest2AudioModel\n+\n+            [[autodoc]] MyTest2AudioModel\n+                - forward\n+\n+            ## MyTest2VisionPreTrainedModel\n+\n+            [[autodoc]] MyTest2VisionPreTrainedModel\n+                - forward\n+\n+            ## MyTest2VisionModel\n+\n+            [[autodoc]] MyTest2VisionModel\n+                - forward\n+\n+            ## MyTest2PreTrainedModel\n+\n+            [[autodoc]] MyTest2PreTrainedModel\n+                - forward\n+\n+            ## MyTest2Model\n+\n+            [[autodoc]] MyTest2Model\n+                - forward\n+\n+            ## MyTest2ForCausalLM\n+\n+            [[autodoc]] MyTest2ForCausalLM\n+\n+            ## MyTest2ImageProcessorFast\n+\n+            [[autodoc]] MyTest2ImageProcessorFast\n+\n+            ## MyTest2FeatureExtractor\n+\n+            [[autodoc]] MyTest2FeatureExtractor\n+\n+            ## MyTest2Processor\n+\n+            [[autodoc]] MyTest2Processor\n+            \"\"\"\n+        )\n+        self.assertFileIsEqual(EXPECTED_DOC, os.path.join(self.DOC_PATH, \"model_doc\", \"my_test2.md\"))"
        },
        {
            "sha": "fd553cc3b9260126f78372b9df4cbdb0fb45c396",
            "filename": "utils/modular_model_converter.py",
            "status": "modified",
            "additions": 11,
            "deletions": 1,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/380b2a031761e444a169f4d3379b30b718c16df6/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/380b2a031761e444a169f4d3379b30b718c16df6/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=380b2a031761e444a169f4d3379b30b718c16df6",
            "patch": "@@ -17,12 +17,12 @@\n import importlib\n import os\n import re\n+import subprocess\n from abc import ABC, abstractmethod\n from collections import Counter, defaultdict, deque\n from typing import Optional, Union\n \n import libcst as cst\n-from check_copies import run_ruff\n from create_dependency_mapping import find_priority_list\n from libcst import ClassDef, CSTVisitor\n from libcst import matchers as m\n@@ -1676,6 +1676,16 @@ def create_modules(modular_mapper: ModularFileMapper) -> dict[str, cst.Module]:\n     return files\n \n \n+def run_ruff(code, check=False):\n+    if check:\n+        command = [\"ruff\", \"check\", \"-\", \"--fix\", \"--exit-zero\"]\n+    else:\n+        command = [\"ruff\", \"format\", \"-\", \"--config\", \"pyproject.toml\", \"--silent\"]\n+    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE)\n+    stdout, _ = process.communicate(input=code.encode())\n+    return stdout.decode()\n+\n+\n def convert_modular_file(modular_file):\n     pattern = re.search(r\"modular_(.*)(?=\\.py$)\", modular_file)\n     output = {}"
        }
    ],
    "stats": {
        "total": 4135,
        "additions": 1352,
        "deletions": 2783
    }
}