{
    "author": "gante",
    "message": "Forbid `PretrainedConfig` from saving `generate` parameters; Update deprecations in `generate`-related code ðŸ§¹  (#32659)\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>",
    "sha": "970a16ec7f693104085fd826523e5c6ce64f2040",
    "files": [
        {
            "sha": "936e4bfb95dad54f8ecab4ed535e2e65529acd0c",
            "filename": "docs/source/en/internal/generation_utils.md",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -140,9 +140,6 @@ generation.\n [[autodoc]] ForcedEOSTokenLogitsProcessor\n     - __call__\n \n-[[autodoc]] ForceTokensLogitsProcessor\n-    - __call__\n-\n [[autodoc]] HammingDiversityLogitsProcessor\n     - __call__\n "
        },
        {
            "sha": "1a5cc1dec079584f471ff3179c868971957ac855",
            "filename": "docs/source/ja/internal/generation_utils.md",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/docs%2Fsource%2Fja%2Finternal%2Fgeneration_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/docs%2Fsource%2Fja%2Finternal%2Fgeneration_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Finternal%2Fgeneration_utils.md?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -139,9 +139,6 @@ generation_output[:2]\n [[autodoc]] ForcedEOSTokenLogitsProcessor\n     - __call__\n \n-[[autodoc]] ForceTokensLogitsProcessor\n-    - __call__\n-\n [[autodoc]] HammingDiversityLogitsProcessor\n     - __call__\n "
        },
        {
            "sha": "084e2a29dc8cfca962b42eeca26e7de33d9f2e28",
            "filename": "docs/source/zh/internal/generation_utils.md",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/docs%2Fsource%2Fzh%2Finternal%2Fgeneration_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/docs%2Fsource%2Fzh%2Finternal%2Fgeneration_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Finternal%2Fgeneration_utils.md?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -133,9 +133,6 @@ generation_output[:2]\n [[autodoc]] ForcedEOSTokenLogitsProcessor\n     - __call__\n \n-[[autodoc]] ForceTokensLogitsProcessor\n-    - __call__\n-\n [[autodoc]] HammingDiversityLogitsProcessor\n     - __call__\n "
        },
        {
            "sha": "ced2b9997366e692dfd9f86ad656d36d4eeb5d26",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -1276,7 +1276,6 @@\n             \"ExponentialDecayLengthPenalty\",\n             \"ForcedBOSTokenLogitsProcessor\",\n             \"ForcedEOSTokenLogitsProcessor\",\n-            \"ForceTokensLogitsProcessor\",\n             \"GenerationMixin\",\n             \"HammingDiversityLogitsProcessor\",\n             \"InfNanRemoveLogitsProcessor\",\n@@ -6059,7 +6058,6 @@\n             ExponentialDecayLengthPenalty,\n             ForcedBOSTokenLogitsProcessor,\n             ForcedEOSTokenLogitsProcessor,\n-            ForceTokensLogitsProcessor,\n             GenerationMixin,\n             HammingDiversityLogitsProcessor,\n             InfNanRemoveLogitsProcessor,"
        },
        {
            "sha": "ad0dbd14e15b5611bbf00d7e04cbb67e77af716c",
            "filename": "src/transformers/commands/pt_to_tf.py",
            "status": "modified",
            "additions": 4,
            "deletions": 299,
            "changes": 303,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fcommands%2Fpt_to_tf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fcommands%2Fpt_to_tf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fpt_to_tf.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -12,45 +12,13 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import inspect\n import os\n from argparse import ArgumentParser, Namespace\n-from importlib import import_module\n \n-import huggingface_hub\n-import numpy as np\n-from packaging import version\n-\n-from .. import (\n-    FEATURE_EXTRACTOR_MAPPING,\n-    IMAGE_PROCESSOR_MAPPING,\n-    PROCESSOR_MAPPING,\n-    TOKENIZER_MAPPING,\n-    AutoConfig,\n-    AutoFeatureExtractor,\n-    AutoImageProcessor,\n-    AutoProcessor,\n-    AutoTokenizer,\n-    is_datasets_available,\n-    is_tf_available,\n-    is_torch_available,\n-)\n-from ..utils import TF2_WEIGHTS_INDEX_NAME, TF2_WEIGHTS_NAME, logging\n+from ..utils import logging\n from . import BaseTransformersCLICommand\n \n \n-if is_tf_available():\n-    import tensorflow as tf\n-\n-    tf.config.experimental.enable_tensor_float_32_execution(False)\n-\n-if is_torch_available():\n-    import torch\n-\n-if is_datasets_available():\n-    from datasets import load_dataset\n-\n-\n MAX_ERROR = 5e-5  # larger error tolerance than in our internal tests, to avoid flaky user-facing errors\n \n \n@@ -136,44 +104,6 @@ def register_subcommand(parser: ArgumentParser):\n         )\n         train_parser.set_defaults(func=convert_command_factory)\n \n-    @staticmethod\n-    def find_pt_tf_differences(pt_outputs, tf_outputs):\n-        \"\"\"\n-        Compares the TensorFlow and PyTorch outputs, returning a dictionary with all tensor differences.\n-        \"\"\"\n-        # 1. All output attributes must be the same\n-        pt_out_attrs = set(pt_outputs.keys())\n-        tf_out_attrs = set(tf_outputs.keys())\n-        if pt_out_attrs != tf_out_attrs:\n-            raise ValueError(\n-                f\"The model outputs have different attributes, aborting. (Pytorch: {pt_out_attrs}, TensorFlow:\"\n-                f\" {tf_out_attrs})\"\n-            )\n-\n-        # 2. For each output attribute, computes the difference\n-        def _find_pt_tf_differences(pt_out, tf_out, differences, attr_name=\"\"):\n-            # If the current attribute is a tensor, it is a leaf and we make the comparison. Otherwise, we will dig in\n-            # recursivelly, keeping the name of the attribute.\n-            if isinstance(pt_out, torch.Tensor):\n-                tensor_difference = np.max(np.abs(pt_out.numpy() - tf_out.numpy()))\n-                differences[attr_name] = tensor_difference\n-            else:\n-                root_name = attr_name\n-                for i, pt_item in enumerate(pt_out):\n-                    # If it is a named attribute, we keep the name. Otherwise, just its index.\n-                    if isinstance(pt_item, str):\n-                        branch_name = root_name + pt_item\n-                        tf_item = tf_out[pt_item]\n-                        pt_item = pt_out[pt_item]\n-                    else:\n-                        branch_name = root_name + f\"[{i}]\"\n-                        tf_item = tf_out[i]\n-                    differences = _find_pt_tf_differences(pt_item, tf_item, differences, branch_name)\n-\n-            return differences\n-\n-        return _find_pt_tf_differences(pt_outputs, tf_outputs, {})\n-\n     def __init__(\n         self,\n         model_name: str,\n@@ -196,237 +126,12 @@ def __init__(\n         self._extra_commit_description = extra_commit_description\n         self._override_model_class = override_model_class\n \n-    def get_inputs(self, pt_model, tf_dummy_inputs, config):\n-        \"\"\"\n-        Returns the right inputs for the model, based on its signature.\n-        \"\"\"\n-\n-        def _get_audio_input():\n-            ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n-            speech_samples = ds.sort(\"id\").select(range(2))[:2][\"audio\"]\n-            raw_samples = [x[\"array\"] for x in speech_samples]\n-            return raw_samples\n-\n-        model_config_class = type(pt_model.config)\n-        if model_config_class in PROCESSOR_MAPPING:\n-            processor = AutoProcessor.from_pretrained(self._local_dir)\n-            if model_config_class in TOKENIZER_MAPPING and processor.tokenizer.pad_token is None:\n-                processor.tokenizer.pad_token = processor.tokenizer.eos_token\n-        elif model_config_class in IMAGE_PROCESSOR_MAPPING:\n-            processor = AutoImageProcessor.from_pretrained(self._local_dir)\n-        elif model_config_class in FEATURE_EXTRACTOR_MAPPING:\n-            processor = AutoFeatureExtractor.from_pretrained(self._local_dir)\n-        elif model_config_class in TOKENIZER_MAPPING:\n-            processor = AutoTokenizer.from_pretrained(self._local_dir)\n-            if processor.pad_token is None:\n-                processor.pad_token = processor.eos_token\n-        else:\n-            raise ValueError(f\"Unknown data processing type (model config type: {model_config_class})\")\n-\n-        model_forward_signature = set(inspect.signature(pt_model.forward).parameters.keys())\n-        processor_inputs = {}\n-        if \"input_ids\" in model_forward_signature:\n-            processor_inputs.update(\n-                {\n-                    \"text\": [\"Hi there!\", \"I am a batch with more than one row and different input lengths.\"],\n-                    \"padding\": True,\n-                    \"truncation\": True,\n-                }\n-            )\n-        if \"pixel_values\" in model_forward_signature:\n-            sample_images = load_dataset(\"uoft-cs/cifar10\", \"plain_text\", split=\"test\")[:2][\"img\"]  # no-script\n-            processor_inputs.update({\"images\": sample_images})\n-        if \"input_features\" in model_forward_signature:\n-            feature_extractor_signature = inspect.signature(processor.feature_extractor).parameters\n-            # Pad to the largest input length by default but take feature extractor default\n-            # padding value if it exists e.g. \"max_length\" and is not False or None\n-            if \"padding\" in feature_extractor_signature:\n-                default_strategy = feature_extractor_signature[\"padding\"].default\n-                if default_strategy is not False and default_strategy is not None:\n-                    padding_strategy = default_strategy\n-                else:\n-                    padding_strategy = True\n-            else:\n-                padding_strategy = True\n-            processor_inputs.update({\"audio\": _get_audio_input(), \"padding\": padding_strategy})\n-        if \"input_values\" in model_forward_signature:  # Wav2Vec2 audio input\n-            processor_inputs.update({\"audio\": _get_audio_input(), \"padding\": True})\n-        pt_input = processor(**processor_inputs, return_tensors=\"pt\")\n-        tf_input = processor(**processor_inputs, return_tensors=\"tf\")\n-\n-        # Extra input requirements, in addition to the input modality\n-        if (\n-            config.is_encoder_decoder\n-            or (hasattr(pt_model, \"encoder\") and hasattr(pt_model, \"decoder\"))\n-            or \"decoder_input_ids\" in tf_dummy_inputs\n-        ):\n-            decoder_input_ids = np.asarray([[1], [1]], dtype=int) * (pt_model.config.decoder_start_token_id or 0)\n-            pt_input.update({\"decoder_input_ids\": torch.tensor(decoder_input_ids)})\n-            tf_input.update({\"decoder_input_ids\": tf.convert_to_tensor(decoder_input_ids)})\n-\n-        return pt_input, tf_input\n-\n     def run(self):\n-        self._logger.warning(\n-            \"\\n\\nConverting PyTorch weights to TensorFlow is deprecated and will be removed in v4.43. \"\n+        # TODO (joao): delete file in v4.47\n+        raise NotImplementedError(\n+            \"\\n\\nConverting PyTorch weights to TensorFlow weights was removed in v4.43. \"\n             \"Instead, we recommend that you convert PyTorch weights to Safetensors, an improved \"\n             \"format that can be loaded by any framework, including TensorFlow. For more information, \"\n             \"please see the Safetensors conversion guide: \"\n             \"https://huggingface.co/docs/safetensors/en/convert-weights\\n\\n\"\n         )\n-        # hub version 0.9.0 introduced the possibility of programmatically opening PRs with normal write tokens.\n-        if version.parse(huggingface_hub.__version__) < version.parse(\"0.9.0\"):\n-            raise ImportError(\n-                \"The huggingface_hub version must be >= 0.9.0 to use this command. Please update your huggingface_hub\"\n-                \" installation.\"\n-            )\n-        else:\n-            from huggingface_hub import Repository, create_commit\n-            from huggingface_hub._commit_api import CommitOperationAdd\n-\n-        # Fetch remote data\n-        repo = Repository(local_dir=self._local_dir, clone_from=self._model_name)\n-\n-        # Load config and get the appropriate architecture -- the latter is needed to convert the head's weights\n-        config = AutoConfig.from_pretrained(self._local_dir)\n-        architectures = config.architectures\n-        if self._override_model_class is not None:\n-            if self._override_model_class.startswith(\"TF\"):\n-                architectures = [self._override_model_class[2:]]\n-            else:\n-                architectures = [self._override_model_class]\n-            try:\n-                pt_class = getattr(import_module(\"transformers\"), architectures[0])\n-            except AttributeError:\n-                raise ValueError(f\"Model class {self._override_model_class} not found in transformers.\")\n-            try:\n-                tf_class = getattr(import_module(\"transformers\"), \"TF\" + architectures[0])\n-            except AttributeError:\n-                raise ValueError(f\"TF model class TF{self._override_model_class} not found in transformers.\")\n-        elif architectures is None:  # No architecture defined -- use auto classes\n-            pt_class = getattr(import_module(\"transformers\"), \"AutoModel\")\n-            tf_class = getattr(import_module(\"transformers\"), \"TFAutoModel\")\n-            self._logger.warning(\"No detected architecture, using AutoModel/TFAutoModel\")\n-        else:  # Architecture defined -- use it\n-            if len(architectures) > 1:\n-                raise ValueError(f\"More than one architecture was found, aborting. (architectures = {architectures})\")\n-            self._logger.warning(f\"Detected architecture: {architectures[0]}\")\n-            pt_class = getattr(import_module(\"transformers\"), architectures[0])\n-            try:\n-                tf_class = getattr(import_module(\"transformers\"), \"TF\" + architectures[0])\n-            except AttributeError:\n-                raise AttributeError(f\"The TensorFlow equivalent of {architectures[0]} doesn't exist in transformers.\")\n-\n-        # Check the TF dummy inputs to see what keys we need in the forward pass\n-        tf_from_pt_model = tf_class.from_config(config)\n-        tf_dummy_inputs = tf_from_pt_model.dummy_inputs\n-\n-        del tf_from_pt_model  # Try to keep only one model in memory at a time\n-\n-        # Load the model and get some basic inputs\n-        pt_model = pt_class.from_pretrained(self._local_dir)\n-        pt_model.eval()\n-\n-        pt_input, tf_input = self.get_inputs(pt_model, tf_dummy_inputs, config)\n-\n-        with torch.no_grad():\n-            pt_outputs = pt_model(**pt_input, output_hidden_states=True)\n-        del pt_model  # will no longer be used, and may have a large memory footprint\n-\n-        tf_from_pt_model = tf_class.from_pretrained(self._local_dir, from_pt=True)\n-        tf_from_pt_outputs = tf_from_pt_model(**tf_input, output_hidden_states=True, training=False)\n-\n-        # Confirms that cross loading PT weights into TF worked.\n-        crossload_differences = self.find_pt_tf_differences(pt_outputs, tf_from_pt_outputs)\n-        output_differences = {k: v for k, v in crossload_differences.items() if \"hidden\" not in k}\n-        hidden_differences = {k: v for k, v in crossload_differences.items() if \"hidden\" in k}\n-        if len(output_differences) == 0 and architectures is not None:\n-            raise ValueError(\n-                f\"Something went wrong -- the config file has architectures ({architectures}), but no model head\"\n-                \" output was found. All outputs start with 'hidden'\"\n-            )\n-        max_crossload_output_diff = max(output_differences.values()) if output_differences else 0.0\n-        max_crossload_hidden_diff = max(hidden_differences.values())\n-        if max_crossload_output_diff > self._max_error or max_crossload_hidden_diff > self._max_error:\n-            raise ValueError(\n-                \"The cross-loaded TensorFlow model has different outputs, something went wrong!\\n\"\n-                + f\"\\nList of maximum output differences above the threshold ({self._max_error}):\\n\"\n-                + \"\\n\".join([f\"{k}: {v:.3e}\" for k, v in output_differences.items() if v > self._max_error])\n-                + f\"\\n\\nList of maximum hidden layer differences above the threshold ({self._max_error}):\\n\"\n-                + \"\\n\".join([f\"{k}: {v:.3e}\" for k, v in hidden_differences.items() if v > self._max_error])\n-            )\n-\n-        # Save the weights in a TF format (if needed) and confirms that the results are still good\n-        tf_weights_path = os.path.join(self._local_dir, TF2_WEIGHTS_NAME)\n-        tf_weights_index_path = os.path.join(self._local_dir, TF2_WEIGHTS_INDEX_NAME)\n-        if (not os.path.exists(tf_weights_path) and not os.path.exists(tf_weights_index_path)) or self._new_weights:\n-            tf_from_pt_model.save_pretrained(self._local_dir)\n-        del tf_from_pt_model  # will no longer be used, and may have a large memory footprint\n-\n-        tf_model = tf_class.from_pretrained(self._local_dir)\n-        tf_outputs = tf_model(**tf_input, output_hidden_states=True)\n-\n-        conversion_differences = self.find_pt_tf_differences(pt_outputs, tf_outputs)\n-        output_differences = {k: v for k, v in conversion_differences.items() if \"hidden\" not in k}\n-        hidden_differences = {k: v for k, v in conversion_differences.items() if \"hidden\" in k}\n-        if len(output_differences) == 0 and architectures is not None:\n-            raise ValueError(\n-                f\"Something went wrong -- the config file has architectures ({architectures}), but no model head\"\n-                \" output was found. All outputs start with 'hidden'\"\n-            )\n-        max_conversion_output_diff = max(output_differences.values()) if output_differences else 0.0\n-        max_conversion_hidden_diff = max(hidden_differences.values())\n-        if max_conversion_output_diff > self._max_error or max_conversion_hidden_diff > self._max_error:\n-            raise ValueError(\n-                \"The converted TensorFlow model has different outputs, something went wrong!\\n\"\n-                + f\"\\nList of maximum output differences above the threshold ({self._max_error}):\\n\"\n-                + \"\\n\".join([f\"{k}: {v:.3e}\" for k, v in output_differences.items() if v > self._max_error])\n-                + f\"\\n\\nList of maximum hidden layer differences above the threshold ({self._max_error}):\\n\"\n-                + \"\\n\".join([f\"{k}: {v:.3e}\" for k, v in hidden_differences.items() if v > self._max_error])\n-            )\n-\n-        commit_message = \"Update TF weights\" if self._new_weights else \"Add TF weights\"\n-        if self._push:\n-            repo.git_add(auto_lfs_track=True)\n-            repo.git_commit(commit_message)\n-            repo.git_push(blocking=True)  # this prints a progress bar with the upload\n-            self._logger.warning(f\"TF weights pushed into {self._model_name}\")\n-        elif not self._no_pr:\n-            self._logger.warning(\"Uploading the weights into a new PR...\")\n-            commit_descrition = (\n-                \"Model converted by the [`transformers`' `pt_to_tf`\"\n-                \" CLI](https://github.com/huggingface/transformers/blob/main/src/transformers/commands/pt_to_tf.py). \"\n-                \"All converted model outputs and hidden layers were validated against its PyTorch counterpart.\\n\\n\"\n-                f\"Maximum crossload output difference={max_crossload_output_diff:.3e}; \"\n-                f\"Maximum crossload hidden layer difference={max_crossload_hidden_diff:.3e};\\n\"\n-                f\"Maximum conversion output difference={max_conversion_output_diff:.3e}; \"\n-                f\"Maximum conversion hidden layer difference={max_conversion_hidden_diff:.3e};\\n\"\n-            )\n-            if self._max_error > MAX_ERROR:\n-                commit_descrition += (\n-                    f\"\\n\\nCAUTION: The maximum admissible error was manually increased to {self._max_error}!\"\n-                )\n-            if self._extra_commit_description:\n-                commit_descrition += \"\\n\\n\" + self._extra_commit_description\n-\n-            # sharded model -> adds all related files (index and .h5 shards)\n-            if os.path.exists(tf_weights_index_path):\n-                operations = [\n-                    CommitOperationAdd(path_in_repo=TF2_WEIGHTS_INDEX_NAME, path_or_fileobj=tf_weights_index_path)\n-                ]\n-                for shard_path in tf.io.gfile.glob(self._local_dir + \"/tf_model-*.h5\"):\n-                    operations += [\n-                        CommitOperationAdd(path_in_repo=os.path.basename(shard_path), path_or_fileobj=shard_path)\n-                    ]\n-            else:\n-                operations = [CommitOperationAdd(path_in_repo=TF2_WEIGHTS_NAME, path_or_fileobj=tf_weights_path)]\n-\n-            hub_pr_url = create_commit(\n-                repo_id=self._model_name,\n-                operations=operations,\n-                commit_message=commit_message,\n-                commit_description=commit_descrition,\n-                repo_type=\"model\",\n-                create_pr=True,\n-            ).pr_url\n-            self._logger.warning(f\"PR open in {hub_pr_url}\")"
        },
        {
            "sha": "c6e3d90b9f0ca0641ebb6eb7fb0663f15b872c58",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 58,
            "deletions": 88,
            "changes": 146,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -81,6 +81,15 @@ class PretrainedConfig(PushToHubMixin):\n       model.\n     - **num_hidden_layers** (`int`) -- The number of blocks in the model.\n \n+    <Tip warning={true}>\n+\n+    Setting parameters for sequence generation in the model config is deprecated. For backward compatibility, loading\n+    some of them will still be possible, but attempting to overwrite them will throw an exception -- you should set\n+    them in a [~transformers.GenerationConfig]. Check the documentation of [~transformers.GenerationConfig] for more\n+    information about the individual parameters.\n+\n+    </Tip>\n+\n     Arg:\n         name_or_path (`str`, *optional*, defaults to `\"\"`):\n             Store the string that was passed to [`PreTrainedModel.from_pretrained`] or\n@@ -117,77 +126,6 @@ class PretrainedConfig(PushToHubMixin):\n             sequence_length embeddings at a time. For more information on feed forward chunking, see [How does Feed\n             Forward Chunking work?](../glossary.html#feed-forward-chunking).\n \n-        > Parameters for sequence generation\n-\n-        max_length (`int`, *optional*, defaults to 20):\n-            Maximum length that will be used by default in the `generate` method of the model.\n-        min_length (`int`, *optional*, defaults to 0):\n-            Minimum length that will be used by default in the `generate` method of the model.\n-        do_sample (`bool`, *optional*, defaults to `False`):\n-            Flag that will be used by default in the `generate` method of the model. Whether or not to use sampling ;\n-            use greedy decoding otherwise.\n-        early_stopping (`bool`, *optional*, defaults to `False`):\n-            Flag that will be used by default in the `generate` method of the model. Whether to stop the beam search\n-            when at least `num_beams` sentences are finished per batch or not.\n-        num_beams (`int`, *optional*, defaults to 1):\n-            Number of beams for beam search that will be used by default in the `generate` method of the model. 1 means\n-            no beam search.\n-        num_beam_groups (`int`, *optional*, defaults to 1):\n-            Number of groups to divide `num_beams` into in order to ensure diversity among different groups of beams\n-            that will be used by default in the `generate` method of the model. 1 means no group beam search.\n-        diversity_penalty (`float`, *optional*, defaults to 0.0):\n-            Value to control diversity for group beam search. that will be used by default in the `generate` method of\n-            the model. 0 means no diversity penalty. The higher the penalty, the more diverse are the outputs.\n-        temperature (`float`, *optional*, defaults to 1.0):\n-            The value used to module the next token probabilities that will be used by default in the `generate` method\n-            of the model. Must be strictly positive.\n-        top_k (`int`, *optional*, defaults to 50):\n-            Number of highest probability vocabulary tokens to keep for top-k-filtering that will be used by default in\n-            the `generate` method of the model.\n-        top_p (`float`, *optional*, defaults to 1):\n-            Value that will be used by default in the `generate` method of the model for `top_p`. If set to float < 1,\n-            only the most probable tokens with probabilities that add up to `top_p` or higher are kept for generation.\n-        typical_p (`float`, *optional*, defaults to 1):\n-            Local typicality measures how similar the conditional probability of predicting a target token next is to\n-            the expected conditional probability of predicting a random token next, given the partial text already\n-            generated. If set to float < 1, the smallest set of the most locally typical tokens with probabilities that\n-            add up to `typical_p` or higher are kept for generation. See [this\n-            paper](https://arxiv.org/pdf/2202.00666.pdf) for more details.\n-        repetition_penalty (`float`, *optional*, defaults to 1):\n-            Parameter for repetition penalty that will be used by default in the `generate` method of the model. 1.0\n-            means no penalty.\n-        length_penalty (`float`, *optional*, defaults to 1):\n-            Exponential penalty to the length that is used with beam-based generation. It is applied as an exponent to\n-            the sequence length, which in turn is used to divide the score of the sequence. Since the score is the log\n-            likelihood of the sequence (i.e. negative), `length_penalty` > 0.0 promotes longer sequences, while\n-            `length_penalty` < 0.0 encourages shorter sequences.\n-        no_repeat_ngram_size (`int`, *optional*, defaults to 0) -- Value that will be used by default in the\n-            `generate` method of the model for `no_repeat_ngram_size`. If set to int > 0, all ngrams of that size can\n-            only occur once.\n-        encoder_no_repeat_ngram_size (`int`, *optional*, defaults to 0) -- Value that will be used by\n-            default in the `generate` method of the model for `encoder_no_repeat_ngram_size`. If set to int > 0, all\n-            ngrams of that size that occur in the `encoder_input_ids` cannot occur in the `decoder_input_ids`.\n-        bad_words_ids (`List[int]`, *optional*):\n-            List of token ids that are not allowed to be generated that will be used by default in the `generate`\n-            method of the model. In order to get the tokens of the words that should not appear in the generated text,\n-            use `tokenizer.encode(bad_word, add_prefix_space=True)`.\n-        num_return_sequences (`int`, *optional*, defaults to 1):\n-            Number of independently computed returned sequences for each element in the batch that will be used by\n-            default in the `generate` method of the model.\n-        output_scores (`bool`, *optional*, defaults to `False`):\n-            Whether the model should return the logits when used for generation.\n-        return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n-            Whether the model should return a [`~transformers.utils.ModelOutput`] instead of a `torch.LongTensor`.\n-        forced_bos_token_id (`int`, *optional*):\n-            The id of the token to force as the first generated token after the `decoder_start_token_id`. Useful for\n-            multilingual models like [mBART](../model_doc/mbart) where the first generated token needs to be the target\n-            language token.\n-        forced_eos_token_id (`int`, *optional*):\n-            The id of the token to force as the last generated token when `max_length` is reached.\n-        remove_invalid_values (`bool`, *optional*):\n-            Whether to remove possible _nan_ and _inf_ outputs of the model to prevent the generation method to crash.\n-            Note that using `remove_invalid_values` can slow down generation.\n-\n         > Parameters for fine-tuning tasks\n \n         architectures (`List[str]`, *optional*):\n@@ -287,7 +225,7 @@ def __init__(self, **kwargs):\n \n         # Retrocompatibility: Parameters for sequence generation. While we will keep the ability to load these\n         # parameters, saving them will be deprecated. In a distant future, we won't need to load them.\n-        for parameter_name, default_value in self._get_generation_defaults().items():\n+        for parameter_name, default_value in self._get_global_generation_defaults().items():\n             setattr(self, parameter_name, kwargs.pop(parameter_name, default_value))\n \n         # Fine-tuning task arguments\n@@ -440,16 +378,13 @@ def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub:\n         if os.path.isfile(save_directory):\n             raise AssertionError(f\"Provided path ({save_directory}) should be a directory, not a file\")\n \n-        non_default_generation_parameters = {}\n-        for parameter_name, default_value in self._get_generation_defaults().items():\n-            if hasattr(self, parameter_name) and getattr(self, parameter_name) != default_value:\n-                non_default_generation_parameters[parameter_name] = getattr(self, parameter_name)\n+        non_default_generation_parameters = self._get_non_default_generation_parameters()\n         if len(non_default_generation_parameters) > 0:\n-            logger.warning(\n-                \"Some non-default generation parameters are set in the model config. These should go into a \"\n-                \"GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) \"\n-                \"instead. This warning will be raised to an exception in v4.41.\\n\"\n-                f\"Non-default generation parameters: {str(non_default_generation_parameters)}\"\n+            raise ValueError(\n+                \"Some non-default generation parameters are set in the model config. These should go into either a) \"\n+                \"`model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file \"\n+                \"(https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) \"\n+                f\"\\nNon-default generation parameters: {str(non_default_generation_parameters)}\"\n             )\n \n         os.makedirs(save_directory, exist_ok=True)\n@@ -1049,7 +984,7 @@ def register_for_auto_class(cls, auto_class=\"AutoConfig\"):\n         cls._auto_class = auto_class\n \n     @staticmethod\n-    def _get_generation_defaults() -> Dict[str, Any]:\n+    def _get_global_generation_defaults() -> Dict[str, Any]:\n         return {\n             \"max_length\": 20,\n             \"min_length\": 0,\n@@ -1078,14 +1013,49 @@ def _get_generation_defaults() -> Dict[str, Any]:\n             \"begin_suppress_tokens\": None,\n         }\n \n-    def _has_non_default_generation_parameters(self) -> bool:\n+    def _get_non_default_generation_parameters(self) -> Dict[str, Any]:\n         \"\"\"\n-        Whether or not this instance holds non-default generation parameters.\n+        Gets the non-default generation parameters on the PretrainedConfig instance\n         \"\"\"\n-        for parameter_name, default_value in self._get_generation_defaults().items():\n-            if hasattr(self, parameter_name) and getattr(self, parameter_name) != default_value:\n-                return True\n-        return False\n+        non_default_generation_parameters = {}\n+        decoder_attribute_name = None\n+        default_config = None\n+\n+        # Composite models don't have a default config, use their decoder config as a fallback for default values\n+        # If no known pattern is matched, then `default_config = None` -> check against the global generation defaults\n+        try:\n+            default_config = self.__class__()\n+        except ValueError:\n+            for decoder_attribute_name in (\"decoder\", \"generator\", \"text_config\"):\n+                if hasattr(self, decoder_attribute_name):\n+                    default_config = getattr(self, decoder_attribute_name).__class__()\n+                    break\n+\n+        # If it is a composite model, we want to check the subconfig that will be used for generation\n+        self_decoder_config = self if decoder_attribute_name is None else getattr(self, decoder_attribute_name)\n+\n+        for parameter_name, default_global_value in self._get_global_generation_defaults().items():\n+            if hasattr(self_decoder_config, parameter_name):\n+                is_default_in_config = is_default_generation_value = None\n+                parameter_value = getattr(self_decoder_config, parameter_name)\n+                # Three cases in which is okay for the model config to hold generation config parameters:\n+                # 1. The parameter is set to `None`, effectivelly delegating its value to the generation config\n+                if parameter_value is None:\n+                    continue\n+                # 2. If we have a default config, then the instance should hold the same generation defaults\n+                if default_config is not None:\n+                    is_default_in_config = parameter_value == getattr(default_config, parameter_name)\n+                # 3. if we don't have a default config, then the instance should hold the global generation defaults\n+                else:\n+                    is_default_generation_value = parameter_value == default_global_value\n+\n+                is_non_default = (is_default_in_config is False) or (\n+                    is_default_in_config is None and is_default_generation_value is False\n+                )\n+                if is_non_default:\n+                    non_default_generation_parameters[parameter_name] = getattr(self_decoder_config, parameter_name)\n+\n+        return non_default_generation_parameters\n \n \n def get_configuration_file(configuration_files: List[str]) -> str:"
        },
        {
            "sha": "faf5266b84aea32ccef9342aedaa4b670ddce5b9",
            "filename": "src/transformers/generation/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fgeneration%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fgeneration%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2F__init__.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -55,7 +55,6 @@\n         \"ExponentialDecayLengthPenalty\",\n         \"ForcedBOSTokenLogitsProcessor\",\n         \"ForcedEOSTokenLogitsProcessor\",\n-        \"ForceTokensLogitsProcessor\",\n         \"HammingDiversityLogitsProcessor\",\n         \"InfNanRemoveLogitsProcessor\",\n         \"LogitNormalization\",\n@@ -201,7 +200,6 @@\n             ExponentialDecayLengthPenalty,\n             ForcedBOSTokenLogitsProcessor,\n             ForcedEOSTokenLogitsProcessor,\n-            ForceTokensLogitsProcessor,\n             HammingDiversityLogitsProcessor,\n             InfNanRemoveLogitsProcessor,\n             LogitNormalization,"
        },
        {
            "sha": "e9ba456068294a13031eaa7a0e50116d1c723ac9",
            "filename": "src/transformers/generation/logits_process.py",
            "status": "modified",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Flogits_process.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -15,7 +15,6 @@\n \n import inspect\n import math\n-import warnings\n from typing import Callable, Dict, Iterable, List, Optional, Tuple, Union\n \n import numpy as np\n@@ -1844,34 +1843,6 @@ def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> to\n         return scores\n \n \n-class ForceTokensLogitsProcessor(LogitsProcessor):\n-    r\"\"\"\n-    This processor takes a list of pairs of integers which indicates a mapping from generation indices to token\n-    indices that will be forced before generation. The processor will set their log probs to `inf` so that they are\n-    sampled at their corresponding index. Originally created for\n-    [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper).\n-    \"\"\"\n-\n-    def __init__(self, force_token_map: List[List[int]], _has_warned: Optional[bool] = False):\n-        self.force_token_map = dict(force_token_map)\n-        if not _has_warned:\n-            # TODO(Sanchit): remove this processor entirely in v4.40\n-            warnings.warn(\n-                \"This `ForceTokensLogitsProcessor` has been deprecated and will be removed in v4.40. Should you need to provide prompt ids for generation, specify `input_ids` to the generate method for decoder-only models, or `decoder_input_ids` for encoder-decoder models.\",\n-                FutureWarning,\n-            )\n-\n-    @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n-    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n-        generation_idx = input_ids.shape[-1]\n-        current_token = self.force_token_map.get(generation_idx, None)\n-        scores_processed = scores\n-        if current_token is not None:\n-            scores_processed = torch.full_like(scores, -float(\"inf\"))\n-            scores_processed[:, current_token] = 0\n-        return scores_processed\n-\n-\n class WhisperTimeStampLogitsProcessor(LogitsProcessor):\n     r\"\"\"\n "
        },
        {
            "sha": "961b6d6f5e43dbddc335e9530a520955b66aa6be",
            "filename": "src/transformers/generation/stopping_criteria.py",
            "status": "modified",
            "additions": 0,
            "deletions": 32,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fgeneration%2Fstopping_criteria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fgeneration%2Fstopping_criteria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fstopping_criteria.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -85,36 +85,6 @@ def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwa\n         return torch.full((input_ids.shape[0],), is_done, device=input_ids.device, dtype=torch.bool)\n \n \n-class MaxNewTokensCriteria(StoppingCriteria):\n-    \"\"\"\n-    This class can be used to stop generation whenever the generated number of tokens exceeds `max_new_tokens`. Keep in\n-    mind for decoder-only type of transformers, this will **not** include the initial prompted tokens. This is very\n-    close to `MaxLengthCriteria` but ignores the number of initial tokens.\n-\n-    Args:\n-        start_length (`int`):\n-            The number of initial tokens.\n-        max_new_tokens (`int`):\n-            The maximum number of tokens to generate.\n-    \"\"\"\n-\n-    def __init__(self, start_length: int, max_new_tokens: int):\n-        warnings.warn(\n-            \"The class `MaxNewTokensCriteria` is deprecated and will be removed in v4.43. \"\n-            f\"Please use `MaxLengthCriteria(max_length={start_length + max_new_tokens})` \"\n-            \"with `max_length = start_length + max_new_tokens` instead.\",\n-            FutureWarning,\n-        )\n-        self.start_length = start_length\n-        self.max_new_tokens = max_new_tokens\n-        self.max_length = start_length + max_new_tokens\n-\n-    @add_start_docstrings(STOPPING_CRITERIA_INPUTS_DOCSTRING)\n-    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.BoolTensor:\n-        is_done = input_ids.shape[-1] >= self.max_length\n-        return torch.full((input_ids.shape[0],), is_done, device=input_ids.device, dtype=torch.bool)\n-\n-\n class MaxTimeCriteria(StoppingCriteria):\n     \"\"\"\n     This class can be used to stop generation whenever the full generation exceeds some amount of time. By default, the\n@@ -516,8 +486,6 @@ def max_length(self) -> Optional[int]:\n         for stopping_criterium in self:\n             if isinstance(stopping_criterium, MaxLengthCriteria):\n                 return stopping_criterium.max_length\n-            elif isinstance(stopping_criterium, MaxNewTokensCriteria):\n-                return stopping_criterium.max_length\n         return None\n \n "
        },
        {
            "sha": "58824b7b0071b74dabd9b1e0345db20dcf8904d7",
            "filename": "src/transformers/generation/tf_logits_process.py",
            "status": "modified",
            "additions": 26,
            "deletions": 14,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fgeneration%2Ftf_logits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fgeneration%2Ftf_logits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Ftf_logits_process.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -520,15 +520,21 @@ def __init__(self, begin_suppress_tokens, begin_index):\n         self.begin_index = begin_index\n \n     def __call__(self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int) -> tf.Tensor:\n-        scores = tf.cond(\n-            tf.equal(cur_len, self.begin_index),\n-            lambda: tf.tensor_scatter_nd_update(\n-                scores,\n-                indices=[[i, token] for i in range(scores.shape[0]) for token in self.begin_suppress_tokens],\n-                updates=[-float(\"inf\") for _ in range(scores.shape[0] * len(self.begin_suppress_tokens))],\n-            ),\n-            lambda: scores,\n-        )\n+        suppressed_indices = []\n+        for token in self.begin_suppress_tokens:\n+            if token < scores.shape[-1]:  # to ensure we don't go beyond the vocab size\n+                suppressed_indices.extend([[i, token] for i in range(scores.shape[0])])\n+\n+        if len(suppressed_indices) > 0:\n+            scores = tf.cond(\n+                tf.equal(cur_len, self.begin_index),\n+                lambda: tf.tensor_scatter_nd_update(\n+                    scores,\n+                    indices=suppressed_indices,\n+                    updates=[-float(\"inf\") for _ in range(scores.shape[0] * len(self.begin_suppress_tokens))],\n+                ),\n+                lambda: scores,\n+            )\n         return scores\n \n \n@@ -540,11 +546,17 @@ def __init__(self, suppress_tokens):\n         self.suppress_tokens = list(suppress_tokens)\n \n     def __call__(self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int) -> tf.Tensor:\n-        scores = tf.tensor_scatter_nd_update(\n-            scores,\n-            indices=[[i, token] for i in range(scores.shape[0]) for token in self.suppress_tokens],\n-            updates=[-float(\"inf\") for _ in range(scores.shape[0] * len(self.suppress_tokens))],\n-        )\n+        suppressed_indices = []\n+        for token in self.suppress_tokens:\n+            if token < scores.shape[-1]:  # to ensure we don't go beyond the vocab size\n+                suppressed_indices.extend([[i, token] for i in range(scores.shape[0])])\n+\n+        if len(suppressed_indices) > 0:\n+            scores = tf.tensor_scatter_nd_update(\n+                scores,\n+                indices=[[i, token] for i in range(scores.shape[0]) for token in self.suppress_tokens],\n+                updates=[-float(\"inf\") for _ in range(scores.shape[0] * len(self.suppress_tokens))],\n+            )\n         return scores\n \n "
        },
        {
            "sha": "e3c70ac10985ccc54e0d55c3a84f22ebdfb0ca34",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 21,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -76,7 +76,6 @@\n     ExponentialDecayLengthPenalty,\n     ForcedBOSTokenLogitsProcessor,\n     ForcedEOSTokenLogitsProcessor,\n-    ForceTokensLogitsProcessor,\n     HammingDiversityLogitsProcessor,\n     InfNanRemoveLogitsProcessor,\n     LogitNormalization,\n@@ -865,9 +864,6 @@ def _get_logits_processor(\n                 if (input_ids_seq_length > 1 or generation_config.forced_bos_token_id is None)\n                 else begin_index + 1\n             )\n-            if generation_config.forced_decoder_ids is not None:\n-                # generation starts after the last token that is forced\n-                begin_index += generation_config.forced_decoder_ids[-1][0]\n             processors.append(\n                 SuppressTokensAtBeginLogitsProcessor(\n                     generation_config.begin_suppress_tokens,\n@@ -876,12 +872,11 @@ def _get_logits_processor(\n                 )\n             )\n         if generation_config.forced_decoder_ids is not None:\n-            # TODO(Sanchit): deprecate in v4.40 by removing this logic\n-            warnings.warn(\n-                \"You have explicitly specified `forced_decoder_ids`. This functionality has been deprecated and will throw an error in v4.40. Please remove the `forced_decoder_ids` argument in favour of `input_ids` or `decoder_input_ids` respectively.\",\n-                FutureWarning,\n+            # TODO (sanchit): move this exception to GenerationConfig.validate() when TF & FLAX are aligned with PT\n+            raise ValueError(\n+                \"You have explicitly specified `forced_decoder_ids`. Please remove the `forced_decoder_ids` argument \"\n+                \"in favour of `input_ids` or `decoder_input_ids` respectively.\",\n             )\n-            processors.append(ForceTokensLogitsProcessor(generation_config.forced_decoder_ids, _has_warned=True))\n         if generation_config.watermarking_config is not None:\n             processors.append(\n                 WatermarkLogitsProcessor(\n@@ -1344,19 +1339,18 @@ def _prepare_generation_config(\n         using_model_generation_config = False\n         if generation_config is None:\n             # legacy: users may modify the model configuration to control generation. To trigger this legacy behavior,\n-            # three conditions must be met\n+            # the following conditions must be met\n             # 1) the generation config must have been created from the model config (`_from_model_config` field);\n             # 2) the generation config must have seen no modification since its creation (the hash is the same);\n             # 3) the user must have set generation parameters in the model config.\n             # NOTE: `torch.compile` can't compile `hash`, this legacy support is disabled with compilation.\n             if (\n                 not is_torchdynamo_compiling()\n-                and self.generation_config._from_model_config\n-                and self.generation_config._original_object_hash == hash(self.generation_config)\n-                and self.config._has_non_default_generation_parameters()\n+                and self.generation_config._from_model_config  # 1)\n+                and self.generation_config._original_object_hash == hash(self.generation_config)  # 2)\n             ):\n                 new_generation_config = GenerationConfig.from_model_config(self.config)\n-                if new_generation_config != self.generation_config:\n+                if new_generation_config != self.generation_config:  # 3)\n                     warnings.warn(\n                         \"You have modified the pretrained model configuration to control generation. This is a\"\n                         \" deprecated strategy to control generation and will be removed soon, in a future version.\"\n@@ -2287,13 +2281,6 @@ def heal_tokens(\n \n         return input_ids\n \n-    def contrastive_search(self, *args, **kwargs):\n-        logger.warning_once(\n-            \"Calling `contrastive_search` directly is deprecated and will be removed in v4.41. Use `generate` or a \"\n-            \"custom generation loop instead.\",\n-        )\n-        return self._contrastive_search(*args, **kwargs)\n-\n     def _dola_decoding(\n         self,\n         input_ids: torch.LongTensor,"
        },
        {
            "sha": "b943b5e7989f034663fd7349d056c1d2822c5087",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 13,
            "deletions": 18,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -2571,26 +2571,21 @@ def save_pretrained(\n         # Save the config\n         if is_main_process:\n             if not _hf_peft_config_loaded:\n+                # If the model config has set attributes that should be in the generation config, move them there.\n+                misplaced_generation_parameters = model_to_save.config._get_non_default_generation_parameters()\n+                if self.can_generate() and len(misplaced_generation_parameters) > 0:\n+                    warnings.warn(\n+                        \"Moving the following attributes in the config to the generation config: \"\n+                        f\"{misplaced_generation_parameters}. You are seeing this warning because you've set \"\n+                        \"generation parameters in the model config, as opposed to in the generation config.\",\n+                        UserWarning,\n+                    )\n+                    for param_name, param_value in misplaced_generation_parameters.items():\n+                        setattr(model_to_save.generation_config, param_name, param_value)\n+                        setattr(model_to_save.config, param_name, None)\n+\n                 model_to_save.config.save_pretrained(save_directory)\n             if self.can_generate():\n-                # generation config built from the model config + the model config holds generation kwargs -> generate\n-                # may revert to legacy behavior if the two don't match\n-                if (\n-                    model_to_save.generation_config._from_model_config\n-                    and model_to_save.config._has_non_default_generation_parameters()\n-                ):\n-                    new_generation_config = GenerationConfig.from_model_config(model_to_save.config)\n-                    if new_generation_config != model_to_save.generation_config:\n-                        logger.warning(\n-                            \"Your generation config was originally created from the model config, but the model \"\n-                            \"config has changed since then. Unless you pass the `generation_config` argument to this \"\n-                            \"model's `generate` calls, they will revert to the legacy behavior where the base \"\n-                            \"`generate` parameterization is loaded from the model config instead. \"\n-                            \"To avoid this behavior and this warning, we recommend you to overwrite the generation \"\n-                            \"config model attribute before calling the model's `save_pretrained`, preferably also \"\n-                            \"removing any generation kwargs from the model config. This warning will be raised to an \"\n-                            \"exception in v4.41.\"\n-                        )\n                 model_to_save.generation_config.save_pretrained(save_directory)\n \n             if _hf_peft_config_loaded:"
        },
        {
            "sha": "7980667a68d7c54833d1ed61489071e61475d549",
            "filename": "src/transformers/models/audio_spectrogram_transformer/configuration_audio_spectrogram_transformer.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fconfiguration_audio_spectrogram_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fconfiguration_audio_spectrogram_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fconfiguration_audio_spectrogram_transformer.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -14,6 +14,8 @@\n # limitations under the License.\n \"\"\"Audio Spectogram Transformer (AST) model configuration\"\"\"\n \n+from typing import Any, Dict\n+\n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n \n@@ -118,3 +120,9 @@ def __init__(\n         self.time_stride = time_stride\n         self.max_length = max_length\n         self.num_mel_bins = num_mel_bins\n+\n+    # Overwritten from the parent class: AST is not compatible with `generate`, but has a config parameter sharing the\n+    # same name (`max_length`). Sharing the same name triggers checks regarding the config -> generation_config\n+    # generative parameters deprecation cycle, overwriting this function prevents this from happening.\n+    def _get_non_default_generation_parameters(self) -> Dict[str, Any]:\n+        return {}"
        },
        {
            "sha": "e365744f8b9e8741deb1dbf3dcb4cddd6289f1d0",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -693,7 +693,7 @@ def forward(\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n             logger.warning_once(\n                 \"Using `past_key_values` as a tuple is deprecated and will be removed in v4.45. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\"\n+                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n             )\n \n         batch_size, seq_length, _ = inputs_embeds.shape"
        },
        {
            "sha": "e668a0dc063132ef161e3dbc3a2bf737236ac2cb",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -532,7 +532,7 @@ def forward(\n             if not self.training:\n                 logger.warning_once(\n                     \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.45. \"\n-                    \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\"\n+                    \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n                 )\n \n         seq_length = inputs_embeds.shape[1]"
        },
        {
            "sha": "2f131d3186f8b0b4d0838b1a389ac2b802d6dedd",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -1066,7 +1066,7 @@ def forward(\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n             logger.warning_once(\n                 \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\"\n+                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n             )\n \n         if cache_position is None:"
        },
        {
            "sha": "ab5d49b32fea904615ef9ec5283bac48324eba82",
            "filename": "src/transformers/models/encoder_decoder/configuration_encoder_decoder.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fconfiguration_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fconfiguration_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fconfiguration_encoder_decoder.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -74,9 +74,11 @@ class EncoderDecoderConfig(PretrainedConfig):\n \n     def __init__(self, **kwargs):\n         super().__init__(**kwargs)\n-        assert (\n-            \"encoder\" in kwargs and \"decoder\" in kwargs\n-        ), \"Config has to be initialized with encoder and decoder config\"\n+        if \"encoder\" not in kwargs or \"decoder\" not in kwargs:\n+            raise ValueError(\n+                f\"A configuraton of type {self.model_type} cannot be instantiated because \"\n+                f\"both `encoder` and `decoder` sub-configurations were not passed, only {kwargs}\"\n+            )\n         encoder_config = kwargs.pop(\"encoder\")\n         encoder_model_type = encoder_config.pop(\"model_type\")\n         decoder_config = kwargs.pop(\"decoder\")"
        },
        {
            "sha": "a9acd171c3aefe3560ca0bba68b9c090f2ef2cc1",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -1029,7 +1029,7 @@ def forward(\n             if not self.training:\n                 logger.warning_once(\n                     \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.45. \"\n-                    \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\"\n+                    \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n                 )\n \n         alibi = None"
        },
        {
            "sha": "bca2b91d02b51551d992c7a44a11bcf49508d749",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -862,7 +862,7 @@ def forward(\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n             logger.warning_once(\n                 \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\"\n+                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n             )\n \n         # decoder layers"
        },
        {
            "sha": "4807289c927cbfefc77f2dc429afb93fca1ba646",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -423,7 +423,7 @@ def forward(\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n             logger.warning_once(\n                 \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.45. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\"\n+                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n             )\n \n         all_hidden_states = () if output_hidden_states else None"
        },
        {
            "sha": "65144ad0c0f18d94016e642e2d4df393d26daebf",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -747,7 +747,7 @@ def forward(\n             if not self.training:\n                 logger.warning_once(\n                     \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.45. \"\n-                    \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\"\n+                    \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n                 )\n \n         seq_length = inputs_embeds.shape[1]"
        },
        {
            "sha": "5d21f2d2a725ffbd4bc7673a7b88eb631ac1c01a",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -928,7 +928,7 @@ def forward(\n             if not self.training:\n                 logger.warning_once(\n                     \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.45. \"\n-                    \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\"\n+                    \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n                 )\n \n         seq_length = inputs_embeds.shape[1]"
        },
        {
            "sha": "ba0f319791e4a8bed2775e208fa6b57e052a6414",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -819,7 +819,7 @@ def forward(\n             if not self.training:\n                 logger.warning_once(\n                     \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.45. \"\n-                    \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\"\n+                    \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n                 )\n \n         seq_length = inputs_embeds.shape[1]"
        },
        {
            "sha": "b4c24a46bb68f3cffc182afc7bbd56dc782f67b0",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -1243,7 +1243,7 @@ def forward(\n             if not self.training:\n                 logger.warning_once(\n                     \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.45. \"\n-                    \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\"\n+                    \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n                 )\n             return_legacy_cache = True\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)"
        },
        {
            "sha": "489c284ac3ab9bfce830102ed852f0919decc7ea",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -951,7 +951,7 @@ def forward(\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n             logger.warning_once(\n                 \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\"\n+                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n             )\n \n         if cache_position is None:"
        },
        {
            "sha": "3cbea6293a99600dc81faeeae6096a494e80811d",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -767,7 +767,7 @@ def forward(\n             return_legacy_cache = True\n             logger.warning_once(\n                 \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\"\n+                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n             )\n \n         if cache_position is None:"
        },
        {
            "sha": "6f75d1b7db6938053ed3b6050a3e78ca681fce2e",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -1023,7 +1023,7 @@ def forward(\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n             logger.warning_once(\n                 \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\"\n+                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n             )\n \n         if inputs_embeds is None:"
        },
        {
            "sha": "0ac2d847d1c77c008fb5c5481701c2908a1bc547",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -873,7 +873,7 @@ def forward(\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n             logger.warning_once(\n                 \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\"\n+                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n             )\n \n         if cache_position is None:"
        },
        {
            "sha": "0b7b85299c5d75d5bee36fbd3fe32eeca2583e91",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -682,7 +682,7 @@ def forward(\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n             logger.warning_once(\n                 \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\"\n+                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n             )\n \n         if inputs_embeds is None:"
        },
        {
            "sha": "da152b6148021b6d772e9411c4b045b8e0b8bc58",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -967,7 +967,7 @@ def forward(\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n             logger.warning_once(\n                 \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\"\n+                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n             )\n \n         if inputs_embeds is None:"
        },
        {
            "sha": "af1b988643a912c163d67a861fdc69b4aeb54c51",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -1008,7 +1008,7 @@ def forward(\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n             logger.warning_once(\n                 \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\"\n+                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n             )\n \n         if inputs_embeds is None:"
        },
        {
            "sha": "d4766927355e7fad0f540251597d076edce366f0",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -872,7 +872,7 @@ def forward(\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n             logger.warning_once(\n                 \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\"\n+                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n             )\n \n         if inputs_embeds is None:"
        },
        {
            "sha": "3fa8baeb3631e525ef1280723ebf86fefea80356",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -1034,7 +1034,7 @@ def forward(\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n             logger.warning_once(\n                 \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\"\n+                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n             )\n \n         if inputs_embeds is None:"
        },
        {
            "sha": "98de7382a45694dc848f7f7b448ac530059ac94f",
            "filename": "src/transformers/models/rag/configuration_rag.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Frag%2Fconfiguration_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Frag%2Fconfiguration_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fconfiguration_rag.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -124,9 +124,11 @@ def __init__(\n             vocab_size=vocab_size,\n             **kwargs,\n         )\n-        assert (\n-            \"question_encoder\" in kwargs and \"generator\" in kwargs\n-        ), \"Config has to be initialized with question_encoder and generator config\"\n+        if \"question_encoder\" not in kwargs or \"generator\" not in kwargs:\n+            raise ValueError(\n+                f\"A configuraton of type {self.model_type} cannot be instantiated because \"\n+                f\"both `question_encoder` and `generator` sub-configurations were not passed, only {kwargs}\"\n+            )\n         question_encoder_config = kwargs.pop(\"question_encoder\")\n         question_encoder_model_type = question_encoder_config.pop(\"model_type\")\n         decoder_config = kwargs.pop(\"generator\")"
        },
        {
            "sha": "bf40dfc99f2c3e434e8384e0abba083e82a6a7fd",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -959,7 +959,7 @@ def forward(\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n             logger.warning_once(\n                 \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\"\n+                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n             )\n \n         if inputs_embeds is None:"
        },
        {
            "sha": "3f201baa9267d1852ff7e54648fe7f392463491b",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -845,7 +845,7 @@ def forward(\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n             logger.warning_once(\n                 \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\"\n+                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n             )\n \n         if inputs_embeds is None:"
        },
        {
            "sha": "18df9ad6193f12a83996147800d292d0c2f0638d",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -289,13 +289,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class ForceTokensLogitsProcessor(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class GenerationMixin(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "a04dac96169e82a2aeb688a0350a6083dbb386ec",
            "filename": "tests/generation/test_stopping_criteria.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/tests%2Fgeneration%2Ftest_stopping_criteria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/tests%2Fgeneration%2Ftest_stopping_criteria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_stopping_criteria.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -28,7 +28,6 @@\n     from transformers.generation import (\n         EosTokenCriteria,\n         MaxLengthCriteria,\n-        MaxNewTokensCriteria,\n         MaxTimeCriteria,\n         StoppingCriteriaList,\n         StopStringCriteria,\n@@ -76,21 +75,6 @@ def test_max_length_criteria(self):\n         input_ids, scores = self._get_tensors(10)\n         self.assertTrue(all(criteria(input_ids, scores)))\n \n-    def test_max_new_tokens_criteria(self):\n-        criteria = MaxNewTokensCriteria(start_length=5, max_new_tokens=5)\n-\n-        input_ids, scores = self._get_tensors(5)\n-        self.assertFalse(all(criteria(input_ids, scores)))\n-\n-        input_ids, scores = self._get_tensors(9)\n-        self.assertFalse(all(criteria(input_ids, scores)))\n-\n-        input_ids, scores = self._get_tensors(10)\n-        self.assertTrue(all(criteria(input_ids, scores)))\n-\n-        criteria_list = StoppingCriteriaList([criteria])\n-        self.assertEqual(criteria_list.max_length, 10)\n-\n     def test_max_time_criteria(self):\n         input_ids, scores = self._get_tensors(5)\n "
        },
        {
            "sha": "165166a2991a564ee7f5fecaeda6612a72474387",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 12,
            "deletions": 6,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -1576,13 +1576,19 @@ def test_generate_continue_from_past_key_values(self):\n             # 3. ignore `token_type_ids` for simplicity\n             # 4. ignore `forced_eos_token_id`, which requires further manipulation of the continuation inputs and is\n             #    active by default on some models\n+            # 5. ignore `encoder_no_repeat_ngram_size`, which is set by default in some encoder-decoder models. When\n+            #    we use their decoder as a stand-alone model, `encoder_no_repeat_ngram_size` actually prevents\n+            #    repetition exclusively from the prompt. This test relies on comparing one call vs 2 calls\n+            #    with cache, what is considered a prompt is different in the two cases.\n+\n             if \"token_type_ids\" in inputs:\n                 del inputs[\"token_type_ids\"]\n \n             model = model_class(config).to(torch_device)\n             model.eval()\n             model.generation_config.pad_token_id = model.generation_config.eos_token_id = -1\n             model.generation_config.forced_eos_token_id = None\n+            model.generation_config.encoder_no_repeat_ngram_size = 0\n             model.generation_config.use_cache = True\n \n             # If \"past_key_values\" is not returned, skip the test (e.g. RWKV uses a different cache name and format)\n@@ -2846,7 +2852,7 @@ def forward(self, input_ids, **kwargs):\n     def test_default_max_length_warning(self):\n         model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)\n         tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n-        model.config.pad_token_id = tokenizer.eos_token_id\n+        model.generation_config.pad_token_id = tokenizer.eos_token_id\n \n         text = \"Hello world\"\n         tokenized_inputs = tokenizer([text], return_tensors=\"pt\")\n@@ -2873,8 +2879,8 @@ def test_length_warning_assisted_generation(self):\n         model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)\n         assistant = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)\n         tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n-        model.config.pad_token_id = tokenizer.eos_token_id\n-        assistant.config.pad_token_id = tokenizer.eos_token_id\n+        model.generation_config.pad_token_id = tokenizer.eos_token_id\n+        assistant.generation_config.pad_token_id = tokenizer.eos_token_id\n \n         text = \"Hello world\"\n         tokenized_inputs = tokenizer([text], return_tensors=\"pt\")\n@@ -2895,8 +2901,8 @@ def test_generated_length_assisted_generation(self):\n         model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)\n         assistant = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)\n         tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n-        model.config.pad_token_id = tokenizer.eos_token_id\n-        assistant.config.pad_token_id = tokenizer.eos_token_id\n+        model.generation_config.pad_token_id = tokenizer.eos_token_id\n+        assistant.generation_config.pad_token_id = tokenizer.eos_token_id\n \n         text = \"Hello world\"\n         tokenized_inputs = tokenizer([text], return_tensors=\"pt\")\n@@ -2922,7 +2928,7 @@ def test_model_kwarg_assisted_decoding_decoder_only(self):\n         # PT-only test: TF doesn't support assisted decoding yet.\n         model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)\n         tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n-        model.config.pad_token_id = tokenizer.eos_token_id\n+        model.generation_config.pad_token_id = tokenizer.eos_token_id\n \n         text = \"Hello world\"\n         tokenized_inputs = tokenizer([text], return_tensors=\"pt\")"
        },
        {
            "sha": "eda51d21199f3192a43e302db25a83be73579c10",
            "filename": "tests/models/bart/test_modeling_bart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -123,12 +123,6 @@ def __init__(\n         self.pad_token_id = pad_token_id\n         self.bos_token_id = bos_token_id\n \n-        # forcing a certain token to be generated, sets all other tokens to -inf\n-        # if however the token to be generated is already at -inf then it can lead token\n-        # `nan` values and thus break generation\n-        self.forced_bos_token_id = None\n-        self.forced_eos_token_id = None\n-\n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size).clamp(\n@@ -158,8 +152,6 @@ def get_config(self):\n             eos_token_id=self.eos_token_id,\n             bos_token_id=self.bos_token_id,\n             pad_token_id=self.pad_token_id,\n-            forced_bos_token_id=self.forced_bos_token_id,\n-            forced_eos_token_id=self.forced_eos_token_id,\n         )\n \n     def get_pipeline_config(self):"
        },
        {
            "sha": "cecedb8a907133095e86d3a2f8d2bed890596135",
            "filename": "tests/models/blenderbot/test_modeling_blenderbot.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/tests%2Fmodels%2Fblenderbot%2Ftest_modeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/tests%2Fmodels%2Fblenderbot%2Ftest_modeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblenderbot%2Ftest_modeling_blenderbot.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -116,12 +116,6 @@ def __init__(\n         self.pad_token_id = pad_token_id\n         self.bos_token_id = bos_token_id\n \n-        # forcing a certain token to be generated, sets all other tokens to -inf\n-        # if however the token to be generated is already at -inf then it can lead token\n-        # `nan` values and thus break generation\n-        self.forced_bos_token_id = None\n-        self.forced_eos_token_id = None\n-\n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size).clamp(\n             3,\n@@ -150,8 +144,6 @@ def get_config(self):\n             eos_token_id=self.eos_token_id,\n             bos_token_id=self.bos_token_id,\n             pad_token_id=self.pad_token_id,\n-            forced_bos_token_id=self.forced_bos_token_id,\n-            forced_eos_token_id=self.forced_eos_token_id,\n         )\n \n     def get_pipeline_config(self):\n@@ -368,7 +360,6 @@ def __init__(\n         decoder_attention_heads=4,\n         max_position_embeddings=30,\n         is_encoder_decoder=False,\n-        encoder_no_repeat_ngram_size=0,\n         pad_token_id=0,\n         bos_token_id=1,\n         eos_token_id=2,\n@@ -399,7 +390,6 @@ def __init__(\n         self.use_cache = use_cache\n         self.max_position_embeddings = max_position_embeddings\n         self.is_encoder_decoder = is_encoder_decoder\n-        self.encoder_no_repeat_ngram_size = encoder_no_repeat_ngram_size\n \n         self.scope = None\n         self.decoder_key_length = decoder_seq_length\n@@ -431,7 +421,6 @@ def prepare_config_and_inputs(self):\n             decoder_start_token_id=self.decoder_start_token_id,\n             max_position_embeddings=self.max_position_embeddings,\n             is_encoder_decoder=self.is_encoder_decoder,\n-            encoder_no_repeat_ngram_size=self.encoder_no_repeat_ngram_size,\n         )\n \n         return ("
        },
        {
            "sha": "59f68b54754796ec466b65312be14796afc9d265",
            "filename": "tests/models/blenderbot_small/test_modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_blenderbot_small.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -113,12 +113,6 @@ def __init__(\n         self.pad_token_id = pad_token_id\n         self.bos_token_id = bos_token_id\n \n-        # forcing a certain token to be generated, sets all other tokens to -inf\n-        # if however the token to be generated is already at -inf then it can lead token\n-        # `nan` values and thus break generation\n-        self.forced_bos_token_id = None\n-        self.forced_eos_token_id = None\n-\n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size).clamp(\n             3,\n@@ -147,8 +141,6 @@ def get_config(self):\n             eos_token_id=self.eos_token_id,\n             bos_token_id=self.bos_token_id,\n             pad_token_id=self.pad_token_id,\n-            forced_bos_token_id=self.forced_bos_token_id,\n-            forced_eos_token_id=self.forced_eos_token_id,\n         )\n \n     def prepare_config_and_inputs_for_common(self):"
        },
        {
            "sha": "0c95e6291c503b59c64ecf9bae5d3413c5b7dd68",
            "filename": "tests/models/decision_transformer/test_modeling_decision_transformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/tests%2Fmodels%2Fdecision_transformer%2Ftest_modeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/tests%2Fmodels%2Fdecision_transformer%2Ftest_modeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdecision_transformer%2Ftest_modeling_decision_transformer.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -41,7 +41,6 @@ def __init__(\n         act_dim=6,\n         state_dim=17,\n         hidden_size=23,\n-        max_length=11,\n         is_training=True,\n     ):\n         self.parent = parent\n@@ -50,7 +49,6 @@ def __init__(\n         self.act_dim = act_dim\n         self.state_dim = state_dim\n         self.hidden_size = hidden_size\n-        self.max_length = max_length\n         self.is_training = is_training\n \n     def prepare_config_and_inputs(self):\n@@ -80,7 +78,6 @@ def get_config(self):\n             act_dim=self.act_dim,\n             state_dim=self.state_dim,\n             hidden_size=self.hidden_size,\n-            max_length=self.max_length,\n         )\n \n     def create_and_check_model("
        },
        {
            "sha": "aed5381fcc706a2ab66df527df7f4366cd77f0d0",
            "filename": "tests/models/marian/test_modeling_marian.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/tests%2Fmodels%2Fmarian%2Ftest_modeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/tests%2Fmodels%2Fmarian%2Ftest_modeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmarian%2Ftest_modeling_marian.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -132,12 +132,6 @@ def __init__(\n         self.bos_token_id = bos_token_id\n         self.decoder_start_token_id = decoder_start_token_id\n \n-        # forcing a certain token to be generated, sets all other tokens to -inf\n-        # if however the token to be generated is already at -inf then it can lead token\n-        # `nan` values and thus break generation\n-        self.forced_bos_token_id = None\n-        self.forced_eos_token_id = None\n-\n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size).clamp(\n             3,\n@@ -167,8 +161,6 @@ def get_config(self):\n             bos_token_id=self.bos_token_id,\n             pad_token_id=self.pad_token_id,\n             decoder_start_token_id=self.decoder_start_token_id,\n-            forced_bos_token_id=self.forced_bos_token_id,\n-            forced_eos_token_id=self.forced_eos_token_id,\n         )\n \n     def prepare_config_and_inputs_for_common(self):"
        },
        {
            "sha": "9401d892daa39b3cdb175876595daa9ede1190cf",
            "filename": "tests/models/mbart/test_modeling_mbart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/tests%2Fmodels%2Fmbart%2Ftest_modeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/tests%2Fmodels%2Fmbart%2Ftest_modeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmbart%2Ftest_modeling_mbart.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -120,12 +120,6 @@ def __init__(\n         self.pad_token_id = pad_token_id\n         self.bos_token_id = bos_token_id\n \n-        # forcing a certain token to be generated, sets all other tokens to -inf\n-        # if however the token to be generated is already at -inf then it can lead token\n-        # `nan` values and thus break generation\n-        self.forced_bos_token_id = None\n-        self.forced_eos_token_id = None\n-\n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size).clamp(\n@@ -155,8 +149,6 @@ def get_config(self):\n             eos_token_id=self.eos_token_id,\n             bos_token_id=self.bos_token_id,\n             pad_token_id=self.pad_token_id,\n-            forced_bos_token_id=self.forced_bos_token_id,\n-            forced_eos_token_id=self.forced_eos_token_id,\n         )\n \n     def prepare_config_and_inputs_for_common(self):"
        },
        {
            "sha": "cd4cfa68e5dcdef10069628a213ad74cf9b183ba",
            "filename": "tests/models/mobilevit/test_modeling_mobilevit.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/tests%2Fmodels%2Fmobilevit%2Ftest_modeling_mobilevit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/tests%2Fmodels%2Fmobilevit%2Ftest_modeling_mobilevit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmobilevit%2Ftest_modeling_mobilevit.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -17,7 +17,7 @@\n import unittest\n \n from transformers import MobileViTConfig\n-from transformers.testing_utils import require_torch, require_vision, slow, torch_device\n+from transformers.testing_utils import is_flaky, require_torch, require_vision, slow, torch_device\n from transformers.utils import cached_property, is_torch_available, is_vision_available\n \n from ...test_configuration_common import ConfigTester\n@@ -274,6 +274,10 @@ def test_model_from_pretrained(self):\n         model = MobileViTModel.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n+    @is_flaky(description=\"is_flaky https://github.com/huggingface/transformers/issues/29516\")\n+    def test_batching_equivalence(self):\n+        super().test_batching_equivalence()\n+\n \n # We will verify our results on an image of cute cats\n def prepare_img():"
        },
        {
            "sha": "2bd102b904e376c70660603f90fcd0ff8c0df344",
            "filename": "tests/models/pegasus/test_modeling_pegasus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/tests%2Fmodels%2Fpegasus%2Ftest_modeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/tests%2Fmodels%2Fpegasus%2Ftest_modeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpegasus%2Ftest_modeling_pegasus.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -112,12 +112,6 @@ def __init__(\n         self.pad_token_id = pad_token_id\n         self.bos_token_id = bos_token_id\n \n-        # forcing a certain token to be generated, sets all other tokens to -inf\n-        # if however the token to be generated is already at -inf then it can lead token\n-        # `nan` values and thus break generation\n-        self.forced_bos_token_id = None\n-        self.forced_eos_token_id = None\n-\n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size).clamp(\n@@ -165,8 +159,6 @@ def get_config(self):\n             eos_token_id=self.eos_token_id,\n             bos_token_id=self.bos_token_id,\n             pad_token_id=self.pad_token_id,\n-            forced_bos_token_id=self.forced_bos_token_id,\n-            forced_eos_token_id=self.forced_eos_token_id,\n         )\n \n     def prepare_config_and_inputs_for_common(self):"
        },
        {
            "sha": "065c6536481d3cd36a874f856a73059eb5147c10",
            "filename": "tests/models/whisper/test_modeling_flax_whisper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/tests%2Fmodels%2Fwhisper%2Ftest_modeling_flax_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/tests%2Fmodels%2Fwhisper%2Ftest_modeling_flax_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_flax_whisper.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -84,7 +84,6 @@ def __init__(\n         decoder_start_token_id=85,\n         num_conv_layers=1,\n         suppress_tokens=None,\n-        begin_suppress_tokens=None,\n     ):\n         self.parent = parent\n         self.batch_size = batch_size\n@@ -118,7 +117,6 @@ def __init__(\n         self.decoder_start_token_id = decoder_start_token_id\n         self.num_conv_layers = num_conv_layers\n         self.suppress_tokens = suppress_tokens\n-        self.begin_suppress_tokens = begin_suppress_tokens\n \n     def prepare_config_and_inputs_for_common(self):\n         input_features = floats_tensor([self.batch_size, self.num_mel_bins, self.seq_length], self.vocab_size)\n@@ -147,7 +145,6 @@ def prepare_config_and_inputs_for_common(self):\n             encoder_ffn_dim=self.encoder_ffn_dim,\n             encoder_layers=self.encoder_layers,\n             suppress_tokens=self.suppress_tokens,\n-            begin_suppress_tokens=self.begin_suppress_tokens,\n         )\n         inputs_dict = prepare_whisper_inputs_dict(config, input_features, decoder_input_ids)\n         return config, inputs_dict\n@@ -741,7 +738,6 @@ def __init__(\n         num_mel_bins=80,\n         num_conv_layers=1,\n         suppress_tokens=None,\n-        begin_suppress_tokens=None,\n         classifier_proj_size=4,\n         num_labels=2,\n         is_encoder_decoder=False,\n@@ -764,7 +760,6 @@ def __init__(\n         self.max_source_positions = max_source_positions\n         self.num_conv_layers = num_conv_layers\n         self.suppress_tokens = suppress_tokens\n-        self.begin_suppress_tokens = begin_suppress_tokens\n         self.classifier_proj_size = classifier_proj_size\n         self.num_labels = num_labels\n         self.is_encoder_decoder = is_encoder_decoder\n@@ -785,7 +780,6 @@ def get_config(self):\n             decoder_ffn_dim=self.hidden_size,\n             encoder_ffn_dim=self.hidden_size,\n             suppress_tokens=self.suppress_tokens,\n-            begin_suppress_tokens=self.begin_suppress_tokens,\n             classifier_proj_size=self.classifier_proj_size,\n             num_labels=self.num_labels,\n             is_encoder_decoder=self.is_encoder_decoder,"
        },
        {
            "sha": "be311486267d28162e69e5856ae75c857bb2b87b",
            "filename": "tests/models/whisper/test_modeling_tf_whisper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/tests%2Fmodels%2Fwhisper%2Ftest_modeling_tf_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/tests%2Fmodels%2Fwhisper%2Ftest_modeling_tf_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_tf_whisper.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -104,7 +104,6 @@ def __init__(\n         decoder_start_token_id=85,\n         num_conv_layers=1,\n         suppress_tokens=None,\n-        begin_suppress_tokens=None,\n     ):\n         self.parent = parent\n         self.batch_size = batch_size\n@@ -129,7 +128,6 @@ def __init__(\n         self.decoder_start_token_id = decoder_start_token_id\n         self.num_conv_layers = num_conv_layers\n         self.suppress_tokens = suppress_tokens\n-        self.begin_suppress_tokens = begin_suppress_tokens\n \n     def prepare_config_and_inputs(self):\n         input_features = floats_tensor([self.batch_size, self.num_mel_bins, self.seq_length], self.vocab_size)\n@@ -166,7 +164,6 @@ def get_config(self):\n             encoder_ffn_dim=self.hidden_size,\n             decoder_start_token_id=self.decoder_start_token_id,\n             suppress_tokens=self.suppress_tokens,\n-            begin_suppress_tokens=self.begin_suppress_tokens,\n         )\n \n     def prepare_config_and_inputs_for_common(self):"
        },
        {
            "sha": "f3d191b4d3c4c64480b1fae550748d198526f669",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -218,7 +218,6 @@ def __init__(\n         decoder_start_token_id=85,\n         num_conv_layers=1,\n         suppress_tokens=None,\n-        begin_suppress_tokens=None,\n     ):\n         self.parent = parent\n         self.batch_size = batch_size\n@@ -243,7 +242,6 @@ def __init__(\n         self.decoder_start_token_id = decoder_start_token_id\n         self.num_conv_layers = num_conv_layers\n         self.suppress_tokens = suppress_tokens\n-        self.begin_suppress_tokens = begin_suppress_tokens\n \n     def prepare_config_and_inputs(self):\n         input_features = floats_tensor([self.batch_size, self.num_mel_bins, self.seq_length], self.vocab_size)\n@@ -280,7 +278,6 @@ def get_config(self):\n             encoder_ffn_dim=self.hidden_size,\n             decoder_start_token_id=self.decoder_start_token_id,\n             suppress_tokens=self.suppress_tokens,\n-            begin_suppress_tokens=self.begin_suppress_tokens,\n         )\n \n     def prepare_config_and_inputs_for_common(self):\n@@ -3309,7 +3306,6 @@ def __init__(\n         num_mel_bins=80,\n         num_conv_layers=1,\n         suppress_tokens=None,\n-        begin_suppress_tokens=None,\n         classifier_proj_size=4,\n         num_labels=2,\n         is_encoder_decoder=False,\n@@ -3332,7 +3328,6 @@ def __init__(\n         self.max_source_positions = max_source_positions\n         self.num_conv_layers = num_conv_layers\n         self.suppress_tokens = suppress_tokens\n-        self.begin_suppress_tokens = begin_suppress_tokens\n         self.classifier_proj_size = classifier_proj_size\n         self.num_labels = num_labels\n         self.is_encoder_decoder = is_encoder_decoder\n@@ -3353,7 +3348,6 @@ def get_config(self):\n             decoder_ffn_dim=self.hidden_size,\n             encoder_ffn_dim=self.hidden_size,\n             suppress_tokens=self.suppress_tokens,\n-            begin_suppress_tokens=self.begin_suppress_tokens,\n             classifier_proj_size=self.classifier_proj_size,\n             num_labels=self.num_labels,\n             is_encoder_decoder=self.is_encoder_decoder,\n@@ -3685,7 +3679,6 @@ def __init__(\n         decoder_start_token_id=85,\n         num_conv_layers=1,\n         suppress_tokens=None,\n-        begin_suppress_tokens=None,\n     ):\n         self.parent = parent\n         self.batch_size = batch_size\n@@ -3709,7 +3702,6 @@ def __init__(\n         self.decoder_start_token_id = decoder_start_token_id\n         self.num_conv_layers = num_conv_layers\n         self.suppress_tokens = suppress_tokens\n-        self.begin_suppress_tokens = begin_suppress_tokens\n \n     def prepare_config_and_inputs(self):\n         input_features = floats_tensor([self.batch_size, self.num_mel_bins, self.seq_length], self.vocab_size)\n@@ -3765,7 +3757,6 @@ def get_config(self):\n             encoder_ffn_dim=self.hidden_size,\n             decoder_start_token_id=self.decoder_start_token_id,\n             suppress_tokens=self.suppress_tokens,\n-            begin_suppress_tokens=self.begin_suppress_tokens,\n         )\n \n     def prepare_config_and_inputs_for_common(self):"
        },
        {
            "sha": "148f091c27942a48168a53daa24b2fb5c516cb12",
            "filename": "tests/utils/test_cli.py",
            "status": "modified",
            "additions": 1,
            "deletions": 13,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/tests%2Futils%2Ftest_cli.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/tests%2Futils%2Ftest_cli.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cli.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -18,7 +18,7 @@\n import unittest\n from unittest.mock import patch\n \n-from transformers.testing_utils import CaptureStd, is_pt_tf_cross_test, require_torch\n+from transformers.testing_utils import CaptureStd, require_torch\n \n \n class CLITest(unittest.TestCase):\n@@ -33,18 +33,6 @@ def test_cli_env(self):\n         self.assertIn(\"Platform\", cs.out)\n         self.assertIn(\"Using distributed or parallel set-up in script?\", cs.out)\n \n-    @is_pt_tf_cross_test\n-    @patch(\n-        \"sys.argv\", [\"fakeprogrampath\", \"pt-to-tf\", \"--model-name\", \"hf-internal-testing/tiny-random-gptj\", \"--no-pr\"]\n-    )\n-    def test_cli_pt_to_tf(self):\n-        import transformers.commands.transformers_cli\n-\n-        shutil.rmtree(\"/tmp/hf-internal-testing/tiny-random-gptj\", ignore_errors=True)  # cleans potential past runs\n-        transformers.commands.transformers_cli.main()\n-\n-        self.assertTrue(os.path.exists(\"/tmp/hf-internal-testing/tiny-random-gptj/tf_model.h5\"))\n-\n     @require_torch\n     @patch(\"sys.argv\", [\"fakeprogrampath\", \"download\", \"hf-internal-testing/tiny-random-gptj\", \"--cache-dir\", \"/tmp\"])\n     def test_cli_download(self):"
        },
        {
            "sha": "6b684867eb870c9594da62376ca81b6e62ef67f0",
            "filename": "tests/utils/test_configuration_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/tests%2Futils%2Ftest_configuration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/tests%2Futils%2Ftest_configuration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_configuration_utils.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -315,21 +315,19 @@ def test_repo_versioning_before(self):\n         old_configuration = old_transformers.models.auto.AutoConfig.from_pretrained(repo)\n         self.assertEqual(old_configuration.hidden_size, 768)\n \n-    def test_saving_config_with_custom_generation_kwargs_raises_warning(self):\n+    def test_saving_config_with_custom_generation_kwargs_raises_exception(self):\n         config = BertConfig(min_length=3)  # `min_length = 3` is a non-default generation kwarg\n         with tempfile.TemporaryDirectory() as tmp_dir:\n-            with self.assertLogs(\"transformers.configuration_utils\", level=\"WARNING\") as logs:\n+            with self.assertRaises(ValueError):\n                 config.save_pretrained(tmp_dir)\n-            self.assertEqual(len(logs.output), 1)\n-            self.assertIn(\"min_length\", logs.output[0])\n \n-    def test_has_non_default_generation_parameters(self):\n+    def test_get_non_default_generation_parameters(self):\n         config = BertConfig()\n-        self.assertFalse(config._has_non_default_generation_parameters())\n+        self.assertFalse(len(config._get_non_default_generation_parameters()) > 0)\n         config = BertConfig(min_length=3)\n-        self.assertTrue(config._has_non_default_generation_parameters())\n+        self.assertTrue(len(config._get_non_default_generation_parameters()) > 0)\n         config = BertConfig(min_length=0)  # `min_length = 0` is a default generation kwarg\n-        self.assertFalse(config._has_non_default_generation_parameters())\n+        self.assertFalse(len(config._get_non_default_generation_parameters()) > 0)\n \n     def test_loading_config_do_not_raise_future_warnings(self):\n         \"\"\"Regression test for https://github.com/huggingface/transformers/issues/31002.\"\"\""
        },
        {
            "sha": "238a9a1fe4d6f81033fbb39301efb2400066bd2e",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 23,
            "deletions": 6,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/970a16ec7f693104085fd826523e5c6ce64f2040/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/970a16ec7f693104085fd826523e5c6ce64f2040/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=970a16ec7f693104085fd826523e5c6ce64f2040",
            "patch": "@@ -23,6 +23,7 @@\n import unittest\n import unittest.mock as mock\n import uuid\n+import warnings\n from pathlib import Path\n \n import requests\n@@ -1599,14 +1600,30 @@ def test_safetensors_torch_from_torch_sharded(self):\n         for p1, p2 in zip(model.parameters(), new_model.parameters()):\n             self.assertTrue(torch.equal(p1, p2))\n \n-    def test_modifying_model_config_causes_warning_saving_generation_config(self):\n+    def test_modifying_model_config_gets_moved_to_generation_config(self):\n+        \"\"\"\n+        Calling `model.save_pretrained` should move the changes made to `generate` parameterization in the model config\n+        to the generation config.\n+        \"\"\"\n         model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n-        model.config.top_k = 1\n-        with tempfile.TemporaryDirectory() as tmp_dir:\n-            with self.assertLogs(\"transformers.modeling_utils\", level=\"WARNING\") as logs:\n+        # Initially, the repetition penalty has its default value in `model.config`. The `model.generation_config` will\n+        # have the exact same default\n+        self.assertTrue(model.config.repetition_penalty == 1.0)\n+        self.assertTrue(model.generation_config.repetition_penalty == 1.0)\n+        # If the user attempts to save a custom generation parameter:\n+        model.config.repetition_penalty = 3.0\n+        with warnings.catch_warnings(record=True) as warning_list:\n+            with tempfile.TemporaryDirectory() as tmp_dir:\n                 model.save_pretrained(tmp_dir)\n-            self.assertEqual(len(logs.output), 1)\n-            self.assertIn(\"Your generation config was originally created from the model config\", logs.output[0])\n+                # 1 - That parameter will be removed from `model.config`. We don't want to use `model.config` to store\n+                # generative parameters, and the old default (1.0) would no longer relect the user's wishes.\n+                self.assertTrue(model.config.repetition_penalty is None)\n+                # 2 - That parameter will be set in `model.generation_config` instead.\n+                self.assertTrue(model.generation_config.repetition_penalty == 3.0)\n+        # 3 - The user will see a warning regarding the custom parameter that has been moved.\n+        self.assertTrue(len(warning_list) == 1)\n+        self.assertTrue(\"Moving the following attributes\" in str(warning_list[0].message))\n+        self.assertTrue(\"repetition_penalty\" in str(warning_list[0].message))\n \n     @require_safetensors\n     def test_model_from_pretrained_from_mlx(self):"
        }
    ],
    "stats": {
        "total": 865,
        "additions": 195,
        "deletions": 670
    }
}