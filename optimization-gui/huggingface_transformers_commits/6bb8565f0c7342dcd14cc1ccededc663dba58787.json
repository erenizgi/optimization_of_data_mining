{
    "author": "BenjaminBossan",
    "message": "FIX FSDP plugin update for QLoRA (#36720)\n\nThe _fsdp_qlora_plugin_updates checks for LoraConfig but other PEFT\nmethods can also support quantized models, e.g. VeRA. Therefore, the\nisinstance check is now looking for PeftConfig in general.\n\nMoreover, the fsdp_plugin variable may be undefined in the 2nd if\ncondition, leading to an `UnboundLocalError` error. This is fixed by not\nassigning the variable at all.\n\nI checked for tests that may need updating but only found\ntest_fsdp_config_transformers_auto_wrap associated with this change.\nAFAICT, this test does not cover the changed code, since the test does\nnot start the training loop. Therefore, I haven't updated any tests. LMK\nif/how this fix should be tested.\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "6bb8565f0c7342dcd14cc1ccededc663dba58787",
    "files": [
        {
            "sha": "351b3f7ae85e8093d9c55ebe814bea7bbe5284b6",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/6bb8565f0c7342dcd14cc1ccededc663dba58787/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6bb8565f0c7342dcd14cc1ccededc663dba58787/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=6bb8565f0c7342dcd14cc1ccededc663dba58787",
            "patch": "@@ -5202,18 +5202,17 @@ def propagate_args_to_deepspeed(self, auto_find_batch_size=False):\n \n     def _fsdp_qlora_plugin_updates(self):\n         if self.is_fsdp_enabled and _is_peft_model(self.model):\n-            from peft import LoraConfig\n+            from peft import PeftConfig\n             from peft.utils.other import fsdp_auto_wrap_policy\n \n-            if isinstance(self.model.active_peft_config, LoraConfig):\n-                fsdp_plugin = self.accelerator.state.fsdp_plugin\n-                fsdp_plugin.auto_wrap_policy = fsdp_auto_wrap_policy(self.model)\n+            if isinstance(self.model.active_peft_config, PeftConfig):\n+                self.accelerator.state.fsdp_plugin.auto_wrap_policy = fsdp_auto_wrap_policy(self.model)\n             if (\n                 getattr(self.model, \"quantization_method\", None) == QuantizationMethod.BITS_AND_BYTES\n                 and self.model.hf_quantizer.quantization_config.bnb_4bit_quant_storage.is_floating_point\n                 and version.parse(accelerate_version) > version.parse(\"0.27.0\")\n             ):\n-                fsdp_plugin.set_mixed_precision(\n+                self.accelerator.state.fsdp_plugin.set_mixed_precision(\n                     self.model.hf_quantizer.quantization_config.bnb_4bit_quant_storage, override=True\n                 )\n "
        }
    ],
    "stats": {
        "total": 9,
        "additions": 4,
        "deletions": 5
    }
}