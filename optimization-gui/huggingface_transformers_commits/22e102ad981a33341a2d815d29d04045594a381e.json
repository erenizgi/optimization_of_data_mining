{
    "author": "VladOS95-cyber",
    "message": "Bug fix gguf qwen2moe (#33940)\n\n* fix qwen2moe tensors mapping, add unit tests\r\n\r\n* add expert tensor split logic, test refactoring\r\n\r\n* small params refactoring\r\n\r\n* add comment to tensor reshaping",
    "sha": "22e102ad981a33341a2d815d29d04045594a381e",
    "files": [
        {
            "sha": "0d23751067742f672051b7c82cbfb22645c76e79",
            "filename": "src/transformers/integrations/ggml.py",
            "status": "modified",
            "additions": 10,
            "deletions": 3,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e102ad981a33341a2d815d29d04045594a381e/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e102ad981a33341a2d815d29d04045594a381e/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fggml.py?ref=22e102ad981a33341a2d815d29d04045594a381e",
            "patch": "@@ -82,10 +82,15 @@\n     \"qwen2moe\": {\n         \"token_embd\": \"model.embed_tokens\",\n         \"blk\": \"model.layers\",\n-        \"ffn_up\": \"mlp.up_proj\",\n-        \"ffn_down\": \"mlp.down_proj\",\n-        \"ffn_gate\": \"mlp.gate_proj\",\n+        \"ffn_up_exps\": \"mlp.experts\",\n+        \"ffn_up_shexp\": \"mlp.shared_expert.up_proj\",\n+        \"ffn_down_exps\": \"mlp.experts\",\n+        \"ffn_down_shexp\": \"mlp.shared_expert.down_proj\",\n         \"ffn_norm\": \"post_attention_layernorm\",\n+        \"ffn_gate_inp.weight\": \"mlp.gate.weight\",\n+        \"ffn_gate_exps\": \"mlp.experts\",\n+        \"ffn_gate_shexp\": \"mlp.shared_expert.gate_proj\",\n+        \"ffn_gate_inp_shexp\": \"mlp.shared_expert_gate\",\n         \"attn_norm\": \"input_layernorm\",\n         \"attn_q\": \"self_attn.q_proj\",\n         \"attn_v\": \"self_attn.v_proj\",\n@@ -200,6 +205,8 @@\n         \"attention.head_count_kv\": \"num_key_value_heads\",\n         \"attention.layer_norm_rms_epsilon\": \"rms_norm_eps\",\n         \"vocab_size\": \"vocab_size\",\n+        \"expert_count\": \"num_experts\",\n+        \"expert_used_count\": \"num_experts_per_tok\",\n     },\n     \"falcon\": {\n         \"context_length\": \"max_position_embeddings\","
        },
        {
            "sha": "0696413ef7603064ff57f88eaa09a44839d5127f",
            "filename": "src/transformers/modeling_gguf_pytorch_utils.py",
            "status": "modified",
            "additions": 33,
            "deletions": 0,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e102ad981a33341a2d815d29d04045594a381e/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e102ad981a33341a2d815d29d04045594a381e/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py?ref=22e102ad981a33341a2d815d29d04045594a381e",
            "patch": "@@ -174,6 +174,15 @@ def load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False):\n                 elif \".attn_k.\" in name:\n                     weights = reverse_permute_weights(weights, num_heads, num_kv_heads)\n \n+            if architecture == \"qwen2moe\":\n+                if \"_exp\" in name:\n+                    split_moe_expert_tensor(weights, parsed_parameters, name, tensor_key_mapping)\n+                    continue\n+                if \"ffn_gate_inp_shexp\" in name:\n+                    # for compatibility tensor shared_expert_gate must be (1, 2048) dim,\n+                    # quantized one is (2048)\n+                    weights = np.expand_dims(weights, axis=0)\n+\n             if architecture == \"bloom\" and \"attn_qkv\" in name:\n                 num_heads = parsed_parameters[\"config\"][\"n_head\"]\n                 n_embed = parsed_parameters[\"config\"][\"hidden_size\"]\n@@ -230,3 +239,27 @@ def reverse_reshape_bias(weights: np.ndarray, n_head: int, n_embed: int):\n \n     qkv_bias = np.stack([q_bias, k_bias, v_bias], axis=1).flatten()\n     return qkv_bias\n+\n+\n+def split_moe_expert_tensor(\n+    weights: np.ndarray, parsed_parameters: dict[str, dict], name: str, tensor_key_mapping: dict\n+):\n+    # Original merge implementation\n+    # https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py#L1994-L2022\n+    exp_name = \"\"\n+    if \"ffn_gate_exps\" in name:\n+        exp_name = \"gate_proj\"\n+    elif \"ffn_down_exps\" in name:\n+        exp_name = \"down_proj\"\n+    elif \"ffn_up_exps\" in name:\n+        exp_name = \"up_proj\"\n+    else:\n+        raise ValueError(f\"Cannot map expert tensor {name} in Qwen2Moe architecture.\")\n+    for tensor_name in tensor_key_mapping:\n+        if tensor_name in name:\n+            name = name.replace(tensor_name, tensor_key_mapping[tensor_name])\n+    w_counter = parsed_parameters[\"config\"].get(\"num_experts\", 60)\n+    for i in range(0, w_counter):\n+        temp_name = name.replace(\".weight\", f\".{i}.{exp_name}.weight\")\n+        exp_weight = weights[i]\n+        parsed_parameters[\"tensors\"][temp_name] = torch.from_numpy(np.copy(exp_weight))"
        },
        {
            "sha": "4e58600c50efb522a7a91d3a7802c4c7d12fbccf",
            "filename": "tests/quantization/ggml/test_ggml.py",
            "status": "modified",
            "additions": 30,
            "deletions": 10,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e102ad981a33341a2d815d29d04045594a381e/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e102ad981a33341a2d815d29d04045594a381e/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fggml%2Ftest_ggml.py?ref=22e102ad981a33341a2d815d29d04045594a381e",
            "patch": "@@ -38,7 +38,8 @@ class GgufIntegrationTests(unittest.TestCase):\n     imatrix_model_id = \"duyntnet/TinyLlama-1.1B-Chat-v1.0-imatrix-GGUF\"\n     mistral_model_id = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n     qwen2_model_id = \"Qwen/Qwen1.5-0.5B-Chat-GGUF\"\n-    qwen2_moe_model_id = \"RichardErkhov/Qwen_-_Qwen1.5-MoE-A2.7B-Chat-gguf\"\n+    qwen2moe_model_id = \"gdax/Qwen1.5-MoE-A2.7B_gguf\"\n+    qwen2moe_original_model_id = \"Qwen/Qwen1.5-MoE-A2.7B\"\n     llama3_model_id = \"NousResearch/Meta-Llama-3-8B-GGUF\"\n     tinyllama_model_id = \"PenutChen/TinyLlama-1.1B-Chat-v1.0-GGUF\"\n     phi3_model_id = \"microsoft/Phi-3-mini-4k-instruct-gguf\"\n@@ -72,14 +73,15 @@ class GgufIntegrationTests(unittest.TestCase):\n     q4_0_phi3_model_id = \"Phi-3-mini-4k-instruct-q4.gguf\"\n     q4_0_mistral_model_id = \"mistral-7b-instruct-v0.2.Q4_0.gguf\"\n     q4_0_qwen2_model_id = \"qwen1_5-0_5b-chat-q4_0.gguf\"\n-    q4_0_qwen2_moe_model_id = \"Qwen1.5-MoE-A2.7B-Chat.Q4_0.gguf\"\n+    q8_qwen2moe_model_id = \"Qwen1.5-MoE-A2.7B_Q8_0.gguf\"\n     q4_llama3_model_id = \"Meta-Llama-3-8B-Q4_K_M.gguf\"\n     fp16_bloom_model_id = \"bloom-560m.fp16.gguf\"\n     q8_bloom_model_id = \"bloom-560m.q8_0.gguf\"\n     f16_tinyllama_model_id = \"TinyLlama-1.1B-Chat-v1.0.FP16.gguf\"\n     q2_k_falcon7b_model_id = \"falcon-7b-q2_k.gguf\"\n     fp16_falcon7b_model_id = \"falcon-7b-fp16.gguf\"\n     q2_k_falcon40b_model_id = \"tiiuae-falcon-40b-Q2_K.gguf\"\n+    fp16_qwen2moe_model_id = \"Qwen1.5-MoE-A2.7B.gguf\"\n \n     example_text = \"Hello\"\n \n@@ -344,21 +346,39 @@ def test_qwen2_q4_0(self):\n         EXPECTED_TEXT = \"Hello.jsoup\\n\\nI am a beginner\"\n         self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n \n-    def test_qwen2_moe_q4_0(self):\n-        tokenizer = AutoTokenizer.from_pretrained(self.qwen2_moe_model_id, gguf_file=self.q4_0_qwen2_moe_model_id)\n+    def test_qwen2moe_q8(self):\n+        tokenizer = AutoTokenizer.from_pretrained(self.qwen2moe_model_id, gguf_file=self.q8_qwen2moe_model_id)\n         model = AutoModelForCausalLM.from_pretrained(\n-            self.qwen2_moe_model_id,\n-            gguf_file=self.q4_0_qwen2_moe_model_id,\n-            device_map=\"auto\",\n+            self.qwen2moe_model_id,\n+            gguf_file=self.q8_qwen2moe_model_id,\n             torch_dtype=torch.float16,\n         )\n \n-        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n+        text = tokenizer(self.example_text, return_tensors=\"pt\")\n         out = model.generate(**text, max_new_tokens=10)\n \n-        EXPECTED_TEXT = \"Hello everyone, I'm a newbie here and would like\"\n+        EXPECTED_TEXT = \"Hello, I am a 20 year old male\"\n         self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n \n+    def test_qwen2moe_weights_conversion_fp16(self):\n+        quantized_model = AutoModelForCausalLM.from_pretrained(\n+            self.qwen2moe_model_id,\n+            gguf_file=self.fp16_qwen2moe_model_id,\n+            torch_dtype=torch.float16,\n+        )\n+        original_model = AutoModelForCausalLM.from_pretrained(\n+            self.qwen2moe_original_model_id,\n+            torch_dtype=torch.float16,\n+        )\n+\n+        quantized_state_dict = quantized_model.state_dict()\n+        original_state_dict = original_model.state_dict()\n+\n+        for layer_name, original_params in original_state_dict.items():\n+            if layer_name in quantized_state_dict:\n+                self.assertTrue(original_params.shape == quantized_state_dict[layer_name].shape)\n+                torch.testing.assert_close(original_params, quantized_state_dict[layer_name])\n+\n     def test_phi3_q4_0(self):\n         tokenizer = AutoTokenizer.from_pretrained(self.phi3_model_id, gguf_file=self.q4_0_phi3_model_id)\n         model = AutoModelForCausalLM.from_pretrained(\n@@ -422,7 +442,7 @@ def test_bloom_q8_0(self):\n         text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n         out = model.generate(**text, max_new_tokens=10)\n \n-        EXPECTED_TEXT = \"Hello, I just want to say that I am very\"\n+        EXPECTED_TEXT = \"Hello, I just want to say that I am just\"\n         self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n \n     def test_bloom_weights_conversion_fp16(self):"
        }
    ],
    "stats": {
        "total": 86,
        "additions": 73,
        "deletions": 13
    }
}