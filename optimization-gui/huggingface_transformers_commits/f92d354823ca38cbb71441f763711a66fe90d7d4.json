{
    "author": "ArthurZucker",
    "message": "fix red check-copies (#33964)",
    "sha": "f92d354823ca38cbb71441f763711a66fe90d7d4",
    "files": [
        {
            "sha": "cba66aa3a81129169273e8ac2b97b0f41dcaca93",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f92d354823ca38cbb71441f763711a66fe90d7d4/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f92d354823ca38cbb71441f763711a66fe90d7d4/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=f92d354823ca38cbb71441f763711a66fe90d7d4",
            "patch": "@@ -46,7 +46,7 @@\n _CONFIG_FOR_DOC = \"PaliGemmaConfig\"\n \n \n-# Adapted from transformers.models.llama.modeling_llama._prepare_4d_causal_attention_mask_with_cache_position\n+# Adapted from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n # But Paligemma has no causal mask on prefix\n def _prepare_4d_causal_attention_mask_with_cache_position(\n     attention_mask: torch.Tensor,"
        },
        {
            "sha": "cacaaf6ac35ab1922f6decc63b5020bbe2c4d9e7",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f92d354823ca38cbb71441f763711a66fe90d7d4/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f92d354823ca38cbb71441f763711a66fe90d7d4/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=f92d354823ca38cbb71441f763711a66fe90d7d4",
            "patch": "@@ -283,7 +283,7 @@ def resize_token_embeddings(self, new_num_tokens: Optional[int] = None, pad_to_m\n         return model_embeds\n \n     # Ignore copy\n-    def get_image_features(self, pixel_values: torch.FloatTensor, vision_feature_layers: list[int]):\n+    def get_image_features(self, pixel_values: torch.FloatTensor, vision_feature_layers: List[int]):\n         image_outputs = self.vision_tower(pixel_values, output_hidden_states=True)\n \n         # For VIP-llava, the image features are computed this way"
        },
        {
            "sha": "2363ed04959d00b68568edcb325fb5f5d4ca630c",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 54,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/f92d354823ca38cbb71441f763711a66fe90d7d4/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f92d354823ca38cbb71441f763711a66fe90d7d4/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=f92d354823ca38cbb71441f763711a66fe90d7d4",
            "patch": "@@ -77,60 +77,6 @@\n _CONFIG_FOR_DOC = \"ZambaConfig\"\n \n \n-# Copied from transformers.models.llama.modeling_llama._prepare_4d_causal_attention_mask_with_cache_position\n-def _prepare_4d_causal_attention_mask_with_cache_position(\n-    attention_mask: torch.Tensor,\n-    sequence_length: int,\n-    target_length: int,\n-    dtype: torch.dtype,\n-    device: torch.device,\n-    min_dtype: float,\n-    cache_position: torch.Tensor,\n-    batch_size: int,\n-):\n-    \"\"\"\n-    Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-    `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-    Args:\n-        attention_mask (`torch.Tensor`):\n-            A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n-        sequence_length (`int`):\n-            The sequence length being processed.\n-        target_length (`int`):\n-            The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n-        dtype (`torch.dtype`):\n-            The dtype to use for the 4D attention mask.\n-        device (`torch.device`):\n-            The device to plcae the 4D attention mask on.\n-        min_dtype (`float`):\n-            The minimum value representable with the dtype `dtype`.\n-        cache_position (`torch.Tensor`):\n-            Indices depicting the position of the input sequence tokens in the sequence.\n-        batch_size (`torch.Tensor`):\n-            Batch size.\n-    \"\"\"\n-    if attention_mask is not None and attention_mask.dim() == 4:\n-        # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-        causal_mask = attention_mask\n-    else:\n-        causal_mask = torch.full((sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device)\n-        if sequence_length != 1:\n-            causal_mask = torch.triu(causal_mask, diagonal=1)\n-        causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n-        causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-        if attention_mask is not None:\n-            causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-            mask_length = attention_mask.shape[-1]\n-            padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n-            padding_mask = padding_mask == 0\n-            causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                padding_mask, min_dtype\n-            )\n-\n-    return causal_mask\n-\n-\n # Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->Zamba\n class ZambaRMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):"
        },
        {
            "sha": "881967076e7ed373c5f588b6f10096ac83e83c50",
            "filename": "tests/models/phimoe/test_modeling_phimoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f92d354823ca38cbb71441f763711a66fe90d7d4/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f92d354823ca38cbb71441f763711a66fe90d7d4/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py?ref=f92d354823ca38cbb71441f763711a66fe90d7d4",
            "patch": "@@ -154,7 +154,7 @@ def prepare_config_and_inputs(self):\n \n         input_mask = None\n         if self.use_input_mask:\n-            input_mask = torch.tril(torch.ones(self.batch_size, self.seq_length)).to(torch_device)\n+            input_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n \n         token_type_ids = None\n         if self.use_token_type_ids:"
        }
    ],
    "stats": {
        "total": 60,
        "additions": 3,
        "deletions": 57
    }
}