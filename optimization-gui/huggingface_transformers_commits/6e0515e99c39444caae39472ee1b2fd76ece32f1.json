{
    "author": "NielsRogge",
    "message": "Add DINOv2 with registers (#35348)\n\n* added changes from 32905\r\n\r\n* fixed mistakes caused by select all paste\r\n\r\n* rename diff_dinov2...\r\n\r\n* ran tests\r\n\r\n* Fix modular\r\n\r\n* Fix tests\r\n\r\n* Use new init\r\n\r\n* Simplify drop path\r\n\r\n* Convert all checkpoints\r\n\r\n* Add figure and summary\r\n\r\n* Update paths\r\n\r\n* Update docs\r\n\r\n* Update docs\r\n\r\n* Update toctree\r\n\r\n* Update docs\r\n\r\n---------\r\n\r\nCo-authored-by: BernardZach <bernardzach00@gmail.com>\r\nCo-authored-by: Zach Bernard <132859071+BernardZach@users.noreply.github.com>",
    "sha": "6e0515e99c39444caae39472ee1b2fd76ece32f1",
    "files": [
        {
            "sha": "a076f704b8ede259a393619c7847a710e16c2b19",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6e0515e99c39444caae39472ee1b2fd76ece32f1/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/6e0515e99c39444caae39472ee1b2fd76ece32f1/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=6e0515e99c39444caae39472ee1b2fd76ece32f1",
            "patch": "@@ -655,6 +655,8 @@\n         title: DiNAT\n       - local: model_doc/dinov2\n         title: DINOV2\n+      - local: model_doc/dinov2_with_registers\n+        title: DINOv2 with Registers\n       - local: model_doc/dit\n         title: DiT\n       - local: model_doc/dpt"
        },
        {
            "sha": "dcecfc872d61d0503349aba49f0546b1b65c01ce",
            "filename": "docs/source/en/index.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6e0515e99c39444caae39472ee1b2fd76ece32f1/docs%2Fsource%2Fen%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6e0515e99c39444caae39472ee1b2fd76ece32f1/docs%2Fsource%2Fen%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Findex.md?ref=6e0515e99c39444caae39472ee1b2fd76ece32f1",
            "patch": "@@ -127,6 +127,7 @@ Flax), PyTorch, and/or TensorFlow.\n |                      [DialoGPT](model_doc/dialogpt)                      |       ‚úÖ        |         ‚úÖ         |      ‚úÖ      |\n |                         [DiNAT](model_doc/dinat)                         |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n |                        [DINOv2](model_doc/dinov2)                        |       ‚úÖ        |         ‚ùå         |      ‚úÖ      |\n+|         [DINOv2 with Registers](model_doc/dinov2_with_registers)         |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n |                    [DistilBERT](model_doc/distilbert)                    |       ‚úÖ        |         ‚úÖ         |      ‚úÖ      |\n |                           [DiT](model_doc/dit)                           |       ‚úÖ        |         ‚ùå         |      ‚úÖ      |\n |                       [DonutSwin](model_doc/donut)                       |       ‚úÖ        |         ‚ùå         |      ‚ùå      |"
        },
        {
            "sha": "360ebf9b8f8a1569ea8f25724ecf57b0c3f09bef",
            "filename": "docs/source/en/model_doc/dinov2_with_registers.md",
            "status": "added",
            "additions": 54,
            "deletions": 0,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/6e0515e99c39444caae39472ee1b2fd76ece32f1/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov2_with_registers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6e0515e99c39444caae39472ee1b2fd76ece32f1/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov2_with_registers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov2_with_registers.md?ref=6e0515e99c39444caae39472ee1b2fd76ece32f1",
            "patch": "@@ -0,0 +1,54 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+http://www.apache.org/licenses/LICENSE-2.0\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+-->\n+\n+# DINOv2 with Registers\n+\n+## Overview\n+\n+The DINOv2 with Registers model was proposed in [Vision Transformers Need Registers](https://arxiv.org/abs/2309.16588) by Timoth√©e Darcet, Maxime Oquab, Julien Mairal, Piotr Bojanowski.\n+\n+The [Vision Transformer](vit) (ViT) is a transformer encoder model (BERT-like) originally introduced to do supervised image classification on ImageNet.\n+\n+Next, people figured out ways to make ViT work really well on self-supervised image feature extraction (i.e. learning meaningful features, also called embeddings) on images without requiring any labels. Some example papers here include [DINOv2](dinov2) and [MAE](vit_mae).\n+\n+The authors of DINOv2 noticed that ViTs have artifacts in attention maps. It‚Äôs due to the model using some image patches as ‚Äúregisters‚Äù. The authors propose a fix: just add some new tokens (called \"register\" tokens), which you only use during pre-training (and throw away afterwards). This results in:\n+- no artifacts\n+- interpretable attention maps\n+- and improved performances.\n+\n+The abstract from the paper is the following:\n+\n+*Transformers have recently emerged as a powerful tool for learning visual representations. In this paper, we identify and characterize artifacts in feature maps of both supervised and self-supervised ViT networks. The artifacts correspond to high-norm tokens appearing during inference primarily in low-informative background areas of images, that are repurposed for internal computations. We propose a simple yet effective solution based on providing additional tokens to the input sequence of the Vision Transformer to fill that role. We show that this solution fixes that problem entirely for both supervised and self-supervised models, sets a new state of the art for self-supervised visual models on dense visual prediction tasks, enables object discovery methods with larger models, and most importantly leads to smoother feature maps and attention maps for downstream visual processing.*\n+\n+<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/dinov2_with_registers_visualization.png\"\n+alt=\"drawing\" width=\"600\"/>\n+\n+<small> Visualization of attention maps of various models trained with vs. without registers. Taken from the <a href=\"https://arxiv.org/abs/2309.16588\">original paper</a>. </small>\n+\n+Tips:\n+\n+- Usage of DINOv2 with Registers is identical to DINOv2 without, you'll just get better performance.\n+\n+This model was contributed by [nielsr](https://huggingface.co/nielsr).\n+The original code can be found [here](https://github.com/facebookresearch/dinov2).\n+\n+\n+## Dinov2WithRegistersConfig\n+\n+[[autodoc]] Dinov2WithRegistersConfig\n+\n+## Dinov2WithRegistersModel\n+\n+[[autodoc]] Dinov2WithRegistersModel\n+    - forward\n+\n+## Dinov2WithRegistersForImageClassification\n+\n+[[autodoc]] Dinov2WithRegistersForImageClassification\n+    - forward\n\\ No newline at end of file"
        },
        {
            "sha": "d79450964180e7611992f3d39415aa3a87bd51fe",
            "filename": "docs/source/en/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6e0515e99c39444caae39472ee1b2fd76ece32f1/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6e0515e99c39444caae39472ee1b2fd76ece32f1/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md?ref=6e0515e99c39444caae39472ee1b2fd76ece32f1",
            "patch": "@@ -238,6 +238,7 @@ For now, Transformers supports SDPA inference and training for the following arc\n * [Dbrx](https://huggingface.co/docs/transformers/model_doc/dbrx#transformers.DbrxModel)\n * [DeiT](https://huggingface.co/docs/transformers/model_doc/deit#transformers.DeiTModel)\n * [Dinov2](https://huggingface.co/docs/transformers/en/model_doc/dinov2)\n+* [Dinov2_with_registers](https://huggingface.co/docs/transformers/en/model_doc/dinov2)\n * [DistilBert](https://huggingface.co/docs/transformers/model_doc/distilbert#transformers.DistilBertModel)\n * [Dpr](https://huggingface.co/docs/transformers/model_doc/dpr#transformers.DprReader)\n * [EncoderDecoder](https://huggingface.co/docs/transformers/model_doc/encoder_decoder#transformers.EncoderDecoderModel)"
        },
        {
            "sha": "7df1af049de6260df58c73fa4c1f3e1298129c11",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/6e0515e99c39444caae39472ee1b2fd76ece32f1/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6e0515e99c39444caae39472ee1b2fd76ece32f1/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=6e0515e99c39444caae39472ee1b2fd76ece32f1",
            "patch": "@@ -404,6 +404,7 @@\n     \"models.dialogpt\": [],\n     \"models.dinat\": [\"DinatConfig\"],\n     \"models.dinov2\": [\"Dinov2Config\"],\n+    \"models.dinov2_with_registers\": [\"Dinov2WithRegistersConfig\"],\n     \"models.distilbert\": [\n         \"DistilBertConfig\",\n         \"DistilBertTokenizer\",\n@@ -2160,6 +2161,14 @@\n             \"Dinov2PreTrainedModel\",\n         ]\n     )\n+    _import_structure[\"models.dinov2_with_registers\"].extend(\n+        [\n+            \"Dinov2WithRegistersBackbone\",\n+            \"Dinov2WithRegistersForImageClassification\",\n+            \"Dinov2WithRegistersModel\",\n+            \"Dinov2WithRegistersPreTrainedModel\",\n+        ]\n+    )\n     _import_structure[\"models.distilbert\"].extend(\n         [\n             \"DistilBertForMaskedLM\",\n@@ -5362,6 +5371,7 @@\n     from .models.detr import DetrConfig\n     from .models.dinat import DinatConfig\n     from .models.dinov2 import Dinov2Config\n+    from .models.dinov2_with_registers import Dinov2WithRegistersConfig\n     from .models.distilbert import (\n         DistilBertConfig,\n         DistilBertTokenizer,\n@@ -7019,6 +7029,12 @@\n             Dinov2Model,\n             Dinov2PreTrainedModel,\n         )\n+        from .models.dinov2_with_registers import (\n+            Dinov2WithRegistersBackbone,\n+            Dinov2WithRegistersForImageClassification,\n+            Dinov2WithRegistersModel,\n+            Dinov2WithRegistersPreTrainedModel,\n+        )\n         from .models.distilbert import (\n             DistilBertForMaskedLM,\n             DistilBertForMultipleChoice,"
        },
        {
            "sha": "ff03d09966a4d694fe80002f7ae1b8f52680bc76",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6e0515e99c39444caae39472ee1b2fd76ece32f1/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6e0515e99c39444caae39472ee1b2fd76ece32f1/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=6e0515e99c39444caae39472ee1b2fd76ece32f1",
            "patch": "@@ -77,6 +77,7 @@\n     dialogpt,\n     dinat,\n     dinov2,\n+    dinov2_with_registers,\n     distilbert,\n     dit,\n     donut,"
        },
        {
            "sha": "6c052aa0eaa0f36083207f9280ef8ed546504ab6",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6e0515e99c39444caae39472ee1b2fd76ece32f1/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6e0515e99c39444caae39472ee1b2fd76ece32f1/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=6e0515e99c39444caae39472ee1b2fd76ece32f1",
            "patch": "@@ -94,6 +94,7 @@\n         (\"detr\", \"DetrConfig\"),\n         (\"dinat\", \"DinatConfig\"),\n         (\"dinov2\", \"Dinov2Config\"),\n+        (\"dinov2_with_registers\", \"Dinov2WithRegistersConfig\"),\n         (\"distilbert\", \"DistilBertConfig\"),\n         (\"donut-swin\", \"DonutSwinConfig\"),\n         (\"dpr\", \"DPRConfig\"),\n@@ -404,6 +405,7 @@\n         (\"dialogpt\", \"DialoGPT\"),\n         (\"dinat\", \"DiNAT\"),\n         (\"dinov2\", \"DINOv2\"),\n+        (\"dinov2_with_registers\", \"DINOv2 with Registers\"),\n         (\"distilbert\", \"DistilBERT\"),\n         (\"dit\", \"DiT\"),\n         (\"donut-swin\", \"DonutSwin\"),"
        },
        {
            "sha": "861754f591769bb1295bb733a84fe18292da2416",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6e0515e99c39444caae39472ee1b2fd76ece32f1/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6e0515e99c39444caae39472ee1b2fd76ece32f1/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=6e0515e99c39444caae39472ee1b2fd76ece32f1",
            "patch": "@@ -92,6 +92,7 @@\n         (\"detr\", \"DetrModel\"),\n         (\"dinat\", \"DinatModel\"),\n         (\"dinov2\", \"Dinov2Model\"),\n+        (\"dinov2_with_registers\", \"Dinov2WithRegistersModel\"),\n         (\"distilbert\", \"DistilBertModel\"),\n         (\"donut-swin\", \"DonutSwinModel\"),\n         (\"dpr\", \"DPRQuestionEncoder\"),\n@@ -584,6 +585,7 @@\n         (\"detr\", \"DetrModel\"),\n         (\"dinat\", \"DinatModel\"),\n         (\"dinov2\", \"Dinov2Model\"),\n+        (\"dinov2_with_registers\", \"Dinov2WithRegistersModel\"),\n         (\"dpt\", \"DPTModel\"),\n         (\"efficientformer\", \"EfficientFormerModel\"),\n         (\"efficientnet\", \"EfficientNetModel\"),\n@@ -659,6 +661,7 @@\n         ),\n         (\"dinat\", \"DinatForImageClassification\"),\n         (\"dinov2\", \"Dinov2ForImageClassification\"),\n+        (\"dinov2_with_registers\", \"Dinov2WithRegistersForImageClassification\"),\n         (\n             \"efficientformer\",\n             (\n@@ -1373,6 +1376,7 @@\n         (\"convnextv2\", \"ConvNextV2Backbone\"),\n         (\"dinat\", \"DinatBackbone\"),\n         (\"dinov2\", \"Dinov2Backbone\"),\n+        (\"dinov2_with_registers\", \"Dinov2WithRegistersBackbone\"),\n         (\"focalnet\", \"FocalNetBackbone\"),\n         (\"hiera\", \"HieraBackbone\"),\n         (\"maskformer-swin\", \"MaskFormerSwinBackbone\"),"
        },
        {
            "sha": "2d10027b6a3b6375235a6785df044e8f0ce5fb33",
            "filename": "src/transformers/models/dinov2_with_registers/__init__.py",
            "status": "added",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/6e0515e99c39444caae39472ee1b2fd76ece32f1/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6e0515e99c39444caae39472ee1b2fd76ece32f1/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2F__init__.py?ref=6e0515e99c39444caae39472ee1b2fd76ece32f1",
            "patch": "@@ -0,0 +1,27 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_dinov2_with_registers import *\n+    from .modeling_dinov2_with_registers import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "80c095cb4648383ef73d404381abe6ab23d0de8d",
            "filename": "src/transformers/models/dinov2_with_registers/configuration_dinov2_with_registers.py",
            "status": "added",
            "additions": 166,
            "deletions": 0,
            "changes": 166,
            "blob_url": "https://github.com/huggingface/transformers/blob/6e0515e99c39444caae39472ee1b2fd76ece32f1/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fconfiguration_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6e0515e99c39444caae39472ee1b2fd76ece32f1/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fconfiguration_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fconfiguration_dinov2_with_registers.py?ref=6e0515e99c39444caae39472ee1b2fd76ece32f1",
            "patch": "@@ -0,0 +1,166 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/dinov2_with_registers/modular_dinov2_with_registers.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_dinov2_with_registers.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# coding=utf-8\n+# Copyright 2024 Meta Inc. and the HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from ...configuration_utils import PretrainedConfig\n+from ...utils.backbone_utils import BackboneConfigMixin, get_aligned_output_features_output_indices\n+\n+\n+class Dinov2WithRegistersConfig(BackboneConfigMixin, PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Dinov2WithRegistersModel`]. It is used to instantiate an\n+    Dinov2WithRegisters model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the DINOv2 with Registers\n+    [facebook/dinov2-with-registers-base](https://huggingface.co/facebook/dinov2-with-registers-base) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 768):\n+            Dimensionality of the encoder layers and the pooler layer.\n+        num_hidden_layers (`int`, *optional*, defaults to 12):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 12):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        mlp_ratio (`int`, *optional*, defaults to 4):\n+            Ratio of the hidden size of the MLPs relative to the `hidden_size`.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` are supported.\n+        hidden_dropout_prob (`float`, *optional*, defaults to 0.0):\n+            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n+        attention_probs_dropout_prob (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the layer normalization layers.\n+        image_size (`int`, *optional*, defaults to 224):\n+            The size (resolution) of each image.\n+        patch_size (`int`, *optional*, defaults to 16):\n+            The size (resolution) of each patch.\n+        num_channels (`int`, *optional*, defaults to 3):\n+            The number of input channels.\n+        qkv_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to add a bias to the queries, keys and values.\n+        layerscale_value (`float`, *optional*, defaults to 1.0):\n+           Initial value to use for layer scale.\n+        drop_path_rate (`float`, *optional*, defaults to 0.0):\n+            Stochastic depth rate per sample (when applied in the main path of residual layers).\n+        use_swiglu_ffn (`bool`, *optional*, defaults to `False`):\n+            Whether to use the SwiGLU feedforward neural network.\n+        num_register_tokens (`int`, *optional*, defaults to 4):\n+            Number of register tokens to use.\n+        interpolate_antialias (`bool`, *optional*, defaults to `True`):\n+            Whether to use antialiasing when interpolating the image patches.\n+        interpolate_offset (`float`, *optional*, defaults to 0.0):\n+            Offset to use when interpolating the image patches.\n+        out_features (`List[str]`, *optional*):\n+            If used as backbone, list of features to output. Can be any of `\"stem\"`, `\"stage1\"`, `\"stage2\"`, etc.\n+            (depending on how many stages the model has). If unset and `out_indices` is set, will default to the\n+            corresponding stages. If unset and `out_indices` is unset, will default to the last stage. Must be in the\n+            same order as defined in the `stage_names` attribute.\n+        out_indices (`List[int]`, *optional*):\n+            If used as backbone, list of indices of features to output. Can be any of 0, 1, 2, etc. (depending on how\n+            many stages the model has). If unset and `out_features` is set, will default to the corresponding stages.\n+            If unset and `out_features` is unset, will default to the last stage. Must be in the\n+            same order as defined in the `stage_names` attribute.\n+        apply_layernorm (`bool`, *optional*, defaults to `True`):\n+            Whether to apply layer normalization to the feature maps in case the model is used as backbone.\n+        reshape_hidden_states (`bool`, *optional*, defaults to `True`):\n+            Whether to reshape the feature maps to 4D tensors of shape `(batch_size, hidden_size, height, width)` in\n+            case the model is used as backbone. If `False`, the feature maps will be 3D tensors of shape `(batch_size,\n+            seq_len, hidden_size)`.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import Dinov2WithRegistersConfig, Dinov2WithRegistersModel\n+\n+    >>> # Initializing a Dinov2WithRegisters base style configuration\n+    >>> configuration = Dinov2WithRegistersConfig()\n+\n+    >>> # Initializing a model (with random weights) from the base style configuration\n+    >>> model = Dinov2WithRegistersModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"dinov2-with-registers-base\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=768,\n+        num_hidden_layers=12,\n+        num_attention_heads=12,\n+        mlp_ratio=4,\n+        hidden_act=\"gelu\",\n+        hidden_dropout_prob=0.0,\n+        attention_probs_dropout_prob=0.0,\n+        initializer_range=0.02,\n+        layer_norm_eps=1e-6,\n+        image_size=224,\n+        patch_size=16,\n+        num_channels=3,\n+        qkv_bias=True,\n+        layerscale_value=1.0,\n+        drop_path_rate=0.0,\n+        use_swiglu_ffn=False,\n+        num_register_tokens=4,\n+        interpolate_antialias=True,\n+        interpolate_offset=0.0,\n+        out_features=None,\n+        out_indices=None,\n+        apply_layernorm=True,\n+        reshape_hidden_states=True,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.mlp_ratio = mlp_ratio\n+        self.hidden_act = hidden_act\n+        self.hidden_dropout_prob = hidden_dropout_prob\n+        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n+        self.initializer_range = initializer_range\n+        self.layer_norm_eps = layer_norm_eps\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.num_channels = num_channels\n+        self.qkv_bias = qkv_bias\n+        self.layerscale_value = layerscale_value\n+        self.drop_path_rate = drop_path_rate\n+        self.use_swiglu_ffn = use_swiglu_ffn\n+        self.num_register_tokens = num_register_tokens\n+        self.interpolate_antialias = interpolate_antialias\n+        self.interpolate_offset = interpolate_offset\n+        self.stage_names = [\"stem\"] + [f\"stage{idx}\" for idx in range(1, num_hidden_layers + 1)]\n+        self._out_features, self._out_indices = get_aligned_output_features_output_indices(\n+            out_features=out_features, out_indices=out_indices, stage_names=self.stage_names\n+        )\n+        self.apply_layernorm = apply_layernorm\n+        self.reshape_hidden_states = reshape_hidden_states\n+\n+\n+__all__ = [\"Dinov2WithRegistersConfig\"]"
        },
        {
            "sha": "0ff2697f74667e1b941ec65adb1a39cfd0a87460",
            "filename": "src/transformers/models/dinov2_with_registers/convert_dinov2_with_registers_to_hf.py",
            "status": "added",
            "additions": 291,
            "deletions": 0,
            "changes": 291,
            "blob_url": "https://github.com/huggingface/transformers/blob/6e0515e99c39444caae39472ee1b2fd76ece32f1/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fconvert_dinov2_with_registers_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6e0515e99c39444caae39472ee1b2fd76ece32f1/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fconvert_dinov2_with_registers_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fconvert_dinov2_with_registers_to_hf.py?ref=6e0515e99c39444caae39472ee1b2fd76ece32f1",
            "patch": "@@ -0,0 +1,291 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Convert DINOv2 with Registers checkpoints from the original repository.\n+\n+URL: https://github.com/facebookresearch/dinov2/tree/main\n+\"\"\"\n+\n+import argparse\n+import json\n+from pathlib import Path\n+\n+import requests\n+import torch\n+import torch.nn as nn\n+from huggingface_hub import hf_hub_download\n+from PIL import Image\n+from torchvision import transforms\n+\n+from transformers import (\n+    BitImageProcessor,\n+    Dinov2WithRegistersConfig,\n+    Dinov2WithRegistersForImageClassification,\n+    Dinov2WithRegistersModel,\n+)\n+from transformers.image_utils import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, PILImageResampling\n+from transformers.utils import logging\n+\n+\n+logging.set_verbosity_info()\n+logger = logging.get_logger(__name__)\n+\n+\n+def get_dinov2_with_registers_config(model_name, image_classifier=False):\n+    config = Dinov2WithRegistersConfig(image_size=518, patch_size=14)\n+\n+    # size of the architecture\n+    if \"vits\" in model_name:\n+        config.hidden_size = 384\n+        config.num_attention_heads = 6\n+    elif \"vitb\" in model_name:\n+        pass\n+    elif \"vitl\" in model_name:\n+        config.hidden_size = 1024\n+        config.num_hidden_layers = 24\n+        config.num_attention_heads = 16\n+    elif \"vitg\" in model_name:\n+        config.use_swiglu_ffn = True\n+        config.hidden_size = 1536\n+        config.num_hidden_layers = 40\n+        config.num_attention_heads = 24\n+    else:\n+        raise ValueError(\"Model not supported\")\n+\n+    if image_classifier:\n+        repo_id = \"huggingface/label-files\"\n+        filename = \"imagenet-1k-id2label.json\"\n+        config.num_labels = 1000\n+        config.id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type=\"dataset\"), \"r\"))\n+        config.id2label = {int(k): v for k, v in config.id2label.items()}\n+\n+    return config\n+\n+\n+def create_rename_keys(config):\n+    rename_keys = []\n+    # fmt: off\n+\n+    # patch embedding layer\n+    rename_keys.append((\"cls_token\", \"embeddings.cls_token\"))\n+    rename_keys.append((\"mask_token\", \"embeddings.mask_token\"))\n+    rename_keys.append((\"pos_embed\", \"embeddings.position_embeddings\"))\n+    rename_keys.append((\"register_tokens\", \"embeddings.register_tokens\"))\n+    rename_keys.append((\"patch_embed.proj.weight\", \"embeddings.patch_embeddings.projection.weight\"))\n+    rename_keys.append((\"patch_embed.proj.bias\", \"embeddings.patch_embeddings.projection.bias\"))\n+\n+    for i in range(config.num_hidden_layers):\n+        # layernorms\n+        rename_keys.append((f\"blocks.{i}.norm1.weight\", f\"encoder.layer.{i}.norm1.weight\"))\n+        rename_keys.append((f\"blocks.{i}.norm1.bias\", f\"encoder.layer.{i}.norm1.bias\"))\n+        rename_keys.append((f\"blocks.{i}.norm2.weight\", f\"encoder.layer.{i}.norm2.weight\"))\n+        rename_keys.append((f\"blocks.{i}.norm2.bias\", f\"encoder.layer.{i}.norm2.bias\"))\n+        # MLP\n+        if config.use_swiglu_ffn:\n+            rename_keys.append((f\"blocks.{i}.mlp.w12.weight\", f\"encoder.layer.{i}.mlp.w12.weight\"))\n+            rename_keys.append((f\"blocks.{i}.mlp.w12.bias\", f\"encoder.layer.{i}.mlp.w12.bias\"))\n+            rename_keys.append((f\"blocks.{i}.mlp.w3.weight\", f\"encoder.layer.{i}.mlp.w3.weight\"))\n+            rename_keys.append((f\"blocks.{i}.mlp.w3.bias\", f\"encoder.layer.{i}.mlp.w3.bias\"))\n+        else:\n+            rename_keys.append((f\"blocks.{i}.mlp.fc1.weight\", f\"encoder.layer.{i}.mlp.fc1.weight\"))\n+            rename_keys.append((f\"blocks.{i}.mlp.fc1.bias\", f\"encoder.layer.{i}.mlp.fc1.bias\"))\n+            rename_keys.append((f\"blocks.{i}.mlp.fc2.weight\", f\"encoder.layer.{i}.mlp.fc2.weight\"))\n+            rename_keys.append((f\"blocks.{i}.mlp.fc2.bias\", f\"encoder.layer.{i}.mlp.fc2.bias\"))\n+        # layerscale\n+        rename_keys.append((f\"blocks.{i}.ls1.gamma\", f\"encoder.layer.{i}.layer_scale1.lambda1\"))\n+        rename_keys.append((f\"blocks.{i}.ls2.gamma\", f\"encoder.layer.{i}.layer_scale2.lambda1\"))\n+        # attention projection layer\n+        rename_keys.append((f\"blocks.{i}.attn.proj.weight\", f\"encoder.layer.{i}.attention.output.dense.weight\"))\n+        rename_keys.append((f\"blocks.{i}.attn.proj.bias\", f\"encoder.layer.{i}.attention.output.dense.bias\"))\n+\n+    # final layernorm\n+    rename_keys.append((\"norm.weight\", \"layernorm.weight\"))\n+    rename_keys.append((\"norm.bias\", \"layernorm.bias\"))\n+\n+    # fmt: on\n+    return rename_keys\n+\n+\n+def rename_key(dct, old, new):\n+    val = dct.pop(old)\n+    dct[new] = val\n+\n+\n+# we split up the matrix of each encoder layer into queries, keys and values\n+def read_in_q_k_v(state_dict, config):\n+    for i in range(config.num_hidden_layers):\n+        # read in weights + bias of input projection layer (in timm, this is a single matrix + bias)\n+        in_proj_weight = state_dict.pop(f\"blocks.{i}.attn.qkv.weight\")\n+        in_proj_bias = state_dict.pop(f\"blocks.{i}.attn.qkv.bias\")\n+        # next, add query, keys and values (in that order) to the state dict\n+        state_dict[f\"encoder.layer.{i}.attention.attention.query.weight\"] = in_proj_weight[: config.hidden_size, :]\n+        state_dict[f\"encoder.layer.{i}.attention.attention.query.bias\"] = in_proj_bias[: config.hidden_size]\n+        state_dict[f\"encoder.layer.{i}.attention.attention.key.weight\"] = in_proj_weight[\n+            config.hidden_size : config.hidden_size * 2, :\n+        ]\n+        state_dict[f\"encoder.layer.{i}.attention.attention.key.bias\"] = in_proj_bias[\n+            config.hidden_size : config.hidden_size * 2\n+        ]\n+        state_dict[f\"encoder.layer.{i}.attention.attention.value.weight\"] = in_proj_weight[-config.hidden_size :, :]\n+        state_dict[f\"encoder.layer.{i}.attention.attention.value.bias\"] = in_proj_bias[-config.hidden_size :]\n+\n+\n+# We will verify our results on an image of cute cats\n+def prepare_img():\n+    url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+    image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n+    return image\n+\n+\n+@torch.no_grad()\n+def convert_dinov2_with_registers_checkpoint(model_name, pytorch_dump_folder_path, push_to_hub=False):\n+    \"\"\"\n+    Copy/paste/tweak model's weights to our Dinov2WithRegisters structure.\n+    \"\"\"\n+\n+    # define default Dinov2WithRegisters configuration\n+    image_classifier = \"1layer\" in model_name\n+    config = get_dinov2_with_registers_config(model_name, image_classifier=image_classifier)\n+\n+    # load original model from torch hub\n+    original_model = torch.hub.load(\"facebookresearch/dinov2\", model_name.replace(\"_1layer\", \"\"))\n+    original_model.eval()\n+\n+    # load state_dict of original model, remove and rename some keys\n+    state_dict = original_model.state_dict()\n+    rename_keys = create_rename_keys(config)\n+    for src, dest in rename_keys:\n+        rename_key(state_dict, src, dest)\n+    read_in_q_k_v(state_dict, config)\n+\n+    for key, val in state_dict.copy().items():\n+        val = state_dict.pop(key)\n+        if \"w12\" in key:\n+            key = key.replace(\"w12\", \"weights_in\")\n+        if \"w3\" in key:\n+            key = key.replace(\"w3\", \"weights_out\")\n+        state_dict[key] = val\n+\n+    # load HuggingFace model\n+    if image_classifier:\n+        model = Dinov2WithRegistersForImageClassification(config).eval()\n+        model.dinov2_with_registers.load_state_dict(state_dict)\n+        model_name_to_classifier_dict_url = {\n+            \"dinov2_vits14_reg_1layer\": \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_reg4_linear_head.pth\",\n+            \"dinov2_vitb14_reg_1layer\": \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_reg4_linear_head.pth\",\n+            \"dinov2_vitl14_reg_1layer\": \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_reg4_linear_head.pth\",\n+            \"dinov2_vitg14_reg_1layer\": \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_reg4_linear_head.pth\",\n+        }\n+        url = model_name_to_classifier_dict_url[model_name]\n+        classifier_state_dict = torch.hub.load_state_dict_from_url(url, map_location=\"cpu\")\n+        model.classifier.weight = nn.Parameter(classifier_state_dict[\"weight\"])\n+        model.classifier.bias = nn.Parameter(classifier_state_dict[\"bias\"])\n+    else:\n+        model = Dinov2WithRegistersModel(config).eval()\n+        model.load_state_dict(state_dict)\n+\n+    # load image\n+    image = prepare_img()\n+\n+    # preprocess image\n+    transformations = transforms.Compose(\n+        [\n+            transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),\n+            transforms.CenterCrop(224),\n+            transforms.ToTensor(),\n+            transforms.Normalize(\n+                mean=IMAGENET_DEFAULT_MEAN,  # these are RGB mean+std values\n+                std=IMAGENET_DEFAULT_STD,  # across a large photo dataset.\n+            ),\n+        ]\n+    )\n+\n+    original_pixel_values = transformations(image).unsqueeze(0)  # insert batch dimension\n+\n+    processor = BitImageProcessor(\n+        size={\"shortest_edge\": 256},\n+        resample=PILImageResampling.BICUBIC,\n+        image_mean=IMAGENET_DEFAULT_MEAN,\n+        image_std=IMAGENET_DEFAULT_STD,\n+    )\n+    pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n+\n+    assert torch.allclose(original_pixel_values, pixel_values)\n+\n+    with torch.no_grad():\n+        outputs = model(pixel_values, output_hidden_states=True)\n+        original_outputs = original_model(pixel_values)\n+\n+    # assert values\n+    if image_classifier:\n+        print(\"Predicted class:\")\n+        class_idx = outputs.logits.argmax(-1).item()\n+        print(model.config.id2label[class_idx])\n+    else:\n+        assert outputs.last_hidden_state[:, 0].shape == original_outputs.shape\n+        assert torch.allclose(outputs.last_hidden_state[:, 0], original_outputs, atol=1e-3)\n+    print(\"Looks ok!\")\n+\n+    if pytorch_dump_folder_path is not None:\n+        Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n+        print(f\"Saving model {model_name} to {pytorch_dump_folder_path}\")\n+        model.save_pretrained(pytorch_dump_folder_path)\n+        print(f\"Saving image processor to {pytorch_dump_folder_path}\")\n+        processor.save_pretrained(pytorch_dump_folder_path)\n+\n+    if push_to_hub:\n+        model_name_to_hf_name = {\n+            \"dinov2_vits14_reg\": \"dinov2-with-registers-small\",\n+            \"dinov2_vitb14_reg\": \"dinov2-with-registers-base\",\n+            \"dinov2_vitl14_reg\": \"dinov2-with-registers-large\",\n+            \"dinov2_vitg14_reg\": \"dinov2-with-registers-giant\",\n+            \"dinov2_vits14_reg_1layer\": \"dinov2-with-registers-small-imagenet1k-1-layer\",\n+            \"dinov2_vitb14_reg_1layer\": \"dinov2-with-registers-base-imagenet1k-1-layer\",\n+            \"dinov2_vitl14_reg_1layer\": \"dinov2-with-registers-large-imagenet1k-1-layer\",\n+            \"dinov2_vitg14_reg_1layer\": \"dinov2-with-registers-giant-imagenet1k-1-layer\",\n+        }\n+\n+        name = model_name_to_hf_name[model_name]\n+        model.push_to_hub(f\"nielsr/{name}\")\n+        processor.push_to_hub(f\"nielsr/{name}\")\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    # Required parameters\n+    parser.add_argument(\n+        \"--model_name\",\n+        default=\"dinov2_vits14_reg\",\n+        type=str,\n+        choices=[\n+            \"dinov2_vits14_reg\",\n+            \"dinov2_vitb14_reg\",\n+            \"dinov2_vitl14_reg\",\n+            \"dinov2_vitg14_reg\",\n+            \"dinov2_vits14_reg_1layer\",\n+            \"dinov2_vitb14_reg_1layer\",\n+            \"dinov2_vitl14_reg_1layer\",\n+            \"dinov2_vitg14_reg_1layer\",\n+        ],\n+        help=\"Name of the model you'd like to convert.\",\n+    )\n+    parser.add_argument(\n+        \"--pytorch_dump_folder_path\", default=None, type=str, help=\"Path to the output PyTorch model directory.\"\n+    )\n+    parser.add_argument(\n+        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+    )\n+\n+    args = parser.parse_args()\n+    convert_dinov2_with_registers_checkpoint(args.model_name, args.pytorch_dump_folder_path, args.push_to_hub)"
        },
        {
            "sha": "4ebefa8bded12baf3b3866705447dc5468ee7dca",
            "filename": "src/transformers/models/dinov2_with_registers/modeling_dinov2_with_registers.py",
            "status": "added",
            "additions": 926,
            "deletions": 0,
            "changes": 926,
            "blob_url": "https://github.com/huggingface/transformers/blob/6e0515e99c39444caae39472ee1b2fd76ece32f1/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6e0515e99c39444caae39472ee1b2fd76ece32f1/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py?ref=6e0515e99c39444caae39472ee1b2fd76ece32f1",
            "patch": "@@ -0,0 +1,926 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/dinov2_with_registers/modular_dinov2_with_registers.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_dinov2_with_registers.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# coding=utf-8\n+# Copyright 2024 Meta Inc. and the HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import collections.abc\n+import math\n+from typing import Dict, List, Optional, Set, Tuple, Union\n+\n+import torch\n+from torch import nn\n+from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n+\n+from ...activations import ACT2FN\n+from ...modeling_outputs import BackboneOutput, BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n+from ...modeling_utils import PreTrainedModel\n+from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n+from ...utils import (\n+    add_code_sample_docstrings,\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    logging,\n+    replace_return_docstrings,\n+)\n+from ...utils.backbone_utils import BackboneMixin\n+from .configuration_dinov2_with_registers import Dinov2WithRegistersConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+# Base docstring\n+_CHECKPOINT_FOR_DOC = \"facebook/dinov2_with_registers-base\"\n+\n+# General docstring\n+_CONFIG_FOR_DOC = \"Dinov2WithRegistersConfig\"\n+\n+\n+class Dinov2WithRegistersPatchEmbeddings(nn.Module):\n+    \"\"\"\n+    This class turns `pixel_values` of shape `(batch_size, num_channels, height, width)` into the initial\n+    `hidden_states` (patch embeddings) of shape `(batch_size, seq_length, hidden_size)` to be consumed by a\n+    Transformer.\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        image_size, patch_size = config.image_size, config.patch_size\n+        num_channels, hidden_size = config.num_channels, config.hidden_size\n+\n+        image_size = image_size if isinstance(image_size, collections.abc.Iterable) else (image_size, image_size)\n+        patch_size = patch_size if isinstance(patch_size, collections.abc.Iterable) else (patch_size, patch_size)\n+        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.num_channels = num_channels\n+        self.num_patches = num_patches\n+\n+        self.projection = nn.Conv2d(num_channels, hidden_size, kernel_size=patch_size, stride=patch_size)\n+\n+    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n+        num_channels = pixel_values.shape[1]\n+        if num_channels != self.num_channels:\n+            raise ValueError(\n+                \"Make sure that the channel dimension of the pixel values match with the one set in the configuration.\"\n+                f\" Expected {self.num_channels} but got {num_channels}.\"\n+            )\n+        embeddings = self.projection(pixel_values).flatten(2).transpose(1, 2)\n+        return embeddings\n+\n+\n+class Dinov2WithRegistersEmbeddings(nn.Module):\n+    \"\"\"\n+    Construct the CLS token, mask token, register tokens, position and patch embeddings.\n+    \"\"\"\n+\n+    def __init__(self, config: Dinov2WithRegistersConfig) -> None:\n+        super().__init__()\n+\n+        self.cls_token = nn.Parameter(torch.randn(1, 1, config.hidden_size))\n+        self.mask_token = nn.Parameter(torch.zeros(1, config.hidden_size))\n+        self.register_tokens = nn.Parameter(torch.zeros(1, config.num_register_tokens, config.hidden_size))\n+        self.patch_embeddings = Dinov2WithRegistersPatchEmbeddings(config)\n+        num_patches = self.patch_embeddings.num_patches\n+        self.position_embeddings = nn.Parameter(torch.randn(1, num_patches + 1, config.hidden_size))\n+        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+        self.config = config\n+\n+    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n+        \"\"\"\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher\n+        resolution images.\n+\n+        Source:\n+        https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174\n+        \"\"\"\n+\n+        num_patches = embeddings.shape[1] - 1\n+        num_positions = self.position_embeddings.shape[1] - 1\n+        if num_patches == num_positions and height == width:\n+            return self.position_embeddings\n+        class_pos_embed = self.position_embeddings[:, 0]\n+        patch_pos_embed = self.position_embeddings[:, 1:]\n+        dim = embeddings.shape[-1]\n+        height = height // self.config.patch_size\n+        width = width // self.config.patch_size\n+        # we add a small number to avoid floating point error in the interpolation\n+        # see discussion at https://github.com/facebookresearch/dino/issues/8\n+        height, width = height + self.config.interpolate_offset, width + self.config.interpolate_offset\n+        patch_pos_embed = patch_pos_embed.reshape(1, int(math.sqrt(num_positions)), int(math.sqrt(num_positions)), dim)\n+        patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+        target_dtype = patch_pos_embed.dtype\n+        patch_pos_embed = nn.functional.interpolate(\n+            patch_pos_embed.to(dtype=torch.float32),\n+            scale_factor=(float(height / math.sqrt(num_positions)), float(width / math.sqrt(num_positions))),\n+            mode=\"bicubic\",\n+            align_corners=False,\n+            antialias=self.config.interpolate_antialias,\n+        )\n+        patch_pos_embed = patch_pos_embed.to(dtype=target_dtype)\n+        if int(height) != patch_pos_embed.shape[-2] or int(width) != patch_pos_embed.shape[-1]:\n+            raise ValueError(\"Width or height does not match with the interpolated position embeddings\")\n+        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n+        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n+\n+    def forward(self, pixel_values: torch.Tensor, bool_masked_pos: Optional[torch.Tensor] = None) -> torch.Tensor:\n+        batch_size, _, height, width = pixel_values.shape\n+        target_dtype = self.patch_embeddings.projection.weight.dtype\n+        embeddings = self.patch_embeddings(pixel_values.to(dtype=target_dtype))\n+\n+        if bool_masked_pos is not None:\n+            embeddings = torch.where(\n+                bool_masked_pos.unsqueeze(-1), self.mask_token.to(embeddings.dtype).unsqueeze(0), embeddings\n+            )\n+\n+        # add the [CLS] token to the embedded patch tokens\n+        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n+        embeddings = torch.cat((cls_tokens, embeddings), dim=1)\n+\n+        # add positional encoding to each token\n+        embeddings = embeddings + self.interpolate_pos_encoding(embeddings, height, width)\n+\n+        # add register tokens\n+        embeddings = torch.cat(\n+            (embeddings[:, :1], self.register_tokens.expand(embeddings.shape[0], -1, -1), embeddings[:, 1:]), dim=1\n+        )\n+\n+        embeddings = self.dropout(embeddings)\n+\n+        return embeddings\n+\n+\n+class Dinov2WithRegistersSelfAttention(nn.Module):\n+    def __init__(self, config: Dinov2WithRegistersConfig) -> None:\n+        super().__init__()\n+        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n+            raise ValueError(\n+                f\"The hidden size {config.hidden_size,} is not a multiple of the number of attention \"\n+                f\"heads {config.num_attention_heads}.\"\n+            )\n+\n+        self.num_attention_heads = config.num_attention_heads\n+        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n+        self.all_head_size = self.num_attention_heads * self.attention_head_size\n+\n+        self.query = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n+        self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n+        self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n+\n+        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n+\n+    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n+        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n+        x = x.view(new_x_shape)\n+        return x.permute(0, 2, 1, 3)\n+\n+    def forward(\n+        self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n+    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n+        mixed_query_layer = self.query(hidden_states)\n+\n+        key_layer = self.transpose_for_scores(self.key(hidden_states))\n+        value_layer = self.transpose_for_scores(self.value(hidden_states))\n+        query_layer = self.transpose_for_scores(mixed_query_layer)\n+\n+        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n+\n+        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n+\n+        # Normalize the attention scores to probabilities.\n+        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n+\n+        # This is actually dropping out entire tokens to attend to, which might\n+        # seem a bit unusual, but is taken from the original Transformer paper.\n+        attention_probs = self.dropout(attention_probs)\n+\n+        # Mask heads if we want to\n+        if head_mask is not None:\n+            attention_probs = attention_probs * head_mask\n+\n+        context_layer = torch.matmul(attention_probs, value_layer)\n+\n+        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n+        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n+        context_layer = context_layer.view(new_context_layer_shape)\n+\n+        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n+\n+        return outputs\n+\n+\n+class Dinov2WithRegistersSdpaSelfAttention(Dinov2WithRegistersSelfAttention):\n+    def __init__(self, config: Dinov2WithRegistersConfig) -> None:\n+        super().__init__(config)\n+        self.attention_probs_dropout_prob = config.attention_probs_dropout_prob\n+\n+    def forward(\n+        self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n+    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n+        if output_attentions:\n+            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n+            logger.warning_once(\n+                \"Dinov2WithRegistersModel is using Dinov2WithRegistersSdpaSelfAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n+                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states=hidden_states, head_mask=head_mask, output_attentions=output_attentions\n+            )\n+\n+        mixed_query_layer = self.query(hidden_states)\n+\n+        key_layer = self.transpose_for_scores(self.key(hidden_states))\n+        value_layer = self.transpose_for_scores(self.value(hidden_states))\n+        query_layer = self.transpose_for_scores(mixed_query_layer)\n+\n+        context_layer = torch.nn.functional.scaled_dot_product_attention(\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            head_mask,\n+            self.attention_probs_dropout_prob if self.training else 0.0,\n+            is_causal=False,\n+            scale=None,\n+        )\n+\n+        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n+        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n+        context_layer = context_layer.view(new_context_layer_shape)\n+\n+        return context_layer, None\n+\n+\n+class Dinov2WithRegistersSelfOutput(nn.Module):\n+    \"\"\"\n+    The residual connection is defined in Dinov2WithRegistersLayer instead of here (as is the case with other models), due to the\n+    layernorm applied before each block.\n+    \"\"\"\n+\n+    def __init__(self, config: Dinov2WithRegistersConfig) -> None:\n+        super().__init__()\n+        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n+        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+\n+    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.dense(hidden_states)\n+        hidden_states = self.dropout(hidden_states)\n+\n+        return hidden_states\n+\n+\n+class Dinov2WithRegistersAttention(nn.Module):\n+    def __init__(self, config: Dinov2WithRegistersConfig) -> None:\n+        super().__init__()\n+        self.attention = Dinov2WithRegistersSelfAttention(config)\n+        self.output = Dinov2WithRegistersSelfOutput(config)\n+        self.pruned_heads = set()\n+\n+    def prune_heads(self, heads: Set[int]) -> None:\n+        if len(heads) == 0:\n+            return\n+        heads, index = find_pruneable_heads_and_indices(\n+            heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads\n+        )\n+\n+        # Prune linear layers\n+        self.attention.query = prune_linear_layer(self.attention.query, index)\n+        self.attention.key = prune_linear_layer(self.attention.key, index)\n+        self.attention.value = prune_linear_layer(self.attention.value, index)\n+        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n+\n+        # Update hyper params and store pruned heads\n+        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)\n+        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n+        self.pruned_heads = self.pruned_heads.union(heads)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n+        self_outputs = self.attention(hidden_states, head_mask, output_attentions)\n+\n+        attention_output = self.output(self_outputs[0], hidden_states)\n+\n+        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n+        return outputs\n+\n+\n+class Dinov2WithRegistersSdpaAttention(Dinov2WithRegistersAttention):\n+    def __init__(self, config: Dinov2WithRegistersConfig) -> None:\n+        super().__init__(config)\n+        self.attention = Dinov2WithRegistersSdpaSelfAttention(config)\n+\n+\n+class Dinov2WithRegistersLayerScale(nn.Module):\n+    def __init__(self, config) -> None:\n+        super().__init__()\n+        self.lambda1 = nn.Parameter(config.layerscale_value * torch.ones(config.hidden_size))\n+\n+    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n+        return hidden_state * self.lambda1\n+\n+\n+def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:\n+    \"\"\"\n+    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n+\n+    Comment by Ross Wightman: This is the same as the DropConnect impl I created for EfficientNet, etc networks,\n+    however, the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n+    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for changing the\n+    layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use 'survival rate' as the\n+    argument.\n+    \"\"\"\n+    if drop_prob == 0.0 or not training:\n+        return input\n+    keep_prob = 1 - drop_prob\n+    shape = (input.shape[0],) + (1,) * (input.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n+    random_tensor = keep_prob + torch.rand(shape, dtype=input.dtype, device=input.device)\n+    random_tensor.floor_()  # binarize\n+    output = input.div(keep_prob) * random_tensor\n+    return output\n+\n+\n+class Dinov2WithRegistersDropPath(nn.Module):\n+    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\"\"\"\n+\n+    def __init__(self, drop_prob: Optional[float] = None) -> None:\n+        super().__init__()\n+        self.drop_prob = drop_prob\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        return drop_path(hidden_states, self.drop_prob, self.training)\n+\n+    def extra_repr(self) -> str:\n+        return \"p={}\".format(self.drop_prob)\n+\n+\n+class Dinov2WithRegistersMLP(nn.Module):\n+    def __init__(self, config) -> None:\n+        super().__init__()\n+        in_features = out_features = config.hidden_size\n+        hidden_features = int(config.hidden_size * config.mlp_ratio)\n+        self.fc1 = nn.Linear(in_features, hidden_features, bias=True)\n+        if isinstance(config.hidden_act, str):\n+            self.activation = ACT2FN[config.hidden_act]\n+        else:\n+            self.activation = config.hidden_act\n+        self.fc2 = nn.Linear(hidden_features, out_features, bias=True)\n+\n+    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n+        hidden_state = self.fc1(hidden_state)\n+        hidden_state = self.activation(hidden_state)\n+        hidden_state = self.fc2(hidden_state)\n+        return hidden_state\n+\n+\n+class Dinov2WithRegistersSwiGLUFFN(nn.Module):\n+    def __init__(self, config) -> None:\n+        super().__init__()\n+        in_features = out_features = config.hidden_size\n+        hidden_features = int(config.hidden_size * config.mlp_ratio)\n+        hidden_features = (int(hidden_features * 2 / 3) + 7) // 8 * 8\n+\n+        self.weights_in = nn.Linear(in_features, 2 * hidden_features, bias=True)\n+        self.weights_out = nn.Linear(hidden_features, out_features, bias=True)\n+\n+    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n+        hidden_state = self.weights_in(hidden_state)\n+        x1, x2 = hidden_state.chunk(2, dim=-1)\n+        hidden = nn.functional.silu(x1) * x2\n+        return self.weights_out(hidden)\n+\n+\n+DINOV2_WITH_REGISTERS_ATTENTION_CLASSES = {\n+    \"eager\": Dinov2WithRegistersAttention,\n+    \"sdpa\": Dinov2WithRegistersSdpaAttention,\n+}\n+\n+\n+class Dinov2WithRegistersLayer(nn.Module):\n+    \"\"\"This corresponds to the Block class in the original implementation.\"\"\"\n+\n+    def __init__(self, config: Dinov2WithRegistersConfig) -> None:\n+        super().__init__()\n+\n+        self.norm1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.attention = DINOV2_WITH_REGISTERS_ATTENTION_CLASSES[config._attn_implementation](config)\n+        self.layer_scale1 = Dinov2WithRegistersLayerScale(config)\n+        self.drop_path = (\n+            Dinov2WithRegistersDropPath(config.drop_path_rate) if config.drop_path_rate > 0.0 else nn.Identity()\n+        )\n+\n+        self.norm2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+\n+        if config.use_swiglu_ffn:\n+            self.mlp = Dinov2WithRegistersSwiGLUFFN(config)\n+        else:\n+            self.mlp = Dinov2WithRegistersMLP(config)\n+        self.layer_scale2 = Dinov2WithRegistersLayerScale(config)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n+        self_attention_outputs = self.attention(\n+            self.norm1(hidden_states),  # in Dinov2WithRegisters, layernorm is applied before self-attention\n+            head_mask,\n+            output_attentions=output_attentions,\n+        )\n+        attention_output = self_attention_outputs[0]\n+\n+        attention_output = self.layer_scale1(attention_output)\n+        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n+\n+        # first residual connection\n+        hidden_states = self.drop_path(attention_output) + hidden_states\n+\n+        # in Dinov2WithRegisters, layernorm is also applied after self-attention\n+        layer_output = self.norm2(hidden_states)\n+        layer_output = self.mlp(layer_output)\n+        layer_output = self.layer_scale2(layer_output)\n+\n+        # second residual connection\n+        layer_output = self.drop_path(layer_output) + hidden_states\n+\n+        outputs = (layer_output,) + outputs\n+\n+        return outputs\n+\n+\n+class Dinov2WithRegistersEncoder(nn.Module):\n+    def __init__(self, config: Dinov2WithRegistersConfig) -> None:\n+        super().__init__()\n+        self.config = config\n+        self.layer = nn.ModuleList([Dinov2WithRegistersLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.gradient_checkpointing = False\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+        output_hidden_states: bool = False,\n+        return_dict: bool = True,\n+    ) -> Union[tuple, BaseModelOutput]:\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attentions = () if output_attentions else None\n+\n+        for i, layer_module in enumerate(self.layer):\n+            if output_hidden_states:\n+                all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+            layer_head_mask = head_mask[i] if head_mask is not None else None\n+\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    layer_module.__call__,\n+                    hidden_states,\n+                    layer_head_mask,\n+                    output_attentions,\n+                )\n+            else:\n+                layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n+\n+        if output_hidden_states:\n+            all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+        if not return_dict:\n+            return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attentions,\n+        )\n+\n+\n+class Dinov2WithRegistersPreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config_class = Dinov2WithRegistersConfig\n+    base_model_prefix = \"dinov2_with_registers\"\n+    main_input_name = \"pixel_values\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"Dinov2WithRegistersSwiGLUFFN\"]\n+    _supports_sdpa = True\n+\n+    def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+            # Upcast the input in `fp32` and cast it back to desired `dtype` to avoid\n+            # `trunc_normal_cpu` not implemented in `half` issues\n+            module.weight.data = nn.init.trunc_normal_(\n+                module.weight.data.to(torch.float32), mean=0.0, std=self.config.initializer_range\n+            ).to(module.weight.dtype)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, Dinov2WithRegistersEmbeddings):\n+            module.position_embeddings.data = nn.init.trunc_normal_(\n+                module.position_embeddings.data.to(torch.float32),\n+                mean=0.0,\n+                std=self.config.initializer_range,\n+            ).to(module.position_embeddings.dtype)\n+\n+            module.cls_token.data = nn.init.trunc_normal_(\n+                module.cls_token.data.to(torch.float32),\n+                mean=0.0,\n+                std=self.config.initializer_range,\n+            ).to(module.cls_token.dtype)\n+\n+\n+_EXPECTED_OUTPUT_SHAPE = [1, 257, 768]\n+\n+\n+DINOV2_WITH_REGISTERS_START_DOCSTRING = r\"\"\"\n+    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it\n+    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n+    behavior.\n+\n+    Parameters:\n+        config ([`Dinov2WithRegistersConfig`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+DINOV2_WITH_REGISTERS_BASE_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See\n+            [`BitImageProcessor.preprocess`] for details.\n+\n+        bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, sequence_length)`):\n+            Boolean masked positions. Indicates which patches are masked (1) and which aren't (0). Only relevant for\n+            pre-training.\n+\n+        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n+            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare Dinov2WithRegisters Model transformer outputting raw hidden-states without any specific head on top.\",\n+    DINOV2_WITH_REGISTERS_START_DOCSTRING,\n+)\n+class Dinov2WithRegistersModel(Dinov2WithRegistersPreTrainedModel):\n+    def __init__(self, config: Dinov2WithRegistersConfig):\n+        super().__init__(config)\n+        self.config = config\n+\n+        self.embeddings = Dinov2WithRegistersEmbeddings(config)\n+        self.encoder = Dinov2WithRegistersEncoder(config)\n+\n+        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self) -> Dinov2WithRegistersPatchEmbeddings:\n+        return self.embeddings.patch_embeddings\n+\n+    def _prune_heads(self, heads_to_prune: Dict[int, List[int]]) -> None:\n+        \"\"\"\n+        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n+        class PreTrainedModel\n+        \"\"\"\n+        for layer, heads in heads_to_prune.items():\n+            self.encoder.layer[layer].attention.prune_heads(heads)\n+\n+    @add_start_docstrings_to_model_forward(DINOV2_WITH_REGISTERS_BASE_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=BaseModelOutputWithPooling,\n+        config_class=_CONFIG_FOR_DOC,\n+        modality=\"vision\",\n+        expected_output=_EXPECTED_OUTPUT_SHAPE,\n+    )\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.Tensor] = None,\n+        bool_masked_pos: Optional[torch.Tensor] = None,\n+        head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if pixel_values is None:\n+            raise ValueError(\"You have to specify pixel_values\")\n+\n+        # Prepare head mask if needed\n+        # 1.0 in head_mask indicate we keep the head\n+        # attention_probs has shape bsz x n_heads x N x N\n+        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n+        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n+        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n+\n+        embedding_output = self.embeddings(pixel_values, bool_masked_pos=bool_masked_pos)\n+\n+        encoder_outputs = self.encoder(\n+            embedding_output,\n+            head_mask=head_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+        sequence_output = encoder_outputs[0]\n+        sequence_output = self.layernorm(sequence_output)\n+        pooled_output = sequence_output[:, 0, :]\n+\n+        if not return_dict:\n+            head_outputs = (sequence_output, pooled_output)\n+            return head_outputs + encoder_outputs[1:]\n+\n+        return BaseModelOutputWithPooling(\n+            last_hidden_state=sequence_output,\n+            pooler_output=pooled_output,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+        )\n+\n+\n+# Image classification docstring\n+_IMAGE_CLASS_CHECKPOINT = \"facebook/dinov2_with_registers-small-imagenet1k-1-layer\"\n+_IMAGE_CLASS_EXPECTED_OUTPUT = \"tabby, tabby cat\"\n+\n+DINOV2_WITH_REGISTERS_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See\n+            [`BitImageProcessor.preprocess`] for details.\n+\n+        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n+            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    Dinov2WithRegisters Model transformer with an image classification head on top (a linear layer on top of the final hidden state\n+    of the [CLS] token) e.g. for ImageNet.\n+    \"\"\",\n+    DINOV2_WITH_REGISTERS_START_DOCSTRING,\n+)\n+class Dinov2WithRegistersForImageClassification(Dinov2WithRegistersPreTrainedModel):\n+    def __init__(self, config: Dinov2WithRegistersConfig) -> None:\n+        super().__init__(config)\n+\n+        self.num_labels = config.num_labels\n+        self.dinov2_with_registers = Dinov2WithRegistersModel(config)\n+\n+        # Classifier head\n+        self.classifier = (\n+            nn.Linear(config.hidden_size * 2, config.num_labels) if config.num_labels > 0 else nn.Identity()\n+        )\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @add_start_docstrings_to_model_forward(DINOV2_WITH_REGISTERS_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_IMAGE_CLASS_CHECKPOINT,\n+        output_type=ImageClassifierOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+        expected_output=_IMAGE_CLASS_EXPECTED_OUTPUT,\n+    )\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.Tensor] = None,\n+        head_mask: Optional[torch.Tensor] = None,\n+        labels: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[tuple, ImageClassifierOutput]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n+        \"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        outputs = self.dinov2_with_registers(\n+            pixel_values,\n+            head_mask=head_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        sequence_output = outputs[0]  # batch_size, sequence_length, hidden_size\n+\n+        cls_token = sequence_output[:, 0]\n+        patch_tokens = sequence_output[:, 1:]\n+\n+        linear_input = torch.cat([cls_token, patch_tokens.mean(dim=1)], dim=1)\n+\n+        logits = self.classifier(linear_input)\n+\n+        loss = None\n+        if labels is not None:\n+            # move labels to correct device to enable model parallelism\n+            labels = labels.to(logits.device)\n+            if self.config.problem_type is None:\n+                if self.num_labels == 1:\n+                    self.config.problem_type = \"regression\"\n+                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n+                    self.config.problem_type = \"single_label_classification\"\n+                else:\n+                    self.config.problem_type = \"multi_label_classification\"\n+\n+            if self.config.problem_type == \"regression\":\n+                loss_fct = MSELoss()\n+                if self.num_labels == 1:\n+                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n+                else:\n+                    loss = loss_fct(logits, labels)\n+            elif self.config.problem_type == \"single_label_classification\":\n+                loss_fct = CrossEntropyLoss()\n+                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n+            elif self.config.problem_type == \"multi_label_classification\":\n+                loss_fct = BCEWithLogitsLoss()\n+                loss = loss_fct(logits, labels)\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[2:]\n+            return ((loss,) + output) if loss is not None else output\n+\n+        return ImageClassifierOutput(\n+            loss=loss,\n+            logits=logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    Dinov2WithRegisters backbone, to be used with frameworks like DETR and MaskFormer.\n+    \"\"\",\n+    DINOV2_WITH_REGISTERS_START_DOCSTRING,\n+)\n+class Dinov2WithRegistersBackbone(Dinov2WithRegistersPreTrainedModel, BackboneMixin):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        super()._init_backbone(config)\n+        self.num_features = [config.hidden_size for _ in range(config.num_hidden_layers + 1)]\n+        self.embeddings = Dinov2WithRegistersEmbeddings(config)\n+        self.encoder = Dinov2WithRegistersEncoder(config)\n+\n+        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+\n+        self.num_register_tokens = config.num_register_tokens\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self) -> Dinov2WithRegistersPatchEmbeddings:\n+        return self.embeddings.patch_embeddings\n+\n+    @add_start_docstrings_to_model_forward(DINOV2_WITH_REGISTERS_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=BackboneOutput, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        pixel_values: torch.Tensor,\n+        output_hidden_states: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> BackboneOutput:\n+        \"\"\"\n+        Returns:\n+\n+        Examples:\n+        Returns:\n+\n+        Examples:\n+\n+\n+        ```python\n+        >>> from transformers import AutoImageProcessor, AutoBackbone\n+        >>> import torch\n+        >>> from PIL import Image\n+        >>> import requests\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-with-registers-base\")\n+        >>> model = AutoBackbone.from_pretrained(\n+        ...     \"facebook/dinov2-with-registers-base\", out_features=[\"stage2\", \"stage5\", \"stage8\", \"stage11\"]\n+        ... )\n+\n+        >>> inputs = processor(image, return_tensors=\"pt\")\n+\n+        >>> outputs = model(**inputs)\n+        >>> feature_maps = outputs.feature_maps\n+        >>> list(feature_maps[-1].shape)\n+        [1, 768, 16, 16]\n+        ```\"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+\n+        embedding_output = self.embeddings(pixel_values)\n+\n+        outputs = self.encoder(\n+            embedding_output, output_hidden_states=True, output_attentions=output_attentions, return_dict=return_dict\n+        )\n+\n+        hidden_states = outputs.hidden_states if return_dict else outputs[1]\n+\n+        feature_maps = ()\n+        for stage, hidden_state in zip(self.stage_names, hidden_states):\n+            if stage in self.out_features:\n+                if self.config.apply_layernorm:\n+                    hidden_state = self.layernorm(hidden_state)\n+                if self.config.reshape_hidden_states:\n+                    hidden_state = hidden_state[:, self.num_register_tokens + 1 :]\n+                    # this was actually a bug in the original implementation that we copied here,\n+                    # cause normally the order is height, width\n+                    batch_size, _, height, width = pixel_values.shape\n+                    patch_size = self.config.patch_size\n+                    hidden_state = hidden_state.reshape(batch_size, height // patch_size, width // patch_size, -1)\n+                    hidden_state = hidden_state.permute(0, 3, 1, 2).contiguous()\n+                feature_maps += (hidden_state,)\n+\n+        if not return_dict:\n+            if output_hidden_states:\n+                output = (feature_maps,) + outputs[1:]\n+            else:\n+                output = (feature_maps,) + outputs[2:]\n+            return output\n+\n+        return BackboneOutput(\n+            feature_maps=feature_maps,\n+            hidden_states=outputs.hidden_states if output_hidden_states else None,\n+            attentions=outputs.attentions if output_attentions else None,\n+        )\n+\n+\n+__all__ = [\n+    \"Dinov2WithRegistersPreTrainedModel\",\n+    \"Dinov2WithRegistersModel\",\n+    \"Dinov2WithRegistersForImageClassification\",\n+    \"Dinov2WithRegistersBackbone\",\n+]"
        },
        {
            "sha": "bbfacd2b5f571df4d8648d1173df0f34067c5639",
            "filename": "src/transformers/models/dinov2_with_registers/modular_dinov2_with_registers.py",
            "status": "added",
            "additions": 381,
            "deletions": 0,
            "changes": 381,
            "blob_url": "https://github.com/huggingface/transformers/blob/6e0515e99c39444caae39472ee1b2fd76ece32f1/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodular_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6e0515e99c39444caae39472ee1b2fd76ece32f1/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodular_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodular_dinov2_with_registers.py?ref=6e0515e99c39444caae39472ee1b2fd76ece32f1",
            "patch": "@@ -0,0 +1,381 @@\n+# coding=utf-8\n+# Copyright 2024 Meta Inc. and the HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import math\n+from typing import Optional\n+\n+import torch\n+import torch.utils.checkpoint\n+from torch import nn\n+\n+from ....transformers.models.dinov2.modeling_dinov2 import (\n+    Dinov2Backbone,\n+    Dinov2Encoder,\n+    Dinov2ForImageClassification,\n+    Dinov2Model,\n+    Dinov2PatchEmbeddings,\n+    Dinov2PreTrainedModel,\n+)\n+from ...configuration_utils import PretrainedConfig\n+from ...modeling_outputs import BackboneOutput\n+from ...utils import logging\n+from ...utils.backbone_utils import BackboneConfigMixin, get_aligned_output_features_output_indices\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Dinov2WithRegistersConfig(BackboneConfigMixin, PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Dinov2WithRegistersModel`]. It is used to instantiate an\n+    Dinov2WithRegisters model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the DINOv2 with Registers\n+    [facebook/dinov2-with-registers-base](https://huggingface.co/facebook/dinov2-with-registers-base) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 768):\n+            Dimensionality of the encoder layers and the pooler layer.\n+        num_hidden_layers (`int`, *optional*, defaults to 12):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 12):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        mlp_ratio (`int`, *optional*, defaults to 4):\n+            Ratio of the hidden size of the MLPs relative to the `hidden_size`.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` are supported.\n+        hidden_dropout_prob (`float`, *optional*, defaults to 0.0):\n+            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n+        attention_probs_dropout_prob (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the layer normalization layers.\n+        image_size (`int`, *optional*, defaults to 224):\n+            The size (resolution) of each image.\n+        patch_size (`int`, *optional*, defaults to 16):\n+            The size (resolution) of each patch.\n+        num_channels (`int`, *optional*, defaults to 3):\n+            The number of input channels.\n+        qkv_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to add a bias to the queries, keys and values.\n+        layerscale_value (`float`, *optional*, defaults to 1.0):\n+           Initial value to use for layer scale.\n+        drop_path_rate (`float`, *optional*, defaults to 0.0):\n+            Stochastic depth rate per sample (when applied in the main path of residual layers).\n+        use_swiglu_ffn (`bool`, *optional*, defaults to `False`):\n+            Whether to use the SwiGLU feedforward neural network.\n+        num_register_tokens (`int`, *optional*, defaults to 4):\n+            Number of register tokens to use.\n+        interpolate_antialias (`bool`, *optional*, defaults to `True`):\n+            Whether to use antialiasing when interpolating the image patches.\n+        interpolate_offset (`float`, *optional*, defaults to 0.0):\n+            Offset to use when interpolating the image patches.\n+        out_features (`List[str]`, *optional*):\n+            If used as backbone, list of features to output. Can be any of `\"stem\"`, `\"stage1\"`, `\"stage2\"`, etc.\n+            (depending on how many stages the model has). If unset and `out_indices` is set, will default to the\n+            corresponding stages. If unset and `out_indices` is unset, will default to the last stage. Must be in the\n+            same order as defined in the `stage_names` attribute.\n+        out_indices (`List[int]`, *optional*):\n+            If used as backbone, list of indices of features to output. Can be any of 0, 1, 2, etc. (depending on how\n+            many stages the model has). If unset and `out_features` is set, will default to the corresponding stages.\n+            If unset and `out_features` is unset, will default to the last stage. Must be in the\n+            same order as defined in the `stage_names` attribute.\n+        apply_layernorm (`bool`, *optional*, defaults to `True`):\n+            Whether to apply layer normalization to the feature maps in case the model is used as backbone.\n+        reshape_hidden_states (`bool`, *optional*, defaults to `True`):\n+            Whether to reshape the feature maps to 4D tensors of shape `(batch_size, hidden_size, height, width)` in\n+            case the model is used as backbone. If `False`, the feature maps will be 3D tensors of shape `(batch_size,\n+            seq_len, hidden_size)`.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import Dinov2WithRegistersConfig, Dinov2WithRegistersModel\n+\n+    >>> # Initializing a Dinov2WithRegisters base style configuration\n+    >>> configuration = Dinov2WithRegistersConfig()\n+\n+    >>> # Initializing a model (with random weights) from the base style configuration\n+    >>> model = Dinov2WithRegistersModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"dinov2-with-registers-base\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=768,\n+        num_hidden_layers=12,\n+        num_attention_heads=12,\n+        mlp_ratio=4,\n+        hidden_act=\"gelu\",\n+        hidden_dropout_prob=0.0,\n+        attention_probs_dropout_prob=0.0,\n+        initializer_range=0.02,\n+        layer_norm_eps=1e-6,\n+        image_size=224,\n+        patch_size=16,\n+        num_channels=3,\n+        qkv_bias=True,\n+        layerscale_value=1.0,\n+        drop_path_rate=0.0,\n+        use_swiglu_ffn=False,\n+        num_register_tokens=4,\n+        interpolate_antialias=True,\n+        interpolate_offset=0.0,\n+        out_features=None,\n+        out_indices=None,\n+        apply_layernorm=True,\n+        reshape_hidden_states=True,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.mlp_ratio = mlp_ratio\n+        self.hidden_act = hidden_act\n+        self.hidden_dropout_prob = hidden_dropout_prob\n+        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n+        self.initializer_range = initializer_range\n+        self.layer_norm_eps = layer_norm_eps\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.num_channels = num_channels\n+        self.qkv_bias = qkv_bias\n+        self.layerscale_value = layerscale_value\n+        self.drop_path_rate = drop_path_rate\n+        self.use_swiglu_ffn = use_swiglu_ffn\n+        self.num_register_tokens = num_register_tokens\n+        self.interpolate_antialias = interpolate_antialias\n+        self.interpolate_offset = interpolate_offset\n+        self.stage_names = [\"stem\"] + [f\"stage{idx}\" for idx in range(1, num_hidden_layers + 1)]\n+        self._out_features, self._out_indices = get_aligned_output_features_output_indices(\n+            out_features=out_features, out_indices=out_indices, stage_names=self.stage_names\n+        )\n+        self.apply_layernorm = apply_layernorm\n+        self.reshape_hidden_states = reshape_hidden_states\n+\n+\n+class Dinov2WithRegistersPatchEmbeddings(Dinov2PatchEmbeddings):\n+    pass\n+\n+\n+class Dinov2WithRegistersEmbeddings(nn.Module):\n+    \"\"\"\n+    Construct the CLS token, mask token, register tokens, position and patch embeddings.\n+    \"\"\"\n+\n+    def __init__(self, config: Dinov2WithRegistersConfig) -> None:\n+        super().__init__()\n+\n+        self.cls_token = nn.Parameter(torch.randn(1, 1, config.hidden_size))\n+        self.mask_token = nn.Parameter(torch.zeros(1, config.hidden_size))\n+        self.register_tokens = nn.Parameter(torch.zeros(1, config.num_register_tokens, config.hidden_size))\n+        self.patch_embeddings = Dinov2WithRegistersPatchEmbeddings(config)\n+        num_patches = self.patch_embeddings.num_patches\n+        self.position_embeddings = nn.Parameter(torch.randn(1, num_patches + 1, config.hidden_size))\n+        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+        self.config = config\n+\n+    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n+        \"\"\"\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher\n+        resolution images.\n+\n+        Source:\n+        https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174\n+        \"\"\"\n+\n+        num_patches = embeddings.shape[1] - 1\n+        num_positions = self.position_embeddings.shape[1] - 1\n+        if num_patches == num_positions and height == width:\n+            return self.position_embeddings\n+        class_pos_embed = self.position_embeddings[:, 0]\n+        patch_pos_embed = self.position_embeddings[:, 1:]\n+        dim = embeddings.shape[-1]\n+        height = height // self.config.patch_size\n+        width = width // self.config.patch_size\n+        # we add a small number to avoid floating point error in the interpolation\n+        # see discussion at https://github.com/facebookresearch/dino/issues/8\n+        height, width = height + self.config.interpolate_offset, width + self.config.interpolate_offset\n+        patch_pos_embed = patch_pos_embed.reshape(1, int(math.sqrt(num_positions)), int(math.sqrt(num_positions)), dim)\n+        patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+        target_dtype = patch_pos_embed.dtype\n+        patch_pos_embed = nn.functional.interpolate(\n+            patch_pos_embed.to(dtype=torch.float32),\n+            scale_factor=(float(height / math.sqrt(num_positions)), float(width / math.sqrt(num_positions))),\n+            mode=\"bicubic\",\n+            align_corners=False,\n+            antialias=self.config.interpolate_antialias,\n+        )\n+        patch_pos_embed = patch_pos_embed.to(dtype=target_dtype)\n+        if int(height) != patch_pos_embed.shape[-2] or int(width) != patch_pos_embed.shape[-1]:\n+            raise ValueError(\"Width or height does not match with the interpolated position embeddings\")\n+        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n+        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n+\n+    def forward(self, pixel_values: torch.Tensor, bool_masked_pos: Optional[torch.Tensor] = None) -> torch.Tensor:\n+        batch_size, _, height, width = pixel_values.shape\n+        target_dtype = self.patch_embeddings.projection.weight.dtype\n+        embeddings = self.patch_embeddings(pixel_values.to(dtype=target_dtype))\n+\n+        if bool_masked_pos is not None:\n+            embeddings = torch.where(\n+                bool_masked_pos.unsqueeze(-1), self.mask_token.to(embeddings.dtype).unsqueeze(0), embeddings\n+            )\n+\n+        # add the [CLS] token to the embedded patch tokens\n+        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n+        embeddings = torch.cat((cls_tokens, embeddings), dim=1)\n+\n+        # add positional encoding to each token\n+        embeddings = embeddings + self.interpolate_pos_encoding(embeddings, height, width)\n+\n+        # add register tokens\n+        embeddings = torch.cat(\n+            (embeddings[:, :1], self.register_tokens.expand(embeddings.shape[0], -1, -1), embeddings[:, 1:]), dim=1\n+        )\n+\n+        embeddings = self.dropout(embeddings)\n+\n+        return embeddings\n+\n+\n+class Dinov2WithRegistersEncoder(Dinov2Encoder):\n+    pass\n+\n+\n+class Dinov2WithRegistersPreTrainedModel(Dinov2PreTrainedModel):\n+    pass\n+\n+\n+class Dinov2WithRegistersModel(Dinov2Model):\n+    pass\n+\n+\n+class Dinov2WithRegistersForImageClassification(Dinov2ForImageClassification):\n+    pass\n+\n+\n+class Dinov2WithRegistersBackbone(Dinov2Backbone):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        super()._init_backbone(config)\n+\n+        self.num_register_tokens = config.num_register_tokens\n+        self.num_features = [config.hidden_size for _ in range(config.num_hidden_layers + 1)]\n+        self.embeddings = Dinov2WithRegistersEmbeddings(config)\n+        self.encoder = Dinov2WithRegistersEncoder(config)\n+\n+        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self) -> Dinov2WithRegistersPatchEmbeddings:\n+        return self.embeddings.patch_embeddings\n+\n+    def forward(\n+        self,\n+        pixel_values: torch.Tensor,\n+        output_hidden_states: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> BackboneOutput:\n+        \"\"\"\n+        Returns:\n+\n+        Examples:\n+\n+        ```python\n+        >>> from transformers import AutoImageProcessor, AutoBackbone\n+        >>> import torch\n+        >>> from PIL import Image\n+        >>> import requests\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-with-registers-base\")\n+        >>> model = AutoBackbone.from_pretrained(\n+        ...     \"facebook/dinov2-with-registers-base\", out_features=[\"stage2\", \"stage5\", \"stage8\", \"stage11\"]\n+        ... )\n+\n+        >>> inputs = processor(image, return_tensors=\"pt\")\n+\n+        >>> outputs = model(**inputs)\n+        >>> feature_maps = outputs.feature_maps\n+        >>> list(feature_maps[-1].shape)\n+        [1, 768, 16, 16]\n+        ```\"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+\n+        embedding_output = self.embeddings(pixel_values)\n+\n+        outputs = self.encoder(\n+            embedding_output, output_hidden_states=True, output_attentions=output_attentions, return_dict=return_dict\n+        )\n+\n+        hidden_states = outputs.hidden_states if return_dict else outputs[1]\n+\n+        feature_maps = ()\n+        for stage, hidden_state in zip(self.stage_names, hidden_states):\n+            if stage in self.out_features:\n+                if self.config.apply_layernorm:\n+                    hidden_state = self.layernorm(hidden_state)\n+                if self.config.reshape_hidden_states:\n+                    hidden_state = hidden_state[:, self.num_register_tokens + 1 :]\n+                    # this was actually a bug in the original implementation that we copied here,\n+                    # cause normally the order is height, width\n+                    batch_size, _, height, width = pixel_values.shape\n+                    patch_size = self.config.patch_size\n+                    hidden_state = hidden_state.reshape(batch_size, height // patch_size, width // patch_size, -1)\n+                    hidden_state = hidden_state.permute(0, 3, 1, 2).contiguous()\n+                feature_maps += (hidden_state,)\n+\n+        if not return_dict:\n+            if output_hidden_states:\n+                output = (feature_maps,) + outputs[1:]\n+            else:\n+                output = (feature_maps,) + outputs[2:]\n+            return output\n+\n+        return BackboneOutput(\n+            feature_maps=feature_maps,\n+            hidden_states=outputs.hidden_states if output_hidden_states else None,\n+            attentions=outputs.attentions if output_attentions else None,\n+        )\n+\n+\n+__all__ = [\n+    \"Dinov2WithRegistersConfig\",\n+    \"Dinov2WithRegistersPreTrainedModel\",\n+    \"Dinov2WithRegistersModel\",\n+    \"Dinov2WithRegistersForImageClassification\",\n+    \"Dinov2WithRegistersBackbone\",\n+]"
        },
        {
            "sha": "922d67264bb142d3008684e06a43c9575ddcf68b",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/6e0515e99c39444caae39472ee1b2fd76ece32f1/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6e0515e99c39444caae39472ee1b2fd76ece32f1/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=6e0515e99c39444caae39472ee1b2fd76ece32f1",
            "patch": "@@ -3635,6 +3635,34 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class Dinov2WithRegistersBackbone(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class Dinov2WithRegistersForImageClassification(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class Dinov2WithRegistersModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class Dinov2WithRegistersPreTrainedModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class DistilBertForMaskedLM(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/dinov2_with_registers/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/6e0515e99c39444caae39472ee1b2fd76ece32f1/tests%2Fmodels%2Fdinov2_with_registers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6e0515e99c39444caae39472ee1b2fd76ece32f1/tests%2Fmodels%2Fdinov2_with_registers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinov2_with_registers%2F__init__.py?ref=6e0515e99c39444caae39472ee1b2fd76ece32f1"
        },
        {
            "sha": "6aa62138e6202ce878f4350316c1ba25cd56cf94",
            "filename": "tests/models/dinov2_with_registers/test_modeling_dinov2_with_registers.py",
            "status": "added",
            "additions": 369,
            "deletions": 0,
            "changes": 369,
            "blob_url": "https://github.com/huggingface/transformers/blob/6e0515e99c39444caae39472ee1b2fd76ece32f1/tests%2Fmodels%2Fdinov2_with_registers%2Ftest_modeling_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6e0515e99c39444caae39472ee1b2fd76ece32f1/tests%2Fmodels%2Fdinov2_with_registers%2Ftest_modeling_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinov2_with_registers%2Ftest_modeling_dinov2_with_registers.py?ref=6e0515e99c39444caae39472ee1b2fd76ece32f1",
            "patch": "@@ -0,0 +1,369 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch Dinov2WithRegisters model.\"\"\"\n+\n+import unittest\n+\n+from transformers import Dinov2WithRegistersConfig\n+from transformers.testing_utils import (\n+    require_torch,\n+    require_vision,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils import cached_property, is_torch_available, is_vision_available\n+\n+from ...test_backbone_common import BackboneTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor, ids_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+    from torch import nn\n+\n+    from transformers import (\n+        Dinov2WithRegistersBackbone,\n+        Dinov2WithRegistersForImageClassification,\n+        Dinov2WithRegistersModel,\n+    )\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    from transformers import AutoImageProcessor\n+\n+\n+class Dinov2WithRegistersModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=13,\n+        image_size=30,\n+        patch_size=2,\n+        num_channels=3,\n+        is_training=True,\n+        use_labels=True,\n+        hidden_size=32,\n+        num_hidden_layers=2,\n+        num_attention_heads=4,\n+        intermediate_size=37,\n+        hidden_act=\"gelu\",\n+        hidden_dropout_prob=0.1,\n+        attention_probs_dropout_prob=0.1,\n+        type_sequence_label_size=10,\n+        initializer_range=0.02,\n+        num_register_tokens=2,\n+        mask_ratio=0.5,\n+        scope=None,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.num_channels = num_channels\n+        self.is_training = is_training\n+        self.use_labels = use_labels\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.intermediate_size = intermediate_size\n+        self.hidden_act = hidden_act\n+        self.hidden_dropout_prob = hidden_dropout_prob\n+        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n+        self.type_sequence_label_size = type_sequence_label_size\n+        self.initializer_range = initializer_range\n+        self.num_register_tokens = num_register_tokens\n+        self.scope = scope\n+\n+        # in DINOv2 with Registers, the seq length equals the number of patches + 1 + num_register_tokens (we add 1 for the [CLS] token)\n+        num_patches = (image_size // patch_size) ** 2\n+        self.seq_length = num_patches + 1 + self.num_register_tokens\n+        self.mask_ratio = mask_ratio\n+        self.num_masks = int(mask_ratio * self.seq_length)\n+        self.mask_length = num_patches\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n+\n+        labels = None\n+        if self.use_labels:\n+            labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n+\n+        config = self.get_config()\n+\n+        return config, pixel_values, labels\n+\n+    def get_config(self):\n+        return Dinov2WithRegistersConfig(\n+            image_size=self.image_size,\n+            patch_size=self.patch_size,\n+            num_channels=self.num_channels,\n+            hidden_size=self.hidden_size,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            intermediate_size=self.intermediate_size,\n+            hidden_act=self.hidden_act,\n+            hidden_dropout_prob=self.hidden_dropout_prob,\n+            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n+            is_decoder=False,\n+            initializer_range=self.initializer_range,\n+            num_register_tokens=self.num_register_tokens,\n+        )\n+\n+    def create_and_check_model(self, config, pixel_values, labels):\n+        model = Dinov2WithRegistersModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(pixel_values)\n+        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n+\n+    def create_and_check_backbone(self, config, pixel_values, labels):\n+        model = Dinov2WithRegistersBackbone(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(pixel_values)\n+\n+        # verify hidden states\n+        self.parent.assertEqual(len(result.feature_maps), len(config.out_features))\n+        expected_size = self.image_size // config.patch_size\n+        self.parent.assertListEqual(\n+            list(result.feature_maps[0].shape), [self.batch_size, model.channels[0], expected_size, expected_size]\n+        )\n+\n+        # verify channels\n+        self.parent.assertEqual(len(model.channels), len(config.out_features))\n+\n+        # verify backbone works with out_features=None\n+        config.out_features = None\n+        model = Dinov2WithRegistersBackbone(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(pixel_values)\n+\n+        # verify feature maps\n+        self.parent.assertEqual(len(result.feature_maps), 1)\n+        self.parent.assertListEqual(\n+            list(result.feature_maps[0].shape), [self.batch_size, model.channels[0], expected_size, expected_size]\n+        )\n+\n+        # verify channels\n+        self.parent.assertEqual(len(model.channels), 1)\n+\n+        # verify backbone works with apply_layernorm=False and reshape_hidden_states=False\n+        config.apply_layernorm = False\n+        config.reshape_hidden_states = False\n+\n+        model = Dinov2WithRegistersBackbone(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(pixel_values)\n+\n+        # verify feature maps\n+        self.parent.assertEqual(len(result.feature_maps), 1)\n+        self.parent.assertListEqual(\n+            list(result.feature_maps[0].shape), [self.batch_size, self.seq_length, self.hidden_size]\n+        )\n+\n+    def create_and_check_for_image_classification(self, config, pixel_values, labels):\n+        config.num_labels = self.type_sequence_label_size\n+        model = Dinov2WithRegistersForImageClassification(config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(pixel_values, labels=labels)\n+        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.type_sequence_label_size))\n+\n+        # test greyscale images\n+        config.num_channels = 1\n+        model = Dinov2WithRegistersForImageClassification(config)\n+        model.to(torch_device)\n+        model.eval()\n+\n+        pixel_values = floats_tensor([self.batch_size, 1, self.image_size, self.image_size])\n+        result = model(pixel_values)\n+        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.type_sequence_label_size))\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        (\n+            config,\n+            pixel_values,\n+            labels,\n+        ) = config_and_inputs\n+        inputs_dict = {\"pixel_values\": pixel_values}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class Dinov2WithRegistersModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Here we also overwrite some of the tests of test_modeling_common.py, as Dinov2WithRegisters does not use input_ids, inputs_embeds,\n+    attention_mask and seq_length.\n+    \"\"\"\n+\n+    all_model_classes = (\n+        (\n+            Dinov2WithRegistersModel,\n+            Dinov2WithRegistersForImageClassification,\n+            Dinov2WithRegistersBackbone,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n+    pipeline_model_mapping = (\n+        {\n+            \"image-feature-extraction\": Dinov2WithRegistersModel,\n+            \"image-classification\": Dinov2WithRegistersForImageClassification,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+    fx_compatible = False\n+\n+    test_pruning = False\n+    test_resize_embeddings = False\n+    test_head_masking = False\n+\n+    def setUp(self):\n+        self.model_tester = Dinov2WithRegistersModelTester(self)\n+        self.config_tester = ConfigTester(\n+            self, config_class=Dinov2WithRegistersConfig, has_text_modality=False, hidden_size=37\n+        )\n+\n+    def test_initialization(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        configs_no_init = _config_zero_init(config)\n+        for model_class in self.all_model_classes:\n+            model = model_class(config=configs_no_init)\n+            for name, param in model.named_parameters():\n+                if param.requires_grad and \"register_tokens\" not in name:\n+                    self.assertIn(\n+                        ((param.data.mean() * 1e9).round() / 1e9).item(),\n+                        [0.0, 1.0],\n+                        msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                    )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    @unittest.skip(reason=\"Dinov2WithRegisters does not use inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+    )\n+    def test_training_gradient_checkpointing(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+    )\n+    def test_training_gradient_checkpointing_use_reentrant(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+    )\n+    def test_training_gradient_checkpointing_use_reentrant_false(self):\n+        pass\n+\n+    def test_model_get_set_embeddings(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            self.assertIsInstance(model.get_input_embeddings(), (nn.Module))\n+            x = model.get_output_embeddings()\n+            self.assertTrue(x is None or isinstance(x, nn.Linear))\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    def test_backbone(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_backbone(*config_and_inputs)\n+\n+    def test_for_image_classification(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_for_image_classification(*config_and_inputs)\n+\n+    @unittest.skip(reason=\"Dinov2WithRegisters does not support feedforward chunking yet\")\n+    def test_feed_forward_chunking(self):\n+        pass\n+\n+    @slow\n+    def test_model_from_pretrained(self):\n+        model_name = \"facebook/dinov2-with-registers-base\"\n+        model = Dinov2WithRegistersModel.from_pretrained(model_name)\n+        self.assertIsNotNone(model)\n+\n+\n+# We will verify our results on an image of cute cats\n+def prepare_img():\n+    image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n+    return image\n+\n+\n+@require_torch\n+@require_vision\n+class Dinov2WithRegistersModelIntegrationTest(unittest.TestCase):\n+    @cached_property\n+    def default_image_processor(self):\n+        return (\n+            AutoImageProcessor.from_pretrained(\"facebook/dinov2-with-registers-base\")\n+            if is_vision_available()\n+            else None\n+        )\n+\n+    @slow\n+    def test_inference_no_head(self):\n+        model = Dinov2WithRegistersModel.from_pretrained(\"facebook/dinov2-with-registers-base\").to(torch_device)\n+\n+        image_processor = self.default_image_processor\n+        image = prepare_img()\n+        inputs = image_processor(image, return_tensors=\"pt\").to(torch_device)\n+\n+        # forward pass\n+        with torch.no_grad():\n+            outputs = model(**inputs)\n+\n+        # verify the last hidden states\n+        # in DINOv2 with Registers, the seq length equals the number of patches + 1 + num_register_tokens (we add 1 for the [CLS] token)\n+        num_patches = (image_processor.crop_size[\"height\"] // model.config.patch_size) ** 2\n+        expected_seq_length = num_patches + 1 + model.config.num_register_tokens\n+        expected_shape = torch.Size((1, expected_seq_length, model.config.hidden_size))\n+        self.assertEqual(outputs.last_hidden_state.shape, expected_shape)\n+\n+        expected_slice = torch.tensor(\n+            [[-0.4636, -1.4582, -0.0274], [-1.4738, -0.8858, 0.3002], [0.0714, -0.2407, -1.5940]],\n+            device=torch_device,\n+        )\n+        self.assertTrue(torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=1e-4))\n+\n+\n+@require_torch\n+class Dinov2WithRegistersBackboneTest(unittest.TestCase, BackboneTesterMixin):\n+    all_model_classes = (Dinov2WithRegistersBackbone,) if is_torch_available() else ()\n+    config_class = Dinov2WithRegistersConfig\n+\n+    has_attentions = False\n+\n+    def setUp(self):\n+        self.model_tester = Dinov2WithRegistersModelTester(self)"
        },
        {
            "sha": "130eebf0b8380150fcb3e573d8eb37a1257a88b8",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6e0515e99c39444caae39472ee1b2fd76ece32f1/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6e0515e99c39444caae39472ee1b2fd76ece32f1/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=6e0515e99c39444caae39472ee1b2fd76ece32f1",
            "patch": "@@ -1009,6 +1009,7 @@ def find_all_documented_objects() -> List[str]:\n     \"ConvNextV2Backbone\",\n     \"DinatBackbone\",\n     \"Dinov2Backbone\",\n+    \"Dinov2WithRegistersBackbone\",\n     \"FocalNetBackbone\",\n     \"HieraBackbone\",\n     \"MaskFormerSwinBackbone\","
        }
    ],
    "stats": {
        "total": 2270,
        "additions": 2270,
        "deletions": 0
    }
}