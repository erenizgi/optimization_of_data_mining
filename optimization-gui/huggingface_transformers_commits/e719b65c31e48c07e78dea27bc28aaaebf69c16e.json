{
    "author": "vgel",
    "message": "Fix `FbgemmFp8Linear` not preserving tensor shape (#33239)\n\n* add tests for linear shape behavior\r\n\r\n* fix linear shape behavior\r\n\r\nended up adding the reshape at the end, after f8f8bf16_rowwise, because adding\r\nit directly after quantize_fp8_per_row caused f8f8bf16_rowwise to drop the\r\nseq_len dimension. (i.e., (17, 23, 1014) -> (17, 1024))\r\n\r\n* save shape up front + comment",
    "sha": "e719b65c31e48c07e78dea27bc28aaaebf69c16e",
    "files": [
        {
            "sha": "71c2b570cc0a7381c66a3b7797a4f72f442d0b6a",
            "filename": "src/transformers/integrations/fbgemm_fp8.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e719b65c31e48c07e78dea27bc28aaaebf69c16e/src%2Ftransformers%2Fintegrations%2Ffbgemm_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e719b65c31e48c07e78dea27bc28aaaebf69c16e/src%2Ftransformers%2Fintegrations%2Ffbgemm_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffbgemm_fp8.py?ref=e719b65c31e48c07e78dea27bc28aaaebf69c16e",
            "patch": "@@ -45,6 +45,8 @@ def __init__(self, in_features, out_features, bias, weight_dtype=torch.float32):\n \n     def forward(self, x):\n         num_tokens = None\n+        # quantize_fp8_per_row will squash the leading dimensions, so save the desired shape here\n+        output_shape = (*x.shape[:-1], -1)\n         # x_quantized and x_scale are not necessarily on the same device as x, this is an issue.\n         # https://github.com/pytorch/FBGEMM/blob/e08af8539c391437f447173863df0f3f6f6f1855/fbgemm_gpu/experimental/gen_ai/src/quantize/quantize.cu#L1237C3-L1237C45\n         x_quantized, x_scale = torch.ops.fbgemm.quantize_fp8_per_row(\n@@ -60,6 +62,7 @@ def forward(self, x):\n         output = output + self.bias if self.bias is not None else output\n         # Hacky for now, we have the output to the device of x\n         output = output.to(x.device)\n+        output = output.reshape(output_shape)\n         del x_quantized, x_scale\n         return output\n "
        },
        {
            "sha": "a9ff650c0397fdb1a0d3e6bd7b93a37dff6df8d9",
            "filename": "tests/quantization/fbgemm_fp8/test_fbgemm_fp8.py",
            "status": "modified",
            "additions": 31,
            "deletions": 0,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/e719b65c31e48c07e78dea27bc28aaaebf69c16e/tests%2Fquantization%2Ffbgemm_fp8%2Ftest_fbgemm_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e719b65c31e48c07e78dea27bc28aaaebf69c16e/tests%2Fquantization%2Ffbgemm_fp8%2Ftest_fbgemm_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ffbgemm_fp8%2Ftest_fbgemm_fp8.py?ref=e719b65c31e48c07e78dea27bc28aaaebf69c16e",
            "patch": "@@ -268,3 +268,34 @@ def test_save_pretrained_multi_gpu(self):\n \n             output = model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n             self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n+\n+\n+@require_torch_gpu\n+@require_accelerate\n+@require_fbgemm_gpu\n+class FbgemmFp8LinearTest(unittest.TestCase):\n+    def test_linear_preserves_shape(self):\n+        \"\"\"\n+        Test that FbgemmFp8Linear preserves shape when in_features == out_features.\n+        \"\"\"\n+        from transformers.integrations import FbgemmFp8Linear\n+\n+        with init_empty_weights(include_buffers=True):\n+            linear = FbgemmFp8Linear(1024, 1024, True)\n+            x = torch.rand((17, 23, 1024))\n+\n+        x_ = linear(x)\n+        self.assertEqual(x_.shape, x.shape)\n+\n+    def test_linear_with_diff_feature_size_preserves_shape(self):\n+        \"\"\"\n+        Test that FbgemmFp8Linear generates the correct shape when in_features != out_features.\n+        \"\"\"\n+        from transformers.integrations import FbgemmFp8Linear\n+\n+        with init_empty_weights(include_buffers=True):\n+            linear = FbgemmFp8Linear(1024, 2048, True)\n+            x = torch.rand((17, 23, 1024))\n+\n+        x_ = linear(x)\n+        self.assertEqual(x_.shape, (17, 23, 2048))"
        }
    ],
    "stats": {
        "total": 34,
        "additions": 34,
        "deletions": 0
    }
}