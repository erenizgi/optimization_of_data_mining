{
    "author": "pcuenca",
    "message": "Llama Guard updates (#37872)\n\n* Unhardcode use_chunked_attention, fix no_rope_layers\n\n* Go back to exhaustive list of bools\n\n* Conversion and modeling updates\n\n* Fix rope\n\n* Unhardcode rope\n\n* Fix context length\n\n* style\n\n* Minor updates to conversion\n\n* Use StaticCache\n\n* Minor simplification\n\n* DynamicCache ðŸ¤¦\n\n* Style\n\n* Style",
    "sha": "63cd4c76f3a068ac36c37453be156e019c55c1b7",
    "files": [
        {
            "sha": "675bf6a5ef8c08297a3be82197fe9b30b360c4c3",
            "filename": "src/transformers/models/llama4/configuration_llama4.py",
            "status": "modified",
            "additions": 12,
            "deletions": 3,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/63cd4c76f3a068ac36c37453be156e019c55c1b7/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63cd4c76f3a068ac36c37453be156e019c55c1b7/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py?ref=63cd4c76f3a068ac36c37453be156e019c55c1b7",
            "patch": "@@ -224,8 +224,13 @@ class Llama4TextConfig(PretrainedConfig):\n                     Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n             <TODO>\n             <TODO>\n-        no_rope_layers (`int`, *optional*): TODO\n-        no_rope_layer_interval (`int`, *optional*, defaults to 4): TODO\n+        no_rope_layers (`List[int]`, *optional*):\n+            List with at least the same length as the number of layers in the model.\n+            A `1` at an index position indicates that the corresponding layer will use RoPE,\n+            while a `0` indicates that it's a NoPE layer.\n+        no_rope_layer_interval (`int`, *optional*, defaults to 4):\n+            If `no_rope_layers` is `None`, it will be created using a NoPE layer every\n+            `no_rope_layer_interval` layers.\n         attention_chunk_size (`int`, *optional*, defaults to 8192):\n             <TODO>\n         attn_temperature_tuning (`bool`, *optional*, defaults to `True`):\n@@ -339,11 +344,15 @@ def __init__(\n         self.output_router_logits = output_router_logits\n         self.router_aux_loss_coef = router_aux_loss_coef\n         self.router_jitter_noise = router_jitter_noise\n+\n+        # Backwards compatibility\n+        if no_rope_layers == []:\n+            no_rope_layers = None\n+\n         default_no_rope_layers = [\n             int((layer_idx + 1) % no_rope_layer_interval != 0) for layer_idx in range(self.num_hidden_layers)\n         ]\n \n-        # no_rope_layers == [] is invalid as we cannot have 0 layers\n         self.no_rope_layers = no_rope_layers if no_rope_layers else default_no_rope_layers\n \n         self.interleave_moe_layer_step = interleave_moe_layer_step"
        },
        {
            "sha": "e8a37afbc4bbf0923967068e29bcc85c72907af5",
            "filename": "src/transformers/models/llama4/convert_llama4_weights_to_hf.py",
            "status": "modified",
            "additions": 31,
            "deletions": 23,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/63cd4c76f3a068ac36c37453be156e019c55c1b7/src%2Ftransformers%2Fmodels%2Fllama4%2Fconvert_llama4_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63cd4c76f3a068ac36c37453be156e019c55c1b7/src%2Ftransformers%2Fmodels%2Fllama4%2Fconvert_llama4_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fconvert_llama4_weights_to_hf.py?ref=63cd4c76f3a068ac36c37453be156e019c55c1b7",
            "patch": "@@ -65,6 +65,7 @@\n     r\"layers.(\\d+).feed_forward.w3.weight\":                  r\"language_model.model.layers.\\1.feed_forward.up_proj.weight\",                 # might need to be fused for efficiency?\n     # r\"layers.(\\d+).feed_forward.mlp.fc1_weight\":             r\"language_model.model.layers.\\1.feed_forward.gate_up_proj.weight\",\n     r\"layers.(\\d+).feed_forward.mlp.fc2_weight\":             r\"language_model.model.layers.\\1.feed_forward.down_proj.weight\",\n+    r\"layers.(\\d+).feed_forward.w2.weight\":                  r\"language_model.model.layers.\\1.feed_forward.down_proj.weight\",\n     r\"layers.(\\d+).feed_forward.mlp.layer_norm.weight\":      r\"language_model.model.layers.\\1.post_attention_layernorm.weight\",\n \n     # Vision encoder mapping\n@@ -166,8 +167,8 @@ def get_concat_dim(key):\n     return 0\n \n \n-def compute_intermediate_size(hidden_dim, multiple_of=1024, ffn_dim_multiplier=1.3):\n-    hidden_dim = 4 * int(2 * hidden_dim / 3)\n+def compute_intermediate_size(hidden_dim, ffn_exp=4, multiple_of=1024, ffn_dim_multiplier=1.2):\n+    hidden_dim = ffn_exp * int(2 * hidden_dim / 3)\n     hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n     hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n     return hidden_dim\n@@ -203,6 +204,8 @@ def max_context_length(model_path, instruct=False):\n     with open(os.path.join(model_path, \"params.json\"), \"r\") as f:\n         params = json.load(f)\n     params = params.get(\"model\", params)\n+    if params.get(\"moe_args\") is None:\n+        return 8192\n     num_experts = params[\"moe_args\"][\"num_experts\"]\n     return 10485760 if num_experts == 16 else 1048576\n \n@@ -242,24 +245,40 @@ def write_model(\n         # some constants from original code\n         rope_scaling = {\n             \"rope_type\": \"llama3\",\n-            \"factor\": 8.0,\n+            \"factor\": params.get(\"rope_scaling_factor\", 8.0),\n             \"low_freq_factor\": 1.0,\n-            \"high_freq_factor\": 4.0,\n+            \"high_freq_factor\": params.get(\"rope_high_freq_factor\", 4.0),\n             \"original_max_position_embeddings\": 8192,\n         }\n         config_kwargs.update({\"rope_scaling\": rope_scaling})\n \n+    if attention_chunk_size is None:\n+        config_kwargs.update({\"cache_implementation\": \"static\"})\n+\n     # compute additional params for weight conversion\n     num_heads_per_shard = num_heads // num_shards\n     dim_per_head = dim // num_heads\n-    # intermediate_size = compute_intermediate_size(dim, multiple_of=params[\"multiple_of\"])\n+    intermediate_size_mlp = compute_intermediate_size(\n+        dim,\n+        ffn_exp=params[\"ffn_exp\"],\n+        multiple_of=params[\"multiple_of\"],\n+        ffn_dim_multiplier=params[\"ffn_dim_multiplier\"],\n+    )\n \n     num_key_value_heads = params[\"n_kv_heads\"]  # for GQA / MQA\n \n-    num_experts = params[\"moe_args\"][\"num_experts\"]\n-    interleave_moe_layer_step = params[\"moe_args\"].get(\"interleave_moe_layer_step\", 1)\n+    if hasattr(params, \"moe_args\"):\n+        num_experts = params[\"moe_args\"][\"num_experts\"]\n+        interleave_moe_layer_step = params[\"moe_args\"].get(\"interleave_moe_layer_step\", 1)\n+    else:\n+        # Dense model (possibly Llama Guard) - disable all moe layers\n+        num_experts = 0\n+        interleave_moe_layer_step = 0\n+        config_kwargs.update({\"moe_layers\": []})\n \n+    # Ensure all layers are rope if `nope_layer_interval` is None\n     no_rope_layer_interval = params[\"nope_layer_interval\"]\n+    no_rope_layer_interval = num_heads * 2 if no_rope_layer_interval is None else no_rope_layer_interval\n \n     bos_token_id = 200000\n     eos_token_id = [200001, 200007, 200008] if instruct else 200001\n@@ -273,7 +292,7 @@ def write_model(\n         rope_theta=rope_theta,\n         num_hidden_layers=num_layers,\n         intermediate_size=8192,\n-        intermediate_size_mlp=16384,\n+        intermediate_size_mlp=intermediate_size_mlp,\n         max_position_embeddings=max_context_length(input_base_path, instruct),\n         num_local_experts=num_experts,\n         interleave_moe_layer_step=interleave_moe_layer_step,\n@@ -336,7 +355,7 @@ def write_model(\n         sharded_keys = []\n         for _key in all_keys_raw:\n             try:\n-                if (loaded[0][_key] == loaded[1][_key]).all():\n+                if num_shards == 1 or (loaded[0][_key] == loaded[1][_key]).all():\n                     repeated_keys.append(_key)\n                 else:\n                     sharded_keys.append(_key)\n@@ -354,7 +373,7 @@ def write_model(\n         for key in tqdm(all_keys, desc=\"Renaming and processing all keys\", unit=\"key\"):\n             new_key = new_keys[key]\n             print(key, new_key)\n-            if not is_param_same_across_shards(new_key):\n+            if num_shards > 1 and not is_param_same_across_shards(new_key):\n                 current_parameter = [chunk.pop(key) for chunk in loaded if not isinstance(chunk[key], io.BytesIO)]\n             else:\n                 print(f\"{key} (now {new_key}) is the same across all shards.\")\n@@ -565,8 +584,8 @@ def get_reserved_special_tokens(name, count, start_index=0):\n     \"<|python_end|>\",\n     \"<|finetune_right_pad|>\",\n ] + get_reserved_special_tokens(\n-    \"text_post_train\", 61, 6\n-)  # <|text_post_train_reserved_special_token_6|>, ..., <|text_post_train_reserved_special_token_66|>\n+    \"text_post_train\", 61, 8\n+)  # <|text_post_train_reserved_special_token_8|>, ..., <|text_post_train_reserved_special_token_68|>\n \n # 200080, ..., 201133\n LLAMA4_VISION_SPECIAL_TOKENS = [\n@@ -621,15 +640,6 @@ def __init__(\n             **kwargs,\n         )\n \n-        # to check\n-        # import tiktoken\n-        # model = tiktoken.Encoding(\n-        #     name=Path(model_path).name,\n-        #     pat_str=self.O200K_PATTERN,\n-        #     mergeable_ranks=mergeable_ranks,\n-        #     special_tokens=self.special_tokens,\n-        # )\n-\n         instruct = chat_template is not None\n         self.update_post_processor(self.converted_tokenizer)\n         # finer special_tokens_map.json\n@@ -687,12 +697,10 @@ def write_tokenizer(args):\n     parser.add_argument(\n         \"--input_dir\",\n         type=str,\n-        default=\"/fsx/arthur/Llama-4-17B-Omni-Instruct-Original\",\n         help=\"Location of the local folder copied from the Hub.\",\n     )\n     parser.add_argument(\n         \"--output_dir\",\n-        default=\"llama4_hf_vision\",\n         type=str,\n         help=\"Location to write HF model and tokenizer\",\n     )"
        },
        {
            "sha": "4d61a011483b45addcdf812708764edd0d160022",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 29,
            "deletions": 23,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/63cd4c76f3a068ac36c37453be156e019c55c1b7/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63cd4c76f3a068ac36c37453be156e019c55c1b7/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=63cd4c76f3a068ac36c37453be156e019c55c1b7",
            "patch": "@@ -20,12 +20,11 @@\n import torch\n import torch.nn as nn\n import torch.nn.functional as F\n-import torch.utils.checkpoint\n \n from transformers.models.llama4.configuration_llama4 import Llama4VisionConfig\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, HybridChunkedCache\n+from ...cache_utils import Cache, DynamicCache, HybridChunkedCache\n from ...generation import GenerationMixin\n from ...integrations.hub_kernels import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n@@ -287,7 +286,7 @@ def __init__(self, config: Llama4TextConfig, layer_idx):\n         self.attn_temperature_tuning = config.attn_temperature_tuning\n         self.attention_dropout = config.attention_dropout\n         self.is_causal = True\n-        self.use_rope = int((layer_idx + 1) % 4 != 0)  # rope unused for dense layers\n+        self.use_rope = config.no_rope_layers[layer_idx]\n         self.q_proj = nn.Linear(\n             config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n         )\n@@ -374,7 +373,7 @@ def __init__(self, config, layer_idx):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n         self.self_attn = Llama4TextAttention(config, layer_idx)\n-        self.use_chunked_attention = int((layer_idx + 1) % 4 != 0)  # <=> use rope\n+        self.use_chunked_attention = config.attention_chunk_size is not None and bool(config.no_rope_layers[layer_idx])\n         self.is_moe_layer = layer_idx in config.moe_layers\n         if self.is_moe_layer:  # the 128E model interleaves dense / sparse\n             self.feed_forward = Llama4TextMoe(config)\n@@ -643,7 +642,10 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids.to(self.embed_tokens.weight.device))\n \n         if use_cache and past_key_values is None:\n-            past_key_values = HybridChunkedCache(self.config, inputs_embeds.shape[0], inputs_embeds.shape[1])\n+            if self.config.get_text_config().get(\"attention_chunk_size\") is not None:\n+                past_key_values = HybridChunkedCache(self.config, inputs_embeds.shape[0], inputs_embeds.shape[1])\n+            else:\n+                past_key_values = DynamicCache(self.config, inputs_embeds.shape[0], inputs_embeds.shape[1])\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -740,6 +742,7 @@ def _update_causal_mask(\n         sequence_length = input_tensor.shape[1]\n         cache_position = cache_position.to(self.device)\n         attention_chunk_size = self.config.attention_chunk_size\n+        using_chunked_attention = attention_chunk_size is not None\n \n         first_cache_position = cache_position[0]\n \n@@ -748,26 +751,28 @@ def _update_causal_mask(\n         else:\n             full_cache_length = attention_mask.shape[-1] if attention_mask is not None else sequence_length\n \n-        cond1 = first_cache_position >= attention_chunk_size\n-        cond2 = (first_cache_position < attention_chunk_size) & (\n-            first_cache_position + sequence_length > attention_chunk_size\n-        )\n-        key_length = (\n-            torch.where(\n-                cond1,\n-                attention_chunk_size + sequence_length - 1,\n-                torch.where(cond2, first_cache_position + sequence_length, attention_chunk_size),\n+        if using_chunked_attention:\n+            cond1 = first_cache_position >= attention_chunk_size\n+            cond2 = (first_cache_position < attention_chunk_size) & (\n+                first_cache_position + sequence_length > attention_chunk_size\n+            )\n+            key_length = (\n+                torch.where(\n+                    cond1,\n+                    attention_chunk_size + sequence_length - 1,\n+                    torch.where(cond2, first_cache_position + sequence_length, attention_chunk_size),\n+                )\n+                if use_cache\n+                else full_cache_length\n             )\n-            if use_cache\n-            else full_cache_length\n-        )\n \n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n-                offsets = (first_cache_position, max(first_cache_position - attention_chunk_size + 1, 0))\n-                chunked_attention_mask = make_flex_block_causal_mask(\n-                    attention_mask, self.config.attention_chunk_size, sequence_length, key_length, offsets=offsets\n-                )\n+                if using_chunked_attention:\n+                    offsets = (first_cache_position, max(first_cache_position - attention_chunk_size + 1, 0))\n+                    chunked_attention_mask = make_flex_block_causal_mask(\n+                        attention_mask, attention_chunk_size, sequence_length, key_length, offsets=offsets\n+                    )\n                 attention_mask = make_flex_block_causal_mask(\n                     attention_mask,\n                     query_length=sequence_length,\n@@ -780,15 +785,16 @@ def _update_causal_mask(\n \n         # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n         dtype, device = input_tensor.dtype, input_tensor.device\n+        target_length = max(full_cache_length, attention_chunk_size) if using_chunked_attention else full_cache_length\n         causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n             attention_mask,\n             sequence_length=sequence_length,\n-            target_length=max(full_cache_length, attention_chunk_size),\n+            target_length=target_length,\n             dtype=dtype,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n-        if full_cache_length > self.config.attention_chunk_size:\n+        if using_chunked_attention and full_cache_length > attention_chunk_size:\n             start_idx = max(first_cache_position - attention_chunk_size + 1, 0)\n             end_idx = start_idx + key_length\n             chunked_attention_mask = self.create_chunked_attention_mask("
        }
    ],
    "stats": {
        "total": 121,
        "additions": 72,
        "deletions": 49
    }
}