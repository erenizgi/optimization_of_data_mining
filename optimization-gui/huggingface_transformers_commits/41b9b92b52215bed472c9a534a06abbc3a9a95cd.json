{
    "author": "zucchini-nlp",
    "message": "[qwen-vl] fix image processor (#37258)\n\n* fix\n\n* add test",
    "sha": "41b9b92b52215bed472c9a534a06abbc3a9a95cd",
    "files": [
        {
            "sha": "732d44d53b83ed3500e09fc380d0c287acb9bfcf",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/41b9b92b52215bed472c9a534a06abbc3a9a95cd/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41b9b92b52215bed472c9a534a06abbc3a9a95cd/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py?ref=41b9b92b52215bed472c9a534a06abbc3a9a95cd",
            "patch": "@@ -379,17 +379,18 @@ def preprocess(\n                 - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n \n         \"\"\"\n+        min_pixels = min_pixels if min_pixels is not None else self.min_pixels\n+        max_pixels = max_pixels if max_pixels is not None else self.max_pixels\n+\n         if size is not None:\n             if \"shortest_edge\" not in size or \"longest_edge\" not in size:\n                 raise ValueError(\"size must contain 'shortest_edge' and 'longest_edge' keys.\")\n             min_pixels = size[\"shortest_edge\"]\n+        elif min_pixels is not None and max_pixels is not None:\n+            # backward compatibility: override size with min_pixels and max_pixels if they are provided\n+            size = {\"shortest_edge\": min_pixels, \"longest_edge\": max_pixels}\n         else:\n             size = {**self.size}\n-        # backward compatibility: override size with min_pixels and max_pixels if they are provided\n-        if min_pixels is not None:\n-            size[\"shortest_edge\"] = min_pixels\n-        if max_pixels is not None:\n-            size[\"longest_edge\"] = max_pixels\n \n         do_resize = do_resize if do_resize is not None else self.do_resize\n "
        },
        {
            "sha": "60b62449d3aed39a9d34280b26f056d3fac23e94",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/41b9b92b52215bed472c9a534a06abbc3a9a95cd/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41b9b92b52215bed472c9a534a06abbc3a9a95cd/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py?ref=41b9b92b52215bed472c9a534a06abbc3a9a95cd",
            "patch": "@@ -334,17 +334,18 @@ def preprocess(\n             device (`torch.device`, *optional*):\n                 The device to process the images on. If unset, the device is inferred from the input images.\n         \"\"\"\n+        min_pixels = min_pixels if min_pixels is not None else self.min_pixels\n+        max_pixels = max_pixels if max_pixels is not None else self.max_pixels\n+\n         if size is not None:\n             if \"shortest_edge\" not in size or \"longest_edge\" not in size:\n                 raise ValueError(\"size must contain 'shortest_edge' and 'longest_edge' keys.\")\n             min_pixels = size[\"shortest_edge\"]\n+        elif min_pixels is not None and max_pixels is not None:\n+            # backward compatibility: override size with min_pixels and max_pixels if they are provided\n+            size = {\"shortest_edge\": min_pixels, \"longest_edge\": max_pixels}\n         else:\n             size = {**self.size}\n-        # backward compatibility: override size with min_pixels and max_pixels if they are provided\n-        if min_pixels is not None:\n-            size[\"shortest_edge\"] = min_pixels\n-        if max_pixels is not None:\n-            size[\"longest_edge\"] = max_pixels\n \n         do_resize = do_resize if do_resize is not None else self.do_resize\n         size = size if size is not None else self.size"
        },
        {
            "sha": "95d758f438169448b9a30563de83daebfbc18d00",
            "filename": "tests/models/qwen2_vl/test_image_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/41b9b92b52215bed472c9a534a06abbc3a9a95cd/tests%2Fmodels%2Fqwen2_vl%2Ftest_image_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41b9b92b52215bed472c9a534a06abbc3a9a95cd/tests%2Fmodels%2Fqwen2_vl%2Ftest_image_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_image_processing_qwen2_vl.py?ref=41b9b92b52215bed472c9a534a06abbc3a9a95cd",
            "patch": "@@ -13,6 +13,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+import tempfile\n import unittest\n \n import numpy as np\n@@ -298,6 +299,20 @@ def test_custom_patch_size(self):\n                 expected_output_video_shape = (171500, 1176)\n                 self.assertEqual(tuple(encoded_video.shape), expected_output_video_shape)\n \n+    def test_custom_image_size(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                image_processing.save_pretrained(tmpdirname)\n+                image_processor_loaded = image_processing_class.from_pretrained(\n+                    tmpdirname, max_pixels=56 * 56, min_pixels=28 * 28\n+                )\n+\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True)\n+            prcocess_out = image_processor_loaded(image_inputs, return_tensors=\"pt\")\n+            expected_output_video_shape = [112, 1176]\n+            self.assertListEqual(list(prcocess_out.pixel_values.shape), expected_output_video_shape)\n+\n     @require_vision\n     @require_torch\n     def test_slow_fast_equivalence(self):"
        }
    ],
    "stats": {
        "total": 37,
        "additions": 27,
        "deletions": 10
    }
}