{
    "author": "shimizust",
    "message": "Updated Trainer's liger-kernel integration to call correct patching API (#33502)\n\n* Updated liger-kernel integration in Trainer to call correct patching API\r\n\r\n* Fixed styling",
    "sha": "ba1f1dc132257f0e92d2266f966dc19758a3f2b3",
    "files": [
        {
            "sha": "97a052093652c10c8f04c5fa86b187d9efefb472",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 6,
            "deletions": 7,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba1f1dc132257f0e92d2266f966dc19758a3f2b3/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba1f1dc132257f0e92d2266f966dc19758a3f2b3/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=ba1f1dc132257f0e92d2266f966dc19758a3f2b3",
            "patch": "@@ -468,19 +468,18 @@ def __init__(\n \n         if self.args.use_liger_kernel:\n             if is_liger_kernel_available():\n-                from liger_kernel.transformers.trainer_integration import _apply_liger_kernel\n+                from liger_kernel.transformers import _apply_liger_kernel_to_instance\n \n-                model_type = getattr(model, \"config\", None) and getattr(model.config, \"model_type\", None)\n-                if model_type:\n-                    # Monkey patch the model with liger kernels. Use the default kernel configurations.\n-                    _apply_liger_kernel(model_type=model_type)\n+                if isinstance(model, PreTrainedModel):\n+                    # Patch the model with liger kernels. Use the default kernel configurations.\n+                    _apply_liger_kernel_to_instance(model=model)\n                 else:\n                     logger.warning(\n-                        \"The model does not have a valid `model_type` specified. No liger kernels will be applied.\"\n+                        \"The model is not an instance of PreTrainedModel. No liger kernels will be applied.\"\n                     )\n             else:\n                 raise ImportError(\n-                    \"You have set `use_liger_kernel` to `True` but liger-kernel >= 0.1.0 is not available. \"\n+                    \"You have set `use_liger_kernel` to `True` but liger-kernel >= 0.3.0 is not available. \"\n                     \"Please install it with `pip install liger-kernel`\"\n                 )\n "
        },
        {
            "sha": "ad8b649aaa4e8439b08ebf8485ad5e925ce83a66",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba1f1dc132257f0e92d2266f966dc19758a3f2b3/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba1f1dc132257f0e92d2266f966dc19758a3f2b3/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=ba1f1dc132257f0e92d2266f966dc19758a3f2b3",
            "patch": "@@ -1187,7 +1187,7 @@ def is_liger_kernel_available():\n     if not _liger_kernel_available:\n         return False\n \n-    return version.parse(importlib.metadata.version(\"liger_kernel\")) >= version.parse(\"0.1.0\")\n+    return version.parse(importlib.metadata.version(\"liger_kernel\")) >= version.parse(\"0.3.0\")\n \n \n # docstyle-ignore"
        },
        {
            "sha": "791486ec837405b1648ce604e731e5fcc59a30e4",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 18,
            "deletions": 12,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba1f1dc132257f0e92d2266f966dc19758a3f2b3/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba1f1dc132257f0e92d2266f966dc19758a3f2b3/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=ba1f1dc132257f0e92d2266f966dc19758a3f2b3",
            "patch": "@@ -1344,22 +1344,28 @@ def test_get_eval_dataloader_with_persistent_workers(self):\n \n     @require_liger_kernel\n     def test_use_liger_kernel_patching(self):\n-        # Test that the model code actually gets patched with Liger kernel\n-        from liger_kernel.transformers.rms_norm import LigerRMSNorm\n+        # Ensure any monkey patching is cleaned up for subsequent tests\n+        with patch(\"transformers.models.llama.modeling_llama\"):\n+            from liger_kernel.transformers import LigerRMSNorm, liger_rotary_pos_emb\n \n-        from transformers.models.llama import modeling_llama\n+            from transformers.models.llama import modeling_llama\n \n-        config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n-        tiny_llama = LlamaForCausalLM(config)\n+            config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n+            tiny_llama = LlamaForCausalLM(config)\n \n-        args = TrainingArguments(\n-            \"./test\",\n-            use_liger_kernel=True,\n-        )\n-        Trainer(tiny_llama, args)\n+            # Spot check that modeling code and model instance variables are not yet patched\n+            self.assertNotEqual(modeling_llama.apply_rotary_pos_emb, liger_rotary_pos_emb)\n+            self.assertFalse(isinstance(tiny_llama.model.norm, LigerRMSNorm))\n+\n+            args = TrainingArguments(\n+                \"./test\",\n+                use_liger_kernel=True,\n+            )\n+            Trainer(tiny_llama, args)\n \n-        # Check that one of the Llama model layers has been correctly patched with Liger kernel\n-        self.assertEqual(modeling_llama.LlamaRMSNorm, LigerRMSNorm)\n+            # Spot check that modeling code and model instance variables are patched\n+            self.assertEqual(modeling_llama.apply_rotary_pos_emb, liger_rotary_pos_emb)\n+            self.assertTrue(isinstance(tiny_llama.model.norm, LigerRMSNorm))\n \n     @require_liger_kernel\n     @require_torch_gpu"
        }
    ],
    "stats": {
        "total": 45,
        "additions": 25,
        "deletions": 20
    }
}