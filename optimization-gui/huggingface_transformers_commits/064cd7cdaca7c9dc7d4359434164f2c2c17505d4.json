{
    "author": "ManuelFay",
    "message": "Fix SDPA implementation in Qwen2-VL (issues with torch==2.6.0) (#36891)\n\n* fix sdpa implementation\n\n* ruff\n\n* also modify 2_5 for consistency",
    "sha": "064cd7cdaca7c9dc7d4359434164f2c2c17505d4",
    "files": [
        {
            "sha": "095a0769ed3b4f6b05b51106856d1d2258d8c9ba",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/064cd7cdaca7c9dc7d4359434164f2c2c17505d4/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/064cd7cdaca7c9dc7d4359434164f2c2c17505d4/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=064cd7cdaca7c9dc7d4359434164f2c2c17505d4",
            "patch": "@@ -316,8 +316,10 @@ def forward(\n         q = q.transpose(0, 1)\n         k = k.transpose(0, 1)\n         v = v.transpose(0, 1)\n-        attn_output = F.scaled_dot_product_attention(q, k, v, attention_mask, dropout_p=0.0)\n-        attn_output = attn_output.transpose(0, 1)\n+        attn_output = F.scaled_dot_product_attention(\n+            q.unsqueeze(0), k.unsqueeze(0), v.unsqueeze(0), attention_mask, dropout_p=0.0\n+        )\n+        attn_output = attn_output.squeeze(0).transpose(0, 1)\n         attn_output = attn_output.reshape(seq_length, -1)\n         attn_output = self.proj(attn_output)\n         return attn_output"
        },
        {
            "sha": "2bcfd3608ded5918eb0a3eb60851446a10a66cfb",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/064cd7cdaca7c9dc7d4359434164f2c2c17505d4/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/064cd7cdaca7c9dc7d4359434164f2c2c17505d4/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=064cd7cdaca7c9dc7d4359434164f2c2c17505d4",
            "patch": "@@ -416,8 +416,10 @@ def forward(\n         q = q.transpose(0, 1)\n         k = k.transpose(0, 1)\n         v = v.transpose(0, 1)\n-        attn_output = F.scaled_dot_product_attention(q, k, v, attention_mask, dropout_p=0.0)\n-        attn_output = attn_output.transpose(0, 1)\n+        attn_output = F.scaled_dot_product_attention(\n+            q.unsqueeze(0), k.unsqueeze(0), v.unsqueeze(0), attention_mask, dropout_p=0.0\n+        )\n+        attn_output = attn_output.squeeze(0).transpose(0, 1)\n         attn_output = attn_output.reshape(seq_length, -1)\n         attn_output = self.proj(attn_output)\n         return attn_output"
        }
    ],
    "stats": {
        "total": 12,
        "additions": 8,
        "deletions": 4
    }
}