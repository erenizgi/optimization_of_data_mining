{
    "author": "vasqu",
    "message": "[`GPT Big Code`] Fix attention scaling (#40041)\n\n* fix\n\n* update integration tests\n\n* fmt\n\n* add regression test",
    "sha": "e29919f9936c36a48f8cb9bb671979d3f0d0799d",
    "files": [
        {
            "sha": "0d21f30f490c5fce55b7dee6ed0606b3b06a5fe0",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e29919f9936c36a48f8cb9bb671979d3f0d0799d/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e29919f9936c36a48f8cb9bb671979d3f0d0799d/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=e29919f9936c36a48f8cb9bb671979d3f0d0799d",
            "patch": "@@ -144,7 +144,7 @@ def __init__(self, config, is_cross_attention=False, layer_idx=None):\n             )\n \n         self.scale_attn_weights = config.scale_attn_weights\n-        self.scaling = self.head_dim**0.5 if config.scale_attn_weights else 1.0\n+        self.scaling = self.head_dim**-0.5 if config.scale_attn_weights else 1.0\n         self.is_cross_attention = is_cross_attention\n \n         self.layer_idx = layer_idx"
        },
        {
            "sha": "a57dc883f3cab2e13e3e2832837605dad4001f7b",
            "filename": "tests/models/gpt_bigcode/test_modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 20,
            "deletions": 3,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/e29919f9936c36a48f8cb9bb671979d3f0d0799d/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e29919f9936c36a48f8cb9bb671979d3f0d0799d/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py?ref=e29919f9936c36a48f8cb9bb671979d3f0d0799d",
            "patch": "@@ -29,6 +29,7 @@\n     import torch\n \n     from transformers import (\n+        AutoTokenizer,\n         GPT2TokenizerFast,\n         GPTBigCodeForCausalLM,\n         GPTBigCodeForSequenceClassification,\n@@ -510,7 +511,7 @@ def test_generate_simple(self):\n         output_sequence = model.generate(input_ids)\n         output_sentence = tokenizer.decode(output_sequence[0], skip_special_tokens=True)\n \n-        expected_output = \"\"\"def print_hello_world():\\n    print(\"Hello World!\")\\n\\n\\ndef print_hello_\"\"\"\n+        expected_output = 'def print_hello_world():\\n    print(\"Hello World!\")\\n\\n\\ndef print_hello_world_with_args(name'  # fmt: skip\n         self.assertEqual(output_sentence, expected_output)\n \n     def test_generate_batched(self):\n@@ -527,11 +528,27 @@ def test_generate_batched(self):\n         outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n \n         expected_output = [\n-            'def print_hello_world():\\n    print(\"Hello World!\")\\n\\n\\ndef print_hello_',\n-            'def say_hello():\\n    print(\"Hello, World!\")\\n\\n\\nsay_hello()',\n+            'def print_hello_world():\\n    print(\"Hello World!\")\\n\\n\\ndef print_hello_world_with_args(name',\n+            'def say_hello():\\n    print(\"Hello, World!\")\\n\\n\\nsay_hello()\\n',\n         ]\n         self.assertListEqual(outputs, expected_output)\n \n+    def test_newline_regression(self):\n+        \"\"\"Added to prevent regressions regarding attention (scaling) indicated by excessive newlines\"\"\"\n+        tokenizer = AutoTokenizer.from_pretrained(\"bigcode/tiny_starcoder_py\")\n+        model = GPTBigCodeForCausalLM.from_pretrained(\"bigcode/tiny_starcoder_py\").to(torch_device)\n+\n+        input_ids = tokenizer(\n+            \"Analyze the impact of the COVID-19 pandemic on global economic structures and future business models.\\n\",\n+            return_tensors=\"pt\",\n+        ).input_ids.to(torch_device)\n+\n+        output_sequence = model.generate(input_ids, max_new_tokens=20, do_sample=False)\n+        output_sentence = tokenizer.decode(output_sequence[0], skip_special_tokens=True)\n+\n+        expected_output = 'Analyze the impact of the COVID-19 pandemic on global economic structures and future business models.\\n\\nThe impact of the COVID-19 pandemic on global economic structures and future business'  # fmt: skip\n+        self.assertEqual(output_sentence, expected_output)\n+\n \n @require_torch\n class GPTBigCodeMQATest(unittest.TestCase):"
        }
    ],
    "stats": {
        "total": 25,
        "additions": 21,
        "deletions": 4
    }
}