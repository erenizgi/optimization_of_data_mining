{
    "author": "ydshieh",
    "message": "CI reporting improvements (#38230)\n\nupdate\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "feec294dea831568068ea90f729b2be3a253b024",
    "files": [
        {
            "sha": "653b50e4cf6516a224fefe024b0e1b21ef2cc0ab",
            "filename": ".github/workflows/check_failed_model_tests.yml",
            "status": "modified",
            "additions": 49,
            "deletions": 2,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/feec294dea831568068ea90f729b2be3a253b024/.github%2Fworkflows%2Fcheck_failed_model_tests.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/feec294dea831568068ea90f729b2be3a253b024/.github%2Fworkflows%2Fcheck_failed_model_tests.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fcheck_failed_model_tests.yml?ref=feec294dea831568068ea90f729b2be3a253b024",
            "patch": "@@ -39,63 +39,110 @@ jobs:\n           name: ci_results_run_models_gpu\n           path: /transformers/ci_results_run_models_gpu\n \n+      - name: Check file\n+        working-directory: /transformers\n+        run: |\n+          if [ -f ci_results_run_models_gpu/new_model_failures.json ]; then\n+            echo \"`ci_results_run_models_gpu/new_model_failures.json` exists, continue ...\"\n+            echo \"process=true\" >> $GITHUB_ENV\n+          else\n+            echo \"`ci_results_run_models_gpu/new_model_failures.json` doesn't exist, abort.\"\n+            echo \"process=false\" >> $GITHUB_ENV\n+          fi\n+\n+      - uses: actions/download-artifact@v4\n+        if: ${{ env.process == 'true' }}\n+        with:\n+          pattern: setup_values*\n+          path: setup_values\n+          merge-multiple: true\n+\n+      - name: Prepare some setup values\n+        if: ${{ env.process == 'true' }}\n+        run: |\n+          if [ -f setup_values/prev_workflow_run_id.txt ]; then\n+            echo \"PREV_WORKFLOW_RUN_ID=$(cat setup_values/prev_workflow_run_id.txt)\" >> $GITHUB_ENV\n+          else\n+            echo \"PREV_WORKFLOW_RUN_ID=\" >> $GITHUB_ENV\n+          fi\n+\n+          if [ -f setup_values/other_workflow_run_id.txt ]; then\n+            echo \"OTHER_WORKFLOW_RUN_ID=$(cat setup_values/other_workflow_run_id.txt)\" >> $GITHUB_ENV\n+          else\n+            echo \"OTHER_WORKFLOW_RUN_ID=\" >> $GITHUB_ENV\n+          fi\n+\n       - name: Update clone\n         working-directory: /transformers\n+        if: ${{ env.process == 'true' }}\n         run: git fetch && git checkout ${{ github.sha }}\n \n       - name: Get target commit\n         working-directory: /transformers/utils\n+        if: ${{ env.process == 'true' }}\n         run: |\n-          echo \"END_SHA=$(TOKEN=${{ secrets.ACCESS_REPO_INFO_TOKEN }} python3 -c 'import os; from get_previous_daily_ci import get_last_daily_ci_run_commit; commit=get_last_daily_ci_run_commit(token=os.environ[\"TOKEN\"]); print(commit)')\" >> $GITHUB_ENV\n+          echo \"END_SHA=$(TOKEN=${{ secrets.ACCESS_REPO_INFO_TOKEN }} python3 -c 'import os; from get_previous_daily_ci import get_last_daily_ci_run_commit; commit=get_last_daily_ci_run_commit(token=os.environ[\"TOKEN\"], workflow_run_id=os.environ[\"PREV_WORKFLOW_RUN_ID\"]); print(commit)')\" >> $GITHUB_ENV\n \n       - name: Checkout to `start_sha`\n         working-directory: /transformers\n+        if: ${{ env.process == 'true' }}\n         run: git fetch && git checkout ${{ inputs.start_sha }}\n \n       - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n         working-directory: /transformers\n+        if: ${{ env.process == 'true' }}\n         run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .\n \n       - name: NVIDIA-SMI\n+        if: ${{ env.process == 'true' }}\n         run: |\n           nvidia-smi\n \n       - name: Environment\n         working-directory: /transformers\n+        if: ${{ env.process == 'true' }}\n         run: |\n           python3 utils/print_env.py\n \n       - name: Show installed libraries and their versions\n         working-directory: /transformers\n+        if: ${{ env.process == 'true' }}\n         run: pip freeze\n \n       - name: Check failed tests\n         working-directory: /transformers\n+        if: ${{ env.process == 'true' }}\n         run: python3 utils/check_bad_commit.py --start_commit ${{ inputs.start_sha }} --end_commit ${{ env.END_SHA }} --file ci_results_run_models_gpu/new_model_failures.json --output_file new_model_failures_with_bad_commit.json\n \n       - name: Show results\n         working-directory: /transformers\n+        if: ${{ env.process == 'true' }}\n         run: |\n           ls -l new_model_failures_with_bad_commit.json\n           cat new_model_failures_with_bad_commit.json\n \n       - name: Checkout back\n         working-directory: /transformers\n+        if: ${{ env.process == 'true' }}\n         run: |\n           git checkout ${{ inputs.start_sha }}\n \n       - name: Process report\n         shell: bash\n         working-directory: /transformers\n+        if: ${{ env.process == 'true' }}\n         env:\n+          ACCESS_REPO_INFO_TOKEN: ${{ secrets.ACCESS_REPO_INFO_TOKEN }}\n           TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN: ${{ secrets.TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN }}\n         run: |\n           python3 utils/process_bad_commit_report.py\n \n       - name: Process report\n         shell: bash\n         working-directory: /transformers\n+        if: ${{ env.process == 'true' }}\n         env:\n+          ACCESS_REPO_INFO_TOKEN: ${{ secrets.ACCESS_REPO_INFO_TOKEN }}\n           TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN: ${{ secrets.TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN }}\n         run: |\n           {\n@@ -105,7 +152,7 @@ jobs:\n           } >> \"$GITHUB_ENV\"\n \n       - name: Send processed report\n-        if: ${{ !endsWith(env.REPORT_TEXT, '{}') }}\n+        if: ${{ env.process == 'true' && !endsWith(env.REPORT_TEXT, '{}') }}\n         uses: slackapi/slack-github-action@6c661ce58804a1a20f6dc5fbee7f0381b469e001\n         with:\n           # Slack channel id, channel name, or user id to post message."
        },
        {
            "sha": "77b33850fe4c7503f37777c04858246691487ed5",
            "filename": ".github/workflows/self-scheduled-caller.yml",
            "status": "modified",
            "additions": 35,
            "deletions": 0,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/feec294dea831568068ea90f729b2be3a253b024/.github%2Fworkflows%2Fself-scheduled-caller.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/feec294dea831568068ea90f729b2be3a253b024/.github%2Fworkflows%2Fself-scheduled-caller.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-scheduled-caller.yml?ref=feec294dea831568068ea90f729b2be3a253b024",
            "patch": "@@ -8,8 +8,43 @@ on:\n   push:\n     branches:\n       - run_scheduled_ci*\n+  workflow_dispatch:\n+    inputs:\n+      prev_workflow_run_id:\n+        description: 'previous workflow run id to compare'\n+        type: string\n+        required: false\n+        default: \"\"\n+      other_workflow_run_id:\n+        description: 'other workflow run id to compare'\n+        type: string\n+        required: false\n+        default: \"\"\n+\n+\n+# Used for `push` to easily modiffy the target workflow runs to compare against\n+env:\n+    prev_workflow_run_id: \"\"\n+    other_workflow_run_id: \"\"\n+\n \n jobs:\n+  setup:\n+    name: Setup\n+    runs-on: ubuntu-22.04\n+    steps:\n+      - name: Setup\n+        run: |\n+          mkdir \"setup_values\"\n+          echo \"${{ inputs.prev_workflow_run_id || env.prev_workflow_run_id }}\" > \"setup_values/prev_workflow_run_id.txt\"\n+          echo \"${{ inputs.other_workflow_run_id || env.other_workflow_run_id }}\" > \"setup_values/other_workflow_run_id.txt\"\n+\n+      - name: Upload artifacts\n+        uses: actions/upload-artifact@v4\n+        with:\n+          name: setup_values\n+          path: setup_values\n+\n   model-ci:\n     name: Model CI\n     uses: ./.github/workflows/self-scheduled.yml"
        },
        {
            "sha": "bea113ca031b8a11cd2762de551240fb34ade3c8",
            "filename": ".github/workflows/slack-report.yml",
            "status": "modified",
            "additions": 15,
            "deletions": 3,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/feec294dea831568068ea90f729b2be3a253b024/.github%2Fworkflows%2Fslack-report.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/feec294dea831568068ea90f729b2be3a253b024/.github%2Fworkflows%2Fslack-report.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fslack-report.yml?ref=feec294dea831568068ea90f729b2be3a253b024",
            "patch": "@@ -39,6 +39,21 @@ jobs:\n \n       - uses: actions/checkout@v4\n       - uses: actions/download-artifact@v4\n+\n+      - name: Prepare some setup values\n+        run: |\n+          if [ -f setup_values/prev_workflow_run_id.txt ]; then\n+            echo \"PREV_WORKFLOW_RUN_ID=$(cat setup_values/prev_workflow_run_id.txt)\" >> $GITHUB_ENV\n+          else\n+            echo \"PREV_WORKFLOW_RUN_ID=\" >> $GITHUB_ENV\n+          fi\n+\n+          if [ -f setup_values/other_workflow_run_id.txt ]; then\n+            echo \"OTHER_WORKFLOW_RUN_ID=$(cat setup_values/other_workflow_run_id.txt)\" >> $GITHUB_ENV\n+          else\n+            echo \"OTHER_WORKFLOW_RUN_ID=\" >> $GITHUB_ENV\n+          fi\n+\n       - name: Send message to Slack\n         if: ${{ inputs.job != 'run_quantization_torch_gpu' }}\n         env:\n@@ -50,15 +65,13 @@ jobs:\n           ACCESS_REPO_INFO_TOKEN: ${{ secrets.ACCESS_REPO_INFO_TOKEN }}\n           CI_EVENT: ${{ inputs.ci_event }}\n           CI_SHA: ${{ github.sha }}\n-          CI_WORKFLOW_REF: ${{ github.workflow_ref }}\n           CI_TEST_JOB: ${{ inputs.job }}\n           SETUP_STATUS: ${{ inputs.setup_status }}\n         # We pass `needs.setup.outputs.matrix` as the argument. A processing in `notification_service.py` to change\n         # `models/bert` to `models_bert` is required, as the artifact names use `_` instead of `/`.\n         # For a job that doesn't depend on (i.e. `needs`) `setup`, the value for `inputs.folder_slices` would be an\n         # empty string, and the called script still get one argument (which is the emtpy string).\n         run: |\n-          sudo apt-get install -y curl\n           pip install huggingface_hub\n           pip install slack_sdk\n           pip show slack_sdk\n@@ -86,7 +99,6 @@ jobs:\n         # We pass `needs.setup.outputs.quantization_matrix` as the argument. A processing in `notification_service_quantization.py` to change\n         # `quantization/bnb` to `quantization_bnb` is required, as the artifact names use `_` instead of `/`.\n         run: |\n-          sudo apt-get install -y curl\n           pip install huggingface_hub\n           pip install slack_sdk\n           pip show slack_sdk"
        },
        {
            "sha": "5d21b1c46519e8f03f588bbd5e3c4b0c552d93a8",
            "filename": "utils/check_bad_commit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/feec294dea831568068ea90f729b2be3a253b024/utils%2Fcheck_bad_commit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/feec294dea831568068ea90f729b2be3a253b024/utils%2Fcheck_bad_commit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_bad_commit.py?ref=feec294dea831568068ea90f729b2be3a253b024",
            "patch": "@@ -144,7 +144,8 @@ def get_commit_info(commit):\n         url = f\"https://api.github.com/repos/huggingface/transformers/pulls/{pr_number}\"\n         pr_for_commit = requests.get(url).json()\n         author = pr_for_commit[\"user\"][\"login\"]\n-        merged_author = pr_for_commit[\"merged_by\"][\"login\"]\n+        if pr_for_commit[\"merged_by\"] is not None:\n+            merged_author = pr_for_commit[\"merged_by\"][\"login\"]\n \n     if author is None:\n         url = f\"https://api.github.com/repos/huggingface/transformers/commits/{commit}\""
        },
        {
            "sha": "c9248facf91375b84e1144b8df7fa01a987199c6",
            "filename": "utils/get_previous_daily_ci.py",
            "status": "modified",
            "additions": 66,
            "deletions": 20,
            "changes": 86,
            "blob_url": "https://github.com/huggingface/transformers/blob/feec294dea831568068ea90f729b2be3a253b024/utils%2Fget_previous_daily_ci.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/feec294dea831568068ea90f729b2be3a253b024/utils%2Fget_previous_daily_ci.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fget_previous_daily_ci.py?ref=feec294dea831568068ea90f729b2be3a253b024",
            "patch": "@@ -5,7 +5,7 @@\n from get_ci_error_statistics import download_artifact, get_artifacts_links\n \n \n-def get_daily_ci_runs(token, num_runs=7):\n+def get_daily_ci_runs(token, num_runs=7, workflow_id=None):\n     \"\"\"Get the workflow runs of the scheduled (daily) CI.\n \n     This only selects the runs triggered by the `schedule` event on the `main` branch.\n@@ -18,7 +18,13 @@ def get_daily_ci_runs(token, num_runs=7):\n     # From a given workflow run (where we have workflow run id), we can get the workflow id by going to\n     # https://api.github.com/repos/huggingface/transformers/actions/runs/{workflow_run_id}\n     # and check the `workflow_id` key.\n-    workflow_id = \"90575235\"\n+\n+    if not workflow_id:\n+        workflow_run_id = os.environ[\"GITHUB_RUN_ID\"]\n+        workflow_run = requests.get(\n+            f\"https://api.github.com/repos/huggingface/transformers/actions/runs/{workflow_run_id}\", headers=headers\n+        ).json()\n+        workflow_id = workflow_run[\"workflow_id\"]\n \n     url = f\"https://api.github.com/repos/huggingface/transformers/actions/workflows/{workflow_id}/runs\"\n     # On `main` branch + event being `schedule` + not returning PRs + only `num_runs` results\n@@ -29,33 +35,64 @@ def get_daily_ci_runs(token, num_runs=7):\n     return result[\"workflow_runs\"]\n \n \n-def get_last_daily_ci_runs(token):\n+def get_last_daily_ci_run(token, workflow_run_id=None, workflow_id=None, commit_sha=None):\n     \"\"\"Get the last completed workflow run id of the scheduled (daily) CI.\"\"\"\n-    workflow_runs = get_daily_ci_runs(token)\n-    workflow_run_id = None\n-    for workflow_run in workflow_runs:\n-        if workflow_run[\"status\"] == \"completed\":\n-            workflow_run_id = workflow_run[\"id\"]\n+    headers = None\n+    if token is not None:\n+        headers = {\"Accept\": \"application/vnd.github+json\", \"Authorization\": f\"Bearer {token}\"}\n+\n+    workflow_run = None\n+    if workflow_run_id is not None and workflow_run_id != \"\":\n+        workflow_run = requests.get(\n+            f\"https://api.github.com/repos/huggingface/transformers/actions/runs/{workflow_run_id}\", headers=headers\n+        ).json()\n+        return workflow_run\n+\n+    workflow_runs = get_daily_ci_runs(token, workflow_id=workflow_id)\n+    for run in workflow_runs:\n+        if commit_sha in [None, \"\"] and run[\"status\"] == \"completed\":\n+            workflow_run = run\n+            break\n+        # if `commit_sha` is specified, and `workflow_run[\"head_sha\"]` matches it, return it.\n+        elif commit_sha not in [None, \"\"] and run[\"head_sha\"] == commit_sha:\n+            workflow_run = run\n             break\n \n+    return workflow_run\n+\n+\n+def get_last_daily_ci_workflow_run_id(token, workflow_run_id=None, workflow_id=None, commit_sha=None):\n+    \"\"\"Get the last completed workflow run id of the scheduled (daily) CI.\"\"\"\n+    if workflow_run_id is not None and workflow_run_id != \"\":\n+        return workflow_run_id\n+\n+    workflow_run = get_last_daily_ci_run(token, workflow_id=workflow_id, commit_sha=commit_sha)\n+    workflow_run_id = None\n+    if workflow_run is not None:\n+        workflow_run_id = workflow_run[\"id\"]\n+\n     return workflow_run_id\n \n \n-def get_last_daily_ci_run_commit(token):\n+def get_last_daily_ci_run_commit(token, workflow_run_id=None, workflow_id=None, commit_sha=None):\n     \"\"\"Get the commit sha of the last completed scheduled daily CI workflow run.\"\"\"\n-    workflow_runs = get_daily_ci_runs(token)\n-    head_sha = None\n-    for workflow_run in workflow_runs:\n-        if workflow_run[\"status\"] == \"completed\":\n-            head_sha = workflow_run[\"head_sha\"]\n-            break\n+    workflow_run = get_last_daily_ci_run(\n+        token, workflow_run_id=workflow_run_id, workflow_id=workflow_id, commit_sha=commit_sha\n+    )\n+    workflow_run_head_sha = None\n+    if workflow_run is not None:\n+        workflow_run_head_sha = workflow_run[\"head_sha\"]\n \n-    return head_sha\n+    return workflow_run_head_sha\n \n \n-def get_last_daily_ci_artifacts(artifact_names, output_dir, token):\n+def get_last_daily_ci_artifacts(\n+    artifact_names, output_dir, token, workflow_run_id=None, workflow_id=None, commit_sha=None\n+):\n     \"\"\"Get the artifacts of last completed workflow run id of the scheduled (daily) CI.\"\"\"\n-    workflow_run_id = get_last_daily_ci_runs(token)\n+    workflow_run_id = get_last_daily_ci_workflow_run_id(\n+        token, workflow_run_id=workflow_run_id, workflow_id=workflow_id, commit_sha=commit_sha\n+    )\n     if workflow_run_id is not None:\n         artifacts_links = get_artifacts_links(worflow_run_id=workflow_run_id, token=token)\n         for artifact_name in artifact_names:\n@@ -66,9 +103,18 @@ def get_last_daily_ci_artifacts(artifact_names, output_dir, token):\n                 )\n \n \n-def get_last_daily_ci_reports(artifact_names, output_dir, token):\n+def get_last_daily_ci_reports(\n+    artifact_names, output_dir, token, workflow_run_id=None, workflow_id=None, commit_sha=None\n+):\n     \"\"\"Get the artifacts' content of the last completed workflow run id of the scheduled (daily) CI.\"\"\"\n-    get_last_daily_ci_artifacts(artifact_names, output_dir, token)\n+    get_last_daily_ci_artifacts(\n+        artifact_names,\n+        output_dir,\n+        token,\n+        workflow_run_id=workflow_run_id,\n+        workflow_id=workflow_id,\n+        commit_sha=commit_sha,\n+    )\n \n     results = {}\n     for artifact_name in artifact_names:"
        },
        {
            "sha": "407ee47e592725c7ec1252a272e168c51c836b57",
            "filename": "utils/notification_service.py",
            "status": "modified",
            "additions": 163,
            "deletions": 118,
            "changes": 281,
            "blob_url": "https://github.com/huggingface/transformers/blob/feec294dea831568068ea90f729b2be3a253b024/utils%2Fnotification_service.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/feec294dea831568068ea90f729b2be3a253b024/utils%2Fnotification_service.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnotification_service.py?ref=feec294dea831568068ea90f729b2be3a253b024",
            "patch": "@@ -14,7 +14,6 @@\n \n import ast\n import collections\n-import datetime\n import functools\n import json\n import operator\n@@ -26,7 +25,7 @@\n \n import requests\n from get_ci_error_statistics import get_jobs\n-from get_previous_daily_ci import get_last_daily_ci_reports\n+from get_previous_daily_ci import get_last_daily_ci_reports, get_last_daily_ci_run, get_last_daily_ci_workflow_run_id\n from huggingface_hub import HfApi\n from slack_sdk import WebClient\n \n@@ -109,6 +108,7 @@ def __init__(\n         additional_results: Dict,\n         selected_warnings: Optional[List] = None,\n         prev_ci_artifacts=None,\n+        other_ci_artifacts=None,\n     ):\n         self.title = title\n         self.ci_title = ci_title\n@@ -159,6 +159,7 @@ def __init__(\n         self.selected_warnings = selected_warnings\n \n         self.prev_ci_artifacts = prev_ci_artifacts\n+        self.other_ci_artifacts = other_ci_artifacts\n \n     @property\n     def time(self) -> str:\n@@ -515,71 +516,83 @@ def payload(self) -> str:\n         if len(self.selected_warnings) > 0:\n             blocks.append(self.warnings)\n \n-        new_failure_blocks = self.get_new_model_failure_blocks(with_header=False)\n-        if len(new_failure_blocks) > 0:\n-            blocks.extend(new_failure_blocks)\n-\n-        # To save the list of new model failures\n-        extra_blocks = self.get_new_model_failure_blocks(to_truncate=False)\n-        if extra_blocks:\n-            failure_text = extra_blocks[-1][\"text\"][\"text\"]\n-            file_path = os.path.join(os.getcwd(), f\"ci_results_{job_name}/new_model_failures.txt\")\n-            with open(file_path, \"w\", encoding=\"UTF-8\") as fp:\n-                fp.write(failure_text)\n-\n-            # upload results to Hub dataset\n-            file_path = os.path.join(os.getcwd(), f\"ci_results_{job_name}/new_model_failures.txt\")\n-            commit_info = api.upload_file(\n-                path_or_fileobj=file_path,\n-                path_in_repo=f\"{datetime.datetime.today().strftime('%Y-%m-%d')}/ci_results_{job_name}/new_model_failures.txt\",\n-                repo_id=\"hf-internal-testing/transformers_daily_ci\",\n-                repo_type=\"dataset\",\n-                token=os.environ.get(\"TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN\", None),\n-            )\n-            url = f\"https://huggingface.co/datasets/hf-internal-testing/transformers_daily_ci/raw/{commit_info.oid}/{datetime.datetime.today().strftime('%Y-%m-%d')}/ci_results_{job_name}/new_model_failures.txt\"\n-\n-            # extra processing to save to json format\n-            new_failed_tests = {}\n-            for line in failure_text.split():\n-                if \"https://github.com/huggingface/transformers/actions/runs\" in line:\n-                    pattern = r\"<(https://github.com/huggingface/transformers/actions/runs/.+?/job/.+?)\\|(.+?)>\"\n-                    items = re.findall(pattern, line)\n-                elif \"tests/\" in line:\n-                    if \"tests/models/\" in line:\n-                        model = line.split(\"/\")[2]\n-                    else:\n-                        model = line.split(\"/\")[1]\n-                    if model not in new_failed_tests:\n-                        new_failed_tests[model] = {\"single-gpu\": [], \"multi-gpu\": []}\n-                    for url, device in items:\n-                        new_failed_tests[model][f\"{device}-gpu\"].append(line)\n-            file_path = os.path.join(os.getcwd(), f\"ci_results_{job_name}/new_model_failures.json\")\n-            with open(file_path, \"w\", encoding=\"UTF-8\") as fp:\n-                json.dump(new_failed_tests, fp, ensure_ascii=False, indent=4)\n-\n-            # upload results to Hub dataset\n-            file_path = os.path.join(os.getcwd(), f\"ci_results_{job_name}/new_model_failures.json\")\n-            _ = api.upload_file(\n-                path_or_fileobj=file_path,\n-                path_in_repo=f\"{datetime.datetime.today().strftime('%Y-%m-%d')}/ci_results_{job_name}/new_model_failures.json\",\n-                repo_id=\"hf-internal-testing/transformers_daily_ci\",\n-                repo_type=\"dataset\",\n-                token=os.environ.get(\"TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN\", None),\n-            )\n+        for idx, (prev_workflow_run_id, prev_ci_artifacts) in enumerate(\n+            [self.prev_ci_artifacts] + self.other_ci_artifacts\n+        ):\n+            if idx == 0:\n+                # This is the truncated version to show on slack. For now.\n+                new_failure_blocks = self.get_new_model_failure_blocks(\n+                    prev_ci_artifacts=prev_ci_artifacts, with_header=False\n+                )\n+                if len(new_failure_blocks) > 0:\n+                    blocks.extend(new_failure_blocks)\n+\n+            # To save the list of new model failures and uploaed to hub repositories\n+            extra_blocks = self.get_new_model_failure_blocks(prev_ci_artifacts=prev_ci_artifacts, to_truncate=False)\n+            if extra_blocks:\n+                filename = \"new_model_failures\"\n+                if idx > 0:\n+                    filename = f\"{filename}_against_{prev_workflow_run_id}\"\n+\n+                failure_text = extra_blocks[-1][\"text\"][\"text\"]\n+                file_path = os.path.join(os.getcwd(), f\"ci_results_{job_name}/{filename}.txt\")\n+                with open(file_path, \"w\", encoding=\"UTF-8\") as fp:\n+                    fp.write(failure_text)\n+\n+                # upload results to Hub dataset\n+                file_path = os.path.join(os.getcwd(), f\"ci_results_{job_name}/{filename}.txt\")\n+                commit_info = api.upload_file(\n+                    path_or_fileobj=file_path,\n+                    path_in_repo=f\"{report_repo_folder}/ci_results_{job_name}/{filename}.txt\",\n+                    repo_id=\"hf-internal-testing/transformers_daily_ci\",\n+                    repo_type=\"dataset\",\n+                    token=os.environ.get(\"TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN\", None),\n+                )\n+                url = f\"https://huggingface.co/datasets/hf-internal-testing/transformers_daily_ci/raw/{commit_info.oid}/{report_repo_folder}/ci_results_{job_name}/{filename}.txt\"\n+\n+                # extra processing to save to json format\n+                new_failed_tests = {}\n+                for line in failure_text.split():\n+                    if \"https://github.com/huggingface/transformers/actions/runs\" in line:\n+                        pattern = r\"<(https://github.com/huggingface/transformers/actions/runs/.+?/job/.+?)\\|(.+?)>\"\n+                        items = re.findall(pattern, line)\n+                    elif \"tests/\" in line:\n+                        if \"tests/models/\" in line:\n+                            model = line.split(\"/\")[2]\n+                        else:\n+                            model = line.split(\"/\")[1]\n+                        if model not in new_failed_tests:\n+                            new_failed_tests[model] = {\"single-gpu\": [], \"multi-gpu\": []}\n+                        for url, device in items:\n+                            new_failed_tests[model][f\"{device}-gpu\"].append(line)\n+                file_path = os.path.join(os.getcwd(), f\"ci_results_{job_name}/{filename}.json\")\n+                with open(file_path, \"w\", encoding=\"UTF-8\") as fp:\n+                    json.dump(new_failed_tests, fp, ensure_ascii=False, indent=4)\n+\n+                # upload results to Hub dataset\n+                file_path = os.path.join(os.getcwd(), f\"ci_results_{job_name}/{filename}.json\")\n+                _ = api.upload_file(\n+                    path_or_fileobj=file_path,\n+                    path_in_repo=f\"{report_repo_folder}/ci_results_{job_name}/{filename}.json\",\n+                    repo_id=\"hf-internal-testing/transformers_daily_ci\",\n+                    repo_type=\"dataset\",\n+                    token=os.environ.get(\"TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN\", None),\n+                )\n \n-            block = {\n-                \"type\": \"section\",\n-                \"text\": {\n-                    \"type\": \"plain_text\",\n-                    \"text\": \" \",\n-                },\n-                \"accessory\": {\n-                    \"type\": \"button\",\n-                    \"text\": {\"type\": \"plain_text\", \"text\": \"Check New model failures\"},\n-                    \"url\": url,\n-                },\n-            }\n-            blocks.append(block)\n+                if idx == 0:\n+                    block = {\n+                        \"type\": \"section\",\n+                        \"text\": {\n+                            \"type\": \"plain_text\",\n+                            \"text\": \" \",\n+                        },\n+                        \"accessory\": {\n+                            \"type\": \"button\",\n+                            \"text\": {\"type\": \"plain_text\", \"text\": \"Check New model failures\"},\n+                            \"url\": url,\n+                        },\n+                    }\n+                    blocks.append(block)\n \n         return json.dumps(blocks)\n \n@@ -700,18 +713,18 @@ def get_reply_blocks(self, job_name, job_result, failures, device, text):\n             {\"type\": \"section\", \"text\": {\"type\": \"mrkdwn\", \"text\": failure_text}},\n         ]\n \n-    def get_new_model_failure_blocks(self, with_header=True, to_truncate=True):\n-        if self.prev_ci_artifacts is None:\n+    def get_new_model_failure_blocks(self, prev_ci_artifacts, with_header=True, to_truncate=True):\n+        if prev_ci_artifacts is None:\n             return []\n \n         sorted_dict = sorted(self.model_results.items(), key=lambda t: t[0])\n \n         prev_model_results = {}\n         if (\n-            f\"ci_results_{job_name}\" in self.prev_ci_artifacts\n-            and \"model_results.json\" in self.prev_ci_artifacts[f\"ci_results_{job_name}\"]\n+            f\"ci_results_{job_name}\" in prev_ci_artifacts\n+            and \"model_results.json\" in prev_ci_artifacts[f\"ci_results_{job_name}\"]\n         ):\n-            prev_model_results = json.loads(self.prev_ci_artifacts[f\"ci_results_{job_name}\"][\"model_results.json\"])\n+            prev_model_results = json.loads(prev_ci_artifacts[f\"ci_results_{job_name}\"][\"model_results.json\"])\n \n         all_failure_lines = {}\n         for job, job_result in sorted_dict:\n@@ -812,20 +825,6 @@ def post_reply(self):\n \n                     time.sleep(1)\n \n-        blocks = self.get_new_model_failure_blocks()\n-        if blocks:\n-            print(\"Sending the following reply\")\n-            print(json.dumps({\"blocks\": blocks}))\n-\n-            client.chat_postMessage(\n-                channel=SLACK_REPORT_CHANNEL_ID,\n-                text=\"Results for new failures\",\n-                blocks=blocks,\n-                thread_ts=self.thread_ts[\"ts\"],\n-            )\n-\n-            time.sleep(1)\n-\n \n def retrieve_artifact(artifact_path: str, gpu: Optional[str]):\n     if gpu not in [None, \"single\", \"multi\"]:\n@@ -1168,6 +1167,23 @@ def pop_default(l: list[Any], i: int, default: Any) -> Any:\n         \"run_torch_cuda_extensions_gpu\": \"DeepSpeed\",\n     }\n \n+    # if it is not a scheduled run, upload the reports to a subfolder under `report_repo_folder`\n+    report_repo_subfolder = \"\"\n+    if os.getenv(\"GITHUB_EVENT_NAME\") != \"schedule\":\n+        report_repo_subfolder = f\"{os.getenv('GITHUB_RUN_NUMBER')}-{os.getenv('GITHUB_RUN_ID')}\"\n+        report_repo_subfolder = f\"runs/{report_repo_subfolder}\"\n+\n+    workflow_run = get_last_daily_ci_run(\n+        token=os.environ[\"ACCESS_REPO_INFO_TOKEN\"], workflow_run_id=os.getenv(\"GITHUB_RUN_ID\")\n+    )\n+    workflow_run_created_time = workflow_run[\"created_at\"]\n+    workflow_id = workflow_run[\"workflow_id\"]\n+\n+    report_repo_folder = workflow_run_created_time.split(\"T\")[0]\n+\n+    if report_repo_subfolder:\n+        report_repo_folder = f\"{report_repo_folder}/{report_repo_subfolder}\"\n+\n     # Remove some entries in `additional_files` if they are not concerned.\n     test_name = None\n     if job_name in job_to_test_map:\n@@ -1241,24 +1257,23 @@ def pop_default(l: list[Any], i: int, default: Any) -> Any:\n     if not os.path.isdir(os.path.join(os.getcwd(), f\"ci_results_{job_name}\")):\n         os.makedirs(os.path.join(os.getcwd(), f\"ci_results_{job_name}\"))\n \n-    target_workflow = \"huggingface/transformers/.github/workflows/self-scheduled-caller.yml@refs/heads/main\"\n-    is_scheduled_ci_run = os.environ.get(\"CI_WORKFLOW_REF\") == target_workflow\n+    nvidia_daily_ci_workflow = \"huggingface/transformers/.github/workflows/self-scheduled-caller.yml\"\n+    is_nvidia_daily_ci_workflow = os.environ.get(\"GITHUB_WORKFLOW_REF\").startswith(nvidia_daily_ci_workflow)\n+    is_scheduled_ci_run = os.environ.get(\"GITHUB_EVENT_NAME\") == \"schedule\"\n \n     # Only the model testing job is concerned: this condition is to avoid other jobs to upload the empty list as\n     # results.\n     if job_name == \"run_models_gpu\":\n         with open(f\"ci_results_{job_name}/model_results.json\", \"w\", encoding=\"UTF-8\") as fp:\n             json.dump(model_results, fp, indent=4, ensure_ascii=False)\n \n-        # upload results to Hub dataset (only for the scheduled daily CI run on `main`)\n-        if is_scheduled_ci_run:\n-            api.upload_file(\n-                path_or_fileobj=f\"ci_results_{job_name}/model_results.json\",\n-                path_in_repo=f\"{datetime.datetime.today().strftime('%Y-%m-%d')}/ci_results_{job_name}/model_results.json\",\n-                repo_id=\"hf-internal-testing/transformers_daily_ci\",\n-                repo_type=\"dataset\",\n-                token=os.environ.get(\"TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN\", None),\n-            )\n+        api.upload_file(\n+            path_or_fileobj=f\"ci_results_{job_name}/model_results.json\",\n+            path_in_repo=f\"{report_repo_folder}/ci_results_{job_name}/model_results.json\",\n+            repo_id=\"hf-internal-testing/transformers_daily_ci\",\n+            repo_type=\"dataset\",\n+            token=os.environ.get(\"TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN\", None),\n+        )\n \n         # Let's create a file contain job --> job link\n         model_job_links = {}\n@@ -1272,15 +1287,13 @@ def pop_default(l: list[Any], i: int, default: Any) -> Any:\n         with open(f\"ci_results_{job_name}/model_job_links.json\", \"w\", encoding=\"UTF-8\") as fp:\n             json.dump(model_job_links, fp, indent=4, ensure_ascii=False)\n \n-        # upload results to Hub dataset (only for the scheduled daily CI run on `main`)\n-        if is_scheduled_ci_run:\n-            api.upload_file(\n-                path_or_fileobj=f\"ci_results_{job_name}/model_job_links.json\",\n-                path_in_repo=f\"{datetime.datetime.today().strftime('%Y-%m-%d')}/ci_results_{job_name}/model_job_links.json\",\n-                repo_id=\"hf-internal-testing/transformers_daily_ci\",\n-                repo_type=\"dataset\",\n-                token=os.environ.get(\"TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN\", None),\n-            )\n+        api.upload_file(\n+            path_or_fileobj=f\"ci_results_{job_name}/model_job_links.json\",\n+            path_in_repo=f\"{report_repo_folder}/ci_results_{job_name}/model_job_links.json\",\n+            repo_id=\"hf-internal-testing/transformers_daily_ci\",\n+            repo_type=\"dataset\",\n+            token=os.environ.get(\"TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN\", None),\n+        )\n \n     # Must have the same keys as in `additional_results`.\n     # The values are used as the file names where to save the corresponding CI job results.\n@@ -1294,26 +1307,57 @@ def pop_default(l: list[Any], i: int, default: Any) -> Any:\n         with open(f\"ci_results_{job_name}/{test_to_result_name[job]}_results.json\", \"w\", encoding=\"UTF-8\") as fp:\n             json.dump(job_result, fp, indent=4, ensure_ascii=False)\n \n-        # upload results to Hub dataset (only for the scheduled daily CI run on `main`)\n-        if is_scheduled_ci_run:\n-            api.upload_file(\n-                path_or_fileobj=f\"ci_results_{job_name}/{test_to_result_name[job]}_results.json\",\n-                path_in_repo=f\"{datetime.datetime.today().strftime('%Y-%m-%d')}/ci_results_{job_name}/{test_to_result_name[job]}_results.json\",\n-                repo_id=\"hf-internal-testing/transformers_daily_ci\",\n-                repo_type=\"dataset\",\n-                token=os.environ.get(\"TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN\", None),\n-            )\n+        api.upload_file(\n+            path_or_fileobj=f\"ci_results_{job_name}/{test_to_result_name[job]}_results.json\",\n+            path_in_repo=f\"{report_repo_folder}/ci_results_{job_name}/{test_to_result_name[job]}_results.json\",\n+            repo_id=\"hf-internal-testing/transformers_daily_ci\",\n+            repo_type=\"dataset\",\n+            token=os.environ.get(\"TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN\", None),\n+        )\n+\n+    prev_workflow_run_id = None\n+    other_workflow_run_ids = []\n \n-    prev_ci_artifacts = None\n     if is_scheduled_ci_run:\n+        # TODO: remove `if job_name == \"run_models_gpu\"`\n         if job_name == \"run_models_gpu\":\n-            # Get the last previously completed CI's failure tables\n+            prev_workflow_run_id = get_last_daily_ci_workflow_run_id(\n+                token=os.environ[\"ACCESS_REPO_INFO_TOKEN\"], workflow_id=workflow_id\n+            )\n+            # For a scheduled run that is not the Nvidia's scheduled daily CI, add Nvidia's scheduled daily CI run as a target to compare.\n+            if not is_nvidia_daily_ci_workflow:\n+                # The id of the workflow `.github/workflows/self-scheduled-caller.yml` (not of a workflow run of it).\n+                other_workflow_id = \"90575235\"\n+                # We need to get the Nvidia's scheduled daily CI run that match the current run (i.e. run with the same commit SHA)\n+                other_workflow_run_id = get_last_daily_ci_workflow_run_id(\n+                    token=os.environ[\"ACCESS_REPO_INFO_TOKEN\"], workflow_id=other_workflow_id, commit_sha=ci_sha\n+                )\n+                other_workflow_run_ids.append(other_workflow_run_id)\n+    else:\n+        prev_workflow_run_id = os.environ[\"PREV_WORKFLOW_RUN_ID\"]\n+        other_workflow_run_id = os.environ[\"OTHER_WORKFLOW_RUN_ID\"]\n+        other_workflow_run_ids.append(other_workflow_run_id)\n+\n+    prev_ci_artifacts = (None, None)\n+    other_ci_artifacts = []\n+\n+    for idx, target_workflow_run_id in enumerate([prev_workflow_run_id] + other_workflow_run_ids):\n+        if target_workflow_run_id is None or target_workflow_run_id == \"\":\n+            continue\n+        else:\n             artifact_names = [f\"ci_results_{job_name}\"]\n             output_dir = os.path.join(os.getcwd(), \"previous_reports\")\n             os.makedirs(output_dir, exist_ok=True)\n-            prev_ci_artifacts = get_last_daily_ci_reports(\n-                artifact_names=artifact_names, output_dir=output_dir, token=os.environ[\"ACCESS_REPO_INFO_TOKEN\"]\n+            ci_artifacts = get_last_daily_ci_reports(\n+                artifact_names=artifact_names,\n+                output_dir=output_dir,\n+                token=os.environ[\"ACCESS_REPO_INFO_TOKEN\"],\n+                workflow_run_id=target_workflow_run_id,\n             )\n+            if idx == 0:\n+                prev_ci_artifacts = (target_workflow_run_id, ci_artifacts)\n+            else:\n+                other_ci_artifacts.append((target_workflow_run_id, ci_artifacts))\n \n     job_to_test_map.update(\n         {\n@@ -1335,6 +1379,7 @@ def pop_default(l: list[Any], i: int, default: Any) -> Any:\n         additional_results,\n         selected_warnings=selected_warnings,\n         prev_ci_artifacts=prev_ci_artifacts,\n+        other_ci_artifacts=other_ci_artifacts,\n     )\n \n     # send report only if there is any failure (for push CI)"
        },
        {
            "sha": "dc9678c78123c659daab5609128647bc6d1222c0",
            "filename": "utils/notification_service_quantization.py",
            "status": "modified",
            "additions": 30,
            "deletions": 12,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/feec294dea831568068ea90f729b2be3a253b024/utils%2Fnotification_service_quantization.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/feec294dea831568068ea90f729b2be3a253b024/utils%2Fnotification_service_quantization.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnotification_service_quantization.py?ref=feec294dea831568068ea90f729b2be3a253b024",
            "patch": "@@ -13,14 +13,14 @@\n # limitations under the License.\n \n import ast\n-import datetime\n import json\n import os\n import sys\n import time\n from typing import Dict\n \n from get_ci_error_statistics import get_jobs\n+from get_previous_daily_ci import get_last_daily_ci_run\n from huggingface_hub import HfApi\n from notification_service import (\n     Message,\n@@ -246,24 +246,42 @@ def post_reply(self):\n                         )\n \n     job_name = os.getenv(\"CI_TEST_JOB\")\n+\n+    # if it is not a scheduled run, upload the reports to a subfolder under `report_repo_folder`\n+    report_repo_subfolder = \"\"\n+    if os.getenv(\"GITHUB_EVENT_NAME\") != \"schedule\":\n+        report_repo_subfolder = f\"{os.getenv('GITHUB_RUN_NUMBER')}-{os.getenv('GITHUB_RUN_ID')}\"\n+        report_repo_subfolder = f\"runs/{report_repo_subfolder}\"\n+\n+    workflow_run = get_last_daily_ci_run(\n+        token=os.environ[\"ACCESS_REPO_INFO_TOKEN\"], workflow_run_id=os.getenv(\"GITHUB_RUN_ID\")\n+    )\n+    workflow_run_created_time = workflow_run[\"created_at\"]\n+    workflow_id = workflow_run[\"workflow_id\"]\n+\n+    report_repo_folder = workflow_run_created_time.split(\"T\")[0]\n+\n+    if report_repo_subfolder:\n+        report_repo_folder = f\"{report_repo_folder}/{report_repo_subfolder}\"\n+\n     if not os.path.isdir(os.path.join(os.getcwd(), f\"ci_results_{job_name}\")):\n         os.makedirs(os.path.join(os.getcwd(), f\"ci_results_{job_name}\"))\n \n+    nvidia_daily_ci_workflow = \"huggingface/transformers/.github/workflows/self-scheduled-caller.yml\"\n+    is_nvidia_daily_ci_workflow = os.environ.get(\"GITHUB_WORKFLOW_REF\").startswith(nvidia_daily_ci_workflow)\n+    is_scheduled_ci_run = os.environ.get(\"GITHUB_EVENT_NAME\") == \"schedule\"\n+\n     with open(f\"ci_results_{job_name}/quantization_results.json\", \"w\", encoding=\"UTF-8\") as fp:\n         json.dump(quantization_results, fp, indent=4, ensure_ascii=False)\n \n-    target_workflow = \"huggingface/transformers/.github/workflows/self-scheduled-caller.yml@refs/heads/main\"\n-    is_scheduled_ci_run = os.environ.get(\"CI_WORKFLOW_REF\") == target_workflow\n-\n     # upload results to Hub dataset (only for the scheduled daily CI run on `main`)\n-    if is_scheduled_ci_run:\n-        api.upload_file(\n-            path_or_fileobj=f\"ci_results_{job_name}/quantization_results.json\",\n-            path_in_repo=f\"{datetime.datetime.today().strftime('%Y-%m-%d')}/ci_results_{job_name}/quantization_results.json\",\n-            repo_id=\"hf-internal-testing/transformers_daily_ci\",\n-            repo_type=\"dataset\",\n-            token=os.environ.get(\"TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN\", None),\n-        )\n+    api.upload_file(\n+        path_or_fileobj=f\"ci_results_{job_name}/quantization_results.json\",\n+        path_in_repo=f\"{report_repo_folder}/ci_results_{job_name}/quantization_results.json\",\n+        repo_id=\"hf-internal-testing/transformers_daily_ci\",\n+        repo_type=\"dataset\",\n+        token=os.environ.get(\"TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN\", None),\n+    )\n \n     message = QuantizationMessage(\n         title,"
        },
        {
            "sha": "50c338b6335833be82e089161fd2c645ea753961",
            "filename": "utils/process_bad_commit_report.py",
            "status": "modified",
            "additions": 19,
            "deletions": 3,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/feec294dea831568068ea90f729b2be3a253b024/utils%2Fprocess_bad_commit_report.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/feec294dea831568068ea90f729b2be3a253b024/utils%2Fprocess_bad_commit_report.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fprocess_bad_commit_report.py?ref=feec294dea831568068ea90f729b2be3a253b024",
            "patch": "@@ -12,12 +12,12 @@\n ```\n \"\"\"\n \n-import datetime\n import json\n import os\n from collections import Counter\n from copy import deepcopy\n \n+from get_previous_daily_ci import get_last_daily_ci_run\n from huggingface_hub import HfApi\n \n \n@@ -76,16 +76,32 @@\n         new_data_full[author] = {k: v for k, v in _data.items() if len(v) > 0}\n \n     # Upload to Hub and get the url\n+    # if it is not a scheduled run, upload the reports to a subfolder under `report_repo_folder`\n+    report_repo_subfolder = \"\"\n+    if os.getenv(\"GITHUB_EVENT_NAME\") != \"schedule\":\n+        report_repo_subfolder = f\"{os.getenv('GITHUB_RUN_NUMBER')}-{os.getenv('GITHUB_RUN_ID')}\"\n+        report_repo_subfolder = f\"runs/{report_repo_subfolder}\"\n+\n+    workflow_run = get_last_daily_ci_run(\n+        token=os.environ[\"ACCESS_REPO_INFO_TOKEN\"], workflow_run_id=os.getenv(\"GITHUB_RUN_ID\")\n+    )\n+    workflow_run_created_time = workflow_run[\"created_at\"]\n+\n+    report_repo_folder = workflow_run_created_time.split(\"T\")[0]\n+\n+    if report_repo_subfolder:\n+        report_repo_folder = f\"{report_repo_folder}/{report_repo_subfolder}\"\n+\n     with open(\"new_model_failures_with_bad_commit_grouped_by_authors.json\", \"w\") as fp:\n         json.dump(new_data_full, fp, ensure_ascii=False, indent=4)\n     commit_info = api.upload_file(\n         path_or_fileobj=\"new_model_failures_with_bad_commit_grouped_by_authors.json\",\n-        path_in_repo=f\"{datetime.datetime.today().strftime('%Y-%m-%d')}/ci_results_run_models_gpu/new_model_failures_with_bad_commit_grouped_by_authors.json\",\n+        path_in_repo=f\"{report_repo_folder}/ci_results_run_models_gpu/new_model_failures_with_bad_commit_grouped_by_authors.json\",\n         repo_id=\"hf-internal-testing/transformers_daily_ci\",\n         repo_type=\"dataset\",\n         token=os.environ.get(\"TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN\", None),\n     )\n-    url = f\"https://huggingface.co/datasets/hf-internal-testing/transformers_daily_ci/raw/{commit_info.oid}/{datetime.datetime.today().strftime('%Y-%m-%d')}/ci_results_run_models_gpu/new_model_failures_with_bad_commit_grouped_by_authors.json\"\n+    url = f\"https://huggingface.co/datasets/hf-internal-testing/transformers_daily_ci/raw/{commit_info.oid}/{report_repo_folder}/ci_results_run_models_gpu/new_model_failures_with_bad_commit_grouped_by_authors.json\"\n \n     # Add `GH_` prefix as keyword mention\n     output = {}"
        }
    ],
    "stats": {
        "total": 538,
        "additions": 379,
        "deletions": 159
    }
}