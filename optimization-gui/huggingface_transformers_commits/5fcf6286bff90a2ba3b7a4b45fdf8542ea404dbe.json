{
    "author": "qubvel",
    "message": "Add TimmWrapper (#34564)\n\n* Add files\r\n\r\n* Init\r\n\r\n* Add TimmWrapperModel\r\n\r\n* Fix up\r\n\r\n* Some fixes\r\n\r\n* Fix up\r\n\r\n* Remove old file\r\n\r\n* Sort out import orders\r\n\r\n* Fix some model loading\r\n\r\n* Compatible with pipeline and trainer\r\n\r\n* Fix up\r\n\r\n* Delete test_timm_model_1/config.json\r\n\r\n* Remove accidentally commited files\r\n\r\n* Delete src/transformers/models/modeling_timm_wrapper.py\r\n\r\n* Remove empty imports; fix transformations applied\r\n\r\n* Tidy up\r\n\r\n* Add image classifcation model to special cases\r\n\r\n* Create pretrained model; enable device_map='auto'\r\n\r\n* Enable most tests; fix init order\r\n\r\n* Sort imports\r\n\r\n* [run-slow] timm_wrapper\r\n\r\n* Pass num_classes into timm.create_model\r\n\r\n* Remove train transforms from image processor\r\n\r\n* Update timm creation with pretrained=False\r\n\r\n* Fix gamma/beta issue for timm models\r\n\r\n* Fixing gamma and beta renaming for timm models\r\n\r\n* Simplify config and model creation\r\n\r\n* Remove attn_implementation diff\r\n\r\n* Fixup\r\n\r\n* Docstrings\r\n\r\n* Fix warning msg text according to test case\r\n\r\n* Fix device_map auto\r\n\r\n* Set dtype and device for pixel_values in forward\r\n\r\n* Enable output hidden states\r\n\r\n* Enable tests for hidden_states and model parallel\r\n\r\n* Remove default scriptable arg\r\n\r\n* Refactor inner model\r\n\r\n* Update timm version\r\n\r\n* Fix _find_mismatched_keys function\r\n\r\n* Change inheritance for Classification model (fix weights loading with device_map)\r\n\r\n* Minor bugfix\r\n\r\n* Disable save pretrained for image processor\r\n\r\n* Rename hook method for loaded keys correction\r\n\r\n* Rename state dict keys on save, remove `timm_model` prefix, make checkpoint compatible with `timm`\r\n\r\n* Managing num_labels <-> num_classes attributes\r\n\r\n* Enable loading checkpoints in Trainer to resume training\r\n\r\n* Update error message for output_hidden_states\r\n\r\n* Add output hidden states test\r\n\r\n* Decouple base and classification models\r\n\r\n* Add more test cases\r\n\r\n* Add save-load-to-timm test\r\n\r\n* Fix test name\r\n\r\n* Fixup\r\n\r\n* Add do_pooling\r\n\r\n* Add test for do_pooling\r\n\r\n* Fix doc\r\n\r\n* Add tests for TimmWrapperModel\r\n\r\n* Add validation for `num_classes=0` in timm config + test for DINO checkpoint\r\n\r\n* Adjust atol for test\r\n\r\n* Fix docs\r\n\r\n* dev-ci\r\n\r\n* dev-ci\r\n\r\n* Add tests for image processor\r\n\r\n* Update docs\r\n\r\n* Update init to new format\r\n\r\n* Update docs in configuration\r\n\r\n* Fix some docs in image processor\r\n\r\n* Improve docs for modeling\r\n\r\n* fix for is_timm_checkpoint\r\n\r\n* Update code examples\r\n\r\n* Fix header\r\n\r\n* Fix typehint\r\n\r\n* Increase tolerance a bit\r\n\r\n* Fix Path\r\n\r\n* Fixing model parallel tests\r\n\r\n* Disable \"parallel\" tests\r\n\r\n* Add comment for metadata\r\n\r\n* Refactor AutoImageProcessor for timm wrapper loading\r\n\r\n* Remove custom test_model_outputs_equivalence\r\n\r\n* Add require_timm decorator\r\n\r\n* Fix comment\r\n\r\n* Make image processor work with older timm versions and tensor input\r\n\r\n* Save config instead of whole model in image processor tests\r\n\r\n* Add docstring for `image_processor_filename`\r\n\r\n* Sanitize kwargs for timm image processor\r\n\r\n* Fix doc style\r\n\r\n* Update check for tensor input\r\n\r\n* Update normalize\r\n\r\n* Remove _load_timm_model function\r\n\r\n---------\r\n\r\nCo-authored-by: Amy Roberts <22614925+amyeroberts@users.noreply.github.com>",
    "sha": "5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe",
    "files": [
        {
            "sha": "4d06cd612cd2e6aadc32338096b21bcb0c5b909c",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe",
            "patch": "@@ -705,6 +705,8 @@\n         title: Swin2SR\n       - local: model_doc/table-transformer\n         title: Table Transformer\n+      - local: model_doc/timm_wrapper\n+        title: Timm Wrapper\n       - local: model_doc/upernet\n         title: UperNet\n       - local: model_doc/van"
        },
        {
            "sha": "36a479dabc8fa6269d09c95434c81ca81901d12b",
            "filename": "docs/source/en/index.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/docs%2Fsource%2Fen%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/docs%2Fsource%2Fen%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Findex.md?ref=5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe",
            "patch": "@@ -321,6 +321,7 @@ Flax), PyTorch, and/or TensorFlow.\n |                         [TAPEX](model_doc/tapex)                         |       ✅        |         ✅         |      ✅      |\n |       [Time Series Transformer](model_doc/time_series_transformer)       |       ✅        |         ❌         |      ❌      |\n |                   [TimeSformer](model_doc/timesformer)                   |       ✅        |         ❌         |      ❌      |\n+|                [TimmWrapperModel](model_doc/timm_wrapper)                |       ✅        |         ❌         |      ❌      |\n |        [Trajectory Transformer](model_doc/trajectory_transformer)        |       ✅        |         ❌         |      ❌      |\n |                  [Transformer-XL](model_doc/transfo-xl)                  |       ✅        |         ✅         |      ❌      |\n |                         [TrOCR](model_doc/trocr)                         |       ✅        |         ❌         |      ❌      |"
        },
        {
            "sha": "5af3d51746c325cc9085076fdfc8feaaa1ddb350",
            "filename": "docs/source/en/model_doc/timm_wrapper.md",
            "status": "added",
            "additions": 67,
            "deletions": 0,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/docs%2Fsource%2Fen%2Fmodel_doc%2Ftimm_wrapper.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/docs%2Fsource%2Fen%2Fmodel_doc%2Ftimm_wrapper.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftimm_wrapper.md?ref=5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe",
            "patch": "@@ -0,0 +1,67 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# TimmWrapper\n+\n+## Overview\n+\n+Helper class to enable loading timm models to be used with the transformers library and its autoclasses.\n+\n+```python\n+>>> import torch\n+>>> from PIL import Image\n+>>> from urllib.request import urlopen\n+>>> from transformers import AutoModelForImageClassification, AutoImageProcessor\n+\n+>>> # Load image\n+>>> image = Image.open(urlopen(\n+...     'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n+... ))\n+\n+>>> # Load model and image processor\n+>>> checkpoint = \"timm/resnet50.a1_in1k\"\n+>>> image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n+>>> model = AutoModelForImageClassification.from_pretrained(checkpoint).eval()\n+\n+>>> # Preprocess image\n+>>> inputs = image_processor(image)\n+\n+>>> # Forward pass\n+>>> with torch.no_grad():\n+...     logits = model(**inputs).logits\n+\n+>>> # Get top 5 predictions\n+>>> top5_probabilities, top5_class_indices = torch.topk(logits.softmax(dim=1) * 100, k=5)\n+```\n+\n+## TimmWrapperConfig\n+\n+[[autodoc]] TimmWrapperConfig\n+\n+## TimmWrapperImageProcessor\n+\n+[[autodoc]] TimmWrapperImageProcessor\n+    - preprocess\n+\n+## TimmWrapperModel\n+\n+[[autodoc]] TimmWrapperModel\n+    - forward\n+\n+## TimmWrapperForImageClassification\n+\n+[[autodoc]] TimmWrapperForImageClassification\n+    - forward"
        },
        {
            "sha": "111d8adce8b41afc3cfe5de8691f0769b52f4cfe",
            "filename": "examples/pytorch/image-classification/run_image_classification.py",
            "status": "modified",
            "additions": 30,
            "deletions": 24,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/examples%2Fpytorch%2Fimage-classification%2Frun_image_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/examples%2Fpytorch%2Fimage-classification%2Frun_image_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fimage-classification%2Frun_image_classification.py?ref=5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe",
            "patch": "@@ -42,6 +42,7 @@\n     AutoImageProcessor,\n     AutoModelForImageClassification,\n     HfArgumentParser,\n+    TimmWrapperImageProcessor,\n     Trainer,\n     TrainingArguments,\n     set_seed,\n@@ -329,31 +330,36 @@ def compute_metrics(p):\n     )\n \n     # Define torchvision transforms to be applied to each image.\n-    if \"shortest_edge\" in image_processor.size:\n-        size = image_processor.size[\"shortest_edge\"]\n+    if isinstance(image_processor, TimmWrapperImageProcessor):\n+        _train_transforms = image_processor.train_transforms\n+        _val_transforms = image_processor.val_transforms\n     else:\n-        size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n-    normalize = (\n-        Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n-        if hasattr(image_processor, \"image_mean\") and hasattr(image_processor, \"image_std\")\n-        else Lambda(lambda x: x)\n-    )\n-    _train_transforms = Compose(\n-        [\n-            RandomResizedCrop(size),\n-            RandomHorizontalFlip(),\n-            ToTensor(),\n-            normalize,\n-        ]\n-    )\n-    _val_transforms = Compose(\n-        [\n-            Resize(size),\n-            CenterCrop(size),\n-            ToTensor(),\n-            normalize,\n-        ]\n-    )\n+        if \"shortest_edge\" in image_processor.size:\n+            size = image_processor.size[\"shortest_edge\"]\n+        else:\n+            size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n+\n+        # Create normalization transform\n+        if hasattr(image_processor, \"image_mean\") and hasattr(image_processor, \"image_std\"):\n+            normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n+        else:\n+            normalize = Lambda(lambda x: x)\n+        _train_transforms = Compose(\n+            [\n+                RandomResizedCrop(size),\n+                RandomHorizontalFlip(),\n+                ToTensor(),\n+                normalize,\n+            ]\n+        )\n+        _val_transforms = Compose(\n+            [\n+                Resize(size),\n+                CenterCrop(size),\n+                ToTensor(),\n+                normalize,\n+            ]\n+        )\n \n     def train_transforms(example_batch):\n         \"\"\"Apply _train_transforms across a batch.\"\"\""
        },
        {
            "sha": "ec62b260a512a33d31a9686847e987e7a9925088",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 30,
            "deletions": 0,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe",
            "patch": "@@ -782,6 +782,7 @@\n     \"models.time_series_transformer\": [\"TimeSeriesTransformerConfig\"],\n     \"models.timesformer\": [\"TimesformerConfig\"],\n     \"models.timm_backbone\": [\"TimmBackboneConfig\"],\n+    \"models.timm_wrapper\": [\"TimmWrapperConfig\"],\n     \"models.trocr\": [\n         \"TrOCRConfig\",\n         \"TrOCRProcessor\",\n@@ -1272,6 +1273,18 @@\n     _import_structure[\"models.rt_detr\"].append(\"RTDetrImageProcessorFast\")\n     _import_structure[\"models.vit\"].append(\"ViTImageProcessorFast\")\n \n+try:\n+    if not is_torchvision_available() and not is_timm_available():\n+        raise OptionalDependencyNotAvailable()\n+except OptionalDependencyNotAvailable:\n+    from .utils import dummy_timm_and_torchvision_objects\n+\n+    _import_structure[\"utils.dummy_timm_and_torchvision_objects\"] = [\n+        name for name in dir(dummy_timm_and_torchvision_objects) if not name.startswith(\"_\")\n+    ]\n+else:\n+    _import_structure[\"models.timm_wrapper\"].extend([\"TimmWrapperImageProcessor\"])\n+\n # PyTorch-backed objects\n try:\n     if not is_torch_available():\n@@ -3532,6 +3545,9 @@\n         ]\n     )\n     _import_structure[\"models.timm_backbone\"].extend([\"TimmBackbone\"])\n+    _import_structure[\"models.timm_wrapper\"].extend(\n+        [\"TimmWrapperForImageClassification\", \"TimmWrapperModel\", \"TimmWrapperPreTrainedModel\"]\n+    )\n     _import_structure[\"models.trocr\"].extend(\n         [\n             \"TrOCRForCausalLM\",\n@@ -5734,6 +5750,7 @@\n         TimesformerConfig,\n     )\n     from .models.timm_backbone import TimmBackboneConfig\n+    from .models.timm_wrapper import TimmWrapperConfig\n     from .models.trocr import (\n         TrOCRConfig,\n         TrOCRProcessor,\n@@ -6227,6 +6244,14 @@\n         from .models.rt_detr import RTDetrImageProcessorFast\n         from .models.vit import ViTImageProcessorFast\n \n+    try:\n+        if not is_torchvision_available() and not is_timm_available():\n+            raise OptionalDependencyNotAvailable()\n+    except OptionalDependencyNotAvailable:\n+        from .utils.dummy_timm_and_torchvision_objects import *\n+    else:\n+        from .models.timm_wrapper import TimmWrapperImageProcessor\n+\n     # Modeling\n     try:\n         if not is_torch_available():\n@@ -8037,6 +8062,11 @@\n             TimesformerPreTrainedModel,\n         )\n         from .models.timm_backbone import TimmBackbone\n+        from .models.timm_wrapper import (\n+            TimmWrapperForImageClassification,\n+            TimmWrapperModel,\n+            TimmWrapperPreTrainedModel,\n+        )\n         from .models.trocr import (\n             TrOCRForCausalLM,\n             TrOCRPreTrainedModel,"
        },
        {
            "sha": "a04b7bd6aa1b6d0d13774d1f88b1db13b58126e4",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe",
            "patch": "@@ -37,6 +37,7 @@\n     download_url,\n     extract_commit_hash,\n     is_remote_url,\n+    is_timm_config_dict,\n     is_torch_available,\n     logging,\n )\n@@ -702,6 +703,11 @@ def _get_config_dict(\n             config_dict[\"custom_pipelines\"] = add_model_info_to_custom_pipelines(\n                 config_dict[\"custom_pipelines\"], pretrained_model_name_or_path\n             )\n+\n+        # timm models are not saved with the model_type in the config file\n+        if \"model_type\" not in config_dict and is_timm_config_dict(config_dict):\n+            config_dict[\"model_type\"] = \"timm_wrapper\"\n+\n         return config_dict, kwargs\n \n     @classmethod"
        },
        {
            "sha": "c5af652decf2a43f103785a29091387cc149ba8f",
            "filename": "src/transformers/image_processing_base.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/src%2Ftransformers%2Fimage_processing_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/src%2Ftransformers%2Fimage_processing_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_base.py?ref=5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe",
            "patch": "@@ -285,6 +285,8 @@ def get_image_processor_dict(\n             subfolder (`str`, *optional*, defaults to `\"\"`):\n                 In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can\n                 specify the folder name here.\n+            image_processor_filename (`str`, *optional*, defaults to `\"config.json\"`):\n+                The name of the file in the model directory to use for the image processor config.\n \n         Returns:\n             `Tuple[Dict, Dict]`: The dictionary(ies) that will be used to instantiate the image processor object.\n@@ -298,6 +300,7 @@ def get_image_processor_dict(\n         local_files_only = kwargs.pop(\"local_files_only\", False)\n         revision = kwargs.pop(\"revision\", None)\n         subfolder = kwargs.pop(\"subfolder\", \"\")\n+        image_processor_filename = kwargs.pop(\"image_processor_filename\", IMAGE_PROCESSOR_NAME)\n \n         from_pipeline = kwargs.pop(\"_from_pipeline\", None)\n         from_auto_class = kwargs.pop(\"_from_auto\", False)\n@@ -324,15 +327,15 @@ def get_image_processor_dict(\n         pretrained_model_name_or_path = str(pretrained_model_name_or_path)\n         is_local = os.path.isdir(pretrained_model_name_or_path)\n         if os.path.isdir(pretrained_model_name_or_path):\n-            image_processor_file = os.path.join(pretrained_model_name_or_path, IMAGE_PROCESSOR_NAME)\n+            image_processor_file = os.path.join(pretrained_model_name_or_path, image_processor_filename)\n         if os.path.isfile(pretrained_model_name_or_path):\n             resolved_image_processor_file = pretrained_model_name_or_path\n             is_local = True\n         elif is_remote_url(pretrained_model_name_or_path):\n             image_processor_file = pretrained_model_name_or_path\n             resolved_image_processor_file = download_url(pretrained_model_name_or_path)\n         else:\n-            image_processor_file = IMAGE_PROCESSOR_NAME\n+            image_processor_file = image_processor_filename\n             try:\n                 # Load from local folder or from cache or download from model Hub and cache\n                 resolved_image_processor_file = cached_file(\n@@ -358,7 +361,7 @@ def get_image_processor_dict(\n                     f\"Can't load image processor for '{pretrained_model_name_or_path}'. If you were trying to load\"\n                     \" it from 'https://huggingface.co/models', make sure you don't have a local directory with the\"\n                     f\" same name. Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a\"\n-                    f\" directory containing a {IMAGE_PROCESSOR_NAME} file\"\n+                    f\" directory containing a {image_processor_filename} file\"\n                 )\n \n         try:"
        },
        {
            "sha": "f349847b1fd7a119e45b4c288c39a0c19227b755",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 93,
            "deletions": 100,
            "changes": 193,
            "blob_url": "https://github.com/huggingface/transformers/blob/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe",
            "patch": "@@ -503,7 +503,7 @@ def load_state_dict(\n         # Check format of the archive\n         with safe_open(checkpoint_file, framework=\"pt\") as f:\n             metadata = f.metadata()\n-        if metadata.get(\"format\") not in [\"pt\", \"tf\", \"flax\", \"mlx\"]:\n+        if metadata is not None and metadata.get(\"format\") not in [\"pt\", \"tf\", \"flax\", \"mlx\"]:\n             raise OSError(\n                 f\"The safetensors archive passed at {checkpoint_file} does not contain the valid metadata. Make sure \"\n                 \"you save your model with the `save_pretrained` method.\"\n@@ -652,36 +652,6 @@ def _find_identical(tensors: List[Set[str]], state_dict: Dict[str, torch.Tensor]\n \n \n def _load_state_dict_into_model(model_to_load, state_dict, start_prefix, assign_to_params_buffers=False):\n-    # Convert old format to new format if needed from a PyTorch state_dict\n-    old_keys = []\n-    new_keys = []\n-    renamed_keys = {}\n-    renamed_gamma = {}\n-    renamed_beta = {}\n-    warning_msg = f\"A pretrained model of type `{model_to_load.__class__.__name__}` \"\n-    for key in state_dict.keys():\n-        new_key = None\n-        if \"gamma\" in key:\n-            # We add only the first key as an example\n-            new_key = key.replace(\"gamma\", \"weight\")\n-            renamed_gamma[key] = new_key if not renamed_gamma else renamed_gamma\n-        if \"beta\" in key:\n-            # We add only the first key as an example\n-            new_key = key.replace(\"beta\", \"bias\")\n-            renamed_beta[key] = new_key if not renamed_beta else renamed_beta\n-        if new_key:\n-            old_keys.append(key)\n-            new_keys.append(new_key)\n-    renamed_keys = {**renamed_gamma, **renamed_beta}\n-    if renamed_keys:\n-        warning_msg += \"contains parameters that have been renamed internally (a few are listed below but more are present in the model):\\n\"\n-        for old_key, new_key in renamed_keys.items():\n-            warning_msg += f\"* `{old_key}` -> `{new_key}`\\n\"\n-        warning_msg += \"If you are using a model from the Hub, consider submitting a PR to adjust these weights and help future users.\"\n-        logger.info_once(warning_msg)\n-    for old_key, new_key in zip(old_keys, new_keys):\n-        state_dict[new_key] = state_dict.pop(old_key)\n-\n     # copy state_dict so _load_from_state_dict can modify it\n     metadata = getattr(state_dict, \"_metadata\", None)\n     state_dict = state_dict.copy()\n@@ -812,46 +782,7 @@ def _load_state_dict_into_meta_model(\n \n     error_msgs = []\n \n-    old_keys = []\n-    new_keys = []\n-    renamed_gamma = {}\n-    renamed_beta = {}\n     is_quantized = hf_quantizer is not None\n-    warning_msg = f\"This model {type(model)}\"\n-    for key in state_dict.keys():\n-        new_key = None\n-        if \"gamma\" in key:\n-            # We add only the first key as an example\n-            new_key = key.replace(\"gamma\", \"weight\")\n-            renamed_gamma[key] = new_key if not renamed_gamma else renamed_gamma\n-        if \"beta\" in key:\n-            # We add only the first key as an example\n-            new_key = key.replace(\"beta\", \"bias\")\n-            renamed_beta[key] = new_key if not renamed_beta else renamed_beta\n-\n-        # To reproduce `_load_state_dict_into_model` behaviour, we need to manually rename parametrized weigth norm, if necessary.\n-        if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n-            if \"weight_g\" in key:\n-                new_key = key.replace(\"weight_g\", \"parametrizations.weight.original0\")\n-            if \"weight_v\" in key:\n-                new_key = key.replace(\"weight_v\", \"parametrizations.weight.original1\")\n-        else:\n-            if \"parametrizations.weight.original0\" in key:\n-                new_key = key.replace(\"parametrizations.weight.original0\", \"weight_g\")\n-            if \"parametrizations.weight.original1\" in key:\n-                new_key = key.replace(\"parametrizations.weight.original1\", \"weight_v\")\n-        if new_key:\n-            old_keys.append(key)\n-            new_keys.append(new_key)\n-    renamed_keys = {**renamed_gamma, **renamed_beta}\n-    if renamed_keys:\n-        warning_msg += \"contains parameters that have been renamed internally (a few are listed below but more are present in the model):\\n\"\n-        for old_key, new_key in renamed_keys.items():\n-            warning_msg += f\"* `{old_key}` -> `{new_key}`\\n\"\n-        warning_msg += \"If you are using a model from the Hub, consider submitting a PR to adjust these weights and help future users.\"\n-        logger.info_once(warning_msg)\n-    for old_key, new_key in zip(old_keys, new_keys):\n-        state_dict[new_key] = state_dict.pop(old_key)\n \n     is_torch_e4m3fn_available = hasattr(torch, \"float8_e4m3fn\")\n \n@@ -2888,6 +2819,11 @@ def save_pretrained(\n             for ignore_key in self._keys_to_ignore_on_save:\n                 if ignore_key in state_dict.keys():\n                     del state_dict[ignore_key]\n+\n+        # Rename state_dict keys before saving to file. Do nothing unless overriden in a particular model.\n+        # (initially introduced with TimmWrapperModel to remove prefix and make checkpoints compatible with timm)\n+        state_dict = self._fix_state_dict_keys_on_save(state_dict)\n+\n         if safe_serialization:\n             # Safetensors does not allow tensor aliasing.\n             # We're going to remove aliases before saving\n@@ -4010,7 +3946,10 @@ def from_pretrained(\n             with safe_open(resolved_archive_file, framework=\"pt\") as f:\n                 metadata = f.metadata()\n \n-            if metadata.get(\"format\") == \"pt\":\n+            if metadata is None:\n+                # Assume it's a pytorch checkpoint (introduced for timm checkpoints)\n+                pass\n+            elif metadata.get(\"format\") == \"pt\":\n                 pass\n             elif metadata.get(\"format\") == \"tf\":\n                 from_tf = True\n@@ -4375,6 +4314,72 @@ def from_pretrained(\n \n         return model\n \n+    @staticmethod\n+    def _fix_state_dict_key_on_load(key):\n+        \"\"\"Replace legacy parameter names with their modern equivalents. E.g. beta -> bias, gamma -> weight.\"\"\"\n+\n+        if \"beta\" in key:\n+            return key.replace(\"beta\", \"bias\")\n+        if \"gamma\" in key:\n+            return key.replace(\"gamma\", \"weight\")\n+\n+        # to avoid logging parametrized weight norm renaming\n+        if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n+            if \"weight_g\" in key:\n+                return key.replace(\"weight_g\", \"parametrizations.weight.original0\")\n+            if \"weight_v\" in key:\n+                return key.replace(\"weight_v\", \"parametrizations.weight.original1\")\n+        else:\n+            if \"parametrizations.weight.original0\" in key:\n+                return key.replace(\"parametrizations.weight.original0\", \"weight_g\")\n+            if \"parametrizations.weight.original1\" in key:\n+                return key.replace(\"parametrizations.weight.original1\", \"weight_v\")\n+        return key\n+\n+    @classmethod\n+    def _fix_state_dict_keys_on_load(cls, state_dict):\n+        \"\"\"Fixes state dict keys by replacing legacy parameter names with their modern equivalents.\n+        Logs if any parameters have been renamed.\n+        \"\"\"\n+\n+        renamed_keys = {}\n+        state_dict_keys = list(state_dict.keys())\n+        for key in state_dict_keys:\n+            new_key = cls._fix_state_dict_key_on_load(key)\n+            if new_key != key:\n+                state_dict[new_key] = state_dict.pop(key)\n+\n+            # add it once for logging\n+            if \"gamma\" in key and \"gamma\" not in renamed_keys:\n+                renamed_keys[\"gamma\"] = (key, new_key)\n+            if \"beta\" in key and \"beta\" not in renamed_keys:\n+                renamed_keys[\"beta\"] = (key, new_key)\n+\n+        if renamed_keys:\n+            warning_msg = f\"A pretrained model of type `{cls.__name__}` \"\n+            warning_msg += \"contains parameters that have been renamed internally (a few are listed below but more are present in the model):\\n\"\n+            for old_key, new_key in renamed_keys.values():\n+                warning_msg += f\"* `{old_key}` -> `{new_key}`\\n\"\n+            warning_msg += \"If you are using a model from the Hub, consider submitting a PR to adjust these weights and help future users.\"\n+            logger.info_once(warning_msg)\n+\n+        return state_dict\n+\n+    @staticmethod\n+    def _fix_state_dict_key_on_save(key):\n+        \"\"\"\n+        Similar to `_fix_state_dict_key_on_load` allows to define hook for state dict key renaming on model save.\n+        Do nothing by default, but can be overriden in particular models.\n+        \"\"\"\n+        return key\n+\n+    def _fix_state_dict_keys_on_save(self, state_dict):\n+        \"\"\"\n+        Similar to `_fix_state_dict_keys_on_load` allows to define hook for state dict key renaming on model save.\n+        Apply `_fix_state_dict_key_on_save` to all keys in `state_dict`.\n+        \"\"\"\n+        return {self._fix_state_dict_key_on_save(key): value for key, value in state_dict.items()}\n+\n     @classmethod\n     def _load_pretrained_model(\n         cls,\n@@ -4430,27 +4435,8 @@ def _load_pretrained_model(\n         if hf_quantizer is not None:\n             expected_keys = hf_quantizer.update_expected_keys(model, expected_keys, loaded_keys)\n \n-        def _fix_key(key):\n-            if \"beta\" in key:\n-                return key.replace(\"beta\", \"bias\")\n-            if \"gamma\" in key:\n-                return key.replace(\"gamma\", \"weight\")\n-\n-            # to avoid logging parametrized weight norm renaming\n-            if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n-                if \"weight_g\" in key:\n-                    return key.replace(\"weight_g\", \"parametrizations.weight.original0\")\n-                if \"weight_v\" in key:\n-                    return key.replace(\"weight_v\", \"parametrizations.weight.original1\")\n-            else:\n-                if \"parametrizations.weight.original0\" in key:\n-                    return key.replace(\"parametrizations.weight.original0\", \"weight_g\")\n-                if \"parametrizations.weight.original1\" in key:\n-                    return key.replace(\"parametrizations.weight.original1\", \"weight_v\")\n-            return key\n-\n         original_loaded_keys = loaded_keys\n-        loaded_keys = [_fix_key(key) for key in loaded_keys]\n+        loaded_keys = [cls._fix_state_dict_key_on_load(key) for key in loaded_keys]\n \n         if len(prefix) > 0:\n             has_prefix_module = any(s.startswith(prefix) for s in loaded_keys)\n@@ -4615,23 +4601,23 @@ def _find_mismatched_keys(\n             state_dict,\n             model_state_dict,\n             loaded_keys,\n+            original_loaded_keys,\n             add_prefix_to_model,\n             remove_prefix_from_model,\n             ignore_mismatched_sizes,\n         ):\n             mismatched_keys = []\n             if ignore_mismatched_sizes:\n-                for checkpoint_key in loaded_keys:\n+                for checkpoint_key, model_key in zip(original_loaded_keys, loaded_keys):\n                     # If the checkpoint is sharded, we may not have the key here.\n                     if checkpoint_key not in state_dict:\n                         continue\n-                    model_key = checkpoint_key\n                     if remove_prefix_from_model:\n                         # The model key starts with `prefix` but `checkpoint_key` doesn't so we add it.\n-                        model_key = f\"{prefix}.{checkpoint_key}\"\n+                        model_key = f\"{prefix}.{model_key}\"\n                     elif add_prefix_to_model:\n                         # The model key doesn't start with `prefix` but `checkpoint_key` does so we remove it.\n-                        model_key = \".\".join(checkpoint_key.split(\".\")[1:])\n+                        model_key = \".\".join(model_key.split(\".\")[1:])\n \n                     if (\n                         model_key in model_state_dict\n@@ -4680,6 +4666,7 @@ def _find_mismatched_keys(\n             mismatched_keys = _find_mismatched_keys(\n                 state_dict,\n                 model_state_dict,\n+                loaded_keys,\n                 original_loaded_keys,\n                 add_prefix_to_model,\n                 remove_prefix_from_model,\n@@ -4688,9 +4675,10 @@ def _find_mismatched_keys(\n \n             # For GGUF models `state_dict` is never set to None as the state dict is always small\n             if gguf_path:\n+                fixed_state_dict = cls._fix_state_dict_keys_on_load(state_dict)\n                 error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(\n                     model_to_load,\n-                    state_dict,\n+                    fixed_state_dict,\n                     start_prefix,\n                     expected_keys,\n                     device_map=device_map,\n@@ -4709,8 +4697,9 @@ def _find_mismatched_keys(\n                 assign_to_params_buffers = check_support_param_buffer_assignment(\n                     model_to_load, state_dict, start_prefix\n                 )\n+                fixed_state_dict = cls._fix_state_dict_keys_on_load(state_dict)\n                 error_msgs = _load_state_dict_into_model(\n-                    model_to_load, state_dict, start_prefix, assign_to_params_buffers\n+                    model_to_load, fixed_state_dict, start_prefix, assign_to_params_buffers\n                 )\n \n         else:\n@@ -4761,6 +4750,7 @@ def _find_mismatched_keys(\n                 mismatched_keys += _find_mismatched_keys(\n                     state_dict,\n                     model_state_dict,\n+                    loaded_keys,\n                     original_loaded_keys,\n                     add_prefix_to_model,\n                     remove_prefix_from_model,\n@@ -4774,9 +4764,10 @@ def _find_mismatched_keys(\n                                     model_to_load, key, \"cpu\", torch.empty(*param.size(), dtype=dtype)\n                                 )\n                     else:\n+                        fixed_state_dict = cls._fix_state_dict_keys_on_load(state_dict)\n                         new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(\n                             model_to_load,\n-                            state_dict,\n+                            fixed_state_dict,\n                             start_prefix,\n                             expected_keys,\n                             device_map=device_map,\n@@ -4797,8 +4788,9 @@ def _find_mismatched_keys(\n                         assign_to_params_buffers = check_support_param_buffer_assignment(\n                             model_to_load, state_dict, start_prefix\n                         )\n+                    fixed_state_dict = cls._fix_state_dict_keys_on_load(state_dict)\n                     error_msgs += _load_state_dict_into_model(\n-                        model_to_load, state_dict, start_prefix, assign_to_params_buffers\n+                        model_to_load, fixed_state_dict, start_prefix, assign_to_params_buffers\n                     )\n \n                 # force memory release\n@@ -4930,9 +4922,10 @@ def _load_pretrained_model_low_mem(\n         _move_model_to_meta(model, loaded_state_dict_keys, start_prefix)\n         state_dict = load_state_dict(resolved_archive_file, weights_only=weights_only)\n         expected_keys = loaded_state_dict_keys  # plug for missing expected_keys. TODO: replace with proper keys\n+        fixed_state_dict = model._fix_state_dict_keys_on_load(state_dict)\n         error_msgs = _load_state_dict_into_meta_model(\n             model,\n-            state_dict,\n+            fixed_state_dict,\n             start_prefix,\n             expected_keys=expected_keys,\n             hf_quantizer=hf_quantizer,"
        },
        {
            "sha": "e606b59a1b51ae124561bf58ea66894060e803c3",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe",
            "patch": "@@ -249,6 +249,7 @@\n     time_series_transformer,\n     timesformer,\n     timm_backbone,\n+    timm_wrapper,\n     trocr,\n     tvp,\n     udop,"
        },
        {
            "sha": "8672de24b1316b9d2a18e2c90bf810e0818dbeb5",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe",
            "patch": "@@ -276,6 +276,7 @@\n         (\"time_series_transformer\", \"TimeSeriesTransformerConfig\"),\n         (\"timesformer\", \"TimesformerConfig\"),\n         (\"timm_backbone\", \"TimmBackboneConfig\"),\n+        (\"timm_wrapper\", \"TimmWrapperConfig\"),\n         (\"trajectory_transformer\", \"TrajectoryTransformerConfig\"),\n         (\"transfo-xl\", \"TransfoXLConfig\"),\n         (\"trocr\", \"TrOCRConfig\"),\n@@ -599,6 +600,7 @@\n         (\"time_series_transformer\", \"Time Series Transformer\"),\n         (\"timesformer\", \"TimeSformer\"),\n         (\"timm_backbone\", \"TimmBackbone\"),\n+        (\"timm_wrapper\", \"TimmWrapperModel\"),\n         (\"trajectory_transformer\", \"Trajectory Transformer\"),\n         (\"transfo-xl\", \"Transformer-XL\"),\n         (\"trocr\", \"TrOCR\"),"
        },
        {
            "sha": "0670637c9152c3b1190c790e6b2301dffa0c8934",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 36,
            "deletions": 1,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe",
            "patch": "@@ -30,6 +30,8 @@\n     CONFIG_NAME,\n     IMAGE_PROCESSOR_NAME,\n     get_file_from_repo,\n+    is_timm_config_dict,\n+    is_timm_local_checkpoint,\n     is_torchvision_available,\n     is_vision_available,\n     logging,\n@@ -137,6 +139,7 @@\n             (\"swinv2\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"table-transformer\", (\"DetrImageProcessor\",)),\n             (\"timesformer\", (\"VideoMAEImageProcessor\",)),\n+            (\"timm_wrapper\", (\"TimmWrapperImageProcessor\",)),\n             (\"tvlt\", (\"TvltImageProcessor\",)),\n             (\"tvp\", (\"TvpImageProcessor\",)),\n             (\"udop\", (\"LayoutLMv3ImageProcessor\",)),\n@@ -376,6 +379,8 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n                 Whether or not to allow for custom models defined on the Hub in their own modeling files. This option\n                 should only be set to `True` for repositories you trust and in which you have read the code, as it will\n                 execute code present on the Hub on your local machine.\n+            image_processor_filename (`str`, *optional*, defaults to `\"config.json\"`):\n+                The name of the file in the model directory to use for the image processor config.\n             kwargs (`Dict[str, Any]`, *optional*):\n                 The values in kwargs of any keys which are image processor attributes will be used to override the\n                 loaded values. Behavior concerning key/value pairs whose keys are *not* image processor attributes is\n@@ -415,7 +420,37 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n         trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n         kwargs[\"_from_auto\"] = True\n \n-        config_dict, _ = ImageProcessingMixin.get_image_processor_dict(pretrained_model_name_or_path, **kwargs)\n+        # Resolve the image processor config filename\n+        if \"image_processor_filename\" in kwargs:\n+            image_processor_filename = kwargs.pop(\"image_processor_filename\")\n+        elif is_timm_local_checkpoint(pretrained_model_name_or_path):\n+            image_processor_filename = CONFIG_NAME\n+        else:\n+            image_processor_filename = IMAGE_PROCESSOR_NAME\n+\n+        # Load the image processor config\n+        try:\n+            # Main path for all transformers models and local TimmWrapper checkpoints\n+            config_dict, _ = ImageProcessingMixin.get_image_processor_dict(\n+                pretrained_model_name_or_path, image_processor_filename=image_processor_filename, **kwargs\n+            )\n+        except Exception as initial_exception:\n+            # Fallback path for Hub TimmWrapper checkpoints. Timm models' image processing is saved in `config.json`\n+            # instead of `preprocessor_config.json`. Because this is an Auto class and we don't have any information\n+            # except the model name, the only way to check if a remote checkpoint is a timm model is to try to\n+            # load `config.json` and if it fails with some error, we raise the initial exception.\n+            try:\n+                config_dict, _ = ImageProcessingMixin.get_image_processor_dict(\n+                    pretrained_model_name_or_path, image_processor_filename=CONFIG_NAME, **kwargs\n+                )\n+            except Exception:\n+                raise initial_exception\n+\n+            # In case we have a config_dict, but it's not a timm config dict, we raise the initial exception,\n+            # because only timm models have image processing in `config.json`.\n+            if not is_timm_config_dict(config_dict):\n+                raise initial_exception\n+\n         image_processor_class = config_dict.get(\"image_processor_type\", None)\n         image_processor_auto_map = None\n         if \"AutoImageProcessor\" in config_dict.get(\"auto_map\", {}):"
        },
        {
            "sha": "c7ca5854a291edc5760e3889ddd9bdc66aa2985e",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe",
            "patch": "@@ -255,6 +255,7 @@\n         (\"time_series_transformer\", \"TimeSeriesTransformerModel\"),\n         (\"timesformer\", \"TimesformerModel\"),\n         (\"timm_backbone\", \"TimmBackbone\"),\n+        (\"timm_wrapper\", \"TimmWrapperModel\"),\n         (\"trajectory_transformer\", \"TrajectoryTransformerModel\"),\n         (\"transfo-xl\", \"TransfoXLModel\"),\n         (\"tvlt\", \"TvltModel\"),\n@@ -605,6 +606,7 @@\n         (\"table-transformer\", \"TableTransformerModel\"),\n         (\"timesformer\", \"TimesformerModel\"),\n         (\"timm_backbone\", \"TimmBackbone\"),\n+        (\"timm_wrapper\", \"TimmWrapperModel\"),\n         (\"van\", \"VanModel\"),\n         (\"videomae\", \"VideoMAEModel\"),\n         (\"vit\", \"ViTModel\"),\n@@ -690,6 +692,7 @@\n         (\"swiftformer\", \"SwiftFormerForImageClassification\"),\n         (\"swin\", \"SwinForImageClassification\"),\n         (\"swinv2\", \"Swinv2ForImageClassification\"),\n+        (\"timm_wrapper\", \"TimmWrapperForImageClassification\"),\n         (\"van\", \"VanForImageClassification\"),\n         (\"vit\", \"ViTForImageClassification\"),\n         (\"vit_hybrid\", \"ViTHybridForImageClassification\"),"
        },
        {
            "sha": "9fbc4150412a73ec6d77a09db13c105e95e50c06",
            "filename": "src/transformers/models/timm_wrapper/__init__.py",
            "status": "added",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2F__init__.py?ref=5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe",
            "patch": "@@ -0,0 +1,28 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_timm_wrapper import *\n+    from .modeling_timm_wrapper import *\n+    from .processing_timm_wrapper import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "691a2b2b76ec3f803d026b1e88004ea6e0ac47e4",
            "filename": "src/transformers/models/timm_wrapper/configuration_timm_wrapper.py",
            "status": "added",
            "additions": 86,
            "deletions": 0,
            "changes": 86,
            "blob_url": "https://github.com/huggingface/transformers/blob/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fconfiguration_timm_wrapper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fconfiguration_timm_wrapper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fconfiguration_timm_wrapper.py?ref=5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe",
            "patch": "@@ -0,0 +1,86 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\"\"\"Configuration for TimmWrapper models\"\"\"\n+\n+from typing import Any, Dict\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...utils import logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class TimmWrapperConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration for a timm backbone [`TimmWrapper`].\n+\n+    It is used to instantiate a timm model according to the specified arguments, defining the model.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        do_pooling (`bool`, *optional*, defaults to `True`):\n+            Whether to do pooling for the last_hidden_state in `TimmWrapperModel` or not.\n+\n+    Example:\n+    ```python\n+    >>> from transformers import TimmWrapperModel\n+\n+    >>> # Initializing a timm model\n+    >>> model = TimmWrapperModel.from_pretrained(\"timm/resnet18.a1_in1k\")\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\n+    \"\"\"\n+\n+    model_type = \"timm_wrapper\"\n+\n+    def __init__(self, initializer_range: float = 0.02, do_pooling: bool = True, **kwargs):\n+        self.initializer_range = initializer_range\n+        self.do_pooling = do_pooling\n+        super().__init__(**kwargs)\n+\n+    @classmethod\n+    def from_dict(cls, config_dict: Dict[str, Any], **kwargs):\n+        # timm config stores the `num_classes` attribute in both the root of config and in the \"pretrained_cfg\" dict.\n+        # We are removing these attributes in order to have the native `transformers` num_labels attribute in config\n+        # and to avoid duplicate attributes\n+\n+        num_labels_in_kwargs = kwargs.pop(\"num_labels\", None)\n+        num_labels_in_dict = config_dict.pop(\"num_classes\", None)\n+\n+        # passed num_labels has priority over num_classes in config_dict\n+        kwargs[\"num_labels\"] = num_labels_in_kwargs or num_labels_in_dict\n+\n+        # pop num_classes from \"pretrained_cfg\",\n+        # it is not necessary to have it, only root one is used in timm\n+        if \"pretrained_cfg\" in config_dict and \"num_classes\" in config_dict[\"pretrained_cfg\"]:\n+            config_dict[\"pretrained_cfg\"].pop(\"num_classes\", None)\n+\n+        return super().from_dict(config_dict, **kwargs)\n+\n+    def to_dict(self) -> Dict[str, Any]:\n+        output = super().to_dict()\n+        output[\"num_classes\"] = self.num_labels\n+        return output\n+\n+\n+__all__ = [\"TimmWrapperConfig\"]"
        },
        {
            "sha": "02075a50fb26763c5ca4698e0598e77692a35a51",
            "filename": "src/transformers/models/timm_wrapper/image_processing_timm_wrapper.py",
            "status": "added",
            "additions": 138,
            "deletions": 0,
            "changes": 138,
            "blob_url": "https://github.com/huggingface/transformers/blob/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fimage_processing_timm_wrapper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fimage_processing_timm_wrapper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fimage_processing_timm_wrapper.py?ref=5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe",
            "patch": "@@ -0,0 +1,138 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import os\n+from typing import Any, Dict, Optional, Tuple, Union\n+\n+import torch\n+\n+from ...image_processing_utils import BaseImageProcessor, BatchFeature\n+from ...image_transforms import to_pil_image\n+from ...image_utils import ImageInput, make_list_of_images\n+from ...utils import TensorType, logging, requires_backends\n+from ...utils.import_utils import is_timm_available, is_torch_available\n+\n+\n+if is_timm_available():\n+    import timm\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class TimmWrapperImageProcessor(BaseImageProcessor):\n+    \"\"\"\n+    Wrapper class for timm models to be used within transformers.\n+\n+    Args:\n+        pretrained_cfg (`Dict[str, Any]`):\n+            The configuration of the pretrained model used to resolve evaluation and\n+            training transforms.\n+        architecture (`Optional[str]`, *optional*):\n+            Name of the architecture of the model.\n+    \"\"\"\n+\n+    main_input_name = \"pixel_values\"\n+\n+    def __init__(\n+        self,\n+        pretrained_cfg: Dict[str, Any],\n+        architecture: Optional[str] = None,\n+        **kwargs,\n+    ):\n+        requires_backends(self, \"timm\")\n+        super().__init__(architecture=architecture)\n+\n+        self.data_config = timm.data.resolve_data_config(pretrained_cfg, model=None, verbose=False)\n+        self.val_transforms = timm.data.create_transform(**self.data_config, is_training=False)\n+\n+        # useful for training, see examples/pytorch/image-classification/run_image_classification.py\n+        self.train_transforms = timm.data.create_transform(**self.data_config, is_training=True)\n+\n+        # If `ToTensor` is in the transforms, then the input should be numpy array or PIL image.\n+        # Otherwise, the input can be a tensor. In later timm versions, `MaybeToTensor` is used\n+        # which can handle both numpy arrays / PIL images and tensors.\n+        self._not_supports_tensor_input = any(\n+            transform.__class__.__name__ == \"ToTensor\" for transform in self.val_transforms.transforms\n+        )\n+\n+    def to_dict(self) -> Dict[str, Any]:\n+        \"\"\"\n+        Serializes this instance to a Python dictionary.\n+        \"\"\"\n+        output = super().to_dict()\n+        output.pop(\"train_transforms\", None)\n+        output.pop(\"val_transforms\", None)\n+        output.pop(\"_not_supports_tensor_input\", None)\n+        return output\n+\n+    @classmethod\n+    def get_image_processor_dict(\n+        cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs\n+    ) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n+        \"\"\"\n+        Get the image processor dict for the model.\n+        \"\"\"\n+        image_processor_filename = kwargs.pop(\"image_processor_filename\", \"config.json\")\n+        return super().get_image_processor_dict(\n+            pretrained_model_name_or_path, image_processor_filename=image_processor_filename, **kwargs\n+        )\n+\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        return_tensors: Optional[Union[str, TensorType]] = \"pt\",\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess an image or batch of images.\n+\n+        Args:\n+            images (`ImageInput`):\n+                Image to preprocess. Expects a single or batch of images\n+            return_tensors (`str` or `TensorType`, *optional*):\n+                The type of tensors to return.\n+        \"\"\"\n+        if return_tensors != \"pt\":\n+            raise ValueError(f\"return_tensors for TimmWrapperImageProcessor must be 'pt', but got {return_tensors}\")\n+\n+        if self._not_supports_tensor_input and isinstance(images, torch.Tensor):\n+            images = images.cpu().numpy()\n+\n+        # If the input is a torch tensor, then no conversion is needed\n+        # Otherwise, we need to pass in a list of PIL images\n+        if isinstance(images, torch.Tensor):\n+            images = self.val_transforms(images)\n+            # Add batch dimension if a single image\n+            images = images.unsqueeze(0) if images.ndim == 3 else images\n+        else:\n+            images = make_list_of_images(images)\n+            images = [to_pil_image(image) for image in images]\n+            images = torch.stack([self.val_transforms(image) for image in images])\n+\n+        return BatchFeature({\"pixel_values\": images}, tensor_type=return_tensors)\n+\n+    def save_pretrained(self, *args, **kwargs):\n+        # disable it to make checkpoint the same as in `timm` library.\n+        logger.warning_once(\n+            \"The `save_pretrained` method is disabled for TimmWrapperImageProcessor. \"\n+            \"The image processor configuration is saved directly in `config.json` when \"\n+            \"`save_pretrained` is called for saving the model.\"\n+        )\n+\n+\n+__all__ = [\"TimmWrapperImageProcessor\"]"
        },
        {
            "sha": "dfb14dfccec4c64125e0227167cb9f08669aeebe",
            "filename": "src/transformers/models/timm_wrapper/modeling_timm_wrapper.py",
            "status": "added",
            "additions": 363,
            "deletions": 0,
            "changes": 363,
            "blob_url": "https://github.com/huggingface/transformers/blob/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py?ref=5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe",
            "patch": "@@ -0,0 +1,363 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from dataclasses import dataclass\n+from typing import List, Optional, Tuple, Union\n+\n+import torch\n+from torch import Tensor, nn\n+from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n+\n+from ...modeling_outputs import ImageClassifierOutput, ModelOutput\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import (\n+    add_start_docstrings_to_model_forward,\n+    is_timm_available,\n+    replace_return_docstrings,\n+    requires_backends,\n+)\n+from .configuration_timm_wrapper import TimmWrapperConfig\n+\n+\n+if is_timm_available():\n+    import timm\n+\n+\n+@dataclass\n+class TimmWrapperModelOutput(ModelOutput):\n+    \"\"\"\n+    Output class for models TimmWrapperModel, containing the last hidden states, an optional pooled output,\n+    and optional hidden states.\n+\n+    Args:\n+        last_hidden_state (`torch.FloatTensor`):\n+            The last hidden state of the model, output before applying the classification head.\n+        pooler_output (`torch.FloatTensor`, *optional*):\n+            The pooled output derived from the last hidden state, if applicable.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n+            A tuple containing the intermediate hidden states of the model at the output of each layer or specified layers.\n+            Returned if `output_hidden_states=True` is set or if `config.output_hidden_states=True`.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*):\n+            A tuple containing the intermediate attention weights of the model at the output of each layer.\n+            Returned if `output_attentions=True` is set or if `config.output_attentions=True`.\n+            Note: Currently, Timm models do not support attentions output.\n+    \"\"\"\n+\n+    last_hidden_state: torch.FloatTensor\n+    pooler_output: Optional[torch.FloatTensor] = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+\n+\n+TIMM_WRAPPER_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See [`TimmWrapperImageProcessor.preprocess`]\n+            for details.\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. Not compatible with timm wrapped models.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. Not compatible with timm wrapped models.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        **kwargs:\n+            Additional keyword arguments passed along to the `timm` model forward.\n+\"\"\"\n+\n+\n+class TimmWrapperPreTrainedModel(PreTrainedModel):\n+    main_input_name = \"pixel_values\"\n+    config_class = TimmWrapperConfig\n+    _no_split_modules = []\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"vision\", \"timm\"])\n+        super().__init__(*args, **kwargs)\n+\n+    @staticmethod\n+    def _fix_state_dict_key_on_load(key):\n+        \"\"\"\n+        Overrides original method that renames `gamma` and `beta` to `weight` and `bias`.\n+        We don't want this behavior for timm wrapped models. Instead, this method adds a\n+        \"timm_model.\" prefix to enable loading official timm Hub checkpoints.\n+        \"\"\"\n+        if \"timm_model.\" not in key:\n+            return f\"timm_model.{key}\"\n+        return key\n+\n+    def _fix_state_dict_key_on_save(self, key):\n+        \"\"\"\n+        Overrides original method to remove \"timm_model.\" prefix from state_dict keys.\n+        Makes the saved checkpoint compatible with the `timm` library.\n+        \"\"\"\n+        return key.replace(\"timm_model.\", \"\")\n+\n+    def load_state_dict(self, state_dict, *args, **kwargs):\n+        \"\"\"\n+        Override original method to fix state_dict keys on load for cases when weights are loaded\n+        without using the `from_pretrained` method (e.g., in Trainer to resume from checkpoint).\n+        \"\"\"\n+        state_dict = self._fix_state_dict_keys_on_load(state_dict)\n+        return super().load_state_dict(state_dict, *args, **kwargs)\n+\n+    def _init_weights(self, module):\n+        \"\"\"\n+        Initialize weights function to properly initialize Linear layer weights.\n+        Since model architectures may vary, we assume only the classifier requires\n+        initialization, while all other weights should be loaded from the checkpoint.\n+        \"\"\"\n+        if isinstance(module, (nn.Linear)):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+\n+\n+class TimmWrapperModel(TimmWrapperPreTrainedModel):\n+    \"\"\"\n+    Wrapper class for timm models to be used in transformers.\n+    \"\"\"\n+\n+    def __init__(self, config: TimmWrapperConfig):\n+        super().__init__(config)\n+        # using num_classes=0 to avoid creating classification head\n+        self.timm_model = timm.create_model(config.architecture, pretrained=False, num_classes=0)\n+        self.post_init()\n+\n+    @add_start_docstrings_to_model_forward(TIMM_WRAPPER_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=TimmWrapperModelOutput, config_class=TimmWrapperConfig)\n+    def forward(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[Union[bool, List[int]]] = None,\n+        return_dict: Optional[bool] = None,\n+        do_pooling: Optional[bool] = None,\n+        **kwargs,\n+    ) -> Union[TimmWrapperModelOutput, Tuple[Tensor, ...]]:\n+        r\"\"\"\n+        do_pooling (`bool`, *optional*):\n+            Whether to do pooling for the last_hidden_state in `TimmWrapperModel` or not. If `None` is passed, the\n+            `do_pooling` value from the config is used.\n+\n+        Returns:\n+\n+        Examples:\n+        ```python\n+        >>> import torch\n+        >>> from PIL import Image\n+        >>> from urllib.request import urlopen\n+        >>> from transformers import AutoModel, AutoImageProcessor\n+\n+        >>> # Load image\n+        >>> image = Image.open(urlopen(\n+        ...     'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n+        ... ))\n+\n+        >>> # Load model and image processor\n+        >>> checkpoint = \"timm/resnet50.a1_in1k\"\n+        >>> image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n+        >>> model = AutoModel.from_pretrained(checkpoint).eval()\n+\n+        >>> # Preprocess image\n+        >>> inputs = image_processor(image)\n+\n+        >>> # Forward pass\n+        >>> with torch.no_grad():\n+        ...     outputs = model(**inputs)\n+\n+        >>> # Get pooled output\n+        >>> pooled_output = outputs.pooler_output\n+\n+        >>> # Get last hidden state\n+        >>> last_hidden_state = outputs.last_hidden_state\n+        ```\n+        \"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        do_pooling = do_pooling if do_pooling is not None else self.config.do_pooling\n+\n+        if output_attentions:\n+            raise ValueError(\"Cannot set `output_attentions` for timm models.\")\n+\n+        if output_hidden_states and not hasattr(self.timm_model, \"forward_intermediates\"):\n+            raise ValueError(\n+                \"The 'output_hidden_states' option cannot be set for this timm model. \"\n+                \"To enable this feature, the 'forward_intermediates' method must be implemented \"\n+                \"in the timm model (available in timm versions > 1.*). Please consider using a \"\n+                \"different architecture or updating the timm package to a compatible version.\"\n+            )\n+\n+        pixel_values = pixel_values.to(self.device, self.dtype)\n+\n+        if output_hidden_states:\n+            # to enable hidden states selection\n+            if isinstance(output_hidden_states, (list, tuple)):\n+                kwargs[\"indices\"] = output_hidden_states\n+            last_hidden_state, hidden_states = self.timm_model.forward_intermediates(pixel_values, **kwargs)\n+        else:\n+            last_hidden_state = self.timm_model.forward_features(pixel_values, **kwargs)\n+            hidden_states = None\n+\n+        if do_pooling:\n+            # classification head is not created, applying pooling only\n+            pooler_output = self.timm_model.forward_head(last_hidden_state)\n+        else:\n+            pooler_output = None\n+\n+        if not return_dict:\n+            outputs = (last_hidden_state, pooler_output, hidden_states)\n+            outputs = tuple(output for output in outputs if output is not None)\n+            return outputs\n+\n+        return TimmWrapperModelOutput(\n+            last_hidden_state=last_hidden_state,\n+            pooler_output=pooler_output,\n+            hidden_states=hidden_states,\n+        )\n+\n+\n+class TimmWrapperForImageClassification(TimmWrapperPreTrainedModel):\n+    \"\"\"\n+    Wrapper class for timm models to be used in transformers for image classification.\n+    \"\"\"\n+\n+    def __init__(self, config: TimmWrapperConfig):\n+        super().__init__(config)\n+\n+        if config.num_labels == 0:\n+            raise ValueError(\n+                \"You are trying to load weights into `TimmWrapperForImageClassification` from a checkpoint with no classifier head. \"\n+                \"Please specify the number of classes, e.g. `model = TimmWrapperForImageClassification.from_pretrained(..., num_labels=10)`, \"\n+                \"or use `TimmWrapperModel` for feature extraction.\"\n+            )\n+\n+        self.timm_model = timm.create_model(config.architecture, pretrained=False, num_classes=config.num_labels)\n+        self.num_labels = config.num_labels\n+        self.post_init()\n+\n+    @add_start_docstrings_to_model_forward(TIMM_WRAPPER_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=ImageClassifierOutput, config_class=TimmWrapperConfig)\n+    def forward(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        labels: Optional[torch.LongTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[Union[bool, List[int]]] = None,\n+        return_dict: Optional[bool] = None,\n+        **kwargs,\n+    ) -> Union[ImageClassifierOutput, Tuple[Tensor, ...]]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n+\n+        Returns:\n+\n+        Examples:\n+        ```python\n+        >>> import torch\n+        >>> from PIL import Image\n+        >>> from urllib.request import urlopen\n+        >>> from transformers import AutoModelForImageClassification, AutoImageProcessor\n+\n+        >>> # Load image\n+        >>> image = Image.open(urlopen(\n+        ...     'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n+        ... ))\n+\n+        >>> # Load model and image processor\n+        >>> checkpoint = \"timm/resnet50.a1_in1k\"\n+        >>> image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n+        >>> model = AutoModelForImageClassification.from_pretrained(checkpoint).eval()\n+\n+        >>> # Preprocess image\n+        >>> inputs = image_processor(image)\n+\n+        >>> # Forward pass\n+        >>> with torch.no_grad():\n+        ...     logits = model(**inputs).logits\n+\n+        >>> # Get top 5 predictions\n+        >>> top5_probabilities, top5_class_indices = torch.topk(logits.softmax(dim=1) * 100, k=5)\n+        ```\n+        \"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+\n+        if output_attentions:\n+            raise ValueError(\"Cannot set `output_attentions` for timm models.\")\n+\n+        if output_hidden_states and not hasattr(self.timm_model, \"forward_intermediates\"):\n+            raise ValueError(\n+                \"The 'output_hidden_states' option cannot be set for this timm model. \"\n+                \"To enable this feature, the 'forward_intermediates' method must be implemented \"\n+                \"in the timm model (available in timm versions > 1.*). Please consider using a \"\n+                \"different architecture or updating the timm package to a compatible version.\"\n+            )\n+\n+        pixel_values = pixel_values.to(self.device, self.dtype)\n+\n+        if output_hidden_states:\n+            # to enable hidden states selection\n+            if isinstance(output_hidden_states, (list, tuple)):\n+                kwargs[\"indices\"] = output_hidden_states\n+            last_hidden_state, hidden_states = self.timm_model.forward_intermediates(pixel_values, **kwargs)\n+            logits = self.timm_model.forward_head(last_hidden_state)\n+        else:\n+            logits = self.timm_model(pixel_values, **kwargs)\n+            hidden_states = None\n+\n+        loss = None\n+        if labels is not None:\n+            if self.config.problem_type is None:\n+                if self.num_labels == 1:\n+                    self.config.problem_type = \"regression\"\n+                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n+                    self.config.problem_type = \"single_label_classification\"\n+                else:\n+                    self.config.problem_type = \"multi_label_classification\"\n+            if self.config.problem_type == \"regression\":\n+                loss_fct = MSELoss()\n+                if self.num_labels == 1:\n+                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n+                else:\n+                    loss = loss_fct(logits, labels)\n+            elif self.config.problem_type == \"single_label_classification\":\n+                loss_fct = CrossEntropyLoss()\n+                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n+            elif self.config.problem_type == \"multi_label_classification\":\n+                loss_fct = BCEWithLogitsLoss()\n+                loss = loss_fct(logits, labels)\n+\n+        if not return_dict:\n+            outputs = (loss, logits, hidden_states)\n+            outputs = tuple(output for output in outputs if output is not None)\n+            return outputs\n+\n+        return ImageClassifierOutput(\n+            loss=loss,\n+            logits=logits,\n+            hidden_states=hidden_states,\n+        )\n+\n+\n+__all__ = [\"TimmWrapperPreTrainedModel\", \"TimmWrapperModel\", \"TimmWrapperForImageClassification\"]"
        },
        {
            "sha": "7fb647b253832ee6c0b2ba507d299887b928a5b8",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe",
            "patch": "@@ -55,6 +55,8 @@\n     is_tensor,\n     is_tf_symbolic_tensor,\n     is_tf_tensor,\n+    is_timm_config_dict,\n+    is_timm_local_checkpoint,\n     is_torch_device,\n     is_torch_dtype,\n     is_torch_tensor,"
        },
        {
            "sha": "26889198228b2fa52bab30e802955bfc9318c624",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe",
            "patch": "@@ -9043,6 +9043,27 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class TimmWrapperForImageClassification(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class TimmWrapperModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class TimmWrapperPreTrainedModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class TrOCRForCausalLM(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "8b67b5dac58db13278706e0838aebbe495a9d2fc",
            "filename": "src/transformers/utils/dummy_timm_and_torchvision_objects.py",
            "status": "added",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/src%2Ftransformers%2Futils%2Fdummy_timm_and_torchvision_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/src%2Ftransformers%2Futils%2Fdummy_timm_and_torchvision_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_timm_and_torchvision_objects.py?ref=5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe",
            "patch": "@@ -0,0 +1,9 @@\n+# This file is autogenerated by the command `make fix-copies`, do not edit.\n+from ..utils import DummyObject, requires_backends\n+\n+\n+class TimmWrapperImageProcessor(metaclass=DummyObject):\n+    _backends = [\"timm\", \"torchvision\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"timm\", \"torchvision\"])"
        },
        {
            "sha": "a997da79e8419de2bdd8e1af5be8f57d970d64ce",
            "filename": "src/transformers/utils/generic.py",
            "status": "modified",
            "additions": 36,
            "deletions": 1,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/src%2Ftransformers%2Futils%2Fgeneric.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/src%2Ftransformers%2Futils%2Fgeneric.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fgeneric.py?ref=5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe",
            "patch": "@@ -16,6 +16,8 @@\n \"\"\"\n \n import inspect\n+import json\n+import os\n import tempfile\n import warnings\n from collections import OrderedDict, UserDict\n@@ -24,7 +26,7 @@\n from dataclasses import fields, is_dataclass\n from enum import Enum\n from functools import partial, wraps\n-from typing import Any, ContextManager, Iterable, List, Optional, Tuple, TypedDict\n+from typing import Any, ContextManager, Dict, Iterable, List, Optional, Tuple, TypedDict\n \n import numpy as np\n from packaging import version\n@@ -867,3 +869,36 @@ class LossKwargs(TypedDict, total=False):\n     \"\"\"\n \n     num_items_in_batch: Optional[int]\n+\n+\n+def is_timm_config_dict(config_dict: Dict[str, Any]) -> bool:\n+    \"\"\"Checks whether a config dict is a timm config dict.\"\"\"\n+    return \"pretrained_cfg\" in config_dict\n+\n+\n+def is_timm_local_checkpoint(pretrained_model_path: str) -> bool:\n+    \"\"\"\n+    Checks whether a checkpoint is a timm model checkpoint.\n+    \"\"\"\n+    if pretrained_model_path is None:\n+        return False\n+\n+    # in case it's Path, not str\n+    pretrained_model_path = str(pretrained_model_path)\n+\n+    is_file = os.path.isfile(pretrained_model_path)\n+    is_dir = os.path.isdir(pretrained_model_path)\n+\n+    # pretrained_model_path is a file\n+    if is_file and pretrained_model_path.endswith(\".json\"):\n+        with open(pretrained_model_path, \"r\") as f:\n+            config_dict = json.load(f)\n+        return is_timm_config_dict(config_dict)\n+\n+    # pretrained_model_path is a directory with a config.json\n+    if is_dir and os.path.exists(os.path.join(pretrained_model_path, \"config.json\")):\n+        with open(os.path.join(pretrained_model_path, \"config.json\"), \"r\") as f:\n+            config_dict = json.load(f)\n+        return is_timm_config_dict(config_dict)\n+\n+    return False"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/timm_wrapper/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/tests%2Fmodels%2Ftimm_wrapper%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/tests%2Fmodels%2Ftimm_wrapper%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftimm_wrapper%2F__init__.py?ref=5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe"
        },
        {
            "sha": "49d864178d14b3ddce1177719679eff50c9b6faf",
            "filename": "tests/models/timm_wrapper/test_image_processing_timm_wrapper.py",
            "status": "added",
            "additions": 103,
            "deletions": 0,
            "changes": 103,
            "blob_url": "https://github.com/huggingface/transformers/blob/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/tests%2Fmodels%2Ftimm_wrapper%2Ftest_image_processing_timm_wrapper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/tests%2Fmodels%2Ftimm_wrapper%2Ftest_image_processing_timm_wrapper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftimm_wrapper%2Ftest_image_processing_timm_wrapper.py?ref=5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe",
            "patch": "@@ -0,0 +1,103 @@\n+# coding=utf-8\n+# Copyright 2024 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import tempfile\n+import unittest\n+\n+import numpy as np\n+\n+from transformers.testing_utils import require_torch, require_torchvision, require_vision\n+from transformers.utils import is_torch_available, is_vision_available\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    from transformers import TimmWrapperConfig, TimmWrapperImageProcessor\n+\n+\n+@require_torch\n+@require_vision\n+@require_torchvision\n+class TimmWrapperImageProcessingTest(unittest.TestCase):\n+    image_processing_class = TimmWrapperImageProcessor if is_vision_available() else None\n+\n+    def setUp(self):\n+        super().setUp()\n+        self.temp_dir = tempfile.TemporaryDirectory()\n+        config = TimmWrapperConfig.from_pretrained(\"timm/resnet18.a1_in1k\")\n+        config.save_pretrained(self.temp_dir.name)\n+\n+    def tearDown(self):\n+        self.temp_dir.cleanup()\n+\n+    def test_load_from_hub(self):\n+        image_processor = TimmWrapperImageProcessor.from_pretrained(\"timm/resnet18.a1_in1k\")\n+        self.assertIsInstance(image_processor, TimmWrapperImageProcessor)\n+\n+    def test_load_from_local_dir(self):\n+        image_processor = TimmWrapperImageProcessor.from_pretrained(self.temp_dir.name)\n+        self.assertIsInstance(image_processor, TimmWrapperImageProcessor)\n+\n+    def test_image_processor_properties(self):\n+        image_processor = TimmWrapperImageProcessor.from_pretrained(self.temp_dir.name)\n+        self.assertTrue(hasattr(image_processor, \"data_config\"))\n+        self.assertTrue(hasattr(image_processor, \"val_transforms\"))\n+        self.assertTrue(hasattr(image_processor, \"train_transforms\"))\n+\n+    def test_image_processor_call_numpy(self):\n+        image_processor = TimmWrapperImageProcessor.from_pretrained(self.temp_dir.name)\n+\n+        single_image = np.random.randint(256, size=(256, 256, 3), dtype=np.uint8)\n+        batch_images = [single_image, single_image, single_image]\n+\n+        # single image\n+        pixel_values = image_processor(single_image).pixel_values\n+        self.assertEqual(pixel_values.shape, (1, 3, 224, 224))\n+\n+        # batch images\n+        pixel_values = image_processor(batch_images).pixel_values\n+        self.assertEqual(pixel_values.shape, (3, 3, 224, 224))\n+\n+    def test_image_processor_call_pil(self):\n+        image_processor = TimmWrapperImageProcessor.from_pretrained(self.temp_dir.name)\n+\n+        single_image = Image.fromarray(np.random.randint(256, size=(256, 256, 3), dtype=np.uint8))\n+        batch_images = [single_image, single_image, single_image]\n+\n+        # single image\n+        pixel_values = image_processor(single_image).pixel_values\n+        self.assertEqual(pixel_values.shape, (1, 3, 224, 224))\n+\n+        # batch images\n+        pixel_values = image_processor(batch_images).pixel_values\n+        self.assertEqual(pixel_values.shape, (3, 3, 224, 224))\n+\n+    def test_image_processor_call_tensor(self):\n+        image_processor = TimmWrapperImageProcessor.from_pretrained(self.temp_dir.name)\n+\n+        single_image = torch.from_numpy(np.random.randint(256, size=(3, 256, 256), dtype=np.uint8)).float()\n+        batch_images = [single_image, single_image, single_image]\n+\n+        # single image\n+        pixel_values = image_processor(single_image).pixel_values\n+        self.assertEqual(pixel_values.shape, (1, 3, 224, 224))\n+\n+        # batch images\n+        pixel_values = image_processor(batch_images).pixel_values\n+        self.assertEqual(pixel_values.shape, (3, 3, 224, 224))"
        },
        {
            "sha": "6f63c0aa147d0914f3e957f75a1b4ef715bb0fe8",
            "filename": "tests/models/timm_wrapper/test_modeling_timm_wrapper.py",
            "status": "added",
            "additions": 366,
            "deletions": 0,
            "changes": 366,
            "blob_url": "https://github.com/huggingface/transformers/blob/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/tests%2Fmodels%2Ftimm_wrapper%2Ftest_modeling_timm_wrapper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/tests%2Fmodels%2Ftimm_wrapper%2Ftest_modeling_timm_wrapper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftimm_wrapper%2Ftest_modeling_timm_wrapper.py?ref=5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe",
            "patch": "@@ -0,0 +1,366 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import inspect\n+import tempfile\n+import unittest\n+\n+from transformers.testing_utils import (\n+    require_bitsandbytes,\n+    require_timm,\n+    require_torch,\n+    require_vision,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils.import_utils import is_timm_available, is_torch_available, is_vision_available\n+\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import TimmWrapperConfig, TimmWrapperForImageClassification, TimmWrapperModel\n+\n+\n+if is_timm_available():\n+    import timm\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    from transformers import TimmWrapperImageProcessor\n+\n+\n+class TimmWrapperModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        model_name=\"timm/resnet18.a1_in1k\",\n+        batch_size=3,\n+        image_size=32,\n+        num_channels=3,\n+        is_training=True,\n+    ):\n+        self.parent = parent\n+        self.model_name = model_name\n+        self.batch_size = batch_size\n+        self.image_size = image_size\n+        self.num_channels = num_channels\n+        self.is_training = is_training\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n+        config = self.get_config()\n+\n+        return config, pixel_values\n+\n+    def get_config(self):\n+        return TimmWrapperConfig.from_pretrained(self.model_name)\n+\n+    def create_and_check_model(self, config, pixel_values):\n+        model = TimmWrapperModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.no_grad():\n+            result = model(pixel_values)\n+        self.parent.assertEqual(\n+            result.feature_map[-1].shape,\n+            (self.batch_size, model.channels[-1], 14, 14),\n+        )\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values = config_and_inputs\n+        inputs_dict = {\"pixel_values\": pixel_values}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+@require_timm\n+class TimmWrapperModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    all_model_classes = (TimmWrapperModel, TimmWrapperForImageClassification) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\"image-feature-extraction\": TimmWrapperModel, \"image-classification\": TimmWrapperForImageClassification}\n+        if is_torch_available()\n+        else {}\n+    )\n+\n+    test_resize_embeddings = False\n+    test_head_masking = False\n+    test_pruning = False\n+    has_attentions = False\n+    test_model_parallel = False\n+\n+    def setUp(self):\n+        self.config_class = TimmWrapperConfig\n+        self.model_tester = TimmWrapperModelTester(self)\n+        self.config_tester = ConfigTester(\n+            self,\n+            config_class=self.config_class,\n+            has_text_modality=False,\n+            common_properties=[],\n+            model_name=\"timm/resnet18.a1_in1k\",\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_hidden_states_output(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+\n+            # check all hidden states\n+            with torch.no_grad():\n+                outputs = model(**inputs_dict, output_hidden_states=True)\n+            self.assertTrue(\n+                len(outputs.hidden_states) == 5, f\"expected 5 hidden states, but got {len(outputs.hidden_states)}\"\n+            )\n+            expected_shapes = [[16, 16], [8, 8], [4, 4], [2, 2], [1, 1]]\n+            resulted_shapes = [list(h.shape[2:]) for h in outputs.hidden_states]\n+            self.assertListEqual(expected_shapes, resulted_shapes)\n+\n+            # check we can select hidden states by indices\n+            with torch.no_grad():\n+                outputs = model(**inputs_dict, output_hidden_states=[-2, -1])\n+            self.assertTrue(\n+                len(outputs.hidden_states) == 2, f\"expected 2 hidden states, but got {len(outputs.hidden_states)}\"\n+            )\n+            expected_shapes = [[2, 2], [1, 1]]\n+            resulted_shapes = [list(h.shape[2:]) for h in outputs.hidden_states]\n+            self.assertListEqual(expected_shapes, resulted_shapes)\n+\n+    @unittest.skip(reason=\"TimmWrapper models doesn't have inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    @unittest.skip(reason=\"TimmWrapper models doesn't have inputs_embeds\")\n+    def test_model_get_set_embeddings(self):\n+        pass\n+\n+    @unittest.skip(reason=\"TimmWrapper doesn't support output_attentions=True.\")\n+    def test_torchscript_output_attentions(self):\n+        pass\n+\n+    @unittest.skip(reason=\"TimmWrapper doesn't support this.\")\n+    def test_retain_grad_hidden_states_attentions(self):\n+        pass\n+\n+    @unittest.skip(reason=\"TimmWrapper initialization is managed on the timm side\")\n+    def test_initialization(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Need to use a timm model and there is no tiny model available.\")\n+    def test_model_is_small(self):\n+        pass\n+\n+    def test_forward_signature(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            signature = inspect.signature(model.forward)\n+            # signature.parameters is an OrderedDict => so arg_names order is deterministic\n+            arg_names = [*signature.parameters.keys()]\n+\n+            expected_arg_names = [\"pixel_values\"]\n+            self.assertListEqual(arg_names[:1], expected_arg_names)\n+\n+    def test_do_pooling_option(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.do_pooling = False\n+\n+        model = TimmWrapperModel._from_config(config)\n+\n+        # check there is no pooling\n+        with torch.no_grad():\n+            output = model(**inputs_dict)\n+        self.assertIsNone(output.pooler_output)\n+\n+        # check there is pooler output\n+        with torch.no_grad():\n+            output = model(**inputs_dict, do_pooling=True)\n+        self.assertIsNotNone(output.pooler_output)\n+\n+\n+# We will verify our results on an image of cute cats\n+def prepare_img():\n+    image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n+    return image\n+\n+\n+@require_torch\n+@require_timm\n+@require_vision\n+class TimmWrapperModelIntegrationTest(unittest.TestCase):\n+    # some popular ones\n+    model_names_to_test = [\n+        \"vit_small_patch16_384.augreg_in21k_ft_in1k\",\n+        \"resnet50.a1_in1k\",\n+        \"tf_mobilenetv3_large_minimal_100.in1k\",\n+        \"swin_tiny_patch4_window7_224.ms_in1k\",\n+        \"ese_vovnet19b_dw.ra_in1k\",\n+        \"hrnet_w18.ms_aug_in1k\",\n+    ]\n+\n+    @slow\n+    def test_inference_image_classification_head(self):\n+        checkpoint = \"timm/resnet18.a1_in1k\"\n+        model = TimmWrapperForImageClassification.from_pretrained(checkpoint, device_map=torch_device).eval()\n+        image_processor = TimmWrapperImageProcessor.from_pretrained(checkpoint)\n+\n+        image = prepare_img()\n+        inputs = image_processor(images=image, return_tensors=\"pt\").to(torch_device)\n+\n+        # forward pass\n+        with torch.no_grad():\n+            outputs = model(**inputs)\n+\n+        # verify the shape and logits\n+        expected_shape = torch.Size((1, 1000))\n+        self.assertEqual(outputs.logits.shape, expected_shape)\n+\n+        expected_label = 281  # tabby cat\n+        self.assertEqual(torch.argmax(outputs.logits).item(), expected_label)\n+\n+        expected_slice = torch.tensor([-11.2618, -9.6192, -10.3205]).to(torch_device)\n+        resulted_slice = outputs.logits[0, :3]\n+        is_close = torch.allclose(resulted_slice, expected_slice, atol=1e-3)\n+        self.assertTrue(is_close, f\"Expected {expected_slice}, but got {resulted_slice}\")\n+\n+    @slow\n+    @require_bitsandbytes\n+    def test_inference_image_classification_quantized(self):\n+        from transformers import BitsAndBytesConfig\n+\n+        checkpoint = \"timm/vit_small_patch16_384.augreg_in21k_ft_in1k\"\n+\n+        quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n+        model = TimmWrapperForImageClassification.from_pretrained(\n+            checkpoint, quantization_config=quantization_config, device_map=torch_device\n+        ).eval()\n+        image_processor = TimmWrapperImageProcessor.from_pretrained(checkpoint)\n+\n+        image = prepare_img()\n+        inputs = image_processor(images=image, return_tensors=\"pt\").to(torch_device)\n+\n+        # forward pass\n+        with torch.no_grad():\n+            outputs = model(**inputs)\n+\n+        # verify the shape and logits\n+        expected_shape = torch.Size((1, 1000))\n+        self.assertEqual(outputs.logits.shape, expected_shape)\n+\n+        expected_label = 281  # tabby cat\n+        self.assertEqual(torch.argmax(outputs.logits).item(), expected_label)\n+\n+        expected_slice = torch.tensor([-2.4043, 1.4492, -0.5127]).to(outputs.logits.dtype)\n+        resulted_slice = outputs.logits[0, :3].cpu()\n+        is_close = torch.allclose(resulted_slice, expected_slice, atol=0.1)\n+        self.assertTrue(is_close, f\"Expected {expected_slice}, but got {resulted_slice}\")\n+\n+    @slow\n+    def test_transformers_model_for_classification_is_equivalent_to_timm(self):\n+        # check that wrapper logits are the same as timm model logits\n+\n+        image = prepare_img()\n+\n+        for model_name in self.model_names_to_test:\n+            checkpoint = f\"timm/{model_name}\"\n+\n+            with self.subTest(msg=model_name):\n+                # prepare inputs\n+                image_processor = TimmWrapperImageProcessor.from_pretrained(checkpoint)\n+                pixel_values = image_processor(images=image).pixel_values.to(torch_device)\n+\n+                # load models\n+                model = TimmWrapperForImageClassification.from_pretrained(checkpoint, device_map=torch_device).eval()\n+                timm_model = timm.create_model(model_name, pretrained=True).to(torch_device).eval()\n+\n+                with torch.inference_mode():\n+                    outputs = model(pixel_values)\n+                    timm_outputs = timm_model(pixel_values)\n+\n+                # check shape is the same\n+                self.assertEqual(outputs.logits.shape, timm_outputs.shape)\n+\n+                # check logits are the same\n+                diff = (outputs.logits - timm_outputs).max().item()\n+                self.assertLess(diff, 1e-4)\n+\n+    @slow\n+    def test_transformers_model_is_equivalent_to_timm(self):\n+        # check that wrapper logits are the same as timm model logits\n+\n+        image = prepare_img()\n+\n+        models_to_test = [\"vit_small_patch16_224.dino\"] + self.model_names_to_test\n+\n+        for model_name in models_to_test:\n+            checkpoint = f\"timm/{model_name}\"\n+\n+            with self.subTest(msg=model_name):\n+                # prepare inputs\n+                image_processor = TimmWrapperImageProcessor.from_pretrained(checkpoint)\n+                pixel_values = image_processor(images=image).pixel_values.to(torch_device)\n+\n+                # load models\n+                model = TimmWrapperModel.from_pretrained(checkpoint, device_map=torch_device).eval()\n+                timm_model = timm.create_model(model_name, pretrained=True, num_classes=0).to(torch_device).eval()\n+\n+                with torch.inference_mode():\n+                    outputs = model(pixel_values)\n+                    timm_outputs = timm_model(pixel_values)\n+\n+                # check shape is the same\n+                self.assertEqual(outputs.pooler_output.shape, timm_outputs.shape)\n+\n+                # check logits are the same\n+                diff = (outputs.pooler_output - timm_outputs).max().item()\n+                self.assertLess(diff, 1e-4)\n+\n+    @slow\n+    def test_save_load_to_timm(self):\n+        # test that timm model can be loaded to transformers, saved and then loaded back into timm\n+\n+        model = TimmWrapperForImageClassification.from_pretrained(\n+            \"timm/resnet18.a1_in1k\", num_labels=10, ignore_mismatched_sizes=True\n+        )\n+\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            model.save_pretrained(tmpdirname)\n+\n+            # there is no direct way to load timm model from folder, use the same config + path to weights\n+            timm_model = timm.create_model(\n+                \"resnet18\", num_classes=10, checkpoint_path=f\"{tmpdirname}/model.safetensors\"\n+            )\n+\n+        # check that all weights are the same after reload\n+        different_weights = []\n+        for (name1, param1), (name2, param2) in zip(\n+            model.timm_model.named_parameters(), timm_model.named_parameters()\n+        ):\n+            if param1.shape != param2.shape or not torch.equal(param1, param2):\n+                different_weights.append((name1, name2))\n+\n+        if different_weights:\n+            self.fail(f\"Found different weights after reloading: {different_weights}\")"
        },
        {
            "sha": "13eacc4a59656210121710324bc5342d35fe34fe",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe",
            "patch": "@@ -3443,6 +3443,7 @@ def test_mismatched_shapes_have_properly_initialized_weights(self):\n                 \"Data2VecAudioForSequenceClassification\",\n                 \"UniSpeechForSequenceClassification\",\n                 \"PvtForImageClassification\",\n+                \"TimmWrapperForImageClassification\",\n             ]\n             special_param_names = [\n                 r\"^bit\\.\",\n@@ -3463,6 +3464,7 @@ def test_mismatched_shapes_have_properly_initialized_weights(self):\n                 r\"^swiftformer\\.\",\n                 r\"^swinv2\\.\",\n                 r\"^transformers\\.models\\.swiftformer\\.\",\n+                r\"^timm_model\\.\",\n                 r\"^unispeech\\.\",\n                 r\"^unispeech_sat\\.\",\n                 r\"^vision_model\\.\","
        },
        {
            "sha": "341fc42b9c68ee841d42bb39bed6f7d82eba61a1",
            "filename": "utils/check_config_docstrings.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/utils%2Fcheck_config_docstrings.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe/utils%2Fcheck_config_docstrings.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_docstrings.py?ref=5fcf6286bff90a2ba3b7a4b45fdf8542ea404dbe",
            "patch": "@@ -41,6 +41,7 @@\n     \"RagConfig\",\n     \"SpeechEncoderDecoderConfig\",\n     \"TimmBackboneConfig\",\n+    \"TimmWrapperConfig\",\n     \"VisionEncoderDecoderConfig\",\n     \"VisionTextDualEncoderConfig\",\n     \"LlamaConfig\","
        }
    ],
    "stats": {
        "total": 1561,
        "additions": 1432,
        "deletions": 129
    }
}