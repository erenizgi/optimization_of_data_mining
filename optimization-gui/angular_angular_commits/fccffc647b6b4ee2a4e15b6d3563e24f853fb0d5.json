{
    "author": "petebacondarwin",
    "message": "refactor(docs-infra): include more info in search index data (#41368)\n\nThe AIO search index is built in a WebWorker on the browser from a set\nof page information that is downloaded as a JSON file (`search-data.json`).\nWe want to keep this file as small as possible while providing enough\ndata to generate a useful index to query against.\n\nPreviously, we only included one copy of each (non-ignored) term from each\ndoc but this prevents more subtle ranking of query results, since the number\nof occurences of a term in a doc is lost.\n\nThis commit changes the generated file in the following ways:\n\n- All non-ignored terms are now included in the order in which they appear\n  in the doc.\n- The terms are indexed into a dictonary to avoid the text of the term being\n  repeated in every doc that contains the term.\n- Each term is pre-\"stemmed\" using the same Porter Stemming algorith that the\n  Lunr search engine uses.\n\nThe web-worker has been updated to decode the new format of the file.\nNow that all terms are included, it may enable some level of phrase based\nmatching in the future.\n\nThe size of the generated file is considerably larger than previously, but\non production HTTP servers the data is sent compressed, which reduces the\nsize dramatically.\n\nPR Close #41368",
    "sha": "fccffc647b6b4ee2a4e15b6d3563e24f853fb0d5",
    "files": [
        {
            "sha": "2157b78ab4d41b3c4f613350965b0687e6c9c208",
            "filename": "aio/package.json",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/angular/angular/blob/fccffc647b6b4ee2a4e15b6d3563e24f853fb0d5/aio%2Fpackage.json",
            "raw_url": "https://github.com/angular/angular/raw/fccffc647b6b4ee2a4e15b6d3563e24f853fb0d5/aio%2Fpackage.json",
            "contents_url": "https://api.github.com/repos/angular/angular/contents/aio%2Fpackage.json?ref=fccffc647b6b4ee2a4e15b6d3563e24f853fb0d5",
            "patch": "@@ -116,6 +116,7 @@\n     \"@types/jasmine\": \"~3.6.0\",\n     \"@types/lunr\": \"^2.3.2\",\n     \"@types/node\": \"^12.7.9\",\n+    \"@types/stemmer\": \"^1.0.2\",\n     \"@types/xregexp\": \"^3.0.30\",\n     \"@yarnpkg/lockfile\": \"^1.1.0\",\n     \"archiver\": \"^1.3.0\",\n@@ -166,6 +167,7 @@\n     \"rimraf\": \"^2.6.1\",\n     \"semver\": \"^5.3.0\",\n     \"shelljs\": \"^0.8.4\",\n+    \"stemmer\": \"^1.0.5\",\n     \"timezone-mock\": \"^1.1.3\",\n     \"tree-kill\": \"^1.1.0\",\n     \"ts-node\": \"^8.4.1\","
        },
        {
            "sha": "d31a5ce9354a013a7d4adb8a15648d201b600943",
            "filename": "aio/src/app/search/search.worker.ts",
            "status": "modified",
            "additions": 51,
            "deletions": 22,
            "changes": 73,
            "blob_url": "https://github.com/angular/angular/blob/fccffc647b6b4ee2a4e15b6d3563e24f853fb0d5/aio%2Fsrc%2Fapp%2Fsearch%2Fsearch.worker.ts",
            "raw_url": "https://github.com/angular/angular/raw/fccffc647b6b4ee2a4e15b6d3563e24f853fb0d5/aio%2Fsrc%2Fapp%2Fsearch%2Fsearch.worker.ts",
            "contents_url": "https://api.github.com/repos/angular/angular/contents/aio%2Fsrc%2Fapp%2Fsearch%2Fsearch.worker.ts?ref=fccffc647b6b4ee2a4e15b6d3563e24f853fb0d5",
            "patch": "@@ -1,10 +1,11 @@\n /// <reference lib=\"webworker\" />\n-import { WebWorkerMessage } from '../shared/web-worker-message';\n import * as lunr from 'lunr';\n \n+import {WebWorkerMessage} from '../shared/web-worker-message';\n+\n const SEARCH_TERMS_URL = '/generated/docs/app/search-data.json';\n let index: lunr.Index;\n-const pages: SearchInfo = {};\n+const pageMap: SearchInfo = {};\n \n interface SearchInfo {\n   [key: string]: PageInfo;\n@@ -13,8 +14,25 @@ interface SearchInfo {\n interface PageInfo {\n   path: string;\n   type: string;\n-  titleWords: string;\n-  keyWords: string;\n+  title: string;\n+  headings: string;\n+  keywords: string;\n+  members: string;\n+  topics: string;\n+}\n+\n+interface EncodedPages {\n+  dictionary: string[];\n+  pages: EncodedPage[];\n+}\n+\n+interface EncodedPage {\n+  path: string;\n+  type: string;\n+  title: string;\n+  headings: number[];\n+  keywords: number[];\n+  members: number[];\n   topics: string;\n }\n \n@@ -24,42 +42,42 @@ addEventListener('message', handleMessage);\n // the path and search terms for a page\n function createIndex(loadIndexFn: IndexLoader): lunr.Index {\n   // The lunr typings are missing QueryLexer so we have to add them here manually.\n-  const queryLexer = (lunr as any as { QueryLexer: { termSeparator: RegExp } }).QueryLexer;\n+  const queryLexer = (lunr as any as {QueryLexer: {termSeparator: RegExp}}).QueryLexer;\n   queryLexer.termSeparator = lunr.tokenizer.separator = /\\s+/;\n   return lunr(function() {\n+    this.pipeline.remove(lunr.stemmer);\n     this.ref('path');\n-    this.field('topics', { boost: 15 });\n-    this.field('titleWords', { boost: 10 });\n-    this.field('headingWords', { boost: 5 });\n-    this.field('members', { boost: 4 });\n-    this.field('keywords', { boost: 2 });\n+    this.field('topics', {boost: 15});\n+    this.field('title', {boost: 10});\n+    this.field('headings', {boost: 5});\n+    this.field('members', {boost: 4});\n+    this.field('keywords', {boost: 2});\n     loadIndexFn(this);\n   });\n }\n \n // The worker receives a message to load the index and to query the index\n-function handleMessage(message: { data: WebWorkerMessage }): void {\n+function handleMessage(message: {data: WebWorkerMessage}): void {\n   const type = message.data.type;\n   const id = message.data.id;\n   const payload = message.data.payload;\n   switch (type) {\n     case 'load-index':\n-      makeRequest(SEARCH_TERMS_URL, (searchInfo: PageInfo[]) => {\n-        index = createIndex(loadIndex(searchInfo));\n-        postMessage({ type, id, payload: true });\n+      makeRequest(SEARCH_TERMS_URL, (encodedPages: EncodedPages) => {\n+        index = createIndex(loadIndex(encodedPages));\n+        postMessage({type, id, payload: true});\n       });\n       break;\n     case 'query-index':\n-      postMessage({ type, id, payload: { query: payload, results: queryIndex(payload) } });\n+      postMessage({type, id, payload: {query: payload, results: queryIndex(payload)}});\n       break;\n     default:\n-      postMessage({ type, id, payload: { error: 'invalid message type' } });\n+      postMessage({type, id, payload: {error: 'invalid message type'}});\n   }\n }\n \n // Use XHR to make a request to the server\n function makeRequest(url: string, callback: (response: any) => void): void {\n-\n   // The JSON file that is loaded should be an array of PageInfo:\n   const searchDataRequest = new XMLHttpRequest();\n   searchDataRequest.onload = function() {\n@@ -70,18 +88,29 @@ function makeRequest(url: string, callback: (response: any) => void): void {\n }\n \n \n-// Create the search index from the searchInfo which contains the information about each page to be indexed\n-function loadIndex(pagesData: PageInfo[]): IndexLoader {\n+// Create the search index from the searchInfo which contains the information about each page to be\n+// indexed\n+function loadIndex({dictionary, pages}: EncodedPages): IndexLoader {\n   return (indexBuilder: lunr.Builder) => {\n     // Store the pages data to be used in mapping query results back to pages\n     // Add search terms from each page to the search index\n-    pagesData.forEach(page => {\n+    pages.forEach(encodedPage => {\n+      const page = decodePage(encodedPage, dictionary);\n       indexBuilder.add(page);\n-      pages[page.path] = page;\n+      pageMap[page.path] = page;\n     });\n   };\n }\n \n+function decodePage(encodedPage: EncodedPage, dictionary: string[]): PageInfo {\n+  return {\n+    ...encodedPage,\n+    headings: encodedPage.headings?.map(i => dictionary[i]).join(' ') ?? '',\n+    keywords: encodedPage.keywords?.map(i => dictionary[i]).join(' ') ?? '',\n+    members: encodedPage.members?.map(i => dictionary[i]).join(' ') ?? '',\n+  };\n+}\n+\n // Query the index and return the processed results\n function queryIndex(query: string): PageInfo[] {\n   // Strip off quotes\n@@ -105,7 +134,7 @@ function queryIndex(query: string): PageInfo[] {\n       }\n \n       // Map the hits into info about each page to be returned as results\n-      return results.map(hit => pages[hit.ref]);\n+      return results.map(hit => pageMap[hit.ref]);\n     }\n   } catch (e) {\n     // If the search query cannot be parsed the index throws an error"
        },
        {
            "sha": "0f56284ddbe4aae3d7e4a2526cd248238597425f",
            "filename": "aio/tools/transforms/angular-base-package/ignore-words.json",
            "status": "added",
            "additions": 705,
            "deletions": 0,
            "changes": 705,
            "blob_url": "https://github.com/angular/angular/blob/fccffc647b6b4ee2a4e15b6d3563e24f853fb0d5/aio%2Ftools%2Ftransforms%2Fangular-base-package%2Fignore-words.json",
            "raw_url": "https://github.com/angular/angular/raw/fccffc647b6b4ee2a4e15b6d3563e24f853fb0d5/aio%2Ftools%2Ftransforms%2Fangular-base-package%2Fignore-words.json",
            "contents_url": "https://api.github.com/repos/angular/angular/contents/aio%2Ftools%2Ftransforms%2Fangular-base-package%2Fignore-words.json?ref=fccffc647b6b4ee2a4e15b6d3563e24f853fb0d5",
            "patch": "@@ -0,0 +1,705 @@\n+{\n+  \"en\": [\n+    \"a\",\n+    \"able\",\n+    \"about\",\n+    \"above\",\n+    \"abst\",\n+    \"accordance\",\n+    \"according\",\n+    \"accordingly\",\n+    \"across\",\n+    \"act\",\n+    \"actually\",\n+    \"added\",\n+    \"adj\",\n+    \"adopted\",\n+    \"affected\",\n+    \"affecting\",\n+    \"affects\",\n+    \"after\",\n+    \"afterwards\",\n+    \"again\",\n+    \"against\",\n+    \"ah\",\n+    \"all\",\n+    \"almost\",\n+    \"alone\",\n+    \"along\",\n+    \"already\",\n+    \"also\",\n+    \"although\",\n+    \"always\",\n+    \"am\",\n+    \"among\",\n+    \"amongst\",\n+    \"an\",\n+    \"and\",\n+    \"announce\",\n+    \"another\",\n+    \"any\",\n+    \"anybody\",\n+    \"anyhow\",\n+    \"anymore\",\n+    \"anyone\",\n+    \"anything\",\n+    \"anyway\",\n+    \"anyways\",\n+    \"anywhere\",\n+    \"apparently\",\n+    \"approximately\",\n+    \"are\",\n+    \"aren\",\n+    \"arent\",\n+    \"arise\",\n+    \"around\",\n+    \"as\",\n+    \"aside\",\n+    \"ask\",\n+    \"asking\",\n+    \"at\",\n+    \"auth\",\n+    \"available\",\n+    \"away\",\n+    \"awfully\",\n+    \"b\",\n+    \"back\",\n+    \"be\",\n+    \"became\",\n+    \"because\",\n+    \"become\",\n+    \"becomes\",\n+    \"becoming\",\n+    \"been\",\n+    \"before\",\n+    \"beforehand\",\n+    \"begin\",\n+    \"beginning\",\n+    \"beginnings\",\n+    \"begins\",\n+    \"behind\",\n+    \"being\",\n+    \"believe\",\n+    \"below\",\n+    \"beside\",\n+    \"besides\",\n+    \"between\",\n+    \"beyond\",\n+    \"biol\",\n+    \"both\",\n+    \"brief\",\n+    \"briefly\",\n+    \"but\",\n+    \"by\",\n+    \"c\",\n+    \"ca\",\n+    \"came\",\n+    \"can\",\n+    \"cannot\",\n+    \"can't\",\n+    \"cant\",\n+    \"cause\",\n+    \"causes\",\n+    \"certain\",\n+    \"certainly\",\n+    \"co\",\n+    \"com\",\n+    \"come\",\n+    \"comes\",\n+    \"contain\",\n+    \"containing\",\n+    \"contains\",\n+    \"could\",\n+    \"couldnt\",\n+    \"d\",\n+    \"date\",\n+    \"did\",\n+    \"didn't\",\n+    \"didnt\",\n+    \"different\",\n+    \"do\",\n+    \"does\",\n+    \"doesn't\",\n+    \"doesnt\",\n+    \"doing\",\n+    \"done\",\n+    \"don't\",\n+    \"dont\",\n+    \"down\",\n+    \"downwards\",\n+    \"due\",\n+    \"during\",\n+    \"e\",\n+    \"each\",\n+    \"ed\",\n+    \"edu\",\n+    \"effect\",\n+    \"eg\",\n+    \"eight\",\n+    \"eighty\",\n+    \"either\",\n+    \"else\",\n+    \"elsewhere\",\n+    \"end\",\n+    \"ending\",\n+    \"enough\",\n+    \"especially\",\n+    \"et\",\n+    \"et-al\",\n+    \"etc\",\n+    \"even\",\n+    \"ever\",\n+    \"every\",\n+    \"everybody\",\n+    \"everyone\",\n+    \"everything\",\n+    \"everywhere\",\n+    \"ex\",\n+    \"except\",\n+    \"f\",\n+    \"far\",\n+    \"few\",\n+    \"ff\",\n+    \"fifth\",\n+    \"first\",\n+    \"five\",\n+    \"fix\",\n+    \"followed\",\n+    \"following\",\n+    \"follows\",\n+    \"for\",\n+    \"former\",\n+    \"formerly\",\n+    \"forth\",\n+    \"found\",\n+    \"four\",\n+    \"from\",\n+    \"further\",\n+    \"furthermore\",\n+    \"g\",\n+    \"gave\",\n+    \"get\",\n+    \"gets\",\n+    \"getting\",\n+    \"give\",\n+    \"given\",\n+    \"gives\",\n+    \"giving\",\n+    \"go\",\n+    \"goes\",\n+    \"gone\",\n+    \"got\",\n+    \"gotten\",\n+    \"h\",\n+    \"had\",\n+    \"happens\",\n+    \"hardly\",\n+    \"has\",\n+    \"hasn't\",\n+    \"hasnt\",\n+    \"have\",\n+    \"haven't\",\n+    \"havent\",\n+    \"having\",\n+    \"he\",\n+    \"hed\",\n+    \"hence\",\n+    \"her\",\n+    \"here\",\n+    \"hereafter\",\n+    \"hereby\",\n+    \"herein\",\n+    \"heres\",\n+    \"hereupon\",\n+    \"hers\",\n+    \"herself\",\n+    \"hes\",\n+    \"hi\",\n+    \"hid\",\n+    \"him\",\n+    \"himself\",\n+    \"his\",\n+    \"hither\",\n+    \"home\",\n+    \"how\",\n+    \"howbeit\",\n+    \"however\",\n+    \"hundred\",\n+    \"i\",\n+    \"id\",\n+    \"ie\",\n+    \"if\",\n+    \"i'll\",\n+    \"ill\",\n+    \"im\",\n+    \"immediate\",\n+    \"immediately\",\n+    \"importance\",\n+    \"important\",\n+    \"in\",\n+    \"inc\",\n+    \"indeed\",\n+    \"index\",\n+    \"information\",\n+    \"instead\",\n+    \"into\",\n+    \"invention\",\n+    \"inward\",\n+    \"is\",\n+    \"isn't\",\n+    \"isnt\",\n+    \"it\",\n+    \"itd\",\n+    \"it'll\",\n+    \"itll\",\n+    \"its\",\n+    \"itself\",\n+    \"i've\",\n+    \"ive\",\n+    \"j\",\n+    \"just\",\n+    \"k\",\n+    \"keep\",\n+    \"keeps\",\n+    \"kept\",\n+    \"keys\",\n+    \"kg\",\n+    \"km\",\n+    \"know\",\n+    \"known\",\n+    \"knows\",\n+    \"l\",\n+    \"largely\",\n+    \"last\",\n+    \"lately\",\n+    \"later\",\n+    \"latter\",\n+    \"latterly\",\n+    \"least\",\n+    \"less\",\n+    \"lest\",\n+    \"let\",\n+    \"lets\",\n+    \"like\",\n+    \"liked\",\n+    \"likely\",\n+    \"line\",\n+    \"little\",\n+    \"'ll\",\n+    \"'ll\",\n+    \"look\",\n+    \"looking\",\n+    \"looks\",\n+    \"ltd\",\n+    \"m\",\n+    \"made\",\n+    \"mainly\",\n+    \"make\",\n+    \"makes\",\n+    \"many\",\n+    \"may\",\n+    \"maybe\",\n+    \"me\",\n+    \"mean\",\n+    \"means\",\n+    \"meantime\",\n+    \"meanwhile\",\n+    \"merely\",\n+    \"mg\",\n+    \"might\",\n+    \"million\",\n+    \"miss\",\n+    \"ml\",\n+    \"more\",\n+    \"moreover\",\n+    \"most\",\n+    \"mostly\",\n+    \"mr\",\n+    \"mrs\",\n+    \"much\",\n+    \"mug\",\n+    \"must\",\n+    \"my\",\n+    \"myself\",\n+    \"n\",\n+    \"na\",\n+    \"name\",\n+    \"namely\",\n+    \"nay\",\n+    \"nd\",\n+    \"near\",\n+    \"nearly\",\n+    \"necessarily\",\n+    \"necessary\",\n+    \"need\",\n+    \"needs\",\n+    \"neither\",\n+    \"never\",\n+    \"nevertheless\",\n+    \"new\",\n+    \"next\",\n+    \"nine\",\n+    \"ninety\",\n+    \"no\",\n+    \"nobody\",\n+    \"non\",\n+    \"none\",\n+    \"nonetheless\",\n+    \"noone\",\n+    \"nor\",\n+    \"normally\",\n+    \"nos\",\n+    \"not\",\n+    \"noted\",\n+    \"nothing\",\n+    \"now\",\n+    \"nowhere\",\n+    \"o\",\n+    \"obtain\",\n+    \"obtained\",\n+    \"obviously\",\n+    \"of\",\n+    \"off\",\n+    \"often\",\n+    \"oh\",\n+    \"ok\",\n+    \"okay\",\n+    \"old\",\n+    \"omitted\",\n+    \"on\",\n+    \"once\",\n+    \"one\",\n+    \"ones\",\n+    \"only\",\n+    \"onto\",\n+    \"or\",\n+    \"ord\",\n+    \"other\",\n+    \"others\",\n+    \"otherwise\",\n+    \"ought\",\n+    \"our\",\n+    \"ours\",\n+    \"ourselves\",\n+    \"out\",\n+    \"outside\",\n+    \"over\",\n+    \"overall\",\n+    \"owing\",\n+    \"own\",\n+    \"p\",\n+    \"page\",\n+    \"pages\",\n+    \"part\",\n+    \"particular\",\n+    \"particularly\",\n+    \"past\",\n+    \"per\",\n+    \"perhaps\",\n+    \"placed\",\n+    \"please\",\n+    \"plus\",\n+    \"poorly\",\n+    \"possible\",\n+    \"possibly\",\n+    \"potentially\",\n+    \"pp\",\n+    \"predominantly\",\n+    \"present\",\n+    \"previously\",\n+    \"primarily\",\n+    \"probably\",\n+    \"promptly\",\n+    \"proud\",\n+    \"provides\",\n+    \"put\",\n+    \"q\",\n+    \"que\",\n+    \"quickly\",\n+    \"quite\",\n+    \"qv\",\n+    \"r\",\n+    \"ran\",\n+    \"rather\",\n+    \"rd\",\n+    \"re\",\n+    \"readily\",\n+    \"really\",\n+    \"recent\",\n+    \"recently\",\n+    \"ref\",\n+    \"refs\",\n+    \"regarding\",\n+    \"regardless\",\n+    \"regards\",\n+    \"related\",\n+    \"relatively\",\n+    \"research\",\n+    \"respectively\",\n+    \"resulted\",\n+    \"resulting\",\n+    \"results\",\n+    \"right\",\n+    \"run\",\n+    \"s\",\n+    \"said\",\n+    \"same\",\n+    \"saw\",\n+    \"say\",\n+    \"saying\",\n+    \"says\",\n+    \"sec\",\n+    \"section\",\n+    \"see\",\n+    \"seeing\",\n+    \"seem\",\n+    \"seemed\",\n+    \"seeming\",\n+    \"seems\",\n+    \"seen\",\n+    \"self\",\n+    \"selves\",\n+    \"sent\",\n+    \"seven\",\n+    \"several\",\n+    \"shall\",\n+    \"she\",\n+    \"shed\",\n+    \"she'll\",\n+    \"shell\",\n+    \"shes\",\n+    \"should\",\n+    \"shouldn't\",\n+    \"shouldnt\",\n+    \"show\",\n+    \"showed\",\n+    \"shown\",\n+    \"showns\",\n+    \"shows\",\n+    \"significant\",\n+    \"significantly\",\n+    \"similar\",\n+    \"similarly\",\n+    \"since\",\n+    \"six\",\n+    \"slightly\",\n+    \"so\",\n+    \"some\",\n+    \"somebody\",\n+    \"somehow\",\n+    \"someone\",\n+    \"somethan\",\n+    \"something\",\n+    \"sometime\",\n+    \"sometimes\",\n+    \"somewhat\",\n+    \"somewhere\",\n+    \"soon\",\n+    \"sorry\",\n+    \"specifically\",\n+    \"specified\",\n+    \"specify\",\n+    \"specifying\",\n+    \"state\",\n+    \"states\",\n+    \"still\",\n+    \"stop\",\n+    \"strongly\",\n+    \"sub\",\n+    \"substantially\",\n+    \"successfully\",\n+    \"such\",\n+    \"sufficiently\",\n+    \"suggest\",\n+    \"sup\",\n+    \"sure\",\n+    \"t\",\n+    \"take\",\n+    \"taken\",\n+    \"taking\",\n+    \"tell\",\n+    \"tends\",\n+    \"th\",\n+    \"than\",\n+    \"thank\",\n+    \"thanks\",\n+    \"thanx\",\n+    \"that\",\n+    \"that'll\",\n+    \"thatll\",\n+    \"thats\",\n+    \"that've\",\n+    \"thatve\",\n+    \"the\",\n+    \"their\",\n+    \"theirs\",\n+    \"them\",\n+    \"themselves\",\n+    \"then\",\n+    \"thence\",\n+    \"there\",\n+    \"thereafter\",\n+    \"thereby\",\n+    \"thered\",\n+    \"therefore\",\n+    \"therein\",\n+    \"there'll\",\n+    \"therell\",\n+    \"thereof\",\n+    \"therere\",\n+    \"theres\",\n+    \"thereto\",\n+    \"thereupon\",\n+    \"there've\",\n+    \"thereve\",\n+    \"these\",\n+    \"they\",\n+    \"theyd\",\n+    \"they'll\",\n+    \"theyll\",\n+    \"theyre\",\n+    \"they've\",\n+    \"theyve\",\n+    \"think\",\n+    \"this\",\n+    \"those\",\n+    \"thou\",\n+    \"though\",\n+    \"thoughh\",\n+    \"thousand\",\n+    \"throug\",\n+    \"through\",\n+    \"throughout\",\n+    \"thru\",\n+    \"thus\",\n+    \"til\",\n+    \"tip\",\n+    \"to\",\n+    \"together\",\n+    \"too\",\n+    \"took\",\n+    \"toward\",\n+    \"towards\",\n+    \"tried\",\n+    \"tries\",\n+    \"truly\",\n+    \"try\",\n+    \"trying\",\n+    \"ts\",\n+    \"twice\",\n+    \"two\",\n+    \"u\",\n+    \"un\",\n+    \"under\",\n+    \"unfortunately\",\n+    \"unless\",\n+    \"unlike\",\n+    \"unlikely\",\n+    \"until\",\n+    \"unto\",\n+    \"up\",\n+    \"upon\",\n+    \"ups\",\n+    \"us\",\n+    \"use\",\n+    \"used\",\n+    \"useful\",\n+    \"usefully\",\n+    \"usefulness\",\n+    \"uses\",\n+    \"using\",\n+    \"usually\",\n+    \"v\",\n+    \"value\",\n+    \"various\",\n+    \"'ve\",\n+    \"'ve\",\n+    \"very\",\n+    \"via\",\n+    \"viz\",\n+    \"vol\",\n+    \"vols\",\n+    \"vs\",\n+    \"w\",\n+    \"want\",\n+    \"wants\",\n+    \"was\",\n+    \"wasn't\",\n+    \"wasnt\",\n+    \"way\",\n+    \"we\",\n+    \"wed\",\n+    \"welcome\",\n+    \"we'll\",\n+    \"well\",\n+    \"went\",\n+    \"were\",\n+    \"weren't\",\n+    \"werent\",\n+    \"we've\",\n+    \"weve\",\n+    \"what\",\n+    \"whatever\",\n+    \"what'll\",\n+    \"whatll\",\n+    \"whats\",\n+    \"when\",\n+    \"whence\",\n+    \"whenever\",\n+    \"where\",\n+    \"whereafter\",\n+    \"whereas\",\n+    \"whereby\",\n+    \"wherein\",\n+    \"wheres\",\n+    \"whereupon\",\n+    \"wherever\",\n+    \"whether\",\n+    \"which\",\n+    \"while\",\n+    \"whim\",\n+    \"whither\",\n+    \"who\",\n+    \"whod\",\n+    \"whoever\",\n+    \"whole\",\n+    \"who'll\",\n+    \"wholl\",\n+    \"whom\",\n+    \"whomever\",\n+    \"whos\",\n+    \"whose\",\n+    \"why\",\n+    \"widely\",\n+    \"will\",\n+    \"willing\",\n+    \"wish\",\n+    \"with\",\n+    \"within\",\n+    \"without\",\n+    \"won't\",\n+    \"wont\",\n+    \"words\",\n+    \"would\",\n+    \"wouldn't\",\n+    \"wouldnt\",\n+    \"www\",\n+    \"x\",\n+    \"y\",\n+    \"yes\",\n+    \"yet\",\n+    \"you\",\n+    \"youd\",\n+    \"you'll\",\n+    \"youll\",\n+    \"your\",\n+    \"youre\",\n+    \"yours\",\n+    \"yourself\",\n+    \"yourselves\",\n+    \"you've\",\n+    \"youve\",\n+    \"z\",\n+    \"zero\"\n+  ]\n+}"
        },
        {
            "sha": "82b9f2fc3fc5dcd658478fd63a6dd4a50e6dcb12",
            "filename": "aio/tools/transforms/angular-base-package/ignore.words",
            "status": "removed",
            "additions": 0,
            "deletions": 701,
            "changes": 701,
            "blob_url": "https://github.com/angular/angular/blob/55f7f1d446e6280541ea525490ba36fe1db2d7ab/aio%2Ftools%2Ftransforms%2Fangular-base-package%2Fignore.words",
            "raw_url": "https://github.com/angular/angular/raw/55f7f1d446e6280541ea525490ba36fe1db2d7ab/aio%2Ftools%2Ftransforms%2Fangular-base-package%2Fignore.words",
            "contents_url": "https://api.github.com/repos/angular/angular/contents/aio%2Ftools%2Ftransforms%2Fangular-base-package%2Fignore.words?ref=55f7f1d446e6280541ea525490ba36fe1db2d7ab",
            "patch": "@@ -1,701 +0,0 @@\n-a\n-able\n-about\n-above\n-abst\n-accordance\n-according\n-accordingly\n-across\n-act\n-actually\n-added\n-adj\n-adopted\n-affected\n-affecting\n-affects\n-after\n-afterwards\n-again\n-against\n-ah\n-all\n-almost\n-alone\n-along\n-already\n-also\n-although\n-always\n-am\n-among\n-amongst\n-an\n-and\n-announce\n-another\n-any\n-anybody\n-anyhow\n-anymore\n-anyone\n-anything\n-anyway\n-anyways\n-anywhere\n-apparently\n-approximately\n-are\n-aren\n-arent\n-arise\n-around\n-as\n-aside\n-ask\n-asking\n-at\n-auth\n-available\n-away\n-awfully\n-b\n-back\n-be\n-became\n-because\n-become\n-becomes\n-becoming\n-been\n-before\n-beforehand\n-begin\n-beginning\n-beginnings\n-begins\n-behind\n-being\n-believe\n-below\n-beside\n-besides\n-between\n-beyond\n-biol\n-both\n-brief\n-briefly\n-but\n-by\n-c\n-ca\n-came\n-can\n-cannot\n-can't\n-cant\n-cause\n-causes\n-certain\n-certainly\n-co\n-com\n-come\n-comes\n-contain\n-containing\n-contains\n-could\n-couldnt\n-d\n-date\n-did\n-didn't\n-didnt\n-different\n-do\n-does\n-doesn't\n-doesnt\n-doing\n-done\n-don't\n-dont\n-down\n-downwards\n-due\n-during\n-e\n-each\n-ed\n-edu\n-effect\n-eg\n-eight\n-eighty\n-either\n-else\n-elsewhere\n-end\n-ending\n-enough\n-especially\n-et\n-et-al\n-etc\n-even\n-ever\n-every\n-everybody\n-everyone\n-everything\n-everywhere\n-ex\n-except\n-f\n-far\n-few\n-ff\n-fifth\n-first\n-five\n-fix\n-followed\n-following\n-follows\n-for\n-former\n-formerly\n-forth\n-found\n-four\n-from\n-further\n-furthermore\n-g\n-gave\n-get\n-gets\n-getting\n-give\n-given\n-gives\n-giving\n-go\n-goes\n-gone\n-got\n-gotten\n-h\n-had\n-happens\n-hardly\n-has\n-hasn't\n-hasnt\n-have\n-haven't\n-havent\n-having\n-he\n-hed\n-hence\n-her\n-here\n-hereafter\n-hereby\n-herein\n-heres\n-hereupon\n-hers\n-herself\n-hes\n-hi\n-hid\n-him\n-himself\n-his\n-hither\n-home\n-how\n-howbeit\n-however\n-hundred\n-i\n-id\n-ie\n-if\n-i'll\n-ill\n-im\n-immediate\n-immediately\n-importance\n-important\n-in\n-inc\n-indeed\n-index\n-information\n-instead\n-into\n-invention\n-inward\n-is\n-isn't\n-isnt\n-it\n-itd\n-it'll\n-itll\n-its\n-itself\n-i've\n-ive\n-j\n-just\n-k\n-keep\n-keeps\n-kept\n-keys\n-kg\n-km\n-know\n-known\n-knows\n-l\n-largely\n-last\n-lately\n-later\n-latter\n-latterly\n-least\n-less\n-lest\n-let\n-lets\n-like\n-liked\n-likely\n-line\n-little\n-'ll\n-'ll\n-look\n-looking\n-looks\n-ltd\n-m\n-made\n-mainly\n-make\n-makes\n-many\n-may\n-maybe\n-me\n-mean\n-means\n-meantime\n-meanwhile\n-merely\n-mg\n-might\n-million\n-miss\n-ml\n-more\n-moreover\n-most\n-mostly\n-mr\n-mrs\n-much\n-mug\n-must\n-my\n-myself\n-n\n-na\n-name\n-namely\n-nay\n-nd\n-near\n-nearly\n-necessarily\n-necessary\n-need\n-needs\n-neither\n-never\n-nevertheless\n-new\n-next\n-nine\n-ninety\n-no\n-nobody\n-non\n-none\n-nonetheless\n-noone\n-nor\n-normally\n-nos\n-not\n-noted\n-nothing\n-now\n-nowhere\n-o\n-obtain\n-obtained\n-obviously\n-of\n-off\n-often\n-oh\n-ok\n-okay\n-old\n-omitted\n-on\n-once\n-one\n-ones\n-only\n-onto\n-or\n-ord\n-other\n-others\n-otherwise\n-ought\n-our\n-ours\n-ourselves\n-out\n-outside\n-over\n-overall\n-owing\n-own\n-p\n-page\n-pages\n-part\n-particular\n-particularly\n-past\n-per\n-perhaps\n-placed\n-please\n-plus\n-poorly\n-possible\n-possibly\n-potentially\n-pp\n-predominantly\n-present\n-previously\n-primarily\n-probably\n-promptly\n-proud\n-provides\n-put\n-q\n-que\n-quickly\n-quite\n-qv\n-r\n-ran\n-rather\n-rd\n-re\n-readily\n-really\n-recent\n-recently\n-ref\n-refs\n-regarding\n-regardless\n-regards\n-related\n-relatively\n-research\n-respectively\n-resulted\n-resulting\n-results\n-right\n-run\n-s\n-said\n-same\n-saw\n-say\n-saying\n-says\n-sec\n-section\n-see\n-seeing\n-seem\n-seemed\n-seeming\n-seems\n-seen\n-self\n-selves\n-sent\n-seven\n-several\n-shall\n-she\n-shed\n-she'll\n-shell\n-shes\n-should\n-shouldn't\n-shouldnt\n-show\n-showed\n-shown\n-showns\n-shows\n-significant\n-significantly\n-similar\n-similarly\n-since\n-six\n-slightly\n-so\n-some\n-somebody\n-somehow\n-someone\n-somethan\n-something\n-sometime\n-sometimes\n-somewhat\n-somewhere\n-soon\n-sorry\n-specifically\n-specified\n-specify\n-specifying\n-state\n-states\n-still\n-stop\n-strongly\n-sub\n-substantially\n-successfully\n-such\n-sufficiently\n-suggest\n-sup\n-sure\n-t\n-take\n-taken\n-taking\n-tell\n-tends\n-th\n-than\n-thank\n-thanks\n-thanx\n-that\n-that'll\n-thatll\n-thats\n-that've\n-thatve\n-the\n-their\n-theirs\n-them\n-themselves\n-then\n-thence\n-there\n-thereafter\n-thereby\n-thered\n-therefore\n-therein\n-there'll\n-therell\n-thereof\n-therere\n-theres\n-thereto\n-thereupon\n-there've\n-thereve\n-these\n-they\n-theyd\n-they'll\n-theyll\n-theyre\n-they've\n-theyve\n-think\n-this\n-those\n-thou\n-though\n-thoughh\n-thousand\n-throug\n-through\n-throughout\n-thru\n-thus\n-til\n-tip\n-to\n-together\n-too\n-took\n-toward\n-towards\n-tried\n-tries\n-truly\n-try\n-trying\n-ts\n-twice\n-two\n-u\n-un\n-under\n-unfortunately\n-unless\n-unlike\n-unlikely\n-until\n-unto\n-up\n-upon\n-ups\n-us\n-use\n-used\n-useful\n-usefully\n-usefulness\n-uses\n-using\n-usually\n-v\n-value\n-various\n-'ve\n-'ve\n-very\n-via\n-viz\n-vol\n-vols\n-vs\n-w\n-want\n-wants\n-was\n-wasn't\n-wasnt\n-way\n-we\n-wed\n-welcome\n-we'll\n-well\n-went\n-were\n-weren't\n-werent\n-we've\n-weve\n-what\n-whatever\n-what'll\n-whatll\n-whats\n-when\n-whence\n-whenever\n-where\n-whereafter\n-whereas\n-whereby\n-wherein\n-wheres\n-whereupon\n-wherever\n-whether\n-which\n-while\n-whim\n-whither\n-who\n-whod\n-whoever\n-whole\n-who'll\n-wholl\n-whom\n-whomever\n-whos\n-whose\n-why\n-widely\n-will\n-willing\n-wish\n-with\n-within\n-without\n-won't\n-wont\n-words\n-would\n-wouldn't\n-wouldnt\n-www\n-x\n-y\n-yes\n-yet\n-you\n-youd\n-you'll\n-youll\n-your\n-youre\n-yours\n-yourself\n-yourselves\n-you've\n-youve\n-z\n-zero"
        },
        {
            "sha": "8c946c89ecabf09588e71a2dd3be4fe4efabbc42",
            "filename": "aio/tools/transforms/angular-base-package/index.js",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/angular/angular/blob/fccffc647b6b4ee2a4e15b6d3563e24f853fb0d5/aio%2Ftools%2Ftransforms%2Fangular-base-package%2Findex.js",
            "raw_url": "https://github.com/angular/angular/raw/fccffc647b6b4ee2a4e15b6d3563e24f853fb0d5/aio%2Ftools%2Ftransforms%2Fangular-base-package%2Findex.js",
            "contents_url": "https://api.github.com/repos/angular/angular/contents/aio%2Ftools%2Ftransforms%2Fangular-base-package%2Findex.js?ref=fccffc647b6b4ee2a4e15b6d3563e24f853fb0d5",
            "patch": "@@ -65,9 +65,9 @@ module.exports = new Package('angular-base', [\n     readFilesProcessor.sourceFiles = [];\n     collectExamples.exampleFolders = [];\n \n-    generateKeywordsProcessor.ignoreWordsFile = path.resolve(__dirname, 'ignore.words');\n+    generateKeywordsProcessor.ignoreWords = require(path.resolve(__dirname, 'ignore-words'))['en'];\n     generateKeywordsProcessor.docTypesToIgnore = ['example-region'];\n-    generateKeywordsProcessor.propertiesToIgnore = ['basePath', 'renderedContent'];\n+    generateKeywordsProcessor.propertiesToIgnore = ['basePath', 'renderedContent', 'docType', 'searchTitle'];\n   })\n \n   // Where do we write the output files?"
        },
        {
            "sha": "020d460de8586649db4bc0881a872a7c3d1d6e6d",
            "filename": "aio/tools/transforms/angular-base-package/processors/generateKeywords.js",
            "status": "modified",
            "additions": 96,
            "deletions": 101,
            "changes": 197,
            "blob_url": "https://github.com/angular/angular/blob/fccffc647b6b4ee2a4e15b6d3563e24f853fb0d5/aio%2Ftools%2Ftransforms%2Fangular-base-package%2Fprocessors%2FgenerateKeywords.js",
            "raw_url": "https://github.com/angular/angular/raw/fccffc647b6b4ee2a4e15b6d3563e24f853fb0d5/aio%2Ftools%2Ftransforms%2Fangular-base-package%2Fprocessors%2FgenerateKeywords.js",
            "contents_url": "https://api.github.com/repos/angular/angular/contents/aio%2Ftools%2Ftransforms%2Fangular-base-package%2Fprocessors%2FgenerateKeywords.js?ref=fccffc647b6b4ee2a4e15b6d3563e24f853fb0d5",
            "patch": "@@ -1,7 +1,6 @@\n 'use strict';\n \n-var fs = require('fs');\n-var path = require('canonical-path');\n+const stem = require('stemmer');\n \n /**\n  * @dgProcessor generateKeywordsProcessor\n@@ -10,103 +9,98 @@ var path = require('canonical-path');\n  * a new document that will be rendered as a JavaScript file containing all\n  * this data.\n  */\n-module.exports = function generateKeywordsProcessor(log, readFilesProcessor) {\n+module.exports = function generateKeywordsProcessor(log) {\n   return {\n-    ignoreWordsFile: undefined,\n+    ignoreWords: [],\n     propertiesToIgnore: [],\n     docTypesToIgnore: [],\n     outputFolder: '',\n     $validate: {\n-      ignoreWordsFile: {},\n+      ignoreWords: {},\n       docTypesToIgnore: {},\n       propertiesToIgnore: {},\n       outputFolder: {presence: true}\n     },\n     $runAfter: ['postProcessHtml'],\n     $runBefore: ['writing-files'],\n-    $process: function(docs) {\n+    $process(docs) {\n \n-      // Keywords to ignore\n-      var wordsToIgnore = [];\n-      var propertiesToIgnore;\n-      var docTypesToIgnore;\n-\n-      // Load up the keywords to ignore, if specified in the config\n-      if (this.ignoreWordsFile) {\n-        var ignoreWordsPath = path.resolve(readFilesProcessor.basePath, this.ignoreWordsFile);\n-        wordsToIgnore = fs.readFileSync(ignoreWordsPath, 'utf8').toString().split(/[,\\s\\n\\r]+/gm);\n+      const dictionary = new Map();\n \n-        log.debug('Loaded ignore words from \"' + ignoreWordsPath + '\"');\n-        log.silly(wordsToIgnore);\n-      }\n-\n-      propertiesToIgnore = convertToMap(this.propertiesToIgnore);\n+      // Keywords to ignore\n+      const ignoreWords = new Set(this.ignoreWords);\n+      log.debug('Words to ignore', ignoreWords);\n+      const propertiesToIgnore = new Set(this.propertiesToIgnore);\n       log.debug('Properties to ignore', propertiesToIgnore);\n-      docTypesToIgnore = convertToMap(this.docTypesToIgnore);\n+      const docTypesToIgnore = new Set(this.docTypesToIgnore);\n       log.debug('Doc types to ignore', docTypesToIgnore);\n \n-      var ignoreWordsMap = convertToMap(wordsToIgnore);\n \n       const filteredDocs = docs\n           // We are not interested in some docTypes\n-          .filter(function(doc) { return !docTypesToIgnore[doc.docType]; })\n+          .filter(doc => !docTypesToIgnore.has(doc.docType))\n           // Ignore internals and private exports (indicated by the Éµ prefix)\n-          .filter(function(doc) { return !doc.internal && !doc.privateExport; });\n+          .filter(doc => !doc.internal && !doc.privateExport);\n \n \n-      filteredDocs.forEach(function(doc) {\n-\n-        var words = [];\n-        var keywordMap = Object.assign({}, ignoreWordsMap);\n-        var members = [];\n-        var membersMap = Object.assign({}, ignoreWordsMap);\n-        const headingWords = [];\n-        const headingWordMap = Object.assign({}, ignoreWordsMap);\n-\n+      for(const doc of filteredDocs) {\n         // Search each top level property of the document for search terms\n-        Object.keys(doc).forEach(function(key) {\n+        let mainTokens = [];\n+        for(const key of Object.keys(doc)) {\n           const value = doc[key];\n-\n-          if (isString(value) && !propertiesToIgnore[key]) {\n-            extractWords(value, words, keywordMap);\n+          if (isString(value) && !propertiesToIgnore.has(key)) {\n+            mainTokens.push(...tokenize(value, ignoreWords, dictionary));\n           }\n-        });\n+        }\n \n-        extractMemberWords(doc, members, membersMap);\n+        const memberTokens = extractMemberTokens(doc, ignoreWords, dictionary);\n \n         // Extract all the keywords from the headings\n+        let headingTokens = [];\n         if (doc.vFile && doc.vFile.headings) {\n-          Object.keys(doc.vFile.headings).forEach(function(headingTag) {\n-            doc.vFile.headings[headingTag].forEach(function(headingText) {\n-              extractWords(headingText, headingWords, headingWordMap);\n-            });\n-          });\n+          for(const headingTag of Object.keys(doc.vFile.headings)) {\n+            for(const headingText of doc.vFile.headings[headingTag]) {\n+              headingTokens.push(...tokenize(headingText, ignoreWords, dictionary));\n+            }\n+          }\n         }\n \n+\n         // Extract the title to use in searches\n         doc.searchTitle = doc.searchTitle || doc.title || doc.vFile && doc.vFile.title || doc.name || '';\n \n         // Attach all this search data to the document\n-        doc.searchTerms = {\n-          titleWords: tokenize(doc.searchTitle).join(' '),\n-          headingWords: headingWords.sort().join(' '),\n-          keywords: words.sort().join(' '),\n-          members: members.sort().join(' '),\n-          topics: doc.searchKeywords\n-        };\n-\n-      });\n+        doc.searchTerms = {};\n+        if (headingTokens.length > 0) {\n+          doc.searchTerms.headings = headingTokens;\n+        }\n+        if (mainTokens.length > 0) {\n+          doc.searchTerms.keywords = mainTokens;\n+        }\n+        if (memberTokens.length > 0) {\n+          doc.searchTerms.members = memberTokens;\n+        }\n+        if (doc.searchKeywords) {\n+          doc.searchTerms.topics = doc.searchKeywords.trim();\n+        }\n+      }\n \n       // Now process all the search data and collect it up to be used in creating a new document\n-      var searchData = filteredDocs.map(function(page) {\n-        // Copy the properties from the searchTerms object onto the search data object\n-        return Object.assign({\n-          path: page.path,\n-          title: page.searchTitle,\n-          type: page.docType,\n-          deprecated: !!page.deprecated,\n-        }, page.searchTerms);\n-      });\n+      const searchData = {\n+        dictionary: Array.from(dictionary.keys()),\n+        pages: filteredDocs.map(page => {\n+          // Copy the properties from the searchTerms object onto the search data object\n+          const searchObj = {\n+            path: page.path,\n+            title: page.searchTitle,\n+            type: page.docType,\n+          };\n+          if (page.deprecated) {\n+            searchObj.deprecated = true;\n+          }\n+          return Object.assign(searchObj, page.searchTerms);\n+        }),\n+      };\n \n       docs.push({\n         docType: 'json-doc',\n@@ -120,63 +114,64 @@ module.exports = function generateKeywordsProcessor(log, readFilesProcessor) {\n   };\n };\n \n-\n function isString(value) {\n   return typeof value == 'string';\n }\n \n-function convertToMap(collection) {\n-  const obj = {};\n-  collection.forEach(key => { obj[key] = true; });\n-  return obj;\n-}\n-\n-// If the heading contains a name starting with ng, e.g. \"ngController\", then add the\n-// name without the ng to the text, e.g. \"controller\".\n-function tokenize(text) {\n-  const rawTokens = text.split(/[\\s\\/]+/mg);\n+function tokenize(text, ignoreWords, dictionary) {\n+  // Split on whitespace and things that are likely to be HTML tags (this is not exhaustive but reduces the unwanted tokens that are indexed).\n+  const rawTokens = text.split(/[\\s\\/]+|<\\/?[a-z]+(?:\\s+\\w+(?:=\"[^\"]+\")?)*>/img);\n   const tokens = [];\n-  rawTokens.forEach(token => {\n+  for(let token of rawTokens) {\n+    token = token.trim();\n+\n     // Strip off unwanted trivial characters\n-    token = token\n-        .trim()\n-        .replace(/^[_\\-\"'`({[<$*)}\\]>.]+/, '')\n-        .replace(/[_\\-\"'`({[<$*)}\\]>.]+$/, '');\n-    // Ignore tokens that contain weird characters\n-    if (/^[\\w.\\-]+$/.test(token)) {\n-      tokens.push(token.toLowerCase());\n-      const ngTokenMatch = /^[nN]g([A-Z]\\w*)/.exec(token);\n-      if (ngTokenMatch) {\n-        tokens.push(ngTokenMatch[1].toLowerCase());\n-      }\n+    token = token.replace(/^[_\\-\"'`({[<$*)}\\]>.]+/, '').replace(/[_\\-\"'`({[<$*)}\\]>.]+$/, '');\n+\n+    // Skip if in the ignored words list\n+    if (ignoreWords.has(token.toLowerCase())) {\n+      continue;\n+    }\n+\n+    // Skip tokens that contain weird characters\n+    if (!/^[\\w._-]+$/.test(token)) {\n+      continue;\n     }\n-  });\n+\n+    storeToken(token, tokens, dictionary);\n+    if (token.startsWith('ng')) {\n+      storeToken(token.substr(2), tokens, dictionary);\n+    }\n+  }\n+\n   return tokens;\n }\n \n-function extractWords(text, words, keywordMap) {\n-  var tokens = tokenize(text);\n-  tokens.forEach(function(token) {\n-    if (!keywordMap[token]) {\n-      words.push(token);\n-      keywordMap[token] = true;\n-    }\n-  });\n+function storeToken(token, tokens, dictionary) {\n+  token = stem(token);\n+  if (!dictionary.has(token)) {\n+    dictionary.set(token, dictionary.size);\n+  }\n+  tokens.push(dictionary.get(token));\n }\n \n-function extractMemberWords(doc, members, membersMap) {\n-  if (!doc) return;\n+function extractMemberTokens(doc, ignoreWords, dictionary) {\n+  if (!doc) return '';\n+\n+  let memberContent = [];\n \n   if (doc.members) {\n-    doc.members.forEach(member => extractWords(member.name, members, membersMap));\n+    doc.members.forEach(member => memberContent.push(...tokenize(member.name, ignoreWords, dictionary)));\n   }\n   if (doc.statics) {\n-    doc.statics.forEach(member => extractWords(member.name, members, membersMap));\n+    doc.statics.forEach(member => memberContent.push(...tokenize(member.name, ignoreWords, dictionary)));\n   }\n   if (doc.extendsClauses) {\n-    doc.extendsClauses.forEach(clause => extractMemberWords(clause.doc, members, membersMap));\n+    doc.extendsClauses.forEach(clause => memberContent.push(...extractMemberTokens(clause.doc, ignoreWords, dictionary)));\n   }\n   if (doc.implementsClauses) {\n-    doc.implementsClauses.forEach(clause => extractMemberWords(clause.doc, members, membersMap));\n+    doc.implementsClauses.forEach(clause => memberContent.push(...extractMemberTokens(clause.doc, ignoreWords, dictionary)));\n   }\n-}\n\\ No newline at end of file\n+\n+  return memberContent;\n+}"
        },
        {
            "sha": "3065a1c16e354b389f3f09f173c5af229f9764d3",
            "filename": "aio/tools/transforms/angular-base-package/processors/generateKeywords.spec.js",
            "status": "modified",
            "additions": 141,
            "deletions": 52,
            "changes": 193,
            "blob_url": "https://github.com/angular/angular/blob/fccffc647b6b4ee2a4e15b6d3563e24f853fb0d5/aio%2Ftools%2Ftransforms%2Fangular-base-package%2Fprocessors%2FgenerateKeywords.spec.js",
            "raw_url": "https://github.com/angular/angular/raw/fccffc647b6b4ee2a4e15b6d3563e24f853fb0d5/aio%2Ftools%2Ftransforms%2Fangular-base-package%2Fprocessors%2FgenerateKeywords.spec.js",
            "contents_url": "https://api.github.com/repos/angular/angular/contents/aio%2Ftools%2Ftransforms%2Fangular-base-package%2Fprocessors%2FgenerateKeywords.spec.js?ref=fccffc647b6b4ee2a4e15b6d3563e24f853fb0d5",
            "patch": "@@ -1,12 +1,22 @@\n+const path = require('canonical-path');\n+const Dgeni = require('dgeni');\n+\n const testPackage = require('../../helpers/test-package');\n const mockLogger = require('dgeni/lib/mocks/log')(false);\n const processorFactory = require('./generateKeywords');\n-const Dgeni = require('dgeni');\n \n const mockReadFilesProcessor = {\n   basePath: 'base/path'\n };\n \n+const ignoreWords = require(path.resolve(__dirname, '../ignore-words'))['en'];\n+\n+function createProcessor() {\n+  const processor = processorFactory(mockLogger, mockReadFilesProcessor);\n+  processor.ignoreWords = ignoreWords;\n+  return processor;\n+}\n+\n describe('generateKeywords processor', () => {\n \n   it('should be available on the injector', () => {\n@@ -17,38 +27,89 @@ describe('generateKeywords processor', () => {\n   });\n \n   it('should run after the correct processor', () => {\n-    const processor = processorFactory(mockLogger, mockReadFilesProcessor);\n+    const processor = createProcessor();\n     expect(processor.$runAfter).toEqual(['postProcessHtml']);\n   });\n \n   it('should run before the correct processor', () => {\n-    const processor = processorFactory(mockLogger, mockReadFilesProcessor);\n+    const processor = createProcessor();\n     expect(processor.$runBefore).toEqual(['writing-files']);\n   });\n \n   it('should ignore internal and private exports', () => {\n-    const processor = processorFactory(mockLogger, mockReadFilesProcessor);\n+    const processor = createProcessor();\n     const docs = [\n       { docType: 'class', name: 'PublicExport' },\n       { docType: 'class', name: 'PrivateExport', privateExport: true },\n       { docType: 'class', name: 'InternalExport', internal: true }\n     ];\n     processor.$process(docs);\n-    expect(docs[docs.length - 1].data).toEqual([\n-      jasmine.objectContaining({ title: 'PublicExport', type: 'class'})\n+    expect(docs[docs.length - 1].data.pages).toEqual([\n+      jasmine.objectContaining({ title: 'PublicExport', type: 'class' })\n+    ]);\n+  });\n+\n+  it('should ignore docs that are in the `docTypesToIgnore` list', () => {\n+    const processor = createProcessor();\n+    processor.docTypesToIgnore = ['interface'];\n+    const docs = [\n+      { docType: 'class', name: 'Class' },\n+      { docType: 'interface', name: 'Interface' },\n+      { docType: 'content', name: 'Guide' },\n+    ];\n+    processor.$process(docs);\n+    expect(docs[docs.length - 1].data.pages).toEqual([\n+      jasmine.objectContaining({ title: 'Class', type: 'class' }),\n+      jasmine.objectContaining({ title: 'Guide', type: 'content' }),\n     ]);\n   });\n \n+  it('should not collect keywords from properties that are in the `propertiesToIgnore` list', () => {\n+    const processor = createProcessor();\n+    processor.propertiesToIgnore = ['docType', 'ignore'];\n+    const docs = [\n+      { docType: 'class', name: 'FooClass', ignore: 'ignore this content' },\n+      { docType: 'interface', name: 'BarInterface', capture: 'capture this content' },\n+    ];\n+    processor.$process(docs);\n+    expect(docs[docs.length - 1].data).toEqual({\n+      dictionary: [ 'fooclass', 'barinterfac', 'captur', 'content' ],\n+      pages: [\n+        jasmine.objectContaining({ title: 'FooClass', type: 'class', keywords: [0] }),\n+        jasmine.objectContaining({ title: 'BarInterface', type: 'interface', keywords: [1, 2, 3] }),\n+      ],\n+    });\n+  });\n+\n+  it('should not collect keywords that look like HTML tags', () => {\n+    const processor = createProcessor();\n+    const docs = [\n+      { docType: 'class', name: 'FooClass', content: `\n+      <table id=\"foo\">\n+        <tr class=\"moo\" id=\"bar\">\n+          <td>Content inside a table</td>\n+        </tr>\n+      </table>` },\n+    ];\n+    processor.$process(docs);\n+    expect(docs[docs.length - 1].data).toEqual({\n+      dictionary: ['class', 'fooclass', 'content', 'insid', 'tabl'],\n+      pages: [\n+        jasmine.objectContaining({keywords: [0, 1, 2, 3, 4] })\n+      ],\n+    });\n+  });\n+\n   it('should compute `doc.searchTitle` from the doc properties if not already provided', () => {\n-    const processor = processorFactory(mockLogger, mockReadFilesProcessor);\n+    const processor = createProcessor();\n     const docs = [\n       { docType: 'class', name: 'A', searchTitle: 'searchTitle A', title: 'title A', vFile: { headings: { h1: ['vFile A'] } } },\n       { docType: 'class', name: 'B', title: 'title B', vFile: { headings: { h1: ['vFile B'] } } },\n       { docType: 'class', name: 'C', vFile: { title: 'vFile C', headings: { h1: ['vFile C'] } } },\n       { docType: 'class', name: 'D' },\n     ];\n     processor.$process(docs);\n-    expect(docs[docs.length - 1].data).toEqual([\n+    expect(docs[docs.length - 1].data.pages).toEqual([\n       jasmine.objectContaining({ title: 'searchTitle A' }),\n       jasmine.objectContaining({ title: 'title B' }),\n       jasmine.objectContaining({ title: 'vFile C' }),\n@@ -57,34 +118,19 @@ describe('generateKeywords processor', () => {\n   });\n \n   it('should use `doc.searchTitle` as the title in the search index', () => {\n-    const processor = processorFactory(mockLogger, mockReadFilesProcessor);\n+    const processor = createProcessor();\n     const docs = [\n       { docType: 'class', name: 'PublicExport', searchTitle: 'class PublicExport' },\n     ];\n     processor.$process(docs);\n     const keywordsDoc = docs[docs.length - 1];\n-    expect(keywordsDoc.data).toEqual([\n-      jasmine.objectContaining({ title: 'class PublicExport', type: 'class'})\n+    expect(keywordsDoc.data.pages).toEqual([\n+      jasmine.objectContaining({ title: 'class PublicExport', type: 'class' })\n     ]);\n   });\n \n-  it('should add title words to the search terms', () => {\n-    const processor = processorFactory(mockLogger, mockReadFilesProcessor);\n-    const docs = [\n-      {\n-        docType: 'class',\n-        name: 'PublicExport',\n-        searchTitle: 'class PublicExport',\n-        vFile: { headings: { h2: ['heading A', 'heading B'] } }\n-      },\n-    ];\n-    processor.$process(docs);\n-    const keywordsDoc = docs[docs.length - 1];\n-    expect(keywordsDoc.data[0].titleWords).toEqual('class publicexport');\n-  });\n-\n   it('should add heading words to the search terms', () => {\n-    const processor = processorFactory(mockLogger, mockReadFilesProcessor);\n+    const processor = createProcessor();\n     const docs = [\n       {\n         docType: 'class',\n@@ -95,11 +141,16 @@ describe('generateKeywords processor', () => {\n     ];\n     processor.$process(docs);\n     const keywordsDoc = docs[docs.length - 1];\n-    expect(keywordsDoc.data[0].headingWords).toEqual('heading important secondary');\n+    expect(keywordsDoc.data).toEqual({\n+      dictionary: ['class', 'publicexport', 'head', 'secondari'],\n+      pages: [\n+        jasmine.objectContaining({ headings: [2, 3, 2] })\n+      ]\n+    });\n   });\n \n   it('should add member doc properties to the search terms', () => {\n-    const processor = processorFactory(mockLogger, mockReadFilesProcessor);\n+    const processor = createProcessor();\n     const docs = [\n       {\n         docType: 'class',\n@@ -123,13 +174,18 @@ describe('generateKeywords processor', () => {\n     ];\n     processor.$process(docs);\n     const keywordsDoc = docs[docs.length - 1];\n-    expect(keywordsDoc.data[0].members).toEqual(\n-      'instancemethoda instancemethodb instancepropertya instancepropertyb staticmethoda staticmethodb staticpropertya staticpropertyb'\n-    );\n+    expect(keywordsDoc.data).toEqual({\n+      dictionary: ['class', 'publicexport', 'content', 'ngclass', 'instancemethoda','instancepropertya','instancemethodb','instancepropertyb','staticmethoda','staticpropertya','staticmethodb','staticpropertyb', 'head'],\n+      pages: [\n+        jasmine.objectContaining({\n+          members: [4, 5, 6, 7, 8, 9, 10, 11]\n+        })\n+      ]\n+    });\n   });\n \n   it('should add inherited member doc properties to the search terms', () => {\n-    const processor = processorFactory(mockLogger, mockReadFilesProcessor);\n+    const processor = createProcessor();\n     const parentClass =       {\n       docType: 'class',\n       name: 'ParentClass',\n@@ -163,13 +219,27 @@ describe('generateKeywords processor', () => {\n     const docs = [childClass, parentClass, parentInterface];\n     processor.$process(docs);\n     const keywordsDoc = docs[docs.length - 1];\n-    expect(keywordsDoc.data[0].members.split(' ').sort().join(' ')).toEqual(\n-      'childmember1 childmember2 parentmember1 parentmember2 parentmember3'\n-    );\n+    expect(keywordsDoc.data).toEqual({\n+      dictionary: ['class', 'child', 'childmember1', 'childmember2', 'parentmember1', 'parentmember2', 'parentmember3', 'parentclass', 'interfac', 'parentinterfac'],\n+      pages: [\n+        jasmine.objectContaining({\n+          title: 'Child',\n+          members: [2, 3, 4, 5, 6]\n+        }),\n+        jasmine.objectContaining({\n+          title: 'ParentClass',\n+          members: [4, 5]\n+        }),\n+        jasmine.objectContaining({\n+          title: 'ParentInterface',\n+          members: [6]\n+        })\n+      ]\n+    });\n   });\n \n-  it('should process terms prefixed with \"ng\" to include the term stripped of \"ng\"', () => {\n-    const processor = processorFactory(mockLogger, mockReadFilesProcessor);\n+  it('should include both stripped and unstripped \"ng\" prefixed tokens', () => {\n+    const processor = createProcessor();\n     const docs = [\n       {\n         docType: 'class',\n@@ -181,34 +251,53 @@ describe('generateKeywords processor', () => {\n     ];\n     processor.$process(docs);\n     const keywordsDoc = docs[docs.length - 1];\n-    expect(keywordsDoc.data[0].titleWords).toEqual('ngcontroller controller');\n-    expect(keywordsDoc.data[0].headingWords).toEqual('model ngmodel');\n-    expect(keywordsDoc.data[0].keywords).toContain('class');\n-    expect(keywordsDoc.data[0].keywords).toContain('ngclass');\n+    expect(keywordsDoc.data).toEqual({\n+      dictionary: ['class', 'publicexport', 'ngcontrol', 'control', 'content', 'ngclass', 'ngmodel', 'model'],\n+      pages: [\n+        jasmine.objectContaining({\n+          headings: [6, 7],\n+          keywords: [0, 1, 2, 3, 4, 5, 0],\n+        })\n+      ],\n+    });\n   });\n \n-  it('should generate renderedContent property', () => {\n-    const processor = processorFactory(mockLogger, mockReadFilesProcessor);\n+  it('should generate compressed encoded renderedContent property', () => {\n+    const processor = createProcessor();\n     const docs = [\n       {\n         docType: 'class',\n         name: 'SomeClass',\n         description: 'The is the documentation for the SomeClass API.',\n         vFile: { headings: { h1: ['SomeClass'], h2: ['Some heading'] } }\n       },\n+      {\n+        docType: 'class',\n+        name: 'SomeClass2',\n+        description: 'description',\n+        members: [\n+          { name: 'member1' },\n+        ],\n+        deprecated: true\n+      },\n     ];\n     processor.$process(docs);\n     const keywordsDoc = docs[docs.length - 1];\n-    expect(JSON.parse(keywordsDoc.renderedContent)).toEqual(\n-      [{\n+    expect(JSON.parse(keywordsDoc.renderedContent)).toEqual({\n+      dictionary: ['class', 'someclass', 'document', 'api', 'head', 'someclass2', 'descript', 'member1'],\n+      pages: [{\n         'title':'SomeClass',\n         'type':'class',\n-        'titleWords':'someclass',\n-        'headingWords':'heading some someclass',\n-        'keywords':'api class documentation for is someclass the',\n-        'members':'',\n-        'deprecated': false,\n+        'headings': [1, 4],\n+        'keywords': [0, 1, 2, 1, 3],\n+      },\n+      {\n+        'title':'SomeClass2',\n+        'type':'class',\n+        'keywords': [0, 5, 6],\n+        'members': [7],\n+        'deprecated': true,\n       }]\n-    );\n+    });\n   });\n });"
        },
        {
            "sha": "bcfb0be3d0936f4b8802c55a95a257609cef52fc",
            "filename": "aio/yarn.lock",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/angular/angular/blob/fccffc647b6b4ee2a4e15b6d3563e24f853fb0d5/aio%2Fyarn.lock",
            "raw_url": "https://github.com/angular/angular/raw/fccffc647b6b4ee2a4e15b6d3563e24f853fb0d5/aio%2Fyarn.lock",
            "contents_url": "https://api.github.com/repos/angular/angular/contents/aio%2Fyarn.lock?ref=fccffc647b6b4ee2a4e15b6d3563e24f853fb0d5",
            "patch": "@@ -2005,6 +2005,11 @@\n   resolved \"https://registry.yarnpkg.com/@types/source-list-map/-/source-list-map-0.1.2.tgz#0078836063ffaf17412349bba364087e0ac02ec9\"\n   integrity sha512-K5K+yml8LTo9bWJI/rECfIPrGgxdpeNbj+d53lwN4QjW1MCwlkhUms+gtdzigTeUyBr09+u8BwOIY3MXvHdcsA==\n \n+\"@types/stemmer@^1.0.2\":\n+  version \"1.0.2\"\n+  resolved \"https://registry.yarnpkg.com/@types/stemmer/-/stemmer-1.0.2.tgz#bd8354f50b3c9b87c351d169240e45cf1fa1f5e8\"\n+  integrity sha512-2gWEIFqVZjjZxo8/TcugCAl7nW9Jd9ArEDpTAc5nH7d+ZUkreHA7GzuFcLZ0sflLrA5b1PZ+2yDyHJcuP9KWWw==\n+\n \"@types/unist@*\", \"@types/unist@^2.0.0\", \"@types/unist@^2.0.2\":\n   version \"2.0.3\"\n   resolved \"https://registry.yarnpkg.com/@types/unist/-/unist-2.0.3.tgz#9c088679876f374eb5983f150d4787aa6fb32d7e\"\n@@ -12802,6 +12807,11 @@ static-extend@^0.1.1:\n   resolved \"https://registry.yarnpkg.com/statuses/-/statuses-1.5.0.tgz#161c7dac177659fd9811f43771fa99381478628c\"\n   integrity sha1-Fhx9rBd2Wf2YEfQ3cfqZOBR4Yow=\n \n+stemmer@^1.0.5:\n+  version \"1.0.5\"\n+  resolved \"https://registry.yarnpkg.com/stemmer/-/stemmer-1.0.5.tgz#fd89beaf8bff5d04b6643bfffcaed0fc420deec0\"\n+  integrity sha512-SLq7annzSKRDStasOJJoftCSCzBCKmBmH38jC4fDtCunAqOzpTpIm9zmaHmwNJiZ8gLe9qpVdBVbEG2DC5dE2A==\n+\n stream-browserify@^2.0.1:\n   version \"2.0.2\"\n   resolved \"https://registry.yarnpkg.com/stream-browserify/-/stream-browserify-2.0.2.tgz#87521d38a44aa7ee91ce1cd2a47df0cb49dd660b\""
        }
    ],
    "stats": {
        "total": 1885,
        "additions": 1007,
        "deletions": 878
    }
}