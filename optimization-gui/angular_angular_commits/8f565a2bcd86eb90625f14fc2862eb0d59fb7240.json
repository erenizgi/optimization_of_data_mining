{
    "author": "petebacondarwin",
    "message": "refactor(compiler): support interpolation tokens when lexing markup (#43132)\n\nThe lexer now splits interpolation tokens out from text tokens.\n\nPreviously the contents of `<div>Hello, {{ name}}<div>` would be a single\ntext token. Now it will be three tokens:\n\n```\nTEXT: \"Hello, \"\nINTERPOLATION: \"{{\", \" name\", \"}}\"\nTEXT: \"\"\n```\n\n- INTERPOLATION tokens have three parts, \"start marker\", \"expression\"\n  and \"end marker\".\n- INTERPOLATION tokens are always preceded and followed by TEXT tokens,\n  even if they represent an empty string.\n\nThe HTML parser has been modified to recombine these tokens to allow this\nrefactoring to have limited effect in this commit. Further refactorings\nto use these new tokens will follow in subsequent commits.\n\nPR Close #43132",
    "sha": "8f565a2bcd86eb90625f14fc2862eb0d59fb7240",
    "files": [
        {
            "sha": "464582bbaa3f1529026ce540c4c97dcc7a9e492b",
            "filename": "packages/compiler/src/ml_parser/lexer.ts",
            "status": "modified",
            "additions": 119,
            "deletions": 33,
            "changes": 152,
            "blob_url": "https://github.com/angular/angular/blob/8f565a2bcd86eb90625f14fc2862eb0d59fb7240/packages%2Fcompiler%2Fsrc%2Fml_parser%2Flexer.ts",
            "raw_url": "https://github.com/angular/angular/raw/8f565a2bcd86eb90625f14fc2862eb0d59fb7240/packages%2Fcompiler%2Fsrc%2Fml_parser%2Flexer.ts",
            "contents_url": "https://api.github.com/repos/angular/angular/contents/packages%2Fcompiler%2Fsrc%2Fml_parser%2Flexer.ts?ref=8f565a2bcd86eb90625f14fc2862eb0d59fb7240",
            "patch": "@@ -22,13 +22,15 @@ export enum TokenType {\n   TEXT,\n   ESCAPABLE_RAW_TEXT,\n   RAW_TEXT,\n+  INTERPOLATION,\n   COMMENT_START,\n   COMMENT_END,\n   CDATA_START,\n   CDATA_END,\n   ATTR_NAME,\n   ATTR_QUOTE,\n-  ATTR_VALUE,\n+  ATTR_VALUE_TEXT,\n+  ATTR_VALUE_INTERPOLATION,\n   DOC_TYPE,\n   EXPANSION_FORM_START,\n   EXPANSION_CASE_VALUE,\n@@ -227,7 +229,11 @@ class _Tokenizer {\n             this._consumeTagOpen(start);\n           }\n         } else if (!(this._tokenizeIcu && this._tokenizeExpansionForm())) {\n-          this._consumeText();\n+          // In (possibly interpolated) text the end of the text is given by `isTextEnd()`, while\n+          // the premature end of an interpolation is given by the start of a new HTML element.\n+          this._consumeWithInterpolation(\n+              TokenType.TEXT, TokenType.INTERPOLATION, () => this._isTextEnd(),\n+              () => this._isTagStart());\n         }\n       } catch (e) {\n         this.handleError(e);\n@@ -285,7 +291,7 @@ class _Tokenizer {\n     }\n     const token = new Token(\n         this._currentTokenType, parts,\n-        this._cursor.getSpan(this._currentTokenStart, this._leadingTriviaCodePoints));\n+        (end ?? this._cursor).getSpan(this._currentTokenStart, this._leadingTriviaCodePoints));\n     this.tokens.push(token);\n     this._currentTokenStart = null;\n     this._currentTokenType = null;\n@@ -594,29 +600,29 @@ class _Tokenizer {\n   private _consumeAttributeValue() {\n     let value: string;\n     if (this._cursor.peek() === chars.$SQ || this._cursor.peek() === chars.$DQ) {\n-      this._beginToken(TokenType.ATTR_QUOTE);\n       const quoteChar = this._cursor.peek();\n-      this._cursor.advance();\n-      this._endToken([String.fromCodePoint(quoteChar)]);\n-      this._beginToken(TokenType.ATTR_VALUE);\n-      const parts: string[] = [];\n-      while (this._cursor.peek() !== quoteChar) {\n-        parts.push(this._readChar(true));\n-      }\n-      value = parts.join('');\n-      this._endToken([this._processCarriageReturns(value)]);\n-      this._beginToken(TokenType.ATTR_QUOTE);\n-      this._cursor.advance();\n-      this._endToken([String.fromCodePoint(quoteChar)]);\n+      this._consumeQuote(quoteChar);\n+      // In an attribute then end of the attribute value and the premature end to an interpolation\n+      // are both triggered by the `quoteChar`.\n+      const endPredicate = () => this._cursor.peek() === quoteChar;\n+      this._consumeWithInterpolation(\n+          TokenType.ATTR_VALUE_TEXT, TokenType.ATTR_VALUE_INTERPOLATION, endPredicate,\n+          endPredicate);\n+      this._consumeQuote(quoteChar);\n     } else {\n-      this._beginToken(TokenType.ATTR_VALUE);\n-      const valueStart = this._cursor.clone();\n-      this._requireCharCodeUntilFn(isNameEnd, 1);\n-      value = this._cursor.getChars(valueStart);\n-      this._endToken([this._processCarriageReturns(value)]);\n+      const endPredicate = () => isNameEnd(this._cursor.peek());\n+      this._consumeWithInterpolation(\n+          TokenType.ATTR_VALUE_TEXT, TokenType.ATTR_VALUE_INTERPOLATION, endPredicate,\n+          endPredicate);\n     }\n   }\n \n+  private _consumeQuote(quoteChar: number) {\n+    this._beginToken(TokenType.ATTR_QUOTE);\n+    this._requireCharCode(quoteChar);\n+    this._endToken([String.fromCodePoint(quoteChar)]);\n+  }\n+\n   private _consumeTagOpenEnd() {\n     const tokenType =\n         this._attemptCharCode(chars.$SLASH) ? TokenType.TAG_OPEN_END_VOID : TokenType.TAG_OPEN_END;\n@@ -695,24 +701,37 @@ class _Tokenizer {\n     this._expansionCaseStack.pop();\n   }\n \n-  private _consumeText() {\n-    const start = this._cursor.clone();\n-    this._beginToken(TokenType.TEXT, start);\n+  /**\n+   * Consume a string that may contain interpolation expressions.\n+   *\n+   * The first token consumed will be of `tokenType` and then there will be alternating\n+   * `interpolationTokenType` and `tokenType` tokens until the `endPredicate()` returns true.\n+   *\n+   * If an interpolation token ends prematurely it will have no end marker in its `parts` array.\n+   *\n+   * @param textTokenType the kind of tokens to interleave around interpolation tokens.\n+   * @param interpolationTokenType the kind of tokens that contain interpolation.\n+   * @param endPredicate a function that should return true when we should stop consuming.\n+   * @param endInterpolation a function that should return true if there is a premature end to an\n+   *     interpolation expression - i.e. before we get to the normal interpolation closing marker.\n+   */\n+  private _consumeWithInterpolation(\n+      textTokenType: TokenType, interpolationTokenType: TokenType, endPredicate: () => boolean,\n+      endInterpolation: () => boolean) {\n+    this._beginToken(textTokenType);\n     const parts: string[] = [];\n \n-    do {\n+    while (!endPredicate()) {\n+      const current = this._cursor.clone();\n       if (this._interpolationConfig && this._attemptStr(this._interpolationConfig.start)) {\n-        parts.push(this._interpolationConfig.start);\n-        this._inInterpolation = true;\n-      } else if (\n-          this._interpolationConfig && this._inInterpolation &&\n-          this._attemptStr(this._interpolationConfig.end)) {\n-        parts.push(this._interpolationConfig.end);\n-        this._inInterpolation = false;\n+        this._endToken([this._processCarriageReturns(parts.join(''))], current);\n+        this._consumeInterpolation(interpolationTokenType, current, endInterpolation);\n+        parts.length = 0;\n+        this._beginToken(textTokenType);\n       } else {\n         parts.push(this._readChar(true));\n       }\n-    } while (!this._isTextEnd());\n+    }\n \n     // It is possible that an interpolation was started but not ended inside this text token.\n     // Make sure that we reset the state of the lexer correctly.\n@@ -721,6 +740,73 @@ class _Tokenizer {\n     this._endToken([this._processCarriageReturns(parts.join(''))]);\n   }\n \n+  /**\n+   * Consume a block of text that has been interpreted as an Angular interpolation.\n+   *\n+   * @param interpolationTokenType the type of the interpolation token to generate.\n+   * @param interpolationStart a cursor that points to the start of this interpolation.\n+   * @param prematureEndPredicate a function that should return true if the next characters indicate\n+   *     an end to the interpolation before its normal closing marker.\n+   */\n+  private _consumeInterpolation(\n+      interpolationTokenType: TokenType, interpolationStart: CharacterCursor,\n+      prematureEndPredicate: (() => boolean)|null) {\n+    const parts: string[] = [];\n+    this._beginToken(interpolationTokenType, interpolationStart);\n+    parts.push(this._interpolationConfig.start);\n+\n+    // Find the end of the interpolation, ignoring content inside quotes.\n+    const expressionStart = this._cursor.clone();\n+    let inQuote: number|null = null;\n+    let inComment = false;\n+    while (this._cursor.peek() !== chars.$EOF &&\n+           (prematureEndPredicate === null || !prematureEndPredicate())) {\n+      const current = this._cursor.clone();\n+\n+      if (this._isTagStart()) {\n+        // We are starting what looks like an HTML element in the middle of this interpolation.\n+        // Reset the cursor to before the `<` character and end the interpolation token.\n+        // (This is actually wrong but here for backward compatibility).\n+        this._cursor = current;\n+        parts.push(this._getProcessedChars(expressionStart, current));\n+        return this._endToken(parts);\n+      }\n+\n+      if (inQuote === null) {\n+        if (this._attemptStr(this._interpolationConfig.end)) {\n+          // We are not in a string, and we hit the end interpolation marker\n+          parts.push(this._getProcessedChars(expressionStart, current));\n+          parts.push(this._interpolationConfig.end);\n+          return this._endToken(parts);\n+        } else if (this._attemptStr('//')) {\n+          // Once we are in a comment we ignore any quotes\n+          inComment = true;\n+        }\n+      }\n+\n+      const char = this._cursor.peek();\n+      this._cursor.advance();\n+      if (char === chars.$BACKSLASH) {\n+        // Skip the next character because it was escaped.\n+        this._cursor.advance();\n+      } else if (char === inQuote) {\n+        // Exiting the current quoted string\n+        inQuote = null;\n+      } else if (!inComment && inQuote === null && chars.isQuote(char)) {\n+        // Entering a new quoted string\n+        inQuote = char;\n+      }\n+    }\n+\n+    // We hit EOF without finding a closing interpolation marker\n+    parts.push(this._getProcessedChars(expressionStart, this._cursor));\n+    return this._endToken(parts);\n+  }\n+\n+  private _getProcessedChars(start: CharacterCursor, end: CharacterCursor): string {\n+    return this._processCarriageReturns(end.getChars(start));\n+  }\n+\n   private _isTextEnd(): boolean {\n     if (this._isTagStart() || this._cursor.peek() === chars.$EOF) {\n       return true;"
        },
        {
            "sha": "6b2cd758ffd1480723d6fcd2c7500d6dd1a5b2e0",
            "filename": "packages/compiler/src/ml_parser/parser.ts",
            "status": "modified",
            "additions": 77,
            "deletions": 14,
            "changes": 91,
            "blob_url": "https://github.com/angular/angular/blob/8f565a2bcd86eb90625f14fc2862eb0d59fb7240/packages%2Fcompiler%2Fsrc%2Fml_parser%2Fparser.ts",
            "raw_url": "https://github.com/angular/angular/raw/8f565a2bcd86eb90625f14fc2862eb0d59fb7240/packages%2Fcompiler%2Fsrc%2Fml_parser%2Fparser.ts",
            "contents_url": "https://api.github.com/repos/angular/angular/contents/packages%2Fcompiler%2Fsrc%2Fml_parser%2Fparser.ts?ref=8f565a2bcd86eb90625f14fc2862eb0d59fb7240",
            "patch": "@@ -6,9 +6,10 @@\n  * found in the LICENSE file at https://angular.io/license\n  */\n \n-import {ParseError, ParseSourceSpan} from '../parse_util';\n+import {ParseError, ParseLocation, ParseSourceSpan} from '../parse_util';\n \n import * as html from './ast';\n+import {NAMED_ENTITIES} from './entities';\n import * as lex from './lexer';\n import {getNsPrefix, mergeNsAndName, splitNsName, TagDefinition} from './tags';\n \n@@ -215,6 +216,7 @@ class _TreeBuilder {\n   }\n \n   private _consumeText(token: lex.Token) {\n+    const startSpan = token.sourceSpan;\n     let text = token.parts[0];\n     if (text.length > 0 && text[0] === '\\n') {\n       const parent = this._getParentElement();\n@@ -224,8 +226,29 @@ class _TreeBuilder {\n       }\n     }\n \n+    // For now recombine text and interpolation tokens\n+    if (this._peek.type === lex.TokenType.INTERPOLATION) {\n+      while (this._peek.type === lex.TokenType.INTERPOLATION ||\n+             this._peek.type === lex.TokenType.TEXT) {\n+        token = this._advance();\n+        if (token.type === lex.TokenType.INTERPOLATION) {\n+          // For backward compatibility we decode HTML entities that appear in interpolation\n+          // expressions. This is arguably a bug, but it could be a considerable breaking change to\n+          // fix it. It should be addressed in a larger project to refactor the entire parser/lexer\n+          // chain after View Engine has been removed.\n+          text += token.parts.join('').replace(/&([^;]+);/g, decodeEntity);\n+        } else {\n+          text += token.parts.join('');\n+        }\n+      }\n+    }\n+\n     if (text.length > 0) {\n-      this._addToParent(new html.Text(text, token.sourceSpan));\n+      const endSpan = token.sourceSpan;\n+      this._addToParent(new html.Text(\n+          text,\n+          new ParseSourceSpan(\n+              startSpan.start, endSpan.end, startSpan.fullStart, startSpan.details)));\n     }\n   }\n \n@@ -339,27 +362,49 @@ class _TreeBuilder {\n \n   private _consumeAttr(attrName: lex.Token): html.Attribute {\n     const fullName = mergeNsAndName(attrName.parts[0], attrName.parts[1]);\n-    let end = attrName.sourceSpan.end;\n-    let value = '';\n-    let valueSpan: ParseSourceSpan = undefined!;\n+    let attrEnd = attrName.sourceSpan.end;\n+\n+    // Consume any quote\n     if (this._peek.type === lex.TokenType.ATTR_QUOTE) {\n       this._advance();\n     }\n-    if (this._peek.type === lex.TokenType.ATTR_VALUE) {\n-      const valueToken = this._advance();\n-      value = valueToken.parts[0];\n-      end = valueToken.sourceSpan.end;\n-      valueSpan = valueToken.sourceSpan;\n+\n+    // Consume the value\n+    let value = '';\n+    let valueStartSpan: ParseSourceSpan|undefined = undefined;\n+    let valueEnd: ParseLocation|undefined = undefined;\n+    if (this._peek.type === lex.TokenType.ATTR_VALUE_TEXT) {\n+      valueStartSpan = this._peek.sourceSpan;\n+      valueEnd = this._peek.sourceSpan.end;\n+      // For now we are recombining text and interpolation tokens\n+      while (this._peek.type === lex.TokenType.ATTR_VALUE_TEXT ||\n+             this._peek.type === lex.TokenType.ATTR_VALUE_INTERPOLATION) {\n+        let valueToken = this._advance();\n+        if (valueToken.type === lex.TokenType.ATTR_VALUE_INTERPOLATION) {\n+          // For backward compatibility we decode HTML entities that appear in interpolation\n+          // expressions. This is arguably a bug, but it could be a considerable breaking change to\n+          // fix it. It should be addressed in a larger project to refactor the entire parser/lexer\n+          // chain after View Engine has been removed.\n+          value += valueToken.parts.join('').replace(/&([^;]+);/g, decodeEntity);\n+        } else {\n+          value += valueToken.parts.join('');\n+        }\n+        valueEnd = attrEnd = valueToken.sourceSpan.end;\n+      }\n     }\n+\n+    // Consume any quote\n     if (this._peek.type === lex.TokenType.ATTR_QUOTE) {\n       const quoteToken = this._advance();\n-      end = quoteToken.sourceSpan.end;\n+      attrEnd = quoteToken.sourceSpan.end;\n     }\n-    const keySpan = new ParseSourceSpan(attrName.sourceSpan.start, attrName.sourceSpan.end);\n+\n+    const valueSpan = valueStartSpan && valueEnd &&\n+        new ParseSourceSpan(valueStartSpan.start, valueEnd, valueStartSpan.fullStart);\n     return new html.Attribute(\n         fullName, value,\n-        new ParseSourceSpan(attrName.sourceSpan.start, end, attrName.sourceSpan.fullStart), keySpan,\n-        valueSpan);\n+        new ParseSourceSpan(attrName.sourceSpan.start, attrEnd, attrName.sourceSpan.fullStart),\n+        attrName.sourceSpan, valueSpan);\n   }\n \n   private _getParentElement(): html.Element|null {\n@@ -395,3 +440,21 @@ class _TreeBuilder {\n function lastOnStack(stack: any[], element: any): boolean {\n   return stack.length > 0 && stack[stack.length - 1] === element;\n }\n+\n+/**\n+ * Decode the `entity` string, which we believe is the contents of an HTML entity.\n+ *\n+ * If the string is not actually a valid/known entity then just return the original `match` string.\n+ */\n+function decodeEntity(match: string, entity: string): string {\n+  if (NAMED_ENTITIES[entity] !== undefined) {\n+    return NAMED_ENTITIES[entity] || match;\n+  }\n+  if (/^#x[a-f0-9]+$/i.test(entity)) {\n+    return String.fromCodePoint(parseInt(entity.slice(2), 16));\n+  }\n+  if (/^#\\d+$/.test(entity)) {\n+    return String.fromCodePoint(parseInt(entity.slice(1), 10));\n+  }\n+  return match;\n+}"
        },
        {
            "sha": "a237b2927870dab339b8535f98731ed993f4d8df",
            "filename": "packages/compiler/test/ml_parser/html_parser_spec.ts",
            "status": "modified",
            "additions": 39,
            "deletions": 0,
            "changes": 39,
            "blob_url": "https://github.com/angular/angular/blob/8f565a2bcd86eb90625f14fc2862eb0d59fb7240/packages%2Fcompiler%2Ftest%2Fml_parser%2Fhtml_parser_spec.ts",
            "raw_url": "https://github.com/angular/angular/raw/8f565a2bcd86eb90625f14fc2862eb0d59fb7240/packages%2Fcompiler%2Ftest%2Fml_parser%2Fhtml_parser_spec.ts",
            "contents_url": "https://api.github.com/repos/angular/angular/contents/packages%2Fcompiler%2Ftest%2Fml_parser%2Fhtml_parser_spec.ts?ref=8f565a2bcd86eb90625f14fc2862eb0d59fb7240",
            "patch": "@@ -290,6 +290,19 @@ import {humanizeDom, humanizeDomSourceSpans, humanizeLineColumn, humanizeNodes}\n               ]);\n         });\n \n+        it('should decode HTML entities in interpolated attributes', () => {\n+          // Note that the detail of decoding corner-cases is tested in the\n+          // \"should decode HTML entities in interpolations\" spec.\n+          expect(humanizeDomSourceSpans(parser.parse('<div foo=\"{{&amp;}}\"></div>', 'TestComp')))\n+              .toEqual([\n+                [\n+                  html.Element, 'div', 0, '<div foo=\"{{&amp;}}\"></div>', '<div foo=\"{{&amp;}}\">',\n+                  '</div>'\n+                ],\n+                [html.Attribute, 'foo', '{{&}}', 'foo=\"{{&amp;}}\"']\n+              ]);\n+        });\n+\n         it('should normalize line endings within attribute values', () => {\n           const result =\n               parser.parse('<div key=\"  \\r\\n line 1 \\r\\n   line 2  \"></div>', 'TestComp');\n@@ -725,6 +738,32 @@ import {humanizeDom, humanizeDomSourceSpans, humanizeLineColumn, humanizeNodes}\n           expect(node.endSourceSpan!.end.offset).toEqual(12);\n         });\n \n+        // This checks backward compatibility with a previous version of the lexer, which would\n+        // treat interpolation expressions as regular HTML escapable text.\n+        it('should decode HTML entities in interpolations', () => {\n+          expect(humanizeDomSourceSpans(parser.parse(\n+                     '{{&amp;}}' +\n+                         '{{&#x25BE;}}' +\n+                         '{{&#9662;}}' +\n+                         '{{&amp (no semi-colon)}}' +\n+                         '{{&#25BE; (invalid decimal)}}',\n+                     'TestComp')))\n+              .toEqual([[\n+                html.Text,\n+                '{{&}}' +\n+                    '{{\\u25BE}}' +\n+                    '{{\\u25BE}}' +\n+                    '{{&amp (no semi-colon)}}' +\n+                    '{{&#25BE; (invalid decimal)}}',\n+                0,\n+                '{{&amp;}}' +\n+                    '{{&#x25BE;}}' +\n+                    '{{&#9662;}}' +\n+                    '{{&amp (no semi-colon)}}' +\n+                    '{{&#25BE; (invalid decimal)}}',\n+              ]]);\n+        });\n+\n         it('should not set the end source span for void elements', () => {\n           expect(humanizeDomSourceSpans(parser.parse('<div><br></div>', 'TestComp'))).toEqual([\n             [html.Element, 'div', 0, '<div><br></div>', '<div>', '</div>'],"
        },
        {
            "sha": "52bdc76b43dd9eda27bd03de3fc5b42885099946",
            "filename": "packages/compiler/test/ml_parser/lexer_spec.ts",
            "status": "modified",
            "additions": 155,
            "deletions": 43,
            "changes": 198,
            "blob_url": "https://github.com/angular/angular/blob/8f565a2bcd86eb90625f14fc2862eb0d59fb7240/packages%2Fcompiler%2Ftest%2Fml_parser%2Flexer_spec.ts",
            "raw_url": "https://github.com/angular/angular/raw/8f565a2bcd86eb90625f14fc2862eb0d59fb7240/packages%2Fcompiler%2Ftest%2Fml_parser%2Flexer_spec.ts",
            "contents_url": "https://api.github.com/repos/angular/angular/contents/packages%2Fcompiler%2Ftest%2Fml_parser%2Flexer_spec.ts?ref=8f565a2bcd86eb90625f14fc2862eb0d59fb7240",
            "patch": "@@ -257,7 +257,7 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n             [lex.TokenType.INCOMPLETE_TAG_OPEN, '<div'],\n             [lex.TokenType.ATTR_NAME, 'class'],\n             [lex.TokenType.ATTR_QUOTE, '\"'],\n-            [lex.TokenType.ATTR_VALUE, 'hi'],\n+            [lex.TokenType.ATTR_VALUE_TEXT, 'hi'],\n             [lex.TokenType.ATTR_QUOTE, '\"'],\n             [lex.TokenType.ATTR_NAME, 'sty'],\n             [lex.TokenType.TAG_OPEN_START, '<span'],\n@@ -295,15 +295,21 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n           [lex.TokenType.TAG_OPEN_START, '', 't'],\n           [lex.TokenType.ATTR_NAME, '', 'a'],\n           [lex.TokenType.ATTR_QUOTE, '\"'],\n-          [lex.TokenType.ATTR_VALUE, '{{v}}'],\n+          [lex.TokenType.ATTR_VALUE_TEXT, ''],\n+          [lex.TokenType.ATTR_VALUE_INTERPOLATION, '{{', 'v', '}}'],\n+          [lex.TokenType.ATTR_VALUE_TEXT, ''],\n           [lex.TokenType.ATTR_QUOTE, '\"'],\n           [lex.TokenType.ATTR_NAME, '', 'b'],\n           [lex.TokenType.ATTR_QUOTE, '\"'],\n-          [lex.TokenType.ATTR_VALUE, 's{{m}}e'],\n+          [lex.TokenType.ATTR_VALUE_TEXT, 's'],\n+          [lex.TokenType.ATTR_VALUE_INTERPOLATION, '{{', 'm', '}}'],\n+          [lex.TokenType.ATTR_VALUE_TEXT, 'e'],\n           [lex.TokenType.ATTR_QUOTE, '\"'],\n           [lex.TokenType.ATTR_NAME, '', 'c'],\n           [lex.TokenType.ATTR_QUOTE, '\"'],\n-          [lex.TokenType.ATTR_VALUE, 's{{m//c}}e'],\n+          [lex.TokenType.ATTR_VALUE_TEXT, 's'],\n+          [lex.TokenType.ATTR_VALUE_INTERPOLATION, '{{', 'm//c', '}}'],\n+          [lex.TokenType.ATTR_VALUE_TEXT, 'e'],\n           [lex.TokenType.ATTR_QUOTE, '\"'],\n           [lex.TokenType.TAG_OPEN_END],\n           [lex.TokenType.EOF],\n@@ -333,7 +339,7 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n           [lex.TokenType.TAG_OPEN_START, '', 't'],\n           [lex.TokenType.ATTR_NAME, '', 'a'],\n           [lex.TokenType.ATTR_QUOTE, '\\''],\n-          [lex.TokenType.ATTR_VALUE, 'b'],\n+          [lex.TokenType.ATTR_VALUE_TEXT, 'b'],\n           [lex.TokenType.ATTR_QUOTE, '\\''],\n           [lex.TokenType.TAG_OPEN_END],\n           [lex.TokenType.EOF],\n@@ -345,7 +351,7 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n           [lex.TokenType.TAG_OPEN_START, '', 't'],\n           [lex.TokenType.ATTR_NAME, '', 'a'],\n           [lex.TokenType.ATTR_QUOTE, '\"'],\n-          [lex.TokenType.ATTR_VALUE, 'b'],\n+          [lex.TokenType.ATTR_VALUE_TEXT, 'b'],\n           [lex.TokenType.ATTR_QUOTE, '\"'],\n           [lex.TokenType.TAG_OPEN_END],\n           [lex.TokenType.EOF],\n@@ -356,7 +362,31 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n         expect(tokenizeAndHumanizeParts('<t a=b>')).toEqual([\n           [lex.TokenType.TAG_OPEN_START, '', 't'],\n           [lex.TokenType.ATTR_NAME, '', 'a'],\n-          [lex.TokenType.ATTR_VALUE, 'b'],\n+          [lex.TokenType.ATTR_VALUE_TEXT, 'b'],\n+          [lex.TokenType.TAG_OPEN_END],\n+          [lex.TokenType.EOF],\n+        ]);\n+      });\n+\n+      it('should parse attributes with unquoted interpolation value', () => {\n+        expect(tokenizeAndHumanizeParts('<a a={{link.text}}>')).toEqual([\n+          [lex.TokenType.TAG_OPEN_START, '', 'a'],\n+          [lex.TokenType.ATTR_NAME, '', 'a'],\n+          [lex.TokenType.ATTR_VALUE_TEXT, ''],\n+          [lex.TokenType.ATTR_VALUE_INTERPOLATION, '{{', 'link.text', '}}'],\n+          [lex.TokenType.ATTR_VALUE_TEXT, ''],\n+          [lex.TokenType.TAG_OPEN_END],\n+          [lex.TokenType.EOF],\n+        ]);\n+      });\n+\n+      it('should parse attributes with empty quoted value', () => {\n+        expect(tokenizeAndHumanizeParts('<t a=\"\">')).toEqual([\n+          [lex.TokenType.TAG_OPEN_START, '', 't'],\n+          [lex.TokenType.ATTR_NAME, '', 'a'],\n+          [lex.TokenType.ATTR_QUOTE, '\"'],\n+          [lex.TokenType.ATTR_VALUE_TEXT, ''],\n+          [lex.TokenType.ATTR_QUOTE, '\"'],\n           [lex.TokenType.TAG_OPEN_END],\n           [lex.TokenType.EOF],\n         ]);\n@@ -372,7 +402,7 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n               [lex.TokenType.ATTR_NAME, '', '[attr]'],\n               [lex.TokenType.ATTR_QUOTE, '\"'],\n               [\n-                lex.TokenType.ATTR_VALUE,\n+                lex.TokenType.ATTR_VALUE_TEXT,\n                 '[\\n' +\n                     '        {text: \\'some text\\',url:\\'//www.google.com\\'},\\n' +\n                     '        {text:\\'other text\\',url:\\'//www.google.com\\'}]'\n@@ -387,7 +417,7 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n         expect(tokenizeAndHumanizeParts('<t a = b >')).toEqual([\n           [lex.TokenType.TAG_OPEN_START, '', 't'],\n           [lex.TokenType.ATTR_NAME, '', 'a'],\n-          [lex.TokenType.ATTR_VALUE, 'b'],\n+          [lex.TokenType.ATTR_VALUE_TEXT, 'b'],\n           [lex.TokenType.TAG_OPEN_END],\n           [lex.TokenType.EOF],\n         ]);\n@@ -398,7 +428,7 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n           [lex.TokenType.TAG_OPEN_START, '', 't'],\n           [lex.TokenType.ATTR_NAME, '', 'a'],\n           [lex.TokenType.ATTR_QUOTE, '\"'],\n-          [lex.TokenType.ATTR_VALUE, 'AA'],\n+          [lex.TokenType.ATTR_VALUE_TEXT, 'AA'],\n           [lex.TokenType.ATTR_QUOTE, '\"'],\n           [lex.TokenType.TAG_OPEN_END],\n           [lex.TokenType.EOF],\n@@ -410,11 +440,11 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n           [lex.TokenType.TAG_OPEN_START, '', 't'],\n           [lex.TokenType.ATTR_NAME, '', 'a'],\n           [lex.TokenType.ATTR_QUOTE, '\"'],\n-          [lex.TokenType.ATTR_VALUE, '&amp'],\n+          [lex.TokenType.ATTR_VALUE_TEXT, '&amp'],\n           [lex.TokenType.ATTR_QUOTE, '\"'],\n           [lex.TokenType.ATTR_NAME, '', 'b'],\n           [lex.TokenType.ATTR_QUOTE, '\"'],\n-          [lex.TokenType.ATTR_VALUE, 'c&&d'],\n+          [lex.TokenType.ATTR_VALUE_TEXT, 'c&&d'],\n           [lex.TokenType.ATTR_QUOTE, '\"'],\n           [lex.TokenType.TAG_OPEN_END],\n           [lex.TokenType.EOF],\n@@ -426,7 +456,7 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n           [lex.TokenType.TAG_OPEN_START, '', 't'],\n           [lex.TokenType.ATTR_NAME, '', 'a'],\n           [lex.TokenType.ATTR_QUOTE, '\"'],\n-          [lex.TokenType.ATTR_VALUE, 'b && c &'],\n+          [lex.TokenType.ATTR_VALUE_TEXT, 'b && c &'],\n           [lex.TokenType.ATTR_QUOTE, '\"'],\n           [lex.TokenType.TAG_OPEN_END],\n           [lex.TokenType.EOF],\n@@ -438,7 +468,7 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n           [lex.TokenType.TAG_OPEN_START, '', 't'],\n           [lex.TokenType.ATTR_NAME, '', 'a'],\n           [lex.TokenType.ATTR_QUOTE, '\\''],\n-          [lex.TokenType.ATTR_VALUE, 't\\ne\\ns\\nt'],\n+          [lex.TokenType.ATTR_VALUE_TEXT, 't\\ne\\ns\\nt'],\n           [lex.TokenType.ATTR_QUOTE, '\\''],\n           [lex.TokenType.TAG_OPEN_END],\n           [lex.TokenType.EOF],\n@@ -449,21 +479,21 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n         expect(tokenizeAndHumanizeSourceSpans('<t a=b>')).toEqual([\n           [lex.TokenType.TAG_OPEN_START, '<t'],\n           [lex.TokenType.ATTR_NAME, 'a'],\n-          [lex.TokenType.ATTR_VALUE, 'b'],\n+          [lex.TokenType.ATTR_VALUE_TEXT, 'b'],\n           [lex.TokenType.TAG_OPEN_END, '>'],\n           [lex.TokenType.EOF, ''],\n         ]);\n       });\n \n       it('should report missing closing single quote', () => {\n         expect(tokenizeAndHumanizeErrors('<t a=\\'b>')).toEqual([\n-          [lex.TokenType.ATTR_VALUE, 'Unexpected character \"EOF\"', '0:8'],\n+          [lex.TokenType.ATTR_VALUE_TEXT, 'Unexpected character \"EOF\"', '0:8'],\n         ]);\n       });\n \n       it('should report missing closing double quote', () => {\n         expect(tokenizeAndHumanizeErrors('<t a=\"b>')).toEqual([\n-          [lex.TokenType.ATTR_VALUE, 'Unexpected character \"EOF\"', '0:8'],\n+          [lex.TokenType.ATTR_VALUE_TEXT, 'Unexpected character \"EOF\"', '0:8'],\n         ]);\n       });\n     });\n@@ -570,32 +600,78 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n       });\n \n       it('should parse interpolation', () => {\n-        expect(tokenizeAndHumanizeParts('{{ a }}b{{ c // comment }}')).toEqual([\n-          [lex.TokenType.TEXT, '{{ a }}b{{ c // comment }}'],\n-          [lex.TokenType.EOF],\n+        expect(tokenizeAndHumanizeParts('{{ a }}b{{ c // comment }}d{{ e \"}}\" f }}g{{ h // \" i }}'))\n+            .toEqual([\n+              [lex.TokenType.TEXT, ''],\n+              [lex.TokenType.INTERPOLATION, '{{', ' a ', '}}'],\n+              [lex.TokenType.TEXT, 'b'],\n+              [lex.TokenType.INTERPOLATION, '{{', ' c // comment ', '}}'],\n+              [lex.TokenType.TEXT, 'd'],\n+              [lex.TokenType.INTERPOLATION, '{{', ' e \"}}\" f ', '}}'],\n+              [lex.TokenType.TEXT, 'g'],\n+              [lex.TokenType.INTERPOLATION, '{{', ' h // \" i ', '}}'],\n+              [lex.TokenType.TEXT, ''],\n+              [lex.TokenType.EOF],\n+            ]);\n+\n+        expect(tokenizeAndHumanizeSourceSpans('{{ a }}b{{ c // comment }}')).toEqual([\n+          [lex.TokenType.TEXT, ''],\n+          [lex.TokenType.INTERPOLATION, '{{ a }}'],\n+          [lex.TokenType.TEXT, 'b'],\n+          [lex.TokenType.INTERPOLATION, '{{ c // comment }}'],\n+          [lex.TokenType.TEXT, ''],\n+          [lex.TokenType.EOF, ''],\n         ]);\n       });\n \n       it('should parse interpolation with custom markers', () => {\n         expect(tokenizeAndHumanizeParts('{% a %}', {interpolationConfig: {start: '{%', end: '%}'}}))\n             .toEqual([\n-              [lex.TokenType.TEXT, '{% a %}'],\n+              [lex.TokenType.TEXT, ''],\n+              [lex.TokenType.INTERPOLATION, '{%', ' a ', '%}'],\n+              [lex.TokenType.TEXT, ''],\n               [lex.TokenType.EOF],\n             ]);\n       });\n \n-      it('should handle CR & LF', () => {\n+      it('should handle CR & LF in text', () => {\n         expect(tokenizeAndHumanizeParts('t\\ne\\rs\\r\\nt')).toEqual([\n           [lex.TokenType.TEXT, 't\\ne\\ns\\nt'],\n           [lex.TokenType.EOF],\n         ]);\n+\n+        expect(tokenizeAndHumanizeSourceSpans('t\\ne\\rs\\r\\nt')).toEqual([\n+          [lex.TokenType.TEXT, 't\\ne\\rs\\r\\nt'],\n+          [lex.TokenType.EOF, ''],\n+        ]);\n+      });\n+\n+      it('should handle CR & LF in interpolation', () => {\n+        expect(tokenizeAndHumanizeParts('{{t\\ne\\rs\\r\\nt}}')).toEqual([\n+          [lex.TokenType.TEXT, ''],\n+          [lex.TokenType.INTERPOLATION, '{{', 't\\ne\\ns\\nt', '}}'],\n+          [lex.TokenType.TEXT, ''],\n+          [lex.TokenType.EOF],\n+        ]);\n+\n+        expect(tokenizeAndHumanizeSourceSpans('{{t\\ne\\rs\\r\\nt}}')).toEqual([\n+          [lex.TokenType.TEXT, ''],\n+          [lex.TokenType.INTERPOLATION, '{{t\\ne\\rs\\r\\nt}}'],\n+          [lex.TokenType.TEXT, ''],\n+          [lex.TokenType.EOF, ''],\n+        ]);\n       });\n \n       it('should parse entities', () => {\n         expect(tokenizeAndHumanizeParts('a&amp;b')).toEqual([\n           [lex.TokenType.TEXT, 'a&b'],\n           [lex.TokenType.EOF],\n         ]);\n+\n+        expect(tokenizeAndHumanizeSourceSpans('a&amp;b')).toEqual([\n+          [lex.TokenType.TEXT, 'a&amp;b'],\n+          [lex.TokenType.EOF, ''],\n+        ]);\n       });\n \n       it('should parse text starting with \"&\"', () => {\n@@ -614,7 +690,9 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n \n       it('should allow \"<\" in text nodes', () => {\n         expect(tokenizeAndHumanizeParts('{{ a < b ? c : d }}')).toEqual([\n-          [lex.TokenType.TEXT, '{{ a < b ? c : d }}'],\n+          [lex.TokenType.TEXT, ''],\n+          [lex.TokenType.INTERPOLATION, '{{', ' a < b ? c : d ', '}}'],\n+          [lex.TokenType.TEXT, ''],\n           [lex.TokenType.EOF],\n         ]);\n \n@@ -635,7 +713,9 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n \n       it('should break out of interpolation in text token on valid start tag', () => {\n         expect(tokenizeAndHumanizeParts('{{ a <b && c > d }}')).toEqual([\n-          [lex.TokenType.TEXT, '{{ a '],\n+          [lex.TokenType.TEXT, ''],\n+          [lex.TokenType.INTERPOLATION, '{{', ' a '],\n+          [lex.TokenType.TEXT, ''],\n           [lex.TokenType.TAG_OPEN_START, '', 'b'],\n           [lex.TokenType.ATTR_NAME, '', '&&'],\n           [lex.TokenType.ATTR_NAME, '', 'c'],\n@@ -647,7 +727,9 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n \n       it('should break out of interpolation in text token on valid comment', () => {\n         expect(tokenizeAndHumanizeParts('{{ a }<!---->}')).toEqual([\n-          [lex.TokenType.TEXT, '{{ a }'],\n+          [lex.TokenType.TEXT, ''],\n+          [lex.TokenType.INTERPOLATION, '{{', ' a }'],\n+          [lex.TokenType.TEXT, ''],\n           [lex.TokenType.COMMENT_START],\n           [lex.TokenType.RAW_TEXT, ''],\n           [lex.TokenType.COMMENT_END],\n@@ -658,7 +740,9 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n \n       it('should break out of interpolation in text token on valid CDATA', () => {\n         expect(tokenizeAndHumanizeParts('{{ a }<![CDATA[]]>}')).toEqual([\n-          [lex.TokenType.TEXT, '{{ a }'],\n+          [lex.TokenType.TEXT, ''],\n+          [lex.TokenType.INTERPOLATION, '{{', ' a }'],\n+          [lex.TokenType.TEXT, ''],\n           [lex.TokenType.CDATA_START],\n           [lex.TokenType.RAW_TEXT, ''],\n           [lex.TokenType.CDATA_END],\n@@ -674,13 +758,14 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n             .toEqual([\n               [lex.TokenType.TAG_OPEN_START, '', 'code'],\n               [lex.TokenType.TAG_OPEN_END],\n-              [lex.TokenType.TEXT, '{{\\'<={\\'}}'],\n+              [lex.TokenType.TEXT, ''],\n+              [lex.TokenType.INTERPOLATION, '{{', '\\'<={\\'', '}}'],\n+              [lex.TokenType.TEXT, ''],\n               [lex.TokenType.TAG_CLOSE, '', 'code'],\n               [lex.TokenType.EOF],\n             ]);\n       });\n \n-\n       it('should parse start tags quotes in place of an attribute name as text', () => {\n         expect(tokenizeAndHumanizeParts('<t \">')).toEqual([\n           [lex.TokenType.INCOMPLETE_TAG_OPEN, '', 't'],\n@@ -701,7 +786,7 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n              [lex.TokenType.INCOMPLETE_TAG_OPEN, '', 't'],\n              [lex.TokenType.ATTR_NAME, '', 'a'],\n              [lex.TokenType.ATTR_QUOTE, '\"'],\n-             [lex.TokenType.ATTR_VALUE, 'b'],\n+             [lex.TokenType.ATTR_VALUE_TEXT, 'b'],\n              [lex.TokenType.ATTR_QUOTE, '\"'],\n              // TODO(ayazhafiz): the \" symbol should be a synthetic attribute,\n              // allowing us to complete the opening tag correctly.\n@@ -713,7 +798,7 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n              [lex.TokenType.INCOMPLETE_TAG_OPEN, '', 't'],\n              [lex.TokenType.ATTR_NAME, '', 'a'],\n              [lex.TokenType.ATTR_QUOTE, '\\''],\n-             [lex.TokenType.ATTR_VALUE, 'b'],\n+             [lex.TokenType.ATTR_VALUE_TEXT, 'b'],\n              [lex.TokenType.ATTR_QUOTE, '\\''],\n              // TODO(ayazhafiz): the ' symbol should be a synthetic attribute,\n              // allowing us to complete the opening tag correctly.\n@@ -724,18 +809,32 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n \n       it('should be able to escape {', () => {\n         expect(tokenizeAndHumanizeParts('{{ \"{\" }}')).toEqual([\n-          [lex.TokenType.TEXT, '{{ \"{\" }}'],\n+          [lex.TokenType.TEXT, ''],\n+          [lex.TokenType.INTERPOLATION, '{{', ' \"{\" ', '}}'],\n+          [lex.TokenType.TEXT, ''],\n           [lex.TokenType.EOF],\n         ]);\n       });\n \n       it('should be able to escape {{', () => {\n         expect(tokenizeAndHumanizeParts('{{ \"{{\" }}')).toEqual([\n-          [lex.TokenType.TEXT, '{{ \"{{\" }}'],\n+          [lex.TokenType.TEXT, ''],\n+          [lex.TokenType.INTERPOLATION, '{{', ' \"{{\" ', '}}'],\n+          [lex.TokenType.TEXT, ''],\n           [lex.TokenType.EOF],\n         ]);\n       });\n \n+      it('should capture everything up to the end of file in the interpolation expression part if there are mismatched quotes',\n+         () => {\n+           expect(tokenizeAndHumanizeParts('{{ \"{{a}}\\' }}')).toEqual([\n+             [lex.TokenType.TEXT, ''],\n+             [lex.TokenType.INTERPOLATION, '{{', ' \"{{a}}\\' }}'],\n+             [lex.TokenType.TEXT, ''],\n+             [lex.TokenType.EOF],\n+           ]);\n+         });\n+\n       it('should treat expansion form as text when they are not parsed', () => {\n         expect(tokenizeAndHumanizeParts(\n                    '<span>{a, b, =4 {c}}</span>', {tokenizeExpansionForms: false}))\n@@ -997,7 +1096,9 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n               [lex.TokenType.RAW_TEXT, 'three'],\n               [lex.TokenType.EXPANSION_CASE_VALUE, '=4'],\n               [lex.TokenType.EXPANSION_CASE_EXP_START],\n-              [lex.TokenType.TEXT, 'four {{a}}'],\n+              [lex.TokenType.TEXT, 'four '],\n+              [lex.TokenType.INTERPOLATION, '{{', 'a', '}}'],\n+              [lex.TokenType.TEXT, ''],\n               [lex.TokenType.EXPANSION_CASE_EXP_END],\n               [lex.TokenType.EXPANSION_FORM_END],\n               [lex.TokenType.EOF],\n@@ -1054,7 +1155,9 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n                  [lex.TokenType.EXPANSION_CASE_EXP_END],\n                  [lex.TokenType.EXPANSION_CASE_VALUE, '=1'],\n                  [lex.TokenType.EXPANSION_CASE_EXP_START],\n-                 [lex.TokenType.TEXT, 'One {{message}}'],\n+                 [lex.TokenType.TEXT, 'One '],\n+                 [lex.TokenType.INTERPOLATION, '{{', 'message', '}}'],\n+                 [lex.TokenType.TEXT, ''],\n                  [lex.TokenType.EXPANSION_CASE_EXP_END],\n                  [lex.TokenType.EXPANSION_FORM_END],\n                  [lex.TokenType.TEXT, '\\n'],\n@@ -1084,7 +1187,9 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n                  [lex.TokenType.EXPANSION_CASE_EXP_END],\n                  [lex.TokenType.EXPANSION_CASE_VALUE, '=1'],\n                  [lex.TokenType.EXPANSION_CASE_EXP_START],\n-                 [lex.TokenType.TEXT, 'One {{message}}'],\n+                 [lex.TokenType.TEXT, 'One '],\n+                 [lex.TokenType.INTERPOLATION, '{{', 'message', '}}'],\n+                 [lex.TokenType.TEXT, ''],\n                  [lex.TokenType.EXPANSION_CASE_EXP_END],\n                  [lex.TokenType.EXPANSION_FORM_END],\n                  [lex.TokenType.TEXT, '\\n'],\n@@ -1165,7 +1270,9 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n                  [lex.TokenType.EXPANSION_CASE_EXP_END],\n                  [lex.TokenType.EXPANSION_CASE_VALUE, '=1'],\n                  [lex.TokenType.EXPANSION_CASE_EXP_START],\n-                 [lex.TokenType.TEXT, 'One {{message}}'],\n+                 [lex.TokenType.TEXT, 'One '],\n+                 [lex.TokenType.INTERPOLATION, '{{', 'message', '}}'],\n+                 [lex.TokenType.TEXT, ''],\n                  [lex.TokenType.EXPANSION_CASE_EXP_END],\n                  [lex.TokenType.EXPANSION_FORM_END],\n                  [lex.TokenType.TEXT, '\\n'],\n@@ -1195,7 +1302,9 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n                  [lex.TokenType.EXPANSION_CASE_EXP_END],\n                  [lex.TokenType.EXPANSION_CASE_VALUE, '=1'],\n                  [lex.TokenType.EXPANSION_CASE_EXP_START],\n-                 [lex.TokenType.TEXT, 'One {{message}}'],\n+                 [lex.TokenType.TEXT, 'One '],\n+                 [lex.TokenType.INTERPOLATION, '{{', 'message', '}}'],\n+                 [lex.TokenType.TEXT, ''],\n                  [lex.TokenType.EXPANSION_CASE_EXP_END],\n                  [lex.TokenType.EXPANSION_FORM_END],\n                  [lex.TokenType.TEXT, '\\n'],\n@@ -1322,8 +1431,11 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n           [lex.TokenType.TEXT, '\\n \\n \\n'],\n           [lex.TokenType.EOF],\n         ]);\n-        expect(tokenizeAndHumanizeParts('\\\\r \\\\r \\\\r', {escapedString: true})).toEqual([\n-          [lex.TokenType.TEXT, '\\n \\n \\n'],  // post processing converts `\\r` to `\\n`\n+        expect(tokenizeAndHumanizeParts('\\\\r{{\\\\r}}\\\\r', {escapedString: true})).toEqual([\n+          // post processing converts `\\r` to `\\n`\n+          [lex.TokenType.TEXT, '\\n'],\n+          [lex.TokenType.INTERPOLATION, '{{', '\\n', '}}'],\n+          [lex.TokenType.TEXT, '\\n'],\n           [lex.TokenType.EOF],\n         ]);\n         expect(tokenizeAndHumanizeParts('\\\\v \\\\v \\\\v', {escapedString: true})).toEqual([\n@@ -1477,11 +1589,11 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n               [lex.TokenType.TAG_OPEN_START, '', 't'],\n               [lex.TokenType.ATTR_NAME, '', 'a'],\n               [lex.TokenType.ATTR_QUOTE, '\"'],\n-              [lex.TokenType.ATTR_VALUE, 'b'],\n+              [lex.TokenType.ATTR_VALUE_TEXT, 'b'],\n               [lex.TokenType.ATTR_QUOTE, '\"'],\n               [lex.TokenType.ATTR_NAME, '', 'c'],\n               [lex.TokenType.ATTR_QUOTE, '\\''],\n-              [lex.TokenType.ATTR_VALUE, 'd'],\n+              [lex.TokenType.ATTR_VALUE_TEXT, 'd'],\n               [lex.TokenType.ATTR_QUOTE, '\\''],\n               [lex.TokenType.TAG_OPEN_END],\n               [lex.TokenType.EOF],\n@@ -1530,7 +1642,7 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n           [lex.TokenType.TAG_OPEN_START, '', 't'],\n           [lex.TokenType.ATTR_NAME, '', 'd'],\n           [lex.TokenType.ATTR_QUOTE, '\"'],\n-          [lex.TokenType.ATTR_VALUE, 'e'],\n+          [lex.TokenType.ATTR_VALUE_TEXT, 'e'],\n           [lex.TokenType.ATTR_QUOTE, '\"'],\n           [lex.TokenType.TAG_OPEN_END],\n           [lex.TokenType.TAG_CLOSE, '', 't'],\n@@ -1543,7 +1655,7 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n         expect(tokenizeAndHumanizeParts(text, {escapedString: true})).toEqual([\n           [lex.TokenType.TAG_OPEN_START, '', 't'],\n           [lex.TokenType.ATTR_NAME, '', 'a'],\n-          [lex.TokenType.ATTR_VALUE, 'b'],\n+          [lex.TokenType.ATTR_VALUE_TEXT, 'b'],\n           [lex.TokenType.TAG_OPEN_END],\n           [lex.TokenType.TAG_CLOSE, '', 't'],\n           [lex.TokenType.EOF],"
        }
    ],
    "stats": {
        "total": 480,
        "additions": 390,
        "deletions": 90
    }
}