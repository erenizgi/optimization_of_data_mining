{
    "author": "petebacondarwin",
    "message": "build(docs-infra): tidy up the generateKeywords processor (#41447)\n\nThe recent PR #41368 contained some changes that could be improved.\n\nPR Close #41447",
    "sha": "65cd44e731b2a6f01e72647264d33015481e82c8",
    "files": [
        {
            "sha": "037f52322e7e28163a49697dfcfffaf5d5863908",
            "filename": "aio/tools/transforms/angular-base-package/index.js",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/angular/angular/blob/65cd44e731b2a6f01e72647264d33015481e82c8/aio%2Ftools%2Ftransforms%2Fangular-base-package%2Findex.js",
            "raw_url": "https://github.com/angular/angular/raw/65cd44e731b2a6f01e72647264d33015481e82c8/aio%2Ftools%2Ftransforms%2Fangular-base-package%2Findex.js",
            "contents_url": "https://api.github.com/repos/angular/angular/contents/aio%2Ftools%2Ftransforms%2Fangular-base-package%2Findex.js?ref=65cd44e731b2a6f01e72647264d33015481e82c8",
            "patch": "@@ -66,7 +66,7 @@ module.exports = new Package('angular-base', [\n     collectExamples.exampleFolders = [];\n \n     generateKeywordsProcessor.ignoreWords = require(path.resolve(__dirname, 'ignore-words'))['en'];\n-    generateKeywordsProcessor.docTypesToIgnore = ['example-region'];\n+    generateKeywordsProcessor.docTypesToIgnore = [undefined, 'example-region', 'json-doc', 'api-list-data', 'api-list-data', 'contributors-json', 'navigation-json', 'announcements-json'];\n     generateKeywordsProcessor.propertiesToIgnore = ['basePath', 'renderedContent', 'docType', 'searchTitle'];\n   })\n "
        },
        {
            "sha": "d5141d88d11c3188efc0b5b3a37001e8de21c317",
            "filename": "aio/tools/transforms/angular-base-package/processors/generateKeywords.js",
            "status": "modified",
            "additions": 23,
            "deletions": 13,
            "changes": 36,
            "blob_url": "https://github.com/angular/angular/blob/65cd44e731b2a6f01e72647264d33015481e82c8/aio%2Ftools%2Ftransforms%2Fangular-base-package%2Fprocessors%2FgenerateKeywords.js",
            "raw_url": "https://github.com/angular/angular/raw/65cd44e731b2a6f01e72647264d33015481e82c8/aio%2Ftools%2Ftransforms%2Fangular-base-package%2Fprocessors%2FgenerateKeywords.js",
            "contents_url": "https://api.github.com/repos/angular/angular/contents/aio%2Ftools%2Ftransforms%2Fangular-base-package%2Fprocessors%2FgenerateKeywords.js?ref=65cd44e731b2a6f01e72647264d33015481e82c8",
            "patch": "@@ -43,10 +43,10 @@ module.exports = function generateKeywordsProcessor(log) {\n           .filter(doc => !doc.internal && !doc.privateExport);\n \n \n-      for(const doc of filteredDocs) {\n+      for (const doc of filteredDocs) {\n         // Search each top level property of the document for search terms\n         let mainTokens = [];\n-        for(const key of Object.keys(doc)) {\n+        for (const key of Object.keys(doc)) {\n           const value = doc[key];\n           if (isString(value) && !propertiesToIgnore.has(key)) {\n             mainTokens.push(...tokenize(value, ignoreWords, dictionary));\n@@ -58,8 +58,8 @@ module.exports = function generateKeywordsProcessor(log) {\n         // Extract all the keywords from the headings\n         let headingTokens = [];\n         if (doc.vFile && doc.vFile.headings) {\n-          for(const headingTag of Object.keys(doc.vFile.headings)) {\n-            for(const headingText of doc.vFile.headings[headingTag]) {\n+          for (const headingTag of Object.keys(doc.vFile.headings)) {\n+            for (const headingText of doc.vFile.headings[headingTag]) {\n               headingTokens.push(...tokenize(headingText, ignoreWords, dictionary));\n             }\n           }\n@@ -120,27 +120,37 @@ function isString(value) {\n \n function tokenize(text, ignoreWords, dictionary) {\n   // Split on whitespace and things that are likely to be HTML tags (this is not exhaustive but reduces the unwanted tokens that are indexed).\n-  const rawTokens = text.split(/[\\s/]+|<\\/?[a-z]+(?:\\s+\\w+(?:=\"[^\"]+\")?)*>/img);\n+  const rawTokens = text.split(new RegExp(\n+                                      '[\\\\s/]+' +                                // whitespace\n+                                      '|' +                                      // or\n+                                      '</?[a-z]+(?:\\\\s+\\\\w+(?:=\"[^\"]+\")?)*/?>',  // simple HTML tags (e.g. <td>, <hr/>, </table>, etc.)\n+                                      'ig'));\n   const tokens = [];\n-  for(let token of rawTokens) {\n+  for (let token of rawTokens) {\n     token = token.trim();\n \n-    // Strip off unwanted trivial characters\n-    token = token.replace(/^[_\\-\"'`({[<$*)}\\]>.]+/, '').replace(/[_\\-\"'`({[<$*)}\\]>.]+$/, '');\n+    // Trim unwanted trivia characters from the start and end of the token\n+    const TRIVIA_CHARS = '[\\\\s_\"\\'`({[<$*)}\\\\]>.,-]';\n+    // Tokens can contain letters, numbers, underscore, dot or hyphen but not at the start or end.\n+    // The leading TRIVIA_CHARS will capture any leading `.`, '-`' or `_` so we don't have to avoid them in this regular expression.\n+    // But we do need to ensure we don't capture the at the end of the token.\n+    const POSSIBLE_TOKEN = '[a-z0-9_.-]*[a-z0-9]';\n+    token = token.replace(new RegExp(`^${TRIVIA_CHARS}*(${POSSIBLE_TOKEN})${TRIVIA_CHARS}*$`, 'i'), '$1');\n \n-    // Skip if in the ignored words list\n-    if (ignoreWords.has(token.toLowerCase())) {\n+    // Skip if blank or in the ignored words list\n+    if (token === '' || ignoreWords.has(token.toLowerCase())) {\n       continue;\n     }\n \n     // Skip tokens that contain weird characters\n-    if (!/^[\\w._-]+$/.test(token)) {\n+    if (!/^\\w[\\w.-]*$/.test(token)) {\n       continue;\n     }\n \n     storeToken(token, tokens, dictionary);\n     if (token.startsWith('ng')) {\n-      storeToken(token.substr(2), tokens, dictionary);\n+      // Strip off `ng`, `ng-`, `ng1`, `ng2`, etc\n+      storeToken(token.replace(/^ng[-12]*/, ''), tokens, dictionary);\n     }\n   }\n \n@@ -156,7 +166,7 @@ function storeToken(token, tokens, dictionary) {\n }\n \n function extractMemberTokens(doc, ignoreWords, dictionary) {\n-  if (!doc) return '';\n+  if (!doc) return [];\n \n   let memberContent = [];\n "
        }
    ],
    "stats": {
        "total": 38,
        "additions": 24,
        "deletions": 14
    }
}