{
    "author": "atscott",
    "message": "Revert \"refactor(compiler): support encoded entity tokens when lexing markup (#42062)\" (#43033)\n\nThis reverts commit 942b24d5ea5d36ad4e53ed435bda35a6ae6876c9.\n\nPR Close #43033",
    "sha": "8d8ab4775c1ff197ad101cbd33690bc8620a3c56",
    "files": [
        {
            "sha": "d7306a23896e8db1aebf1bc209cecda2f1bdb58b",
            "filename": "packages/compiler/src/ml_parser/lexer.ts",
            "status": "modified",
            "additions": 28,
            "deletions": 44,
            "changes": 72,
            "blob_url": "https://github.com/angular/angular/blob/8d8ab4775c1ff197ad101cbd33690bc8620a3c56/packages%2Fcompiler%2Fsrc%2Fml_parser%2Flexer.ts",
            "raw_url": "https://github.com/angular/angular/raw/8d8ab4775c1ff197ad101cbd33690bc8620a3c56/packages%2Fcompiler%2Fsrc%2Fml_parser%2Flexer.ts",
            "contents_url": "https://api.github.com/repos/angular/angular/contents/packages%2Fcompiler%2Fsrc%2Fml_parser%2Flexer.ts?ref=8d8ab4775c1ff197ad101cbd33690bc8620a3c56",
            "patch": "@@ -23,7 +23,6 @@ export enum TokenType {\n   ESCAPABLE_RAW_TEXT,\n   RAW_TEXT,\n   INTERPOLATION,\n-  ENCODED_ENTITY,\n   COMMENT_START,\n   COMMENT_END,\n   CDATA_START,\n@@ -396,16 +395,19 @@ class _Tokenizer {\n     }\n   }\n \n-  private _readChar(): string {\n-    // Don't rely upon reading directly from `_input` as the actual char value\n-    // may have been generated from an escape sequence.\n-    const char = String.fromCodePoint(this._cursor.peek());\n-    this._cursor.advance();\n-    return char;\n+  private _readChar(decodeEntities: boolean): string {\n+    if (decodeEntities && this._cursor.peek() === chars.$AMPERSAND) {\n+      return this._decodeEntity();\n+    } else {\n+      // Don't rely upon reading directly from `_input` as the actual char value\n+      // may have been generated from an escape sequence.\n+      const char = String.fromCodePoint(this._cursor.peek());\n+      this._cursor.advance();\n+      return char;\n+    }\n   }\n \n-  private _consumeEntity(textTokenType: TokenType): void {\n-    this._beginToken(TokenType.ENCODED_ENTITY);\n+  private _decodeEntity(): string {\n     const start = this._cursor.clone();\n     this._cursor.advance();\n     if (this._attemptCharCode(chars.$HASH)) {\n@@ -425,7 +427,7 @@ class _Tokenizer {\n       this._cursor.advance();\n       try {\n         const charCode = parseInt(strNum, isHex ? 16 : 10);\n-        this._endToken([String.fromCharCode(charCode), this._cursor.getChars(start)]);\n+        return String.fromCharCode(charCode);\n       } catch {\n         throw this._createError(\n             _unknownEntityErrorMsg(this._cursor.getChars(start)), this._cursor.getSpan());\n@@ -434,25 +436,21 @@ class _Tokenizer {\n       const nameStart = this._cursor.clone();\n       this._attemptCharCodeUntilFn(isNamedEntityEnd);\n       if (this._cursor.peek() != chars.$SEMICOLON) {\n-        // No semicolon was found so abort the encoded entity token that was in progress, and treat\n-        // this as a text token\n-        this._beginToken(textTokenType, start);\n         this._cursor = nameStart;\n-        this._endToken(['&']);\n-      } else {\n-        const name = this._cursor.getChars(nameStart);\n-        this._cursor.advance();\n-        const char = NAMED_ENTITIES[name];\n-        if (!char) {\n-          throw this._createError(_unknownEntityErrorMsg(name), this._cursor.getSpan(start));\n-        }\n-        this._endToken([char, `&${name};`]);\n+        return '&';\n       }\n+      const name = this._cursor.getChars(nameStart);\n+      this._cursor.advance();\n+      const char = NAMED_ENTITIES[name];\n+      if (!char) {\n+        throw this._createError(_unknownEntityErrorMsg(name), this._cursor.getSpan(start));\n+      }\n+      return char;\n     }\n   }\n \n-  private _consumeRawText(consumeEntities: boolean, endMarkerPredicate: () => boolean): void {\n-    this._beginToken(consumeEntities ? TokenType.ESCAPABLE_RAW_TEXT : TokenType.RAW_TEXT);\n+  private _consumeRawText(decodeEntities: boolean, endMarkerPredicate: () => boolean): Token {\n+    this._beginToken(decodeEntities ? TokenType.ESCAPABLE_RAW_TEXT : TokenType.RAW_TEXT);\n     const parts: string[] = [];\n     while (true) {\n       const tagCloseStart = this._cursor.clone();\n@@ -461,16 +459,9 @@ class _Tokenizer {\n       if (foundEndMarker) {\n         break;\n       }\n-      if (consumeEntities && this._cursor.peek() === chars.$AMPERSAND) {\n-        this._endToken([this._processCarriageReturns(parts.join(''))]);\n-        parts.length = 0;\n-        this._consumeEntity(TokenType.ESCAPABLE_RAW_TEXT);\n-        this._beginToken(TokenType.ESCAPABLE_RAW_TEXT);\n-      } else {\n-        parts.push(this._readChar());\n-      }\n+      parts.push(this._readChar(decodeEntities));\n     }\n-    this._endToken([this._processCarriageReturns(parts.join(''))]);\n+    return this._endToken([this._processCarriageReturns(parts.join(''))]);\n   }\n \n   private _consumeComment(start: CharacterCursor) {\n@@ -572,8 +563,8 @@ class _Tokenizer {\n     }\n   }\n \n-  private _consumeRawTextWithTagClose(prefix: string, tagName: string, consumeEntities: boolean) {\n-    this._consumeRawText(consumeEntities, () => {\n+  private _consumeRawTextWithTagClose(prefix: string, tagName: string, decodeEntities: boolean) {\n+    this._consumeRawText(decodeEntities, () => {\n       if (!this._attemptCharCode(chars.$LT)) return false;\n       if (!this._attemptCharCode(chars.$SLASH)) return false;\n       this._attemptCharCodeUntilFn(isNotWhitespace);\n@@ -721,16 +712,11 @@ class _Tokenizer {\n       const current = this._cursor.clone();\n       if (this._interpolationConfig && this._attemptStr(this._interpolationConfig.start)) {\n         this._endToken([this._processCarriageReturns(parts.join(''))], current);\n-        parts.length = 0;\n         this._consumeInterpolation(interpolationTokenType, current);\n-        this._beginToken(textTokenType);\n-      } else if (this._cursor.peek() === chars.$AMPERSAND) {\n-        this._endToken([this._processCarriageReturns(parts.join(''))]);\n         parts.length = 0;\n-        this._consumeEntity(textTokenType);\n         this._beginToken(textTokenType);\n       } else {\n-        parts.push(this._readChar());\n+        parts.push(this._readChar(true));\n       }\n     }\n \n@@ -909,9 +895,7 @@ function mergeTextTokens(srcTokens: Token[]): Token[] {\n   let lastDstToken: Token|undefined = undefined;\n   for (let i = 0; i < srcTokens.length; i++) {\n     const token = srcTokens[i];\n-    if ((lastDstToken && lastDstToken.type == TokenType.TEXT && token.type == TokenType.TEXT) ||\n-        (lastDstToken && lastDstToken.type == TokenType.ATTR_VALUE_TEXT &&\n-         token.type == TokenType.ATTR_VALUE_TEXT)) {\n+    if (lastDstToken && lastDstToken.type == TokenType.TEXT && token.type == TokenType.TEXT) {\n       lastDstToken.parts[0]! += token.parts[0];\n       lastDstToken.sourceSpan.end = token.sourceSpan.end;\n     } else {"
        },
        {
            "sha": "9ac0b944e42a7cafd56570ff1261114c8b67766d",
            "filename": "packages/compiler/src/ml_parser/parser.ts",
            "status": "modified",
            "additions": 17,
            "deletions": 21,
            "changes": 38,
            "blob_url": "https://github.com/angular/angular/blob/8d8ab4775c1ff197ad101cbd33690bc8620a3c56/packages%2Fcompiler%2Fsrc%2Fml_parser%2Fparser.ts",
            "raw_url": "https://github.com/angular/angular/raw/8d8ab4775c1ff197ad101cbd33690bc8620a3c56/packages%2Fcompiler%2Fsrc%2Fml_parser%2Fparser.ts",
            "contents_url": "https://api.github.com/repos/angular/angular/contents/packages%2Fcompiler%2Fsrc%2Fml_parser%2Fparser.ts?ref=8d8ab4775c1ff197ad101cbd33690bc8620a3c56",
            "patch": "@@ -226,21 +226,20 @@ class _TreeBuilder {\n       }\n     }\n \n-    // For now recombine text, interpolation and entity tokens\n-    while (this._peek.type === lex.TokenType.INTERPOLATION ||\n-           this._peek.type === lex.TokenType.TEXT ||\n-           this._peek.type === lex.TokenType.ENCODED_ENTITY) {\n-      token = this._advance();\n-      if (token.type === lex.TokenType.INTERPOLATION) {\n-        // For backward compatibility we decode HTML entities that appear in interpolation\n-        // expressions. This is arguably a bug, but it could be a considerable breaking change to\n-        // fix it. It should be addressed in a larger project to refactor the entire parser/lexer\n-        // chain after View Engine has been removed.\n-        text += token.parts.join('').replace(/&([^;]+);/g, decodeEntity);\n-      } else if (token.type === lex.TokenType.ENCODED_ENTITY) {\n-        text += token.parts[0];\n-      } else {\n-        text += token.parts.join('');\n+    // For now recombine text and interpolation tokens\n+    if (this._peek.type === lex.TokenType.INTERPOLATION) {\n+      while (this._peek.type === lex.TokenType.INTERPOLATION ||\n+             this._peek.type === lex.TokenType.TEXT) {\n+        token = this._advance();\n+        if (token.type === lex.TokenType.INTERPOLATION) {\n+          // For backward compatibility we decode HTML entities that appear in interpolation\n+          // expressions. This is arguably a bug, but it could be a considerable breaking change to\n+          // fix it. It should be addressed in a larger project to refactor the entire parser/lexer\n+          // chain after View Engine has been removed.\n+          text += token.parts.join('').replace(/&([^;]+);/g, decodeEntity);\n+        } else {\n+          text += token.parts.join('');\n+        }\n       }\n     }\n \n@@ -370,26 +369,23 @@ class _TreeBuilder {\n       this._advance();\n     }\n \n-    // Consume the attribute value\n+    // Consume the value\n     let value = '';\n     let valueStartSpan: ParseSourceSpan|undefined = undefined;\n     let valueEnd: ParseLocation|undefined = undefined;\n     if (this._peek.type === lex.TokenType.ATTR_VALUE_TEXT) {\n       valueStartSpan = this._peek.sourceSpan;\n       valueEnd = this._peek.sourceSpan.end;\n-      // For now recombine text, interpolation and entity tokens\n+      // For now we are recombining text and interpolation tokens\n       while (this._peek.type === lex.TokenType.ATTR_VALUE_TEXT ||\n-             this._peek.type === lex.TokenType.ATTR_VALUE_INTERPOLATION ||\n-             this._peek.type === lex.TokenType.ENCODED_ENTITY) {\n+             this._peek.type === lex.TokenType.ATTR_VALUE_INTERPOLATION) {\n         let valueToken = this._advance();\n         if (valueToken.type === lex.TokenType.ATTR_VALUE_INTERPOLATION) {\n           // For backward compatibility we decode HTML entities that appear in interpolation\n           // expressions. This is arguably a bug, but it could be a considerable breaking change to\n           // fix it. It should be addressed in a larger project to refactor the entire parser/lexer\n           // chain after View Engine has been removed.\n           value += valueToken.parts.join('').replace(/&([^;]+);/g, decodeEntity);\n-        } else if (valueToken.type === lex.TokenType.ENCODED_ENTITY) {\n-          value += valueToken.parts[0];\n         } else {\n           value += valueToken.parts.join('');\n         }"
        },
        {
            "sha": "835d59970dab0defb01a211075cbed931728e5a4",
            "filename": "packages/compiler/test/ml_parser/lexer_spec.ts",
            "status": "modified",
            "additions": 12,
            "deletions": 32,
            "changes": 44,
            "blob_url": "https://github.com/angular/angular/blob/8d8ab4775c1ff197ad101cbd33690bc8620a3c56/packages%2Fcompiler%2Ftest%2Fml_parser%2Flexer_spec.ts",
            "raw_url": "https://github.com/angular/angular/raw/8d8ab4775c1ff197ad101cbd33690bc8620a3c56/packages%2Fcompiler%2Ftest%2Fml_parser%2Flexer_spec.ts",
            "contents_url": "https://api.github.com/repos/angular/angular/contents/packages%2Fcompiler%2Ftest%2Fml_parser%2Flexer_spec.ts?ref=8d8ab4775c1ff197ad101cbd33690bc8620a3c56",
            "patch": "@@ -407,11 +407,7 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n           [lex.TokenType.TAG_OPEN_START, '', 't'],\n           [lex.TokenType.ATTR_NAME, '', 'a'],\n           [lex.TokenType.ATTR_QUOTE, '\"'],\n-          [lex.TokenType.ATTR_VALUE_TEXT, ''],\n-          [lex.TokenType.ENCODED_ENTITY, 'A', '&#65;'],\n-          [lex.TokenType.ATTR_VALUE_TEXT, ''],\n-          [lex.TokenType.ENCODED_ENTITY, 'A', '&#x41;'],\n-          [lex.TokenType.ATTR_VALUE_TEXT, ''],\n+          [lex.TokenType.ATTR_VALUE_TEXT, 'AA'],\n           [lex.TokenType.ATTR_QUOTE, '\"'],\n           [lex.TokenType.TAG_OPEN_END],\n           [lex.TokenType.EOF],\n@@ -526,60 +522,50 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n     describe('entities', () => {\n       it('should parse named entities', () => {\n         expect(tokenizeAndHumanizeParts('a&amp;b')).toEqual([\n-          [lex.TokenType.TEXT, 'a'],\n-          [lex.TokenType.ENCODED_ENTITY, '&', '&amp;'],\n-          [lex.TokenType.TEXT, 'b'],\n+          [lex.TokenType.TEXT, 'a&b'],\n           [lex.TokenType.EOF],\n         ]);\n       });\n \n       it('should parse hexadecimal entities', () => {\n         expect(tokenizeAndHumanizeParts('&#x41;&#X41;')).toEqual([\n-          [lex.TokenType.TEXT, ''],\n-          [lex.TokenType.ENCODED_ENTITY, 'A', '&#x41;'],\n-          [lex.TokenType.TEXT, ''],\n-          [lex.TokenType.ENCODED_ENTITY, 'A', '&#X41;'],\n-          [lex.TokenType.TEXT, ''],\n+          [lex.TokenType.TEXT, 'AA'],\n           [lex.TokenType.EOF],\n         ]);\n       });\n \n       it('should parse decimal entities', () => {\n         expect(tokenizeAndHumanizeParts('&#65;')).toEqual([\n-          [lex.TokenType.TEXT, ''],\n-          [lex.TokenType.ENCODED_ENTITY, 'A', '&#65;'],\n-          [lex.TokenType.TEXT, ''],\n+          [lex.TokenType.TEXT, 'A'],\n           [lex.TokenType.EOF],\n         ]);\n       });\n \n       it('should store the locations', () => {\n         expect(tokenizeAndHumanizeSourceSpans('a&amp;b')).toEqual([\n-          [lex.TokenType.TEXT, 'a'],\n-          [lex.TokenType.ENCODED_ENTITY, '&amp;'],\n-          [lex.TokenType.TEXT, 'b'],\n+          [lex.TokenType.TEXT, 'a&amp;b'],\n           [lex.TokenType.EOF, ''],\n         ]);\n       });\n \n       it('should report malformed/unknown entities', () => {\n         expect(tokenizeAndHumanizeErrors('&tbo;')).toEqual([[\n-          lex.TokenType.ENCODED_ENTITY,\n+          lex.TokenType.TEXT,\n           'Unknown entity \"tbo\" - use the \"&#<decimal>;\" or  \"&#x<hex>;\" syntax', '0:0'\n         ]]);\n         expect(tokenizeAndHumanizeErrors('&#3sdf;')).toEqual([[\n-          lex.TokenType.ENCODED_ENTITY,\n+          lex.TokenType.TEXT,\n           'Unable to parse entity \"&#3s\" - decimal character reference entities must end with \";\"',\n           '0:4'\n         ]]);\n         expect(tokenizeAndHumanizeErrors('&#xasdf;')).toEqual([[\n-          lex.TokenType.ENCODED_ENTITY,\n+          lex.TokenType.TEXT,\n           'Unable to parse entity \"&#xas\" - hexadecimal character reference entities must end with \";\"',\n           '0:5'\n         ]]);\n \n         expect(tokenizeAndHumanizeErrors('&#xABC')).toEqual([\n-          [lex.TokenType.ENCODED_ENTITY, 'Unexpected character \"EOF\"', '0:6']\n+          [lex.TokenType.TEXT, 'Unexpected character \"EOF\"', '0:6']\n         ]);\n       });\n     });\n@@ -657,16 +643,12 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n \n       it('should parse entities', () => {\n         expect(tokenizeAndHumanizeParts('a&amp;b')).toEqual([\n-          [lex.TokenType.TEXT, 'a'],\n-          [lex.TokenType.ENCODED_ENTITY, '&', '&amp;'],\n-          [lex.TokenType.TEXT, 'b'],\n+          [lex.TokenType.TEXT, 'a&b'],\n           [lex.TokenType.EOF],\n         ]);\n \n         expect(tokenizeAndHumanizeSourceSpans('a&amp;b')).toEqual([\n-          [lex.TokenType.TEXT, 'a'],\n-          [lex.TokenType.ENCODED_ENTITY, '&amp;'],\n-          [lex.TokenType.TEXT, 'b'],\n+          [lex.TokenType.TEXT, 'a&amp;b'],\n           [lex.TokenType.EOF, ''],\n         ]);\n       });\n@@ -912,9 +894,7 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n         expect(tokenizeAndHumanizeParts(`<title>&amp;</title>`)).toEqual([\n           [lex.TokenType.TAG_OPEN_START, '', 'title'],\n           [lex.TokenType.TAG_OPEN_END],\n-          [lex.TokenType.ESCAPABLE_RAW_TEXT, ''],\n-          [lex.TokenType.ENCODED_ENTITY, '&', '&amp;'],\n-          [lex.TokenType.ESCAPABLE_RAW_TEXT, ''],\n+          [lex.TokenType.ESCAPABLE_RAW_TEXT, '&'],\n           [lex.TokenType.TAG_CLOSE, '', 'title'],\n           [lex.TokenType.EOF],\n         ]);"
        }
    ],
    "stats": {
        "total": 154,
        "additions": 57,
        "deletions": 97
    }
}