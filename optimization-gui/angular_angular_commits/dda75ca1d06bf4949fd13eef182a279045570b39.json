{
    "author": "atscott",
    "message": "Revert \"refactor(compiler): support interpolation tokens when lexing markup (#42062)\" (#43033)\n\nThis reverts commit c8a46bfdcd5dac0044c4322a5b3967163056b339.\n\nPR Close #43033",
    "sha": "dda75ca1d06bf4949fd13eef182a279045570b39",
    "files": [
        {
            "sha": "f0fb361232ac8aef0bf3d212d31a2053f0f929e6",
            "filename": "packages/compiler/src/ml_parser/lexer.ts",
            "status": "modified",
            "additions": 10,
            "deletions": 63,
            "changes": 73,
            "blob_url": "https://github.com/angular/angular/blob/dda75ca1d06bf4949fd13eef182a279045570b39/packages%2Fcompiler%2Fsrc%2Fml_parser%2Flexer.ts",
            "raw_url": "https://github.com/angular/angular/raw/dda75ca1d06bf4949fd13eef182a279045570b39/packages%2Fcompiler%2Fsrc%2Fml_parser%2Flexer.ts",
            "contents_url": "https://api.github.com/repos/angular/angular/contents/packages%2Fcompiler%2Fsrc%2Fml_parser%2Flexer.ts?ref=dda75ca1d06bf4949fd13eef182a279045570b39",
            "patch": "@@ -22,7 +22,6 @@ export enum TokenType {\n   TEXT,\n   ESCAPABLE_RAW_TEXT,\n   RAW_TEXT,\n-  INTERPOLATION,\n   COMMENT_START,\n   COMMENT_END,\n   CDATA_START,\n@@ -286,7 +285,7 @@ class _Tokenizer {\n     }\n     const token = new Token(\n         this._currentTokenType, parts,\n-        (end ?? this._cursor).getSpan(this._currentTokenStart, this._leadingTriviaCodePoints));\n+        this._cursor.getSpan(this._currentTokenStart, this._leadingTriviaCodePoints));\n     this.tokens.push(token);\n     this._currentTokenStart = null;\n     this._currentTokenType = null;\n@@ -697,16 +696,19 @@ class _Tokenizer {\n   }\n \n   private _consumeText() {\n-    this._beginToken(TokenType.TEXT);\n+    const start = this._cursor.clone();\n+    this._beginToken(TokenType.TEXT, start);\n     const parts: string[] = [];\n \n     do {\n-      const current = this._cursor.clone();\n       if (this._interpolationConfig && this._attemptStr(this._interpolationConfig.start)) {\n-        this._endToken([this._processCarriageReturns(parts.join(''))], current);\n-        this._consumeInterpolation(current);\n-        parts.length = 0;\n-        this._beginToken(TokenType.TEXT);\n+        parts.push(this._interpolationConfig.start);\n+        this._inInterpolation = true;\n+      } else if (\n+          this._interpolationConfig && this._inInterpolation &&\n+          this._attemptStr(this._interpolationConfig.end)) {\n+        parts.push(this._interpolationConfig.end);\n+        this._inInterpolation = false;\n       } else {\n         parts.push(this._readChar(true));\n       }\n@@ -719,61 +721,6 @@ class _Tokenizer {\n     this._endToken([this._processCarriageReturns(parts.join(''))]);\n   }\n \n-  private _consumeInterpolation(interpolationStart: CharacterCursor) {\n-    const parts: string[] = [];\n-    this._beginToken(TokenType.INTERPOLATION, interpolationStart);\n-    parts.push(this._interpolationConfig.start);\n-\n-    // Find the end of the interpolation, ignoring content inside quotes.\n-    const expressionStart = this._cursor.clone();\n-    let inQuote: string|null = null;\n-    let inComment = false;\n-    while (this._cursor.peek() !== chars.$EOF) {\n-      const current = this._cursor.clone();\n-\n-      if (this._isTagStart()) {\n-        // We are starting what looks like an HTML element in the middle of this interpolation.\n-        // Reset the cursor to before the `<` character and end the interpolation token.\n-        // (This is actually wrong but here for backward compatibility).\n-        this._cursor = current;\n-        parts.push(this._getProcessedChars(expressionStart, current));\n-        return this._endToken(parts);\n-      }\n-\n-      if (inQuote === null) {\n-        if (this._attemptStr(this._interpolationConfig.end)) {\n-          // We are not in a string, and we hit the end interpolation marker\n-          parts.push(this._getProcessedChars(expressionStart, current));\n-          parts.push(this._interpolationConfig.end);\n-          return this._endToken(parts);\n-        } else if (this._attemptStr('//')) {\n-          // Once we are in a comment we ignore any quotes\n-          inComment = true;\n-        }\n-      }\n-\n-      const char = this._readChar(true);\n-      if (char === '\\\\') {\n-        // Skip the next character because it was escaped.\n-        this._readChar(true);\n-      } else if (char === inQuote) {\n-        // Exiting the current quoted string\n-        inQuote = null;\n-      } else if (!inComment && /['\"`]/.test(char)) {\n-        // Entering a new quoted string\n-        inQuote = char;\n-      }\n-    }\n-\n-    // We hit EOF without finding a closing interpolation marker\n-    parts.push(this._getProcessedChars(expressionStart, this._cursor));\n-    return this._endToken(parts);\n-  }\n-\n-  private _getProcessedChars(start: CharacterCursor, end: CharacterCursor): string {\n-    return this._processCarriageReturns(end.getChars(start))\n-  }\n-\n   private _isTextEnd(): boolean {\n     if (this._isTagStart() || this._cursor.peek() === chars.$EOF) {\n       return true;"
        },
        {
            "sha": "24465f8e972b0d355fa7c950c7c8d78c0a0fb7e8",
            "filename": "packages/compiler/src/ml_parser/parser.ts",
            "status": "modified",
            "additions": 1,
            "deletions": 42,
            "changes": 43,
            "blob_url": "https://github.com/angular/angular/blob/dda75ca1d06bf4949fd13eef182a279045570b39/packages%2Fcompiler%2Fsrc%2Fml_parser%2Fparser.ts",
            "raw_url": "https://github.com/angular/angular/raw/dda75ca1d06bf4949fd13eef182a279045570b39/packages%2Fcompiler%2Fsrc%2Fml_parser%2Fparser.ts",
            "contents_url": "https://api.github.com/repos/angular/angular/contents/packages%2Fcompiler%2Fsrc%2Fml_parser%2Fparser.ts?ref=dda75ca1d06bf4949fd13eef182a279045570b39",
            "patch": "@@ -9,7 +9,6 @@\n import {ParseError, ParseSourceSpan} from '../parse_util';\n \n import * as html from './ast';\n-import {NAMED_ENTITIES} from './entities';\n import * as lex from './lexer';\n import {getNsPrefix, mergeNsAndName, splitNsName, TagDefinition} from './tags';\n \n@@ -216,7 +215,6 @@ class _TreeBuilder {\n   }\n \n   private _consumeText(token: lex.Token) {\n-    const startSpan = token.sourceSpan;\n     let text = token.parts[0];\n     if (text.length > 0 && text[0] == '\\n') {\n       const parent = this._getParentElement();\n@@ -226,29 +224,8 @@ class _TreeBuilder {\n       }\n     }\n \n-    // For now recombine text and interpolation tokens\n-    if (this._peek.type === lex.TokenType.INTERPOLATION) {\n-      while (this._peek.type === lex.TokenType.INTERPOLATION ||\n-             this._peek.type === lex.TokenType.TEXT) {\n-        token = this._advance();\n-        if (token.type === lex.TokenType.INTERPOLATION) {\n-          // For backward compatibility we decode HTML entities that appear in interpolation\n-          // expressions. This is arguably a bug, but it could be a considerable breaking change to\n-          // fix it. It should be addressed in a larger project to refactor the entire parser/lexer\n-          // chain after View Engine has been removed.\n-          text += token.parts.join('').replace(/&([^;]+);/g, decodeEntity);\n-        } else {\n-          text += token.parts.join('');\n-        }\n-      }\n-    }\n-\n     if (text.length > 0) {\n-      const endSpan = token.sourceSpan;\n-      this._addToParent(new html.Text(\n-          text,\n-          new ParseSourceSpan(\n-              startSpan.start, endSpan.end, startSpan.fullStart, startSpan.details)));\n+      this._addToParent(new html.Text(text, token.sourceSpan));\n     }\n   }\n \n@@ -418,21 +395,3 @@ class _TreeBuilder {\n function lastOnStack(stack: any[], element: any): boolean {\n   return stack.length > 0 && stack[stack.length - 1] === element;\n }\n-\n-/**\n- * Decode the `entity` string, which we believe is the contents of an HTML entity.\n- *\n- * If the string is not actually a valid/known entity then just return the original `match` string.\n- */\n-function decodeEntity(match: string, entity: string): string {\n-  if (NAMED_ENTITIES[entity] !== undefined) {\n-    return NAMED_ENTITIES[entity] || match;\n-  }\n-  if (/^#x[a-f0-9]+$/i.test(entity)) {\n-    return String.fromCodePoint(parseInt(entity.slice(2), 16));\n-  }\n-  if (/^#\\d+$/.test(entity)) {\n-    return String.fromCodePoint(parseInt(entity.slice(1), 10));\n-  }\n-  return match;\n-}"
        },
        {
            "sha": "b971d9187a7249f362b619c35a7ccb58ad62f62c",
            "filename": "packages/compiler/test/ml_parser/html_parser_spec.ts",
            "status": "modified",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/angular/angular/blob/dda75ca1d06bf4949fd13eef182a279045570b39/packages%2Fcompiler%2Ftest%2Fml_parser%2Fhtml_parser_spec.ts",
            "raw_url": "https://github.com/angular/angular/raw/dda75ca1d06bf4949fd13eef182a279045570b39/packages%2Fcompiler%2Ftest%2Fml_parser%2Fhtml_parser_spec.ts",
            "contents_url": "https://api.github.com/repos/angular/angular/contents/packages%2Fcompiler%2Ftest%2Fml_parser%2Fhtml_parser_spec.ts?ref=dda75ca1d06bf4949fd13eef182a279045570b39",
            "patch": "@@ -675,32 +675,6 @@ import {humanizeDom, humanizeDomSourceSpans, humanizeLineColumn, humanizeNodes}\n           expect(node.endSourceSpan!.end.offset).toEqual(12);\n         });\n \n-        // This checks backward compatibility with a previous version of the lexer, which would\n-        // treat interpolation expressions as regular HTML escapable text.\n-        it('should decode HTML entities in interpolations', () => {\n-          expect(humanizeDomSourceSpans(parser.parse(\n-                     '{{&amp;}}' +\n-                         '{{&#x25BE;}}' +\n-                         '{{&#9662;}}' +\n-                         '{{&amp (no semi-colon)}}' +\n-                         '{{&#25BE; (invalid decimal)}}',\n-                     'TestComp')))\n-              .toEqual([[\n-                html.Text,\n-                '{{&}}' +\n-                    '{{\\u25BE}}' +\n-                    '{{\\u25BE}}' +\n-                    '{{&amp (no semi-colon)}}' +\n-                    '{{&#25BE; (invalid decimal)}}',\n-                0,\n-                '{{&amp;}}' +\n-                    '{{&#x25BE;}}' +\n-                    '{{&#9662;}}' +\n-                    '{{&amp (no semi-colon)}}' +\n-                    '{{&#25BE; (invalid decimal)}}',\n-              ]]);\n-        });\n-\n         it('should not set the end source span for void elements', () => {\n           expect(humanizeDomSourceSpans(parser.parse('<div><br></div>', 'TestComp'))).toEqual([\n             [html.Element, 'div', 0, '<div><br></div>', '<div>', '</div>'],"
        },
        {
            "sha": "5c795ed95964a0ff07450fd6c4dc80a86f49551d",
            "filename": "packages/compiler/test/ml_parser/lexer_spec.ts",
            "status": "modified",
            "additions": 20,
            "deletions": 102,
            "changes": 122,
            "blob_url": "https://github.com/angular/angular/blob/dda75ca1d06bf4949fd13eef182a279045570b39/packages%2Fcompiler%2Ftest%2Fml_parser%2Flexer_spec.ts",
            "raw_url": "https://github.com/angular/angular/raw/dda75ca1d06bf4949fd13eef182a279045570b39/packages%2Fcompiler%2Ftest%2Fml_parser%2Flexer_spec.ts",
            "contents_url": "https://api.github.com/repos/angular/angular/contents/packages%2Fcompiler%2Ftest%2Fml_parser%2Flexer_spec.ts?ref=dda75ca1d06bf4949fd13eef182a279045570b39",
            "patch": "@@ -549,78 +549,32 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n       });\n \n       it('should parse interpolation', () => {\n-        expect(tokenizeAndHumanizeParts('{{ a }}b{{ c // comment }}d{{ e \"}}\" f }}g{{ h // \" i }}'))\n-            .toEqual([\n-              [lex.TokenType.TEXT, ''],\n-              [lex.TokenType.INTERPOLATION, '{{', ' a ', '}}'],\n-              [lex.TokenType.TEXT, 'b'],\n-              [lex.TokenType.INTERPOLATION, '{{', ' c // comment ', '}}'],\n-              [lex.TokenType.TEXT, 'd'],\n-              [lex.TokenType.INTERPOLATION, '{{', ' e \"}}\" f ', '}}'],\n-              [lex.TokenType.TEXT, 'g'],\n-              [lex.TokenType.INTERPOLATION, '{{', ' h // \" i ', '}}'],\n-              [lex.TokenType.TEXT, ''],\n-              [lex.TokenType.EOF],\n-            ]);\n-\n-        expect(tokenizeAndHumanizeSourceSpans('{{ a }}b{{ c // comment }}')).toEqual([\n-          [lex.TokenType.TEXT, ''],\n-          [lex.TokenType.INTERPOLATION, '{{ a }}'],\n-          [lex.TokenType.TEXT, 'b'],\n-          [lex.TokenType.INTERPOLATION, '{{ c // comment }}'],\n-          [lex.TokenType.TEXT, ''],\n-          [lex.TokenType.EOF, ''],\n+        expect(tokenizeAndHumanizeParts('{{ a }}b{{ c // comment }}')).toEqual([\n+          [lex.TokenType.TEXT, '{{ a }}b{{ c // comment }}'],\n+          [lex.TokenType.EOF],\n         ]);\n       });\n \n       it('should parse interpolation with custom markers', () => {\n         expect(tokenizeAndHumanizeParts('{% a %}', {interpolationConfig: {start: '{%', end: '%}'}}))\n             .toEqual([\n-              [lex.TokenType.TEXT, ''],\n-              [lex.TokenType.INTERPOLATION, '{%', ' a ', '%}'],\n-              [lex.TokenType.TEXT, ''],\n+              [lex.TokenType.TEXT, '{% a %}'],\n               [lex.TokenType.EOF],\n             ]);\n       });\n \n-      it('should handle CR & LF in text', () => {\n+      it('should handle CR & LF', () => {\n         expect(tokenizeAndHumanizeParts('t\\ne\\rs\\r\\nt')).toEqual([\n           [lex.TokenType.TEXT, 't\\ne\\ns\\nt'],\n           [lex.TokenType.EOF],\n         ]);\n-\n-        expect(tokenizeAndHumanizeSourceSpans('t\\ne\\rs\\r\\nt')).toEqual([\n-          [lex.TokenType.TEXT, 't\\ne\\rs\\r\\nt'],\n-          [lex.TokenType.EOF, ''],\n-        ]);\n-      });\n-\n-      it('should handle CR & LF in interpolation', () => {\n-        expect(tokenizeAndHumanizeParts('{{t\\ne\\rs\\r\\nt}}')).toEqual([\n-          [lex.TokenType.TEXT, ''],\n-          [lex.TokenType.INTERPOLATION, '{{', 't\\ne\\ns\\nt', '}}'],\n-          [lex.TokenType.TEXT, ''],\n-          [lex.TokenType.EOF],\n-        ]);\n-\n-        expect(tokenizeAndHumanizeSourceSpans('{{t\\ne\\rs\\r\\nt}}')).toEqual([\n-          [lex.TokenType.TEXT, ''],\n-          [lex.TokenType.INTERPOLATION, '{{t\\ne\\rs\\r\\nt}}'],\n-          [lex.TokenType.TEXT, ''],\n-          [lex.TokenType.EOF, ''],\n-        ]);\n       });\n \n       it('should parse entities', () => {\n         expect(tokenizeAndHumanizeParts('a&amp;b')).toEqual([\n           [lex.TokenType.TEXT, 'a&b'],\n           [lex.TokenType.EOF],\n         ]);\n-\n-        expect(tokenizeAndHumanizeSourceSpans('a&amp;b')).toEqual([\n-          [lex.TokenType.TEXT, 'a&amp;b'],\n-          [lex.TokenType.EOF, ''],\n-        ]);\n       });\n \n       it('should parse text starting with \"&\"', () => {\n@@ -639,9 +593,7 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n \n       it('should allow \"<\" in text nodes', () => {\n         expect(tokenizeAndHumanizeParts('{{ a < b ? c : d }}')).toEqual([\n-          [lex.TokenType.TEXT, ''],\n-          [lex.TokenType.INTERPOLATION, '{{', ' a < b ? c : d ', '}}'],\n-          [lex.TokenType.TEXT, ''],\n+          [lex.TokenType.TEXT, '{{ a < b ? c : d }}'],\n           [lex.TokenType.EOF],\n         ]);\n \n@@ -662,9 +614,7 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n \n       it('should break out of interpolation in text token on valid start tag', () => {\n         expect(tokenizeAndHumanizeParts('{{ a <b && c > d }}')).toEqual([\n-          [lex.TokenType.TEXT, ''],\n-          [lex.TokenType.INTERPOLATION, '{{', ' a '],\n-          [lex.TokenType.TEXT, ''],\n+          [lex.TokenType.TEXT, '{{ a '],\n           [lex.TokenType.TAG_OPEN_START, '', 'b'],\n           [lex.TokenType.ATTR_NAME, '', '&&'],\n           [lex.TokenType.ATTR_NAME, '', 'c'],\n@@ -676,9 +626,7 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n \n       it('should break out of interpolation in text token on valid comment', () => {\n         expect(tokenizeAndHumanizeParts('{{ a }<!---->}')).toEqual([\n-          [lex.TokenType.TEXT, ''],\n-          [lex.TokenType.INTERPOLATION, '{{', ' a }'],\n-          [lex.TokenType.TEXT, ''],\n+          [lex.TokenType.TEXT, '{{ a }'],\n           [lex.TokenType.COMMENT_START],\n           [lex.TokenType.RAW_TEXT, ''],\n           [lex.TokenType.COMMENT_END],\n@@ -689,9 +637,7 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n \n       it('should break out of interpolation in text token on valid CDATA', () => {\n         expect(tokenizeAndHumanizeParts('{{ a }<![CDATA[]]>}')).toEqual([\n-          [lex.TokenType.TEXT, ''],\n-          [lex.TokenType.INTERPOLATION, '{{', ' a }'],\n-          [lex.TokenType.TEXT, ''],\n+          [lex.TokenType.TEXT, '{{ a }'],\n           [lex.TokenType.CDATA_START],\n           [lex.TokenType.RAW_TEXT, ''],\n           [lex.TokenType.CDATA_END],\n@@ -707,14 +653,13 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n             .toEqual([\n               [lex.TokenType.TAG_OPEN_START, '', 'code'],\n               [lex.TokenType.TAG_OPEN_END],\n-              [lex.TokenType.TEXT, ''],\n-              [lex.TokenType.INTERPOLATION, '{{', '\\'<={\\'', '}}'],\n-              [lex.TokenType.TEXT, ''],\n+              [lex.TokenType.TEXT, '{{\\'<={\\'}}'],\n               [lex.TokenType.TAG_CLOSE, '', 'code'],\n               [lex.TokenType.EOF],\n             ]);\n       });\n \n+\n       it('should parse start tags quotes in place of an attribute name as text', () => {\n         expect(tokenizeAndHumanizeParts('<t \">')).toEqual([\n           [lex.TokenType.INCOMPLETE_TAG_OPEN, '', 't'],\n@@ -758,32 +703,18 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n \n       it('should be able to escape {', () => {\n         expect(tokenizeAndHumanizeParts('{{ \"{\" }}')).toEqual([\n-          [lex.TokenType.TEXT, ''],\n-          [lex.TokenType.INTERPOLATION, '{{', ' \"{\" ', '}}'],\n-          [lex.TokenType.TEXT, ''],\n+          [lex.TokenType.TEXT, '{{ \"{\" }}'],\n           [lex.TokenType.EOF],\n         ]);\n       });\n \n       it('should be able to escape {{', () => {\n         expect(tokenizeAndHumanizeParts('{{ \"{{\" }}')).toEqual([\n-          [lex.TokenType.TEXT, ''],\n-          [lex.TokenType.INTERPOLATION, '{{', ' \"{{\" ', '}}'],\n-          [lex.TokenType.TEXT, ''],\n+          [lex.TokenType.TEXT, '{{ \"{{\" }}'],\n           [lex.TokenType.EOF],\n         ]);\n       });\n \n-      it('should capture everything up to the end of file in the interpolation expression part if there are mismatched quotes',\n-         () => {\n-           expect(tokenizeAndHumanizeParts('{{ \"{{a}}\\' }}')).toEqual([\n-             [lex.TokenType.TEXT, ''],\n-             [lex.TokenType.INTERPOLATION, '{{', ' \"{{a}}\\' }}'],\n-             [lex.TokenType.TEXT, ''],\n-             [lex.TokenType.EOF],\n-           ]);\n-         });\n-\n       it('should treat expansion form as text when they are not parsed', () => {\n         expect(tokenizeAndHumanizeParts(\n                    '<span>{a, b, =4 {c}}</span>', {tokenizeExpansionForms: false}))\n@@ -1045,9 +976,7 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n               [lex.TokenType.RAW_TEXT, 'three'],\n               [lex.TokenType.EXPANSION_CASE_VALUE, '=4'],\n               [lex.TokenType.EXPANSION_CASE_EXP_START],\n-              [lex.TokenType.TEXT, 'four '],\n-              [lex.TokenType.INTERPOLATION, '{{', 'a', '}}'],\n-              [lex.TokenType.TEXT, ''],\n+              [lex.TokenType.TEXT, 'four {{a}}'],\n               [lex.TokenType.EXPANSION_CASE_EXP_END],\n               [lex.TokenType.EXPANSION_FORM_END],\n               [lex.TokenType.EOF],\n@@ -1104,9 +1033,7 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n                  [lex.TokenType.EXPANSION_CASE_EXP_END],\n                  [lex.TokenType.EXPANSION_CASE_VALUE, '=1'],\n                  [lex.TokenType.EXPANSION_CASE_EXP_START],\n-                 [lex.TokenType.TEXT, 'One '],\n-                 [lex.TokenType.INTERPOLATION, '{{', 'message', '}}'],\n-                 [lex.TokenType.TEXT, ''],\n+                 [lex.TokenType.TEXT, 'One {{message}}'],\n                  [lex.TokenType.EXPANSION_CASE_EXP_END],\n                  [lex.TokenType.EXPANSION_FORM_END],\n                  [lex.TokenType.TEXT, '\\n'],\n@@ -1136,9 +1063,7 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n                  [lex.TokenType.EXPANSION_CASE_EXP_END],\n                  [lex.TokenType.EXPANSION_CASE_VALUE, '=1'],\n                  [lex.TokenType.EXPANSION_CASE_EXP_START],\n-                 [lex.TokenType.TEXT, 'One '],\n-                 [lex.TokenType.INTERPOLATION, '{{', 'message', '}}'],\n-                 [lex.TokenType.TEXT, ''],\n+                 [lex.TokenType.TEXT, 'One {{message}}'],\n                  [lex.TokenType.EXPANSION_CASE_EXP_END],\n                  [lex.TokenType.EXPANSION_FORM_END],\n                  [lex.TokenType.TEXT, '\\n'],\n@@ -1219,9 +1144,7 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n                  [lex.TokenType.EXPANSION_CASE_EXP_END],\n                  [lex.TokenType.EXPANSION_CASE_VALUE, '=1'],\n                  [lex.TokenType.EXPANSION_CASE_EXP_START],\n-                 [lex.TokenType.TEXT, 'One '],\n-                 [lex.TokenType.INTERPOLATION, '{{', 'message', '}}'],\n-                 [lex.TokenType.TEXT, ''],\n+                 [lex.TokenType.TEXT, 'One {{message}}'],\n                  [lex.TokenType.EXPANSION_CASE_EXP_END],\n                  [lex.TokenType.EXPANSION_FORM_END],\n                  [lex.TokenType.TEXT, '\\n'],\n@@ -1251,9 +1174,7 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n                  [lex.TokenType.EXPANSION_CASE_EXP_END],\n                  [lex.TokenType.EXPANSION_CASE_VALUE, '=1'],\n                  [lex.TokenType.EXPANSION_CASE_EXP_START],\n-                 [lex.TokenType.TEXT, 'One '],\n-                 [lex.TokenType.INTERPOLATION, '{{', 'message', '}}'],\n-                 [lex.TokenType.TEXT, ''],\n+                 [lex.TokenType.TEXT, 'One {{message}}'],\n                  [lex.TokenType.EXPANSION_CASE_EXP_END],\n                  [lex.TokenType.EXPANSION_FORM_END],\n                  [lex.TokenType.TEXT, '\\n'],\n@@ -1380,11 +1301,8 @@ import {ParseLocation, ParseSourceFile, ParseSourceSpan} from '../../src/parse_u\n           [lex.TokenType.TEXT, '\\n \\n \\n'],\n           [lex.TokenType.EOF],\n         ]);\n-        expect(tokenizeAndHumanizeParts('\\\\r{{\\\\r}}\\\\r', {escapedString: true})).toEqual([\n-          // post processing converts `\\r` to `\\n`\n-          [lex.TokenType.TEXT, '\\n'],\n-          [lex.TokenType.INTERPOLATION, '{{', '\\n', '}}'],\n-          [lex.TokenType.TEXT, '\\n'],\n+        expect(tokenizeAndHumanizeParts('\\\\r \\\\r \\\\r', {escapedString: true})).toEqual([\n+          [lex.TokenType.TEXT, '\\n \\n \\n'],  // post processing converts `\\r` to `\\n`\n           [lex.TokenType.EOF],\n         ]);\n         expect(tokenizeAndHumanizeParts('\\\\v \\\\v \\\\v', {escapedString: true})).toEqual(["
        }
    ],
    "stats": {
        "total": 264,
        "additions": 31,
        "deletions": 233
    }
}