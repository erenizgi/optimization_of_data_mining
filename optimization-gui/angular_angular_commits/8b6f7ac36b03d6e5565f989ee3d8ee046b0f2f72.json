{
    "author": "atscott",
    "message": "Revert \"refactor(compiler): define interfaces for each lexer token (#42062)\" (#43033)\n\nThis reverts commit 9b3d4f5575bfccfbbfb943dc9689f3915dd63752.\n\nPR Close #43033",
    "sha": "8b6f7ac36b03d6e5565f989ee3d8ee046b0f2f72",
    "files": [
        {
            "sha": "dda395f873fbd1a84676b03e01247d0134c69d6a",
            "filename": "packages/compiler/src/i18n/i18n_parser.ts",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/angular/angular/blob/8b6f7ac36b03d6e5565f989ee3d8ee046b0f2f72/packages%2Fcompiler%2Fsrc%2Fi18n%2Fi18n_parser.ts",
            "raw_url": "https://github.com/angular/angular/raw/8b6f7ac36b03d6e5565f989ee3d8ee046b0f2f72/packages%2Fcompiler%2Fsrc%2Fi18n%2Fi18n_parser.ts",
            "contents_url": "https://api.github.com/repos/angular/angular/contents/packages%2Fcompiler%2Fsrc%2Fi18n%2Fi18n_parser.ts?ref=8b6f7ac36b03d6e5565f989ee3d8ee046b0f2f72",
            "patch": "@@ -11,7 +11,7 @@ import {Parser as ExpressionParser} from '../expression_parser/parser';\n import * as html from '../ml_parser/ast';\n import {getHtmlTagDefinition} from '../ml_parser/html_tags';\n import {InterpolationConfig} from '../ml_parser/interpolation_config';\n-import {InterpolatedAttributeToken, InterpolatedTextToken, TokenType} from '../ml_parser/tokens';\n+import {Token, TokenType} from '../ml_parser/lexer';\n import {ParseSourceSpan} from '../parse_util';\n \n import * as i18n from './i18n_ast';\n@@ -163,16 +163,16 @@ class _I18nVisitor implements html.Visitor {\n   }\n \n   /**\n-   * Convert, text and interpolated tokens up into text and placeholder pieces.\n+   * Split the, potentially interpolated, text up into text and placeholder pieces.\n    *\n-   * @param tokens The text and interpolated tokens.\n+   * @param text The potentially interpolated string to be split.\n    * @param sourceSpan The span of the whole of the `text` string.\n    * @param context The current context of the visitor, used to compute and store placeholders.\n    * @param previousI18n Any i18n metadata associated with this `text` from a previous pass.\n    */\n   private _visitTextWithInterpolation(\n-      tokens: (InterpolatedTextToken|InterpolatedAttributeToken)[], sourceSpan: ParseSourceSpan,\n-      context: I18nMessageVisitorContext, previousI18n: i18n.I18nMeta|undefined): i18n.Node {\n+      tokens: Token[], sourceSpan: ParseSourceSpan, context: I18nMessageVisitorContext,\n+      previousI18n: i18n.I18nMeta|undefined): i18n.Node {\n     // Return a sequence of `Text` and `Placeholder` nodes grouped in a `Container`.\n     const nodes: i18n.Node[] = [];\n     for (const token of tokens) {"
        },
        {
            "sha": "a8abcc80a612b205f19d65f0872c645602ac69f0",
            "filename": "packages/compiler/src/ml_parser/ast.ts",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/angular/angular/blob/8b6f7ac36b03d6e5565f989ee3d8ee046b0f2f72/packages%2Fcompiler%2Fsrc%2Fml_parser%2Fast.ts",
            "raw_url": "https://github.com/angular/angular/raw/8b6f7ac36b03d6e5565f989ee3d8ee046b0f2f72/packages%2Fcompiler%2Fsrc%2Fml_parser%2Fast.ts",
            "contents_url": "https://api.github.com/repos/angular/angular/contents/packages%2Fcompiler%2Fsrc%2Fml_parser%2Fast.ts?ref=8b6f7ac36b03d6e5565f989ee3d8ee046b0f2f72",
            "patch": "@@ -9,7 +9,7 @@\n import {AstPath} from '../ast_path';\n import {I18nMeta} from '../i18n/i18n_ast';\n import {ParseSourceSpan} from '../parse_util';\n-import {InterpolatedAttributeToken, InterpolatedTextToken} from './tokens';\n+import {Token} from './lexer';\n \n interface BaseNode {\n   sourceSpan: ParseSourceSpan;\n@@ -25,8 +25,7 @@ export abstract class NodeWithI18n implements BaseNode {\n \n export class Text extends NodeWithI18n {\n   constructor(\n-      public value: string, sourceSpan: ParseSourceSpan, public tokens: InterpolatedTextToken[],\n-      i18n?: I18nMeta) {\n+      public value: string, sourceSpan: ParseSourceSpan, public tokens: Token[], i18n?: I18nMeta) {\n     super(sourceSpan, i18n);\n   }\n   override visit(visitor: Visitor, context: any): any {\n@@ -59,7 +58,7 @@ export class Attribute extends NodeWithI18n {\n   constructor(\n       public name: string, public value: string, sourceSpan: ParseSourceSpan,\n       readonly keySpan: ParseSourceSpan|undefined, public valueSpan: ParseSourceSpan|undefined,\n-      public valueTokens: InterpolatedAttributeToken[]|undefined, i18n: I18nMeta|undefined) {\n+      public valueTokens: Token[]|undefined, i18n: I18nMeta|undefined) {\n     super(sourceSpan, i18n);\n   }\n   override visit(visitor: Visitor, context: any): any {"
        },
        {
            "sha": "57045ee8dcc9fefef2383d74530fceda12dead5c",
            "filename": "packages/compiler/src/ml_parser/html_whitespaces.ts",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/angular/angular/blob/8b6f7ac36b03d6e5565f989ee3d8ee046b0f2f72/packages%2Fcompiler%2Fsrc%2Fml_parser%2Fhtml_whitespaces.ts",
            "raw_url": "https://github.com/angular/angular/raw/8b6f7ac36b03d6e5565f989ee3d8ee046b0f2f72/packages%2Fcompiler%2Fsrc%2Fml_parser%2Fhtml_whitespaces.ts",
            "contents_url": "https://api.github.com/repos/angular/angular/contents/packages%2Fcompiler%2Fsrc%2Fml_parser%2Fhtml_whitespaces.ts?ref=8b6f7ac36b03d6e5565f989ee3d8ee046b0f2f72",
            "patch": "@@ -8,8 +8,8 @@\n \n import * as html from './ast';\n import {NGSP_UNICODE} from './entities';\n+import {Token, TokenType} from './lexer';\n import {ParseTreeResult} from './parser';\n-import {TextToken, TokenType} from './tokens';\n \n export const PRESERVE_WS_ATTR_NAME = 'ngPreserveWhitespaces';\n \n@@ -77,8 +77,8 @@ export class WhitespaceVisitor implements html.Visitor {\n     if (isNotBlank || hasExpansionSibling) {\n       // Process the whitespace in the tokens of this Text node\n       const tokens = text.tokens.map(\n-          token =>\n-              token.type === TokenType.TEXT ? createWhitespaceProcessedTextToken(token) : token);\n+          token => token.type === TokenType.TEXT ? createTextTokenAfterWhitespaceProcessing(token) :\n+                                                   token);\n       // Process the whitespace of the value of this Text node\n       const value = processWhitespace(text.value);\n       return new html.Text(value, text.sourceSpan, tokens, text.i18n);\n@@ -100,8 +100,8 @@ export class WhitespaceVisitor implements html.Visitor {\n   }\n }\n \n-function createWhitespaceProcessedTextToken({type, parts, sourceSpan}: TextToken): TextToken {\n-  return {type, parts: [processWhitespace(parts[0])], sourceSpan};\n+function createTextTokenAfterWhitespaceProcessing(token: Token): Token {\n+  return new Token(token.type, [processWhitespace(token.parts[0])], token.sourceSpan);\n }\n \n function processWhitespace(text: string): string {"
        },
        {
            "sha": "3c95825e3426965fd344474cccd633329fb8dbda",
            "filename": "packages/compiler/src/ml_parser/lexer.ts",
            "status": "modified",
            "additions": 43,
            "deletions": 16,
            "changes": 59,
            "blob_url": "https://github.com/angular/angular/blob/8b6f7ac36b03d6e5565f989ee3d8ee046b0f2f72/packages%2Fcompiler%2Fsrc%2Fml_parser%2Flexer.ts",
            "raw_url": "https://github.com/angular/angular/raw/8b6f7ac36b03d6e5565f989ee3d8ee046b0f2f72/packages%2Fcompiler%2Fsrc%2Fml_parser%2Flexer.ts",
            "contents_url": "https://api.github.com/repos/angular/angular/contents/packages%2Fcompiler%2Fsrc%2Fml_parser%2Flexer.ts?ref=8b6f7ac36b03d6e5565f989ee3d8ee046b0f2f72",
            "patch": "@@ -12,7 +12,39 @@ import {NAMED_ENTITIES} from './entities';\n \n import {DEFAULT_INTERPOLATION_CONFIG, InterpolationConfig} from './interpolation_config';\n import {TagContentType, TagDefinition} from './tags';\n-import {IncompleteTagOpenToken, TagOpenStartToken, Token, TokenType} from './tokens';\n+\n+export enum TokenType {\n+  TAG_OPEN_START,\n+  TAG_OPEN_END,\n+  TAG_OPEN_END_VOID,\n+  TAG_CLOSE,\n+  INCOMPLETE_TAG_OPEN,\n+  TEXT,\n+  ESCAPABLE_RAW_TEXT,\n+  RAW_TEXT,\n+  INTERPOLATION,\n+  ENCODED_ENTITY,\n+  COMMENT_START,\n+  COMMENT_END,\n+  CDATA_START,\n+  CDATA_END,\n+  ATTR_NAME,\n+  ATTR_QUOTE,\n+  ATTR_VALUE_TEXT,\n+  ATTR_VALUE_INTERPOLATION,\n+  DOC_TYPE,\n+  EXPANSION_FORM_START,\n+  EXPANSION_CASE_VALUE,\n+  EXPANSION_CASE_EXP_START,\n+  EXPANSION_CASE_EXP_END,\n+  EXPANSION_FORM_END,\n+  EOF\n+}\n+\n+export class Token {\n+  constructor(\n+      public type: TokenType|null, public parts: string[], public sourceSpan: ParseSourceSpan) {}\n+}\n \n export class TokenError extends ParseError {\n   constructor(errorMsg: string, public tokenType: TokenType|null, span: ParseSourceSpan) {\n@@ -258,12 +290,9 @@ class _Tokenizer {\n           'Programming error - attempted to end a token which has no token type', null,\n           this._cursor.getSpan(this._currentTokenStart));\n     }\n-    const token = {\n-      type: this._currentTokenType,\n-      parts,\n-      sourceSpan:\n-          (end ?? this._cursor).getSpan(this._currentTokenStart, this._leadingTriviaCodePoints),\n-    } as Token;\n+    const token = new Token(\n+        this._currentTokenType, parts,\n+        (end ?? this._cursor).getSpan(this._currentTokenStart, this._leadingTriviaCodePoints));\n     this.tokens.push(token);\n     this._currentTokenStart = null;\n     this._currentTokenType = null;\n@@ -498,7 +527,7 @@ class _Tokenizer {\n   private _consumeTagOpen(start: CharacterCursor) {\n     let tagName: string;\n     let prefix: string;\n-    let openTagToken: TagOpenStartToken|IncompleteTagOpenToken|undefined;\n+    let openTagToken: Token|undefined;\n     try {\n       if (!chars.isAsciiLetter(this._cursor.peek())) {\n         throw this._createError(\n@@ -561,10 +590,10 @@ class _Tokenizer {\n     this._endToken([prefix, tagName]);\n   }\n \n-  private _consumeTagOpenStart(start: CharacterCursor): TagOpenStartToken {\n+  private _consumeTagOpenStart(start: CharacterCursor) {\n     this._beginToken(TokenType.TAG_OPEN_START, start);\n     const parts = this._consumePrefixAndName();\n-    return this._endToken(parts) as TagOpenStartToken;\n+    return this._endToken(parts);\n   }\n \n   private _consumeAttributeName() {\n@@ -735,7 +764,7 @@ class _Tokenizer {\n    */\n   private _consumeInterpolation(\n       interpolationTokenType: TokenType, interpolationStart: CharacterCursor,\n-      prematureEndPredicate: (() => boolean)|null): void {\n+      prematureEndPredicate: (() => boolean)|null) {\n     const parts: string[] = [];\n     this._beginToken(interpolationTokenType, interpolationStart);\n     parts.push(this._interpolationConfig.start);\n@@ -754,17 +783,15 @@ class _Tokenizer {\n         // (This is actually wrong but here for backward compatibility).\n         this._cursor = current;\n         parts.push(this._getProcessedChars(expressionStart, current));\n-        this._endToken(parts);\n-        return;\n+        return this._endToken(parts);\n       }\n \n       if (inQuote === null) {\n         if (this._attemptStr(this._interpolationConfig.end)) {\n           // We are not in a string, and we hit the end interpolation marker\n           parts.push(this._getProcessedChars(expressionStart, current));\n           parts.push(this._interpolationConfig.end);\n-          this._endToken(parts);\n-          return;\n+          return this._endToken(parts);\n         } else if (this._attemptStr('//')) {\n           // Once we are in a comment we ignore any quotes\n           inComment = true;\n@@ -787,7 +814,7 @@ class _Tokenizer {\n \n     // We hit EOF without finding a closing interpolation marker\n     parts.push(this._getProcessedChars(expressionStart, this._cursor));\n-    this._endToken(parts);\n+    return this._endToken(parts);\n   }\n \n   private _getProcessedChars(start: CharacterCursor, end: CharacterCursor): string {"
        },
        {
            "sha": "d9f5d17bd32bbf0d786a11ed99c79cae18e3c49c",
            "filename": "packages/compiler/src/ml_parser/parser.ts",
            "status": "modified",
            "additions": 77,
            "deletions": 82,
            "changes": 159,
            "blob_url": "https://github.com/angular/angular/blob/8b6f7ac36b03d6e5565f989ee3d8ee046b0f2f72/packages%2Fcompiler%2Fsrc%2Fml_parser%2Fparser.ts",
            "raw_url": "https://github.com/angular/angular/raw/8b6f7ac36b03d6e5565f989ee3d8ee046b0f2f72/packages%2Fcompiler%2Fsrc%2Fml_parser%2Fparser.ts",
            "contents_url": "https://api.github.com/repos/angular/angular/contents/packages%2Fcompiler%2Fsrc%2Fml_parser%2Fparser.ts?ref=8b6f7ac36b03d6e5565f989ee3d8ee046b0f2f72",
            "patch": "@@ -10,9 +10,8 @@ import {ParseError, ParseLocation, ParseSourceSpan} from '../parse_util';\n \n import * as html from './ast';\n import {NAMED_ENTITIES} from './entities';\n-import {tokenize, TokenizeOptions} from './lexer';\n+import * as lex from './lexer';\n import {getNsPrefix, mergeNsAndName, splitNsName, TagDefinition} from './tags';\n-import {AttributeNameToken, AttributeQuoteToken, CdataStartToken, CommentStartToken, ExpansionCaseExpressionEndToken, ExpansionCaseExpressionStartToken, ExpansionCaseValueToken, ExpansionFormStartToken, IncompleteTagOpenToken, InterpolatedAttributeToken, InterpolatedTextToken, TagCloseToken, TagOpenStartToken, TextToken, Token, TokenType} from './tokens';\n \n export class TreeError extends ParseError {\n   static create(elementName: string|null, span: ParseSourceSpan, msg: string): TreeError {\n@@ -31,8 +30,8 @@ export class ParseTreeResult {\n export class Parser {\n   constructor(public getTagDefinition: (tagName: string) => TagDefinition) {}\n \n-  parse(source: string, url: string, options?: TokenizeOptions): ParseTreeResult {\n-    const tokenizeResult = tokenize(source, url, this.getTagDefinition, options);\n+  parse(source: string, url: string, options?: lex.TokenizeOptions): ParseTreeResult {\n+    const tokenizeResult = lex.tokenize(source, url, this.getTagDefinition, options);\n     const parser = new _TreeBuilder(tokenizeResult.tokens, this.getTagDefinition);\n     parser.build();\n     return new ParseTreeResult(\n@@ -44,89 +43,89 @@ export class Parser {\n \n class _TreeBuilder {\n   private _index: number = -1;\n-  // `_peek` will be initialized by the call to `_advance()` in the constructor.\n-  private _peek!: Token;\n+  // `_peek` will be initialized by the call to `advance()` in the constructor.\n+  private _peek!: lex.Token;\n   private _elementStack: html.Element[] = [];\n \n   rootNodes: html.Node[] = [];\n   errors: TreeError[] = [];\n \n   constructor(\n-      private tokens: Token[], private getTagDefinition: (tagName: string) => TagDefinition) {\n+      private tokens: lex.Token[], private getTagDefinition: (tagName: string) => TagDefinition) {\n     this._advance();\n   }\n \n   build(): void {\n-    while (this._peek.type !== TokenType.EOF) {\n-      if (this._peek.type === TokenType.TAG_OPEN_START ||\n-          this._peek.type === TokenType.INCOMPLETE_TAG_OPEN) {\n-        this._consumeStartTag(this._advance<TagOpenStartToken|IncompleteTagOpenToken>());\n-      } else if (this._peek.type === TokenType.TAG_CLOSE) {\n-        this._consumeEndTag(this._advance<TagCloseToken>());\n-      } else if (this._peek.type === TokenType.CDATA_START) {\n+    while (this._peek.type !== lex.TokenType.EOF) {\n+      if (this._peek.type === lex.TokenType.TAG_OPEN_START ||\n+          this._peek.type === lex.TokenType.INCOMPLETE_TAG_OPEN) {\n+        this._consumeStartTag(this._advance());\n+      } else if (this._peek.type === lex.TokenType.TAG_CLOSE) {\n+        this._consumeEndTag(this._advance());\n+      } else if (this._peek.type === lex.TokenType.CDATA_START) {\n         this._closeVoidElement();\n-        this._consumeCdata(this._advance<CdataStartToken>());\n-      } else if (this._peek.type === TokenType.COMMENT_START) {\n+        this._consumeCdata(this._advance());\n+      } else if (this._peek.type === lex.TokenType.COMMENT_START) {\n         this._closeVoidElement();\n-        this._consumeComment(this._advance<CommentStartToken>());\n+        this._consumeComment(this._advance());\n       } else if (\n-          this._peek.type === TokenType.TEXT || this._peek.type === TokenType.RAW_TEXT ||\n-          this._peek.type === TokenType.ESCAPABLE_RAW_TEXT) {\n+          this._peek.type === lex.TokenType.TEXT || this._peek.type === lex.TokenType.RAW_TEXT ||\n+          this._peek.type === lex.TokenType.ESCAPABLE_RAW_TEXT) {\n         this._closeVoidElement();\n-        this._consumeText(this._advance<TextToken>());\n-      } else if (this._peek.type === TokenType.EXPANSION_FORM_START) {\n-        this._consumeExpansion(this._advance<ExpansionFormStartToken>());\n+        this._consumeText(this._advance());\n+      } else if (this._peek.type === lex.TokenType.EXPANSION_FORM_START) {\n+        this._consumeExpansion(this._advance());\n       } else {\n         // Skip all other tokens...\n         this._advance();\n       }\n     }\n   }\n \n-  private _advance<T extends Token>(): T {\n+  private _advance(): lex.Token {\n     const prev = this._peek;\n     if (this._index < this.tokens.length - 1) {\n       // Note: there is always an EOF token at the end\n       this._index++;\n     }\n     this._peek = this.tokens[this._index];\n-    return prev as T;\n+    return prev;\n   }\n \n-  private _advanceIf<T extends TokenType>(type: T): (Token&{type: T})|null {\n+  private _advanceIf(type: lex.TokenType): lex.Token|null {\n     if (this._peek.type === type) {\n-      return this._advance<Token&{type: T}>();\n+      return this._advance();\n     }\n     return null;\n   }\n \n-  private _consumeCdata(_startToken: CdataStartToken) {\n-    this._consumeText(this._advance<TextToken>());\n-    this._advanceIf(TokenType.CDATA_END);\n+  private _consumeCdata(_startToken: lex.Token) {\n+    this._consumeText(this._advance());\n+    this._advanceIf(lex.TokenType.CDATA_END);\n   }\n \n-  private _consumeComment(token: CommentStartToken) {\n-    const text = this._advanceIf(TokenType.RAW_TEXT);\n-    this._advanceIf(TokenType.COMMENT_END);\n+  private _consumeComment(token: lex.Token) {\n+    const text = this._advanceIf(lex.TokenType.RAW_TEXT);\n+    this._advanceIf(lex.TokenType.COMMENT_END);\n     const value = text != null ? text.parts[0].trim() : null;\n     this._addToParent(new html.Comment(value, token.sourceSpan));\n   }\n \n-  private _consumeExpansion(token: ExpansionFormStartToken) {\n-    const switchValue = this._advance<TextToken>();\n+  private _consumeExpansion(token: lex.Token) {\n+    const switchValue = this._advance();\n \n-    const type = this._advance<TextToken>();\n+    const type = this._advance();\n     const cases: html.ExpansionCase[] = [];\n \n     // read =\n-    while (this._peek.type === TokenType.EXPANSION_CASE_VALUE) {\n+    while (this._peek.type === lex.TokenType.EXPANSION_CASE_VALUE) {\n       const expCase = this._parseExpansionCase();\n       if (!expCase) return;  // error\n       cases.push(expCase);\n     }\n \n     // read the final }\n-    if (this._peek.type !== TokenType.EXPANSION_FORM_END) {\n+    if (this._peek.type !== lex.TokenType.EXPANSION_FORM_END) {\n       this.errors.push(\n           TreeError.create(null, this._peek.sourceSpan, `Invalid ICU message. Missing '}'.`));\n       return;\n@@ -140,23 +139,23 @@ class _TreeBuilder {\n   }\n \n   private _parseExpansionCase(): html.ExpansionCase|null {\n-    const value = this._advance<ExpansionCaseValueToken>();\n+    const value = this._advance();\n \n     // read {\n-    if (this._peek.type !== TokenType.EXPANSION_CASE_EXP_START) {\n+    if (this._peek.type !== lex.TokenType.EXPANSION_CASE_EXP_START) {\n       this.errors.push(\n           TreeError.create(null, this._peek.sourceSpan, `Invalid ICU message. Missing '{'.`));\n       return null;\n     }\n \n     // read until }\n-    const start = this._advance<ExpansionCaseExpressionStartToken>();\n+    const start = this._advance();\n \n     const exp = this._collectExpansionExpTokens(start);\n     if (!exp) return null;\n \n-    const end = this._advance<ExpansionCaseExpressionEndToken>();\n-    exp.push({type: TokenType.EOF, parts: [], sourceSpan: end.sourceSpan});\n+    const end = this._advance();\n+    exp.push(new lex.Token(lex.TokenType.EOF, [], end.sourceSpan));\n \n     // parse everything in between { and }\n     const expansionCaseParser = new _TreeBuilder(exp, this.getTagDefinition);\n@@ -174,18 +173,18 @@ class _TreeBuilder {\n         value.parts[0], expansionCaseParser.rootNodes, sourceSpan, value.sourceSpan, expSourceSpan);\n   }\n \n-  private _collectExpansionExpTokens(start: Token): Token[]|null {\n-    const exp: Token[] = [];\n-    const expansionFormStack = [TokenType.EXPANSION_CASE_EXP_START];\n+  private _collectExpansionExpTokens(start: lex.Token): lex.Token[]|null {\n+    const exp: lex.Token[] = [];\n+    const expansionFormStack = [lex.TokenType.EXPANSION_CASE_EXP_START];\n \n     while (true) {\n-      if (this._peek.type === TokenType.EXPANSION_FORM_START ||\n-          this._peek.type === TokenType.EXPANSION_CASE_EXP_START) {\n+      if (this._peek.type === lex.TokenType.EXPANSION_FORM_START ||\n+          this._peek.type === lex.TokenType.EXPANSION_CASE_EXP_START) {\n         expansionFormStack.push(this._peek.type);\n       }\n \n-      if (this._peek.type === TokenType.EXPANSION_CASE_EXP_END) {\n-        if (lastOnStack(expansionFormStack, TokenType.EXPANSION_CASE_EXP_START)) {\n+      if (this._peek.type === lex.TokenType.EXPANSION_CASE_EXP_END) {\n+        if (lastOnStack(expansionFormStack, lex.TokenType.EXPANSION_CASE_EXP_START)) {\n           expansionFormStack.pop();\n           if (expansionFormStack.length == 0) return exp;\n \n@@ -196,8 +195,8 @@ class _TreeBuilder {\n         }\n       }\n \n-      if (this._peek.type === TokenType.EXPANSION_FORM_END) {\n-        if (lastOnStack(expansionFormStack, TokenType.EXPANSION_FORM_START)) {\n+      if (this._peek.type === lex.TokenType.EXPANSION_FORM_END) {\n+        if (lastOnStack(expansionFormStack, lex.TokenType.EXPANSION_FORM_START)) {\n           expansionFormStack.pop();\n         } else {\n           this.errors.push(\n@@ -206,7 +205,7 @@ class _TreeBuilder {\n         }\n       }\n \n-      if (this._peek.type === TokenType.EOF) {\n+      if (this._peek.type === lex.TokenType.EOF) {\n         this.errors.push(\n             TreeError.create(null, start.sourceSpan, `Invalid ICU message. Missing '}'.`));\n         return null;\n@@ -216,7 +215,7 @@ class _TreeBuilder {\n     }\n   }\n \n-  private _consumeText(token: InterpolatedTextToken) {\n+  private _consumeText(token: lex.Token) {\n     const tokens = [token];\n     const startSpan = token.sourceSpan;\n     let text = token.parts[0];\n@@ -225,21 +224,22 @@ class _TreeBuilder {\n       if (parent != null && parent.children.length == 0 &&\n           this.getTagDefinition(parent.name).ignoreFirstLf) {\n         text = text.substring(1);\n-        tokens[0] = {type: token.type, sourceSpan: token.sourceSpan, parts: [text]} as typeof token;\n+        tokens[0] = {type: token.type, sourceSpan: token.sourceSpan, parts: [text]};\n       }\n     }\n \n-    while (this._peek.type === TokenType.INTERPOLATION || this._peek.type === TokenType.TEXT ||\n-           this._peek.type === TokenType.ENCODED_ENTITY) {\n+    while (this._peek.type === lex.TokenType.INTERPOLATION ||\n+           this._peek.type === lex.TokenType.TEXT ||\n+           this._peek.type === lex.TokenType.ENCODED_ENTITY) {\n       token = this._advance();\n       tokens.push(token);\n-      if (token.type === TokenType.INTERPOLATION) {\n+      if (token.type === lex.TokenType.INTERPOLATION) {\n         // For backward compatibility we decode HTML entities that appear in interpolation\n         // expressions. This is arguably a bug, but it could be a considerable breaking change to\n         // fix it. It should be addressed in a larger project to refactor the entire parser/lexer\n         // chain after View Engine has been removed.\n         text += token.parts.join('').replace(/&([^;]+);/g, decodeEntity);\n-      } else if (token.type === TokenType.ENCODED_ENTITY) {\n+      } else if (token.type === lex.TokenType.ENCODED_ENTITY) {\n         text += token.parts[0];\n       } else {\n         text += token.parts.join('');\n@@ -262,17 +262,17 @@ class _TreeBuilder {\n     }\n   }\n \n-  private _consumeStartTag(startTagToken: TagOpenStartToken|IncompleteTagOpenToken) {\n+  private _consumeStartTag(startTagToken: lex.Token) {\n     const [prefix, name] = startTagToken.parts;\n     const attrs: html.Attribute[] = [];\n-    while (this._peek.type === TokenType.ATTR_NAME) {\n-      attrs.push(this._consumeAttr(this._advance<AttributeNameToken>()));\n+    while (this._peek.type === lex.TokenType.ATTR_NAME) {\n+      attrs.push(this._consumeAttr(this._advance()));\n     }\n     const fullName = this._getElementFullName(prefix, name, this._getParentElement());\n     let selfClosing = false;\n     // Note: There could have been a tokenizer error\n     // so that we don't get a token for the end tag...\n-    if (this._peek.type === TokenType.TAG_OPEN_END_VOID) {\n+    if (this._peek.type === lex.TokenType.TAG_OPEN_END_VOID) {\n       this._advance();\n       selfClosing = true;\n       const tagDef = this.getTagDefinition(fullName);\n@@ -281,7 +281,7 @@ class _TreeBuilder {\n             fullName, startTagToken.sourceSpan,\n             `Only void and foreign elements can be self closed \"${startTagToken.parts[1]}\"`));\n       }\n-    } else if (this._peek.type === TokenType.TAG_OPEN_END) {\n+    } else if (this._peek.type === lex.TokenType.TAG_OPEN_END) {\n       this._advance();\n       selfClosing = false;\n     }\n@@ -297,7 +297,7 @@ class _TreeBuilder {\n       // Elements that are self-closed have their `endSourceSpan` set to the full span, as the\n       // element start tag also represents the end tag.\n       this._popElement(fullName, span);\n-    } else if (startTagToken.type === TokenType.INCOMPLETE_TAG_OPEN) {\n+    } else if (startTagToken.type === lex.TokenType.INCOMPLETE_TAG_OPEN) {\n       // We already know the opening tag is not complete, so it is unlikely it has a corresponding\n       // close tag. Let's optimistically parse it as a full element and emit an error.\n       this._popElement(fullName, null);\n@@ -317,7 +317,7 @@ class _TreeBuilder {\n     this._elementStack.push(el);\n   }\n \n-  private _consumeEndTag(endTagToken: TagCloseToken) {\n+  private _consumeEndTag(endTagToken: lex.Token) {\n     const fullName = this._getElementFullName(\n         endTagToken.parts[0], endTagToken.parts[1], this._getParentElement());\n \n@@ -363,40 +363,35 @@ class _TreeBuilder {\n     return false;\n   }\n \n-  private _consumeAttr(attrName: AttributeNameToken): html.Attribute {\n+  private _consumeAttr(attrName: lex.Token): html.Attribute {\n     const fullName = mergeNsAndName(attrName.parts[0], attrName.parts[1]);\n     let attrEnd = attrName.sourceSpan.end;\n \n     // Consume any quote\n-    if (this._peek.type === TokenType.ATTR_QUOTE) {\n+    if (this._peek.type === lex.TokenType.ATTR_QUOTE) {\n       this._advance();\n     }\n \n     // Consume the attribute value\n     let value = '';\n-    const valueTokens: InterpolatedAttributeToken[] = [];\n+    const valueTokens: lex.Token[] = [];\n     let valueStartSpan: ParseSourceSpan|undefined = undefined;\n     let valueEnd: ParseLocation|undefined = undefined;\n-    // NOTE: We need to use a new variable `nextTokenType` here to hide the actual type of\n-    // `_peek.type` from TS. Otherwise TS will narrow the type of `_peek.type` preventing it from\n-    // being able to consider `ATTR_VALUE_INTERPOLATION` as an option. This is because TS is not\n-    // able to see that `_advance()` will actually mutate `_peek`.\n-    const nextTokenType = this._peek.type;\n-    if (nextTokenType === TokenType.ATTR_VALUE_TEXT) {\n+    if (this._peek.type === lex.TokenType.ATTR_VALUE_TEXT) {\n       valueStartSpan = this._peek.sourceSpan;\n       valueEnd = this._peek.sourceSpan.end;\n-      while (this._peek.type === TokenType.ATTR_VALUE_TEXT ||\n-             this._peek.type === TokenType.ATTR_VALUE_INTERPOLATION ||\n-             this._peek.type === TokenType.ENCODED_ENTITY) {\n-        const valueToken = this._advance<InterpolatedAttributeToken>();\n+      while (this._peek.type === lex.TokenType.ATTR_VALUE_TEXT ||\n+             this._peek.type === lex.TokenType.ATTR_VALUE_INTERPOLATION ||\n+             this._peek.type === lex.TokenType.ENCODED_ENTITY) {\n+        const valueToken = this._advance();\n         valueTokens.push(valueToken);\n-        if (valueToken.type === TokenType.ATTR_VALUE_INTERPOLATION) {\n+        if (valueToken.type === lex.TokenType.ATTR_VALUE_INTERPOLATION) {\n           // For backward compatibility we decode HTML entities that appear in interpolation\n           // expressions. This is arguably a bug, but it could be a considerable breaking change to\n           // fix it. It should be addressed in a larger project to refactor the entire parser/lexer\n           // chain after View Engine has been removed.\n           value += valueToken.parts.join('').replace(/&([^;]+);/g, decodeEntity);\n-        } else if (valueToken.type === TokenType.ENCODED_ENTITY) {\n+        } else if (valueToken.type === lex.TokenType.ENCODED_ENTITY) {\n           value += valueToken.parts[0];\n         } else {\n           value += valueToken.parts.join('');\n@@ -406,8 +401,8 @@ class _TreeBuilder {\n     }\n \n     // Consume any quote\n-    if (this._peek.type === TokenType.ATTR_QUOTE) {\n-      const quoteToken = this._advance<AttributeQuoteToken>();\n+    if (this._peek.type === lex.TokenType.ATTR_QUOTE) {\n+      const quoteToken = this._advance();\n       attrEnd = quoteToken.sourceSpan.end;\n     }\n "
        },
        {
            "sha": "643dc25003172830cf941a96091e4d44b9d33f4f",
            "filename": "packages/compiler/src/ml_parser/tokens.ts",
            "status": "removed",
            "additions": 0,
            "deletions": 172,
            "changes": 172,
            "blob_url": "https://github.com/angular/angular/blob/fac6ea5faeace50514ff28f25eac8fee570a982f/packages%2Fcompiler%2Fsrc%2Fml_parser%2Ftokens.ts",
            "raw_url": "https://github.com/angular/angular/raw/fac6ea5faeace50514ff28f25eac8fee570a982f/packages%2Fcompiler%2Fsrc%2Fml_parser%2Ftokens.ts",
            "contents_url": "https://api.github.com/repos/angular/angular/contents/packages%2Fcompiler%2Fsrc%2Fml_parser%2Ftokens.ts?ref=fac6ea5faeace50514ff28f25eac8fee570a982f",
            "patch": "@@ -1,172 +0,0 @@\n-/**\n- * @license\n- * Copyright Google LLC All Rights Reserved.\n- *\n- * Use of this source code is governed by an MIT-style license that can be\n- * found in the LICENSE file at https://angular.io/license\n- */\n-\n-import {ParseSourceSpan} from '../parse_util';\n-\n-export const enum TokenType {\n-  TAG_OPEN_START,\n-  TAG_OPEN_END,\n-  TAG_OPEN_END_VOID,\n-  TAG_CLOSE,\n-  INCOMPLETE_TAG_OPEN,\n-  TEXT,\n-  ESCAPABLE_RAW_TEXT,\n-  RAW_TEXT,\n-  INTERPOLATION,\n-  ENCODED_ENTITY,\n-  COMMENT_START,\n-  COMMENT_END,\n-  CDATA_START,\n-  CDATA_END,\n-  ATTR_NAME,\n-  ATTR_QUOTE,\n-  ATTR_VALUE_TEXT,\n-  ATTR_VALUE_INTERPOLATION,\n-  DOC_TYPE,\n-  EXPANSION_FORM_START,\n-  EXPANSION_CASE_VALUE,\n-  EXPANSION_CASE_EXP_START,\n-  EXPANSION_CASE_EXP_END,\n-  EXPANSION_FORM_END,\n-  EOF\n-}\n-\n-export type Token = TagOpenStartToken|TagOpenEndToken|TagOpenEndVoidToken|TagCloseToken|\n-    IncompleteTagOpenToken|TextToken|InterpolationToken|EncodedEntityToken|CommentStartToken|\n-    CommentEndToken|CdataStartToken|CdataEndToken|AttributeNameToken|AttributeQuoteToken|\n-    AttributeValueTextToken|AttributeValueInterpolationToken|DocTypeToken|ExpansionFormStartToken|\n-    ExpansionCaseValueToken|ExpansionCaseExpressionStartToken|ExpansionCaseExpressionEndToken|\n-    ExpansionFormEndToken|EndOfFileToken;\n-\n-export type InterpolatedTextToken = TextToken|InterpolationToken|EncodedEntityToken;\n-\n-export type InterpolatedAttributeToken =\n-    AttributeValueTextToken|AttributeValueInterpolationToken|EncodedEntityToken;\n-\n-export interface TokenBase {\n-  type: TokenType;\n-  parts: string[];\n-  sourceSpan: ParseSourceSpan;\n-}\n-\n-export interface TagOpenStartToken extends TokenBase {\n-  type: TokenType.TAG_OPEN_START;\n-  parts: [prefix: string, name: string];\n-}\n-\n-export interface TagOpenEndToken extends TokenBase {\n-  type: TokenType.TAG_OPEN_END;\n-  parts: [];\n-}\n-\n-export interface TagOpenEndVoidToken extends TokenBase {\n-  type: TokenType.TAG_OPEN_END_VOID;\n-  parts: [];\n-}\n-\n-export interface TagCloseToken extends TokenBase {\n-  type: TokenType.TAG_CLOSE;\n-  parts: [prefix: string, name: string];\n-}\n-\n-export interface IncompleteTagOpenToken extends TokenBase {\n-  type: TokenType.INCOMPLETE_TAG_OPEN;\n-  parts: [prefix: string, name: string];\n-}\n-\n-export interface TextToken extends TokenBase {\n-  type: TokenType.TEXT|TokenType.ESCAPABLE_RAW_TEXT|TokenType.RAW_TEXT;\n-  parts: [text: string];\n-}\n-\n-export interface InterpolationToken extends TokenBase {\n-  type: TokenType.INTERPOLATION;\n-  parts: [startMarker: string, expression: string, endMarker: string]|\n-      [startMarker: string, expression: string];\n-}\n-\n-export interface EncodedEntityToken extends TokenBase {\n-  type: TokenType.ENCODED_ENTITY;\n-  parts: [decoded: string, encoded: string];\n-}\n-\n-export interface CommentStartToken extends TokenBase {\n-  type: TokenType.COMMENT_START;\n-  parts: [];\n-}\n-\n-export interface CommentEndToken extends TokenBase {\n-  type: TokenType.COMMENT_END;\n-  parts: [];\n-}\n-\n-export interface CdataStartToken extends TokenBase {\n-  type: TokenType.CDATA_START;\n-  parts: [];\n-}\n-\n-export interface CdataEndToken extends TokenBase {\n-  type: TokenType.CDATA_END;\n-  parts: [];\n-}\n-\n-export interface AttributeNameToken extends TokenBase {\n-  type: TokenType.ATTR_NAME;\n-  parts: [prefix: string, name: string];\n-}\n-\n-export interface AttributeQuoteToken extends TokenBase {\n-  type: TokenType.ATTR_QUOTE;\n-  parts: [quote: '\\''|'\"'];\n-}\n-\n-export interface AttributeValueTextToken extends TokenBase {\n-  type: TokenType.ATTR_VALUE_TEXT;\n-  parts: [value: string];\n-}\n-\n-export interface AttributeValueInterpolationToken extends TokenBase {\n-  type: TokenType.ATTR_VALUE_INTERPOLATION;\n-  parts: [startMarker: string, expression: string, endMarker: string]|\n-      [startMarker: string, expression: string];\n-}\n-\n-export interface DocTypeToken extends TokenBase {\n-  type: TokenType.DOC_TYPE;\n-  parts: [content: string];\n-}\n-\n-export interface ExpansionFormStartToken extends TokenBase {\n-  type: TokenType.EXPANSION_FORM_START;\n-  parts: [];\n-}\n-\n-export interface ExpansionCaseValueToken extends TokenBase {\n-  type: TokenType.EXPANSION_CASE_VALUE;\n-  parts: [value: string];\n-}\n-\n-export interface ExpansionCaseExpressionStartToken extends TokenBase {\n-  type: TokenType.EXPANSION_CASE_EXP_START;\n-  parts: [];\n-}\n-\n-export interface ExpansionCaseExpressionEndToken extends TokenBase {\n-  type: TokenType.EXPANSION_CASE_EXP_END;\n-  parts: [];\n-}\n-\n-export interface ExpansionFormEndToken extends TokenBase {\n-  type: TokenType.EXPANSION_FORM_END;\n-  parts: [];\n-}\n-\n-export interface EndOfFileToken extends TokenBase {\n-  type: TokenType.EOF;\n-  parts: [];\n-}"
        },
        {
            "sha": "01562f53cf24581f70b03075b56a27e23491d7d8",
            "filename": "packages/compiler/test/ml_parser/html_parser_spec.ts",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/angular/angular/blob/8b6f7ac36b03d6e5565f989ee3d8ee046b0f2f72/packages%2Fcompiler%2Ftest%2Fml_parser%2Fhtml_parser_spec.ts",
            "raw_url": "https://github.com/angular/angular/raw/8b6f7ac36b03d6e5565f989ee3d8ee046b0f2f72/packages%2Fcompiler%2Ftest%2Fml_parser%2Fhtml_parser_spec.ts",
            "contents_url": "https://api.github.com/repos/angular/angular/contents/packages%2Fcompiler%2Ftest%2Fml_parser%2Fhtml_parser_spec.ts?ref=8b6f7ac36b03d6e5565f989ee3d8ee046b0f2f72",
            "patch": "@@ -8,7 +8,7 @@\n \n import * as html from '../../src/ml_parser/ast';\n import {HtmlParser, ParseTreeResult, TreeError} from '../../src/ml_parser/html_parser';\n-import {TokenType} from '../../src/ml_parser/tokens';\n+import {TokenType} from '../../src/ml_parser/lexer';\n import {ParseError} from '../../src/parse_util';\n \n import {humanizeDom, humanizeDomSourceSpans, humanizeLineColumn, humanizeNodes} from './ast_spec_utils';"
        },
        {
            "sha": "c0302a3345493ae843130b0208147e1c800986d0",
            "filename": "packages/compiler/test/ml_parser/lexer_spec.ts",
            "status": "modified",
            "additions": 821,
            "deletions": 818,
            "changes": 1639,
            "blob_url": "https://github.com/angular/angular/blob/8b6f7ac36b03d6e5565f989ee3d8ee046b0f2f72/packages%2Fcompiler%2Ftest%2Fml_parser%2Flexer_spec.ts",
            "raw_url": "https://github.com/angular/angular/raw/8b6f7ac36b03d6e5565f989ee3d8ee046b0f2f72/packages%2Fcompiler%2Ftest%2Fml_parser%2Flexer_spec.ts",
            "contents_url": "https://api.github.com/repos/angular/angular/contents/packages%2Fcompiler%2Ftest%2Fml_parser%2Flexer_spec.ts?ref=8b6f7ac36b03d6e5565f989ee3d8ee046b0f2f72"
        }
    ],
    "stats": {
        "total": 2058,
        "additions": 955,
        "deletions": 1103
    }
}