{
    "author": "gkalpak",
    "message": "build: update stemmer to version 2.0.0 (#41724)\n\nNOTE:\n`stemmer` v2.0.0 switched to ES modules (see\nwords/stemmer@03519229c84555a38d3256409d1871edfd43528f), which means\nthat the only way to consume it in our CommonJS setup (for example, in\n[generateKeywords][1]) is via an async `import()`.\n\nThis commit makes the `generateKeywords` processor asynchronous in order\nto be able to dynamically import and use `stemmer`.\n\n[1]: https://github.com/angular/angular/blob/251bec159af1e8f90c3af9820695c645800207e4/aio/tools/transforms/angular-base-package/processors/generateKeywords.js\n\nPR Close #41724",
    "sha": "99f2ffc740279509b89bb96093e2515ce0b46ae1",
    "files": [
        {
            "sha": "c47e2fb9d0bd33bdaf88d7a37b37cec4dec871b3",
            "filename": "aio/package.json",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/angular/angular/blob/99f2ffc740279509b89bb96093e2515ce0b46ae1/aio%2Fpackage.json",
            "raw_url": "https://github.com/angular/angular/raw/99f2ffc740279509b89bb96093e2515ce0b46ae1/aio%2Fpackage.json",
            "contents_url": "https://api.github.com/repos/angular/angular/contents/aio%2Fpackage.json?ref=99f2ffc740279509b89bb96093e2515ce0b46ae1",
            "patch": "@@ -162,7 +162,7 @@\n     \"rimraf\": \"^3.0.2\",\n     \"semver\": \"^7.3.5\",\n     \"shelljs\": \"^0.8.4\",\n-    \"stemmer\": \"^1.0.5\",\n+    \"stemmer\": \"^2.0.0\",\n     \"timezone-mock\": \"^1.1.3\",\n     \"tree-kill\": \"^1.1.0\",\n     \"ts-node\": \"^9.1.1\","
        },
        {
            "sha": "473f5a49da6d90c79542f82c216c13b7d9cdb628",
            "filename": "aio/tools/transforms/.eslintrc.js",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/angular/angular/blob/99f2ffc740279509b89bb96093e2515ce0b46ae1/aio%2Ftools%2Ftransforms%2F.eslintrc.js",
            "raw_url": "https://github.com/angular/angular/raw/99f2ffc740279509b89bb96093e2515ce0b46ae1/aio%2Ftools%2Ftransforms%2F.eslintrc.js",
            "contents_url": "https://api.github.com/repos/angular/angular/contents/aio%2Ftools%2Ftransforms%2F.eslintrc.js?ref=99f2ffc740279509b89bb96093e2515ce0b46ae1",
            "patch": "@@ -8,6 +8,9 @@ module.exports = {\n     'eslint:recommended',\n     'plugin:jasmine/recommended'\n   ],\n+  'parserOptions': {\n+    'ecmaVersion': 2020,\n+  },\n   'plugins': [\n     'jasmine'\n   ],"
        },
        {
            "sha": "dfe430c7051ffe63398e49d75c3323b9acf6b89d",
            "filename": "aio/tools/transforms/angular-base-package/processors/generateKeywords.js",
            "status": "modified",
            "additions": 73,
            "deletions": 70,
            "changes": 143,
            "blob_url": "https://github.com/angular/angular/blob/99f2ffc740279509b89bb96093e2515ce0b46ae1/aio%2Ftools%2Ftransforms%2Fangular-base-package%2Fprocessors%2FgenerateKeywords.js",
            "raw_url": "https://github.com/angular/angular/raw/99f2ffc740279509b89bb96093e2515ce0b46ae1/aio%2Ftools%2Ftransforms%2Fangular-base-package%2Fprocessors%2FgenerateKeywords.js",
            "contents_url": "https://api.github.com/repos/angular/angular/contents/aio%2Ftools%2Ftransforms%2Fangular-base-package%2Fprocessors%2FgenerateKeywords.js?ref=99f2ffc740279509b89bb96093e2515ce0b46ae1",
            "patch": "@@ -1,7 +1,5 @@\n 'use strict';\n \n-const stem = require('stemmer');\n-\n /**\n  * @dgProcessor generateKeywordsProcessor\n  * @description\n@@ -23,7 +21,9 @@ module.exports = function generateKeywordsProcessor(log) {\n     },\n     $runAfter: ['postProcessHtml'],\n     $runBefore: ['writing-files'],\n-    $process(docs) {\n+    async $process(docs) {\n+      const {stemmer: stem} = await import('stemmer');\n+\n \n       const dictionary = new Map();\n \n@@ -110,78 +110,81 @@ module.exports = function generateKeywordsProcessor(log) {\n         data: searchData,\n         renderedContent: JSON.stringify(searchData)\n       });\n-    }\n-  };\n-};\n \n-function isString(value) {\n-  return typeof value == 'string';\n-}\n+      return docs;\n+\n+      // Helpers\n+      function tokenize(text, ignoreWords, dictionary) {\n+        // Split on whitespace and things that are likely to be HTML tags (this is not exhaustive but reduces the unwanted tokens that are indexed).\n+        const rawTokens = text.split(new RegExp(\n+                                            '[\\\\s/]+' +                                // whitespace\n+                                            '|' +                                      // or\n+                                            '</?[a-z]+(?:\\\\s+\\\\w+(?:=\"[^\"]+\")?)*/?>',  // simple HTML tags (e.g. <td>, <hr/>, </table>, etc.)\n+                                            'ig'));\n+        const tokens = [];\n+        for (let token of rawTokens) {\n+          token = token.trim();\n+\n+          // Trim unwanted trivia characters from the start and end of the token\n+          const TRIVIA_CHARS = '[\\\\s_\"\\'`({[<$*)}\\\\]>.,-]';\n+          // Tokens can contain letters, numbers, underscore, dot or hyphen but not at the start or end.\n+          // The leading TRIVIA_CHARS will capture any leading `.`, '-`' or `_` so we don't have to avoid them in this regular expression.\n+          // But we do need to ensure we don't capture the at the end of the token.\n+          const POSSIBLE_TOKEN = '[a-z0-9_.-]*[a-z0-9]';\n+          token = token.replace(new RegExp(`^${TRIVIA_CHARS}*(${POSSIBLE_TOKEN})${TRIVIA_CHARS}*$`, 'i'), '$1');\n+\n+          // Skip if blank or in the ignored words list\n+          if (token === '' || ignoreWords.has(token.toLowerCase())) {\n+            continue;\n+          }\n \n-function tokenize(text, ignoreWords, dictionary) {\n-  // Split on whitespace and things that are likely to be HTML tags (this is not exhaustive but reduces the unwanted tokens that are indexed).\n-  const rawTokens = text.split(new RegExp(\n-                                      '[\\\\s/]+' +                                // whitespace\n-                                      '|' +                                      // or\n-                                      '</?[a-z]+(?:\\\\s+\\\\w+(?:=\"[^\"]+\")?)*/?>',  // simple HTML tags (e.g. <td>, <hr/>, </table>, etc.)\n-                                      'ig'));\n-  const tokens = [];\n-  for (let token of rawTokens) {\n-    token = token.trim();\n-\n-    // Trim unwanted trivia characters from the start and end of the token\n-    const TRIVIA_CHARS = '[\\\\s_\"\\'`({[<$*)}\\\\]>.,-]';\n-    // Tokens can contain letters, numbers, underscore, dot or hyphen but not at the start or end.\n-    // The leading TRIVIA_CHARS will capture any leading `.`, '-`' or `_` so we don't have to avoid them in this regular expression.\n-    // But we do need to ensure we don't capture the at the end of the token.\n-    const POSSIBLE_TOKEN = '[a-z0-9_.-]*[a-z0-9]';\n-    token = token.replace(new RegExp(`^${TRIVIA_CHARS}*(${POSSIBLE_TOKEN})${TRIVIA_CHARS}*$`, 'i'), '$1');\n-\n-    // Skip if blank or in the ignored words list\n-    if (token === '' || ignoreWords.has(token.toLowerCase())) {\n-      continue;\n-    }\n+          // Skip tokens that contain weird characters\n+          if (!/^\\w[\\w.-]*$/.test(token)) {\n+            continue;\n+          }\n \n-    // Skip tokens that contain weird characters\n-    if (!/^\\w[\\w.-]*$/.test(token)) {\n-      continue;\n-    }\n+          storeToken(token, tokens, dictionary);\n+          if (token.startsWith('ng')) {\n+            // Strip off `ng`, `ng-`, `ng1`, `ng2`, etc\n+            storeToken(token.replace(/^ng[-12]*/, ''), tokens, dictionary);\n+          }\n+        }\n \n-    storeToken(token, tokens, dictionary);\n-    if (token.startsWith('ng')) {\n-      // Strip off `ng`, `ng-`, `ng1`, `ng2`, etc\n-      storeToken(token.replace(/^ng[-12]*/, ''), tokens, dictionary);\n-    }\n-  }\n+        return tokens;\n+      }\n \n-  return tokens;\n-}\n+      function storeToken(token, tokens, dictionary) {\n+        token = stem(token);\n+        if (!dictionary.has(token)) {\n+          dictionary.set(token, dictionary.size);\n+        }\n+        tokens.push(dictionary.get(token));\n+      }\n \n-function storeToken(token, tokens, dictionary) {\n-  token = stem(token);\n-  if (!dictionary.has(token)) {\n-    dictionary.set(token, dictionary.size);\n-  }\n-  tokens.push(dictionary.get(token));\n-}\n+      function extractMemberTokens(doc, ignoreWords, dictionary) {\n+        if (!doc) return [];\n+\n+        let memberContent = [];\n+\n+        if (doc.members) {\n+          doc.members.forEach(member => memberContent.push(...tokenize(member.name, ignoreWords, dictionary)));\n+        }\n+        if (doc.statics) {\n+          doc.statics.forEach(member => memberContent.push(...tokenize(member.name, ignoreWords, dictionary)));\n+        }\n+        if (doc.extendsClauses) {\n+          doc.extendsClauses.forEach(clause => memberContent.push(...extractMemberTokens(clause.doc, ignoreWords, dictionary)));\n+        }\n+        if (doc.implementsClauses) {\n+          doc.implementsClauses.forEach(clause => memberContent.push(...extractMemberTokens(clause.doc, ignoreWords, dictionary)));\n+        }\n+\n+        return memberContent;\n+      }\n+    }\n+  };\n+};\n \n-function extractMemberTokens(doc, ignoreWords, dictionary) {\n-  if (!doc) return [];\n-\n-  let memberContent = [];\n-\n-  if (doc.members) {\n-    doc.members.forEach(member => memberContent.push(...tokenize(member.name, ignoreWords, dictionary)));\n-  }\n-  if (doc.statics) {\n-    doc.statics.forEach(member => memberContent.push(...tokenize(member.name, ignoreWords, dictionary)));\n-  }\n-  if (doc.extendsClauses) {\n-    doc.extendsClauses.forEach(clause => memberContent.push(...extractMemberTokens(clause.doc, ignoreWords, dictionary)));\n-  }\n-  if (doc.implementsClauses) {\n-    doc.implementsClauses.forEach(clause => memberContent.push(...extractMemberTokens(clause.doc, ignoreWords, dictionary)));\n-  }\n-\n-  return memberContent;\n+function isString(value) {\n+  return typeof value == 'string';\n }"
        },
        {
            "sha": "d250eea7eed3578c2ed67f1d0efadc084a09859a",
            "filename": "aio/tools/transforms/angular-base-package/processors/generateKeywords.spec.js",
            "status": "modified",
            "additions": 32,
            "deletions": 43,
            "changes": 75,
            "blob_url": "https://github.com/angular/angular/blob/99f2ffc740279509b89bb96093e2515ce0b46ae1/aio%2Ftools%2Ftransforms%2Fangular-base-package%2Fprocessors%2FgenerateKeywords.spec.js",
            "raw_url": "https://github.com/angular/angular/raw/99f2ffc740279509b89bb96093e2515ce0b46ae1/aio%2Ftools%2Ftransforms%2Fangular-base-package%2Fprocessors%2FgenerateKeywords.spec.js",
            "contents_url": "https://api.github.com/repos/angular/angular/contents/aio%2Ftools%2Ftransforms%2Fangular-base-package%2Fprocessors%2FgenerateKeywords.spec.js?ref=99f2ffc740279509b89bb96093e2515ce0b46ae1",
            "patch": "@@ -36,42 +36,39 @@ describe('generateKeywords processor', () => {\n     expect(processor.$runBefore).toEqual(['writing-files']);\n   });\n \n-  it('should ignore internal and private exports', () => {\n+  it('should ignore internal and private exports', async () => {\n     const processor = createProcessor();\n-    const docs = [\n+    const docs = await processor.$process([\n       { docType: 'class', name: 'PublicExport' },\n       { docType: 'class', name: 'PrivateExport', privateExport: true },\n       { docType: 'class', name: 'InternalExport', internal: true }\n-    ];\n-    processor.$process(docs);\n+    ]);\n     expect(docs[docs.length - 1].data.pages).toEqual([\n       jasmine.objectContaining({ title: 'PublicExport', type: 'class' })\n     ]);\n   });\n \n-  it('should ignore docs that are in the `docTypesToIgnore` list', () => {\n+  it('should ignore docs that are in the `docTypesToIgnore` list', async () => {\n     const processor = createProcessor();\n     processor.docTypesToIgnore = ['interface'];\n-    const docs = [\n+    const docs = await processor.$process([\n       { docType: 'class', name: 'Class' },\n       { docType: 'interface', name: 'Interface' },\n       { docType: 'content', name: 'Guide' },\n-    ];\n-    processor.$process(docs);\n+    ]);\n     expect(docs[docs.length - 1].data.pages).toEqual([\n       jasmine.objectContaining({ title: 'Class', type: 'class' }),\n       jasmine.objectContaining({ title: 'Guide', type: 'content' }),\n     ]);\n   });\n \n-  it('should not collect keywords from properties that are in the `propertiesToIgnore` list', () => {\n+  it('should not collect keywords from properties that are in the `propertiesToIgnore` list', async () => {\n     const processor = createProcessor();\n     processor.propertiesToIgnore = ['docType', 'ignore'];\n-    const docs = [\n+    const docs = await processor.$process([\n       { docType: 'class', name: 'FooClass', ignore: 'ignore this content' },\n       { docType: 'interface', name: 'BarInterface', capture: 'capture this content' },\n-    ];\n-    processor.$process(docs);\n+    ]);\n     expect(docs[docs.length - 1].data).toEqual({\n       dictionary: 'fooclass barinterfac captur content',\n       pages: [\n@@ -81,17 +78,16 @@ describe('generateKeywords processor', () => {\n     });\n   });\n \n-  it('should not collect keywords that look like HTML tags', () => {\n+  it('should not collect keywords that look like HTML tags', async () => {\n     const processor = createProcessor();\n-    const docs = [\n+    const docs = await processor.$process([\n       { docType: 'class', name: 'FooClass', content: `\n       <table id=\"foo\">\n         <tr class=\"moo\" id=\"bar\">\n           <td>Content inside a table</td>\n         </tr>\n       </table>` },\n-    ];\n-    processor.$process(docs);\n+    ]);\n     expect(docs[docs.length - 1].data).toEqual({\n       dictionary: 'class fooclass content insid tabl',\n       pages: [\n@@ -100,15 +96,14 @@ describe('generateKeywords processor', () => {\n     });\n   });\n \n-  it('should compute `doc.searchTitle` from the doc properties if not already provided', () => {\n+  it('should compute `doc.searchTitle` from the doc properties if not already provided', async () => {\n     const processor = createProcessor();\n-    const docs = [\n+    const docs = await processor.$process([\n       { docType: 'class', name: 'A', searchTitle: 'searchTitle A', title: 'title A', vFile: { headings: { h1: ['vFile A'] } } },\n       { docType: 'class', name: 'B', title: 'title B', vFile: { headings: { h1: ['vFile B'] } } },\n       { docType: 'class', name: 'C', vFile: { title: 'vFile C', headings: { h1: ['vFile C'] } } },\n       { docType: 'class', name: 'D' },\n-    ];\n-    processor.$process(docs);\n+    ]);\n     expect(docs[docs.length - 1].data.pages).toEqual([\n       jasmine.objectContaining({ title: 'searchTitle A' }),\n       jasmine.objectContaining({ title: 'title B' }),\n@@ -117,29 +112,27 @@ describe('generateKeywords processor', () => {\n     ]);\n   });\n \n-  it('should use `doc.searchTitle` as the title in the search index', () => {\n+  it('should use `doc.searchTitle` as the title in the search index', async () => {\n     const processor = createProcessor();\n-    const docs = [\n+    const docs = await processor.$process([\n       { docType: 'class', name: 'PublicExport', searchTitle: 'class PublicExport' },\n-    ];\n-    processor.$process(docs);\n+    ]);\n     const keywordsDoc = docs[docs.length - 1];\n     expect(keywordsDoc.data.pages).toEqual([\n       jasmine.objectContaining({ title: 'class PublicExport', type: 'class' })\n     ]);\n   });\n \n-  it('should add heading words to the search terms', () => {\n+  it('should add heading words to the search terms', async () => {\n     const processor = createProcessor();\n-    const docs = [\n+    const docs = await processor.$process([\n       {\n         docType: 'class',\n         name: 'PublicExport',\n         searchTitle: 'class PublicExport',\n         vFile: { headings: { h2: ['Important heading', 'Secondary heading'] } }\n       },\n-    ];\n-    processor.$process(docs);\n+    ]);\n     const keywordsDoc = docs[docs.length - 1];\n     expect(keywordsDoc.data).toEqual({\n       dictionary: 'class publicexport head secondari',\n@@ -149,9 +142,9 @@ describe('generateKeywords processor', () => {\n     });\n   });\n \n-  it('should add member doc properties to the search terms', () => {\n+  it('should add member doc properties to the search terms', async () => {\n     const processor = createProcessor();\n-    const docs = [\n+    const docs = await processor.$process([\n       {\n         docType: 'class',\n         name: 'PublicExport',\n@@ -171,8 +164,7 @@ describe('generateKeywords processor', () => {\n           { name: 'staticPropertyB' },\n         ],\n       },\n-    ];\n-    processor.$process(docs);\n+    ]);\n     const keywordsDoc = docs[docs.length - 1];\n     expect(keywordsDoc.data).toEqual({\n       dictionary: 'class publicexport content ngclass instancemethoda instancepropertya instancemethodb instancepropertyb staticmethoda staticpropertya staticmethodb staticpropertyb head',\n@@ -184,7 +176,7 @@ describe('generateKeywords processor', () => {\n     });\n   });\n \n-  it('should add inherited member doc properties to the search terms', () => {\n+  it('should add inherited member doc properties to the search terms', async () => {\n     const processor = createProcessor();\n     const parentClass =       {\n       docType: 'class',\n@@ -216,8 +208,7 @@ describe('generateKeywords processor', () => {\n       extendsClauses: [{ doc: parentClass }],\n       implementsClauses: [{ doc: parentInterface }]\n     };\n-    const docs = [childClass, parentClass, parentInterface];\n-    processor.$process(docs);\n+    const docs = await processor.$process([childClass, parentClass, parentInterface]);\n     const keywordsDoc = docs[docs.length - 1];\n     expect(keywordsDoc.data).toEqual({\n       dictionary: 'class child childmember1 childmember2 parentmember1 parentmember2 parentmember3 parentclass interfac parentinterfac',\n@@ -238,18 +229,17 @@ describe('generateKeywords processor', () => {\n     });\n   });\n \n-  it('should include both stripped and unstripped \"ng\" prefixed tokens', () => {\n+  it('should include both stripped and unstripped \"ng\" prefixed tokens', async () => {\n     const processor = createProcessor();\n-    const docs = [\n+    const docs = await processor.$process([\n       {\n         docType: 'class',\n         name: 'PublicExport',\n         searchTitle: 'ngController',\n         vFile: { headings: { h2: ['ngModel'] } },\n         content: 'Some content with ngClass in it.'\n       },\n-    ];\n-    processor.$process(docs);\n+    ]);\n     const keywordsDoc = docs[docs.length - 1];\n     expect(keywordsDoc.data).toEqual({\n       dictionary: 'class publicexport ngcontrol control content ngclass ngmodel model',\n@@ -262,9 +252,9 @@ describe('generateKeywords processor', () => {\n     });\n   });\n \n-  it('should generate compressed encoded renderedContent property', () => {\n+  it('should generate compressed encoded renderedContent property', async () => {\n     const processor = createProcessor();\n-    const docs = [\n+    const docs = await processor.$process([\n       {\n         docType: 'class',\n         name: 'SomeClass',\n@@ -280,8 +270,7 @@ describe('generateKeywords processor', () => {\n         ],\n         deprecated: true\n       },\n-    ];\n-    processor.$process(docs);\n+    ]);\n     const keywordsDoc = docs[docs.length - 1];\n     expect(JSON.parse(keywordsDoc.renderedContent)).toEqual({\n       dictionary: 'class someclass document api head someclass2 descript member1',"
        },
        {
            "sha": "505d44dad9a6d209a5772ec66e177e4ebe392119",
            "filename": "aio/yarn.lock",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/angular/angular/blob/99f2ffc740279509b89bb96093e2515ce0b46ae1/aio%2Fyarn.lock",
            "raw_url": "https://github.com/angular/angular/raw/99f2ffc740279509b89bb96093e2515ce0b46ae1/aio%2Fyarn.lock",
            "contents_url": "https://api.github.com/repos/angular/angular/contents/aio%2Fyarn.lock?ref=99f2ffc740279509b89bb96093e2515ce0b46ae1",
            "patch": "@@ -11955,10 +11955,10 @@ stealthy-require@^1.1.1:\n   resolved \"https://registry.yarnpkg.com/stealthy-require/-/stealthy-require-1.1.1.tgz#35b09875b4ff49f26a777e509b3090a3226bf24b\"\n   integrity sha1-NbCYdbT/SfJqd35QmzCQoyJr8ks=\n \n-stemmer@^1.0.5:\n-  version \"1.0.5\"\n-  resolved \"https://registry.yarnpkg.com/stemmer/-/stemmer-1.0.5.tgz#fd89beaf8bff5d04b6643bfffcaed0fc420deec0\"\n-  integrity sha512-SLq7annzSKRDStasOJJoftCSCzBCKmBmH38jC4fDtCunAqOzpTpIm9zmaHmwNJiZ8gLe9qpVdBVbEG2DC5dE2A==\n+stemmer@^2.0.0:\n+  version \"2.0.0\"\n+  resolved \"https://registry.yarnpkg.com/stemmer/-/stemmer-2.0.0.tgz#05fcaf174c423b0fec85e660759ebd4867d811c9\"\n+  integrity sha512-0YS2oMdTZ/wAWUHMMpf7AAJ8Gm6dHXyHddJ0zCu2DIfOfIbdwqAm1bbk4+Vti6gxNIcOrnm5jAP7vYTzQDvc5A==\n \n stream-browserify@^2.0.1:\n   version \"2.0.2\""
        }
    ],
    "stats": {
        "total": 231,
        "additions": 113,
        "deletions": 118
    }
}