{
    "author": "sokra",
    "message": "[Turbopack] refactor persistent caching from log based to cow approach (#76234)\n\n### What?\n\nInstead of collecting all modifications to the task graph in a log\nstructure (which ends up taking a lot of memory), use a modified and\nsnapshot flag to avoid using more memory than needed.\n\nThis way we don't store extra memory before a snapshot is requested.\nOnce a snapshot is captured, we store at most twice the memory. We store\nonly the memory of the modified tasks.\n\nIt works this way:\n\nIdle Phase: (before a snapshot is requested)\n\n* When a task is modified, we only set the modified flag on that task.\n\nCapturing a snapshot want to avoid locking all tasks. So it first\nswitches to the Snapshot Phase.\n\nSnapshot Phase: (after a snapshot is captured)\n\n* When a task is modified:\n  * If there is already a snapshot, skip\n* If the task has the modified flag set, clone the task and store the\nsnapshot.\n* If the task doesn't have the modified flag set, store a None snapshot.\n\nTo actually capture the snapshot, go over all modified or snapshot tasks\n(which is stored in a separate map):\n\n* If the task has a snapshot (not None) stored, use that.\n* If the task has only the modified flag set, read the task state and\ncapture a snapshot from that.\n* There is a race condition here, when the task is modified inbetween.\nWe handle that by having a snapshot flag too.\n\nWe have all snapshot captured now. To leave Snapshot Phase, we \n\n* First unset all modified flags.\n* Switch to Idle Phase.\n* Change all stored snapshots to modified.",
    "sha": "0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9",
    "files": [
        {
            "sha": "ae70994b432b154d92c2f99bcc93027682895fc8",
            "filename": "Cargo.lock",
            "status": "modified",
            "additions": 100,
            "deletions": 79,
            "changes": 179,
            "blob_url": "https://github.com/vercel/next.js/blob/0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9/Cargo.lock",
            "raw_url": "https://github.com/vercel/next.js/raw/0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9/Cargo.lock",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/Cargo.lock?ref=0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9",
            "patch": "@@ -259,7 +259,7 @@ checksum = \"0ae92a5119aa49cdbcf6b9f893fe4e1d98b04ccbf82ee0584ad948a44a734dea\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -305,7 +305,7 @@ dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n  \"swc_macros_common\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -538,7 +538,7 @@ checksum = \"644dd749086bf3771a2fbc5f256fdb982d53f011c7d5d560304eafeecebce79d\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -588,7 +588,7 @@ checksum = \"3c87f3f15e7794432337fc718554eaa4dc8f04c9677a950ffe366f20a162ae42\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -745,7 +745,7 @@ dependencies = [\n  \"regex\",\n  \"rustc-hash 1.1.0\",\n  \"shlex\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -769,6 +769,26 @@ dependencies = [\n  \"wasm-bindgen-futures\",\n ]\n \n+[[package]]\n+name = \"bitfield\"\n+version = \"0.18.1\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"4c7e6caee68becd795bfd65f1a026e4d00d8f0c2bc9be5eb568e1015f9ce3c34\"\n+dependencies = [\n+ \"bitfield-macros\",\n+]\n+\n+[[package]]\n+name = \"bitfield-macros\"\n+version = \"0.18.1\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"331afbb18ce7b644c0b428726d369c5dd37ca0b815d72a459fcc2896c3c8ad32\"\n+dependencies = [\n+ \"proc-macro2\",\n+ \"quote\",\n+ \"syn 2.0.100\",\n+]\n+\n [[package]]\n name = \"bitflags\"\n version = \"1.3.2\"\n@@ -938,7 +958,7 @@ checksum = \"523363cbe1df49b68215efdf500b103ac3b0fb4836aed6d15689a076eadb8fff\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -1252,7 +1272,7 @@ dependencies = [\n  \"heck 0.4.1\",\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -1774,7 +1794,7 @@ source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"13b588ba4ac1a99f7f2964d24b3d896ddc6bf847ee3855dbd4366f058cfcd331\"\n dependencies = [\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -1784,7 +1804,7 @@ source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"dd4056f63fce3b82d852c3da92b08ea59959890813a7f4ce9c0ff85b10cf301b\"\n dependencies = [\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -1873,7 +1893,7 @@ dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n  \"strsim 0.11.1\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -1895,7 +1915,7 @@ checksum = \"d336a2a514f6ccccaa3e09b02d41d35330c07ddf03a62165fcec10bb561c7806\"\n dependencies = [\n  \"darling_core 0.20.10\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -2010,7 +2030,7 @@ dependencies = [\n  \"darling 0.20.10\",\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -2030,7 +2050,7 @@ source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"ab63b0e2bf4d5928aff72e83a7dace85d7bba5fe12dcc3c5a572d78caffd3f3c\"\n dependencies = [\n  \"derive_builder_core 0.20.2\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -2041,7 +2061,7 @@ checksum = \"5f33878137e4dafd7fa914ad4e259e18a4e8e532b9617a2d0150262bf53abfce\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -2061,7 +2081,7 @@ checksum = \"cb7330aeadfbe296029522e6c40f315320aba36fc43a5b3632f3795348f3bd22\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n  \"unicode-xid\",\n ]\n \n@@ -2112,7 +2132,7 @@ checksum = \"97369cbbc041bc366949bc74d34658d6cda5621039731c6310521892a3a20ae0\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -2228,7 +2248,7 @@ dependencies = [\n  \"darling 0.20.10\",\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -2403,7 +2423,7 @@ checksum = \"8d7ccf961415e7aa17ef93dcb6c2441faaa8e768abe09e659b908089546f74c5\"\n dependencies = [\n  \"proc-macro2\",\n  \"swc_macros_common\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -2511,7 +2531,7 @@ checksum = \"162ee34ebcb7c64a8abebc059ce0fee27c2262618d7b60ed8faf72fef13c3650\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -3222,7 +3242,7 @@ checksum = \"1ec89e9337638ecdc08744df490b221a7399bf8d164eb52a665454e60e075ad6\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -3430,7 +3450,7 @@ checksum = \"c34819042dc3d3971c46c2190835914dfbe0c3c13f61449b2997f4e9722dfa60\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -3468,7 +3488,7 @@ dependencies = [\n  \"Inflector\",\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -3655,7 +3675,7 @@ dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n  \"regex\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -4177,7 +4197,7 @@ checksum = \"dcf09caffaac8068c346b6df2a7fc27a177fd20b39421a39ce0a211bde679a6c\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -4311,7 +4331,7 @@ checksum = \"1bb5c1d8184f13f7d0ccbeeca0def2f9a181bce2624302793005f5ca8aa62e5e\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -4348,7 +4368,7 @@ dependencies = [\n  \"napi-derive-backend\",\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -4363,7 +4383,7 @@ dependencies = [\n  \"quote\",\n  \"regex\",\n  \"semver 1.0.23\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -4750,7 +4770,7 @@ checksum = \"9e6a0fd4f737c707bd9086cc16c925f294943eb62eb71499e9fd4cf71f8b9f4e\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -4944,7 +4964,7 @@ dependencies = [\n  \"proc-macro-error\",\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -5135,7 +5155,7 @@ dependencies = [\n  \"pest_meta\",\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -5211,7 +5231,7 @@ dependencies = [\n  \"phf_shared\",\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -5240,7 +5260,7 @@ checksum = \"39407670928234ebc5e6e580247dd567ad73a3578460c5990f9503df207e8f07\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -5308,7 +5328,7 @@ checksum = \"52a40bc70c2c58040d2d8b167ba9a5ff59fc9dab7ad44771cfde3dcfde7a09c6\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -5439,7 +5459,7 @@ source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"483f8c21f64f3ea09fe0f30f5d48c3e8eefe5dac9129f0075f76593b4c1da705\"\n dependencies = [\n  \"proc-macro2\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -5495,14 +5515,14 @@ dependencies = [\n  \"proc-macro-error-attr2\",\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n name = \"proc-macro2\"\n-version = \"1.0.92\"\n+version = \"1.0.94\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"37d3544b3f2748c54e147655edb5025752e2303145b5aefb3c3ea2c78b973bb0\"\n+checksum = \"a31971752e70b8b2686d7e46ec17fb38dad4051d94024c88df49b667caea9c84\"\n dependencies = [\n  \"unicode-ident\",\n ]\n@@ -5523,7 +5543,7 @@ source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"8021cf59c8ec9c432cfc2526ac6b8aa508ecaf29cd415f271b8406c1b851c3fd\"\n dependencies = [\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -5546,7 +5566,7 @@ dependencies = [\n  \"itertools 0.13.0\",\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -5604,7 +5624,7 @@ checksum = \"ca414edb151b4c8d125c12566ab0d74dc9cdba36fb80eb7b848c15f495fd32d1\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -5658,9 +5678,9 @@ dependencies = [\n \n [[package]]\n name = \"quote\"\n-version = \"1.0.36\"\n+version = \"1.0.40\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"0fa76aaf39101c457836aec0ce2316dbdc3ab723cdda1c6bd4e6ad4208acaca7\"\n+checksum = \"1885c039570dc00dcb4ff087a89e185fd56bae234ddc7f056a945bf36467248d\"\n dependencies = [\n  \"proc-macro2\",\n ]\n@@ -5867,7 +5887,7 @@ checksum = \"7f7473c2cfcf90008193dd0e3e16599455cb601a9fce322b5bb55de799664925\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -6130,7 +6150,7 @@ checksum = \"beb382a4d9f53bd5c0be86b10d8179c3f8a14c30bf774ff77096ed6581e35981\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -6380,7 +6400,7 @@ dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n  \"serde_derive_internals\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -6519,7 +6539,7 @@ checksum = \"5a9bf7cf98d04a2b28aead066b7496853d4779c9cc183c440dbac457641e19a0\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -6530,7 +6550,7 @@ checksum = \"18d26a20a969b9e3fdf2fc2d9f21eda6c40e2de84c9408bb5d3b05d499aae711\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -6585,7 +6605,7 @@ checksum = \"6c64451ba24fc7a6a2d60fc75dd9c83c90903b19028d4eff35e88fc1e86564e9\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -6645,7 +6665,7 @@ dependencies = [\n  \"darling 0.20.10\",\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -6730,7 +6750,7 @@ checksum = \"16d9bafdb4ba0cafd45a5aea7e8bc35b0f6280a603795c2ba9a823ca6afaba73\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -6960,7 +6980,7 @@ dependencies = [\n  \"pmutil\",\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -7000,7 +7020,7 @@ dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n  \"swc_macros_common\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -7307,7 +7327,7 @@ dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n  \"swc_macros_common\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -7385,7 +7405,7 @@ dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n  \"swc_macros_common\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -7537,7 +7557,7 @@ dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n  \"swc_macros_common\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -7897,7 +7917,7 @@ dependencies = [\n  \"swc_ecma_ast\",\n  \"swc_ecma_parser\",\n  \"swc_macros_common\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -8020,7 +8040,7 @@ dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n  \"swc_macros_common\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -8257,7 +8277,7 @@ checksum = \"e96e15288bf385ab85eb83cff7f9e2d834348da58d0a31b33bdb572e66ee413e\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -8297,7 +8317,7 @@ checksum = \"a509f56fca05b39ba6c15f3e58636c3924c78347d63853632ed2ffcb6f5a0ac7\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -8334,7 +8354,7 @@ checksum = \"ace467dfafbbdf3aecff786b8605b35db57d945e92fd88800569aa2cba0cdf61\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -8418,7 +8438,7 @@ checksum = \"4c78717a841565df57f811376a3d19c9156091c55175e12d378f3a522de70cef\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -8475,9 +8495,9 @@ dependencies = [\n \n [[package]]\n name = \"syn\"\n-version = \"2.0.95\"\n+version = \"2.0.100\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"46f71c0377baf4ef1cc3e3402ded576dccc315800fbc62dfc7fe04b009773b4a\"\n+checksum = \"b09a44accad81e1ba1cd74a32461ba89dee89095ba17b32f5d03683b1b1fc2a0\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n@@ -8498,7 +8518,7 @@ checksum = \"c8af7666ab7b6390ab78131fb5b0fce11d6b7a6951602017c35fa82800708971\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -8654,7 +8674,7 @@ dependencies = [\n  \"quote\",\n  \"regex\",\n  \"relative-path\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -8694,7 +8714,7 @@ checksum = \"4fee6c4efc90059e10f81e6d42c60a18f76588c3d74cb83a0b242a2b6c7504c1\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -8705,7 +8725,7 @@ checksum = \"7f7cf42b4507d8ea322120659672cf1b9dbb93f8f2d4ecfd6e51350ff5b17a1d\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -8819,7 +8839,7 @@ checksum = \"6e06d43f1345a3bcd39f6a56dbb7dcab2ba47e68e8ac134855e7e2bdbaf8cab8\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -9042,7 +9062,7 @@ checksum = \"34704c8d6ebcbc939824180af020566b01a7c01f80641264eba0999f6c2b6be7\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -9270,7 +9290,7 @@ dependencies = [\n  \"serde\",\n  \"serde_json\",\n  \"serde_path_to_error\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n  \"tracing\",\n  \"tracing-subscriber\",\n  \"walkdir\",\n@@ -9323,6 +9343,7 @@ dependencies = [\n  \"arc-swap\",\n  \"async-trait\",\n  \"auto-hash-map\",\n+ \"bitfield\",\n  \"byteorder\",\n  \"criterion\",\n  \"dashmap 6.1.0\",\n@@ -10799,7 +10820,7 @@ dependencies = [\n  \"once_cell\",\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n  \"wasm-bindgen-shared\",\n ]\n \n@@ -10833,7 +10854,7 @@ checksum = \"642f325be6301eb8107a83d12a8ac6c1e1c54345a7ef1a9261962dfefda09e66\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n  \"wasm-bindgen-backend\",\n  \"wasm-bindgen-shared\",\n ]\n@@ -11432,7 +11453,7 @@ checksum = \"2bbd5b46c938e506ecbce286b6628a02171d56153ba733b6c741fc627ec9579b\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -11443,7 +11464,7 @@ checksum = \"053c4c462dc91d3b1504c6fe5a726dd15e216ba718e84a0e46a88fbe5ded3515\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -11831,7 +11852,7 @@ checksum = \"2380878cad4ac9aac1e2435f3eb4020e8374b5f13c296cb75b4620ff8e229154\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n  \"synstructure\",\n ]\n \n@@ -11861,7 +11882,7 @@ checksum = \"9ce1b18ccd8e73a9321186f97e46f9f04b778851177567b1975109d26a08d2a6\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -11872,7 +11893,7 @@ checksum = \"a996a8f63c5c4448cd959ac1bab0aaa3306ccfd060472f85943ee0750f0169be\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]\n@@ -11892,7 +11913,7 @@ checksum = \"595eed982f7d355beb85837f651fa22e90b3c044842dc7f2c2842c086f295808\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n  \"synstructure\",\n ]\n \n@@ -11921,7 +11942,7 @@ checksum = \"6eafa6dfb17584ea3e2bd6e76e0cc15ad7af12b09abdd1ca55961bed9b1063c6\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n- \"syn 2.0.95\",\n+ \"syn 2.0.100\",\n ]\n \n [[package]]"
        },
        {
            "sha": "454ebbe12efe17645808dcf4d345a4ba66e30813",
            "filename": "Cargo.toml",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/vercel/next.js/blob/0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9/Cargo.toml",
            "raw_url": "https://github.com/vercel/next.js/raw/0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9/Cargo.toml",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/Cargo.toml?ref=0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9",
            "patch": "@@ -332,6 +332,7 @@ async-compression = { version = \"0.3.13\", default-features = false, features = [\n ] }\n async-trait = \"0.1.64\"\n atty = \"0.2.14\"\n+bitfield = \"0.18.0\"\n bytes = \"1.1.0\"\n chrono = \"0.4.23\"\n clap = { version = \"4.5.2\", features = [\"derive\"] }"
        },
        {
            "sha": "466e69f9c3bc9c4f388f5122a74176a7b84c9d85",
            "filename": "turbopack/crates/turbo-tasks-backend/Cargo.toml",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/vercel/next.js/blob/0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9/turbopack%2Fcrates%2Fturbo-tasks-backend%2FCargo.toml",
            "raw_url": "https://github.com/vercel/next.js/raw/0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9/turbopack%2Fcrates%2Fturbo-tasks-backend%2FCargo.toml",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2FCargo.toml?ref=0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9",
            "patch": "@@ -26,6 +26,7 @@ anyhow = { workspace = true }\n arc-swap = { version = \"1.7.1\" }\n async-trait = { workspace = true }\n auto-hash-map = { workspace = true }\n+bitfield = { workspace = true }\n byteorder = \"1.5.0\"\n dashmap = { workspace = true, features = [\"raw-api\"]}\n either = { workspace = true }"
        },
        {
            "sha": "ee76eebdaa330060285ae8f9eeb0e3ca4a86f32d",
            "filename": "turbopack/crates/turbo-tasks-backend/src/backend/dynamic_storage.rs",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/vercel/next.js/blob/0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Fdynamic_storage.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Fdynamic_storage.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Fdynamic_storage.rs?ref=0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9",
            "patch": "@@ -6,6 +6,7 @@ use crate::data::{\n     CachedDataItemValue, CachedDataItemValueRef, CachedDataItemValueRefMut,\n };\n \n+#[derive(Debug, Clone)]\n pub struct DynamicStorage {\n     map: Vec<CachedDataItemStorage>,\n }\n@@ -162,9 +163,24 @@ impl DynamicStorage {\n         }\n     }\n \n+    pub fn len(&self) -> usize {\n+        self.map.iter().map(|m| m.len()).sum()\n+    }\n+\n     pub fn shrink_to_fit(&mut self, ty: CachedDataItemType) {\n         if let Some(map) = self.get_map_mut(ty) {\n             map.shrink_to_fit();\n         }\n     }\n+\n+    pub fn snapshot_for_persisting(&self) -> Self {\n+        Self {\n+            map: self\n+                .map\n+                .iter()\n+                .filter(|m| m.ty().is_persistent())\n+                .cloned()\n+                .collect(),\n+        }\n+    }\n }"
        },
        {
            "sha": "ec4215f04a4363a671c5f9ec074021c5a37880e4",
            "filename": "turbopack/crates/turbo-tasks-backend/src/backend/mod.rs",
            "status": "modified",
            "additions": 126,
            "deletions": 59,
            "changes": 185,
            "blob_url": "https://github.com/vercel/next.js/blob/0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Fmod.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Fmod.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Fmod.rs?ref=0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9",
            "patch": "@@ -1,6 +1,5 @@\n mod dynamic_storage;\n mod operation;\n-mod persisted_storage_log;\n mod storage;\n \n use std::{\n@@ -22,7 +21,7 @@ use auto_hash_map::{AutoMap, AutoSet};\n use indexmap::IndexSet;\n use parking_lot::{Condvar, Mutex};\n use rustc_hash::{FxHashMap, FxHashSet, FxHasher};\n-use smallvec::smallvec;\n+use smallvec::{smallvec, SmallVec};\n use tokio::time::{Duration, Instant};\n use turbo_tasks::{\n     backend::{\n@@ -34,8 +33,8 @@ use turbo_tasks::{\n     task_statistics::TaskStatisticsApi,\n     trace::TraceRawVcs,\n     util::IdFactoryWithReuse,\n-    CellId, FunctionId, FxDashMap, RawVc, ReadCellOptions, ReadConsistency, SessionId, TaskId,\n-    TraitTypeId, TurboTasksBackendApi, ValueTypeId, TRANSIENT_TASK_BIT,\n+    CellId, FunctionId, FxDashMap, KeyValuePair, RawVc, ReadCellOptions, ReadConsistency,\n+    SessionId, TaskId, TraitTypeId, TurboTasksBackendApi, ValueTypeId, TRANSIENT_TASK_BIT,\n };\n \n pub use self::{operation::AnyOperation, storage::TaskDataCategory};\n@@ -49,17 +48,21 @@ use crate::{\n             CleanupOldEdgesOperation, ConnectChildOperation, ExecuteContext, ExecuteContextImpl,\n             Operation, OutdatedEdge, TaskGuard,\n         },\n-        persisted_storage_log::PersistedStorageLog,\n-        storage::{get, get_many, get_mut, get_mut_or_insert_with, iter_many, remove, Storage},\n+        storage::{\n+            get, get_many, get_mut, get_mut_or_insert_with, iter_many, remove,\n+            InnerStorageSnapshot, Storage,\n+        },\n     },\n     backing_storage::BackingStorage,\n     data::{\n         ActivenessState, AggregationNumber, CachedDataItem, CachedDataItemKey, CachedDataItemType,\n-        CachedDataItemValue, CachedDataItemValueRef, CachedDataUpdate, CellRef, CollectibleRef,\n-        CollectiblesRef, DirtyState, InProgressCellState, InProgressState, InProgressStateInner,\n-        OutputValue, RootType,\n+        CachedDataItemValue, CachedDataItemValueRef, CellRef, CollectibleRef, CollectiblesRef,\n+        DirtyState, InProgressCellState, InProgressState, InProgressStateInner, OutputValue,\n+        RootType,\n+    },\n+    utils::{\n+        bi_map::BiMap, chunked_vec::ChunkedVec, ptr_eq_arc::PtrEqArc, sharded::Sharded, swap_retain,\n     },\n-    utils::{bi_map::BiMap, chunked_vec::ChunkedVec, ptr_eq_arc::PtrEqArc, sharded::Sharded},\n };\n \n const BACKEND_JOB_INITIAL_SNAPSHOT: BackendJobId = unsafe { BackendJobId::new_unchecked(1) };\n@@ -163,8 +166,6 @@ struct TurboTasksBackendInner<B: BackingStorage> {\n     task_cache: BiMap<Arc<CachedTaskType>, TaskId>,\n     transient_tasks: FxDashMap<TaskId, Arc<TransientTask>>,\n \n-    persisted_storage_data_log: Option<PersistedStorageLog>,\n-    persisted_storage_meta_log: Option<PersistedStorageLog>,\n     storage: Storage,\n \n     /// Number of executing operations + Highest bit is set when snapshot is\n@@ -227,8 +228,6 @@ impl<B: BackingStorage> TurboTasksBackendInner<B> {\n             persisted_task_cache_log: need_log.then(|| Sharded::new(shard_amount)),\n             task_cache: BiMap::new(),\n             transient_tasks: FxDashMap::default(),\n-            persisted_storage_data_log: need_log.then(|| PersistedStorageLog::new(shard_amount)),\n-            persisted_storage_meta_log: need_log.then(|| PersistedStorageLog::new(shard_amount)),\n             storage: Storage::new(),\n             in_progress_operations: AtomicUsize::new(0),\n             snapshot_request: Mutex::new(SnapshotRequest::new()),\n@@ -333,15 +332,6 @@ impl<B: BackingStorage> TurboTasksBackendInner<B> {\n         }\n     }\n \n-    fn persisted_storage_log(&self, category: TaskDataCategory) -> Option<&PersistedStorageLog> {\n-        match category {\n-            TaskDataCategory::Data => &self.persisted_storage_data_log,\n-            TaskDataCategory::Meta => &self.persisted_storage_meta_log,\n-            TaskDataCategory::All => unreachable!(),\n-        }\n-        .as_ref()\n-    }\n-\n     fn should_persist(&self) -> bool {\n         matches!(self.options.storage_mode, Some(StorageMode::ReadWrite))\n     }\n@@ -827,16 +817,12 @@ impl<B: BackingStorage> TurboTasksBackendInner<B> {\n             .map(|op| op.arc().clone())\n             .collect::<Vec<_>>();\n         drop(snapshot_request);\n-        fn take_from_log(log: &Option<PersistedStorageLog>) -> Vec<ChunkedVec<CachedDataUpdate>> {\n-            log.as_ref().map(|l| l.take()).unwrap_or_default()\n-        }\n-        let persisted_storage_meta_log = take_from_log(&self.persisted_storage_meta_log);\n-        let persisted_storage_data_log = take_from_log(&self.persisted_storage_data_log);\n-        let persisted_task_cache_log = self\n+        let mut persisted_task_cache_log = self\n             .persisted_task_cache_log\n             .as_ref()\n             .map(|l| l.take(|i| i))\n             .unwrap_or_default();\n+        self.storage.start_snapshot();\n         let mut snapshot_request = self.snapshot_request.lock();\n         snapshot_request.snapshot_requested = false;\n         self.in_progress_operations\n@@ -845,50 +831,131 @@ impl<B: BackingStorage> TurboTasksBackendInner<B> {\n         let snapshot_time = Instant::now();\n         drop(snapshot_request);\n \n-        // TODO track which items are persisting\n-        // TODO This is very inefficient, maybe the BackingStorage could compute that since it need\n-        // to iterate items anyway.\n-        // let mut counts: FxHashMap<TaskId, u32> =\n-        // FxHashMap::with_capacity_and_hasher(); for log in persisted_storage_meta_log\n-        //     .iter()\n-        //     .chain(persisted_storage_data_log.iter())\n-        // {\n-        //     for CachedDataUpdate { task, .. } in log.iter() {\n-        //         *counts.entry(*task).or_default() += 1;\n-        //     }\n-        // }\n+        let preprocess = |task_id: TaskId, inner: &storage::InnerStorage| {\n+            if task_id.is_transient() {\n+                return (None, None);\n+            }\n+            let len = inner.len();\n+            let mut meta = Vec::with_capacity(len);\n+            let mut data = Vec::with_capacity(len);\n+            for (key, value) in inner.iter_all() {\n+                if key.is_persistent() && value.is_persistent() {\n+                    match key.category() {\n+                        TaskDataCategory::Meta => {\n+                            meta.push(CachedDataItem::from_key_and_value_ref(key, value))\n+                        }\n+                        TaskDataCategory::Data => {\n+                            data.push(CachedDataItem::from_key_and_value_ref(key, value))\n+                        }\n+                        _ => {}\n+                    }\n+                }\n+            }\n \n-        let mut new_items = false;\n+            (\n+                inner.state().meta_restored().then_some(meta),\n+                inner.state().data_restored().then_some(data),\n+            )\n+        };\n+        let process = |task_id: TaskId, (meta, data): (Option<Vec<_>>, Option<Vec<_>>)| {\n+            (\n+                task_id,\n+                meta.map(|d| B::serialize(task_id, &d)),\n+                data.map(|d| B::serialize(task_id, &d)),\n+            )\n+        };\n+        let process_snapshot = |task_id: TaskId, inner: Box<InnerStorageSnapshot>| {\n+            if task_id.is_transient() {\n+                return (task_id, None, None);\n+            }\n+            let len = inner.len();\n+            let mut meta = Vec::with_capacity(len);\n+            let mut data = Vec::with_capacity(len);\n+            for (key, value) in inner.iter_all() {\n+                if key.is_persistent() && value.is_persistent() {\n+                    match key.category() {\n+                        TaskDataCategory::Meta => {\n+                            meta.push(CachedDataItem::from_key_and_value_ref(key, value))\n+                        }\n+                        TaskDataCategory::Data => {\n+                            data.push(CachedDataItem::from_key_and_value_ref(key, value))\n+                        }\n+                        _ => {}\n+                    }\n+                }\n+            }\n+            (\n+                task_id,\n+                inner.meta_restored.then(|| B::serialize(task_id, &meta)),\n+                inner.data_restored.then(|| B::serialize(task_id, &data)),\n+            )\n+        };\n \n-        fn shards_empty<T>(shards: &[ChunkedVec<T>]) -> bool {\n-            shards.iter().all(|shard| shard.is_empty())\n-        }\n+        let snapshot = {\n+            let _span = tracing::trace_span!(\"take snapshot\");\n+            self.storage\n+                .take_snapshot(&preprocess, &process, &process_snapshot)\n+        };\n \n-        if !shards_empty(&persisted_task_cache_log)\n-            || !shards_empty(&persisted_storage_meta_log)\n-            || !shards_empty(&persisted_storage_data_log)\n-        {\n+        let task_snapshots = snapshot\n+            .into_iter()\n+            .filter_map(|iter| {\n+                let mut iter = iter\n+                    .filter_map(\n+                        |(task_id, meta, data): (\n+                            _,\n+                            Option<Result<SmallVec<_>>>,\n+                            Option<Result<SmallVec<_>>>,\n+                        )| {\n+                            let meta = match meta {\n+                                Some(Ok(meta)) => Some(meta),\n+                                None => None,\n+                                Some(Err(err)) => {\n+                                    println!(\n+                                        \"Serializing task {} failed (meta): {:?}\",\n+                                        self.get_task_description(task_id),\n+                                        err\n+                                    );\n+                                    None\n+                                }\n+                            };\n+                            let data = match data {\n+                                Some(Ok(data)) => Some(data),\n+                                None => None,\n+                                Some(Err(err)) => {\n+                                    println!(\n+                                        \"Serializing task {} failed (data): {:?}\",\n+                                        self.get_task_description(task_id),\n+                                        err\n+                                    );\n+                                    None\n+                                }\n+                            };\n+                            (meta.is_some() || data.is_some()).then_some((task_id, meta, data))\n+                        },\n+                    )\n+                    .peekable();\n+                iter.peek().is_some().then_some(iter)\n+            })\n+            .collect::<Vec<_>>();\n+\n+        swap_retain(&mut persisted_task_cache_log, |shard| !shard.is_empty());\n+\n+        let mut new_items = false;\n+\n+        if !persisted_task_cache_log.is_empty() || !task_snapshots.is_empty() {\n             new_items = true;\n             if let Err(err) = self.backing_storage.save_snapshot(\n                 self.session_id,\n                 suspended_operations,\n                 persisted_task_cache_log,\n-                persisted_storage_meta_log,\n-                persisted_storage_data_log,\n+                task_snapshots,\n             ) {\n                 println!(\"Persisting failed: {:?}\", err);\n                 return None;\n             }\n         }\n \n-        // TODO add when we need to track persisted items\n-        // for (task_id, count) in counts {\n-        //     self.storage\n-        //         .access_mut(task_id)\n-        //         .persistance_state_mut()\n-        //         .finish_persisting_items(count);\n-        // }\n-\n         Some((snapshot_time, new_items))\n     }\n "
        },
        {
            "sha": "325246e466f349ab60cb801720d9ca4625b6c010",
            "filename": "turbopack/crates/turbo-tasks-backend/src/backend/operation/aggregation_update.rs",
            "status": "modified",
            "additions": 2,
            "deletions": 27,
            "changes": 29,
            "blob_url": "https://github.com/vercel/next.js/blob/0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Foperation%2Faggregation_update.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Foperation%2Faggregation_update.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Foperation%2Faggregation_update.rs?ref=0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9",
            "patch": "@@ -30,7 +30,7 @@ use crate::{\n         ActivenessState, AggregationNumber, CachedDataItem, CachedDataItemKey, CollectibleRef,\n         DirtyContainerCount,\n     },\n-    utils::deque_set::DequeSet,\n+    utils::{deque_set::DequeSet, swap_retain},\n };\n \n pub const LEAF_NUMBER: u32 = 16;\n@@ -1526,6 +1526,7 @@ impl AggregationUpdateQueue {\n         let mut upper_upper_ids_with_new_follower = SmallVec::new();\n         let mut tasks_for_which_increment_active_count = SmallVec::new();\n         let mut is_active = false;\n+\n         swap_retain(&mut upper_ids, |&mut upper_id| {\n             let mut upper = ctx.task(\n                 upper_id,\n@@ -2355,29 +2356,3 @@ impl Operation for AggregationUpdateQueue {\n         }\n     }\n }\n-\n-fn swap_retain<T, const N: usize>(vec: &mut SmallVec<[T; N]>, mut f: impl FnMut(&mut T) -> bool) {\n-    let mut i = 0;\n-    while i < vec.len() {\n-        if !f(&mut vec[i]) {\n-            vec.swap_remove(i);\n-        } else {\n-            i += 1;\n-        }\n-    }\n-}\n-\n-#[cfg(test)]\n-mod tests {\n-    use smallvec::{smallvec, SmallVec};\n-\n-    use crate::backend::operation::aggregation_update::swap_retain;\n-\n-    #[test]\n-    fn test_swap_retain() {\n-        let mut vec: SmallVec<[i32; 4]> = smallvec![1, 2, 3, 4, 5];\n-        swap_retain(&mut vec, |a| *a % 2 != 0);\n-        let expected: SmallVec<[i32; 4]> = smallvec![1, 5, 3];\n-        assert_eq!(vec, expected);\n-    }\n-}"
        },
        {
            "sha": "f76057e6ae7c48bd1f0ecb793e85b34a28cb1044",
            "filename": "turbopack/crates/turbo-tasks-backend/src/backend/operation/mod.rs",
            "status": "modified",
            "additions": 35,
            "deletions": 165,
            "changes": 200,
            "blob_url": "https://github.com/vercel/next.js/blob/0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Foperation%2Fmod.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Foperation%2Fmod.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Foperation%2Fmod.rs?ref=0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9",
            "patch": "@@ -13,9 +13,8 @@ use std::{\n     mem::{take, transmute},\n };\n \n-use either::Either;\n use serde::{Deserialize, Serialize};\n-use turbo_tasks::{KeyValuePair, SessionId, TaskId, TurboTasksBackendApi};\n+use turbo_tasks::{SessionId, TaskId, TurboTasksBackendApi};\n \n use crate::{\n     backend::{\n@@ -184,23 +183,22 @@ where\n \n     fn task(&mut self, task_id: TaskId, category: TaskDataCategory) -> impl TaskGuard + 'e {\n         let mut task = self.backend.storage.access_mut(task_id);\n-        if !task.persistance_state().is_restored(category) {\n+        if !task.state().is_restored(category) {\n             if task_id.is_transient() {\n-                task.persistance_state_mut()\n-                    .set_restored(TaskDataCategory::All);\n+                task.state_mut().set_restored(TaskDataCategory::All);\n             } else {\n                 for category in category {\n-                    if !task.persistance_state().is_restored(category) {\n+                    if !task.state().is_restored(category) {\n                         // Avoid holding the lock too long since this can also affect other tasks\n                         drop(task);\n \n                         let items = self.restore_task_data(task_id, category);\n                         task = self.backend.storage.access_mut(task_id);\n-                        if !task.persistance_state().is_restored(category) {\n+                        if !task.state().is_restored(category) {\n                             for item in items {\n                                 task.add(item);\n                             }\n-                            task.persistance_state_mut().set_restored(category);\n+                            task.state_mut().set_restored(category);\n                         }\n                     }\n                 }\n@@ -233,8 +231,8 @@ where\n         category: TaskDataCategory,\n     ) -> (impl TaskGuard + 'e, impl TaskGuard + 'e) {\n         let (mut task1, mut task2) = self.backend.storage.access_pair_mut(task_id1, task_id2);\n-        let is_restored1 = task1.persistance_state().is_restored(category);\n-        let is_restored2 = task2.persistance_state().is_restored(category);\n+        let is_restored1 = task1.state().is_restored(category);\n+        let is_restored2 = task2.state().is_restored(category);\n         if !is_restored1 || !is_restored2 {\n             for category in category {\n                 // Avoid holding the lock too long since this can also affect other tasks\n@@ -247,17 +245,17 @@ where\n                 let (t1, t2) = self.backend.storage.access_pair_mut(task_id1, task_id2);\n                 task1 = t1;\n                 task2 = t2;\n-                if !task1.persistance_state().is_restored(category) {\n+                if !task1.state().is_restored(category) {\n                     for item in items1.unwrap() {\n                         task1.add(item);\n                     }\n-                    task1.persistance_state_mut().set_restored(category);\n+                    task1.state_mut().set_restored(category);\n                 }\n-                if !task2.persistance_state().is_restored(category) {\n+                if !task2.state().is_restored(category) {\n                     for item in items2.unwrap() {\n                         task2.add(item);\n                     }\n-                    task2.persistance_state_mut().set_restored(category);\n+                    task2.state_mut().set_restored(category);\n                 }\n             }\n         }\n@@ -466,19 +464,10 @@ impl<B: BackingStorage> TaskGuard for TaskGuardImpl<'_, B> {\n \n     fn add(&mut self, item: CachedDataItem) -> bool {\n         self.check_access(item.category());\n-        if !self.backend.should_persist() || self.task_id.is_transient() || !item.is_persistent() {\n-            self.task.add(item)\n-        } else if self.task.add(item.clone()) {\n-            let (key, value) = item.into_key_and_value();\n-            self.task.persistance_state_mut().add_persisting_item();\n-            self.backend\n-                .persisted_storage_log(key.category())\n-                .unwrap()\n-                .push(self.task_id, key, None, Some(value));\n-            true\n-        } else {\n-            false\n+        if !self.task_id.is_transient() && item.is_persistent() {\n+            self.task.track_modification();\n         }\n+        self.task.add(item)\n     }\n \n     fn add_new(&mut self, item: CachedDataItem) {\n@@ -489,41 +478,10 @@ impl<B: BackingStorage> TaskGuard for TaskGuardImpl<'_, B> {\n \n     fn insert(&mut self, item: CachedDataItem) -> Option<CachedDataItemValue> {\n         self.check_access(item.category());\n-        let (key, value) = item.into_key_and_value();\n-        if !self.backend.should_persist() || self.task_id.is_transient() || !key.is_persistent() {\n-            self.task\n-                .insert(CachedDataItem::from_key_and_value(key, value))\n-        } else if value.is_persistent() {\n-            let old = self\n-                .task\n-                .insert(CachedDataItem::from_key_and_value(key, value.clone()));\n-            self.task.persistance_state_mut().add_persisting_item();\n-            self.backend\n-                .persisted_storage_log(key.category())\n-                .unwrap()\n-                .push(\n-                    self.task_id,\n-                    key,\n-                    old.as_ref()\n-                        .and_then(|old| old.is_persistent().then(|| old.clone())),\n-                    Some(value),\n-                );\n-            old\n-        } else {\n-            let item = CachedDataItem::from_key_and_value(key, value);\n-            if let Some(old) = self.task.insert(item) {\n-                if old.is_persistent() {\n-                    self.task.persistance_state_mut().add_persisting_item();\n-                    self.backend\n-                        .persisted_storage_log(key.category())\n-                        .unwrap()\n-                        .push(self.task_id, key, Some(old.clone()), None);\n-                }\n-                Some(old)\n-            } else {\n-                None\n-            }\n+        if !self.task_id.is_transient() && item.is_persistent() {\n+            self.task.track_modification();\n         }\n+        self.task.insert(item)\n     }\n \n     fn update(\n@@ -532,78 +490,18 @@ impl<B: BackingStorage> TaskGuard for TaskGuardImpl<'_, B> {\n         update: impl FnOnce(Option<CachedDataItemValue>) -> Option<CachedDataItemValue>,\n     ) {\n         self.check_access(key.category());\n-        if !self.backend.should_persist() || self.task_id.is_transient() || !key.is_persistent() {\n-            self.task.update(key, update);\n-            return;\n-        }\n-        let Self {\n-            task,\n-            task_id,\n-            backend,\n-            #[cfg(debug_assertions)]\n-                category: _,\n-        } = self;\n-        let mut add_persisting_item = false;\n-        task.update(key, |old| {\n-            let old_value_when_persistent = old\n-                .as_ref()\n-                .and_then(|old| old.is_persistent().then(|| old.clone()));\n-            let new = update(old);\n-            let new_persistent = new.as_ref().map(|new| new.is_persistent()).unwrap_or(false);\n-\n-            match (old_value_when_persistent, new_persistent) {\n-                (None, false) => {}\n-                (Some(old_value), false) => {\n-                    add_persisting_item = true;\n-                    backend.persisted_storage_log(key.category()).unwrap().push(\n-                        *task_id,\n-                        key,\n-                        Some(old_value),\n-                        None,\n-                    );\n-                }\n-                (old_value, true) => {\n-                    add_persisting_item = true;\n-                    backend.persisted_storage_log(key.category()).unwrap().push(\n-                        *task_id,\n-                        key,\n-                        old_value,\n-                        new.clone(),\n-                    );\n-                }\n-            }\n-\n-            new\n-        });\n-        if add_persisting_item {\n-            task.persistance_state_mut().add_persisting_item();\n+        if !self.task_id.is_transient() && key.is_persistent() {\n+            self.task.track_modification();\n         }\n+        self.task.update(key, update);\n     }\n \n     fn remove(&mut self, key: &CachedDataItemKey) -> Option<CachedDataItemValue> {\n         self.check_access(key.category());\n-        let old_value = self.task.remove(key);\n-        if let Some(value) = old_value {\n-            if self.backend.should_persist()\n-                && !self.task_id.is_transient()\n-                && key.is_persistent()\n-                && value.is_persistent()\n-            {\n-                self.task.persistance_state_mut().add_persisting_item();\n-                self.backend\n-                    .persisted_storage_log(key.category())\n-                    .unwrap()\n-                    .push(\n-                        self.task_id,\n-                        *key,\n-                        value.is_persistent().then(|| value.clone()),\n-                        None,\n-                    );\n-            }\n-            Some(value)\n-        } else {\n-            None\n+        if !self.task_id.is_transient() && key.is_persistent() {\n+            self.task.track_modification();\n         }\n+        self.task.remove(key)\n     }\n \n     fn get(&self, key: &CachedDataItemKey) -> Option<CachedDataItemValueRef<'_>> {\n@@ -613,6 +511,9 @@ impl<B: BackingStorage> TaskGuard for TaskGuardImpl<'_, B> {\n \n     fn get_mut(&mut self, key: &CachedDataItemKey) -> Option<CachedDataItemValueRefMut<'_>> {\n         self.check_access(key.category());\n+        if !self.task_id.is_transient() && key.is_persistent() {\n+            self.task.track_modification();\n+        }\n         self.task.get_mut(key)\n     }\n \n@@ -622,6 +523,9 @@ impl<B: BackingStorage> TaskGuard for TaskGuardImpl<'_, B> {\n         insert: impl FnOnce() -> CachedDataItemValue,\n     ) -> CachedDataItemValueRefMut<'_> {\n         self.check_access(key.category());\n+        if !self.task_id.is_transient() && key.is_persistent() {\n+            self.task.track_modification();\n+        }\n         self.task.get_mut_or_insert_with(key, insert)\n     }\n \n@@ -656,49 +560,15 @@ impl<B: BackingStorage> TaskGuard for TaskGuardImpl<'_, B> {\n         F: for<'a> FnMut(CachedDataItemKey, CachedDataItemValueRef<'a>) -> bool + 'l,\n     {\n         self.check_access(ty.category());\n-        if !self.backend.should_persist() || self.task_id.is_transient() {\n-            return Either::Left(self.task.extract_if(ty, f));\n+        if !self.task_id.is_transient() && ty.is_persistent() {\n+            self.task.track_modification();\n         }\n-        Either::Right(self.task.extract_if(ty, f).inspect(|item| {\n-            if item.is_persistent() {\n-                let key = item.key();\n-                let value = item.value();\n-                self.backend\n-                    .persisted_storage_log(key.category())\n-                    .unwrap()\n-                    .push(self.task_id, key, Some(value), None);\n-            }\n-        }))\n+        self.task.extract_if(ty, f)\n     }\n \n     fn invalidate_serialization(&mut self) {\n-        if !self.backend.should_persist() {\n-            return;\n-        }\n-        let mut count = 0;\n-        let cell_data = self\n-            .iter(CachedDataItemType::CellData)\n-            .filter_map(|(key, value)| match (key, value) {\n-                (\n-                    CachedDataItemKey::CellData { cell },\n-                    CachedDataItemValueRef::CellData { value },\n-                ) => {\n-                    count += 1;\n-                    Some(CachedDataItem::CellData {\n-                        cell,\n-                        value: value.clone(),\n-                    })\n-                }\n-                _ => None,\n-            });\n-        {\n-            self.backend\n-                .persisted_storage_log(TaskDataCategory::Data)\n-                .unwrap()\n-                .push_batch_insert(self.task_id, cell_data);\n-            self.task\n-                .persistance_state_mut()\n-                .add_persisting_items(count);\n+        if !self.task_id.is_transient() {\n+            self.task.track_modification();\n         }\n     }\n }"
        },
        {
            "sha": "7754e75865d18a4c39f59e20f4d442c1e30f0646",
            "filename": "turbopack/crates/turbo-tasks-backend/src/backend/persisted_storage_log.rs",
            "status": "removed",
            "additions": 0,
            "deletions": 78,
            "changes": 78,
            "blob_url": "https://github.com/vercel/next.js/blob/e9982de2af9cbaa273bd8c73f07b6f9eb1d7cf68/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Fpersisted_storage_log.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/e9982de2af9cbaa273bd8c73f07b6f9eb1d7cf68/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Fpersisted_storage_log.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Fpersisted_storage_log.rs?ref=e9982de2af9cbaa273bd8c73f07b6f9eb1d7cf68",
            "patch": "@@ -1,78 +0,0 @@\n-use turbo_tasks::{KeyValuePair, TaskId};\n-\n-use crate::{\n-    data::{CachedDataItem, CachedDataItemKey, CachedDataItemValue, CachedDataUpdate},\n-    utils::{chunked_vec::ChunkedVec, sharded::Sharded},\n-};\n-\n-#[derive(Default)]\n-struct ShardData {\n-    last_task: Option<TaskId>,\n-    data: ChunkedVec<CachedDataUpdate>,\n-}\n-\n-impl ShardData {\n-    fn set_task(&mut self, task: TaskId) {\n-        if self.last_task != Some(task) {\n-            self.data.push(CachedDataUpdate::Task { task });\n-            self.last_task = Some(task);\n-        }\n-    }\n-}\n-\n-pub struct PersistedStorageLog {\n-    data: Sharded<ShardData>,\n-}\n-\n-impl PersistedStorageLog {\n-    pub fn new(shard_amount: usize) -> Self {\n-        Self {\n-            data: Sharded::new(shard_amount),\n-        }\n-    }\n-\n-    pub fn push(\n-        &self,\n-        task: TaskId,\n-        key: CachedDataItemKey,\n-        old_value: Option<CachedDataItemValue>,\n-        new_value: Option<CachedDataItemValue>,\n-    ) {\n-        let mut guard = self.data.lock(task);\n-        guard.set_task(task);\n-        match (old_value, new_value) {\n-            (None, None) => {}\n-            (None, Some(new_value)) => guard.data.push(CachedDataUpdate::New {\n-                item: CachedDataItem::from_key_and_value(key, new_value),\n-            }),\n-            (Some(old_value), None) => guard.data.push(CachedDataUpdate::Removed {\n-                old_item: CachedDataItem::from_key_and_value(key, old_value),\n-            }),\n-            (Some(old_value), Some(new_value)) => {\n-                guard.data.push(CachedDataUpdate::Replace1 {\n-                    old_item: CachedDataItem::from_key_and_value(key, old_value),\n-                });\n-                guard\n-                    .data\n-                    .push(CachedDataUpdate::Replace2 { value: new_value });\n-            }\n-        }\n-    }\n-\n-    pub fn push_batch_insert(\n-        &self,\n-        task: TaskId,\n-        updates: impl IntoIterator<Item = CachedDataItem>,\n-    ) {\n-        let updates = updates\n-            .into_iter()\n-            .map(|item| CachedDataUpdate::New { item });\n-        let mut guard = self.data.lock(task);\n-        guard.set_task(task);\n-        guard.data.extend(updates);\n-    }\n-\n-    pub fn take(&self) -> Vec<ChunkedVec<CachedDataUpdate>> {\n-        self.data.take(|shard| shard.data)\n-    }\n-}"
        },
        {
            "sha": "4719105ce152de34f90ee87968531fbd62a638e1",
            "filename": "turbopack/crates/turbo-tasks-backend/src/backend/storage.rs",
            "status": "modified",
            "additions": 373,
            "deletions": 45,
            "changes": 418,
            "blob_url": "https://github.com/vercel/next.js/blob/0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Fstorage.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Fstorage.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Fstorage.rs?ref=0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9",
            "patch": "@@ -1,9 +1,12 @@\n use std::{\n     hash::Hash,\n     ops::{Deref, DerefMut},\n+    sync::{atomic::AtomicBool, Arc},\n     thread::available_parallelism,\n };\n \n+use bitfield::bitfield;\n+use rayon::iter::{IndexedParallelIterator, IntoParallelRefIterator, ParallelIterator};\n use turbo_tasks::{FxDashMap, TaskId};\n \n use crate::{\n@@ -16,26 +19,13 @@ use crate::{\n     utils::dash_map_multi::{get_multiple_mut, RefMut},\n };\n \n-const META_UNRESTORED: u32 = 1 << 31;\n-const DATA_UNRESTORED: u32 = 1 << 30;\n-\n #[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]\n pub enum TaskDataCategory {\n     Meta,\n     Data,\n     All,\n }\n \n-impl TaskDataCategory {\n-    pub fn flag(&self) -> u32 {\n-        match self {\n-            TaskDataCategory::Meta => META_UNRESTORED,\n-            TaskDataCategory::Data => DATA_UNRESTORED,\n-            TaskDataCategory::All => META_UNRESTORED | DATA_UNRESTORED,\n-        }\n-    }\n-}\n-\n impl IntoIterator for TaskDataCategory {\n     type Item = TaskDataCategory;\n \n@@ -79,52 +69,119 @@ impl Iterator for TaskDataCategoryIterator {\n     }\n }\n \n-pub struct PersistanceState {\n-    value: u32,\n+bitfield! {\n+    // Note: Due to alignment in InnerStorage it doesn't matter if this struct is 1 or 4 bytes.\n+    #[derive(Clone, Default)]\n+    pub struct InnerStorageState(u32);\n+    impl Debug;\n+    pub meta_restored, set_meta_restored: 0;\n+    pub data_restored, set_data_restored: 1;\n+    /// Item was modified before snapshot mode was entered.\n+    pub modified, set_modified: 2;\n+    /// Item was modified after snapshot mode was entered. A snapshot was taken.\n+    pub snapshot, set_snapshot: 3;\n }\n \n-impl Default for PersistanceState {\n-    fn default() -> Self {\n-        Self {\n-            value: META_UNRESTORED | DATA_UNRESTORED,\n+impl InnerStorageState {\n+    pub fn set_restored(&mut self, category: TaskDataCategory) {\n+        match category {\n+            TaskDataCategory::Meta => {\n+                self.set_meta_restored(true);\n+            }\n+            TaskDataCategory::Data => {\n+                self.set_data_restored(true);\n+            }\n+            TaskDataCategory::All => {\n+                self.set_meta_restored(true);\n+                self.set_data_restored(true);\n+            }\n         }\n     }\n-}\n \n-impl PersistanceState {\n-    pub fn set_restored(&mut self, category: TaskDataCategory) {\n-        self.value &= !category.flag();\n+    pub fn is_restored(&self, category: TaskDataCategory) -> bool {\n+        match category {\n+            TaskDataCategory::Meta => self.meta_restored(),\n+            TaskDataCategory::Data => self.data_restored(),\n+            TaskDataCategory::All => self.meta_restored() && self.data_restored(),\n+        }\n     }\n+}\n \n-    pub fn add_persisting_item(&mut self) {\n-        // TODO add when we need to track unpersisted items\n-        // self.value += 1;\n-    }\n+pub struct InnerStorageSnapshot {\n+    aggregation_number: OptionStorage<AggregationNumber>,\n+    output_dependent: AutoMapStorage<TaskId, ()>,\n+    output: OptionStorage<OutputValue>,\n+    upper: AutoMapStorage<TaskId, i32>,\n+    dynamic: DynamicStorage,\n+    pub meta_restored: bool,\n+    pub data_restored: bool,\n+}\n \n-    pub fn add_persisting_items(&mut self, _count: u32) {\n-        // TODO add when we need to track unpersisted items\n-        // self.value += count;\n+impl From<&InnerStorage> for InnerStorageSnapshot {\n+    fn from(inner: &InnerStorage) -> Self {\n+        Self {\n+            aggregation_number: inner.aggregation_number.clone(),\n+            output_dependent: inner.output_dependent.clone(),\n+            output: inner.output.clone(),\n+            upper: inner.upper.clone(),\n+            dynamic: inner.dynamic.snapshot_for_persisting(),\n+            meta_restored: inner.state.meta_restored(),\n+            data_restored: inner.state.data_restored(),\n+        }\n     }\n+}\n \n-    // TODO remove when we need to track unpersisted items\n-    #[allow(dead_code)]\n-    pub fn finish_persisting_items(&mut self, _count: u32) {\n-        // TODO add when we need to track unpersisted items\n-        // self.value -= count;\n+impl InnerStorageSnapshot {\n+    pub fn iter_all(\n+        &self,\n+    ) -> impl Iterator<Item = (CachedDataItemKey, CachedDataItemValueRef<'_>)> {\n+        use crate::data_storage::Storage;\n+        self.dynamic\n+            .iter_all()\n+            .chain(self.aggregation_number.iter().map(|(_, value)| {\n+                (\n+                    CachedDataItemKey::AggregationNumber {},\n+                    CachedDataItemValueRef::AggregationNumber { value },\n+                )\n+            }))\n+            .chain(self.output.iter().map(|(_, value)| {\n+                (\n+                    CachedDataItemKey::Output {},\n+                    CachedDataItemValueRef::Output { value },\n+                )\n+            }))\n+            .chain(self.upper.iter().map(|(k, value)| {\n+                (\n+                    CachedDataItemKey::Upper { task: *k },\n+                    CachedDataItemValueRef::Upper { value },\n+                )\n+            }))\n+            .chain(self.output_dependent.iter().map(|(k, value)| {\n+                (\n+                    CachedDataItemKey::OutputDependent { task: *k },\n+                    CachedDataItemValueRef::OutputDependent { value },\n+                )\n+            }))\n     }\n \n-    pub fn is_restored(&self, category: TaskDataCategory) -> bool {\n-        (self.value & category.flag()) == 0\n+    pub fn len(&self) -> usize {\n+        use crate::data_storage::Storage;\n+        self.dynamic.len()\n+            + self.aggregation_number.len()\n+            + self.output.len()\n+            + self.upper.len()\n+            + self.output_dependent.len()\n     }\n }\n \n+#[derive(Debug, Clone)]\n pub struct InnerStorage {\n     aggregation_number: OptionStorage<AggregationNumber>,\n     output_dependent: AutoMapStorage<TaskId, ()>,\n     output: OptionStorage<OutputValue>,\n     upper: AutoMapStorage<TaskId, i32>,\n     dynamic: DynamicStorage,\n-    persistance_state: PersistanceState,\n+    state: InnerStorageState,\n }\n \n impl InnerStorage {\n@@ -135,16 +192,16 @@ impl InnerStorage {\n             output: Default::default(),\n             upper: Default::default(),\n             dynamic: DynamicStorage::new(),\n-            persistance_state: PersistanceState::default(),\n+            state: InnerStorageState::default(),\n         }\n     }\n \n-    pub fn persistance_state(&self) -> &PersistanceState {\n-        &self.persistance_state\n+    pub fn state(&self) -> &InnerStorageState {\n+        &self.state\n     }\n \n-    pub fn persistance_state_mut(&mut self) -> &mut PersistanceState {\n-        &mut self.persistance_state\n+    pub fn state_mut(&mut self) -> &mut InnerStorageState {\n+        &mut self.state\n     }\n }\n \n@@ -497,9 +554,33 @@ impl InnerStorage {\n                 )\n             }))\n     }\n+\n+    pub fn len(&self) -> usize {\n+        use crate::data_storage::Storage;\n+        self.dynamic.len()\n+            + self.aggregation_number.len()\n+            + self.output.len()\n+            + self.upper.len()\n+            + self.output_dependent.len()\n+    }\n+}\n+\n+enum ModifiedState {\n+    /// It was modified before snapshot mode was entered, but it was not accessed during snapshot\n+    /// mode.\n+    Modified,\n+    /// Snapshot(Some):\n+    /// It was modified before snapshot mode was entered and it was accessed again during snapshot\n+    /// mode. A copy of the version of the item when snapshot mode was entered is stored here.\n+    /// Snapshot(None):\n+    /// It was not modified before snapshot mode was entered, but it was accessed during snapshot\n+    /// mode. Or the snapshot was already taken out by the snapshot operation.\n+    Snapshot(Option<Box<InnerStorageSnapshot>>),\n }\n \n pub struct Storage {\n+    snapshot_mode: AtomicBool,\n+    modified: FxDashMap<TaskId, ModifiedState>,\n     map: FxDashMap<TaskId, Box<InnerStorage>>,\n }\n \n@@ -508,6 +589,12 @@ impl Storage {\n         let shard_amount =\n             (available_parallelism().map_or(4, |v| v.get()) * 64).next_power_of_two();\n         Self {\n+            snapshot_mode: AtomicBool::new(false),\n+            modified: FxDashMap::with_capacity_and_hasher_and_shard_amount(\n+                1024,\n+                Default::default(),\n+                shard_amount,\n+            ),\n             map: FxDashMap::with_capacity_and_hasher_and_shard_amount(\n                 1024 * 1024,\n                 Default::default(),\n@@ -516,12 +603,152 @@ impl Storage {\n         }\n     }\n \n+    /// Processes every modified item (resp. a snapshot of it) with the given functions and returns\n+    /// the results. Ends snapshot mode afterwards.\n+    /// preprocess is potentially called within a lock, so it should be fast.\n+    /// process is called outside of locks, so it could do more expensive operations.\n+    pub fn take_snapshot<\n+        'l,\n+        T,\n+        R,\n+        PP: for<'a> Fn(TaskId, &'a InnerStorage) -> T + Sync,\n+        P: Fn(TaskId, T) -> R + Sync,\n+        PS: Fn(TaskId, Box<InnerStorageSnapshot>) -> R + Sync,\n+    >(\n+        &'l self,\n+        preprocess: &'l PP,\n+        process: &'l P,\n+        process_snapshot: &'l PS,\n+    ) -> Vec<SnapshotShard<'l, PP, P, PS>> {\n+        if !self.snapshot_mode() {\n+            self.start_snapshot();\n+        }\n+\n+        let guard = Arc::new(SnapshotGuard { storage: self });\n+\n+        // The number of shards is much larger than the number of threads, so the effect of the\n+        // locks held is negligible.\n+        let shards = self\n+            .modified\n+            .shards()\n+            .par_iter()\n+            .with_max_len(1)\n+            .map(|shard| {\n+                let mut direct_snapshots: Vec<(TaskId, Box<InnerStorageSnapshot>)> = Vec::new();\n+                let mut modified: Vec<TaskId> = Vec::new();\n+                {\n+                    // Take the snapshots from the modified map\n+                    let guard = shard.write();\n+                    // Safety: guard must outlive the iterator.\n+                    for bucket in unsafe { guard.iter() } {\n+                        // Safety: the guard guarantees that the bucket is not removed and the ptr\n+                        // is valid.\n+                        let (key, shared_value) = unsafe { bucket.as_mut() };\n+                        let modified_state = shared_value.get_mut();\n+                        match modified_state {\n+                            ModifiedState::Modified => {\n+                                modified.push(*key);\n+                            }\n+                            ModifiedState::Snapshot(snapshot) => {\n+                                if let Some(snapshot) = snapshot.take() {\n+                                    direct_snapshots.push((*key, snapshot));\n+                                }\n+                            }\n+                        }\n+                    }\n+                    // Safety: guard must outlive the iterator.\n+                    drop(guard);\n+                }\n+\n+                SnapshotShard {\n+                    direct_snapshots,\n+                    modified,\n+                    storage: self,\n+                    guard: Some(guard.clone()),\n+                    process,\n+                    preprocess,\n+                    process_snapshot,\n+                }\n+            })\n+            .collect::<Vec<_>>();\n+\n+        shards\n+    }\n+\n+    /// Start snapshot mode.\n+    pub fn start_snapshot(&self) {\n+        self.snapshot_mode\n+            .store(true, std::sync::atomic::Ordering::Release);\n+    }\n+\n+    /// End snapshot mode.\n+    /// Items that have snapshots will be kept as modified since they have been accessed during the\n+    /// snapshot mode. Items that are modified will be removed and considered as unmodified.\n+    /// When items are accessed in future they will be marked as modified.\n+    fn end_snapshot(&self) {\n+        // We are still in snapshot mode, so all accessed items would be stored as snapshot.\n+        // This means we can start by removing all modified items.\n+        let mut removed_modified = Vec::new();\n+        self.modified.retain(|key, inner| {\n+            if matches!(inner, ModifiedState::Modified) {\n+                removed_modified.push(*key);\n+                false\n+            } else {\n+                true\n+            }\n+        });\n+\n+        // We also need to unset all the modified flags.\n+        for key in removed_modified {\n+            if let Some(mut inner) = self.map.get_mut(&key) {\n+                inner.state_mut().set_modified(false);\n+            }\n+        }\n+\n+        // Now modified only contains snapshots.\n+        // We leave snapshot mode. Any access would be stored as modified and not as snapshot.\n+        self.snapshot_mode\n+            .store(false, std::sync::atomic::Ordering::Release);\n+\n+        // We can change all the snapshots to modified now.\n+        let mut removed_snapshots = Vec::new();\n+        for mut item in self.modified.iter_mut() {\n+            match item.value() {\n+                ModifiedState::Snapshot(_) => {\n+                    removed_snapshots.push(*item.key());\n+                    *item.value_mut() = ModifiedState::Modified;\n+                }\n+                ModifiedState::Modified => {\n+                    // This means it was concurrently modified.\n+                    // It's already in the correct state.\n+                }\n+            }\n+        }\n+\n+        // And update the flags\n+        for key in removed_snapshots {\n+            if let Some(mut inner) = self.map.get_mut(&key) {\n+                inner.state_mut().set_snapshot(false);\n+                inner.state_mut().set_modified(true);\n+            }\n+        }\n+\n+        // Remove excessive capacity in modified\n+        self.modified.shrink_to_fit();\n+    }\n+\n+    fn snapshot_mode(&self) -> bool {\n+        self.snapshot_mode\n+            .load(std::sync::atomic::Ordering::Acquire)\n+    }\n+\n     pub fn access_mut(&self, key: TaskId) -> StorageWriteGuard<'_> {\n         let inner = match self.map.entry(key) {\n             dashmap::mapref::entry::Entry::Occupied(e) => e.into_ref(),\n             dashmap::mapref::entry::Entry::Vacant(e) => e.insert(Box::new(InnerStorage::new())),\n         };\n         StorageWriteGuard {\n+            storage: self,\n             inner: inner.into(),\n         }\n     }\n@@ -533,16 +760,60 @@ impl Storage {\n     ) -> (StorageWriteGuard<'_>, StorageWriteGuard<'_>) {\n         let (a, b) = get_multiple_mut(&self.map, key1, key2, || Box::new(InnerStorage::new()));\n         (\n-            StorageWriteGuard { inner: a },\n-            StorageWriteGuard { inner: b },\n+            StorageWriteGuard {\n+                storage: self,\n+                inner: a,\n+            },\n+            StorageWriteGuard {\n+                storage: self,\n+                inner: b,\n+            },\n         )\n     }\n }\n \n pub struct StorageWriteGuard<'a> {\n+    storage: &'a Storage,\n     inner: RefMut<'a, TaskId, Box<InnerStorage>>,\n }\n \n+impl StorageWriteGuard<'_> {\n+    /// Tracks mutation of this task\n+    pub fn track_modification(&mut self) {\n+        if !self.inner.state().snapshot() {\n+            match (self.storage.snapshot_mode(), self.inner.state().modified()) {\n+                (false, false) => {\n+                    // Not in snapshot mode and item is unmodified\n+                    self.storage\n+                        .modified\n+                        .insert(*self.inner.key(), ModifiedState::Modified);\n+                    self.inner.state_mut().set_modified(true);\n+                }\n+                (false, true) => {\n+                    // Not in snapshot mode and item is already modfied\n+                    // Do nothing\n+                }\n+                (true, false) => {\n+                    // In snapshot mode and item is unmodified (so it's not part of the snapshot)\n+                    self.storage\n+                        .modified\n+                        .insert(*self.inner.key(), ModifiedState::Snapshot(None));\n+                    self.inner.state_mut().set_snapshot(true);\n+                }\n+                (true, true) => {\n+                    // In snapshot mode and item is modified (so it's part of the snapshot)\n+                    // We need to store the original version that is part of the snapshot\n+                    self.storage.modified.insert(\n+                        *self.inner.key(),\n+                        ModifiedState::Snapshot(Some(Box::new((&**self.inner).into()))),\n+                    );\n+                    self.inner.state_mut().set_snapshot(true);\n+                }\n+            }\n+        }\n+    }\n+}\n+\n impl Deref for StorageWriteGuard<'_> {\n     type Target = InnerStorage;\n \n@@ -749,3 +1020,60 @@ pub(crate) use iter_many;\n pub(crate) use remove;\n pub(crate) use update;\n pub(crate) use update_count;\n+\n+pub struct SnapshotGuard<'l> {\n+    storage: &'l Storage,\n+}\n+\n+impl Drop for SnapshotGuard<'_> {\n+    fn drop(&mut self) {\n+        self.storage.end_snapshot();\n+    }\n+}\n+\n+pub struct SnapshotShard<'l, PP, P, PS> {\n+    direct_snapshots: Vec<(TaskId, Box<InnerStorageSnapshot>)>,\n+    modified: Vec<TaskId>,\n+    storage: &'l Storage,\n+    guard: Option<Arc<SnapshotGuard<'l>>>,\n+    process: &'l P,\n+    preprocess: &'l PP,\n+    process_snapshot: &'l PS,\n+}\n+\n+impl<'l, T, R, PP, P, PS> Iterator for SnapshotShard<'l, PP, P, PS>\n+where\n+    PP: for<'a> Fn(TaskId, &'a InnerStorage) -> T + Sync,\n+    P: Fn(TaskId, T) -> R + Sync,\n+    PS: Fn(TaskId, Box<InnerStorageSnapshot>) -> R + Sync,\n+{\n+    type Item = R;\n+\n+    fn next(&mut self) -> Option<Self::Item> {\n+        if let Some((task_id, snapshot)) = self.direct_snapshots.pop() {\n+            return Some((self.process_snapshot)(task_id, snapshot));\n+        }\n+        while let Some(task_id) = self.modified.pop() {\n+            let inner = self.storage.map.get(&task_id).unwrap();\n+            if !inner.state().snapshot() {\n+                let preprocessed = (self.preprocess)(task_id, &inner);\n+                drop(inner);\n+                return Some((self.process)(task_id, preprocessed));\n+            } else {\n+                drop(inner);\n+                let maybe_snapshot = {\n+                    let mut modified_state = self.storage.modified.get_mut(&task_id).unwrap();\n+                    let ModifiedState::Snapshot(snapshot) = &mut *modified_state else {\n+                        unreachable!(\"The snapshot bit was set, so it must be in Snapshot state\");\n+                    };\n+                    snapshot.take()\n+                };\n+                if let Some(snapshot) = maybe_snapshot {\n+                    return Some((self.process_snapshot)(task_id, snapshot));\n+                }\n+            }\n+        }\n+        self.guard = None;\n+        None\n+    }\n+}"
        },
        {
            "sha": "87ea14ef3df83b2a0502ba44c36336f0da7db26a",
            "filename": "turbopack/crates/turbo-tasks-backend/src/backing_storage.rs",
            "status": "modified",
            "additions": 16,
            "deletions": 5,
            "changes": 21,
            "blob_url": "https://github.com/vercel/next.js/blob/0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbacking_storage.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbacking_storage.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbacking_storage.rs?ref=0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9",
            "patch": "@@ -1,11 +1,12 @@\n use std::sync::Arc;\n \n use anyhow::Result;\n+use smallvec::SmallVec;\n use turbo_tasks::{backend::CachedTaskType, SessionId, TaskId};\n \n use crate::{\n     backend::{AnyOperation, TaskDataCategory},\n-    data::{CachedDataItem, CachedDataUpdate},\n+    data::CachedDataItem,\n     utils::chunked_vec::ChunkedVec,\n };\n \n@@ -17,14 +18,24 @@ pub trait BackingStorage: 'static + Send + Sync {\n     fn next_free_task_id(&self) -> TaskId;\n     fn next_session_id(&self) -> SessionId;\n     fn uncompleted_operations(&self) -> Vec<AnyOperation>;\n-    fn save_snapshot(\n+    #[allow(clippy::ptr_arg)]\n+    fn serialize(task: TaskId, data: &Vec<CachedDataItem>) -> Result<SmallVec<[u8; 16]>>;\n+    fn save_snapshot<I>(\n         &self,\n         session_id: SessionId,\n         operations: Vec<Arc<AnyOperation>>,\n         task_cache_updates: Vec<ChunkedVec<(Arc<CachedTaskType>, TaskId)>>,\n-        meta_updates: Vec<ChunkedVec<CachedDataUpdate>>,\n-        data_updates: Vec<ChunkedVec<CachedDataUpdate>>,\n-    ) -> Result<()>;\n+        snapshots: Vec<I>,\n+    ) -> Result<()>\n+    where\n+        I: Iterator<\n+                Item = (\n+                    TaskId,\n+                    Option<SmallVec<[u8; 16]>>,\n+                    Option<SmallVec<[u8; 16]>>,\n+                ),\n+            > + Send\n+            + Sync;\n     fn start_read_transaction(&self) -> Option<Self::ReadTransaction<'_>>;\n     /// # Safety\n     ///"
        },
        {
            "sha": "d046b24c3bd1f183fce03f9dd5eff5adf309b818",
            "filename": "turbopack/crates/turbo-tasks-backend/src/data.rs",
            "status": "modified",
            "additions": 40,
            "deletions": 22,
            "changes": 62,
            "blob_url": "https://github.com/vercel/next.js/blob/0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fdata.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fdata.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fdata.rs?ref=0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9",
            "patch": "@@ -588,6 +588,10 @@ impl CachedDataItem {\n             | Self::Activeness { .. } => TaskDataCategory::All,\n         }\n     }\n+\n+    pub fn is_optional(&self) -> bool {\n+        matches!(self, CachedDataItem::CellData { .. })\n+    }\n }\n \n impl CachedDataItemKey {\n@@ -627,10 +631,6 @@ impl CachedDataItemKey {\n         }\n     }\n \n-    pub fn is_optional(&self) -> bool {\n-        matches!(self, CachedDataItemKey::CellData { .. })\n-    }\n-\n     pub fn category(&self) -> TaskDataCategory {\n         self.ty().category()\n     }\n@@ -670,6 +670,39 @@ impl CachedDataItemType {\n             | Self::Activeness { .. } => TaskDataCategory::All,\n         }\n     }\n+\n+    pub fn is_persistent(&self) -> bool {\n+        match self {\n+            Self::Output\n+            | Self::Collectible\n+            | Self::Dirty\n+            | Self::Child\n+            | Self::CellData\n+            | Self::CellTypeMaxIndex\n+            | Self::OutputDependency\n+            | Self::CellDependency\n+            | Self::CollectiblesDependency\n+            | Self::OutputDependent\n+            | Self::CellDependent\n+            | Self::CollectiblesDependent\n+            | Self::AggregationNumber\n+            | Self::Follower\n+            | Self::Upper\n+            | Self::AggregatedDirtyContainer\n+            | Self::AggregatedCollectible\n+            | Self::AggregatedDirtyContainerCount\n+            | Self::Stateful => true,\n+\n+            Self::Activeness\n+            | Self::InProgress\n+            | Self::InProgressCell\n+            | Self::OutdatedCollectible\n+            | Self::OutdatedOutputDependency\n+            | Self::OutdatedCellDependency\n+            | Self::OutdatedCollectiblesDependency\n+            | Self::Error => false,\n+        }\n+    }\n }\n \n /// Used by the [`get_mut`][crate::backend::storage::get_mut] macro to restrict mutable access to a\n@@ -681,32 +714,18 @@ pub mod allow_mut_access {\n     pub const Activeness: () = ();\n }\n \n-impl CachedDataItemValue {\n+impl CachedDataItemValueRef<'_> {\n     pub fn is_persistent(&self) -> bool {\n         match self {\n-            CachedDataItemValue::Output { value } => !value.is_transient(),\n-            CachedDataItemValue::CellData { value } => {\n+            CachedDataItemValueRef::Output { value } => !value.is_transient(),\n+            CachedDataItemValueRef::CellData { value } => {\n                 registry::get_value_type(value.0).is_serializable()\n             }\n             _ => true,\n         }\n     }\n }\n \n-#[derive(Debug)]\n-pub enum CachedDataUpdate {\n-    /// Sets the current task id.\n-    Task { task: TaskId },\n-    /// An item was added. There was no old value.\n-    New { item: CachedDataItem },\n-    /// An item was removed.\n-    Removed { old_item: CachedDataItem },\n-    /// An item was replaced. This is step 1 and tells about the key and the old value\n-    Replace1 { old_item: CachedDataItem },\n-    /// An item was replaced. This is step 2 and tells about the new value.\n-    Replace2 { value: CachedDataItemValue },\n-}\n-\n #[cfg(test)]\n mod tests {\n \n@@ -716,6 +735,5 @@ mod tests {\n         assert_eq!(std::mem::size_of::<super::CachedDataItemKey>(), 20);\n         assert_eq!(std::mem::size_of::<super::CachedDataItemValue>(), 32);\n         assert_eq!(std::mem::size_of::<super::CachedDataItemStorage>(), 48);\n-        assert_eq!(std::mem::size_of::<super::CachedDataUpdate>(), 48);\n     }\n }"
        },
        {
            "sha": "9b20939a3e8b512cf02959d90ae806b27432e3fb",
            "filename": "turbopack/crates/turbo-tasks-backend/src/data_storage.rs",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/vercel/next.js/blob/0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fdata_storage.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fdata_storage.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fdata_storage.rs?ref=0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9",
            "patch": "@@ -32,7 +32,7 @@ fn value_to_key_value<V>(value: &V) -> (&(), &V) {\n     (&(), value)\n }\n \n-#[derive(Debug)]\n+#[derive(Debug, Clone)]\n pub struct OptionStorage<V> {\n     value: Option<V>,\n }\n@@ -121,7 +121,7 @@ impl<V> Storage for OptionStorage<V> {\n     }\n }\n \n-#[derive(Debug)]\n+#[derive(Debug, Clone)]\n pub struct AutoMapStorage<K, V> {\n     map: AutoMap<K, V, BuildHasherDefault<FxHasher>, 1>,\n }"
        },
        {
            "sha": "49f6fd8ac01b0633a8ecc1bfdda1ae4cd74c4ac9",
            "filename": "turbopack/crates/turbo-tasks-backend/src/kv_backing_storage.rs",
            "status": "modified",
            "additions": 129,
            "deletions": 324,
            "changes": 453,
            "blob_url": "https://github.com/vercel/next.js/blob/0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fkv_backing_storage.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fkv_backing_storage.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fkv_backing_storage.rs?ref=0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9",
            "patch": "@@ -1,17 +1,16 @@\n-use std::{borrow::Borrow, cmp::max, collections::hash_map::Entry, sync::Arc};\n+use std::{borrow::Borrow, cmp::max, sync::Arc};\n \n use anyhow::{anyhow, Context, Result};\n use rayon::iter::{IndexedParallelIterator, IntoParallelIterator, ParallelIterator};\n-use rustc_hash::FxHashMap;\n-use serde::{ser::SerializeSeq, Serialize};\n+use serde::Serialize;\n use smallvec::SmallVec;\n use tracing::Span;\n-use turbo_tasks::{backend::CachedTaskType, turbo_tasks_scope, KeyValuePair, SessionId, TaskId};\n+use turbo_tasks::{backend::CachedTaskType, turbo_tasks_scope, SessionId, TaskId};\n \n use crate::{\n     backend::{AnyOperation, TaskDataCategory},\n     backing_storage::BackingStorage,\n-    data::{CachedDataItem, CachedDataItemKey, CachedDataItemValue, CachedDataUpdate},\n+    data::CachedDataItem,\n     database::{\n         key_value_database::{KeySpace, KeyValueDatabase},\n         write_batch::{\n@@ -54,6 +53,7 @@ fn pot_ser_symbol_map() -> pot::ser::SymbolMap {\n     pot::ser::SymbolMap::new().with_compatibility(pot::Compatibility::V4)\n }\n \n+#[cfg(feature = \"verify_serialization\")]\n fn pot_de_symbol_list<'l>() -> pot::de::SymbolList<'l> {\n     pot::de::SymbolList::new()\n }\n@@ -152,40 +152,38 @@ impl<T: KeyValueDatabase + Send + Sync + 'static> BackingStorage\n         get(&self.database).unwrap_or_default()\n     }\n \n-    fn save_snapshot(\n+    fn serialize(task: TaskId, data: &Vec<CachedDataItem>) -> Result<SmallVec<[u8; 16]>> {\n+        serialize(task, data)\n+    }\n+\n+    fn save_snapshot<I>(\n         &self,\n         session_id: SessionId,\n         operations: Vec<Arc<AnyOperation>>,\n         task_cache_updates: Vec<ChunkedVec<(Arc<CachedTaskType>, TaskId)>>,\n-        meta_updates: Vec<ChunkedVec<CachedDataUpdate>>,\n-        data_updates: Vec<ChunkedVec<CachedDataUpdate>>,\n-    ) -> Result<()> {\n+        snapshots: Vec<I>,\n+    ) -> Result<()>\n+    where\n+        I: Iterator<\n+                Item = (\n+                    TaskId,\n+                    Option<SmallVec<[u8; 16]>>,\n+                    Option<SmallVec<[u8; 16]>>,\n+                ),\n+            > + Send\n+            + Sync,\n+    {\n         let _span = tracing::trace_span!(\"save snapshot\", session_id = ?session_id, operations = operations.len());\n         let mut batch = self.database.write_batch()?;\n-        let mut task_meta_items_result = Ok(Vec::new());\n-        let mut task_data_items_result = Ok(Vec::new());\n+        let mut task_items_result = Ok(Vec::new());\n \n         // Start organizing the updates in parallel\n         match &mut batch {\n             WriteBatch::Concurrent(ref batch, _) => {\n                 turbo_tasks::scope(|s| {\n                     s.spawn(|_| {\n                         let _span = tracing::trace_span!(\"update task meta\").entered();\n-                        task_meta_items_result = process_task_data(\n-                            &self.database,\n-                            KeySpace::TaskMeta,\n-                            meta_updates,\n-                            Some(batch),\n-                        );\n-                    });\n-                    s.spawn(|_| {\n-                        let _span = tracing::trace_span!(\"update task data\").entered();\n-                        task_data_items_result = process_task_data(\n-                            &self.database,\n-                            KeySpace::TaskData,\n-                            data_updates,\n-                            Some(batch),\n-                        );\n+                        task_items_result = process_task_data(snapshots, Some(batch));\n                     });\n \n                     let mut next_task_id =\n@@ -260,26 +258,13 @@ impl<T: KeyValueDatabase + Send + Sync + 'static> BackingStorage\n                     anyhow::Ok(())\n                 })?;\n \n-                task_meta_items_result?;\n-                task_data_items_result?;\n+                task_items_result?;\n             }\n             WriteBatch::Serial(batch) => {\n                 turbo_tasks::scope(|s| {\n                     s.spawn(|_| {\n-                        task_meta_items_result = process_task_data(\n-                            &self.database,\n-                            KeySpace::TaskMeta,\n-                            meta_updates,\n-                            None::<&T::ConcurrentWriteBatch<'_>>,\n-                        );\n-                    });\n-                    s.spawn(|_| {\n-                        task_data_items_result = process_task_data(\n-                            &self.database,\n-                            KeySpace::TaskData,\n-                            data_updates,\n-                            None::<&T::ConcurrentWriteBatch<'_>>,\n-                        );\n+                        task_items_result =\n+                            process_task_data(snapshots, None::<&T::ConcurrentWriteBatch<'_>>);\n                     });\n \n                     let mut next_task_id =\n@@ -330,28 +315,25 @@ impl<T: KeyValueDatabase + Send + Sync + 'static> BackingStorage\n                     anyhow::Ok(())\n                 })?;\n \n-                let jobs = [\n-                    (\n-                        KeySpace::TaskMeta,\n-                        tracing::trace_span!(\"update task meta\"),\n-                        task_meta_items_result?,\n-                    ),\n-                    (\n-                        KeySpace::TaskData,\n-                        tracing::trace_span!(\"update task data\"),\n-                        task_data_items_result?,\n-                    ),\n-                ];\n-                for (key_space, span, task_items) in jobs {\n-                    let _span = span.entered();\n-                    for (task_id, value) in task_items.into_iter().flatten() {\n-                        batch\n-                            .put(\n-                                key_space,\n-                                WriteBuffer::Borrowed(IntKey::new(*task_id).as_ref()),\n-                                value,\n-                            )\n-                            .with_context(|| anyhow!(\"Unable to write data items for {task_id}\"))?;\n+                {\n+                    let _span = tracing::trace_span!(\"update tasks\").entered();\n+                    for (task_id, meta, data) in task_items_result?.into_iter().flatten() {\n+                        let key = IntKey::new(*task_id);\n+                        let key = key.as_ref();\n+                        if let Some(meta) = meta {\n+                            batch\n+                                .put(KeySpace::TaskMeta, WriteBuffer::Borrowed(key), meta)\n+                                .with_context(|| {\n+                                    anyhow!(\"Unable to write meta items for {task_id}\")\n+                                })?;\n+                        }\n+                        if let Some(data) = data {\n+                            batch\n+                                .put(KeySpace::TaskData, WriteBuffer::Borrowed(key), data)\n+                                .with_context(|| {\n+                                    anyhow!(\"Unable to write data items for {task_id}\")\n+                                })?;\n+                        }\n                     }\n                 }\n             }\n@@ -549,291 +531,114 @@ fn serialize_task_type(\n     Ok(())\n }\n \n-type SerializedTasks = Vec<Vec<(TaskId, WriteBuffer<'static>)>>;\n-type TaskUpdates =\n-    FxHashMap<CachedDataItemKey, (Option<CachedDataItemValue>, Option<CachedDataItemValue>)>;\n+type SerializedTasks = Vec<\n+    Vec<(\n+        TaskId,\n+        Option<WriteBuffer<'static>>,\n+        Option<WriteBuffer<'static>>,\n+    )>,\n+>;\n \n-fn process_task_data<'a, B: ConcurrentWriteBatch<'a> + Send + Sync>(\n-    database: &(impl KeyValueDatabase + Sync),\n-    key_space: KeySpace,\n-    updates: Vec<ChunkedVec<CachedDataUpdate>>,\n+fn process_task_data<'a, B: ConcurrentWriteBatch<'a> + Send + Sync, I>(\n+    tasks: Vec<I>,\n     batch: Option<&B>,\n-) -> Result<SerializedTasks> {\n+) -> Result<SerializedTasks>\n+where\n+    I: Iterator<\n+            Item = (\n+                TaskId,\n+                Option<SmallVec<[u8; 16]>>,\n+                Option<SmallVec<[u8; 16]>>,\n+            ),\n+        > + Send\n+        + Sync,\n+{\n     let span = Span::current();\n     let turbo_tasks = turbo_tasks::turbo_tasks();\n     let handle = tokio::runtime::Handle::current();\n-    updates\n+    tasks\n         .into_par_iter()\n-        .with_max_len(1)\n-        .map(|updates| {\n+        .map(|tasks| {\n             let _span = span.clone().entered();\n             let _guard = handle.clone().enter();\n             turbo_tasks_scope(turbo_tasks.clone(), || {\n-                let mut task_updates: FxHashMap<TaskId, TaskUpdates> =\n-                    FxHashMap::with_capacity_and_hasher(updates.len(), Default::default());\n-\n-                {\n-                    let span = tracing::trace_span!(\n-                        \"organize updates\",\n-                        updates = updates.len(),\n-                        tasks = tracing::field::Empty\n-                    )\n-                    .entered();\n-\n-                    // The store the last task data and the last value as pointers to avoid looking\n-                    // them up in the map again. Everytime we modify the map the pointers are\n-                    // updated, so we never have a dangling pointer.\n-                    let mut current_task_data: Option<*mut TaskUpdates> = None;\n-                    let mut last_value: Option<*mut (\n-                        Option<CachedDataItemValue>,\n-                        Option<CachedDataItemValue>,\n-                    )> = None;\n-\n-                    // Organize the updates by task\n-                    for update in updates.into_iter() {\n-                        match update {\n-                            CachedDataUpdate::Task { task } => {\n-                                current_task_data = Some(task_updates.entry(task).or_default())\n-                            }\n-                            CachedDataUpdate::New { item } => {\n-                                let data = current_task_data\n-                                    .expect(\"Task update must be before data updates\");\n-                                // Safety: task_updates are not modified while we hold this pointer.\n-                                // We update the pointer every time we update the map.\n-                                let data = unsafe { &mut *data };\n-                                let (key, new_value) = item.into_key_and_value();\n-                                match data.entry(key) {\n-                                    Entry::Occupied(mut entry) => {\n-                                        let entry = entry.get_mut();\n-                                        entry.1 = Some(new_value);\n-                                        last_value = Some(entry);\n-                                    }\n-                                    Entry::Vacant(entry) => {\n-                                        last_value = Some(entry.insert((None, Some(new_value))));\n-                                    }\n-                                }\n-                            }\n-                            CachedDataUpdate::Removed { old_item } => {\n-                                let data = current_task_data\n-                                    .expect(\"Task update must be before data updates\");\n-                                // Safety: task_updates are not modified while we hold this pointer.\n-                                // We update the pointer every time we update the map.\n-                                let data = unsafe { &mut *data };\n-                                let (key, old_value) = old_item.into_key_and_value();\n-                                match data.entry(key) {\n-                                    Entry::Occupied(mut entry) => {\n-                                        let entry = entry.get_mut();\n-                                        entry.1 = None;\n-                                        last_value = Some(entry);\n-                                    }\n-                                    Entry::Vacant(entry) => {\n-                                        last_value = Some(entry.insert((Some(old_value), None)));\n-                                    }\n-                                }\n-                            }\n-                            CachedDataUpdate::Replace1 { old_item } => {\n-                                let data = current_task_data\n-                                    .expect(\"Task update must be before data updates\");\n-                                // Safety: task_updates are not modified while we hold this pointer.\n-                                // We update the pointer every time we update the map.\n-                                let data = unsafe { &mut *data };\n-                                let (key, old_value) = old_item.into_key_and_value();\n-                                match data.entry(key) {\n-                                    Entry::Occupied(mut entry) => {\n-                                        last_value = Some(entry.get_mut());\n-                                    }\n-                                    Entry::Vacant(entry) => {\n-                                        last_value = Some(entry.insert((Some(old_value), None)));\n-                                    }\n-                                }\n-                            }\n-                            CachedDataUpdate::Replace2 { value: new_value } => {\n-                                let last_value =\n-                                    last_value.expect(\"Task update must be before data updates\");\n-                                // Safety: the inner map of task_updates is not modified while we\n-                                // hold this pointer. We update the\n-                                // pointer every time we update the map.\n-                                let last_value = unsafe { &mut *last_value };\n-                                last_value.1 = Some(new_value);\n-                            }\n+                let mut result = Vec::new();\n+                for (task_id, meta, data) in tasks {\n+                    if let Some(batch) = batch {\n+                        let key = IntKey::new(*task_id);\n+                        let key = key.as_ref();\n+                        if let Some(meta) = meta {\n+                            batch.put(\n+                                KeySpace::TaskMeta,\n+                                WriteBuffer::Borrowed(key),\n+                                WriteBuffer::SmallVec(meta),\n+                            )?;\n                         }\n-                    }\n-\n-                    span.record(\"tasks\", task_updates.len());\n-                }\n-\n-                {\n-                    let span = tracing::trace_span!(\n-                        \"dedupe updates\",\n-                        before = task_updates.len(),\n-                        after = tracing::field::Empty\n-                    )\n-                    .entered();\n-\n-                    // Remove no-op task updates (so we have less tasks to restore)\n-                    task_updates.retain(|_, data| {\n-                        data.retain(|_, (old_value, value)| *old_value != *value);\n-                        !data.is_empty()\n-                    });\n-\n-                    span.record(\"after\", task_updates.len());\n-                }\n-\n-                let tx = database.begin_read_transaction()?;\n-\n-                let span = tracing::trace_span!(\n-                    \"restore, update and serialize\",\n-                    tasks = task_updates.len(),\n-                    restored_tasks = tracing::field::Empty\n-                )\n-                .entered();\n-                let mut restored_tasks = 0;\n-\n-                // Restore the old task data, apply the updates and serialize the new data\n-                let mut tasks = if batch.is_some() {\n-                    Vec::new()\n-                } else {\n-                    Vec::with_capacity(task_updates.len())\n-                };\n-                for (task, mut updates) in task_updates {\n-                    // Restore the old task data\n-                    if let Some(old_data) =\n-                        database.get(&tx, key_space, IntKey::new(*task).as_ref())?\n-                    {\n-                        let old_data: Vec<CachedDataItem> = match POT_CONFIG\n-                            .deserialize(old_data.borrow())\n-                        {\n-                            Ok(d) => d,\n-                            Err(_) => serde_path_to_error::deserialize(\n-                                &mut pot_de_symbol_list()\n-                                    .deserializer_for_slice(old_data.borrow())?,\n-                            )\n-                            .with_context(|| {\n-                                let old_data: &[u8] = old_data.borrow();\n-                                anyhow!(\"Unable to deserialize old value of {task}: {old_data:?}\")\n-                            })?,\n-                        };\n-\n-                        // Reserve capacity to avoid rehashing later\n-                        updates.reserve(old_data.len());\n-\n-                        // Apply the old data to the updates, so updates includes the whole data\n-                        for item in old_data.into_iter() {\n-                            let (key, value) = item.into_key_and_value();\n-                            updates.entry(key).or_insert((None, Some(value)));\n+                        if let Some(data) = data {\n+                            batch.put(\n+                                KeySpace::TaskData,\n+                                WriteBuffer::Borrowed(key),\n+                                WriteBuffer::SmallVec(data),\n+                            )?;\n                         }\n-                        restored_tasks += 1;\n-                    }\n-\n-                    // Remove all deletions\n-                    updates.retain(|_, (_, value)| value.is_some());\n-\n-                    // Serialize new data\n-                    let value = serialize(task, &mut updates)?;\n-\n-                    if let Some(batch) = batch {\n-                        batch.put(\n-                            key_space,\n-                            WriteBuffer::Borrowed(IntKey::new(*task).as_ref()),\n-                            WriteBuffer::SmallVec(value),\n-                        )?;\n                     } else {\n                         // Store the new task data\n-                        tasks.push((task, WriteBuffer::SmallVec(value)));\n+                        result.push((\n+                            task_id,\n+                            meta.map(WriteBuffer::SmallVec),\n+                            data.map(WriteBuffer::SmallVec),\n+                        ));\n                     }\n                 }\n \n-                span.record(\"restored_tasks\", restored_tasks);\n-                Ok(tasks)\n+                Ok(result)\n             })\n         })\n         .collect::<Result<Vec<_>>>()\n }\n \n-fn serialize(task: TaskId, data: &mut TaskUpdates) -> Result<SmallVec<[u8; 16]>> {\n-    Ok(\n-        match pot_serialize_small_vec(&SerializeLikeVecOfCachedDataItem(data)) {\n-            #[cfg(not(feature = \"verify_serialization\"))]\n-            Ok(value) => value,\n-            _ => {\n-                let mut error = Ok(());\n-                data.retain(|key, (_, value)| {\n-                    let mut buf = Vec::<u8>::new();\n-                    let mut symbol_map = pot_ser_symbol_map();\n-                    let mut serializer = symbol_map.serializer_for(&mut buf).unwrap();\n-                    if let Err(err) = serde_path_to_error::serialize(\n-                        &SerializeLikeCachedDataItem(\n-                            key,\n-                            value\n-                                .as_ref()\n-                                .expect(\"serialize data must not contain None values\"),\n-                        ),\n-                        &mut serializer,\n-                    ) {\n-                        if key.is_optional() {\n-                            #[cfg(feature = \"verify_serialization\")]\n+fn serialize(task: TaskId, data: &Vec<CachedDataItem>) -> Result<SmallVec<[u8; 16]>> {\n+    Ok(match pot_serialize_small_vec(data) {\n+        #[cfg(not(feature = \"verify_serialization\"))]\n+        Ok(value) => value,\n+        _ => {\n+            let mut error = Ok(());\n+            let mut data = data.clone();\n+            data.retain(|item| {\n+                let mut buf = Vec::<u8>::new();\n+                let mut symbol_map = pot_ser_symbol_map();\n+                let mut serializer = symbol_map.serializer_for(&mut buf).unwrap();\n+                if let Err(err) = serde_path_to_error::serialize(&item, &mut serializer) {\n+                    if item.is_optional() {\n+                        #[cfg(feature = \"verify_serialization\")]\n+                        println!(\"Skipping non-serializable optional item for {task}: {item:?}\");\n+                    } else {\n+                        error = Err(err).context({\n+                            anyhow!(\"Unable to serialize data item for {task}: {item:?}\")\n+                        });\n+                    }\n+                    false\n+                } else {\n+                    #[cfg(feature = \"verify_serialization\")]\n+                    {\n+                        let deserialize: Result<CachedDataItem, _> =\n+                            serde_path_to_error::deserialize(\n+                                &mut pot_de_symbol_list().deserializer_for_slice(&buf).unwrap(),\n+                            );\n+                        if let Err(err) = deserialize {\n                             println!(\n-                                \"Skipping non-serializable optional item: {key:?} = {value:?}\"\n+                                \"Data item would not be deserializable {task}: {err:?}\\n{item:?}\"\n                             );\n-                        } else {\n-                            error = Err(err).context({\n-                                anyhow!(\n-                                    \"Unable to serialize data item for {task}: {key:?} = \\\n-                                     {value:#?}\"\n-                                )\n-                            });\n-                        }\n-                        false\n-                    } else {\n-                        #[cfg(feature = \"verify_serialization\")]\n-                        {\n-                            let deserialize: Result<CachedDataItem, _> =\n-                                serde_path_to_error::deserialize(\n-                                    &mut pot_de_symbol_list().deserializer_for_slice(&buf).unwrap(),\n-                                );\n-                            if let Err(err) = deserialize {\n-                                println!(\n-                                    \"Data item would not be deserializable {task}: \\\n-                                     {err:?}\\n{key:?} = {value:#?}\"\n-                                );\n-                                return false;\n-                            }\n+                            return false;\n                         }\n-                        true\n                     }\n-                });\n-                error?;\n-\n-                pot_serialize_small_vec(&SerializeLikeVecOfCachedDataItem(data)).with_context(\n-                    || anyhow!(\"Unable to serialize data items for {task}: {data:#?}\"),\n-                )?\n-            }\n-        },\n-    )\n-}\n+                    true\n+                }\n+            });\n+            error?;\n \n-struct SerializeLikeVecOfCachedDataItem<'l>(&'l TaskUpdates);\n-\n-impl Serialize for SerializeLikeVecOfCachedDataItem<'_> {\n-    fn serialize<S: serde::Serializer>(&self, serializer: S) -> Result<S::Ok, S::Error> {\n-        let map = &self.0;\n-        let mut seq = serializer.serialize_seq(Some(map.len()))?;\n-        for (key, (_, value)) in map.iter() {\n-            let value = value\n-                .as_ref()\n-                .expect(\"SerializeLikeVecOfCachedDataItem must not contain None values\");\n-            seq.serialize_element(&SerializeLikeCachedDataItem(key, value))?;\n+            pot_serialize_small_vec(&data)\n+                .with_context(|| anyhow!(\"Unable to serialize data items for {task}: {data:#?}\"))?\n         }\n-        seq.end()\n-    }\n-}\n-\n-struct SerializeLikeCachedDataItem<'l>(&'l CachedDataItemKey, &'l CachedDataItemValue);\n-\n-impl Serialize for SerializeLikeCachedDataItem<'_> {\n-    fn serialize<S: serde::Serializer>(&self, serializer: S) -> Result<S::Ok, S::Error> {\n-        // TODO add CachedDataItemRef to avoid cloning\n-        let item = CachedDataItem::from_key_and_value(*self.0, self.1.clone());\n-        item.serialize(serializer)\n-    }\n+    })\n }"
        },
        {
            "sha": "21bd576d462ccf75da1780a2adf99d943e505641",
            "filename": "turbopack/crates/turbo-tasks-backend/src/utils/dash_map_multi.rs",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/vercel/next.js/blob/0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Futils%2Fdash_map_multi.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Futils%2Fdash_map_multi.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Futils%2Fdash_map_multi.rs?ref=0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9",
            "patch": "@@ -25,6 +25,10 @@ unsafe impl<K: Eq + Hash + Sync, V: Sync> Send for RefMut<'_, K, V> {}\n unsafe impl<K: Eq + Hash + Sync, V: Sync> Sync for RefMut<'_, K, V> {}\n \n impl<K: Eq + Hash, V> RefMut<'_, K, V> {\n+    pub fn key(&self) -> &K {\n+        self.pair().0\n+    }\n+\n     pub fn value(&self) -> &V {\n         self.pair().1\n     }"
        },
        {
            "sha": "83727119a3844eb11b84d25dd15a946434f8f2f0",
            "filename": "turbopack/crates/turbo-tasks-backend/src/utils/mod.rs",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/vercel/next.js/blob/0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Futils%2Fmod.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Futils%2Fmod.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Futils%2Fmod.rs?ref=0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9",
            "patch": "@@ -4,3 +4,6 @@ pub mod dash_map_multi;\n pub mod deque_set;\n pub mod ptr_eq_arc;\n pub mod sharded;\n+pub mod swap_retain;\n+\n+pub use swap_retain::swap_retain;"
        },
        {
            "sha": "8103df0719aa384572de019e7afa234a363219d9",
            "filename": "turbopack/crates/turbo-tasks-backend/src/utils/swap_retain.rs",
            "status": "added",
            "additions": 54,
            "deletions": 0,
            "changes": 54,
            "blob_url": "https://github.com/vercel/next.js/blob/0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Futils%2Fswap_retain.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Futils%2Fswap_retain.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Futils%2Fswap_retain.rs?ref=0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9",
            "patch": "@@ -0,0 +1,54 @@\n+use std::ops::{Index, IndexMut};\n+\n+use smallvec::SmallVec;\n+\n+pub trait VecLike<T>: Index<usize, Output = T> + IndexMut<usize, Output = T> {\n+    fn len(&self) -> usize;\n+    fn swap_remove(&mut self, index: usize);\n+}\n+\n+impl<T> VecLike<T> for Vec<T> {\n+    fn len(&self) -> usize {\n+        Vec::len(self)\n+    }\n+\n+    fn swap_remove(&mut self, index: usize) {\n+        Vec::swap_remove(self, index);\n+    }\n+}\n+\n+impl<T, const N: usize> VecLike<T> for SmallVec<[T; N]> {\n+    fn len(&self) -> usize {\n+        SmallVec::len(self)\n+    }\n+\n+    fn swap_remove(&mut self, index: usize) {\n+        SmallVec::swap_remove(self, index);\n+    }\n+}\n+\n+pub fn swap_retain<T>(vec: &mut impl VecLike<T>, mut f: impl FnMut(&mut T) -> bool) {\n+    let mut i = 0;\n+    while i < vec.len() {\n+        if !f(&mut vec[i]) {\n+            vec.swap_remove(i);\n+        } else {\n+            i += 1;\n+        }\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use smallvec::{smallvec, SmallVec};\n+\n+    use super::swap_retain;\n+\n+    #[test]\n+    fn test_swap_retain() {\n+        let mut vec: SmallVec<[i32; 4]> = smallvec![1, 2, 3, 4, 5];\n+        swap_retain(&mut vec, |a| *a % 2 != 0);\n+        let expected: SmallVec<[i32; 4]> = smallvec![1, 5, 3];\n+        assert_eq!(vec, expected);\n+    }\n+}"
        },
        {
            "sha": "baa1b0a00b4527e9ca89a7d27210b2a57b93da96",
            "filename": "turbopack/crates/turbo-tasks-macros/src/derive/key_value_pair_macro.rs",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/vercel/next.js/blob/0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9/turbopack%2Fcrates%2Fturbo-tasks-macros%2Fsrc%2Fderive%2Fkey_value_pair_macro.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9/turbopack%2Fcrates%2Fturbo-tasks-macros%2Fsrc%2Fderive%2Fkey_value_pair_macro.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-macros%2Fsrc%2Fderive%2Fkey_value_pair_macro.rs?ref=0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9",
            "patch": "@@ -219,6 +219,15 @@ pub fn derive_key_value_pair(input: TokenStream) -> TokenStream {\n                 }\n             }\n \n+            fn from_key_and_value_ref(key: #key_name, value_ref: #value_ref_name) -> Self {\n+                match (key, value_ref) {\n+                    #(\n+                        (#key_name::#variant_names { #key_pat }, #value_ref_name::#variant_names { #value_pat }) => #ident::#variant_names { #key_pat #value_clone_fields },\n+                    )*\n+                    _ => panic!(\"Invalid key and value combination\"),\n+                }\n+            }\n+\n             fn into_key_and_value(self) -> (#key_name, #value_name) {\n                 match self {\n                     #(\n@@ -303,6 +312,7 @@ pub fn derive_key_value_pair(input: TokenStream) -> TokenStream {\n             }\n         }\n \n+        #[derive(Debug, Clone)]\n         #vis enum #storage_name {\n             #(\n                 #variant_names {"
        },
        {
            "sha": "1da615e85d89f4ef465e38ae063449b7c461213c",
            "filename": "turbopack/crates/turbo-tasks/src/effect.rs",
            "status": "modified",
            "additions": 18,
            "deletions": 26,
            "changes": 44,
            "blob_url": "https://github.com/vercel/next.js/blob/0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Feffect.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Feffect.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Feffect.rs?ref=0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9",
            "patch": "@@ -168,15 +168,18 @@ pub async fn apply_effects(source: impl CollectiblesSource) -> Result<()> {\n     }\n     let span = tracing::info_span!(\"apply effects\", count = effects.len());\n     async move {\n-        let mut first_error = anyhow::Ok(());\n-        for effect in effects {\n-            let Some(effect) = Vc::try_resolve_downcast_type::<EffectInstance>(effect).await?\n-            else {\n-                panic!(\"Effect must only be implemented by EffectInstance\");\n-            };\n-            apply_effect(&effect.await?, &mut first_error).await;\n-        }\n-        first_error\n+        effects\n+            .into_iter()\n+            .map(async |effect| {\n+                let Some(effect) = Vc::try_resolve_downcast_type::<EffectInstance>(effect).await?\n+                else {\n+                    panic!(\"Effect must only be implemented by EffectInstance\");\n+                };\n+                effect.await?.apply().await\n+            })\n+            .try_join()\n+            .await?;\n+        Ok(())\n     }\n     .instrument(span)\n     .await\n@@ -249,29 +252,18 @@ impl Effects {\n     pub async fn apply(&self) -> Result<()> {\n         let span = tracing::info_span!(\"apply effects\", count = self.effects.len());\n         async move {\n-            let mut first_error = anyhow::Ok(());\n-            for effect in self.effects.iter() {\n-                apply_effect(effect, &mut first_error).await;\n-            }\n-            first_error\n+            self.effects\n+                .iter()\n+                .map(async |effect| effect.apply().await)\n+                .try_join()\n+                .await?;\n+            Ok(())\n         }\n         .instrument(span)\n         .await\n     }\n }\n \n-async fn apply_effect(\n-    effect: &ReadRef<EffectInstance>,\n-    first_error: &mut std::result::Result<(), anyhow::Error>,\n-) {\n-    match effect.apply().await {\n-        Err(err) if first_error.is_ok() => {\n-            *first_error = Err(err);\n-        }\n-        _ => {}\n-    }\n-}\n-\n #[cfg(test)]\n mod tests {\n     use crate::{apply_effects, get_effects, CollectiblesSource};"
        },
        {
            "sha": "c5de5eb265b4f444afe933d14eecd122b914f791",
            "filename": "turbopack/crates/turbo-tasks/src/key_value_pair.rs",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/vercel/next.js/blob/0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Fkey_value_pair.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Fkey_value_pair.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Fkey_value_pair.rs?ref=0dd0f20a2f6c4d9b5150cdb1f20a3258bb0328b9",
            "patch": "@@ -16,5 +16,6 @@ pub trait KeyValuePair {\n     fn value_ref(&self) -> Self::ValueRef<'_>;\n     fn value_mut(&mut self) -> Self::ValueRefMut<'_>;\n     fn from_key_and_value(key: Self::Key, value: Self::Value) -> Self;\n+    fn from_key_and_value_ref(key: Self::Key, value_ref: Self::ValueRef<'_>) -> Self;\n     fn into_key_and_value(self) -> (Self::Key, Self::Value);\n }"
        }
    ],
    "stats": {
        "total": 1763,
        "additions": 931,
        "deletions": 832
    }
}