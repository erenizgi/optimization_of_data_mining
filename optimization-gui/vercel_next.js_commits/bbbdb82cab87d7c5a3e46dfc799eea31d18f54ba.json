{
    "author": "sokra",
    "message": "Turbopack: cleanup db log and add verbose option (#85965)\n\n### What?\n\nPrints nice graphs into the database LOG file.\n\nExample:\n\n```\n  2 | 00000405 | Compaction:\n  2 | 00000405 | MERGE (1311566 keys):\n  2 | 00000405 | 00000349 INPUT  |                                                                            [======]                  | c00004c1488ef798-d40e1767c1ee6786 \n  2 | 00000405 | 00000353 INPUT  |                                                                                   [========]         | d40e1a134691c633-e91bfab1b0dab95a \n  2 | 00000405 | 00000357 INPUT  |                                                                            [===========]             | c00084722c196aa3-e0d85966f3086922 \n  2 | 00000405 | 00000358 INPUT  |                                                                                        [===========] | e0d9aaacdbea0b12-ffffbaef025822aa \n  2 | 00000405 | 00000365 INPUT  |                                                                                            [===]     | e91bfe6c3e8c62ea-f5c19d448c217d89 \n  2 | 00000405 | 00000366 INPUT  |                                                                                                [===] | f5c1a023c1b77b86-fffffe0d895c3b37 \n  2 | 00000405 | 00000372 INPUT  |                                                                            [=======================] | c00084722c196aa3-ffffbaef025822aa \n  2 | 00000405 | 00000374 INPUT  |                                                                            [=======================] | c0250dd8b533294a-ffb01c4e6519a470 \n  2 | 00000405 | 00000384 OUTPUT |                                                                            [======]                  | c00004c1488ef798-d40e1767c1ee6786\n  2 | 00000405 | 00000388 OUTPUT |                                                                                   [========]         | d40e1a134691c633-e91bfab1b0dab95a\n  2 | 00000405 | 00000393 OUTPUT |                                                                            [===========]             | c00084722c196aa3-e0d85966f3086922\n  2 | 00000405 | 00000394 OUTPUT |                                                                                        [===========] | e0d9aaacdbea0b12-ffffbaef025822aa\n  2 | 00000405 | 00000401 OUTPUT |                                                                                            [===]     | e91bfe6c3e8c62ea-f5c19d448c217d89\n  2 | 00000405 | 00000402 OUTPUT |                                                                                                [===] | f5c1a023c1b77b86-fffffe0d895c3b37\n  2 | 00000405 | MERGE (1277134 keys):\n  2 | 00000405 | 00000347 INPUT  |                          [=======]                                                                   | 4000034716c961a1-549678c058a07106 \n  2 | 00000405 | 00000351 INPUT  |                                  [=======]                                                           | 54967af27cc1ec1d-6979dbc1b9969add \n  2 | 00000405 | 00000354 INPUT  |                          [=======================]                                                   | 400174554370f74b-7fff39f8ec73f8b7 \n  2 | 00000405 | 00000361 INPUT  |                                          [====]                                                      | 6979dd7b4df64f96-75e162f9f837a260 \n  2 | 00000405 | 00000362 INPUT  |                                               [==]                                                   | 75e1648525c2154a-7ffffea14ef427f3 \n  2 | 00000405 | 00000371 INPUT  |                          [=======================]                                                   | 400174554370f74b-7fff39f8ec73f8b7 \n  2 | 00000405 | 00000373 INPUT  |                          [=======================]                                                   | 40038dd87f963da6-7fff0105911c9647 \n  2 | 00000405 | 00000382 OUTPUT |                          [=======]                                                                   | 4000034716c961a1-549678c058a07106\n  2 | 00000405 | 00000386 OUTPUT |                                  [=======]                                                           | 54967af27cc1ec1d-6979dbc1b9969add\n  2 | 00000405 | 00000390 OUTPUT |                          [=======================]                                                   | 400174554370f74b-7fff39f8ec73f8b7\n  2 | 00000405 | 00000391 OUTPUT |                                          [====]                                                      | 6979dd7b4df64f96-75e162f9f837a260\n  2 | 00000405 | 00000392 OUTPUT |                                               [==]                                                   | 75e1648525c2154a-7ffffea14ef427f3\n  2 | 00000405 | MERGE (1389734 keys):\n  2 | 00000405 | 00000348 INPUT  | [=======]                                                                                            | 00000da30f0f1507-15bdf81f772b13ab \n  2 | 00000405 | 00000352 INPUT  |         [======]                                                                                     | 15bdfdf36416a0c5-27bb1097c991db6c \n  2 | 00000405 | 00000359 INPUT  | [===========]                                                                                        | 0000bed76899fc20-20f90e531b79c213 \n  2 | 00000405 | 00000360 INPUT  |             [===========]                                                                            | 20fa2f7e9952973f-3fff4d10047dbfc4 \n  2 | 00000405 | 00000367 INPUT  |                [====]                                                                                | 27bb1d45d57871a5-33d7f6c31d6a87f2 \n  2 | 00000405 | 00000368 INPUT  |                     [===]                                                                            | 33d7f95aefeb6ca0-3ffff899c293a0b4 \n  2 | 00000405 | 00000376 INPUT  | [=======================]                                                                            | 0000bed76899fc20-3fff4d10047dbfc4 \n  2 | 00000405 | 00000385 OUTPUT | [=======]                                                                                            | 00000da30f0f1507-15bdf81f772b13ab\n  2 | 00000405 | 00000389 OUTPUT |         [======]                                                                                     | 15bdfdf36416a0c5-27bb1097c991db6c\n  2 | 00000405 | 00000397 OUTPUT | [===========]                                                                                        | 0000bed76899fc20-20f90e531b79c213\n  2 | 00000405 | 00000398 OUTPUT |             [===========]                                                                            | 20fa2f7e9952973f-3fff4d10047dbfc4\n  2 | 00000405 | 00000403 OUTPUT |                [====]                                                                                | 27bb1d45d57871a5-33d7f6c31d6a87f2\n  2 | 00000405 | 00000404 OUTPUT |                     [===]                                                                            | 33d7f95aefeb6ca0-3ffff899c293a0b4\n  2 | 00000405 | MERGE (1362988 keys):\n  2 | 00000405 | 00000346 INPUT  |                                                   [=======]                                          | 8000063004f10520-96da09f7fbce981e \n  2 | 00000405 | 00000350 INPUT  |                                                           [======]                                   | 96da108f86d302c8-a6f4e6fc64d75324 \n  2 | 00000405 | 00000355 INPUT  |                                                   [============]                                     | 8000b4968b026f12-a18f4d0889cdaa27 \n  2 | 00000405 | 00000356 INPUT  |                                                                [==========]                          | a190533ade79230a-bfff4bd5dbeb964b \n  2 | 00000405 | 00000363 INPUT  |                                                                  [====]                              | a6f4e8052ab381ac-b37e203806567e5a \n  2 | 00000405 | 00000364 INPUT  |                                                                       [===]                          | b37e2083733ac276-bffffe42b4a8071b \n  2 | 00000405 | 00000375 INPUT  |                                                   [=======================]                          | 8000b4968b026f12-bfff4bd5dbeb964b \n  2 | 00000405 | 00000383 OUTPUT |                                                   [=======]                                          | 8000063004f10520-96da09f7fbce981e\n  2 | 00000405 | 00000387 OUTPUT |                                                           [======]                                   | 96da108f86d302c8-a6f4e6fc64d75324\n  2 | 00000405 | 00000395 OUTPUT |                                                   [============]                                     | 8000b4968b026f12-a18f4d0889cdaa27\n  2 | 00000405 | 00000396 OUTPUT |                                                                [==========]                          | a190533ade79230a-bfff4bd5dbeb964b\n  2 | 00000405 | 00000399 OUTPUT |                                                                  [====]                              | a6f4e8052ab381ac-b37e203806567e5a\n  2 | 00000405 | 00000400 OUTPUT |                                                                       [===]                          | b37e2083733ac276-bffffe42b4a8071b\nTime 2025-11-10T11:05:02.041664Z\nCommit 00000406 5341422 keys in 333ms 302Âµs\nFAM | META SEQ | SST SEQ        | RANGE\n  2 | 00000405 | 00000384 SST    |                                                                            [======]                  | c00004c1488ef798-d40e1767c1ee6786 (96 MiB, cold)\n  2 | 00000405 | 00000388 SST    |                                                                                   [========]         | d40e1a134691c633-e91bfab1b0dab95a (96 MiB, cold)\n  2 | 00000405 | 00000393 SST    |                                                                            [===========]             | c00084722c196aa3-e0d85966f3086922 (34 MiB, warm)\n  2 | 00000405 | 00000394 SST    |                                                                                        [===========] | e0d9aaacdbea0b12-ffffbaef025822aa (48 MiB, warm)\n  2 | 00000405 | 00000401 SST    |                                                                                            [===]     | e91bfe6c3e8c62ea-f5c19d448c217d89 (72 MiB, cold)\n  2 | 00000405 | 00000402 SST    |                                                                                                [===] | f5c1a023c1b77b86-fffffe0d895c3b37 (77 MiB, cold)\n  2 | 00000405 | 00000382 SST    |                          [=======]                                                                   | 4000034716c961a1-549678c058a07106 (95 MiB, cold)\n  2 | 00000405 | 00000386 SST    |                                  [=======]                                                           | 54967af27cc1ec1d-6979dbc1b9969add (96 MiB, cold)\n  2 | 00000405 | 00000390 SST    |                          [=======================]                                                   | 400174554370f74b-7fff39f8ec73f8b7 (82 MiB, warm)\n  2 | 00000405 | 00000391 SST    |                                          [====]                                                      | 6979dd7b4df64f96-75e162f9f837a260 (70 MiB, cold)\n  2 | 00000405 | 00000392 SST    |                                               [==]                                                   | 75e1648525c2154a-7ffffea14ef427f3 (76 MiB, cold)\n  2 | 00000405 | 00000385 SST    | [=======]                                                                                            | 00000da30f0f1507-15bdf81f772b13ab (95 MiB, cold)\n  2 | 00000405 | 00000389 SST    |         [======]                                                                                     | 15bdfdf36416a0c5-27bb1097c991db6c (95 MiB, cold)\n  2 | 00000405 | 00000397 SST    | [===========]                                                                                        | 0000bed76899fc20-20f90e531b79c213 (41 MiB, warm)\n  2 | 00000405 | 00000398 SST    |             [===========]                                                                            | 20fa2f7e9952973f-3fff4d10047dbfc4 (39 MiB, warm)\n  2 | 00000405 | 00000403 SST    |                [====]                                                                                | 27bb1d45d57871a5-33d7f6c31d6a87f2 (82 MiB, cold)\n  2 | 00000405 | 00000404 SST    |                     [===]                                                                            | 33d7f95aefeb6ca0-3ffff899c293a0b4 (89 MiB, cold)\n  2 | 00000405 | 00000383 SST    |                                                   [=======]                                          | 8000063004f10520-96da09f7fbce981e (95 MiB, cold)\n  2 | 00000405 | 00000387 SST    |                                                           [======]                                   | 96da108f86d302c8-a6f4e6fc64d75324 (94 MiB, cold)\n  2 | 00000405 | 00000395 SST    |                                                   [============]                                     | 8000b4968b026f12-a18f4d0889cdaa27 (40 MiB, warm)\n  2 | 00000405 | 00000396 SST    |                                                                [==========]                          | a190533ade79230a-bfff4bd5dbeb964b (44 MiB, warm)\n  2 | 00000405 | 00000399 SST    |                                                                  [====]                              | a6f4e8052ab381ac-b37e203806567e5a (83 MiB, cold)\n  2 | 00000405 | 00000400 SST    |                                                                       [===]                          | b37e2083733ac276-bffffe42b4a8071b (88 MiB, cold)\n  2 | 00000405 | 00000346 00000347 00000348 00000349 00000350 00000351 00000352 00000353 00000354 00000355 00000356 00000357 00000358 00000359 00000360 OBSOLETE SST\n  2 | 00000405 | 00000361 00000362 00000363 00000364 00000365 00000366 00000367 00000368 00000371 00000372 00000373 00000374 00000375 00000376 OBSOLETE SST\n    |          | 00000346 00000347 00000348 00000349 00000350 00000351 00000352 00000353 00000354 00000355 00000356 00000357 00000358 00000359 00000360 SST DELETED\n    |          | 00000361 00000362 00000363 00000364 00000365 00000366 00000367 00000368 00000371 00000372 00000373 00000374 00000375 00000376 SST DELETED\n    |          | 00000381 META DELETED\n```",
    "sha": "bbbdb82cab87d7c5a3e46dfc799eea31d18f54ba",
    "files": [
        {
            "sha": "4f0b035eb4e8f0be29f57cd411f67871339e685a",
            "filename": "turbopack/crates/turbo-persistence/Cargo.toml",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/vercel/next.js/blob/bbbdb82cab87d7c5a3e46dfc799eea31d18f54ba/turbopack%2Fcrates%2Fturbo-persistence%2FCargo.toml",
            "raw_url": "https://github.com/vercel/next.js/raw/bbbdb82cab87d7c5a3e46dfc799eea31d18f54ba/turbopack%2Fcrates%2Fturbo-persistence%2FCargo.toml",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2FCargo.toml?ref=bbbdb82cab87d7c5a3e46dfc799eea31d18f54ba",
            "patch": "@@ -10,6 +10,7 @@ verify_sst_content = []\n strict_checks = []\n stats = [\"quick_cache/stats\"]\n print_stats = [\"stats\"]\n+verbose_log = []\n \n [dependencies]\n anyhow = { workspace = true }"
        },
        {
            "sha": "c2d3eea23e14e3afc36f4b816a2c525c232ca9f5",
            "filename": "turbopack/crates/turbo-persistence/src/db.rs",
            "status": "modified",
            "additions": 348,
            "deletions": 235,
            "changes": 583,
            "blob_url": "https://github.com/vercel/next.js/blob/bbbdb82cab87d7c5a3e46dfc799eea31d18f54ba/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fdb.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/bbbdb82cab87d7c5a3e46dfc799eea31d18f54ba/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fdb.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fdb.rs?ref=bbbdb82cab87d7c5a3e46dfc799eea31d18f54ba",
            "patch": "@@ -14,6 +14,7 @@ use byteorder::{BE, ReadBytesExt, WriteBytesExt};\n use jiff::Timestamp;\n use memmap2::Mmap;\n use parking_lot::{Mutex, RwLock};\n+use smallvec::SmallVec;\n \n pub use crate::compaction::selector::CompactConfig;\n use crate::{\n@@ -476,7 +477,7 @@ impl<S: ParallelScheduler> TurboPersistence<S> {\n         &self,\n         CommitOptions {\n             mut new_meta_files,\n-            mut new_sst_files,\n+            new_sst_files,\n             mut new_blob_files,\n             mut sst_seq_numbers_to_delete,\n             mut blob_seq_numbers_to_delete,\n@@ -611,35 +612,88 @@ impl<S: ParallelScheduler> TurboPersistence<S> {\n                 writeln!(log, \"Time {time}\")?;\n                 let span = time.until(Timestamp::now())?;\n                 writeln!(log, \"Commit {seq:08} {keys_written} keys in {span:#}\")?;\n-                for (seq, family, ssts, obsolete) in new_meta_info {\n-                    writeln!(log, \"{seq:08} META family:{family}\",)?;\n+                writeln!(log, \"FAM | META SEQ | SST SEQ        | RANGE\")?;\n+                for (meta_seq, family, ssts, obsolete) in new_meta_info {\n                     for (seq, min, max, size) in ssts {\n                         writeln!(\n                             log,\n-                            \"  {seq:08} SST  {min:016x}-{max:016x} {} MiB\",\n+                            \"{family:3} | {meta_seq:08} | {seq:08} SST    | {} ({} MiB)\",\n+                            range_to_str(min, max),\n                             size / 1024 / 1024\n                         )?;\n                     }\n-                    for seq in obsolete {\n-                        writeln!(log, \"  {seq:08} OBSOLETE SST\")?;\n+                    for obsolete in obsolete.chunks(15) {\n+                        write!(log, \"{family:3} | {meta_seq:08} |\")?;\n+                        for seq in obsolete {\n+                            write!(log, \" {seq:08}\")?;\n+                        }\n+                        writeln!(log, \" OBSOLETE SST\")?;\n                     }\n                 }\n-                new_sst_files.sort_unstable_by_key(|(seq, _)| *seq);\n-                for (seq, _) in new_sst_files.iter() {\n-                    writeln!(log, \"{seq:08} NEW SST\")?;\n+\n+                fn write_seq_numbers<W: std::io::Write, T, I>(\n+                    log: &mut W,\n+                    items: I,\n+                    label: &str,\n+                    extract_seq: fn(&T) -> u32,\n+                ) -> std::io::Result<()>\n+                where\n+                    I: IntoIterator<Item = T>,\n+                {\n+                    let items: Vec<T> = items.into_iter().collect();\n+                    for chunk in items.chunks(15) {\n+                        write!(log, \"    |          |\")?;\n+                        for item in chunk {\n+                            write!(log, \" {:08}\", extract_seq(item))?;\n+                        }\n+                        writeln!(log, \" {}\", label)?;\n+                    }\n+                    Ok(())\n                 }\n+\n                 new_blob_files.sort_unstable_by_key(|(seq, _)| *seq);\n-                for (seq, _) in new_blob_files.iter() {\n-                    writeln!(log, \"{seq:08} NEW BLOB\")?;\n-                }\n-                for seq in sst_seq_numbers_to_delete.iter() {\n-                    writeln!(log, \"{seq:08} SST DELETED\")?;\n-                }\n-                for seq in meta_seq_numbers_to_delete.iter() {\n-                    writeln!(log, \"{seq:08} META DELETED\")?;\n-                }\n-                for seq in blob_seq_numbers_to_delete.iter() {\n-                    writeln!(log, \"{seq:08} BLOB DELETED\")?;\n+                write_seq_numbers(&mut log, new_blob_files, \"NEW BLOB\", |&(seq, _)| seq)?;\n+                write_seq_numbers(\n+                    &mut log,\n+                    blob_seq_numbers_to_delete,\n+                    \"BLOB DELETED\",\n+                    |&seq| seq,\n+                )?;\n+                write_seq_numbers(&mut log, sst_seq_numbers_to_delete, \"SST DELETED\", |&seq| {\n+                    seq\n+                })?;\n+                write_seq_numbers(\n+                    &mut log,\n+                    meta_seq_numbers_to_delete,\n+                    \"META DELETED\",\n+                    |&seq| seq,\n+                )?;\n+                #[cfg(feature = \"verbose_log\")]\n+                {\n+                    writeln!(log, \"New database state:\")?;\n+                    writeln!(log, \"FAM | META SEQ | SST SEQ        | RANGE\")?;\n+                    let inner = self.inner.read();\n+                    let families = inner.meta_files.iter().map(|meta| meta.family()).filter({\n+                        let mut set = HashSet::new();\n+                        move |family| set.insert(*family)\n+                    });\n+                    for family in families {\n+                        for meta in inner.meta_files.iter() {\n+                            if meta.family() != family {\n+                                continue;\n+                            }\n+                            let meta_seq = meta.sequence_number();\n+                            for entry in meta.entries().iter() {\n+                                let seq = entry.sequence_number();\n+                                let range = entry.range();\n+                                writeln!(\n+                                    log,\n+                                    \"{family:3} | {meta_seq:08} | {seq:08}        | {}\",\n+                                    range_to_str(range.min_hash, range.max_hash)\n+                                )?;\n+                            }\n+                        }\n+                    }\n                 }\n             }\n             anyhow::Ok(())\n@@ -835,30 +889,7 @@ impl<S: ParallelScheduler> TurboPersistence<S> {\n                         });\n                     }\n \n-                    self.parallel_scheduler.block_in_place(|| {\n-                        let metrics = compute_metrics(&ssts_with_ranges, 0..=u64::MAX);\n-                        let guard = log_mutex.lock();\n-                        let mut log = self.open_log()?;\n-                        writeln!(\n-                            log,\n-                            \"Compaction for family {family} (coverage: {}, overlap: {}, \\\n-                             duplication: {} / {} MiB):\",\n-                            metrics.coverage,\n-                            metrics.overlap,\n-                            metrics.duplication,\n-                            metrics.duplicated_size / 1024 / 1024\n-                        )?;\n-                        for job in merge_jobs.iter() {\n-                            writeln!(log, \"  merge\")?;\n-                            for i in job.iter() {\n-                                let seq = ssts_with_ranges[*i].seq;\n-                                let (min, max) = ssts_with_ranges[*i].range().into_inner();\n-                                writeln!(log, \"    {seq:08} {min:016x}-{max:016x}\")?;\n-                            }\n-                        }\n-                        drop(guard);\n-                        anyhow::Ok(())\n-                    })?;\n+                    let metrics = compute_metrics(&ssts_with_ranges, 0..=u64::MAX);\n \n                     // Later we will remove the merged files\n                     let sst_seq_numbers_to_delete = merge_jobs\n@@ -875,6 +906,7 @@ impl<S: ParallelScheduler> TurboPersistence<S> {\n                             new_sst_files: Vec<(u32, File, StaticSortedFileBuilderMeta<'static>)>,\n                             blob_seq_numbers_to_delete: Vec<u32>,\n                             keys_written: u64,\n+                            indicies: SmallVec<[usize; 1]>,\n                         },\n                         Move {\n                             seq: u32,\n@@ -883,185 +915,193 @@ impl<S: ParallelScheduler> TurboPersistence<S> {\n                     }\n                     let merge_result = self\n                         .parallel_scheduler\n-                        .parallel_map_collect_owned::<_, _, Result<Vec<_>>>(merge_jobs, |indices| {\n-                            let _span = span.clone().entered();\n-                            if indices.len() == 1 {\n-                                // If we only have one file, we can just move it\n-                                let index = indices[0];\n-                                let meta_index = ssts_with_ranges[index].meta_index;\n-                                let index_in_meta = ssts_with_ranges[index].index_in_meta;\n-                                let meta_file = &meta_files[meta_index];\n-                                let entry = meta_file.entry(index_in_meta);\n-                                let amqf = Cow::Borrowed(entry.raw_amqf(meta_file.amqf_data()));\n-                                let meta = StaticSortedFileBuilderMeta {\n-                                    min_hash: entry.min_hash(),\n-                                    max_hash: entry.max_hash(),\n-                                    amqf,\n-                                    key_compression_dictionary_length: entry\n-                                        .key_compression_dictionary_length(),\n-                                    block_count: entry.block_count(),\n-                                    size: entry.size(),\n-                                    entries: 0,\n-                                };\n-                                return Ok(PartialMergeResult::Move {\n-                                    seq: entry.sequence_number(),\n-                                    meta,\n-                                });\n-                            }\n-\n-                            fn create_sst_file<'l, S: ParallelScheduler>(\n-                                parallel_scheduler: &S,\n-                                entries: &[LookupEntry<'l>],\n-                                total_key_size: usize,\n-                                path: &Path,\n-                                seq: u32,\n-                            ) -> Result<(u32, File, StaticSortedFileBuilderMeta<'static>)>\n-                            {\n-                                let _span = tracing::trace_span!(\"write merged sst file\").entered();\n-                                let (meta, file) = parallel_scheduler.block_in_place(|| {\n-                                    write_static_stored_file(\n-                                        entries,\n-                                        total_key_size,\n-                                        &path.join(format!(\"{seq:08}.sst\")),\n-                                    )\n-                                })?;\n-                                Ok((seq, file, meta))\n-                            }\n-\n-                            let mut new_sst_files = Vec::new();\n-\n-                            // Iterate all SST files\n-                            let iters = indices\n-                                .iter()\n-                                .map(|&index| {\n+                        .parallel_map_collect_owned::<_, _, Result<Vec<_>>>(\n+                            merge_jobs,\n+                            |indicies| {\n+                                let _span = span.clone().entered();\n+                                if indicies.len() == 1 {\n+                                    // If we only have one file, we can just move it\n+                                    let index = indicies[0];\n                                     let meta_index = ssts_with_ranges[index].meta_index;\n                                     let index_in_meta = ssts_with_ranges[index].index_in_meta;\n-                                    let meta = &meta_files[meta_index];\n-                                    meta.entry(index_in_meta)\n-                                        .sst(meta)?\n-                                        .iter(key_block_cache, value_block_cache)\n-                                })\n-                                .collect::<Result<Vec<_>>>()?;\n-\n-                            let iter = MergeIter::new(iters.into_iter())?;\n-\n-                            // TODO figure out how to delete blobs when they are no longer\n-                            // referenced\n-                            let blob_seq_numbers_to_delete: Vec<u32> = Vec::new();\n-\n-                            let mut keys_written = 0;\n-\n-                            let mut total_key_size = 0;\n-                            let mut total_value_size = 0;\n-                            let mut current: Option<LookupEntry<'_>> = None;\n-                            let mut entries = Vec::new();\n-                            let mut last_entries = Vec::new();\n-                            let mut last_entries_total_key_size = 0;\n-                            for entry in iter {\n-                                let entry = entry?;\n-\n-                                // Remove duplicates\n-                                if let Some(current) = current.take() {\n-                                    if current.key != entry.key {\n-                                        let key_size = current.key.len();\n-                                        let value_size = current.value.uncompressed_size_in_sst();\n-                                        total_key_size += key_size;\n-                                        total_value_size += value_size;\n-\n-                                        if total_key_size + total_value_size\n-                                            > DATA_THRESHOLD_PER_COMPACTED_FILE\n-                                            || entries.len() >= MAX_ENTRIES_PER_COMPACTED_FILE\n-                                        {\n-                                            let selected_total_key_size =\n-                                                last_entries_total_key_size;\n-                                            swap(&mut entries, &mut last_entries);\n-                                            last_entries_total_key_size = total_key_size - key_size;\n-                                            total_key_size = key_size;\n-                                            total_value_size = value_size;\n-\n-                                            if !entries.is_empty() {\n-                                                let seq = sequence_number\n-                                                    .fetch_add(1, Ordering::SeqCst)\n-                                                    + 1;\n-\n-                                                keys_written += entries.len() as u64;\n-                                                new_sst_files.push(create_sst_file(\n-                                                    &self.parallel_scheduler,\n-                                                    &entries,\n-                                                    selected_total_key_size,\n-                                                    path,\n-                                                    seq,\n-                                                )?);\n-\n-                                                entries.clear();\n+                                    let meta_file = &meta_files[meta_index];\n+                                    let entry = meta_file.entry(index_in_meta);\n+                                    let amqf = Cow::Borrowed(entry.raw_amqf(meta_file.amqf_data()));\n+                                    let meta = StaticSortedFileBuilderMeta {\n+                                        min_hash: entry.min_hash(),\n+                                        max_hash: entry.max_hash(),\n+                                        amqf,\n+                                        key_compression_dictionary_length: entry\n+                                            .key_compression_dictionary_length(),\n+                                        block_count: entry.block_count(),\n+                                        size: entry.size(),\n+                                        entries: 0,\n+                                    };\n+                                    return Ok(PartialMergeResult::Move {\n+                                        seq: entry.sequence_number(),\n+                                        meta,\n+                                    });\n+                                }\n+\n+                                fn create_sst_file<'l, S: ParallelScheduler>(\n+                                    parallel_scheduler: &S,\n+                                    entries: &[LookupEntry<'l>],\n+                                    total_key_size: usize,\n+                                    path: &Path,\n+                                    seq: u32,\n+                                ) -> Result<(u32, File, StaticSortedFileBuilderMeta<'static>)>\n+                                {\n+                                    let _span =\n+                                        tracing::trace_span!(\"write merged sst file\").entered();\n+                                    let (meta, file) = parallel_scheduler.block_in_place(|| {\n+                                        write_static_stored_file(\n+                                            entries,\n+                                            total_key_size,\n+                                            &path.join(format!(\"{seq:08}.sst\")),\n+                                        )\n+                                    })?;\n+                                    Ok((seq, file, meta))\n+                                }\n+\n+                                let mut new_sst_files = Vec::new();\n+\n+                                // Iterate all SST files\n+                                let iters = indicies\n+                                    .iter()\n+                                    .map(|&index| {\n+                                        let meta_index = ssts_with_ranges[index].meta_index;\n+                                        let index_in_meta = ssts_with_ranges[index].index_in_meta;\n+                                        let meta = &meta_files[meta_index];\n+                                        meta.entry(index_in_meta)\n+                                            .sst(meta)?\n+                                            .iter(key_block_cache, value_block_cache)\n+                                    })\n+                                    .collect::<Result<Vec<_>>>()?;\n+\n+                                let iter = MergeIter::new(iters.into_iter())?;\n+\n+                                // TODO figure out how to delete blobs when they are no longer\n+                                // referenced\n+                                let blob_seq_numbers_to_delete: Vec<u32> = Vec::new();\n+\n+                                let mut keys_written = 0;\n+\n+                                let mut total_key_size = 0;\n+                                let mut total_value_size = 0;\n+                                let mut current: Option<LookupEntry<'_>> = None;\n+                                let mut entries = Vec::new();\n+                                let mut last_entries = Vec::new();\n+                                let mut last_entries_total_key_size = 0;\n+                                for entry in iter {\n+                                    let entry = entry?;\n+\n+                                    // Remove duplicates\n+                                    if let Some(current) = current.take() {\n+                                        if current.key != entry.key {\n+                                            let key_size = current.key.len();\n+                                            let value_size =\n+                                                current.value.uncompressed_size_in_sst();\n+                                            total_key_size += key_size;\n+                                            total_value_size += value_size;\n+\n+                                            if total_key_size + total_value_size\n+                                                > DATA_THRESHOLD_PER_COMPACTED_FILE\n+                                                || entries.len() >= MAX_ENTRIES_PER_COMPACTED_FILE\n+                                            {\n+                                                let selected_total_key_size =\n+                                                    last_entries_total_key_size;\n+                                                swap(&mut entries, &mut last_entries);\n+                                                last_entries_total_key_size =\n+                                                    total_key_size - key_size;\n+                                                total_key_size = key_size;\n+                                                total_value_size = value_size;\n+\n+                                                if !entries.is_empty() {\n+                                                    let seq = sequence_number\n+                                                        .fetch_add(1, Ordering::SeqCst)\n+                                                        + 1;\n+\n+                                                    keys_written += entries.len() as u64;\n+                                                    new_sst_files.push(create_sst_file(\n+                                                        &self.parallel_scheduler,\n+                                                        &entries,\n+                                                        selected_total_key_size,\n+                                                        path,\n+                                                        seq,\n+                                                    )?);\n+\n+                                                    entries.clear();\n+                                                }\n                                             }\n-                                        }\n \n-                                        entries.push(current);\n-                                    } else {\n-                                        // Override value\n+                                            entries.push(current);\n+                                        } else {\n+                                            // Override value\n+                                        }\n                                     }\n+                                    current = Some(entry);\n+                                }\n+                                if let Some(entry) = current {\n+                                    total_key_size += entry.key.len();\n+                                    // Obsolete as we no longer need total_value_size\n+                                    // total_value_size += entry.value.uncompressed_size_in_sst();\n+                                    entries.push(entry);\n                                 }\n-                                current = Some(entry);\n-                            }\n-                            if let Some(entry) = current {\n-                                total_key_size += entry.key.len();\n-                                // Obsolete as we no longer need total_value_size\n-                                // total_value_size += entry.value.uncompressed_size_in_sst();\n-                                entries.push(entry);\n-                            }\n \n-                            // If we have one set of entries left, write them to a new SST file\n-                            if last_entries.is_empty() && !entries.is_empty() {\n-                                let seq = sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n-\n-                                keys_written += entries.len() as u64;\n-                                new_sst_files.push(create_sst_file(\n-                                    &self.parallel_scheduler,\n-                                    &entries,\n-                                    total_key_size,\n-                                    path,\n-                                    seq,\n-                                )?);\n-                            } else\n-                            // If we have two sets of entries left, merge them and\n-                            // split it into two SST files, to avoid having a\n-                            // single SST file that is very small.\n-                            if !last_entries.is_empty() {\n-                                last_entries.append(&mut entries);\n-\n-                                last_entries_total_key_size += total_key_size;\n-\n-                                let (part1, part2) = last_entries.split_at(last_entries.len() / 2);\n-\n-                                let seq1 = sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n-                                let seq2 = sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n-\n-                                keys_written += part1.len() as u64;\n-                                new_sst_files.push(create_sst_file(\n-                                    &self.parallel_scheduler,\n-                                    part1,\n-                                    // We don't know the exact sizes so we estimate them\n-                                    last_entries_total_key_size / 2,\n-                                    path,\n-                                    seq1,\n-                                )?);\n-\n-                                keys_written += part2.len() as u64;\n-                                new_sst_files.push(create_sst_file(\n-                                    &self.parallel_scheduler,\n-                                    part2,\n-                                    last_entries_total_key_size / 2,\n-                                    path,\n-                                    seq2,\n-                                )?);\n-                            }\n-                            Ok(PartialMergeResult::Merged {\n-                                new_sst_files,\n-                                blob_seq_numbers_to_delete,\n-                                keys_written,\n-                            })\n-                        })\n+                                // If we have one set of entries left, write them to a new SST file\n+                                if last_entries.is_empty() && !entries.is_empty() {\n+                                    let seq = sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n+\n+                                    keys_written += entries.len() as u64;\n+                                    new_sst_files.push(create_sst_file(\n+                                        &self.parallel_scheduler,\n+                                        &entries,\n+                                        total_key_size,\n+                                        path,\n+                                        seq,\n+                                    )?);\n+                                } else\n+                                // If we have two sets of entries left, merge them and\n+                                // split it into two SST files, to avoid having a\n+                                // single SST file that is very small.\n+                                if !last_entries.is_empty() {\n+                                    last_entries.append(&mut entries);\n+\n+                                    last_entries_total_key_size += total_key_size;\n+\n+                                    let (part1, part2) =\n+                                        last_entries.split_at(last_entries.len() / 2);\n+\n+                                    let seq1 = sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n+                                    let seq2 = sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n+\n+                                    keys_written += part1.len() as u64;\n+                                    new_sst_files.push(create_sst_file(\n+                                        &self.parallel_scheduler,\n+                                        part1,\n+                                        // We don't know the exact sizes so we estimate them\n+                                        last_entries_total_key_size / 2,\n+                                        path,\n+                                        seq1,\n+                                    )?);\n+\n+                                    keys_written += part2.len() as u64;\n+                                    new_sst_files.push(create_sst_file(\n+                                        &self.parallel_scheduler,\n+                                        part2,\n+                                        last_entries_total_key_size / 2,\n+                                        path,\n+                                        seq2,\n+                                    )?);\n+                                }\n+                                Ok(PartialMergeResult::Merged {\n+                                    new_sst_files,\n+                                    blob_seq_numbers_to_delete,\n+                                    keys_written,\n+                                    indicies,\n+                                })\n+                            },\n+                        )\n                         .with_context(|| {\n                             format!(\"Failed to merge database files for family {family}\")\n                         })?;\n@@ -1072,6 +1112,7 @@ impl<S: ParallelScheduler> TurboPersistence<S> {\n                             if let PartialMergeResult::Merged {\n                                 new_sst_files,\n                                 blob_seq_numbers_to_delete,\n+                                indicies: _,\n                                 keys_written: _,\n                             } = r\n                             {\n@@ -1088,43 +1129,91 @@ impl<S: ParallelScheduler> TurboPersistence<S> {\n                     let mut new_sst_files = Vec::with_capacity(sst_files_len);\n                     let mut blob_seq_numbers_to_delete = Vec::with_capacity(blob_delete_len);\n \n+                    let meta_seq = sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n                     let mut meta_file_builder = MetaFileBuilder::new(family);\n \n                     let mut keys_written = 0;\n-                    for result in merge_result {\n-                        match result {\n-                            PartialMergeResult::Merged {\n-                                new_sst_files: merged_new_sst_files,\n-                                blob_seq_numbers_to_delete: merged_blob_seq_numbers_to_delete,\n-                                keys_written: merged_keys_written,\n-                            } => {\n-                                for (seq, file, meta) in merged_new_sst_files {\n+                    self.parallel_scheduler.block_in_place(|| {\n+                        let guard = log_mutex.lock();\n+                        let mut log = self.open_log()?;\n+                        writeln!(\n+                            log,\n+                            \"{family:3} | {meta_seq:08} | Compaction (coverage: {}, overlap: {}, \\\n+                             duplication: {} / {} MiB):\",\n+                            metrics.coverage,\n+                            metrics.overlap,\n+                            metrics.duplication,\n+                            metrics.duplicated_size / 1024 / 1024\n+                        )?;\n+                        for result in merge_result {\n+                            match result {\n+                                PartialMergeResult::Merged {\n+                                    new_sst_files: merged_new_sst_files,\n+                                    blob_seq_numbers_to_delete: merged_blob_seq_numbers_to_delete,\n+                                    keys_written: merged_keys_written,\n+                                    indicies,\n+                                } => {\n+                                    writeln!(\n+                                        log,\n+                                        \"{family:3} | {meta_seq:08} | MERGE \\\n+                                         ({merged_keys_written} keys):\"\n+                                    )?;\n+                                    for i in indicies.iter() {\n+                                        let seq = ssts_with_ranges[*i].seq;\n+                                        let (min, max) = ssts_with_ranges[*i].range().into_inner();\n+                                        writeln!(\n+                                            log,\n+                                            \"{family:3} | {meta_seq:08} | {seq:08} INPUT  | {} \",\n+                                            range_to_str(min, max)\n+                                        )?;\n+                                    }\n+                                    for (seq, file, meta) in merged_new_sst_files {\n+                                        let min = meta.min_hash;\n+                                        let max = meta.max_hash;\n+                                        writeln!(\n+                                            log,\n+                                            \"{family:3} | {meta_seq:08} | {seq:08} OUTPUT | {}\",\n+                                            range_to_str(min, max)\n+                                        )?;\n+\n+                                        meta_file_builder.add(seq, meta);\n+                                        new_sst_files.push((seq, file));\n+                                    }\n+                                    blob_seq_numbers_to_delete\n+                                        .extend(merged_blob_seq_numbers_to_delete);\n+                                    keys_written += merged_keys_written;\n+                                }\n+                                PartialMergeResult::Move { seq, meta } => {\n+                                    let min = meta.min_hash;\n+                                    let max = meta.max_hash;\n+                                    writeln!(\n+                                        log,\n+                                        \"{family:3} | {meta_seq:08} | {seq:08} MOVED  | {}\",\n+                                        range_to_str(min, max)\n+                                    )?;\n+\n                                     meta_file_builder.add(seq, meta);\n-                                    new_sst_files.push((seq, file));\n                                 }\n-                                blob_seq_numbers_to_delete\n-                                    .extend(merged_blob_seq_numbers_to_delete);\n-                                keys_written += merged_keys_written;\n-                            }\n-                            PartialMergeResult::Move { seq, meta } => {\n-                                meta_file_builder.add(seq, meta);\n                             }\n                         }\n-                    }\n+                        drop(log);\n+                        drop(guard);\n+\n+                        anyhow::Ok(())\n+                    })?;\n \n                     for &seq in sst_seq_numbers_to_delete.iter() {\n                         meta_file_builder.add_obsolete_sst_file(seq);\n                     }\n \n-                    let seq = sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n                     let meta_file = {\n                         let _span = tracing::trace_span!(\"write meta file\").entered();\n                         self.parallel_scheduler\n-                            .block_in_place(|| meta_file_builder.write(&self.path, seq))?\n+                            .block_in_place(|| meta_file_builder.write(&self.path, meta_seq))?\n                     };\n \n                     Ok(PartialResultPerFamily {\n-                        new_meta_file: Some((seq, meta_file)),\n+                        new_meta_file: Some((meta_seq, meta_file)),\n                         new_sst_files,\n                         sst_seq_numbers_to_delete,\n                         blob_seq_numbers_to_delete,\n@@ -1273,6 +1362,30 @@ impl<S: ParallelScheduler> TurboPersistence<S> {\n     }\n }\n \n+fn range_to_str(min: u64, max: u64) -> String {\n+    use std::fmt::Write;\n+    const DISPLAY_SIZE: usize = 100;\n+    const TOTAL_SIZE: u64 = u64::MAX;\n+    let start_pos = (min as u128 * DISPLAY_SIZE as u128 / TOTAL_SIZE as u128) as usize;\n+    let end_pos = (max as u128 * DISPLAY_SIZE as u128 / TOTAL_SIZE as u128) as usize;\n+    let mut range_str = String::new();\n+    for i in 0..DISPLAY_SIZE {\n+        if i == start_pos && i == end_pos {\n+            range_str.push('O');\n+        } else if i == start_pos {\n+            range_str.push('[');\n+        } else if i == end_pos {\n+            range_str.push(']');\n+        } else if i > start_pos && i < end_pos {\n+            range_str.push('=');\n+        } else {\n+            range_str.push(' ');\n+        }\n+    }\n+    write!(range_str, \" | {min:016x}-{max:016x}\").unwrap();\n+    range_str\n+}\n+\n pub struct MetaFileInfo {\n     pub sequence_number: u32,\n     pub family: u32,"
        }
    ],
    "stats": {
        "total": 584,
        "additions": 349,
        "deletions": 235
    }
}