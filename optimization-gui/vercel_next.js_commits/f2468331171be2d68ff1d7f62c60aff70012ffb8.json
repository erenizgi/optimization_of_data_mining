{
    "author": "sokra",
    "message": "Turbopack: remove value compression dictionary (#82338)\n\n### What?\n\nremove value compression dictionary\n\nIt doesn't have benefit as each block is already large as we merge small values together and medium size values are already large.\n\nThis also gives the benefit that we do not need to recompress medium value blocks when doing compaction",
    "sha": "f2468331171be2d68ff1d7f62c60aff70012ffb8",
    "files": [
        {
            "sha": "a1b2bb15a1f09f2ff4250ff8cca5859f323add62",
            "filename": "turbopack/crates/turbo-persistence-tools/src/main.rs",
            "status": "modified",
            "additions": 3,
            "deletions": 8,
            "changes": 11,
            "blob_url": "https://github.com/vercel/next.js/blob/f2468331171be2d68ff1d7f62c60aff70012ffb8/turbopack%2Fcrates%2Fturbo-persistence-tools%2Fsrc%2Fmain.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/f2468331171be2d68ff1d7f62c60aff70012ffb8/turbopack%2Fcrates%2Fturbo-persistence-tools%2Fsrc%2Fmain.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence-tools%2Fsrc%2Fmain.rs?ref=f2468331171be2d68ff1d7f62c60aff70012ffb8",
            "patch": "@@ -35,7 +35,6 @@ fn main() -> Result<()> {\n             amqf_entries,\n             sst_size,\n             key_compression_dictionary_size,\n-            value_compression_dictionary_size,\n             block_count,\n         } in meta_file.entries\n         {\n@@ -45,15 +44,11 @@ fn main() -> Result<()> {\n             );\n             println!(\"    AMQF {amqf_entries} entries = {} KiB\", amqf_size / 1024);\n             println!(\n-                \"    {} KiB = {} kiB key compression dict + {} KiB value compression dict + \\\n-                 {block_count} blocks (avg {} bytes/block)\",\n+                \"    {} KiB = {} kiB key compression dict + {block_count} blocks (avg {} \\\n+                 bytes/block)\",\n                 sst_size / 1024,\n                 key_compression_dictionary_size / 1024,\n-                value_compression_dictionary_size / 1024,\n-                (sst_size\n-                    - key_compression_dictionary_size as u64\n-                    - value_compression_dictionary_size as u64)\n-                    / block_count as u64\n+                (sst_size - key_compression_dictionary_size as u64) / block_count as u64\n             );\n         }\n         if !meta_file.obsolete_sst_files.is_empty() {"
        },
        {
            "sha": "51baa4c5a457c6b1840c6b2d7571e21e98cfb6ed",
            "filename": "turbopack/crates/turbo-persistence/README.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/vercel/next.js/blob/f2468331171be2d68ff1d7f62c60aff70012ffb8/turbopack%2Fcrates%2Fturbo-persistence%2FREADME.md",
            "raw_url": "https://github.com/vercel/next.js/raw/f2468331171be2d68ff1d7f62c60aff70012ffb8/turbopack%2Fcrates%2Fturbo-persistence%2FREADME.md",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2FREADME.md?ref=f2468331171be2d68ff1d7f62c60aff70012ffb8",
            "patch": "@@ -45,7 +45,6 @@ A meta file can contain metadata about multiple SST files. The metadata is store\n   - foreach described SST file\n     - 4 bytes sequence number of the SST file\n     - 2 bytes key Compression Dictionary length\n-    - 2 bytes value Compression Dictionary length\n     - 2 bytes block count\n     - 8 bytes min hash\n     - 8 bytes max hash\n@@ -59,7 +58,6 @@ A meta file can contain metadata about multiple SST files. The metadata is store\n The SST file contains only data without any header.\n \n - serialized key Compression Dictionary\n-- serialized value Compression Dictionary\n - foreach block\n   - 4 bytes uncompressed block length\n   - compressed data"
        },
        {
            "sha": "b955d6102bec14b661e909ced8cf54946a51e83d",
            "filename": "turbopack/crates/turbo-persistence/src/collector.rs",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/vercel/next.js/blob/f2468331171be2d68ff1d7f62c60aff70012ffb8/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fcollector.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/f2468331171be2d68ff1d7f62c60aff70012ffb8/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fcollector.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fcollector.rs?ref=f2468331171be2d68ff1d7f62c60aff70012ffb8",
            "patch": "@@ -92,11 +92,11 @@ impl<K: StoreKey, const SIZE_SHIFT: usize> Collector<K, SIZE_SHIFT> {\n         self.entries.push(entry);\n     }\n \n-    /// Sorts the entries and returns them along with the total key and value sizes. This doesn't\n+    /// Sorts the entries and returns them along with the total key size. This doesn't\n     /// clear the entries.\n-    pub fn sorted(&mut self) -> (&[CollectorEntry<K>], usize, usize) {\n+    pub fn sorted(&mut self) -> (&[CollectorEntry<K>], usize) {\n         self.entries.sort_unstable_by(|a, b| a.key.cmp(&b.key));\n-        (&self.entries, self.total_key_size, self.total_value_size)\n+        (&self.entries, self.total_key_size)\n     }\n \n     /// Clears the collector."
        },
        {
            "sha": "644019a4b64f96912ad78ac38aec76d003f52378",
            "filename": "turbopack/crates/turbo-persistence/src/db.rs",
            "status": "modified",
            "additions": 9,
            "deletions": 25,
            "changes": 34,
            "blob_url": "https://github.com/vercel/next.js/blob/f2468331171be2d68ff1d7f62c60aff70012ffb8/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fdb.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/f2468331171be2d68ff1d7f62c60aff70012ffb8/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fdb.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fdb.rs?ref=f2468331171be2d68ff1d7f62c60aff70012ffb8",
            "patch": "@@ -898,8 +898,6 @@ impl<S: ParallelScheduler> TurboPersistence<S> {\n                                     amqf,\n                                     key_compression_dictionary_length: entry\n                                         .key_compression_dictionary_length(),\n-                                    value_compression_dictionary_length: entry\n-                                        .value_compression_dictionary_length(),\n                                     block_count: entry.block_count(),\n                                     size: entry.size(),\n                                     entries: 0,\n@@ -914,7 +912,6 @@ impl<S: ParallelScheduler> TurboPersistence<S> {\n                                 parallel_scheduler: &S,\n                                 entries: &[LookupEntry<'l>],\n                                 total_key_size: usize,\n-                                total_value_size: usize,\n                                 path: &Path,\n                                 seq: u32,\n                             ) -> Result<(u32, File, StaticSortedFileBuilderMeta<'static>)>\n@@ -924,7 +921,6 @@ impl<S: ParallelScheduler> TurboPersistence<S> {\n                                     write_static_stored_file(\n                                         entries,\n                                         total_key_size,\n-                                        total_value_size,\n                                         &path.join(format!(\"{seq:08}.sst\")),\n                                     )\n                                 })?;\n@@ -959,7 +955,7 @@ impl<S: ParallelScheduler> TurboPersistence<S> {\n                             let mut current: Option<LookupEntry<'_>> = None;\n                             let mut entries = Vec::new();\n                             let mut last_entries = Vec::new();\n-                            let mut last_entries_total_sizes = (0, 0);\n+                            let mut last_entries_total_key_size = 0;\n                             for entry in iter {\n                                 let entry = entry?;\n \n@@ -975,15 +971,10 @@ impl<S: ParallelScheduler> TurboPersistence<S> {\n                                             > DATA_THRESHOLD_PER_COMPACTED_FILE\n                                             || entries.len() >= MAX_ENTRIES_PER_COMPACTED_FILE\n                                         {\n-                                            let (\n-                                                selected_total_key_size,\n-                                                selected_total_value_size,\n-                                            ) = last_entries_total_sizes;\n+                                            let selected_total_key_size =\n+                                                last_entries_total_key_size;\n                                             swap(&mut entries, &mut last_entries);\n-                                            last_entries_total_sizes = (\n-                                                total_key_size - key_size,\n-                                                total_value_size - value_size,\n-                                            );\n+                                            last_entries_total_key_size = total_key_size - key_size;\n                                             total_key_size = key_size;\n                                             total_value_size = value_size;\n \n@@ -997,7 +988,6 @@ impl<S: ParallelScheduler> TurboPersistence<S> {\n                                                     &self.parallel_scheduler,\n                                                     &entries,\n                                                     selected_total_key_size,\n-                                                    selected_total_value_size,\n                                                     path,\n                                                     seq,\n                                                 )?);\n@@ -1015,7 +1005,8 @@ impl<S: ParallelScheduler> TurboPersistence<S> {\n                             }\n                             if let Some(entry) = current {\n                                 total_key_size += entry.key.len();\n-                                total_value_size += entry.value.uncompressed_size_in_sst();\n+                                // Obsolete as we no longer need total_value_size\n+                                // total_value_size += entry.value.uncompressed_size_in_sst();\n                                 entries.push(entry);\n                             }\n \n@@ -1028,7 +1019,6 @@ impl<S: ParallelScheduler> TurboPersistence<S> {\n                                     &self.parallel_scheduler,\n                                     &entries,\n                                     total_key_size,\n-                                    total_value_size,\n                                     path,\n                                     seq,\n                                 )?);\n@@ -1039,8 +1029,7 @@ impl<S: ParallelScheduler> TurboPersistence<S> {\n                             if !last_entries.is_empty() {\n                                 last_entries.append(&mut entries);\n \n-                                last_entries_total_sizes.0 += total_key_size;\n-                                last_entries_total_sizes.1 += total_value_size;\n+                                last_entries_total_key_size += total_key_size;\n \n                                 let (part1, part2) = last_entries.split_at(last_entries.len() / 2);\n \n@@ -1052,8 +1041,7 @@ impl<S: ParallelScheduler> TurboPersistence<S> {\n                                     &self.parallel_scheduler,\n                                     part1,\n                                     // We don't know the exact sizes so we estimate them\n-                                    last_entries_total_sizes.0 / 2,\n-                                    last_entries_total_sizes.1 / 2,\n+                                    last_entries_total_key_size / 2,\n                                     path,\n                                     seq1,\n                                 )?);\n@@ -1062,8 +1050,7 @@ impl<S: ParallelScheduler> TurboPersistence<S> {\n                                 new_sst_files.push(create_sst_file(\n                                     &self.parallel_scheduler,\n                                     part2,\n-                                    last_entries_total_sizes.0 / 2,\n-                                    last_entries_total_sizes.1 / 2,\n+                                    last_entries_total_key_size / 2,\n                                     path,\n                                     seq2,\n                                 )?);\n@@ -1263,8 +1250,6 @@ impl<S: ParallelScheduler> TurboPersistence<S> {\n                             amqf_entries: amqf.len(),\n                             key_compression_dictionary_size: entry\n                                 .key_compression_dictionary_length(),\n-                            value_compression_dictionary_size: entry\n-                                .value_compression_dictionary_length(),\n                             block_count: entry.block_count(),\n                         }\n                     })\n@@ -1302,6 +1287,5 @@ pub struct MetaFileEntryInfo {\n     pub amqf_entries: usize,\n     pub sst_size: u64,\n     pub key_compression_dictionary_size: u16,\n-    pub value_compression_dictionary_size: u16,\n     pub block_count: u16,\n }"
        },
        {
            "sha": "c55adca31eaeae7c2338ce73df69d67d81321ee1",
            "filename": "turbopack/crates/turbo-persistence/src/lookup_entry.rs",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/vercel/next.js/blob/f2468331171be2d68ff1d7f62c60aff70012ffb8/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Flookup_entry.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/f2468331171be2d68ff1d7f62c60aff70012ffb8/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Flookup_entry.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Flookup_entry.rs?ref=f2468331171be2d68ff1d7f62c60aff70012ffb8",
            "patch": "@@ -22,7 +22,6 @@ pub enum LazyLookupValue<'l> {\n     Medium {\n         uncompressed_size: u32,\n         block: &'l [u8],\n-        dictionary: &'l [u8],\n     },\n }\n \n@@ -79,11 +78,9 @@ impl Entry for LookupEntry<'_> {\n             LazyLookupValue::Medium {\n                 uncompressed_size,\n                 block,\n-                dictionary,\n             } => EntryValue::MediumCompressed {\n                 uncompressed_size: *uncompressed_size,\n                 block,\n-                dictionary,\n             },\n         }\n     }"
        },
        {
            "sha": "3c0f1b3aa755e9542486b0bb0e212279a4663815",
            "filename": "turbopack/crates/turbo-persistence/src/meta_file.rs",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/vercel/next.js/blob/f2468331171be2d68ff1d7f62c60aff70012ffb8/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fmeta_file.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/f2468331171be2d68ff1d7f62c60aff70012ffb8/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fmeta_file.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fmeta_file.rs?ref=f2468331171be2d68ff1d7f62c60aff70012ffb8",
            "patch": "@@ -144,10 +144,6 @@ impl MetaEntry {\n         self.sst_data.key_compression_dictionary_length\n     }\n \n-    pub fn value_compression_dictionary_length(&self) -> u16 {\n-        self.sst_data.value_compression_dictionary_length\n-    }\n-\n     pub fn block_count(&self) -> u16 {\n         self.sst_data.block_count\n     }\n@@ -222,7 +218,6 @@ impl MetaFile {\n                 sst_data: StaticSortedFileMetaData {\n                     sequence_number: file.read_u32::<BE>()?,\n                     key_compression_dictionary_length: file.read_u16::<BE>()?,\n-                    value_compression_dictionary_length: file.read_u16::<BE>()?,\n                     block_count: file.read_u16::<BE>()?,\n                 },\n                 family,"
        },
        {
            "sha": "67831753683717bda2855c13686b77ccd16cc40a",
            "filename": "turbopack/crates/turbo-persistence/src/meta_file_builder.rs",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/vercel/next.js/blob/f2468331171be2d68ff1d7f62c60aff70012ffb8/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fmeta_file_builder.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/f2468331171be2d68ff1d7f62c60aff70012ffb8/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fmeta_file_builder.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fmeta_file_builder.rs?ref=f2468331171be2d68ff1d7f62c60aff70012ffb8",
            "patch": "@@ -58,7 +58,6 @@ impl<'a> MetaFileBuilder<'a> {\n         for (sequence_number, sst) in &self.entries {\n             file.write_u32::<BE>(*sequence_number)?;\n             file.write_u16::<BE>(sst.key_compression_dictionary_length)?;\n-            file.write_u16::<BE>(sst.value_compression_dictionary_length)?;\n             file.write_u16::<BE>(sst.block_count)?;\n             file.write_u64::<BE>(sst.min_hash)?;\n             file.write_u64::<BE>(sst.max_hash)?;"
        },
        {
            "sha": "eac0b9b33d97aa26b44c98aa6f2635beb77eb467",
            "filename": "turbopack/crates/turbo-persistence/src/static_sorted_file.rs",
            "status": "modified",
            "additions": 11,
            "deletions": 21,
            "changes": 32,
            "blob_url": "https://github.com/vercel/next.js/blob/f2468331171be2d68ff1d7f62c60aff70012ffb8/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fstatic_sorted_file.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/f2468331171be2d68ff1d7f62c60aff70012ffb8/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fstatic_sorted_file.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fstatic_sorted_file.rs?ref=f2468331171be2d68ff1d7f62c60aff70012ffb8",
            "patch": "@@ -65,8 +65,6 @@ pub struct StaticSortedFileMetaData {\n     pub sequence_number: u32,\n     /// The length of the key compression dictionary.\n     pub key_compression_dictionary_length: u16,\n-    /// The length of the value compression dictionary.\n-    pub value_compression_dictionary_length: u16,\n     /// The number of blocks in the SST file.\n     pub block_count: u16,\n }\n@@ -79,21 +77,14 @@ impl StaticSortedFileMetaData {\n \n     pub fn blocks_start(&self) -> usize {\n         let k: usize = self.key_compression_dictionary_length.into();\n-        let v: usize = self.value_compression_dictionary_length.into();\n-        k + v\n+        k\n     }\n \n     pub fn key_compression_dictionary_range(&self) -> Range<usize> {\n         let start = 0;\n         let end: usize = self.key_compression_dictionary_length.into();\n         start..end\n     }\n-\n-    pub fn value_compression_dictionary_range(&self) -> Range<usize> {\n-        let start = self.key_compression_dictionary_length as usize;\n-        let end = start + self.value_compression_dictionary_length as usize;\n-        start..end\n-    }\n }\n \n /// A memory mapped SST file.\n@@ -310,7 +301,7 @@ impl StaticSortedFile {\n             match value_block_cache.get_value_or_guard(&(self.meta.sequence_number, block), None) {\n                 GuardResult::Value(block) => block,\n                 GuardResult::Guard(guard) => {\n-                    let block = self.read_value_block(block)?;\n+                    let block = self.read_small_value_block(block)?;\n                     let _ = guard.insert(block.clone());\n                     block\n                 }\n@@ -323,33 +314,34 @@ impl StaticSortedFile {\n     fn read_key_block(&self, block_index: u16) -> Result<ArcSlice<u8>> {\n         self.read_block(\n             block_index,\n-            &self.mmap[self.meta.key_compression_dictionary_range()],\n+            Some(&self.mmap[self.meta.key_compression_dictionary_range()]),\n             false,\n         )\n     }\n \n+    /// Reads a value block from the file.\n+    fn read_small_value_block(&self, block_index: u16) -> Result<ArcSlice<u8>> {\n+        self.read_block(block_index, None, false)\n+    }\n+\n     /// Reads a value block from the file.\n     fn read_value_block(&self, block_index: u16) -> Result<ArcSlice<u8>> {\n-        self.read_block(\n-            block_index,\n-            &self.mmap[self.meta.value_compression_dictionary_range()],\n-            false,\n-        )\n+        self.read_block(block_index, None, true)\n     }\n \n     /// Reads a block from the file.\n     fn read_block(\n         &self,\n         block_index: u16,\n-        compression_dictionary: &[u8],\n+        compression_dictionary: Option<&[u8]>,\n         long_term: bool,\n     ) -> Result<ArcSlice<u8>> {\n         let (uncompressed_length, block) = self.get_compressed_block(block_index)?;\n \n         let buffer = decompress_into_arc(\n             uncompressed_length,\n             block,\n-            Some(compression_dictionary),\n+            compression_dictionary,\n             long_term,\n         )?;\n         Ok(ArcSlice::from(buffer))\n@@ -496,8 +488,6 @@ impl<'l> StaticSortedFileIter<'l> {\n                     LazyLookupValue::Medium {\n                         uncompressed_size,\n                         block,\n-                        dictionary: &self.this.mmap\n-                            [self.this.meta.value_compression_dictionary_range()],\n                     }\n                 } else {\n                     let value = self"
        },
        {
            "sha": "66d0d043ce25f527b8269864c398ef2bda03d4a1",
            "filename": "turbopack/crates/turbo-persistence/src/static_sorted_file_builder.rs",
            "status": "modified",
            "additions": 44,
            "deletions": 89,
            "changes": 133,
            "blob_url": "https://github.com/vercel/next.js/blob/f2468331171be2d68ff1d7f62c60aff70012ffb8/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fstatic_sorted_file_builder.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/f2468331171be2d68ff1d7f62c60aff70012ffb8/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fstatic_sorted_file_builder.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fstatic_sorted_file_builder.rs?ref=f2468331171be2d68ff1d7f62c60aff70012ffb8",
            "patch": "@@ -10,7 +10,7 @@ use anyhow::{Context, Result};\n use byteorder::{BE, ByteOrder, WriteBytesExt};\n \n use crate::{\n-    compression::{compress_into_buffer, decompress_into_arc},\n+    compression::compress_into_buffer,\n     static_sorted_file::{\n         BLOCK_TYPE_INDEX, BLOCK_TYPE_KEY, KEY_BLOCK_ENTRY_TYPE_BLOB, KEY_BLOCK_ENTRY_TYPE_DELETED,\n         KEY_BLOCK_ENTRY_TYPE_MEDIUM, KEY_BLOCK_ENTRY_TYPE_SMALL,\n@@ -31,23 +31,16 @@ const MAX_SMALL_VALUE_BLOCK_SIZE: usize = 64 * 1024;\n /// The aimed false positive rate for the AMQF\n const AMQF_FALSE_POSITIVE_RATE: f64 = 0.01;\n \n-/// The maximum compression dictionary size for value blocks\n-const VALUE_COMPRESSION_DICTIONARY_SIZE: usize = 64 * 1024 - 1;\n /// The maximum compression dictionary size for key and index blocks\n const KEY_COMPRESSION_DICTIONARY_SIZE: usize = 64 * 1024 - 1;\n-/// The maximum bytes that should be selected as value samples to create a compression dictionary\n-const VALUE_COMPRESSION_SAMPLES_SIZE: usize = 256 * 1024;\n /// The maximum bytes that should be selected as key samples to create a compression dictionary\n const KEY_COMPRESSION_SAMPLES_SIZE: usize = 256 * 1024;\n-/// The minimum bytes that should be selected as value samples. Below that no compression dictionary\n-/// is used.\n-const MIN_VALUE_COMPRESSION_SAMPLES_SIZE: usize = 1024;\n-/// The minimum bytes that should be selected as key samples. Below that no compression dictionary\n+/// The minimum bytes that should be selected as keys samples. Below that no compression dictionary\n /// is used.\n const MIN_KEY_COMPRESSION_SAMPLES_SIZE: usize = 1024;\n-/// The bytes that are used per key/value entry for a sample.\n+/// The bytes that are used per key entry for a sample.\n const COMPRESSION_DICTIONARY_SAMPLE_PER_ENTRY: usize = 100;\n-/// The minimum bytes that are used per key/value entry for a sample.\n+/// The minimum bytes that are used per key entry for a sample.\n const MIN_COMPRESSION_DICTIONARY_SAMPLE_PER_ENTRY: usize = 16;\n \n /// Trait for entries from that SST files can be created\n@@ -74,7 +67,6 @@ pub enum EntryValue<'l> {\n     MediumCompressed {\n         uncompressed_size: u32,\n         block: &'l [u8],\n-        dictionary: &'l [u8],\n     },\n     /// Large-sized value. They are stored in a blob file.\n     Large { blob: u32 },\n@@ -92,8 +84,6 @@ pub struct StaticSortedFileBuilderMeta<'a> {\n     pub amqf: Cow<'a, [u8]>,\n     /// The key compression dictionary\n     pub key_compression_dictionary_length: u16,\n-    /// The value compression dictionary\n-    pub value_compression_dictionary_length: u16,\n     /// The number of blocks in the SST file\n     pub block_count: u16,\n     /// The file size of the SST file\n@@ -105,21 +95,18 @@ pub struct StaticSortedFileBuilderMeta<'a> {\n pub fn write_static_stored_file<E: Entry>(\n     entries: &[E],\n     total_key_size: usize,\n-    total_value_size: usize,\n     file: &Path,\n ) -> Result<(StaticSortedFileBuilderMeta<'static>, File)> {\n     debug_assert!(entries.iter().map(|e| e.key_hash()).is_sorted());\n \n     let mut file = BufWriter::new(File::create(file)?);\n \n-    let capacity = get_compression_buffer_capacity(total_key_size, total_value_size);\n+    let capacity = get_compression_buffer_capacity(total_key_size);\n     // We use a shared buffer for all operations to avoid excessive allocations\n     let mut buffer = Vec::with_capacity(capacity);\n \n     let key_dict = compute_key_compression_dictionary(entries, total_key_size, &mut buffer)?;\n-    let value_dict = compute_value_compression_dictionary(entries, total_value_size, &mut buffer)?;\n     file.write_all(&key_dict)?;\n-    file.write_all(&value_dict)?;\n \n     let mut block_writer = BlockWriter::new(&mut file, &mut buffer);\n \n@@ -129,7 +116,7 @@ pub fn write_static_stored_file<E: Entry>(\n     let mut buffer = Vec::new();\n \n     let min_hash = entries.first().map_or(u64::MAX, |e| e.key_hash());\n-    let value_locations = write_value_blocks(entries, &value_dict, &mut block_writer, &mut buffer)\n+    let value_locations = write_value_blocks(entries, &mut block_writer, &mut buffer)\n         .context(\"Failed to write value blocks\")?;\n     let amqf = write_key_blocks_and_compute_amqf(\n         entries,\n@@ -152,25 +139,19 @@ pub fn write_static_stored_file<E: Entry>(\n         max_hash,\n         amqf: Cow::Owned(amqf),\n         key_compression_dictionary_length: key_dict.len().try_into().unwrap(),\n-        value_compression_dictionary_length: value_dict.len().try_into().unwrap(),\n         block_count,\n         size: file.stream_position()?,\n         entries: entries.len() as u64,\n     };\n     Ok((meta, file.into_inner()?))\n }\n \n-fn get_compression_buffer_capacity(total_key_size: usize, total_value_size: usize) -> usize {\n+fn get_compression_buffer_capacity(total_key_size: usize) -> usize {\n     let mut size = 0;\n     if total_key_size >= MIN_KEY_COMPRESSION_SAMPLES_SIZE {\n         let key_compression_samples_size = min(KEY_COMPRESSION_SAMPLES_SIZE, total_key_size / 16);\n         size = key_compression_samples_size;\n     }\n-    if total_value_size >= MIN_VALUE_COMPRESSION_SAMPLES_SIZE {\n-        let value_compression_samples_size =\n-            min(VALUE_COMPRESSION_SAMPLES_SIZE, total_value_size / 16);\n-        size = size.max(value_compression_samples_size);\n-    }\n     size\n }\n \n@@ -223,53 +204,6 @@ fn compute_key_compression_dictionary<E: Entry>(\n     Ok(result)\n }\n \n-/// Computes compression dictionaries from values of all entries\n-#[tracing::instrument(level = \"trace\", skip(entries))]\n-fn compute_value_compression_dictionary<E: Entry>(\n-    entries: &[E],\n-    total_value_size: usize,\n-    buffer: &mut Vec<u8>,\n-) -> Result<Vec<u8>> {\n-    if total_value_size < MIN_VALUE_COMPRESSION_SAMPLES_SIZE {\n-        return Ok(Vec::new());\n-    }\n-    let value_compression_samples_size = min(VALUE_COMPRESSION_SAMPLES_SIZE, total_value_size / 16);\n-    let mut sample_sizes = Vec::new();\n-\n-    // Limit the number of iterations to avoid infinite loops\n-    let max_iterations = total_value_size / COMPRESSION_DICTIONARY_SAMPLE_PER_ENTRY * 2;\n-    for i in 0..max_iterations {\n-        let entry = &entries[i % entries.len()];\n-        let remaining = value_compression_samples_size - buffer.len();\n-        if remaining < MIN_COMPRESSION_DICTIONARY_SAMPLE_PER_ENTRY {\n-            break;\n-        }\n-        if let EntryValue::Small { value } | EntryValue::Medium { value } = entry.value() {\n-            let len = value.len();\n-            if len >= MIN_COMPRESSION_DICTIONARY_SAMPLE_PER_ENTRY {\n-                let used_len = min(remaining, COMPRESSION_DICTIONARY_SAMPLE_PER_ENTRY);\n-                if len <= used_len {\n-                    sample_sizes.push(len);\n-                    buffer.extend_from_slice(value);\n-                } else {\n-                    sample_sizes.push(used_len);\n-                    let p = buffer.len() % (len - used_len);\n-                    buffer.extend_from_slice(&value[p..p + used_len]);\n-                };\n-            }\n-        }\n-    }\n-    debug_assert!(buffer.len() == sample_sizes.iter().sum::<usize>());\n-    let result = if buffer.len() > MIN_VALUE_COMPRESSION_SAMPLES_SIZE && sample_sizes.len() > 5 {\n-        zstd::dict::from_continuous(buffer, &sample_sizes, VALUE_COMPRESSION_DICTIONARY_SIZE)\n-            .context(\"Value dictionary creation failed\")?\n-    } else {\n-        Vec::new()\n-    };\n-    buffer.clear();\n-    Ok(result)\n-}\n-\n struct BlockWriter<'l> {\n     buffer: &'l mut Vec<u8>,\n     block_offsets: Vec<u32>,\n@@ -301,23 +235,29 @@ impl<'l> BlockWriter<'l> {\n \n     #[tracing::instrument(level = \"trace\", skip_all)]\n     fn write_key_block(&mut self, block: &[u8], dict: &[u8]) -> Result<()> {\n-        self.write_block(block, dict, false)\n+        self.write_block(block, Some(dict), false)\n             .context(\"Failed to write key block\")\n     }\n \n     #[tracing::instrument(level = \"trace\", skip_all)]\n     fn write_index_block(&mut self, block: &[u8], dict: &[u8]) -> Result<()> {\n-        self.write_block(block, dict, false)\n+        self.write_block(block, Some(dict), false)\n             .context(\"Failed to write index block\")\n     }\n \n     #[tracing::instrument(level = \"trace\", skip_all)]\n-    fn write_value_block(&mut self, block: &[u8], dict: &[u8]) -> Result<()> {\n-        self.write_block(block, dict, false)\n+    fn write_small_value_block(&mut self, block: &[u8]) -> Result<()> {\n+        self.write_block(block, None, false)\n+            .context(\"Failed to write small value block\")\n+    }\n+\n+    #[tracing::instrument(level = \"trace\", skip_all)]\n+    fn write_value_block(&mut self, block: &[u8]) -> Result<()> {\n+        self.write_block(block, None, true)\n             .context(\"Failed to write value block\")\n     }\n \n-    fn write_block(&mut self, block: &[u8], dict: &[u8], long_term: bool) -> Result<()> {\n+    fn write_block(&mut self, block: &[u8], dict: Option<&[u8]>, long_term: bool) -> Result<()> {\n         let uncompressed_size = block.len().try_into().unwrap();\n         self.compress_block_into_buffer(block, dict, long_term)?;\n         let len = (self.buffer.len() + 4).try_into().unwrap();\n@@ -340,22 +280,41 @@ impl<'l> BlockWriter<'l> {\n         Ok(())\n     }\n \n+    fn write_compressed_block(&mut self, uncompressed_size: u32, block: &[u8]) -> Result<()> {\n+        let len = (block.len() + 4).try_into().unwrap();\n+        let offset = self\n+            .block_offsets\n+            .last()\n+            .copied()\n+            .unwrap_or_default()\n+            .checked_add(len)\n+            .expect(\"Block offset overflow\");\n+        self.block_offsets.push(offset);\n+\n+        self.writer\n+            .write_u32::<BE>(uncompressed_size)\n+            .context(\"Failed to write uncompressed size\")?;\n+        self.writer\n+            .write_all(block)\n+            .context(\"Failed to write compressed block\")?;\n+        Ok(())\n+    }\n+\n     /// Compresses a block with a compression dictionary.\n     fn compress_block_into_buffer(\n         &mut self,\n         block: &[u8],\n-        dict: &[u8],\n+        dict: Option<&[u8]>,\n         long_term: bool,\n     ) -> Result<()> {\n-        compress_into_buffer(block, Some(dict), long_term, self.buffer)\n+        compress_into_buffer(block, dict, long_term, self.buffer)\n     }\n }\n \n /// Splits the values of the entries into blocks and writes them to the writer.\n #[tracing::instrument(level = \"trace\", skip_all)]\n fn write_value_blocks(\n     entries: &[impl Entry],\n-    value_compression_dictionary: &[u8],\n     writer: &mut BlockWriter<'_>,\n     buffer: &mut Vec<u8>,\n ) -> Result<Vec<(u16, u32)>> {\n@@ -378,7 +337,7 @@ fn write_value_blocks(\n                             value_locations[j].0 = block_index;\n                         }\n                     }\n-                    writer.write_value_block(buffer, value_compression_dictionary)?;\n+                    writer.write_small_value_block(buffer)?;\n                     buffer.clear();\n                     current_block_start = i;\n                     current_block_size = 0;\n@@ -391,19 +350,15 @@ fn write_value_blocks(\n             EntryValue::Medium { value } => {\n                 let block_index = writer.next_block_index();\n                 value_locations.push((block_index, 0));\n-                writer.write_value_block(value, value_compression_dictionary)?;\n+                writer.write_value_block(value)?;\n             }\n             EntryValue::MediumCompressed {\n                 uncompressed_size,\n                 block,\n-                dictionary,\n             } => {\n                 let block_index = writer.next_block_index();\n                 value_locations.push((block_index, 0));\n-                // Recompress block with a different dictionary\n-                let decompressed =\n-                    decompress_into_arc(uncompressed_size, block, Some(dictionary), false)?;\n-                writer.write_value_block(&decompressed, value_compression_dictionary)?;\n+                writer.write_compressed_block(uncompressed_size, block)?;\n             }\n             EntryValue::Deleted | EntryValue::Large { .. } => {\n                 value_locations.push((0, 0));\n@@ -419,7 +374,7 @@ fn write_value_blocks(\n                 value_locations[j].0 = block_index;\n             }\n         }\n-        writer.write_value_block(buffer, value_compression_dictionary)?;\n+        writer.write_small_value_block(buffer)?;\n         buffer.clear();\n     }\n "
        },
        {
            "sha": "4426f8795af4411752aedf35605c81f9c79ce6e9",
            "filename": "turbopack/crates/turbo-persistence/src/write_batch.rs",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/vercel/next.js/blob/f2468331171be2d68ff1d7f62c60aff70012ffb8/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fwrite_batch.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/f2468331171be2d68ff1d7f62c60aff70012ffb8/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fwrite_batch.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fwrite_batch.rs?ref=f2468331171be2d68ff1d7f62c60aff70012ffb8",
            "patch": "@@ -407,17 +407,15 @@ impl<K: StoreKey + Send + Sync, S: ParallelScheduler, const FAMILIES: usize>\n     fn create_sst_file(\n         &self,\n         family: u32,\n-        collector_data: (&[CollectorEntry<K>], usize, usize),\n+        collector_data: (&[CollectorEntry<K>], usize),\n     ) -> Result<(u32, File)> {\n-        let (entries, total_key_size, total_value_size) = collector_data;\n+        let (entries, total_key_size) = collector_data;\n         let seq = self.current_sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n \n         let path = self.db_path.join(format!(\"{seq:08}.sst\"));\n         let (meta, file) = self\n             .parallel_scheduler\n-            .block_in_place(|| {\n-                write_static_stored_file(entries, total_key_size, total_value_size, &path)\n-            })\n+            .block_in_place(|| write_static_stored_file(entries, total_key_size, &path))\n             .with_context(|| format!(\"Unable to write SST file {seq:08}.sst\"))?;\n \n         #[cfg(feature = \"verify_sst_content\")]\n@@ -440,7 +438,6 @@ impl<K: StoreKey + Send + Sync, S: ParallelScheduler, const FAMILIES: usize>\n                 StaticSortedFileMetaData {\n                     sequence_number: seq,\n                     key_compression_dictionary_length: meta.key_compression_dictionary_length,\n-                    value_compression_dictionary_length: meta.value_compression_dictionary_length,\n                     block_count: meta.block_count,\n                 },\n             )?;"
        }
    ],
    "stats": {
        "total": 236,
        "additions": 73,
        "deletions": 163
    }
}