{
    "author": "sokra",
    "message": "Turbopack: Scope with work queue (#84225)\n\n### What?\n\n* Use a work queue and worker tasks in the scope implementation to reduce the number of spawned tokio tasks. Only spawn a tokio task per CPU - 1.\n* Run work in the current tokio task too to avoid blocking time and use one less task.\n\nBefore we only did a blocking wait on the current tokio task, which did require calling block_in_place pretty often.\nNow we also do work on the current tokio task too, which requires calling block_in_place less often. (We only call block_in_place when we need to block longer than 1ms)",
    "sha": "b1f05782a01efde5a7a6f74738784bf67978661f",
    "files": [
        {
            "sha": "339265897e0659c1a04ac3ce32221f6bf3b4b8a4",
            "filename": "turbopack/crates/turbo-tasks/src/parallel.rs",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/vercel/next.js/blob/b1f05782a01efde5a7a6f74738784bf67978661f/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Fparallel.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/b1f05782a01efde5a7a6f74738784bf67978661f/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Fparallel.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Fparallel.rs?ref=b1f05782a01efde5a7a6f74738784bf67978661f",
            "patch": "@@ -25,7 +25,7 @@ where\n     let f = &f;\n     let _results = scope_and_block(len.div_ceil(chunk_size), |scope| {\n         for chunk in items.chunks(chunk_size) {\n-            scope.spawn(async move {\n+            scope.spawn(move || {\n                 for item in chunk {\n                     f(item);\n                 }\n@@ -49,7 +49,7 @@ where\n     let f = &f;\n     let _results = scope_and_block(len.div_ceil(chunk_size), |scope| {\n         for chunk in into_chunks(items, chunk_size) {\n-            scope.spawn(async move {\n+            scope.spawn(move || {\n                 // SAFETY: Even when f() panics we drop all items in the chunk.\n                 for item in chunk {\n                     f(item);\n@@ -78,7 +78,7 @@ where\n     let f = &f;\n     scope_and_block(len.div_ceil(chunk_size), |scope| {\n         for chunk in items.chunks(chunk_size) {\n-            scope.spawn(async move {\n+            scope.spawn(move || {\n                 for item in chunk {\n                     f(item)?;\n                 }\n@@ -108,7 +108,7 @@ where\n     let f = &f;\n     scope_and_block(len.div_ceil(chunk_size), |scope| {\n         for chunk in items.chunks_mut(chunk_size) {\n-            scope.spawn(async move {\n+            scope.spawn(move || {\n                 for item in chunk {\n                     f(item)?;\n                 }\n@@ -138,7 +138,7 @@ where\n     let f = &f;\n     scope_and_block(len.div_ceil(chunk_size), |scope| {\n         for chunk in into_chunks(items, chunk_size) {\n-            scope.spawn(async move {\n+            scope.spawn(move || {\n                 for item in chunk {\n                     f(item)?;\n                 }\n@@ -167,7 +167,7 @@ where\n     let f = &f;\n     scope_and_block(len.div_ceil(chunk_size), |scope| {\n         for chunk in items.chunks(chunk_size) {\n-            scope.spawn(async move { chunk.iter().map(f).collect::<Vec<_>>() })\n+            scope.spawn(move || chunk.iter().map(f).collect::<Vec<_>>())\n         }\n     })\n     .flatten()\n@@ -192,7 +192,7 @@ where\n     let f = &f;\n     scope_and_block(len.div_ceil(chunk_size), |scope| {\n         for chunk in into_chunks(items, chunk_size) {\n-            scope.spawn(async move { chunk.map(f).collect::<Vec<_>>() })\n+            scope.spawn(move || chunk.map(f).collect::<Vec<_>>())\n         }\n     })\n     .flatten()"
        },
        {
            "sha": "ae047ae6d58bfdf99a13b8a39cb98f2349b6555b",
            "filename": "turbopack/crates/turbo-tasks/src/scope.rs",
            "status": "modified",
            "additions": 112,
            "deletions": 42,
            "changes": 154,
            "blob_url": "https://github.com/vercel/next.js/blob/b1f05782a01efde5a7a6f74738784bf67978661f/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Fscope.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/b1f05782a01efde5a7a6f74738784bf67978661f/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Fscope.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Fscope.rs?ref=b1f05782a01efde5a7a6f74738784bf67978661f",
            "patch": "@@ -2,33 +2,43 @@\n \n use std::{\n     any::Any,\n+    collections::VecDeque,\n     marker::PhantomData,\n     panic::{self, AssertUnwindSafe, catch_unwind},\n-    pin::Pin,\n     sync::{\n         Arc,\n         atomic::{AtomicUsize, Ordering},\n     },\n-    thread::{self, Thread},\n+    thread::{self, Thread, available_parallelism},\n     time::{Duration, Instant},\n };\n \n-use futures::FutureExt;\n-use parking_lot::Mutex;\n+use once_cell::sync::Lazy;\n+use parking_lot::{Condvar, Mutex};\n use tokio::{runtime::Handle, task::block_in_place};\n-use tracing::{Instrument, Span, info_span};\n+use tracing::{Span, info_span};\n \n-use crate::{\n-    TurboTasksApi,\n-    manager::{try_turbo_tasks, turbo_tasks_future_scope},\n-};\n+use crate::{TurboTasksApi, manager::try_turbo_tasks, turbo_tasks_scope};\n+\n+/// Number of worker tasks to spawn that process jobs. It's 1 less than the number of cpus as we\n+/// also use the current task as worker.\n+static WORKER_TASKS: Lazy<usize> = Lazy::new(|| available_parallelism().map_or(0, |n| n.get() - 1));\n+\n+enum WorkQueueJob {\n+    Job(usize, Box<dyn FnOnce() + Send + 'static>),\n+    End,\n+}\n \n struct ScopeInner {\n     main_thread: Thread,\n     remaining_tasks: AtomicUsize,\n     /// The first panic that occurred in the tasks, by task index.\n     /// The usize value is the index of the task.\n     panic: Mutex<Option<(Box<dyn Any + Send + 'static>, usize)>>,\n+    /// The work queue for spawned jobs that have not yet been picked up by a worker task.\n+    work_queue: Mutex<VecDeque<WorkQueueJob>>,\n+    /// A condition variable to notify worker tasks of new work or end of work.\n+    work_queue_condition_var: Condvar,\n }\n \n impl ScopeInner {\n@@ -82,6 +92,57 @@ impl ScopeInner {\n             panic::resume_unwind(err);\n         }\n     }\n+\n+    fn worker(&self, first_job_index: usize, first_job: Box<dyn FnOnce() + Send + 'static>) {\n+        let mut current_job_index = first_job_index;\n+        let mut current_job = first_job;\n+        loop {\n+            let result = catch_unwind(AssertUnwindSafe(current_job));\n+            let panic = result.err().map(|e| (e, current_job_index));\n+            self.on_task_finished(panic);\n+            let Some((index, job)) = self.pick_job_from_work_queue() else {\n+                return;\n+            };\n+            current_job_index = index;\n+            current_job = job;\n+        }\n+    }\n+\n+    fn pick_job_from_work_queue(&self) -> Option<(usize, Box<dyn FnOnce() + Send + 'static>)> {\n+        let mut work_queue = self.work_queue.lock();\n+        let job = loop {\n+            if let Some(job) = work_queue.pop_front() {\n+                break job;\n+            } else {\n+                self.work_queue_condition_var.wait(&mut work_queue);\n+            };\n+        };\n+        match job {\n+            WorkQueueJob::Job(index, job) => {\n+                drop(work_queue);\n+                Some((index, job))\n+            }\n+            WorkQueueJob::End => {\n+                work_queue.push_front(WorkQueueJob::End);\n+                drop(work_queue);\n+                self.work_queue_condition_var.notify_all();\n+                None\n+            }\n+        }\n+    }\n+\n+    fn end_and_help_complete(&self) {\n+        let job;\n+        {\n+            let mut work_queue = self.work_queue.lock();\n+            job = work_queue.pop_front();\n+            work_queue.push_back(WorkQueueJob::End);\n+        }\n+        self.work_queue_condition_var.notify_all();\n+        if let Some(WorkQueueJob::Job(index, job)) = job {\n+            self.worker(index, job);\n+        }\n+    }\n }\n \n /// Scope to allow spawning tasks with a limited lifetime.\n@@ -115,6 +176,8 @@ impl<'scope, 'env: 'scope, R: Send + 'env> Scope<'scope, 'env, R> {\n                 main_thread: thread::current(),\n                 remaining_tasks: AtomicUsize::new(0),\n                 panic: Mutex::new(None),\n+                work_queue: Mutex::new(VecDeque::new()),\n+                work_queue_condition_var: Condvar::new(),\n             }),\n             handle: Handle::current(),\n             turbo_tasks: try_turbo_tasks(),\n@@ -126,57 +189,64 @@ impl<'scope, 'env: 'scope, R: Send + 'env> Scope<'scope, 'env, R> {\n     /// Spawns a new task in the scope.\n     pub fn spawn<F>(&self, f: F)\n     where\n-        F: Future<Output = R> + Send + 'env,\n+        F: FnOnce() -> R + Send + 'env,\n     {\n         let index = self.index.fetch_add(1, Ordering::Relaxed);\n         assert!(index < self.results.len(), \"Too many tasks spawned\");\n         let result_cell: &Mutex<Option<R>> = &self.results[index];\n \n-        let f: Box<dyn Future<Output = ()> + Send + 'scope> = Box::new(async move {\n-            let result = f.await;\n+        let f: Box<dyn FnOnce() + Send + 'scope> = Box::new(|| {\n+            let result = f();\n             *result_cell.lock() = Some(result);\n         });\n-        let f: *mut (dyn Future<Output = ()> + Send + 'scope) = Box::into_raw(f);\n+        let f: *mut (dyn FnOnce() + Send + 'scope) = Box::into_raw(f);\n         // SAFETY: Scope ensures (e. g. in Drop) that spawned tasks is awaited before the\n         // lifetime `'env` ends.\n         #[allow(\n             clippy::unnecessary_cast,\n             reason = \"Clippy thinks this is unnecessary, but it actually changes the lifetime\"\n         )]\n-        let f = f as *mut (dyn Future<Output = ()> + Send + 'static);\n+        let f = f as *mut (dyn FnOnce() + Send + 'static);\n         // SAFETY: We just called `Box::into_raw`.\n         let f = unsafe { Box::from_raw(f) };\n-        // We pin the future in the box in memory to be able to await it.\n-        let f = Pin::from(f);\n \n         let turbo_tasks = self.turbo_tasks.clone();\n         let span = self.span.clone();\n \n-        let inner = self.inner.clone();\n-        inner.remaining_tasks.fetch_add(1, Ordering::Relaxed);\n-        self.handle.spawn(async move {\n-            let result = AssertUnwindSafe(\n-                async move {\n-                    if let Some(turbo_tasks) = turbo_tasks {\n-                        // Ensure that the turbo tasks context is maintained across the task.\n-                        turbo_tasks_future_scope(turbo_tasks, f).await;\n-                    } else {\n-                        // If no turbo tasks context is available, just run the future.\n-                        f.await;\n-                    }\n+        self.inner.remaining_tasks.fetch_add(1, Ordering::Relaxed);\n+\n+        // The first job always goes to the work_queue to be worked on by the main thread.\n+        // After that we spawn a new worker for every job until we reach WORKER_TASKS.\n+        // After that we queue up jobs in the work_queue again.\n+        if (1..=*WORKER_TASKS).contains(&index) {\n+            let inner = self.inner.clone();\n+            // Spawn a worker task that will process that tasks and potentially more.\n+            self.handle.spawn(async move {\n+                let _span = span.entered();\n+                if let Some(turbo_tasks) = turbo_tasks {\n+                    // Ensure that the turbo tasks context is maintained across the worker.\n+                    turbo_tasks_scope(turbo_tasks, || {\n+                        inner.worker(index, f);\n+                    });\n+                } else {\n+                    // If no turbo tasks context is available, just run the worker.\n+                    inner.worker(index, f);\n                 }\n-                .instrument(span),\n-            )\n-            .catch_unwind()\n-            .await;\n-            let panic = result.err().map(|e| (e, index));\n-            inner.on_task_finished(panic);\n-        });\n+            });\n+        } else {\n+            // Queue the task to be processed by a worker task.\n+            self.inner\n+                .work_queue\n+                .lock()\n+                .push_back(WorkQueueJob::Job(index, f));\n+            self.inner.work_queue_condition_var.notify_one();\n+        }\n     }\n }\n \n impl<'scope, 'env: 'scope, R: Send + 'env> Drop for Scope<'scope, 'env, R> {\n     fn drop(&mut self) {\n+        self.inner.end_and_help_complete();\n         self.inner.wait_and_rethrow_panic();\n     }\n }\n@@ -223,7 +293,7 @@ mod tests {\n     async fn test_scope() {\n         let results = scope_and_block(1000, |scope| {\n             for i in 0..1000 {\n-                scope.spawn(async move { i });\n+                scope.spawn(move || i);\n             }\n         });\n         results.enumerate().for_each(|(i, result)| {\n@@ -235,7 +305,7 @@ mod tests {\n     async fn test_empty_scope() {\n         let results = scope_and_block(0, |scope| {\n             if false {\n-                scope.spawn(async move { 42 });\n+                scope.spawn(|| 42);\n             }\n         });\n         assert_eq!(results.count(), 0);\n@@ -244,7 +314,7 @@ mod tests {\n     #[tokio::test(flavor = \"multi_thread\", worker_threads = 2)]\n     async fn test_single_task() {\n         let results = scope_and_block(1, |scope| {\n-            scope.spawn(async move { 42 });\n+            scope.spawn(|| 42);\n         })\n         .collect::<Vec<_>>();\n         assert_eq!(results, vec![42]);\n@@ -253,7 +323,7 @@ mod tests {\n     #[tokio::test(flavor = \"multi_thread\", worker_threads = 2)]\n     async fn test_task_finish_before_scope() {\n         let results = scope_and_block(1, |scope| {\n-            scope.spawn(async move { 42 });\n+            scope.spawn(|| 42);\n             thread::sleep(std::time::Duration::from_millis(100));\n         })\n         .collect::<Vec<_>>();\n@@ -263,7 +333,7 @@ mod tests {\n     #[tokio::test(flavor = \"multi_thread\", worker_threads = 2)]\n     async fn test_task_finish_after_scope() {\n         let results = scope_and_block(1, |scope| {\n-            scope.spawn(async move {\n+            scope.spawn(|| {\n                 thread::sleep(std::time::Duration::from_millis(100));\n                 42\n             });\n@@ -277,7 +347,7 @@ mod tests {\n         let result = catch_unwind(AssertUnwindSafe(|| {\n             let _results = scope_and_block(1000, |scope| {\n                 for i in 0..500 {\n-                    scope.spawn(async move { i });\n+                    scope.spawn(move || i);\n                 }\n                 panic!(\"Intentional panic\");\n             });\n@@ -295,7 +365,7 @@ mod tests {\n         let result = catch_unwind(AssertUnwindSafe(|| {\n             let _results = scope_and_block(1000, |scope| {\n                 for i in 0..1000 {\n-                    scope.spawn(async move {\n+                    scope.spawn(move || {\n                         if i == 500 {\n                             panic!(\"Intentional panic\");\n                         } else if i == 501 {"
        }
    ],
    "stats": {
        "total": 168,
        "additions": 119,
        "deletions": 49
    }
}