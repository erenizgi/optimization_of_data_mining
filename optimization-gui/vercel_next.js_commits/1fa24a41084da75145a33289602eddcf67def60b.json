{
    "author": "sokra",
    "message": "Turbopack: Lazy decompress medium value compressed blocks (#82257)\n\n### What?\n\nDuring compaction we can lazily recompress medium-sized values to avoid the large temporary memory usage.",
    "sha": "1fa24a41084da75145a33289602eddcf67def60b",
    "files": [
        {
            "sha": "093ac87a99ad50f4f98e69aae3b0176d576d3cae",
            "filename": "turbopack/crates/turbo-persistence/src/compression.rs",
            "status": "added",
            "additions": 56,
            "deletions": 0,
            "changes": 56,
            "blob_url": "https://github.com/vercel/next.js/blob/1fa24a41084da75145a33289602eddcf67def60b/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fcompression.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/1fa24a41084da75145a33289602eddcf67def60b/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fcompression.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fcompression.rs?ref=1fa24a41084da75145a33289602eddcf67def60b",
            "patch": "@@ -0,0 +1,56 @@\n+use std::{mem::MaybeUninit, sync::Arc};\n+\n+use anyhow::{Context, Result};\n+use lzzzz::lz4::{ACC_LEVEL_DEFAULT, decompress, decompress_with_dict};\n+\n+#[tracing::instrument(level = \"trace\", skip_all)]\n+pub fn decompress_into_arc(\n+    uncompressed_length: u32,\n+    block: &[u8],\n+    compression_dictionary: Option<&[u8]>,\n+    _long_term: bool,\n+) -> Result<Arc<[u8]>> {\n+    // We directly allocate the buffer in an Arc to avoid copying it into an Arc and avoiding\n+    // double indirection. This is a dynamically sized arc.\n+    let buffer: Arc<[MaybeUninit<u8>]> = Arc::new_zeroed_slice(uncompressed_length as usize);\n+    // Assume that the buffer is initialized.\n+    let buffer = Arc::into_raw(buffer);\n+    // Safety: Assuming that the buffer is initialized is safe because we just created it as\n+    // zeroed slice and u8 doesn't require initialization.\n+    let mut buffer = unsafe { Arc::from_raw(buffer as *mut [u8]) };\n+    // Safety: We know that the buffer is not shared yet.\n+    let decompressed = unsafe { Arc::get_mut_unchecked(&mut buffer) };\n+    let bytes_writes = if let Some(dict) = compression_dictionary {\n+        // Safety: decompress_with_dict will only write to `decompressed` and not read from it.\n+        decompress_with_dict(block, decompressed, dict)?\n+    } else {\n+        // Safety: decompress will only write to `decompressed` and not read from it.\n+        decompress(block, decompressed)?\n+    };\n+    assert_eq!(\n+        bytes_writes, uncompressed_length as usize,\n+        \"Decompressed length does not match expected length\"\n+    );\n+    // Safety: The buffer is now fully initialized and can be used.\n+    Ok(buffer)\n+}\n+\n+#[tracing::instrument(level = \"trace\", skip_all)]\n+pub fn compress_into_buffer(\n+    block: &[u8],\n+    dict: Option<&[u8]>,\n+    _long_term: bool,\n+    buffer: &mut Vec<u8>,\n+) -> Result<()> {\n+    let mut compressor = if let Some(dict) = dict {\n+        lzzzz::lz4::Compressor::with_dict(dict)\n+    } else {\n+        lzzzz::lz4::Compressor::new()\n+    }\n+    .context(\"LZ4 compressor creation failed\")?;\n+    let acc_factor = ACC_LEVEL_DEFAULT;\n+    compressor\n+        .next_to_vec(block, buffer, acc_factor)\n+        .context(\"Compression failed\")?;\n+    Ok(())\n+}"
        },
        {
            "sha": "95cadfc14b37ed463a31af74772ee373b77a1d54",
            "filename": "turbopack/crates/turbo-persistence/src/db.rs",
            "status": "modified",
            "additions": 11,
            "deletions": 19,
            "changes": 30,
            "blob_url": "https://github.com/vercel/next.js/blob/1fa24a41084da75145a33289602eddcf67def60b/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fdb.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/1fa24a41084da75145a33289602eddcf67def60b/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fdb.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fdb.rs?ref=1fa24a41084da75145a33289602eddcf67def60b",
            "patch": "@@ -3,19 +3,15 @@ use std::{\n     collections::HashSet,\n     fs::{self, File, OpenOptions, ReadDir},\n     io::{BufWriter, Write},\n-    mem::{MaybeUninit, swap, transmute},\n+    mem::swap,\n     ops::RangeInclusive,\n     path::{Path, PathBuf},\n-    sync::{\n-        Arc,\n-        atomic::{AtomicBool, AtomicU32, Ordering},\n-    },\n+    sync::atomic::{AtomicBool, AtomicU32, Ordering},\n };\n \n use anyhow::{Context, Result, bail};\n use byteorder::{BE, ReadBytesExt, WriteBytesExt};\n use jiff::Timestamp;\n-use lzzzz::lz4::decompress;\n use memmap2::Mmap;\n use parking_lot::{Mutex, RwLock};\n \n@@ -24,6 +20,7 @@ use crate::{\n     QueryKey,\n     arc_slice::ArcSlice,\n     compaction::selector::{Compactable, compute_metrics, get_merge_segments},\n+    compression::decompress_into_arc,\n     constants::{\n         AMQF_AVG_SIZE, AMQF_CACHE_SIZE, DATA_THRESHOLD_PER_COMPACTED_FILE, KEY_BLOCK_AVG_SIZE,\n         KEY_BLOCK_CACHE_SIZE, MAX_ENTRIES_PER_COMPACTED_FILE, VALUE_BLOCK_AVG_SIZE,\n@@ -392,14 +389,9 @@ impl<S: ParallelScheduler> TurboPersistence<S> {\n         #[cfg(target_os = \"linux\")]\n         mmap.advise(memmap2::Advice::Unmergeable)?;\n         let mut compressed = &mmap[..];\n-        let uncompressed_length = compressed.read_u32::<BE>()? as usize;\n-\n-        let buffer = Arc::new_zeroed_slice(uncompressed_length);\n-        // Safety: MaybeUninit<u8> can be safely transmuted to u8.\n-        let mut buffer = unsafe { transmute::<Arc<[MaybeUninit<u8>]>, Arc<[u8]>>(buffer) };\n-        // Safety: We know that the buffer is not shared yet.\n-        let decompressed = unsafe { Arc::get_mut_unchecked(&mut buffer) };\n-        decompress(compressed, decompressed)?;\n+        let uncompressed_length = compressed.read_u32::<BE>()?;\n+\n+        let buffer = decompress_into_arc(uncompressed_length, compressed, None, true)?;\n         Ok(ArcSlice::from(buffer))\n     }\n \n@@ -918,9 +910,9 @@ impl<S: ParallelScheduler> TurboPersistence<S> {\n                                 });\n                             }\n \n-                            fn create_sst_file<S: ParallelScheduler>(\n+                            fn create_sst_file<'l, S: ParallelScheduler>(\n                                 parallel_scheduler: &S,\n-                                entries: &[LookupEntry],\n+                                entries: &[LookupEntry<'l>],\n                                 total_key_size: usize,\n                                 total_value_size: usize,\n                                 path: &Path,\n@@ -964,7 +956,7 @@ impl<S: ParallelScheduler> TurboPersistence<S> {\n \n                             let mut total_key_size = 0;\n                             let mut total_value_size = 0;\n-                            let mut current: Option<LookupEntry> = None;\n+                            let mut current: Option<LookupEntry<'_>> = None;\n                             let mut entries = Vec::new();\n                             let mut last_entries = Vec::new();\n                             let mut last_entries_total_sizes = (0, 0);\n@@ -975,7 +967,7 @@ impl<S: ParallelScheduler> TurboPersistence<S> {\n                                 if let Some(current) = current.take() {\n                                     if current.key != entry.key {\n                                         let key_size = current.key.len();\n-                                        let value_size = current.value.size_in_sst();\n+                                        let value_size = current.value.uncompressed_size_in_sst();\n                                         total_key_size += key_size;\n                                         total_value_size += value_size;\n \n@@ -1023,7 +1015,7 @@ impl<S: ParallelScheduler> TurboPersistence<S> {\n                             }\n                             if let Some(entry) = current {\n                                 total_key_size += entry.key.len();\n-                                total_value_size += entry.value.size_in_sst();\n+                                total_value_size += entry.value.uncompressed_size_in_sst();\n                                 entries.push(entry);\n                             }\n "
        },
        {
            "sha": "b95f481c9002ab10ec0ac927dd5aebf60c1a3455",
            "filename": "turbopack/crates/turbo-persistence/src/lib.rs",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/vercel/next.js/blob/1fa24a41084da75145a33289602eddcf67def60b/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Flib.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/1fa24a41084da75145a33289602eddcf67def60b/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Flib.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Flib.rs?ref=1fa24a41084da75145a33289602eddcf67def60b",
            "patch": "@@ -8,6 +8,7 @@ mod arc_slice;\n mod collector;\n mod collector_entry;\n mod compaction;\n+mod compression;\n mod constants;\n mod db;\n mod key;"
        },
        {
            "sha": "1c5048998a01f02ddd2d96ae54a09083ca766595",
            "filename": "turbopack/crates/turbo-persistence/src/lookup_entry.rs",
            "status": "modified",
            "additions": 35,
            "deletions": 11,
            "changes": 46,
            "blob_url": "https://github.com/vercel/next.js/blob/1fa24a41084da75145a33289602eddcf67def60b/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Flookup_entry.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/1fa24a41084da75145a33289602eddcf67def60b/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Flookup_entry.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Flookup_entry.rs?ref=1fa24a41084da75145a33289602eddcf67def60b",
            "patch": "@@ -14,28 +14,43 @@ pub enum LookupValue {\n     Blob { sequence_number: u32 },\n }\n \n-impl LookupValue {\n+/// A value from a SST file lookup.\n+pub enum LazyLookupValue<'l> {\n+    /// A LookupValue\n+    Eager(LookupValue),\n+    /// A medium sized value that is still compressed.\n+    Medium {\n+        uncompressed_size: u32,\n+        block: &'l [u8],\n+        dictionary: &'l [u8],\n+    },\n+}\n+\n+impl LazyLookupValue<'_> {\n     /// Returns the size of the value in the SST file.\n-    pub fn size_in_sst(&self) -> usize {\n+    pub fn uncompressed_size_in_sst(&self) -> usize {\n         match self {\n-            LookupValue::Slice { value } => value.len(),\n-            LookupValue::Deleted => 0,\n-            LookupValue::Blob { .. } => 0,\n+            LazyLookupValue::Eager(LookupValue::Slice { value }) => value.len(),\n+            LazyLookupValue::Eager(LookupValue::Deleted) => 0,\n+            LazyLookupValue::Eager(LookupValue::Blob { .. }) => 0,\n+            LazyLookupValue::Medium {\n+                uncompressed_size, ..\n+            } => *uncompressed_size as usize,\n         }\n     }\n }\n \n /// An entry from a SST file lookup.\n-pub struct LookupEntry {\n+pub struct LookupEntry<'l> {\n     /// The hash of the key.\n     pub hash: u64,\n     /// The key.\n     pub key: ArcSlice<u8>,\n     /// The value.\n-    pub value: LookupValue,\n+    pub value: LazyLookupValue<'l>,\n }\n \n-impl Entry for LookupEntry {\n+impl Entry for LookupEntry<'_> {\n     fn key_hash(&self) -> u64 {\n         self.hash\n     }\n@@ -50,17 +65,26 @@ impl Entry for LookupEntry {\n \n     fn value(&self) -> EntryValue<'_> {\n         match &self.value {\n-            LookupValue::Deleted => EntryValue::Deleted,\n-            LookupValue::Slice { value } => {\n+            LazyLookupValue::Eager(LookupValue::Deleted) => EntryValue::Deleted,\n+            LazyLookupValue::Eager(LookupValue::Slice { value }) => {\n                 if value.len() > MAX_SMALL_VALUE_SIZE {\n                     EntryValue::Medium { value }\n                 } else {\n                     EntryValue::Small { value }\n                 }\n             }\n-            LookupValue::Blob { sequence_number } => EntryValue::Large {\n+            LazyLookupValue::Eager(LookupValue::Blob { sequence_number }) => EntryValue::Large {\n                 blob: *sequence_number,\n             },\n+            LazyLookupValue::Medium {\n+                uncompressed_size,\n+                block,\n+                dictionary,\n+            } => EntryValue::MediumCompressed {\n+                uncompressed_size: *uncompressed_size,\n+                block,\n+                dictionary,\n+            },\n         }\n     }\n }"
        },
        {
            "sha": "087484db4ca3b87ea8bffa2bc368c705d5131f93",
            "filename": "turbopack/crates/turbo-persistence/src/merge_iter.rs",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/vercel/next.js/blob/1fa24a41084da75145a33289602eddcf67def60b/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fmerge_iter.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/1fa24a41084da75145a33289602eddcf67def60b/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fmerge_iter.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fmerge_iter.rs?ref=1fa24a41084da75145a33289602eddcf67def60b",
            "patch": "@@ -6,27 +6,27 @@ use crate::lookup_entry::LookupEntry;\n \n /// An active iterator that is being merged. It has peeked the next element and can be compared\n /// according to that element. The `order` is used when multiple iterators have the same key.\n-struct ActiveIterator<T: Iterator<Item = Result<LookupEntry>>> {\n+struct ActiveIterator<'l, T: Iterator<Item = Result<LookupEntry<'l>>>> {\n     iter: T,\n     order: usize,\n-    entry: LookupEntry,\n+    entry: LookupEntry<'l>,\n }\n \n-impl<T: Iterator<Item = Result<LookupEntry>>> PartialEq for ActiveIterator<T> {\n+impl<'l, T: Iterator<Item = Result<LookupEntry<'l>>>> PartialEq for ActiveIterator<'l, T> {\n     fn eq(&self, other: &Self) -> bool {\n         self.entry.hash == other.entry.hash && *self.entry.key == *other.entry.key\n     }\n }\n \n-impl<T: Iterator<Item = Result<LookupEntry>>> Eq for ActiveIterator<T> {}\n+impl<'l, T: Iterator<Item = Result<LookupEntry<'l>>>> Eq for ActiveIterator<'l, T> {}\n \n-impl<T: Iterator<Item = Result<LookupEntry>>> PartialOrd for ActiveIterator<T> {\n+impl<'l, T: Iterator<Item = Result<LookupEntry<'l>>>> PartialOrd for ActiveIterator<'l, T> {\n     fn partial_cmp(&self, other: &Self) -> Option<Ordering> {\n         Some(self.cmp(other))\n     }\n }\n \n-impl<T: Iterator<Item = Result<LookupEntry>>> Ord for ActiveIterator<T> {\n+impl<'l, T: Iterator<Item = Result<LookupEntry<'l>>>> Ord for ActiveIterator<'l, T> {\n     fn cmp(&self, other: &Self) -> Ordering {\n         self.entry\n             .hash\n@@ -39,11 +39,11 @@ impl<T: Iterator<Item = Result<LookupEntry>>> Ord for ActiveIterator<T> {\n \n /// An iterator that merges multiple sorted iterators into a single sorted iterator. Internal it\n /// uses an heap of iterators to iterate them in order.\n-pub struct MergeIter<T: Iterator<Item = Result<LookupEntry>>> {\n-    heap: BinaryHeap<ActiveIterator<T>>,\n+pub struct MergeIter<'l, T: Iterator<Item = Result<LookupEntry<'l>>>> {\n+    heap: BinaryHeap<ActiveIterator<'l, T>>,\n }\n \n-impl<T: Iterator<Item = Result<LookupEntry>>> MergeIter<T> {\n+impl<'l, T: Iterator<Item = Result<LookupEntry<'l>>>> MergeIter<'l, T> {\n     pub fn new(iters: impl Iterator<Item = T>) -> Result<Self> {\n         let mut heap = BinaryHeap::new();\n         for (order, mut iter) in iters.enumerate() {\n@@ -56,8 +56,8 @@ impl<T: Iterator<Item = Result<LookupEntry>>> MergeIter<T> {\n     }\n }\n \n-impl<T: Iterator<Item = Result<LookupEntry>>> Iterator for MergeIter<T> {\n-    type Item = Result<LookupEntry>;\n+impl<'l, T: Iterator<Item = Result<LookupEntry<'l>>>> Iterator for MergeIter<'l, T> {\n+    type Item = Result<LookupEntry<'l>>;\n \n     fn next(&mut self) -> Option<Self::Item> {\n         let ActiveIterator {"
        },
        {
            "sha": "55765d12861075d125896cc4b9f0dad9bcdb3c47",
            "filename": "turbopack/crates/turbo-persistence/src/static_sorted_file.rs",
            "status": "modified",
            "additions": 46,
            "deletions": 23,
            "changes": 69,
            "blob_url": "https://github.com/vercel/next.js/blob/1fa24a41084da75145a33289602eddcf67def60b/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fstatic_sorted_file.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/1fa24a41084da75145a33289602eddcf67def60b/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fstatic_sorted_file.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fstatic_sorted_file.rs?ref=1fa24a41084da75145a33289602eddcf67def60b",
            "patch": "@@ -2,23 +2,21 @@ use std::{\n     cmp::Ordering,\n     fs::File,\n     hash::BuildHasherDefault,\n-    mem::{MaybeUninit, transmute},\n     ops::Range,\n     path::{Path, PathBuf},\n-    sync::Arc,\n };\n \n use anyhow::{Context, Result, bail};\n use byteorder::{BE, ReadBytesExt};\n-use lzzzz::lz4::decompress_with_dict;\n use memmap2::Mmap;\n use quick_cache::sync::GuardResult;\n use rustc_hash::FxHasher;\n \n use crate::{\n     QueryKey,\n     arc_slice::ArcSlice,\n-    lookup_entry::{LookupEntry, LookupValue},\n+    compression::decompress_into_arc,\n+    lookup_entry::{LazyLookupValue, LookupEntry, LookupValue},\n };\n \n /// The block header for an index block.\n@@ -326,6 +324,7 @@ impl StaticSortedFile {\n         self.read_block(\n             block_index,\n             &self.mmap[self.meta.key_compression_dictionary_range()],\n+            false,\n         )\n     }\n \n@@ -334,11 +333,30 @@ impl StaticSortedFile {\n         self.read_block(\n             block_index,\n             &self.mmap[self.meta.value_compression_dictionary_range()],\n+            false,\n         )\n     }\n \n     /// Reads a block from the file.\n-    fn read_block(&self, block_index: u16, compression_dictionary: &[u8]) -> Result<ArcSlice<u8>> {\n+    fn read_block(\n+        &self,\n+        block_index: u16,\n+        compression_dictionary: &[u8],\n+        long_term: bool,\n+    ) -> Result<ArcSlice<u8>> {\n+        let (uncompressed_length, block) = self.get_compressed_block(block_index)?;\n+\n+        let buffer = decompress_into_arc(\n+            uncompressed_length,\n+            block,\n+            Some(compression_dictionary),\n+            long_term,\n+        )?;\n+        Ok(ArcSlice::from(buffer))\n+    }\n+\n+    /// Gets the slice of the compressed block from the memory mapped file.\n+    fn get_compressed_block(&self, block_index: u16) -> Result<(u32, &[u8])> {\n         #[cfg(feature = \"strict_checks\")]\n         if block_index >= self.meta.block_count {\n             bail!(\n@@ -386,17 +404,9 @@ impl StaticSortedFile {\n                 self.meta.blocks_start()\n             );\n         }\n-        let uncompressed_length =\n-            (&self.mmap[block_start..block_start + 4]).read_u32::<BE>()? as usize;\n-        let block = self.mmap[block_start + 4..block_end].to_vec();\n-\n-        let buffer = Arc::new_zeroed_slice(uncompressed_length);\n-        // Safety: MaybeUninit<u8> can be safely transmuted to u8.\n-        let mut buffer = unsafe { transmute::<Arc<[MaybeUninit<u8>]>, Arc<[u8]>>(buffer) };\n-        // Safety: We know that the buffer is not shared yet.\n-        let decompressed = unsafe { Arc::get_mut_unchecked(&mut buffer) };\n-        decompress_with_dict(&block, decompressed, compression_dictionary)?;\n-        Ok(ArcSlice::from(buffer))\n+        let uncompressed_length = (&self.mmap[block_start..block_start + 4]).read_u32::<BE>()?;\n+        let block = &self.mmap[block_start + 4..block_end];\n+        Ok((uncompressed_length, block))\n     }\n }\n \n@@ -423,15 +433,15 @@ struct CurrentIndexBlock {\n     index: usize,\n }\n \n-impl Iterator for StaticSortedFileIter<'_> {\n-    type Item = Result<LookupEntry>;\n+impl<'l> Iterator for StaticSortedFileIter<'l> {\n+    type Item = Result<LookupEntry<'l>>;\n \n     fn next(&mut self) -> Option<Self::Item> {\n         self.next_internal().transpose()\n     }\n }\n \n-impl StaticSortedFileIter<'_> {\n+impl<'l> StaticSortedFileIter<'l> {\n     /// Enters a block at the given index.\n     fn enter_block(&mut self, block_index: u16) -> Result<()> {\n         let block_arc = self.this.get_key_block(block_index, self.key_block_cache)?;\n@@ -468,7 +478,7 @@ impl StaticSortedFileIter<'_> {\n     }\n \n     /// Gets the next entry in the file and moves the cursor.\n-    fn next_internal(&mut self) -> Result<Option<LookupEntry>> {\n+    fn next_internal(&mut self) -> Result<Option<LookupEntry<'l>>> {\n         loop {\n             if let Some(CurrentKeyBlock {\n                 offsets,\n@@ -479,9 +489,22 @@ impl StaticSortedFileIter<'_> {\n             {\n                 let GetKeyEntryResult { hash, key, ty, val } =\n                     get_key_entry(&offsets, &entries, entry_count, index)?;\n-                let value = self\n-                    .this\n-                    .handle_key_match(ty, val, self.value_block_cache)?;\n+                let value = if ty == KEY_BLOCK_ENTRY_TYPE_MEDIUM {\n+                    let mut val = val;\n+                    let block = val.read_u16::<BE>()?;\n+                    let (uncompressed_size, block) = self.this.get_compressed_block(block)?;\n+                    LazyLookupValue::Medium {\n+                        uncompressed_size,\n+                        block,\n+                        dictionary: &self.this.mmap\n+                            [self.this.meta.value_compression_dictionary_range()],\n+                    }\n+                } else {\n+                    let value = self\n+                        .this\n+                        .handle_key_match(ty, val, self.value_block_cache)?;\n+                    LazyLookupValue::Eager(value)\n+                };\n                 let entry = LookupEntry {\n                     hash,\n                     // Safety: The key is a valid slice of the entries."
        },
        {
            "sha": "fb20b026d7b11e44a595942dad8052f8c38e8f46",
            "filename": "turbopack/crates/turbo-persistence/src/static_sorted_file_builder.rs",
            "status": "modified",
            "additions": 38,
            "deletions": 19,
            "changes": 57,
            "blob_url": "https://github.com/vercel/next.js/blob/1fa24a41084da75145a33289602eddcf67def60b/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fstatic_sorted_file_builder.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/1fa24a41084da75145a33289602eddcf67def60b/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fstatic_sorted_file_builder.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fstatic_sorted_file_builder.rs?ref=1fa24a41084da75145a33289602eddcf67def60b",
            "patch": "@@ -8,11 +8,13 @@ use std::{\n \n use anyhow::{Context, Result};\n use byteorder::{BE, ByteOrder, WriteBytesExt};\n-use lzzzz::lz4::{ACC_LEVEL_DEFAULT, max_compressed_size};\n \n-use crate::static_sorted_file::{\n-    BLOCK_TYPE_INDEX, BLOCK_TYPE_KEY, KEY_BLOCK_ENTRY_TYPE_BLOB, KEY_BLOCK_ENTRY_TYPE_DELETED,\n-    KEY_BLOCK_ENTRY_TYPE_MEDIUM, KEY_BLOCK_ENTRY_TYPE_SMALL,\n+use crate::{\n+    compression::{compress_into_buffer, decompress_into_arc},\n+    static_sorted_file::{\n+        BLOCK_TYPE_INDEX, BLOCK_TYPE_KEY, KEY_BLOCK_ENTRY_TYPE_BLOB, KEY_BLOCK_ENTRY_TYPE_DELETED,\n+        KEY_BLOCK_ENTRY_TYPE_MEDIUM, KEY_BLOCK_ENTRY_TYPE_SMALL,\n+    },\n };\n \n /// The maximum number of entries that should go into a single key block\n@@ -68,6 +70,12 @@ pub enum EntryValue<'l> {\n     Small { value: &'l [u8] },\n     /// Medium-sized value. They are stored in their own value block.\n     Medium { value: &'l [u8] },\n+    /// Medium-sized value. They are stored in their own value block. Precompressed.\n+    MediumCompressed {\n+        uncompressed_size: u32,\n+        block: &'l [u8],\n+        dictionary: &'l [u8],\n+    },\n     /// Large-sized value. They are stored in a blob file.\n     Large { blob: u32 },\n     /// Tombstone. The value was removed.\n@@ -293,25 +301,25 @@ impl<'l> BlockWriter<'l> {\n \n     #[tracing::instrument(level = \"trace\", skip_all)]\n     fn write_key_block(&mut self, block: &[u8], dict: &[u8]) -> Result<()> {\n-        self.write_block(block, dict)\n+        self.write_block(block, dict, false)\n             .context(\"Failed to write key block\")\n     }\n \n     #[tracing::instrument(level = \"trace\", skip_all)]\n     fn write_index_block(&mut self, block: &[u8], dict: &[u8]) -> Result<()> {\n-        self.write_block(block, dict)\n+        self.write_block(block, dict, false)\n             .context(\"Failed to write index block\")\n     }\n \n     #[tracing::instrument(level = \"trace\", skip_all)]\n     fn write_value_block(&mut self, block: &[u8], dict: &[u8]) -> Result<()> {\n-        self.write_block(block, dict)\n+        self.write_block(block, dict, false)\n             .context(\"Failed to write value block\")\n     }\n \n-    fn write_block(&mut self, block: &[u8], dict: &[u8]) -> Result<()> {\n+    fn write_block(&mut self, block: &[u8], dict: &[u8], long_term: bool) -> Result<()> {\n         let uncompressed_size = block.len().try_into().unwrap();\n-        self.compress_block_into_buffer(block, dict);\n+        self.compress_block_into_buffer(block, dict, long_term)?;\n         let len = (self.buffer.len() + 4).try_into().unwrap();\n         let offset = self\n             .block_offsets\n@@ -333,14 +341,13 @@ impl<'l> BlockWriter<'l> {\n     }\n \n     /// Compresses a block with a compression dictionary.\n-    #[tracing::instrument(level = \"trace\", skip_all)]\n-    fn compress_block_into_buffer(&mut self, block: &[u8], dict: &[u8]) {\n-        let mut compressor =\n-            lzzzz::lz4::Compressor::with_dict(dict).expect(\"LZ4 compressor creation failed\");\n-        self.buffer.reserve(max_compressed_size(block.len()));\n-        compressor\n-            .next_to_vec(block, self.buffer, ACC_LEVEL_DEFAULT)\n-            .expect(\"Compression failed\");\n+    fn compress_block_into_buffer(\n+        &mut self,\n+        block: &[u8],\n+        dict: &[u8],\n+        long_term: bool,\n+    ) -> Result<()> {\n+        compress_into_buffer(block, Some(dict), long_term, self.buffer)\n     }\n }\n \n@@ -386,7 +393,19 @@ fn write_value_blocks(\n                 value_locations.push((block_index, 0));\n                 writer.write_value_block(value, value_compression_dictionary)?;\n             }\n-            _ => {\n+            EntryValue::MediumCompressed {\n+                uncompressed_size,\n+                block,\n+                dictionary,\n+            } => {\n+                let block_index = writer.next_block_index();\n+                value_locations.push((block_index, 0));\n+                // Recompress block with a different dictionary\n+                let decompressed =\n+                    decompress_into_arc(uncompressed_size, block, Some(dictionary), false)?;\n+                writer.write_value_block(&decompressed, value_compression_dictionary)?;\n+            }\n+            EntryValue::Deleted | EntryValue::Large { .. } => {\n                 value_locations.push((0, 0));\n             }\n         }\n@@ -438,7 +457,7 @@ fn write_key_blocks_and_compute_amqf(\n                     value.len().try_into().unwrap(),\n                 );\n             }\n-            EntryValue::Medium { .. } => {\n+            EntryValue::Medium { .. } | EntryValue::MediumCompressed { .. } => {\n                 block.put_medium(entry, value_location.0);\n             }\n             EntryValue::Large { blob } => {"
        },
        {
            "sha": "9696abc9a9a57d203cb34090454ab8a09cab1142",
            "filename": "turbopack/crates/turbo-persistence/src/write_batch.rs",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/vercel/next.js/blob/1fa24a41084da75145a33289602eddcf67def60b/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fwrite_batch.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/1fa24a41084da75145a33289602eddcf67def60b/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fwrite_batch.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fwrite_batch.rs?ref=1fa24a41084da75145a33289602eddcf67def60b",
            "patch": "@@ -10,7 +10,6 @@ use std::{\n use anyhow::{Context, Result};\n use byteorder::{BE, WriteBytesExt};\n use either::Either;\n-use lzzzz::lz4::{self, ACC_LEVEL_DEFAULT};\n use parking_lot::Mutex;\n use smallvec::SmallVec;\n use thread_local::ThreadLocal;\n@@ -19,6 +18,7 @@ use crate::{\n     ValueBuffer,\n     collector::Collector,\n     collector_entry::CollectorEntry,\n+    compression::compress_into_buffer,\n     constants::{MAX_MEDIUM_VALUE_SIZE, THREAD_LOCAL_SIZE_SHIFT},\n     key::StoreKey,\n     meta_file_builder::MetaFileBuilder,\n@@ -390,7 +390,7 @@ impl<K: StoreKey + Send + Sync, S: ParallelScheduler, const FAMILIES: usize>\n         let seq = self.current_sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n         let mut buffer = Vec::new();\n         buffer.write_u32::<BE>(value.len() as u32)?;\n-        lz4::compress_to_vec(value, &mut buffer, ACC_LEVEL_DEFAULT)\n+        compress_into_buffer(value, None, true, &mut buffer)\n             .context(\"Compression of value for blob file failed\")?;\n \n         let file = self.db_path.join(format!(\"{seq:08}.blob\"));"
        }
    ],
    "stats": {
        "total": 285,
        "additions": 200,
        "deletions": 85
    }
}