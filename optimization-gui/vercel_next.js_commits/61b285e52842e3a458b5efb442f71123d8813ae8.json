{
    "author": "acdlite",
    "message": "Generalize Segment Cache fallback implementation (#84652)\n\nThis is a stack of refactors related to the internal map type used to\nstore Segment Cache entries on the client. Refer to each commit message\nfor full details.\n\nThe main motivation is to generalize the \"fallback entry\" mechanism used\nfor interception routes to more kinds of params. Currently this is only\nused for interception routes and search params, but in the future we\nwill use it for route params, too.",
    "sha": "61b285e52842e3a458b5efb442f71123d8813ae8",
    "files": [
        {
            "sha": "a8f1d53b5be4602fb109cf613961264139246e2f",
            "filename": "packages/next/src/client/components/segment-cache-impl/cache-key.ts",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/vercel/next.js/blob/61b285e52842e3a458b5efb442f71123d8813ae8/packages%2Fnext%2Fsrc%2Fclient%2Fcomponents%2Fsegment-cache-impl%2Fcache-key.ts",
            "raw_url": "https://github.com/vercel/next.js/raw/61b285e52842e3a458b5efb442f71123d8813ae8/packages%2Fnext%2Fsrc%2Fclient%2Fcomponents%2Fsegment-cache-impl%2Fcache-key.ts",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/packages%2Fnext%2Fsrc%2Fclient%2Fcomponents%2Fsegment-cache-impl%2Fcache-key.ts?ref=61b285e52842e3a458b5efb442f71123d8813ae8",
            "patch": "@@ -2,14 +2,14 @@\n type Opaque<K, T> = T & { __brand: K }\n \n // Only functions in this module should be allowed to create CacheKeys.\n-export type NormalizedHref = Opaque<'NormalizedHref', string>\n+export type NormalizedPathname = Opaque<'NormalizedPathname', string>\n export type NormalizedSearch = Opaque<'NormalizedSearch', string>\n export type NormalizedNextUrl = Opaque<'NormalizedNextUrl', string>\n \n export type RouteCacheKey = Opaque<\n   'RouteCacheKey',\n   {\n-    href: NormalizedHref\n+    pathname: NormalizedPathname\n     search: NormalizedSearch\n     nextUrl: NormalizedNextUrl | null\n \n@@ -21,11 +21,9 @@ export function createCacheKey(\n   originalHref: string,\n   nextUrl: string | null\n ): RouteCacheKey {\n-  // TODO: We should remove the hash from the href and track that separately.\n-  // There's no reason to vary route entries by hash.\n   const originalUrl = new URL(originalHref)\n   const cacheKey = {\n-    href: originalHref as NormalizedHref,\n+    pathname: originalUrl.pathname as NormalizedPathname,\n     search: originalUrl.search as NormalizedSearch,\n     nextUrl: nextUrl as NormalizedNextUrl | null,\n   } as RouteCacheKey"
        },
        {
            "sha": "159f3272a9444b6c13855193e7468bd06f1ee0d3",
            "filename": "packages/next/src/client/components/segment-cache-impl/cache-map.ts",
            "status": "added",
            "additions": 472,
            "deletions": 0,
            "changes": 472,
            "blob_url": "https://github.com/vercel/next.js/blob/61b285e52842e3a458b5efb442f71123d8813ae8/packages%2Fnext%2Fsrc%2Fclient%2Fcomponents%2Fsegment-cache-impl%2Fcache-map.ts",
            "raw_url": "https://github.com/vercel/next.js/raw/61b285e52842e3a458b5efb442f71123d8813ae8/packages%2Fnext%2Fsrc%2Fclient%2Fcomponents%2Fsegment-cache-impl%2Fcache-map.ts",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/packages%2Fnext%2Fsrc%2Fclient%2Fcomponents%2Fsegment-cache-impl%2Fcache-map.ts?ref=61b285e52842e3a458b5efb442f71123d8813ae8",
            "patch": "@@ -0,0 +1,472 @@\n+import { lruPut, updateLruSize, deleteFromLru } from './lru'\n+\n+/**\n+ * A specialized data type for storing multi-key cache entries.\n+ *\n+ * The basic structure is a map whose keys are tuples, called the keypath.\n+ * When querying the cache, keypaths are compared per-element.\n+ *\n+ * Example:\n+ *   set(map, ['https://localhost', 'foo/bar/baz'], 'yay');\n+ *   get(map, ['https://localhost', 'foo/bar/baz']) -> 'yay'\n+ *\n+ * The parts of the keypath represent the different inputs that contribute\n+ * to the entry value. To illustrate, if you were to use this data type to store\n+ * HTTP responses, the keypath would include the URL and everything listed by\n+ * the Vary header.\n+ *\n+ * The order of elements in a keypath must be consistent between lookups to\n+ * be considered the same, but besides that, the order of the keys is not\n+ * semantically meaningful.\n+ *\n+ * Keypaths may include a special kind of key called Fallback. When an entry is\n+ * stored with Fallback as part of its keypath, it means that the entry does not\n+ * vary by that key. When querying the cache, if an exact match is not found for\n+ * a keypath, the cache will check for a Fallback match instead. Each element of\n+ * the keypath may have a Fallback, so retrieval is an O(n ^ 2) operation, but\n+ * it's expected that keypaths are relatively short.\n+ *\n+ * Example:\n+ *   set(cacheMap, ['store', 'product', 1], PRODUCT_PAGE_1);\n+ *   set(cacheMap, ['store', 'product', Fallback], GENERIC_PRODUCT_PAGE);\n+ *\n+ *   // Exact match\n+ *   get(cacheMap, ['store', 'product', 1]) -> PRODUCT_PAGE_1\n+ *\n+ *   // Fallback match\n+ *   get(cacheMap, ['store', 'product', 2]) -> GENERIC_PRODUCT_PAGE\n+ *\n+ * Because we have the Fallback mechanism, we can impose a constraint that\n+ * regular JS maps do not have: a value cannot be stored at multiple keypaths\n+ * simultaneously. These cases should be expressed with Fallback keys instead.\n+ *\n+ * Additionally, because values only exist at a single keypath at a time, we can\n+ * optimize successive lookups by caching the internal map entry on the value\n+ * itself, using the `ref` field. This is especially useful because it lets us\n+ * skip the O(n ^ 2) lookup that occurs when Fallback entries are present.\n+ *\n+\n+ * How to decide if stuff belongs in here, or in cache.ts?\n+ * -------------------------------------------------------\n+ * \n+ * Anything to do with retrival, lifetimes, or eviction needs to go in this\n+ * module because it affects the fallback algorithm. For example, when\n+ * performing a lookup, if an entry is stale, it needs to be treated as\n+ * semantically equivalent to if the entry was not present at all.\n+ * \n+ * If there's logic that's not related to the fallback algorithm, though, we\n+ * should prefer to put it in cache.ts.\n+ */\n+\n+type MapEntryShared<K extends readonly unknown[], V extends MapValue> = {\n+  parent: MapEntry<K, V> | null\n+  key: any\n+  map: Map<any, MapEntry<K, V>> | null\n+\n+  // LRU-related fields\n+  prev: MapEntry<any, any> | null\n+  next: MapEntry<any, any> | null\n+  size: number\n+}\n+\n+type EmptyMapEntry<\n+  K extends readonly unknown[],\n+  V extends MapValue,\n+> = MapEntryShared<K, V> & {\n+  value: null\n+}\n+\n+type FullMapEntry<\n+  K extends readonly unknown[],\n+  V extends MapValue,\n+> = MapEntryShared<K, V> & {\n+  value: V\n+}\n+\n+export type MapEntry<K extends readonly unknown[], V extends MapValue> =\n+  | EmptyMapEntry<K, V>\n+  | FullMapEntry<K, V>\n+\n+// The CacheMap type is just the root entry of the map.\n+export type CacheMap<\n+  K extends readonly unknown[],\n+  V extends MapValue,\n+> = MapEntry<K, V>\n+\n+// The protocol that values must implement. In practice, the only two types that\n+// we ever actually deal with in this module are RouteCacheEntry and\n+// SegmentCacheEntry; this is just to keep track of the coupling so we don't\n+// leak concerns between the modules unnecessarily.\n+export interface MapValue {\n+  ref: MapEntry<any, any> | null\n+  size: number\n+  staleAt: number\n+  version: number\n+}\n+\n+type KeyWithFallback<K extends readonly unknown[]> = {\n+  [I in keyof K]: K[I] | FallbackType\n+}\n+\n+export type FallbackType = { __brand: 'Fallback' }\n+export const Fallback = {} as FallbackType\n+\n+// This is a special internal key that is used for \"revalidation\" entries. It's\n+// an implementation detail that shouldn't leak outside of this module.\n+const Revalidation = {}\n+\n+export function createCacheMap<\n+  Keypath extends Array<any>,\n+  V extends MapValue,\n+>(): CacheMap<Keypath, V> {\n+  let cacheMap: MapEntry<Keypath, V> = {\n+    parent: null,\n+    key: null,\n+    value: null,\n+    map: null,\n+\n+    // LRU-related fields\n+    prev: null,\n+    next: null,\n+    size: 0,\n+  }\n+  return cacheMap\n+}\n+\n+function getOrInitialize<K extends readonly unknown[], V extends MapValue>(\n+  cacheMap: CacheMap<K, V>,\n+  keys: K,\n+  isRevalidation: boolean\n+): MapEntry<K, V> {\n+  // Go through each level of keys until we find the entry that matches, or\n+  // create a new entry if one doesn't exist.\n+  //\n+  // This function will only return entries that match the keypath _exactly_.\n+  // Unlike getWithFallback, it will not access fallback entries unless it's\n+  // explicitly part of the keypath.\n+  let entry = cacheMap\n+  let i = 0\n+  while (true) {\n+    let key\n+    if (i < keys.length) {\n+      key = keys[i]\n+    } else if (isRevalidation && i === keys.length) {\n+      // During a revalidation, we append an internal \"Revalidation\" key to\n+      // the end of the keypath. The \"normal\" entry is its parent.\n+\n+      // However, if the parent entry is currently empty, we don't need to store\n+      // this as a revalidation entry. Just insert the revalidation into the\n+      // normal slot.\n+      if (entry.value === null) {\n+        return entry\n+      }\n+\n+      // Otheriwse, create a child entry.\n+      key = Revalidation\n+    } else {\n+      // There are no more keys. This is the terminal entry.\n+      break\n+    }\n+    i++\n+\n+    let map = entry.map\n+    if (map !== null) {\n+      const existingEntry = map.get(key)\n+      if (existingEntry !== undefined) {\n+        // Found a match. Keep going.\n+        entry = existingEntry\n+        continue\n+      }\n+    } else {\n+      map = new Map()\n+      entry.map = map\n+    }\n+    // No entry exists yet at this level. Create a new one.\n+    const newEntry: EmptyMapEntry<K, V> = {\n+      parent: entry,\n+      key,\n+      value: null,\n+      map: null,\n+\n+      // LRU-related fields\n+      prev: null,\n+      next: null,\n+      size: 0,\n+    }\n+    map.set(key, newEntry)\n+    entry = newEntry\n+  }\n+\n+  return entry\n+}\n+\n+export function getFromCacheMap<\n+  K extends readonly unknown[],\n+  V extends MapValue,\n+>(\n+  now: number,\n+  currentCacheVersion: number,\n+  rootEntry: CacheMap<K, V>,\n+  keys: KeyWithFallback<K>,\n+  isRevalidation: boolean\n+): V | null {\n+  const entry = getEntryWithFallbackImpl(\n+    now,\n+    currentCacheVersion,\n+    rootEntry,\n+    keys,\n+    isRevalidation,\n+    0\n+  )\n+  if (entry === null || entry.value === null) {\n+    return null\n+  }\n+  // This is an LRU access. Move the entry to the front of the list.\n+  lruPut(entry)\n+  return entry.value\n+}\n+\n+export function isValueExpired<V extends MapValue>(\n+  now: number,\n+  currentCacheVersion: number,\n+  value: V\n+): boolean {\n+  return value.staleAt <= now || value.version < currentCacheVersion\n+}\n+\n+function lazilyEvictIfNeeded<K extends readonly unknown[], V extends MapValue>(\n+  now: number,\n+  currentCacheVersion: number,\n+  entry: MapEntry<K, V>\n+) {\n+  // We have a matching entry, but before we can return it, we need to check if\n+  // it's still fresh. Otherwise it should be treated the same as a cache miss.\n+\n+  if (entry.value === null) {\n+    // This entry has no value, so there's nothing to evict.\n+    return entry\n+  }\n+\n+  const value = entry.value\n+  if (isValueExpired(now, currentCacheVersion, value)) {\n+    // The value expired. Lazily evict it from the cache, and return null. This\n+    // is conceptually the same as a cache miss.\n+    deleteMapEntry(entry)\n+    return null\n+  }\n+\n+  // The matched entry has not expired. Return it.\n+  return entry\n+}\n+\n+function getEntryWithFallbackImpl<\n+  K extends readonly unknown[],\n+  V extends MapValue,\n+>(\n+  now: number,\n+  currentCacheVersion: number,\n+  entry: MapEntry<K, V>,\n+  keys: K,\n+  isRevalidation: boolean,\n+  index: number\n+): MapEntry<K, V> | null {\n+  // This is similar to getExactEntry, but if an exact match is not found for\n+  // a key, it will return the fallback entry instead. This is recursive at\n+  // every level, e.g. an entry with keypath [a, Fallback, c, Fallback] is\n+  // valid match for [a, b, c, d].\n+  //\n+  // It will return the most specific match available.\n+  let key\n+  if (index < keys.length) {\n+    key = keys[index]\n+  } else if (isRevalidation && index === keys.length) {\n+    // During a revalidation, we append an internal \"Revalidation\" key to\n+    // the end of the keypath.\n+    key = Revalidation\n+  } else {\n+    // There are no more keys. This is the terminal entry.\n+\n+    // TODO: When performing a lookup during a navigation, as opposed to a\n+    // prefetch, we may want to skip entries that are Pending if there's also\n+    // a Fulfilled fallback entry. Tricky to say, though, since if it's\n+    // already pending, it's likely to stream in soon. Maybe we could do this\n+    // just on slow connections and offline mode.\n+\n+    return lazilyEvictIfNeeded(now, currentCacheVersion, entry)\n+  }\n+  const map = entry.map\n+  if (map !== null) {\n+    const existingEntry = map.get(key)\n+    if (existingEntry !== undefined) {\n+      // Found an exact match for this key. Keep searching.\n+      const result = getEntryWithFallbackImpl<K, V>(\n+        now,\n+        currentCacheVersion,\n+        existingEntry,\n+        keys,\n+        isRevalidation,\n+        index + 1\n+      )\n+      if (result !== null) {\n+        return result\n+      }\n+    }\n+    // No match found for this key. Check if there's a fallback.\n+    const fallbackEntry = map.get(Fallback)\n+    if (fallbackEntry !== undefined) {\n+      // Found a fallback for this key. Keep searching.\n+      return getEntryWithFallbackImpl(\n+        now,\n+        currentCacheVersion,\n+        fallbackEntry,\n+        keys,\n+        isRevalidation,\n+        index + 1\n+      )\n+    }\n+  }\n+  return null\n+}\n+\n+export function setInCacheMap<K extends readonly unknown[], V extends MapValue>(\n+  cacheMap: CacheMap<K, V>,\n+  keys: K,\n+  value: V,\n+  isRevalidation: boolean\n+): void {\n+  // Add a value to the map at the given keypath. If the value is already\n+  // part of the map, it's removed from its previous keypath. (NOTE: This is\n+  // unlike a regular JS map, but the behavior is intentional.)\n+  const entry = getOrInitialize(cacheMap, keys, isRevalidation)\n+  setMapEntryValue(entry, value)\n+\n+  // This is an LRU access. Move the entry to the front of the list.\n+  lruPut(entry)\n+  updateLruSize(entry, value.size)\n+}\n+\n+function setMapEntryValue<K extends readonly unknown[], V extends MapValue>(\n+  entry: MapEntry<K, V>,\n+  value: V\n+): void {\n+  if (entry.value !== null) {\n+    // There's already a value at the given keypath. Disconnect the old value\n+    // from the map. We're not calling `deleteMapEntry` here because the\n+    // entry itself is still in the map. We just want to overwrite its value.\n+    dropRef(entry.value)\n+\n+    // Fill the entry with the updated value.\n+    const emptyEntry: EmptyMapEntry<K, V> = entry as any\n+    emptyEntry.value = null\n+    fillEmptyReference(emptyEntry, value)\n+  } else {\n+    fillEmptyReference(entry as any, value)\n+  }\n+}\n+\n+function fillEmptyReference<K extends readonly unknown[], V extends MapValue>(\n+  entry: EmptyMapEntry<K, V>,\n+  value: V\n+): void {\n+  // This value may already be in the map at a different keypath.\n+  // Grab a reference before we overwrite it.\n+  const oldEntry = value.ref\n+\n+  const fullEntry: FullMapEntry<K, V> = entry as any\n+  fullEntry.value = value\n+  value.ref = fullEntry\n+\n+  updateLruSize(fullEntry, value.size)\n+\n+  if (oldEntry !== null && oldEntry !== entry && oldEntry.value === value) {\n+    // This value is already in the map at a different keypath in the map.\n+    // Values only exist at a single keypath at a time. Remove it from the\n+    // previous keypath.\n+    //\n+    // Note that only the internal map entry is garbage collected; we don't\n+    // call `dropRef` here because it's still in the map, just\n+    // at a new keypath (the one we just set, above).\n+    deleteMapEntry(oldEntry)\n+  }\n+}\n+\n+export function deleteFromCacheMap<V extends MapValue>(value: V): void {\n+  const entry = value.ref\n+  if (entry === null) {\n+    // This value is not a member of any map.\n+    return\n+  }\n+\n+  dropRef(value)\n+  deleteMapEntry(entry)\n+}\n+\n+function dropRef<V extends MapValue>(value: V): void {\n+  // Drop the value from the map by setting its `ref` backpointer to\n+  // null. This is a separate operation from `deleteMapEntry` because when\n+  // re-keying a value we need to be able to delete the old, internal map\n+  // entry without garbage collecting the value itself.\n+  value.ref = null\n+}\n+\n+function deleteMapEntry<K extends readonly unknown[], V extends MapValue>(\n+  entry: MapEntry<K, V>\n+): void {\n+  // Delete the entry from the cache.\n+  const emptyEntry: EmptyMapEntry<K, V> = entry as any\n+  emptyEntry.value = null\n+\n+  deleteFromLru(entry)\n+\n+  // Check if we can garbage collect the entry.\n+  const map = emptyEntry.map\n+  if (map === null) {\n+    // Since this entry has no value, and also no child entries, we can\n+    // garbage collect it. Remove it from its parent, and keep garbage\n+    // collecting the parents until we reach a non-empty entry.\n+    let parent = emptyEntry.parent\n+    let key = emptyEntry.key\n+    while (parent !== null) {\n+      const parentMap = parent.map\n+      if (parentMap !== null) {\n+        parentMap.delete(key)\n+        if (parentMap.size === 0) {\n+          // We just removed the last entry in the parent map.\n+          parent.map = null\n+          if (parent.value === null) {\n+            // The parent node has no child entries, nor does it have a value\n+            // on itself. It can be garbage collected. Keep going.\n+            key = parent.key\n+            parent = parent.parent\n+            continue\n+          }\n+        }\n+      }\n+      // The parent is not empty. Stop garbage collecting.\n+      break\n+    }\n+  } else {\n+    // Check if there's a revalidating entry. If so, promote it to a\n+    // \"normal\" entry, since the normal one was just deleted.\n+    const revalidatingEntry = map.get(Revalidation)\n+    if (revalidatingEntry !== undefined && revalidatingEntry.value !== null) {\n+      setMapEntryValue(emptyEntry, revalidatingEntry.value)\n+    }\n+  }\n+}\n+\n+export function setSizeInCacheMap<V extends MapValue>(\n+  value: V,\n+  size: number\n+): void {\n+  const entry = value.ref\n+  if (entry === null) {\n+    // This value is not a member of any map.\n+    return\n+  }\n+  // Except during initialization (when the size is set to 0), this is the only\n+  // place the `size` field should be updated, to ensure it's in sync with the\n+  // the LRU.\n+  value.size = size\n+  updateLruSize(entry, size)\n+}"
        },
        {
            "sha": "63558304748f73a2038e4f6a62db5260589214f1",
            "filename": "packages/next/src/client/components/segment-cache-impl/cache.ts",
            "status": "modified",
            "additions": 241,
            "deletions": 340,
            "changes": 581,
            "blob_url": "https://github.com/vercel/next.js/blob/61b285e52842e3a458b5efb442f71123d8813ae8/packages%2Fnext%2Fsrc%2Fclient%2Fcomponents%2Fsegment-cache-impl%2Fcache.ts",
            "raw_url": "https://github.com/vercel/next.js/raw/61b285e52842e3a458b5efb442f71123d8813ae8/packages%2Fnext%2Fsrc%2Fclient%2Fcomponents%2Fsegment-cache-impl%2Fcache.ts",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/packages%2Fnext%2Fsrc%2Fclient%2Fcomponents%2Fsegment-cache-impl%2Fcache.ts?ref=61b285e52842e3a458b5efb442f71123d8813ae8",
            "patch": "@@ -39,7 +39,7 @@ import {\n import { getAppBuildId } from '../../app-build-id'\n import { createHrefFromUrl } from '../router-reducer/create-href-from-url'\n import type {\n-  NormalizedHref,\n+  NormalizedPathname,\n   NormalizedNextUrl,\n   NormalizedSearch,\n   RouteCacheKey,\n@@ -55,8 +55,18 @@ import {\n   parseDynamicParamFromURLPart,\n   type RouteParam,\n } from '../../route-params'\n-import { createTupleMap, type TupleMap, type Prefix } from './tuple-map'\n-import { createLRU } from './lru'\n+import {\n+  createCacheMap,\n+  getFromCacheMap,\n+  setInCacheMap,\n+  setSizeInCacheMap,\n+  deleteFromCacheMap,\n+  isValueExpired,\n+  Fallback,\n+  type CacheMap,\n+  type MapEntry,\n+  type FallbackType,\n+} from './cache-map'\n import {\n   appendSegmentCacheKeyPart,\n   appendSegmentRequestKeyPart,\n@@ -134,7 +144,6 @@ export type RouteTree = {\n }\n \n type RouteCacheEntryShared = {\n-  staleAt: number\n   // This is false only if we're certain the route cannot be intercepted. It's\n   // true in all other cases, including on initialization when we haven't yet\n   // received a response from the server.\n@@ -144,11 +153,11 @@ type RouteCacheEntryShared = {\n   TODO_metadataStatus: EntryStatus.Empty | EntryStatus.Fulfilled\n   TODO_isHeadDynamic: boolean\n \n-  // LRU-related fields\n-  keypath: null | Prefix<RouteCacheKeypath>\n-  next: null | RouteCacheEntry\n-  prev: null | RouteCacheEntry\n+  // Map-related fields.\n+  ref: null | MapEntry<RouteCacheKeypath, RouteCacheEntry>\n   size: number\n+  staleAt: number\n+  version: number\n }\n \n /**\n@@ -202,15 +211,13 @@ export type RouteCacheEntry =\n   | RejectedRouteCacheEntry\n \n type SegmentCacheEntryShared = {\n-  staleAt: number\n   fetchStrategy: FetchStrategy\n-  revalidating: SegmentCacheEntry | null\n \n-  // LRU-related fields\n-  keypath: null | Prefix<SegmentCacheKeypath>\n-  next: null | SegmentCacheEntry\n-  prev: null | SegmentCacheEntry\n+  // Map-related fields.\n+  ref: null | MapEntry<SegmentCacheKeypath, SegmentCacheEntry>\n   size: number\n+  staleAt: number\n+  version: number\n }\n \n export type EmptySegmentCacheEntry = SegmentCacheEntryShared & {\n@@ -273,33 +280,18 @@ function getStaleTimeMs(staleTimeSeconds: number): number {\n // concatenate the keys into a single key, we use a multi-level map, where the\n // first level is keyed by href, the second level is keyed by Next-Url, and so\n // on (if were to add more levels).\n-type RouteCacheKeypath = [NormalizedHref, NormalizedNextUrl]\n-let routeCacheMap: TupleMap<RouteCacheKeypath, RouteCacheEntry> =\n-  createTupleMap()\n-\n-// We use an LRU for memory management. We must update this whenever we add or\n-// remove a new cache entry, or when an entry changes size.\n-// TODO: I chose the max size somewhat arbitrarily. Consider setting this based\n-// on navigator.deviceMemory, or some other heuristic. We should make this\n-// customizable via the Next.js config, too.\n-const maxRouteLruSize = 10 * 1024 * 1024 // 10 MB\n-let routeCacheLru = createLRU<RouteCacheEntry>(\n-  maxRouteLruSize,\n-  onRouteLRUEviction\n-)\n-\n-type SegmentCacheKeypath = [string, NormalizedSearch]\n-let segmentCacheMap: TupleMap<SegmentCacheKeypath, SegmentCacheEntry> =\n-  createTupleMap()\n-// NOTE: Segments and Route entries are managed by separate LRUs. We could\n-// combine them into a single LRU, but because they are separate types, we'd\n-// need to wrap each one in an extra LRU node (to maintain monomorphism, at the\n-// cost of additional memory).\n-const maxSegmentLruSize = 50 * 1024 * 1024 // 50 MB\n-let segmentCacheLru = createLRU<SegmentCacheEntry>(\n-  maxSegmentLruSize,\n-  onSegmentLRUEviction\n-)\n+type RouteCacheKeypath = [\n+  NormalizedPathname,\n+  NormalizedSearch,\n+  NormalizedNextUrl | null | FallbackType,\n+]\n+let routeCacheMap: CacheMap<RouteCacheKeypath, RouteCacheEntry> =\n+  createCacheMap()\n+\n+export type SegmentCacheKeypath = [string, NormalizedSearch | FallbackType]\n+\n+let segmentCacheMap: CacheMap<SegmentCacheKeypath, SegmentCacheEntry> =\n+  createCacheMap()\n \n // All invalidation listeners for the whole cache are tracked in single set.\n // Since we don't yet support tag or path-based invalidation, there's no point\n@@ -326,21 +318,17 @@ export function revalidateEntireCache(\n   nextUrl: string | null,\n   tree: FlightRouterState\n ) {\n+  // Increment the current cache version. This does not eagerly evict anything\n+  // from the cache, but because all the entries are versioned, and we check\n+  // the version when reading from the cache, this effectively causes all\n+  // entries to be evicted lazily. We do it lazily because in the future,\n+  // actions like revalidateTag or refresh will not evict the entire cache,\n+  // but rather some subset of the entries.\n   currentCacheVersion++\n \n   // Start a cooldown before re-prefetching to allow CDN cache propagation.\n   startRevalidationCooldown()\n \n-  // Clearing the cache also effectively rejects any pending requests, because\n-  // when the response is received, it gets written into a cache entry that is\n-  // no longer reachable.\n-  // TODO: There's an exception to this case that we don't currently handle\n-  // correctly: background revalidations. See note in `upsertSegmentEntry`.\n-  routeCacheMap = createTupleMap()\n-  routeCacheLru = createLRU(maxRouteLruSize, onRouteLRUEviction)\n-  segmentCacheMap = createTupleMap()\n-  segmentCacheLru = createLRU(maxSegmentLruSize, onSegmentLRUEviction)\n-\n   // Prefetch all the currently visible links again, to re-fill the cache.\n   pingVisibleLinks(nextUrl, tree)\n \n@@ -404,154 +392,93 @@ export function pingInvalidationListeners(\n   }\n }\n \n-export function readExactRouteCacheEntry(\n-  now: number,\n-  href: NormalizedHref,\n-  nextUrl: NormalizedNextUrl | null\n-): RouteCacheEntry | null {\n-  const keypath: Prefix<RouteCacheKeypath> =\n-    nextUrl === null ? [href] : [href, nextUrl]\n-  const existingEntry = routeCacheMap.get(keypath)\n-  if (existingEntry !== null) {\n-    // Check if the entry is stale\n-    if (existingEntry.staleAt > now) {\n-      // Reuse the existing entry.\n-\n-      // Since this is an access, move the entry to the front of the LRU.\n-      routeCacheLru.put(existingEntry)\n-\n-      return existingEntry\n-    } else {\n-      // Evict the stale entry from the cache.\n-      deleteRouteFromCache(existingEntry, keypath)\n-    }\n-  }\n-  return null\n-}\n-\n export function readRouteCacheEntry(\n   now: number,\n   key: RouteCacheKey\n ): RouteCacheEntry | null {\n-  // First check if there's a non-intercepted entry. Most routes cannot be\n-  // intercepted, so this is the common case.\n-  const nonInterceptedEntry = readExactRouteCacheEntry(now, key.href, null)\n-  if (nonInterceptedEntry !== null && !nonInterceptedEntry.couldBeIntercepted) {\n-    // Found a match, and the route cannot be intercepted. We can reuse it.\n-    return nonInterceptedEntry\n-  }\n-  // There was no match. Check again but include the Next-Url this time.\n-  return readExactRouteCacheEntry(now, key.href, key.nextUrl)\n+  const keypath: RouteCacheKeypath = [key.pathname, key.search, key.nextUrl]\n+  const isRevalidation = false\n+  return getFromCacheMap(\n+    now,\n+    getCurrentCacheVersion(),\n+    routeCacheMap,\n+    keypath,\n+    isRevalidation\n+  )\n }\n \n-export function getSegmentKeypath(\n-  fetchStrategy: FetchStrategy,\n+export function getCanonicalSegmentKeypath(\n   route: FulfilledRouteCacheEntry,\n   cacheKey: SegmentCacheKey\n-): Prefix<SegmentCacheKeypath> {\n-  // When a prefetch includes dynamic data, the search params are included\n-  // in the result, so we must include the search string in the segment\n-  // cache key. (Note that this is true even if the search string is empty.)\n-  //\n-  // If we're fetching using PPR, we do not need to include the search params in\n-  // the cache key, because the search params are treated as dynamic data. The\n-  // cache entry is valid for all possible search param values.\n-  const isDynamic =\n-    fetchStrategy === FetchStrategy.Full ||\n-    fetchStrategy === FetchStrategy.PPRRuntime ||\n-    !route.isPPREnabled\n-  return isDynamic && cacheKey.endsWith('/' + PAGE_SEGMENT_KEY)\n-    ? [cacheKey, route.renderedSearch]\n-    : [cacheKey]\n+): SegmentCacheKeypath {\n+  // Returns the actual keypath for a segment, without omitting any params.\n+  return [\n+    cacheKey,\n+    cacheKey.endsWith('/' + PAGE_SEGMENT_KEY)\n+      ? route.renderedSearch\n+      : // Non-page segments never contain search params, so there's no reason\n+        // to include it in the keypath.\n+        Fallback,\n+  ]\n }\n \n-export function readSegmentCacheEntry(\n-  now: number,\n+export function getGenericSegmentKeypathFromFetchStrategy(\n+  fetchStrategy: FetchStrategy,\n   route: FulfilledRouteCacheEntry,\n   cacheKey: SegmentCacheKey\n-): SegmentCacheEntry | null {\n-  if (!cacheKey.endsWith('/' + PAGE_SEGMENT_KEY)) {\n-    // Fast path. Search params only exist on page segments.\n-    return readExactSegmentCacheEntry(now, [cacheKey])\n-  }\n-\n-  const renderedSearch = route.renderedSearch\n-  if (renderedSearch !== null) {\n-    // Page segments may or may not contain search params. If they were prefetched\n-    // using a dynamic request, then we will have an entry with search params.\n-    // Check for that case first.\n-    const entryWithSearchParams = readExactSegmentCacheEntry(now, [\n-      cacheKey,\n-      renderedSearch,\n-    ])\n-    if (entryWithSearchParams !== null) {\n-      return entryWithSearchParams\n-    }\n-  }\n-\n-  // If we did not find an entry with the given search params, check for a\n-  // \"fallback\" entry, where the search params are treated as dynamic data. This\n-  // is the common case because PPR/static prerenders always treat search params\n-  // as dynamic.\n+): SegmentCacheKeypath {\n+  // Returns the most generic possible keypath for a segment, based on the\n+  // strategy used to fetch it, i.e. static/PPR versus runtime prefetching.\n   //\n-  // See corresponding logic in `getSegmentKeypath`.\n-  const entryWithoutSearchParams = readExactSegmentCacheEntry(now, [cacheKey])\n-  return entryWithoutSearchParams\n+  // This is used when _writing_ to the cache. We want to choose the most\n+  // generic keypath so that it can be reused as much as possible.\n+  //\n+  // We may be able to re-key the response to something even more generic once\n+  // we receive it — for example, if the server tells us that the response\n+  // doesn't vary on a particular param — but even before we send the request,\n+  // we know somethings based on the fetch strategy alone.\n+  const doesVaryOnSearchParams =\n+    // Non-page segments never include search params\n+    cacheKey.endsWith('/' + PAGE_SEGMENT_KEY) &&\n+    // Only a runtime prefetch will include search params in the result. Static\n+    // prefetches never include search params, so they can be reused across all\n+    // possible search param values.\n+    (fetchStrategy === FetchStrategy.Full ||\n+      fetchStrategy === FetchStrategy.PPRRuntime)\n+  const keypath: SegmentCacheKeypath = [\n+    cacheKey,\n+\n+    doesVaryOnSearchParams ? route.renderedSearch : Fallback,\n+  ]\n+  return keypath\n }\n \n-function readExactSegmentCacheEntry(\n+export function readSegmentCacheEntry(\n   now: number,\n-  keypath: Prefix<SegmentCacheKeypath>\n+  keypath: SegmentCacheKeypath\n ): SegmentCacheEntry | null {\n-  const existingEntry = segmentCacheMap.get(keypath)\n-  if (existingEntry !== null) {\n-    // Check if the entry is stale\n-    if (existingEntry.staleAt > now) {\n-      // Reuse the existing entry.\n-\n-      // Since this is an access, move the entry to the front of the LRU.\n-      segmentCacheLru.put(existingEntry)\n-\n-      return existingEntry\n-    } else {\n-      // This is a stale entry.\n-      const revalidatingEntry = existingEntry.revalidating\n-      if (revalidatingEntry !== null) {\n-        // There's a revalidation in progress. Upsert it.\n-        const upsertedEntry = upsertSegmentEntry(\n-          now,\n-          keypath,\n-          revalidatingEntry\n-        )\n-        if (upsertedEntry !== null && upsertedEntry.staleAt > now) {\n-          // We can use the upserted revalidation entry.\n-          return upsertedEntry\n-        }\n-      } else {\n-        // Evict the stale entry from the cache.\n-        deleteSegmentFromCache(existingEntry, keypath)\n-      }\n-    }\n-  }\n-  return null\n+  const isRevalidation = false\n+  return getFromCacheMap(\n+    now,\n+    getCurrentCacheVersion(),\n+    segmentCacheMap,\n+    keypath,\n+    isRevalidation\n+  )\n }\n \n function readRevalidatingSegmentCacheEntry(\n   now: number,\n-  owner: SegmentCacheEntry\n+  keypath: SegmentCacheKeypath\n ): SegmentCacheEntry | null {\n-  const existingRevalidation = owner.revalidating\n-  if (existingRevalidation !== null) {\n-    if (existingRevalidation.staleAt > now) {\n-      // There's already a revalidation in progress. Or a previous revalidation\n-      // failed and it has not yet expired.\n-      return existingRevalidation\n-    } else {\n-      // Clear the stale revalidation from its owner.\n-      clearRevalidatingSegmentFromOwner(owner)\n-    }\n-  }\n-  return null\n+  const isRevalidation = true\n+  return getFromCacheMap(\n+    now,\n+    getCurrentCacheVersion(),\n+    segmentCacheMap,\n+    keypath,\n+    isRevalidation\n+  )\n }\n \n export function waitForSegmentCacheEntry(\n@@ -592,9 +519,6 @@ export function readOrCreateRouteCacheEntry(\n     tree: null,\n     head: null,\n     isHeadPartial: true,\n-    // Since this is an empty entry, there's no reason to ever evict it. It will\n-    // be updated when the data is populated.\n-    staleAt: Infinity,\n     // This is initialized to true because we don't know yet whether the route\n     // could be intercepted. It's only set to false once we receive a response\n     // from the server.\n@@ -606,19 +530,17 @@ export function readOrCreateRouteCacheEntry(\n     TODO_metadataStatus: EntryStatus.Empty,\n     TODO_isHeadDynamic: false,\n \n-    // LRU-related fields\n-    keypath: null,\n-    next: null,\n-    prev: null,\n+    // Map-related fields\n+    ref: null,\n     size: 0,\n+    // Since this is an empty entry, there's no reason to ever evict it. It will\n+    // be updated when the data is populated.\n+    staleAt: Infinity,\n+    version: getCurrentCacheVersion(),\n   }\n-  const keypath: Prefix<RouteCacheKeypath> =\n-    key.nextUrl === null ? [key.href] : [key.href, key.nextUrl]\n-  routeCacheMap.set(keypath, pendingEntry)\n-  // Stash the keypath on the entry so we know how to remove it from the map\n-  // if it gets evicted from the LRU.\n-  pendingEntry.keypath = keypath\n-  routeCacheLru.put(pendingEntry)\n+  const keypath: RouteCacheKeypath = [key.pathname, key.search, key.nextUrl]\n+  const isRevalidation = false\n+  setInCacheMap(routeCacheMap, keypath, pendingEntry, isRevalidation)\n   return pendingEntry\n }\n \n@@ -748,7 +670,6 @@ export function requestOptimisticRouteCacheEntry(\n     tree: routeWithNoSearchParams.tree,\n     head,\n     isHeadPartial,\n-    staleAt: routeWithNoSearchParams.staleAt,\n     couldBeIntercepted: routeWithNoSearchParams.couldBeIntercepted,\n     isPPREnabled: routeWithNoSearchParams.isPPREnabled,\n \n@@ -758,11 +679,11 @@ export function requestOptimisticRouteCacheEntry(\n     TODO_metadataStatus,\n     TODO_isHeadDynamic,\n \n-    // LRU-related fields\n-    keypath: null,\n-    next: null,\n-    prev: null,\n+    // Map-related fields\n+    ref: null,\n     size: 0,\n+    staleAt: routeWithNoSearchParams.staleAt,\n+    version: routeWithNoSearchParams.version,\n   }\n \n   // Do not insert this entry into the cache. It only exists so we can\n@@ -780,49 +701,95 @@ export function readOrCreateSegmentCacheEntry(\n   route: FulfilledRouteCacheEntry,\n   cacheKey: SegmentCacheKey\n ): SegmentCacheEntry {\n-  const keypath = getSegmentKeypath(fetchStrategy, route, cacheKey)\n-  const existingEntry = readExactSegmentCacheEntry(now, keypath)\n+  const canonicalKeypath = getCanonicalSegmentKeypath(route, cacheKey)\n+  const existingEntry = readSegmentCacheEntry(now, canonicalKeypath)\n   if (existingEntry !== null) {\n     return existingEntry\n   }\n   // Create a pending entry and add it to the cache.\n+  const genericKeypath = getGenericSegmentKeypathFromFetchStrategy(\n+    fetchStrategy,\n+    route,\n+    cacheKey\n+  )\n   const pendingEntry = createDetachedSegmentCacheEntry(route.staleAt)\n-  segmentCacheMap.set(keypath, pendingEntry)\n-  // Stash the keypath on the entry so we know how to remove it from the map\n-  // if it gets evicted from the LRU.\n-  pendingEntry.keypath = keypath\n-  segmentCacheLru.put(pendingEntry)\n+  const isRevalidation = false\n+  setInCacheMap(segmentCacheMap, genericKeypath, pendingEntry, isRevalidation)\n   return pendingEntry\n }\n \n export function readOrCreateRevalidatingSegmentEntry(\n   now: number,\n-  prevEntry: SegmentCacheEntry\n+  fetchStrategy: FetchStrategy,\n+  route: FulfilledRouteCacheEntry,\n+  cacheKey: SegmentCacheKey\n ): SegmentCacheEntry {\n-  const existingRevalidation = readRevalidatingSegmentCacheEntry(now, prevEntry)\n-  if (existingRevalidation !== null) {\n-    return existingRevalidation\n-  }\n-  const pendingEntry = createDetachedSegmentCacheEntry(prevEntry.staleAt)\n-\n-  // Background revalidations are not stored directly in the cache map or LRU;\n-  // they're stashed on the entry that they will (potentially) replace.\n+  // This function is called when we've already confirmed that a particular\n+  // segment is cached, but we want to perform another request anyway in case it\n+  // returns more complete and/or fresher data than we already have. The logic\n+  // for deciding whether to replace the existing entry is handled elsewhere;\n+  // this function just handles retrieving a cache entry that we can use to\n+  // track the revalidation.\n   //\n-  // Note that we don't actually ever clear this field, except when the entry\n-  // expires. When the revalidation finishes, one of two things will happen:\n+  // The reason revalidations are stored in the cache is because we need to be\n+  // able to dedupe multiple revalidation requests. The reason they have to be\n+  // handled specially is because we shouldn't overwrite a \"normal\" entry if\n+  // one exists at the same keypath. So, for each internal cache location, there\n+  // is a special \"revalidation\" slot that is used solely for this purpose.\n   //\n-  //  1) the revalidation is successful, `prevEntry` is removed from the cache\n-  //     and garbage collected (so there's no point clearing any of its fields)\n-  //  2) the revalidation fails, and we'll use the `revalidating` field to\n-  //     prevent subsequent revalidation attempts, until it expires.\n-  prevEntry.revalidating = pendingEntry\n+  // You can think of it as if all the revalidation entries were stored in a\n+  // separate cache map from the canonical entries, and then transfered to the\n+  // canonical cache map once the request is complete — this isn't how it's\n+  // actually implemented, since it's more efficient to store them in the same\n+  // data structure as the normal entries, but that's how it's modeled\n+  // conceptually.\n+\n+  // TODO: Once we implement Fallback behavior for params, where an entry is\n+  // re-keyed based on response information, we'll need to account for the\n+  // possibility that the keypath of the previous entry is more generic than\n+  // the keypath of the revalidating entry. In other words, the server could\n+  // return a less generic entry upon revalidation. For now, though, this isn't\n+  // a concern because the keypath is based solely on the prefetch strategy,\n+  // not on data contained in the response.\n+  const canonicalKeypath = getCanonicalSegmentKeypath(route, cacheKey)\n+  const existingEntry = readRevalidatingSegmentCacheEntry(now, canonicalKeypath)\n+  if (existingEntry !== null) {\n+    return existingEntry\n+  }\n+  // Create a pending entry and add it to the cache.\n+  const genericKeypath = getGenericSegmentKeypathFromFetchStrategy(\n+    fetchStrategy,\n+    route,\n+    cacheKey\n+  )\n+  const pendingEntry = createDetachedSegmentCacheEntry(route.staleAt)\n+  const isRevalidation = true\n+  setInCacheMap(segmentCacheMap, genericKeypath, pendingEntry, isRevalidation)\n+  return pendingEntry\n+}\n \n+export function overwriteRevalidatingSegmentCacheEntry(\n+  fetchStrategy: FetchStrategy,\n+  route: FulfilledRouteCacheEntry,\n+  cacheKey: SegmentCacheKey\n+) {\n+  // This function is called when we've already decided to replace an existing\n+  // revalidation entry. Create a new entry and write it into the cache,\n+  // overwriting the previous value.\n+  const genericKeypath = getGenericSegmentKeypathFromFetchStrategy(\n+    fetchStrategy,\n+    route,\n+    cacheKey\n+  )\n+  const pendingEntry = createDetachedSegmentCacheEntry(route.staleAt)\n+  const isRevalidation = true\n+  setInCacheMap(segmentCacheMap, genericKeypath, pendingEntry, isRevalidation)\n   return pendingEntry\n }\n \n export function upsertSegmentEntry(\n   now: number,\n-  keypath: Prefix<SegmentCacheKeypath>,\n+  keypath: SegmentCacheKeypath,\n   candidateEntry: SegmentCacheEntry\n ): SegmentCacheEntry | null {\n   // We have a new entry that has not yet been inserted into the cache. Before\n@@ -831,7 +798,13 @@ export function upsertSegmentEntry(\n   // TODO: We should not upsert an entry if its key was invalidated in the time\n   // since the request was made. We can do that by passing the \"owner\" entry to\n   // this function and confirming it's the same as `existingEntry`.\n-  const existingEntry = readExactSegmentCacheEntry(now, keypath)\n+\n+  if (isValueExpired(now, getCurrentCacheVersion(), candidateEntry)) {\n+    // The entry is expired. We cannot upsert it.\n+    return null\n+  }\n+\n+  const existingEntry = readSegmentCacheEntry(now, keypath)\n   if (existingEntry !== null) {\n     // Don't replace a more specific segment with a less-specific one. A case where this\n     // might happen is if the existing segment was fetched via\n@@ -848,11 +821,11 @@ export function upsertSegmentEntry(\n       // (TODO: can this be true if `candidateEntry.fetchStrategy >= existingEntry.fetchStrategy`?)\n       (!existingEntry.isPartial && candidateEntry.isPartial)\n     ) {\n-      // We're going to leave the entry on the owner's `revalidating` field\n-      // so that it doesn't get revalidated again unnecessarily. Downgrade the\n-      // Fulfilled entry to Rejected and null out the data so it can be garbage\n-      // collected. We leave `staleAt` intact to prevent subsequent revalidation\n-      // attempts only until the entry expires.\n+      // We're going to leave revalidating entry in the cache so that it doesn't\n+      // get revalidated again unnecessarily. Downgrade the Fulfilled entry to\n+      // Rejected and null out the data so it can be garbage collected. We leave\n+      // `staleAt` intact to prevent subsequent revalidation attempts only until\n+      // the entry expires.\n       const rejectedEntry: RejectedSegmentCacheEntry = candidateEntry as any\n       rejectedEntry.status = EntryStatus.Rejected\n       rejectedEntry.loading = null\n@@ -861,13 +834,11 @@ export function upsertSegmentEntry(\n     }\n \n     // Evict the existing entry from the cache.\n-    deleteSegmentFromCache(existingEntry, keypath)\n+    deleteFromCacheMap(existingEntry)\n   }\n-  segmentCacheMap.set(keypath, candidateEntry)\n-  // Stash the keypath on the entry so we know how to remove it from the map\n-  // if it gets evicted from the LRU.\n-  candidateEntry.keypath = keypath\n-  segmentCacheLru.put(candidateEntry)\n+\n+  const isRevalidation = false\n+  setInCacheMap(segmentCacheMap, keypath, candidateEntry, isRevalidation)\n   return candidateEntry\n }\n \n@@ -879,18 +850,16 @@ export function createDetachedSegmentCacheEntry(\n     // Default to assuming the fetch strategy will be PPR. This will be updated\n     // when a fetch is actually initiated.\n     fetchStrategy: FetchStrategy.PPR,\n-    revalidating: null,\n     rsc: null,\n     loading: null,\n-    staleAt,\n     isPartial: true,\n     promise: null,\n \n-    // LRU-related fields\n-    keypath: null,\n-    next: null,\n-    prev: null,\n+    // Map-related fields\n+    ref: null,\n     size: 0,\n+    staleAt,\n+    version: 0,\n   }\n   return emptyEntry\n }\n@@ -902,81 +871,15 @@ export function upgradeToPendingSegment(\n   const pendingEntry: PendingSegmentCacheEntry = emptyEntry as any\n   pendingEntry.status = EntryStatus.Pending\n   pendingEntry.fetchStrategy = fetchStrategy\n+  // Set the version here, since this is right before the request is initiated.\n+  // The next time the global cache version is incremented, the entry will\n+  // effectively be evicted. This happens before initiating the request, rather\n+  // than when receiving the response, because it's guaranteed to happen\n+  // before the data is read on the server.\n+  pendingEntry.version = getCurrentCacheVersion()\n   return pendingEntry\n }\n \n-function deleteRouteFromCache(\n-  entry: RouteCacheEntry,\n-  keypath: Prefix<RouteCacheKeypath>\n-): void {\n-  pingBlockedTasks(entry)\n-  routeCacheMap.delete(keypath)\n-  routeCacheLru.delete(entry)\n-}\n-\n-function deleteSegmentFromCache(\n-  entry: SegmentCacheEntry,\n-  keypath: Prefix<SegmentCacheKeypath>\n-): void {\n-  cancelEntryListeners(entry)\n-  segmentCacheMap.delete(keypath)\n-  segmentCacheLru.delete(entry)\n-  clearRevalidatingSegmentFromOwner(entry)\n-}\n-\n-function clearRevalidatingSegmentFromOwner(owner: SegmentCacheEntry): void {\n-  // Revalidating segments are not stored in the cache directly; they're\n-  // stored as a field on the entry that they will (potentially) replace. So\n-  // to dispose of an existing revalidation, we just need to null out the field\n-  // on the owner.\n-  const revalidatingSegment = owner.revalidating\n-  if (revalidatingSegment !== null) {\n-    cancelEntryListeners(revalidatingSegment)\n-    owner.revalidating = null\n-  }\n-}\n-\n-export function resetRevalidatingSegmentEntry(\n-  owner: SegmentCacheEntry\n-): EmptySegmentCacheEntry {\n-  clearRevalidatingSegmentFromOwner(owner)\n-  const emptyEntry = createDetachedSegmentCacheEntry(owner.staleAt)\n-  owner.revalidating = emptyEntry\n-  return emptyEntry\n-}\n-\n-function onRouteLRUEviction(entry: RouteCacheEntry): void {\n-  // The LRU evicted this entry. Remove it from the map.\n-  const keypath = entry.keypath\n-  if (keypath !== null) {\n-    entry.keypath = null\n-    pingBlockedTasks(entry)\n-    routeCacheMap.delete(keypath)\n-  }\n-}\n-\n-function onSegmentLRUEviction(entry: SegmentCacheEntry): void {\n-  // The LRU evicted this entry. Remove it from the map.\n-  const keypath = entry.keypath\n-  if (keypath !== null) {\n-    entry.keypath = null\n-    cancelEntryListeners(entry)\n-    segmentCacheMap.delete(keypath)\n-  }\n-}\n-\n-function cancelEntryListeners(entry: SegmentCacheEntry): void {\n-  if (entry.status === EntryStatus.Pending && entry.promise !== null) {\n-    // There were listeners for this entry. Resolve them with `null` to indicate\n-    // that the prefetch failed. It's up to the listener to decide how to handle\n-    // this case.\n-    // NOTE: We don't currently propagate the reason the prefetch was canceled\n-    // but we could by accepting a `reason` argument.\n-    entry.promise.resolve(null)\n-    entry.promise = null\n-  }\n-}\n-\n function pingBlockedTasks(entry: {\n   blockedTasks: Set<PrefetchTask> | null\n }): void {\n@@ -1319,7 +1222,8 @@ export async function fetchRouteOnCacheMiss(\n   // fetch that gets issued on a cache miss. Notice it writes the result to the\n   // cache entry directly, rather than return data that is then written by\n   // the caller.\n-  const href = key.href\n+  const pathname = key.pathname\n+  const search = key.search\n   const nextUrl = key.nextUrl\n   const segmentPath = '/_tree' as SegmentRequestKey\n \n@@ -1333,6 +1237,7 @@ export async function fetchRouteOnCacheMiss(\n   }\n \n   try {\n+    const url = new URL(pathname + search, location.origin)\n     let response\n     let urlAfterRedirects\n     if (isOutputExportMode) {\n@@ -1363,8 +1268,7 @@ export async function fetchRouteOnCacheMiss(\n       // NOTE: We could embed the route tree into the HTML document, to avoid\n       // a second request. We're not doing that currently because it would make\n       // the HTML document larger and affect normal page loads.\n-      const url = new URL(href)\n-      const htmlResponse = await fetch(href, {\n+      const htmlResponse = await fetch(url, {\n         headers: {\n           Range: DOC_PREFETCH_RANGE_HEADER_VALUE,\n         },\n@@ -1388,7 +1292,6 @@ export async function fetchRouteOnCacheMiss(\n       // TODO: The eventual plan is to get rid of our custom request headers and\n       // encode everything into the URL, using a similar strategy to the\n       // \"output: export\" block above.\n-      const url = new URL(href)\n       response = await fetchPrefetchResponse(url, headers)\n       urlAfterRedirects =\n         response !== null && response.redirected ? new URL(response.url) : url\n@@ -1451,7 +1354,7 @@ export async function fetchRouteOnCacheMiss(\n         response.body,\n         closed.resolve,\n         function onResponseSizeUpdate(size) {\n-          routeCacheLru.updateSize(entry, size)\n+          setSizeInCacheMap(entry, size)\n         }\n       )\n       const serverData = await createFromNextReadableStream<RootTreePrefetch>(\n@@ -1503,7 +1406,7 @@ export async function fetchRouteOnCacheMiss(\n         response.body,\n         closed.resolve,\n         function onResponseSizeUpdate(size) {\n-          routeCacheLru.updateSize(entry, size)\n+          setSizeInCacheMap(entry, size)\n         }\n       )\n       const serverData =\n@@ -1537,28 +1440,21 @@ export async function fetchRouteOnCacheMiss(\n       )\n     }\n \n-    if (!couldBeIntercepted && nextUrl !== null) {\n+    if (!couldBeIntercepted) {\n       // This route will never be intercepted. So we can use this entry for all\n       // requests to this route, regardless of the Next-Url header. This works\n       // because when reading the cache we always check for a valid\n       // non-intercepted entry first.\n-      //\n-      // Re-key the entry. Since we're in an async task, we must first confirm\n-      // that the entry hasn't been concurrently modified by a different task.\n-      const currentKeypath: Prefix<RouteCacheKeypath> = [href, nextUrl]\n-      const expectedEntry = routeCacheMap.get(currentKeypath)\n-      if (expectedEntry === entry) {\n-        routeCacheMap.delete(currentKeypath)\n-        const newKeypath: Prefix<RouteCacheKeypath> = [href]\n-        routeCacheMap.set(newKeypath, entry)\n-        // We don't need to update the LRU because the entry is already in it.\n-        // But since we changed the keypath, we do need to update that, so we\n-        // know how to remove it from the map if it gets evicted from the LRU.\n-        entry.keypath = newKeypath\n-      } else {\n-        // Something else modified this entry already. Since the re-keying is\n-        // just a performance optimization, we can safely skip it.\n-      }\n+\n+      // Re-key the entry. The `set` implementation handles removing it from\n+      // its previous position in the cache. We don't need to do anything to\n+      // update the LRU, because the entry is already in it.\n+      // TODO: Treat this as an upsert — should check if an entry already\n+      // exists at the new keypath, and if so, whether we should keep that\n+      // one instead.\n+      const newKeypath: RouteCacheKeypath = [pathname, search, Fallback]\n+      const isRevalidation = false\n+      setInCacheMap(routeCacheMap, newKeypath, entry, isRevalidation)\n     }\n     // Return a promise that resolves when the network connection closes, so\n     // the scheduler can track the number of concurrent network connections.\n@@ -1589,7 +1485,7 @@ export async function fetchSegmentOnCacheMiss(\n   // are usually the same, but the canonical URL will be different if the route\n   // tree response was redirected. To avoid an extra waterfall on every segment\n   // request, we pass the redirected URL instead of the original one.\n-  const url = new URL(route.canonicalUrl, routeKey.href)\n+  const url = new URL(route.canonicalUrl, location.origin)\n   const nextUrl = routeKey.nextUrl\n \n   const requestKey = tree.requestKey\n@@ -1650,7 +1546,7 @@ export async function fetchSegmentOnCacheMiss(\n       response.body,\n       closed.resolve,\n       function onResponseSizeUpdate(size) {\n-        segmentCacheLru.updateSize(segmentCacheEntry, size)\n+        setSizeInCacheMap(segmentCacheEntry, size)\n       }\n     )\n     const serverData = await (createFromNextReadableStream(\n@@ -1698,8 +1594,9 @@ export async function fetchSegmentPrefetchesUsingDynamicRequest(\n   dynamicRequestTree: FlightRouterState,\n   spawnedEntries: Map<SegmentCacheKey, PendingSegmentCacheEntry>\n ): Promise<PrefetchSubtaskResult<null> | null> {\n-  const url = new URL(route.canonicalUrl, task.key.href)\n-  const nextUrl = task.key.nextUrl\n+  const key = task.key\n+  const url = new URL(route.canonicalUrl, location.origin)\n+  const nextUrl = key.nextUrl\n   const headers: RequestHeaders = {\n     [RSC_HEADER]: '1',\n     [NEXT_ROUTER_STATE_TREE_HEADER]:\n@@ -1768,7 +1665,7 @@ export async function fetchSegmentPrefetchesUsingDynamicRequest(\n         }\n         const averageSize = totalBytesReceivedSoFar / fulfilledEntries.length\n         for (const entry of fulfilledEntries) {\n-          segmentCacheLru.updateSize(entry, averageSize)\n+          setSizeInCacheMap(entry, averageSize)\n         }\n       }\n     )\n@@ -2106,7 +2003,11 @@ function writeSeedDataIntoCache(\n       )\n       upsertSegmentEntry(\n         now,\n-        getSegmentKeypath(fetchStrategy, route, cacheKey),\n+        getGenericSegmentKeypathFromFetchStrategy(\n+          fetchStrategy,\n+          route,\n+          cacheKey\n+        ),\n         newEntry\n       )\n     }"
        },
        {
            "sha": "d7d6d03479d98f56563e03f48d493309f191d690",
            "filename": "packages/next/src/client/components/segment-cache-impl/lru.ts",
            "status": "modified",
            "additions": 101,
            "deletions": 110,
            "changes": 211,
            "blob_url": "https://github.com/vercel/next.js/blob/61b285e52842e3a458b5efb442f71123d8813ae8/packages%2Fnext%2Fsrc%2Fclient%2Fcomponents%2Fsegment-cache-impl%2Flru.ts",
            "raw_url": "https://github.com/vercel/next.js/raw/61b285e52842e3a458b5efb442f71123d8813ae8/packages%2Fnext%2Fsrc%2Fclient%2Fcomponents%2Fsegment-cache-impl%2Flru.ts",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/packages%2Fnext%2Fsrc%2Fclient%2Fcomponents%2Fsegment-cache-impl%2Flru.ts?ref=61b285e52842e3a458b5efb442f71123d8813ae8",
            "patch": "@@ -1,136 +1,127 @@\n-export type LRU<T extends LRUNode> = {\n-  put(node: T): void\n-  delete(node: T): void\n-  updateSize(node: T, size: number): void\n-}\n+import type { MapEntry } from './cache-map'\n+import { deleteFromCacheMap } from './cache-map'\n \n-// Doubly-linked list\n-type LRUNode<T = any> = {\n-  // Although it's not encoded in the type, these are both null if the node is\n-  // not in the LRU; both non-null if it is.\n-  prev: T | null\n-  next: T | null\n-  size: number\n-}\n+// We use an LRU for memory management. We must update this whenever we add or\n+// remove a new cache entry, or when an entry changes size.\n \n-// Rather than create an internal LRU node, the passed-in type must conform\n-// the LRUNode interface. This is just a memory optimization to avoid creating\n-// another object; we only use this for Segment Cache entries so it doesn't need\n-// to be general purpose.\n-export function createLRU<T extends LRUNode>(\n-  // From the LRU's perspective, the size unit is arbitrary, but for our\n-  // purposes this is the byte size.\n-  maxLruSize: number,\n-  onEviction: (node: T) => void\n-): LRU<T> {\n-  let head: T | null = null\n-  let didScheduleCleanup: boolean = false\n-  let lruSize: number = 0\n+// The MapEntry type is used as an LRU node, too. We choose this one instead of\n+// the inner cache entry type (RouteCacheEntry, SegmentCacheEntry) because it's\n+// monomorphic and can be optimized by the VM.\n+type LRUNode = MapEntry<any, any>\n \n-  function put(node: T) {\n-    if (head === node) {\n-      // Already at the head\n-      return\n-    }\n-    const prev = node.prev\n-    const next = node.next\n-    if (next === null || prev === null) {\n-      // This is an insertion\n-      lruSize += node.size\n-      // Whenever we add an entry, we need to check if we've exceeded the\n-      // max size. We don't evict entries immediately; they're evicted later in\n-      // an asynchronous task.\n-      ensureCleanupIsScheduled()\n-    } else {\n-      // This is a move. Remove from its current position.\n-      prev.next = next\n-      next.prev = prev\n-    }\n+let head: LRUNode | null = null\n+let didScheduleCleanup: boolean = false\n+let lruSize: number = 0\n \n-    // Move to the front of the list\n-    if (head === null) {\n-      // This is the first entry\n-      node.prev = node\n-      node.next = node\n-    } else {\n-      // Add to the front of the list\n-      const tail = head.prev\n-      node.prev = tail\n+// TODO: I chose the max size somewhat arbitrarily. Consider setting this based\n+// on navigator.deviceMemory, or some other heuristic. We should make this\n+// customizable via the Next.js config, too.\n+const maxLruSize = 50 * 1024 * 1024 // 50 MB\n+\n+export function lruPut(node: LRUNode) {\n+  if (head === node) {\n+    // Already at the head\n+    return\n+  }\n+  const prev = node.prev\n+  const next = node.next\n+  if (next === null || prev === null) {\n+    // This is an insertion\n+    lruSize += node.size\n+    // Whenever we add an entry, we need to check if we've exceeded the\n+    // max size. We don't evict entries immediately; they're evicted later in\n+    // an asynchronous task.\n+    ensureCleanupIsScheduled()\n+  } else {\n+    // This is a move. Remove from its current position.\n+    prev.next = next\n+    next.prev = prev\n+  }\n+\n+  // Move to the front of the list\n+  if (head === null) {\n+    // This is the first entry\n+    node.prev = node\n+    node.next = node\n+  } else {\n+    // Add to the front of the list\n+    const tail = head.prev\n+    node.prev = tail\n+    // In practice, this is never null, but that isn't encoded in the type\n+    if (tail !== null) {\n       tail.next = node\n-      node.next = head\n-      head.prev = node\n     }\n-    head = node\n+    node.next = head\n+    head.prev = node\n   }\n+  head = node\n+}\n \n-  function updateSize(node: T, newNodeSize: number) {\n-    // This is a separate function from `put` so that we can resize the entry\n-    // regardless of whether it's currently being tracked by the LRU.\n-    const prevNodeSize = node.size\n-    node.size = newNodeSize\n-    if (node.next === null) {\n-      // This entry is not currently being tracked by the LRU.\n-      return\n-    }\n-    // Update the total LRU size\n-    lruSize = lruSize - prevNodeSize + newNodeSize\n-    ensureCleanupIsScheduled()\n+export function updateLruSize(node: LRUNode, newNodeSize: number) {\n+  // This is a separate function from `put` so that we can resize the entry\n+  // regardless of whether it's currently being tracked by the LRU.\n+  const prevNodeSize = node.size\n+  node.size = newNodeSize\n+  if (node.next === null) {\n+    // This entry is not currently being tracked by the LRU.\n+    return\n   }\n+  // Update the total LRU size\n+  lruSize = lruSize - prevNodeSize + newNodeSize\n+  ensureCleanupIsScheduled()\n+}\n \n-  function deleteNode(deleted: T) {\n-    const next = deleted.next\n-    const prev = deleted.prev\n-    if (next !== null && prev !== null) {\n-      lruSize -= deleted.size\n+export function deleteFromLru(deleted: LRUNode) {\n+  const next = deleted.next\n+  const prev = deleted.prev\n+  if (next !== null && prev !== null) {\n+    lruSize -= deleted.size\n \n-      deleted.next = null\n-      deleted.prev = null\n+    deleted.next = null\n+    deleted.prev = null\n \n-      // Remove from the list\n-      if (head === deleted) {\n-        // Update the head\n-        if (next === head) {\n-          // This was the last entry\n-          head = null\n-        } else {\n-          head = next\n-        }\n+    // Remove from the list\n+    if (head === deleted) {\n+      // Update the head\n+      if (next === head) {\n+        // This was the last entry\n+        head = null\n       } else {\n-        prev.next = next\n-        next.prev = prev\n+        head = next\n       }\n     } else {\n-      // Already deleted\n+      prev.next = next\n+      next.prev = prev\n     }\n+  } else {\n+    // Already deleted\n   }\n+}\n \n-  function ensureCleanupIsScheduled() {\n-    if (didScheduleCleanup || lruSize <= maxLruSize) {\n-      return\n-    }\n-    didScheduleCleanup = true\n-    requestCleanupCallback(cleanup)\n+function ensureCleanupIsScheduled() {\n+  if (didScheduleCleanup || lruSize <= maxLruSize) {\n+    return\n   }\n+  didScheduleCleanup = true\n+  requestCleanupCallback(cleanup)\n+}\n \n-  function cleanup() {\n-    didScheduleCleanup = false\n+function cleanup() {\n+  didScheduleCleanup = false\n \n-    // Evict entries until we're at 90% capacity. We can assume this won't\n-    // infinite loop because even if `maxLruSize` were 0, eventually\n-    // `deleteNode` sets `head` to `null` when we run out entries.\n-    const ninetyPercentMax = maxLruSize * 0.9\n-    while (lruSize > ninetyPercentMax && head !== null) {\n-      const tail = head.prev\n-      deleteNode(tail)\n-      onEviction(tail)\n+  // Evict entries until we're at 90% capacity. We can assume this won't\n+  // infinite loop because even if `maxLruSize` were 0, eventually\n+  // `deleteFromLru` sets `head` to `null` when we run out entries.\n+  const ninetyPercentMax = maxLruSize * 0.9\n+  while (lruSize > ninetyPercentMax && head !== null) {\n+    const tail = head.prev\n+    // In practice, this is never null, but that isn't encoded in the type\n+    if (tail !== null) {\n+      // Delete the entry from the map. In turn, this will remove it from\n+      // the LRU.\n+      deleteFromCacheMap(tail.value)\n     }\n   }\n-\n-  return {\n-    put,\n-    delete: deleteNode,\n-    updateSize,\n-  }\n }\n \n const requestCleanupCallback ="
        },
        {
            "sha": "398aaa91e5a2466240e83a4aa6ae37d59a07b044",
            "filename": "packages/next/src/client/components/segment-cache-impl/navigation.ts",
            "status": "modified",
            "additions": 14,
            "deletions": 3,
            "changes": 17,
            "blob_url": "https://github.com/vercel/next.js/blob/61b285e52842e3a458b5efb442f71123d8813ae8/packages%2Fnext%2Fsrc%2Fclient%2Fcomponents%2Fsegment-cache-impl%2Fnavigation.ts",
            "raw_url": "https://github.com/vercel/next.js/raw/61b285e52842e3a458b5efb442f71123d8813ae8/packages%2Fnext%2Fsrc%2Fclient%2Fcomponents%2Fsegment-cache-impl%2Fnavigation.ts",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/packages%2Fnext%2Fsrc%2Fclient%2Fcomponents%2Fsegment-cache-impl%2Fnavigation.ts?ref=61b285e52842e3a458b5efb442f71123d8813ae8",
            "patch": "@@ -19,6 +19,7 @@ import { createHrefFromUrl } from '../router-reducer/create-href-from-url'\n import {\n   EntryStatus,\n   readRouteCacheEntry,\n+  getCanonicalSegmentKeypath,\n   readSegmentCacheEntry,\n   waitForSegmentCacheEntry,\n   requestOptimisticRouteCacheEntry,\n@@ -123,7 +124,13 @@ export function navigate(\n     const prefetchSeedData = snapshot.seedData\n     const prefetchHead = route.head\n     const isPrefetchHeadPartial = route.isHeadPartial\n-    const newCanonicalUrl = route.canonicalUrl\n+    // TODO: The \"canonicalUrl\" stored in the cache doesn't include the hash,\n+    // because hash entries do not vary by hash fragment. However, the one\n+    // we set in the router state *does* include the hash, and it's used to\n+    // sync with the actual browser location. To make this less of a refactor\n+    // hazard, we should always track the hash separately from the rest of\n+    // the URL.\n+    const newCanonicalUrl = route.canonicalUrl + url.hash\n     const renderedSearch = route.renderedSearch\n     return navigateUsingPrefetchedRouteTree(\n       now,\n@@ -166,7 +173,7 @@ export function navigate(\n       const prefetchSeedData = snapshot.seedData\n       const prefetchHead = optimisticRoute.head\n       const isPrefetchHeadPartial = optimisticRoute.isHeadPartial\n-      const newCanonicalUrl = optimisticRoute.canonicalUrl\n+      const newCanonicalUrl = optimisticRoute.canonicalUrl + url.hash\n       const newRenderedSearch = optimisticRoute.renderedSearch\n       return navigateUsingPrefetchedRouteTree(\n         now,\n@@ -338,7 +345,11 @@ function readRenderSnapshotFromCache(\n   let loading: LoadingModuleData | Promise<LoadingModuleData> = null\n   let isPartial: boolean = true\n \n-  const segmentEntry = readSegmentCacheEntry(now, route, tree.cacheKey)\n+  const canonicalSegmentKeypath = getCanonicalSegmentKeypath(\n+    route,\n+    tree.cacheKey\n+  )\n+  const segmentEntry = readSegmentCacheEntry(now, canonicalSegmentKeypath)\n   if (segmentEntry !== null) {\n     switch (segmentEntry.status) {\n       case EntryStatus.Fulfilled: {"
        },
        {
            "sha": "de0d8df44619b91776a8ba53c00ab267f4ea658e",
            "filename": "packages/next/src/client/components/segment-cache-impl/scheduler.ts",
            "status": "modified",
            "additions": 35,
            "deletions": 38,
            "changes": 73,
            "blob_url": "https://github.com/vercel/next.js/blob/61b285e52842e3a458b5efb442f71123d8813ae8/packages%2Fnext%2Fsrc%2Fclient%2Fcomponents%2Fsegment-cache-impl%2Fscheduler.ts",
            "raw_url": "https://github.com/vercel/next.js/raw/61b285e52842e3a458b5efb442f71123d8813ae8/packages%2Fnext%2Fsrc%2Fclient%2Fcomponents%2Fsegment-cache-impl%2Fscheduler.ts",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/packages%2Fnext%2Fsrc%2Fclient%2Fcomponents%2Fsegment-cache-impl%2Fscheduler.ts?ref=61b285e52842e3a458b5efb442f71123d8813ae8",
            "patch": "@@ -23,9 +23,10 @@ import {\n   type FulfilledSegmentCacheEntry,\n   upgradeToPendingSegment,\n   waitForSegmentCacheEntry,\n-  resetRevalidatingSegmentEntry,\n-  getSegmentKeypath,\n+  overwriteRevalidatingSegmentCacheEntry,\n+  getGenericSegmentKeypathFromFetchStrategy,\n   canNewFetchStrategyProvideMoreContent,\n+  type SegmentCacheKeypath,\n } from './cache'\n import type { RouteCacheKey } from './cache-key'\n import { createCacheKey } from './cache-key'\n@@ -556,8 +557,7 @@ function pingRoute(now: number, task: PrefetchTask): PrefetchTaskExitStatus {\n     // a wildcard lookup method to the TupleMap implementation. This is\n     // non-trivial to implement because it needs to account for things like\n     // fallback route entries, hence this temporary workaround.\n-    const url = new URL(key.href)\n-    url.search = ''\n+    const url = new URL(key.pathname, location.origin)\n     const keyWithoutSearch = createCacheKey(url.href, key.nextUrl)\n     const routeWithoutSearch = readOrCreateRouteCacheEntry(\n       now,\n@@ -1265,7 +1265,6 @@ function pingRouteTreeAndIncludeDynamicData(\n         spawnedSegment = pingFullSegmentRevalidation(\n           now,\n           route,\n-          segment,\n           tree,\n           fetchStrategy\n         )\n@@ -1285,7 +1284,6 @@ function pingRouteTreeAndIncludeDynamicData(\n         spawnedSegment = pingFullSegmentRevalidation(\n           now,\n           route,\n-          segment,\n           tree,\n           fetchStrategy\n         )\n@@ -1421,14 +1419,7 @@ function pingStaticSegmentData(\n           if (background(task)) {\n             // TODO: Instead of speculatively revalidating, consider including\n             // `hasLoading` in the route tree prefetch response.\n-            pingPPRSegmentRevalidation(\n-              now,\n-              task,\n-              segment,\n-              route,\n-              routeKey,\n-              tree\n-            )\n+            pingPPRSegmentRevalidation(now, route, routeKey, tree)\n           }\n           break\n         default:\n@@ -1456,7 +1447,7 @@ function pingStaticSegmentData(\n           // Because a rejected segment will definitely prevent the segment (and\n           // all of its children) from rendering, we perform this revalidation\n           // immediately instead of deferring it to a background task.\n-          pingPPRSegmentRevalidation(now, task, segment, route, routeKey, tree)\n+          pingPPRSegmentRevalidation(now, route, routeKey, tree)\n           break\n         default:\n           segment.fetchStrategy satisfies never\n@@ -1477,31 +1468,33 @@ function pingStaticSegmentData(\n \n function pingPPRSegmentRevalidation(\n   now: number,\n-  task: PrefetchTask,\n-  currentSegment: SegmentCacheEntry,\n   route: FulfilledRouteCacheEntry,\n   routeKey: RouteCacheKey,\n   tree: RouteTree\n ): void {\n   const revalidatingSegment = readOrCreateRevalidatingSegmentEntry(\n     now,\n-    currentSegment\n+    FetchStrategy.PPR,\n+    route,\n+    tree.cacheKey\n   )\n   switch (revalidatingSegment.status) {\n     case EntryStatus.Empty:\n       // Spawn a prefetch request and upsert the segment into the cache\n       // upon completion.\n       upsertSegmentOnCompletion(\n-        task.fetchStrategy,\n-        route,\n-        tree.cacheKey,\n         spawnPrefetchSubtask(\n           fetchSegmentOnCacheMiss(\n             route,\n             upgradeToPendingSegment(revalidatingSegment, FetchStrategy.PPR),\n             routeKey,\n             tree\n           )\n+        ),\n+        getGenericSegmentKeypathFromFetchStrategy(\n+          FetchStrategy.PPR,\n+          route,\n+          tree.cacheKey\n         )\n       )\n       break\n@@ -1522,13 +1515,14 @@ function pingPPRSegmentRevalidation(\n function pingFullSegmentRevalidation(\n   now: number,\n   route: FulfilledRouteCacheEntry,\n-  currentSegment: SegmentCacheEntry,\n   tree: RouteTree,\n   fetchStrategy: FetchStrategy.Full | FetchStrategy.PPRRuntime\n ): PendingSegmentCacheEntry | null {\n   const revalidatingSegment = readOrCreateRevalidatingSegmentEntry(\n     now,\n-    currentSegment\n+    fetchStrategy,\n+    route,\n+    tree.cacheKey\n   )\n   if (revalidatingSegment.status === EntryStatus.Empty) {\n     // During a Full/PPRRuntime prefetch, a single dynamic request is made for all the\n@@ -1541,10 +1535,12 @@ function pingFullSegmentRevalidation(\n       fetchStrategy\n     )\n     upsertSegmentOnCompletion(\n-      fetchStrategy,\n-      route,\n-      tree.cacheKey,\n-      waitForSegmentCacheEntry(pendingSegment)\n+      waitForSegmentCacheEntry(pendingSegment),\n+      getGenericSegmentKeypathFromFetchStrategy(\n+        fetchStrategy,\n+        route,\n+        tree.cacheKey\n+      )\n     )\n     return pendingSegment\n   } else {\n@@ -1558,18 +1554,22 @@ function pingFullSegmentRevalidation(\n     ) {\n       // The existing revalidation was fetched using a less specific strategy.\n       // Reset it and start a new revalidation.\n-      const emptySegment = resetRevalidatingSegmentEntry(\n-        nonEmptyRevalidatingSegment\n+      const emptySegment = overwriteRevalidatingSegmentCacheEntry(\n+        fetchStrategy,\n+        route,\n+        tree.cacheKey\n       )\n       const pendingSegment = upgradeToPendingSegment(\n         emptySegment,\n         fetchStrategy\n       )\n       upsertSegmentOnCompletion(\n-        fetchStrategy,\n-        route,\n-        tree.cacheKey,\n-        waitForSegmentCacheEntry(pendingSegment)\n+        waitForSegmentCacheEntry(pendingSegment),\n+        getGenericSegmentKeypathFromFetchStrategy(\n+          fetchStrategy,\n+          route,\n+          tree.cacheKey\n+        )\n       )\n       return pendingSegment\n     }\n@@ -1593,16 +1593,13 @@ function pingFullSegmentRevalidation(\n const noop = () => {}\n \n function upsertSegmentOnCompletion(\n-  fetchStrategy: FetchStrategy,\n-  route: FulfilledRouteCacheEntry,\n-  cacheKey: SegmentCacheKey,\n-  promise: Promise<FulfilledSegmentCacheEntry | null>\n+  promise: Promise<FulfilledSegmentCacheEntry | null>,\n+  keypath: SegmentCacheKeypath\n ) {\n   // Wait for a segment to finish loading, then upsert it into the cache\n   promise.then((fulfilled) => {\n     if (fulfilled !== null) {\n       // Received new data. Attempt to replace the existing entry in the cache.\n-      const keypath = getSegmentKeypath(fetchStrategy, route, cacheKey)\n       upsertSegmentEntry(Date.now(), keypath, fulfilled)\n     }\n   }, noop)"
        },
        {
            "sha": "d78ba1b1727a2cb56c7dc587095acad36851cea1",
            "filename": "packages/next/src/client/components/segment-cache-impl/tuple-map.ts",
            "status": "removed",
            "additions": 0,
            "deletions": 196,
            "changes": 196,
            "blob_url": "https://github.com/vercel/next.js/blob/a99aaa9c423c627e483cb7b5d117f2ffdcf44fe4/packages%2Fnext%2Fsrc%2Fclient%2Fcomponents%2Fsegment-cache-impl%2Ftuple-map.ts",
            "raw_url": "https://github.com/vercel/next.js/raw/a99aaa9c423c627e483cb7b5d117f2ffdcf44fe4/packages%2Fnext%2Fsrc%2Fclient%2Fcomponents%2Fsegment-cache-impl%2Ftuple-map.ts",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/packages%2Fnext%2Fsrc%2Fclient%2Fcomponents%2Fsegment-cache-impl%2Ftuple-map.ts?ref=a99aaa9c423c627e483cb7b5d117f2ffdcf44fe4",
            "patch": "@@ -1,196 +0,0 @@\n-// Utility type. Prefix<[A, B, C, D]> matches [A], [A, B], [A, B, C] etc.\n-export type Prefix<T extends any[]> = T extends [infer First, ...infer Rest]\n-  ? [] | [First] | [First, ...Prefix<Rest>]\n-  : []\n-\n-export type TupleMap<Keypath extends Array<any>, V> = {\n-  set(keys: Prefix<Keypath>, value: V): void\n-  get(keys: Prefix<Keypath>): V | null\n-  delete(keys: Prefix<Keypath>): void\n-}\n-\n-/**\n- * Creates a map whose keys are tuples. Tuples are compared per-element. This\n- * is useful when a key has multiple parts, but you don't want to concatenate\n- * them into a single string value.\n- *\n- * In the Segment Cache, we use this to store cache entries by both their href\n- * and their Next-URL.\n- *\n- * Example:\n- *   map.set(['https://localhost', 'foo/bar/baz'], 'yay');\n- *   map.get(['https://localhost', 'foo/bar/baz']); // returns 'yay'\n- */\n-export function createTupleMap<Keypath extends Array<any>, V>(): TupleMap<\n-  Keypath,\n-  V\n-> {\n-  type MapEntryShared = {\n-    parent: MapEntry | null\n-    key: any\n-    map: Map<any, MapEntry> | null\n-  }\n-\n-  type EmptyMapEntry = MapEntryShared & {\n-    value: null\n-    hasValue: false\n-  }\n-\n-  type FullMapEntry = MapEntryShared & {\n-    value: V\n-    hasValue: true\n-  }\n-\n-  type MapEntry = EmptyMapEntry | FullMapEntry\n-\n-  let rootEntry: MapEntry = {\n-    parent: null,\n-    key: null,\n-    hasValue: false,\n-    value: null,\n-    map: null,\n-  }\n-\n-  // To optimize successive lookups, we cache the last accessed keypath.\n-  // Although it's not encoded in the type, these are both null or\n-  // both non-null. It uses object equality, so to take advantage of this\n-  // optimization, you must pass the same array instance to each successive\n-  // method call, and you must also not mutate the array between calls.\n-  let lastAccessedEntry: MapEntry | null = null\n-  let lastAccessedKeys: Prefix<Keypath> | null = null\n-\n-  function getOrCreateEntry(keys: Prefix<Keypath>): MapEntry {\n-    if (lastAccessedKeys === keys) {\n-      return lastAccessedEntry!\n-    }\n-\n-    // Go through each level of keys until we find the entry that matches,\n-    // or create a new one if it doesn't already exist.\n-    let entry = rootEntry\n-    for (let i = 0; i < keys.length; i++) {\n-      const key = keys[i]\n-      let map = entry.map\n-      if (map !== null) {\n-        const existingEntry = map.get(key)\n-        if (existingEntry !== undefined) {\n-          // Found a match. Keep going.\n-          entry = existingEntry\n-          continue\n-        }\n-      } else {\n-        map = new Map()\n-        entry.map = map\n-      }\n-      // No entry exists yet at this level. Create a new one.\n-      const newEntry: MapEntry = {\n-        parent: entry,\n-        key,\n-        value: null,\n-        hasValue: false,\n-        map: null,\n-      }\n-      map.set(key, newEntry)\n-      entry = newEntry\n-    }\n-\n-    lastAccessedKeys = keys\n-    lastAccessedEntry = entry\n-\n-    return entry\n-  }\n-\n-  function getEntryIfExists(keys: Prefix<Keypath>): MapEntry | null {\n-    if (lastAccessedKeys === keys) {\n-      return lastAccessedEntry\n-    }\n-\n-    // Go through each level of keys until we find the entry that matches, or\n-    // return null if no match exists.\n-    let entry = rootEntry\n-    for (let i = 0; i < keys.length; i++) {\n-      const key = keys[i]\n-      let map = entry.map\n-      if (map !== null) {\n-        const existingEntry = map.get(key)\n-        if (existingEntry !== undefined) {\n-          // Found a match. Keep going.\n-          entry = existingEntry\n-          continue\n-        }\n-      }\n-      // No entry exists at this level.\n-      return null\n-    }\n-\n-    lastAccessedKeys = keys\n-    lastAccessedEntry = entry\n-\n-    return entry\n-  }\n-\n-  function set(keys: Prefix<Keypath>, value: V): void {\n-    const entry = getOrCreateEntry(keys)\n-    entry.hasValue = true\n-    entry.value = value\n-  }\n-\n-  function get(keys: Prefix<Keypath>): V | null {\n-    const entry = getEntryIfExists(keys)\n-    if (entry === null || !entry.hasValue) {\n-      return null\n-    }\n-    return entry.value\n-  }\n-\n-  function deleteEntry(keys: Prefix<Keypath>): void {\n-    const entry = getEntryIfExists(keys)\n-    if (entry === null || !entry.hasValue) {\n-      return\n-    }\n-\n-    // Found a match. Delete it from the cache.\n-    const deletedEntry: EmptyMapEntry = entry as any\n-    deletedEntry.hasValue = false\n-    deletedEntry.value = null\n-\n-    // Check if we can garbage collect the entry.\n-    if (deletedEntry.map === null) {\n-      // Since this entry has no value, and also no child entries, we can\n-      // garbage collect it. Remove it from its parent, and keep garbage\n-      // collecting the parents until we reach a non-empty entry.\n-\n-      // Unlike a `set` operation, these are no longer valid because the entry\n-      // itself is being modified, not just the value it contains.\n-      lastAccessedEntry = null\n-      lastAccessedKeys = null\n-\n-      let parent = deletedEntry.parent\n-      let key = deletedEntry.key\n-      while (parent !== null) {\n-        const parentMap = parent.map\n-        if (parentMap !== null) {\n-          parentMap.delete(key)\n-          if (parentMap.size === 0) {\n-            // We just removed the last entry in the parent map.\n-            parent.map = null\n-            if (parent.value === null) {\n-              // The parent node has no child entries, nor does it have a value\n-              // on itself. It can be garbage collected. Keep going.\n-              key = parent.key\n-              parent = parent.parent\n-              continue\n-            }\n-          }\n-        }\n-        // The parent is not empty. Stop garbage collecting.\n-        break\n-      }\n-    }\n-  }\n-\n-  return {\n-    set,\n-    get,\n-    delete: deleteEntry,\n-  }\n-}"
        }
    ],
    "stats": {
        "total": 1558,
        "additions": 866,
        "deletions": 692
    }
}