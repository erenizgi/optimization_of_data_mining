{
    "author": "sokra",
    "message": "Turbopack: Use batch get request to read more efficient from database (#87106)\n\n### What?\n\nMaking a batch read from database can be more efficient:\n* Order keys by hash for better cache locallity (in AMQF, key blocks, value blocks)\n* Use binary search to filter keys by min/max hash of the SST files\n* Only need to check family once instead of per key\n\nPrefetch tasks with batch reads.\nAdd helper for \"for each task\" which can use batch read.\n\nadd tracing for database reads",
    "sha": "1d09f42590ef47dfde7061f3a01621654f2a14df",
    "files": [
        {
            "sha": "6a96f9bd385fe73428e299cc5e213f20e1e2389d",
            "filename": "turbopack/crates/turbo-persistence/src/db.rs",
            "status": "modified",
            "additions": 115,
            "deletions": 0,
            "changes": 115,
            "blob_url": "https://github.com/vercel/next.js/blob/1d09f42590ef47dfde7061f3a01621654f2a14df/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fdb.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/1d09f42590ef47dfde7061f3a01621654f2a14df/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fdb.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fdb.rs?ref=1d09f42590ef47dfde7061f3a01621654f2a14df",
            "patch": "@@ -1338,6 +1338,9 @@ impl<S: ParallelScheduler, const FAMILIES: usize> TurboPersistence<S, FAMILIES>\n     /// might hold onto a block of the database and it should not be hold long-term.\n     pub fn get<K: QueryKey>(&self, family: usize, key: &K) -> Result<Option<ArcSlice<u8>>> {\n         debug_assert!(family < FAMILIES, \"Family index out of bounds\");\n+        let span =\n+            tracing::trace_span!(\"database read\", family, result_size = tracing::field::Empty)\n+                .entered();\n         let hash = hash_key(key);\n         let inner = self.inner.read();\n         for meta in inner.meta_files.iter().rev() {\n@@ -1368,17 +1371,20 @@ impl<S: ParallelScheduler, const FAMILIES: usize> TurboPersistence<S, FAMILIES>\n                             LookupValue::Deleted => {\n                                 #[cfg(feature = \"stats\")]\n                                 self.stats.hits_deleted.fetch_add(1, Ordering::Relaxed);\n+                                span.record(\"result_size\", \"deleted\");\n                                 return Ok(None);\n                             }\n                             LookupValue::Slice { value } => {\n                                 #[cfg(feature = \"stats\")]\n                                 self.stats.hits_small.fetch_add(1, Ordering::Relaxed);\n+                                span.record(\"result_size\", value.len());\n                                 return Ok(Some(value));\n                             }\n                             LookupValue::Blob { sequence_number } => {\n                                 #[cfg(feature = \"stats\")]\n                                 self.stats.hits_blob.fetch_add(1, Ordering::Relaxed);\n                                 let blob = self.read_blob(sequence_number)?;\n+                                span.record(\"result_size\", blob.len());\n                                 return Ok(Some(blob));\n                             }\n                         }\n@@ -1392,9 +1398,118 @@ impl<S: ParallelScheduler, const FAMILIES: usize> TurboPersistence<S, FAMILIES>\n         }\n         #[cfg(feature = \"stats\")]\n         self.stats.miss_global.fetch_add(1, Ordering::Relaxed);\n+        span.record(\"result_size\", \"not found\");\n         Ok(None)\n     }\n \n+    pub fn batch_get<K: QueryKey>(\n+        &self,\n+        family: usize,\n+        keys: &[K],\n+    ) -> Result<Vec<Option<ArcSlice<u8>>>> {\n+        debug_assert!(family < FAMILIES, \"Family index out of bounds\");\n+        let span = tracing::trace_span!(\n+            \"database batch read\",\n+            family,\n+            keys = keys.len(),\n+            not_found = tracing::field::Empty,\n+            deleted = tracing::field::Empty,\n+            result_size = tracing::field::Empty\n+        )\n+        .entered();\n+        let mut cells: Vec<(u64, usize, Option<LookupValue>)> = Vec::with_capacity(keys.len());\n+        let mut empty_cells = keys.len();\n+        for (index, key) in keys.iter().enumerate() {\n+            let hash = hash_key(key);\n+            cells.push((hash, index, None));\n+        }\n+        cells.sort_by_key(|(hash, _, _)| *hash);\n+        let inner = self.inner.read();\n+        for meta in inner.meta_files.iter().rev() {\n+            let _result = meta.batch_lookup(\n+                family as u32,\n+                keys,\n+                &mut cells,\n+                &mut empty_cells,\n+                &self.amqf_cache,\n+                &self.key_block_cache,\n+                &self.value_block_cache,\n+            )?;\n+\n+            #[cfg(feature = \"stats\")]\n+            {\n+                let crate::meta_file::MetaBatchLookupResult {\n+                    family_miss,\n+                    range_misses,\n+                    quick_filter_misses,\n+                    sst_misses,\n+                    hits: _,\n+                } = _result;\n+                if family_miss {\n+                    self.stats.miss_family.fetch_add(1, Ordering::Relaxed);\n+                }\n+                if range_misses > 0 {\n+                    self.stats\n+                        .miss_range\n+                        .fetch_add(range_misses as u64, Ordering::Relaxed);\n+                }\n+                if quick_filter_misses > 0 {\n+                    self.stats\n+                        .miss_amqf\n+                        .fetch_add(quick_filter_misses as u64, Ordering::Relaxed);\n+                }\n+                if sst_misses > 0 {\n+                    self.stats\n+                        .miss_key\n+                        .fetch_add(sst_misses as u64, Ordering::Relaxed);\n+                }\n+            }\n+\n+            if empty_cells == 0 {\n+                break;\n+            }\n+        }\n+        let mut deleted = 0;\n+        let mut not_found = 0;\n+        let mut result_size = 0;\n+        let mut results = vec![None; keys.len()];\n+        for (hash, index, result) in cells {\n+            if let Some(result) = result {\n+                inner.accessed_key_hashes[family].insert(hash);\n+                let result = match result {\n+                    LookupValue::Deleted => {\n+                        #[cfg(feature = \"stats\")]\n+                        self.stats.hits_deleted.fetch_add(1, Ordering::Relaxed);\n+                        deleted += 1;\n+                        None\n+                    }\n+                    LookupValue::Slice { value } => {\n+                        #[cfg(feature = \"stats\")]\n+                        self.stats.hits_small.fetch_add(1, Ordering::Relaxed);\n+                        result_size += value.len();\n+                        Some(value)\n+                    }\n+                    LookupValue::Blob { sequence_number } => {\n+                        #[cfg(feature = \"stats\")]\n+                        self.stats.hits_blob.fetch_add(1, Ordering::Relaxed);\n+                        let blob = self.read_blob(sequence_number)?;\n+                        result_size += blob.len();\n+                        Some(blob)\n+                    }\n+                };\n+                results[index] = result;\n+            } else {\n+                #[cfg(feature = \"stats\")]\n+                self.stats.miss_global.fetch_add(1, Ordering::Relaxed);\n+                not_found += 1;\n+            }\n+        }\n+        span.record(\"not_found\", not_found);\n+        span.record(\"deleted\", deleted);\n+        span.record(\"result_size\", result_size);\n+        Ok(results)\n+    }\n+\n     /// Returns database statistics.\n     #[cfg(feature = \"stats\")]\n     pub fn statistics(&self) -> Statistics {"
        },
        {
            "sha": "b396c4fd8f7a2bf0093921c1fe2441c60dc2c575",
            "filename": "turbopack/crates/turbo-persistence/src/meta_file.rs",
            "status": "modified",
            "additions": 118,
            "deletions": 0,
            "changes": 118,
            "blob_url": "https://github.com/vercel/next.js/blob/1d09f42590ef47dfde7061f3a01621654f2a14df/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fmeta_file.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/1d09f42590ef47dfde7061f3a01621654f2a14df/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fmeta_file.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fmeta_file.rs?ref=1d09f42590ef47dfde7061f3a01621654f2a14df",
            "patch": "@@ -1,4 +1,5 @@\n use std::{\n+    cmp::Ordering,\n     fmt::Display,\n     fs::File,\n     hash::BuildHasherDefault,\n@@ -20,6 +21,7 @@ use turbo_bincode::turbo_bincode_decode;\n \n use crate::{\n     QueryKey,\n+    lookup_entry::LookupValue,\n     static_sorted_file::{BlockCache, SstLookupResult, StaticSortedFile, StaticSortedFileMetaData},\n };\n \n@@ -211,6 +213,27 @@ pub enum MetaLookupResult {\n     SstLookup(SstLookupResult),\n }\n \n+/// The result of a batch lookup operation.\n+#[derive(Default)]\n+pub struct MetaBatchLookupResult {\n+    /// The key was not found because it is from a different key family.\n+    #[cfg(feature = \"stats\")]\n+    pub family_miss: bool,\n+    /// The key was not found because it is out of the range of this SST file. But it was the\n+    /// correct key family.\n+    #[cfg(feature = \"stats\")]\n+    pub range_misses: usize,\n+    /// The key was not found because it was not in the AMQF filter. But it was in the range.\n+    #[cfg(feature = \"stats\")]\n+    pub quick_filter_misses: usize,\n+    /// The key was unsuccessfully looked up in the SST file. It was in the AMQF filter.\n+    #[cfg(feature = \"stats\")]\n+    pub sst_misses: usize,\n+    /// The key was found in the SST file.\n+    #[cfg(feature = \"stats\")]\n+    pub hits: usize,\n+}\n+\n /// The key family and hash range of an SST file.\n #[derive(Clone, Copy)]\n pub struct StaticSortedFileRange {\n@@ -406,4 +429,99 @@ impl MetaFile {\n         }\n         Ok(miss_result)\n     }\n+\n+    pub fn batch_lookup<K: QueryKey>(\n+        &self,\n+        key_family: u32,\n+        keys: &[K],\n+        cells: &mut [(u64, usize, Option<LookupValue>)],\n+        empty_cells: &mut usize,\n+        amqf_cache: &AmqfCache,\n+        key_block_cache: &BlockCache,\n+        value_block_cache: &BlockCache,\n+    ) -> Result<MetaBatchLookupResult> {\n+        if key_family != self.family {\n+            #[cfg(feature = \"stats\")]\n+            return Ok(MetaBatchLookupResult {\n+                family_miss: true,\n+                ..Default::default()\n+            });\n+            #[cfg(not(feature = \"stats\"))]\n+            return Ok(MetaBatchLookupResult {});\n+        }\n+        debug_assert!(\n+            cells.is_sorted_by_key(|(hash, _, _)| *hash),\n+            \"Cells must be sorted by key hash\"\n+        );\n+        #[allow(unused_mut, reason = \"It's used when stats are enabled\")]\n+        let mut lookup_result = MetaBatchLookupResult::default();\n+        for entry in self.entries.iter().rev() {\n+            let start_index = cells\n+                .binary_search_by(|(hash, _, _)| hash.cmp(&entry.min_hash).then(Ordering::Greater))\n+                .err()\n+                .unwrap();\n+            if start_index >= cells.len() {\n+                #[cfg(feature = \"stats\")]\n+                {\n+                    lookup_result.range_misses += 1;\n+                }\n+                continue;\n+            }\n+            let end_index = cells\n+                .binary_search_by(|(hash, _, _)| hash.cmp(&entry.max_hash).then(Ordering::Less))\n+                .err()\n+                .unwrap()\n+                .checked_sub(1);\n+            let Some(end_index) = end_index else {\n+                #[cfg(feature = \"stats\")]\n+                {\n+                    lookup_result.range_misses += 1;\n+                }\n+                continue;\n+            };\n+            if start_index > end_index {\n+                #[cfg(feature = \"stats\")]\n+                {\n+                    lookup_result.range_misses += 1;\n+                }\n+                continue;\n+            }\n+            let amqf = entry.amqf(self, amqf_cache)?;\n+            for (hash, index, result) in &mut cells[start_index..=end_index] {\n+                if result.is_some() {\n+                    continue;\n+                }\n+                if !amqf.contains_fingerprint(*hash) {\n+                    #[cfg(feature = \"stats\")]\n+                    {\n+                        lookup_result.quick_filter_misses += 1;\n+                    }\n+                    continue;\n+                }\n+                let sst_result = entry.sst(self)?.lookup(\n+                    *hash,\n+                    &keys[*index],\n+                    key_block_cache,\n+                    value_block_cache,\n+                )?;\n+                if let SstLookupResult::Found(value) = sst_result {\n+                    *result = Some(value);\n+                    *empty_cells -= 1;\n+                    #[cfg(feature = \"stats\")]\n+                    {\n+                        lookup_result.hits += 1;\n+                    }\n+                    if *empty_cells == 0 {\n+                        return Ok(lookup_result);\n+                    }\n+                } else {\n+                    #[cfg(feature = \"stats\")]\n+                    {\n+                        lookup_result.sst_misses += 1;\n+                    }\n+                }\n+            }\n+        }\n+        Ok(lookup_result)\n+    }\n }"
        },
        {
            "sha": "a88061a665a950a665e6525f8974a273b95a4e21",
            "filename": "turbopack/crates/turbo-persistence/src/tests.rs",
            "status": "modified",
            "additions": 466,
            "deletions": 0,
            "changes": 466,
            "blob_url": "https://github.com/vercel/next.js/blob/1d09f42590ef47dfde7061f3a01621654f2a14df/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Ftests.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/1d09f42590ef47dfde7061f3a01621654f2a14df/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Ftests.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Ftests.rs?ref=1d09f42590ef47dfde7061f3a01621654f2a14df",
            "patch": "@@ -837,3 +837,469 @@ fn merge_file_removal() -> Result<()> {\n \n     Ok(())\n }\n+\n+#[test]\n+fn batch_get_basic() -> Result<()> {\n+    let tempdir = tempfile::tempdir()?;\n+    let path = tempdir.path();\n+\n+    let db = TurboPersistence::<_, 16>::open_with_parallel_scheduler(\n+        path.to_path_buf(),\n+        RayonParallelScheduler,\n+    )?;\n+\n+    // Write some test data\n+    let batch = db.write_batch()?;\n+    for i in 0..100u8 {\n+        batch.put(0, vec![i], vec![i].into())?;\n+    }\n+    db.commit_write_batch(batch)?;\n+\n+    // Test batch_get with mixed existing and non-existing keys\n+    let keys_to_fetch = vec![vec![10u8], vec![20u8], vec![200u8], vec![50u8], vec![255u8]];\n+    let results = db.batch_get(0, &keys_to_fetch)?;\n+\n+    assert_eq!(results.len(), 5);\n+    assert_eq!(results[0].as_deref(), Some(&[10u8][..]));\n+    assert_eq!(results[1].as_deref(), Some(&[20u8][..]));\n+    assert_eq!(results[2], None); // 200 doesn't exist\n+    assert_eq!(results[3].as_deref(), Some(&[50u8][..]));\n+    assert_eq!(results[4], None); // 255 doesn't exist\n+\n+    db.shutdown()?;\n+    Ok(())\n+}\n+\n+#[test]\n+fn batch_get_all_existing() -> Result<()> {\n+    let tempdir = tempfile::tempdir()?;\n+    let path = tempdir.path();\n+\n+    let db = TurboPersistence::<_, 16>::open_with_parallel_scheduler(\n+        path.to_path_buf(),\n+        RayonParallelScheduler,\n+    )?;\n+\n+    // Write test data\n+    let batch = db.write_batch()?;\n+    for i in 0..50u8 {\n+        batch.put(0, vec![i], vec![i * 2].into())?;\n+    }\n+    db.commit_write_batch(batch)?;\n+\n+    // Fetch all existing keys\n+    let keys_to_fetch: Vec<Vec<u8>> = (0..50u8).map(|i| vec![i]).collect();\n+    let results = db.batch_get(0, &keys_to_fetch)?;\n+\n+    assert_eq!(results.len(), 50);\n+    for (i, result) in results.iter().enumerate() {\n+        assert_eq!(result.as_deref(), Some(&[(i * 2) as u8][..]));\n+    }\n+\n+    db.shutdown()?;\n+    Ok(())\n+}\n+\n+#[test]\n+fn batch_get_none_existing() -> Result<()> {\n+    let tempdir = tempfile::tempdir()?;\n+    let path = tempdir.path();\n+\n+    let db = TurboPersistence::<_, 16>::open_with_parallel_scheduler(\n+        path.to_path_buf(),\n+        RayonParallelScheduler,\n+    )?;\n+\n+    // Write some data but query different keys\n+    let batch = db.write_batch()?;\n+    for i in 0..10u8 {\n+        batch.put(0, vec![i], vec![i].into())?;\n+    }\n+    db.commit_write_batch(batch)?;\n+\n+    // Fetch non-existing keys\n+    let keys_to_fetch: Vec<Vec<u8>> = (100..110u8).map(|i| vec![i]).collect();\n+    let results = db.batch_get(0, &keys_to_fetch)?;\n+\n+    assert_eq!(results.len(), 10);\n+    for result in results.iter() {\n+        assert_eq!(result, &None);\n+    }\n+\n+    db.shutdown()?;\n+    Ok(())\n+}\n+\n+#[test]\n+fn batch_get_empty() -> Result<()> {\n+    let tempdir = tempfile::tempdir()?;\n+    let path = tempdir.path();\n+\n+    let db = TurboPersistence::<_, 16>::open_with_parallel_scheduler(\n+        path.to_path_buf(),\n+        RayonParallelScheduler,\n+    )?;\n+\n+    // Write some data\n+    let batch = db.write_batch()?;\n+    batch.put(0, vec![1u8], vec![1u8].into())?;\n+    db.commit_write_batch(batch)?;\n+\n+    // Fetch with empty key list\n+    let keys_to_fetch: Vec<Vec<u8>> = vec![];\n+    let results = db.batch_get(0, &keys_to_fetch)?;\n+\n+    assert_eq!(results.len(), 0);\n+\n+    db.shutdown()?;\n+    Ok(())\n+}\n+\n+#[test]\n+fn batch_get_duplicate_keys() -> Result<()> {\n+    let tempdir = tempfile::tempdir()?;\n+    let path = tempdir.path();\n+\n+    let db = TurboPersistence::<_, 16>::open_with_parallel_scheduler(\n+        path.to_path_buf(),\n+        RayonParallelScheduler,\n+    )?;\n+\n+    // Write test data\n+    let batch = db.write_batch()?;\n+    batch.put(0, vec![42u8], vec![100u8].into())?;\n+    batch.put(0, vec![43u8], vec![101u8].into())?;\n+    db.commit_write_batch(batch)?;\n+\n+    // Fetch with duplicate keys - results should maintain order\n+    let keys_to_fetch = vec![\n+        vec![42u8],\n+        vec![43u8],\n+        vec![42u8],\n+        vec![99u8], // non-existing\n+        vec![42u8],\n+    ];\n+    let results = db.batch_get(0, &keys_to_fetch)?;\n+\n+    assert_eq!(results.len(), 5);\n+    assert_eq!(results[0].as_deref(), Some(&[100u8][..]));\n+    assert_eq!(results[1].as_deref(), Some(&[101u8][..]));\n+    assert_eq!(results[2].as_deref(), Some(&[100u8][..]));\n+    assert_eq!(results[3], None);\n+    assert_eq!(results[4].as_deref(), Some(&[100u8][..]));\n+\n+    db.shutdown()?;\n+    Ok(())\n+}\n+\n+#[test]\n+fn batch_get_large_batch() -> Result<()> {\n+    let tempdir = tempfile::tempdir()?;\n+    let path = tempdir.path();\n+\n+    let db = TurboPersistence::<_, 16>::open_with_parallel_scheduler(\n+        path.to_path_buf(),\n+        RayonParallelScheduler,\n+    )?;\n+\n+    // Write many entries\n+    let batch = db.write_batch()?;\n+    for i in 0..1000u32 {\n+        batch.put(\n+            0,\n+            i.to_be_bytes().to_vec(),\n+            (i * 2).to_be_bytes().to_vec().into(),\n+        )?;\n+    }\n+    db.commit_write_batch(batch)?;\n+\n+    // Fetch a large batch (every 10th entry)\n+    let keys_to_fetch: Vec<Vec<u8>> = (0..1000u32)\n+        .filter(|i| i % 10 == 0)\n+        .map(|i| i.to_be_bytes().to_vec())\n+        .collect();\n+    let results = db.batch_get(0, &keys_to_fetch)?;\n+\n+    assert_eq!(results.len(), 100);\n+    for (idx, i) in (0..1000u32).filter(|i| i % 10 == 0).enumerate() {\n+        assert_eq!(\n+            results[idx].as_deref(),\n+            Some(&(i * 2).to_be_bytes()[..]),\n+            \"Failed at index {idx} for key {i}\"\n+        );\n+    }\n+\n+    db.shutdown()?;\n+    Ok(())\n+}\n+\n+#[test]\n+fn batch_get_different_sizes() -> Result<()> {\n+    let tempdir = tempfile::tempdir()?;\n+    let path = tempdir.path();\n+\n+    let db = TurboPersistence::<_, 16>::open_with_parallel_scheduler(\n+        path.to_path_buf(),\n+        RayonParallelScheduler,\n+    )?;\n+\n+    // Write values of different sizes\n+    let batch = db.write_batch()?;\n+    batch.put(0, vec![1u8], vec![1u8; 10].into())?; // small\n+    batch.put(0, vec![2u8], vec![2u8; 1024].into())?; // medium\n+    batch.put(0, vec![3u8], vec![3u8; 10 * 1024].into())?; // larger\n+    db.commit_write_batch(batch)?;\n+\n+    // Fetch all with different sizes\n+    let keys_to_fetch = vec![vec![1u8], vec![2u8], vec![3u8], vec![4u8]];\n+    let results = db.batch_get(0, &keys_to_fetch)?;\n+\n+    assert_eq!(results.len(), 4);\n+    assert_eq!(results[0].as_deref(), Some(&vec![1u8; 10][..]));\n+    assert_eq!(results[1].as_deref(), Some(&vec![2u8; 1024][..]));\n+    assert_eq!(results[2].as_deref(), Some(&vec![3u8; 10 * 1024][..]));\n+    assert_eq!(results[3], None);\n+\n+    db.shutdown()?;\n+    Ok(())\n+}\n+\n+#[test]\n+fn batch_get_across_families() -> Result<()> {\n+    let tempdir = tempfile::tempdir()?;\n+    let path = tempdir.path();\n+\n+    let db = TurboPersistence::<_, 16>::open_with_parallel_scheduler(\n+        path.to_path_buf(),\n+        RayonParallelScheduler,\n+    )?;\n+\n+    // Write to multiple families\n+    let batch = db.write_batch()?;\n+    for family in 0..4u32 {\n+        for i in 0..20u8 {\n+            batch.put(family, vec![i], vec![family as u8, i].into())?;\n+        }\n+    }\n+    db.commit_write_batch(batch)?;\n+\n+    // Fetch from each family separately\n+    for family in 0..4usize {\n+        let keys_to_fetch: Vec<Vec<u8>> = (0..20u8).map(|i| vec![i]).collect();\n+        let results = db.batch_get(family, &keys_to_fetch)?;\n+\n+        assert_eq!(results.len(), 20);\n+        for (i, result) in results.iter().enumerate() {\n+            assert_eq!(\n+                result.as_deref(),\n+                Some(&vec![family as u8, i as u8][..]),\n+                \"Failed at family {family}, index {i}\"\n+            );\n+        }\n+    }\n+\n+    // Verify family isolation - keys from family 0 shouldn't be in family 1\n+    let keys_to_fetch: Vec<Vec<u8>> = (0..20u8).map(|i| vec![i]).collect();\n+    let results_f0 = db.batch_get(0, &keys_to_fetch)?;\n+    let results_f1 = db.batch_get(1, &keys_to_fetch)?;\n+\n+    // Same keys, but different values per family\n+    assert_ne!(results_f0[0].as_deref(), results_f1[0].as_deref());\n+\n+    db.shutdown()?;\n+    Ok(())\n+}\n+\n+#[test]\n+fn batch_get_after_compaction() -> Result<()> {\n+    let tempdir = tempfile::tempdir()?;\n+    let path = tempdir.path();\n+\n+    let db = TurboPersistence::<_, 16>::open_with_parallel_scheduler(\n+        path.to_path_buf(),\n+        RayonParallelScheduler,\n+    )?;\n+\n+    // Write data across multiple batches to create multiple SST files\n+    for batch_num in 0..5u8 {\n+        let batch = db.write_batch()?;\n+        for i in 0..20u8 {\n+            let key = batch_num * 20 + i;\n+            batch.put(0, vec![key], vec![key].into())?;\n+        }\n+        db.commit_write_batch(batch)?;\n+    }\n+\n+    // Fetch before compaction\n+    let keys_to_fetch: Vec<Vec<u8>> = (0..100u8).map(|i| vec![i]).collect();\n+    let results_before = db.batch_get(0, &keys_to_fetch)?;\n+\n+    // Compact database\n+    db.full_compact()?;\n+\n+    // Fetch after compaction\n+    let results_after = db.batch_get(0, &keys_to_fetch)?;\n+\n+    // Results should be identical\n+    assert_eq!(results_before.len(), results_after.len());\n+    for i in 0..100 {\n+        assert_eq!(\n+            results_before[i].as_deref(),\n+            results_after[i].as_deref(),\n+            \"Mismatch at index {i}\"\n+        );\n+        assert_eq!(results_after[i].as_deref(), Some(&[i as u8][..]));\n+    }\n+\n+    db.shutdown()?;\n+    Ok(())\n+}\n+\n+#[test]\n+fn batch_get_with_overwrites() -> Result<()> {\n+    let tempdir = tempfile::tempdir()?;\n+    let path = tempdir.path();\n+\n+    let db = TurboPersistence::<_, 16>::open_with_parallel_scheduler(\n+        path.to_path_buf(),\n+        RayonParallelScheduler,\n+    )?;\n+\n+    // Write initial data\n+    let batch = db.write_batch()?;\n+    for i in 0..50u8 {\n+        batch.put(0, vec![i], vec![i].into())?;\n+    }\n+    db.commit_write_batch(batch)?;\n+\n+    // Overwrite some keys\n+    let batch = db.write_batch()?;\n+    for i in 0..25u8 {\n+        batch.put(0, vec![i], vec![i + 100].into())?;\n+    }\n+    db.commit_write_batch(batch)?;\n+\n+    // Fetch all keys\n+    let keys_to_fetch: Vec<Vec<u8>> = (0..50u8).map(|i| vec![i]).collect();\n+    let results = db.batch_get(0, &keys_to_fetch)?;\n+\n+    assert_eq!(results.len(), 50);\n+    // First 25 should have new values\n+    for (i, result) in results.iter().enumerate().take(25) {\n+        assert_eq!(\n+            result.as_deref(),\n+            Some(&[i as u8 + 100][..]),\n+            \"Failed at index {i}\"\n+        );\n+    }\n+    // Last 25 should have original values\n+    for (i, result) in results.iter().enumerate().skip(25) {\n+        assert_eq!(\n+            result.as_deref(),\n+            Some(&[i as u8][..]),\n+            \"Failed at index {i}\"\n+        );\n+    }\n+\n+    db.shutdown()?;\n+    Ok(())\n+}\n+\n+#[test]\n+fn batch_get_comparison_with_get() -> Result<()> {\n+    let tempdir = tempfile::tempdir()?;\n+    let path = tempdir.path();\n+\n+    let db = TurboPersistence::<_, 16>::open_with_parallel_scheduler(\n+        path.to_path_buf(),\n+        RayonParallelScheduler,\n+    )?;\n+\n+    // Write test data\n+    let batch = db.write_batch()?;\n+    for i in 0..100u32 {\n+        batch.put(\n+            0,\n+            i.to_be_bytes().to_vec(),\n+            (i * 3).to_be_bytes().to_vec().into(),\n+        )?;\n+    }\n+    db.commit_write_batch(batch)?;\n+\n+    // Prepare keys\n+    let keys_to_fetch: Vec<Vec<u8>> = (0..150u32)\n+        .filter(|i| i % 3 == 0)\n+        .map(|i| i.to_be_bytes().to_vec())\n+        .collect();\n+\n+    // Get results using batch_get\n+    let batch_results = db.batch_get(0, &keys_to_fetch)?;\n+\n+    // Get results using individual get calls\n+    let mut individual_results = Vec::new();\n+    for key in &keys_to_fetch {\n+        individual_results.push(db.get(0, key)?);\n+    }\n+\n+    // Compare results\n+    assert_eq!(batch_results.len(), individual_results.len());\n+    for (i, (batch_result, individual_result)) in batch_results\n+        .iter()\n+        .zip(individual_results.iter())\n+        .enumerate()\n+    {\n+        assert_eq!(\n+            batch_result.as_deref(),\n+            individual_result.as_deref(),\n+            \"Mismatch at index {i}\"\n+        );\n+    }\n+\n+    db.shutdown()?;\n+    Ok(())\n+}\n+\n+#[test]\n+fn batch_get_after_restore() -> Result<()> {\n+    let tempdir = tempfile::tempdir()?;\n+    let path = tempdir.path();\n+\n+    // Write data and close\n+    {\n+        let db = TurboPersistence::<_, 16>::open_with_parallel_scheduler(\n+            path.to_path_buf(),\n+            RayonParallelScheduler,\n+        )?;\n+\n+        let batch = db.write_batch()?;\n+        for i in 0..100u8 {\n+            batch.put(0, vec![i], vec![i, i + 1].into())?;\n+        }\n+        db.commit_write_batch(batch)?;\n+        db.shutdown()?;\n+    }\n+\n+    // Reopen and test batch_get\n+    {\n+        let db = TurboPersistence::<_, 16>::open_with_parallel_scheduler(\n+            path.to_path_buf(),\n+            RayonParallelScheduler,\n+        )?;\n+\n+        let keys_to_fetch: Vec<Vec<u8>> = (0..100u8).step_by(5).map(|i| vec![i]).collect();\n+        let results = db.batch_get(0, &keys_to_fetch)?;\n+\n+        assert_eq!(results.len(), 20);\n+        for (idx, i) in (0..100u8).step_by(5).enumerate() {\n+            assert_eq!(\n+                results[idx].as_deref(),\n+                Some(&vec![i, i + 1][..]),\n+                \"Failed at index {idx} for key {i}\"\n+            );\n+        }\n+\n+        db.shutdown()?;\n+    }\n+\n+    Ok(())\n+}"
        },
        {
            "sha": "e6a14037899a7d2c67589c0e21c444cbe835d6a0",
            "filename": "turbopack/crates/turbo-tasks-backend/src/backend/mod.rs",
            "status": "modified",
            "additions": 19,
            "deletions": 47,
            "changes": 66,
            "blob_url": "https://github.com/vercel/next.js/blob/1d09f42590ef47dfde7061f3a01621654f2a14df/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Fmod.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/1d09f42590ef47dfde7061f3a01621654f2a14df/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Fmod.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Fmod.rs?ref=1d09f42590ef47dfde7061f3a01621654f2a14df",
            "patch": "@@ -4,12 +4,10 @@ mod storage;\n \n use std::{\n     borrow::Cow,\n-    cmp::min,\n     fmt::{self, Write},\n     future::Future,\n     hash::BuildHasherDefault,\n     mem::take,\n-    ops::Range,\n     pin::Pin,\n     sync::{\n         Arc, LazyLock,\n@@ -26,8 +24,8 @@ use smallvec::{SmallVec, smallvec};\n use tokio::time::{Duration, Instant};\n use tracing::{Span, trace_span};\n use turbo_tasks::{\n-    CellId, FxDashMap, FxIndexMap, KeyValuePair, RawVc, ReadCellOptions, ReadConsistency,\n-    ReadOutputOptions, ReadTracking, TRANSIENT_TASK_BIT, TaskExecutionReason, TaskId, TraitTypeId,\n+    CellId, FxDashMap, KeyValuePair, RawVc, ReadCellOptions, ReadConsistency, ReadOutputOptions,\n+    ReadTracking, TRANSIENT_TASK_BIT, TaskExecutionReason, TaskId, TraitTypeId,\n     TurboTasksBackendApi, ValueTypeId,\n     backend::{\n         Backend, CachedTaskType, CellContent, TaskExecutionSpec, TransientTaskRoot,\n@@ -39,7 +37,7 @@ use turbo_tasks::{\n     task_statistics::TaskStatisticsApi,\n     trace::TraceRawVcs,\n     turbo_tasks,\n-    util::{IdFactoryWithReuse, good_chunk_size},\n+    util::IdFactoryWithReuse,\n };\n \n pub use self::{operation::AnyOperation, storage::TaskDataCategory};\n@@ -169,10 +167,6 @@ impl Default for BackendOptions {\n pub enum TurboTasksBackendJob {\n     InitialSnapshot,\n     FollowUpSnapshot,\n-    Prefetch {\n-        data: Arc<FxIndexMap<TaskId, bool>>,\n-        range: Option<Range<usize>>,\n-    },\n }\n \n pub struct TurboTasksBackend<B: BackingStorage>(Arc<TurboTasksBackendInner<B>>);\n@@ -1643,6 +1637,11 @@ impl<B: BackingStorage> TurboTasksBackendInner<B> {\n         {\n             let mut ctx = self.execute_context(turbo_tasks);\n             let mut task = ctx.task(task_id, TaskDataCategory::All);\n+            if let Some(tasks) = task.prefetch() {\n+                drop(task);\n+                ctx.prepare_tasks(tasks);\n+                task = ctx.task(task_id, TaskDataCategory::All);\n+            }\n             let in_progress = remove!(task, InProgress)?;\n             let InProgressState::Scheduled { done_event, reason } = in_progress else {\n                 task.add_new(CachedDataItem::InProgress { value: in_progress });\n@@ -2171,6 +2170,14 @@ impl<B: BackingStorage> TurboTasksBackendInner<B> {\n     ) {\n         debug_assert!(!output_dependent_tasks.is_empty());\n \n+        if output_dependent_tasks.len() > 1 {\n+            ctx.prepare_tasks(\n+                output_dependent_tasks\n+                    .iter()\n+                    .map(|&id| (id, TaskDataCategory::All)),\n+            );\n+        }\n+\n         let mut queue = AggregationUpdateQueue::new();\n         for dependent_task_id in output_dependent_tasks {\n             #[cfg(feature = \"trace_task_output_dependencies\")]\n@@ -2228,9 +2235,9 @@ impl<B: BackingStorage> TurboTasksBackendInner<B> {\n         debug_assert!(!new_children.is_empty());\n \n         let mut queue = AggregationUpdateQueue::new();\n-        for &child_id in new_children {\n-            let child_task = ctx.task(child_id, TaskDataCategory::Meta);\n+        ctx.for_each_task_meta(new_children.iter().copied(), |child_task, ctx| {\n             if !child_task.has_key(&CachedDataItemKey::Output {}) {\n+                let child_id = child_task.id();\n                 make_task_dirty_internal(\n                     child_task,\n                     child_id,\n@@ -2241,7 +2248,7 @@ impl<B: BackingStorage> TurboTasksBackendInner<B> {\n                     ctx,\n                 );\n             }\n-        }\n+        });\n \n         queue.execute(ctx);\n     }\n@@ -2579,41 +2586,6 @@ impl<B: BackingStorage> TurboTasksBackendInner<B> {\n                         }\n                     }\n                 }\n-                TurboTasksBackendJob::Prefetch { data, range } => {\n-                    let range: Range<usize> = if let Some(range) = range {\n-                        range\n-                    } else {\n-                        if data.len() > 128 {\n-                            let chunk_size = good_chunk_size(data.len());\n-                            let chunks = data.len().div_ceil(chunk_size);\n-                            for i in 0..chunks {\n-                                turbo_tasks.schedule_backend_background_job(\n-                                    TurboTasksBackendJob::Prefetch {\n-                                        data: data.clone(),\n-                                        range: Some(\n-                                            (i * chunk_size)..min(data.len(), (i + 1) * chunk_size),\n-                                        ),\n-                                    },\n-                                );\n-                            }\n-                            return;\n-                        }\n-                        0..data.len()\n-                    };\n-\n-                    let _span = trace_span!(\"prefetching\").entered();\n-                    let mut ctx = self.execute_context(turbo_tasks);\n-                    for i in range {\n-                        let (&task, &with_data) = data.get_index(i).unwrap();\n-                        let category = if with_data {\n-                            TaskDataCategory::All\n-                        } else {\n-                            TaskDataCategory::Meta\n-                        };\n-                        // Prefetch the task\n-                        drop(ctx.task(task, category));\n-                    }\n-                }\n             }\n         })\n     }"
        },
        {
            "sha": "00f89b7a8118fe513dcfc445d84afc46140339ad",
            "filename": "turbopack/crates/turbo-tasks-backend/src/backend/operation/aggregation_update.rs",
            "status": "modified",
            "additions": 11,
            "deletions": 23,
            "changes": 34,
            "blob_url": "https://github.com/vercel/next.js/blob/1d09f42590ef47dfde7061f3a01621654f2a14df/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Foperation%2Faggregation_update.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/1d09f42590ef47dfde7061f3a01621654f2a14df/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Foperation%2Faggregation_update.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Foperation%2Faggregation_update.rs?ref=1d09f42590ef47dfde7061f3a01621654f2a14df",
            "patch": "@@ -1501,12 +1501,8 @@ impl AggregationUpdateQueue {\n         ctx: &mut impl ExecuteContext,\n         update: AggregatedDataUpdate,\n     ) {\n-        for upper_id in upper_ids {\n-            let mut upper = ctx.task(\n-                upper_id,\n-                // For performance reasons this should stay `Meta` and not `All`\n-                TaskDataCategory::Meta,\n-            );\n+        // For performance reasons this should stay `Meta` and not `All`\n+        ctx.for_each_task_meta(upper_ids.iter().copied(), |mut upper, ctx| {\n             let diff = update.apply(&mut upper, ctx.should_track_activeness(), self);\n             if !diff.is_empty() {\n                 let upper_ids = get_uppers(&upper);\n@@ -1520,7 +1516,7 @@ impl AggregationUpdateQueue {\n                     );\n                 }\n             }\n-        }\n+        });\n     }\n \n     fn inner_of_uppers_lost_follower(\n@@ -1565,13 +1561,9 @@ impl AggregationUpdateQueue {\n                 drop(follower);\n \n                 if !data.is_empty() {\n-                    for upper_id in removed_uppers.iter() {\n-                        // remove data from upper\n-                        let mut upper = ctx.task(\n-                            *upper_id,\n-                            // For performance reasons this should stay `Meta` and not `All`\n-                            TaskDataCategory::Meta,\n-                        );\n+                    // remove data from upper\n+                    // For performance reasons this should stay `Meta` and not `All`\n+                    ctx.for_each_task_meta(removed_uppers.iter().copied(), |mut upper, ctx| {\n                         let diff = data.apply(&mut upper, ctx.should_track_activeness(), self);\n                         if !diff.is_empty() {\n                             let upper_ids = get_uppers(&upper);\n@@ -1583,7 +1575,7 @@ impl AggregationUpdateQueue {\n                                 .into(),\n                             )\n                         }\n-                    }\n+                    });\n                 }\n                 if !followers.is_empty() {\n                     self.push(\n@@ -1956,13 +1948,9 @@ impl AggregationUpdateQueue {\n \n                 let has_data = !data.is_empty();\n                 if has_data || !is_active {\n-                    for upper_id in upper_ids.iter() {\n-                        // add data to upper\n-                        let mut upper = ctx.task(\n-                            *upper_id,\n-                            // For performance reasons this should stay `Meta` and not `All`\n-                            TaskDataCategory::Meta,\n-                        );\n+                    // add data to upper\n+                    // For performance reasons this should stay `Meta` and not `All`\n+                    ctx.for_each_task_meta(upper_ids.iter().copied(), |mut upper, ctx| {\n                         if has_data {\n                             let diff = data.apply(&mut upper, ctx.should_track_activeness(), self);\n                             if !diff.is_empty() {\n@@ -1983,7 +1971,7 @@ impl AggregationUpdateQueue {\n                                 is_active = true;\n                             }\n                         }\n-                    }\n+                    });\n                 }\n                 if !children.is_empty() {\n                     self.push("
        },
        {
            "sha": "3415ac61630c01179d586e8561323a5a8e0b2636",
            "filename": "turbopack/crates/turbo-tasks-backend/src/backend/operation/mod.rs",
            "status": "modified",
            "additions": 293,
            "deletions": 40,
            "changes": 333,
            "blob_url": "https://github.com/vercel/next.js/blob/1d09f42590ef47dfde7061f3a01621654f2a14df/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Foperation%2Fmod.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/1d09f42590ef47dfde7061f3a01621654f2a14df/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Foperation%2Fmod.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Foperation%2Fmod.rs?ref=1d09f42590ef47dfde7061f3a01621654f2a14df",
            "patch": "@@ -10,7 +10,7 @@ mod update_collectible;\n use std::{\n     fmt::{Debug, Formatter},\n     mem::transmute,\n-    sync::{Arc, atomic::Ordering},\n+    sync::atomic::Ordering,\n };\n \n use bincode::{Decode, Encode};\n@@ -21,10 +21,9 @@ use turbo_tasks::{\n use crate::{\n     backend::{\n         OperationGuard, TaskDataCategory, TransientTask, TurboTasksBackend, TurboTasksBackendInner,\n-        TurboTasksBackendJob,\n         storage::{SpecificTaskDataCategory, StorageWriteGuard, get, iter_many, remove},\n     },\n-    backing_storage::BackingStorage,\n+    backing_storage::{BackingStorage, BackingStorageSealed},\n     data::{\n         CachedDataItem, CachedDataItemKey, CachedDataItemType, CachedDataItemValue,\n         CachedDataItemValueRef, CachedDataItemValueRefMut, Dirtyness,\n@@ -50,6 +49,27 @@ pub trait ExecuteContext<'e>: Sized {\n     where\n         'e: 'l;\n     fn task(&mut self, task_id: TaskId, category: TaskDataCategory) -> Self::TaskGuardImpl;\n+    /// Prepares (as in fetches from persistent storage) a list of tasks.\n+    /// The iterator should not have duplicates, as this would cause over-fetching.\n+    fn prepare_tasks(\n+        &mut self,\n+        task_ids: impl IntoIterator<Item = (TaskId, TaskDataCategory)> + Clone,\n+    );\n+    fn for_each_task(\n+        &mut self,\n+        task_ids: impl IntoIterator<Item = (TaskId, TaskDataCategory)>,\n+        func: impl FnMut(Self::TaskGuardImpl, &mut Self),\n+    );\n+    fn for_each_task_meta(\n+        &mut self,\n+        task_ids: impl IntoIterator<Item = TaskId>,\n+        func: impl FnMut(Self::TaskGuardImpl, &mut Self),\n+    ) {\n+        self.for_each_task(\n+            task_ids.into_iter().map(|id| (id, TaskDataCategory::Meta)),\n+            func,\n+        )\n+    }\n     fn is_once_task(&self, task_id: TaskId) -> bool;\n     fn task_pair(\n         &mut self,\n@@ -83,7 +103,7 @@ where\n     _operation_guard: Option<OperationGuard<'e, B>>,\n     transaction: TransactionState<'e, 'tx, B>,\n     #[cfg(debug_assertions)]\n-    active_task_locks: Arc<std::sync::atomic::AtomicU8>,\n+    active_task_locks: std::sync::Arc<std::sync::atomic::AtomicU8>,\n }\n \n impl<'e, 'tx, B: BackingStorage> ExecuteContextImpl<'e, 'tx, B>\n@@ -100,7 +120,7 @@ where\n             _operation_guard: Some(backend.start_operation()),\n             transaction: TransactionState::None,\n             #[cfg(debug_assertions)]\n-            active_task_locks: Arc::new(std::sync::atomic::AtomicU8::new(0)),\n+            active_task_locks: std::sync::Arc::new(std::sync::atomic::AtomicU8::new(0)),\n         }\n     }\n \n@@ -115,21 +135,16 @@ where\n             _operation_guard: Some(backend.start_operation()),\n             transaction: TransactionState::Borrowed(transaction),\n             #[cfg(debug_assertions)]\n-            active_task_locks: Arc::new(std::sync::atomic::AtomicU8::new(0)),\n+            active_task_locks: std::sync::Arc::new(std::sync::atomic::AtomicU8::new(0)),\n         }\n     }\n \n-    fn restore_task_data(\n-        &mut self,\n-        task_id: TaskId,\n-        category: TaskDataCategory,\n-    ) -> Vec<CachedDataItem> {\n+    fn ensure_transaction(&mut self) -> bool {\n         if matches!(self.transaction, TransactionState::None) {\n             let check_backing_storage = self.backend.should_restore()\n                 && self.backend.local_is_partial.load(Ordering::Acquire);\n             if !check_backing_storage {\n-                // If we don't need to restore, we can just return an empty vector\n-                return Vec::new();\n+                return false;\n             }\n             let tx = self.backend.backing_storage.start_read_transaction();\n             let tx = tx.map(|tx| {\n@@ -138,11 +153,19 @@ where\n             });\n             self.transaction = TransactionState::Owned(tx);\n         }\n-        let tx = match &self.transaction {\n-            TransactionState::None => unreachable!(),\n-            TransactionState::Borrowed(tx) => *tx,\n-            TransactionState::Owned(tx) => tx.as_ref(),\n-        };\n+        true\n+    }\n+\n+    fn restore_task_data(\n+        &mut self,\n+        task_id: TaskId,\n+        category: TaskDataCategory,\n+    ) -> Vec<CachedDataItem> {\n+        if !self.ensure_transaction() {\n+            // If we don't need to restore, we can just return an empty vector\n+            return Vec::new();\n+        }\n+        let tx = self.get_tx();\n         // Safety: `tx` is a valid transaction from `self.backend.backing_storage`.\n         let result = unsafe {\n             self.backend\n@@ -160,6 +183,210 @@ where\n             }\n         }\n     }\n+\n+    fn restore_task_data_batch(\n+        &mut self,\n+        task_ids: &[TaskId],\n+        category: TaskDataCategory,\n+    ) -> Option<Vec<Vec<CachedDataItem>>> {\n+        debug_assert!(task_ids.len() > 1, \"Use restore_task_data for single task\");\n+        if !self.ensure_transaction() {\n+            // If we don't need to restore, we return None\n+            return None;\n+        }\n+        let tx = self.get_tx();\n+        // Safety: `tx` is a valid transaction from `self.backend.backing_storage`.\n+        let result = unsafe {\n+            self.backend\n+                .backing_storage\n+                .batch_lookup_data(tx, task_ids, category)\n+        };\n+        match result {\n+            Ok(result) => Some(result),\n+            Err(e) => {\n+                panic!(\n+                    \"Failed to restore task data (corrupted database or bug): {:?}\",\n+                    e.context(format!(\n+                        \"{category:?} for batch of {} tasks\",\n+                        task_ids.len()\n+                    ))\n+                )\n+            }\n+        }\n+    }\n+\n+    fn get_tx(&self) -> Option<&<B as BackingStorageSealed>::ReadTransaction<'tx>> {\n+        match &self.transaction {\n+            TransactionState::None => unreachable!(),\n+            TransactionState::Borrowed(tx) => *tx,\n+            TransactionState::Owned(tx) => tx.as_ref(),\n+        }\n+    }\n+\n+    fn prepare_tasks_with_callback(\n+        &mut self,\n+        task_ids: impl IntoIterator<Item = (TaskId, TaskDataCategory)>,\n+        call_prepared_task_callback_for_transient_tasks: bool,\n+        mut prepared_task_callback: impl FnMut(\n+            &mut Self,\n+            TaskId,\n+            TaskDataCategory,\n+            StorageWriteGuard<'e>,\n+        ),\n+    ) {\n+        let mut data_count = 0;\n+        let mut meta_count = 0;\n+        let mut all_count = 0;\n+        let mut tasks = task_ids\n+            .into_iter()\n+            .filter(|&(id, category)| {\n+                if id.is_transient() {\n+                    if call_prepared_task_callback_for_transient_tasks {\n+                        let mut task = self.backend.storage.access_mut(id);\n+                        if !task.state().is_restored(category) {\n+                            task.state_mut().set_restored(TaskDataCategory::All);\n+                        }\n+                        prepared_task_callback(self, id, category, task);\n+                    }\n+                    false\n+                } else {\n+                    true\n+                }\n+            })\n+            .inspect(|(_, category)| match category {\n+                TaskDataCategory::Data => data_count += 1,\n+                TaskDataCategory::Meta => meta_count += 1,\n+                TaskDataCategory::All => all_count += 1,\n+            })\n+            .map(|(id, category)| (id, category, None, None))\n+            .collect::<Vec<_>>();\n+        data_count += all_count;\n+        meta_count += all_count;\n+\n+        let mut tasks_to_restore_for_data = Vec::with_capacity(data_count);\n+        let mut tasks_to_restore_for_data_indicies = Vec::with_capacity(data_count);\n+        let mut tasks_to_restore_for_meta = Vec::with_capacity(meta_count);\n+        let mut tasks_to_restore_for_meta_indicies = Vec::with_capacity(meta_count);\n+        for (i, &(task_id, category, _, _)) in tasks.iter().enumerate() {\n+            #[cfg(debug_assertions)]\n+            if self.active_task_locks.fetch_add(1, Ordering::AcqRel) != 0 {\n+                panic!(\n+                    \"Concurrent task lock acquisition detected. This is not allowed and indicates \\\n+                     a bug. It can lead to deadlocks.\"\n+                );\n+            }\n+\n+            let task = self.backend.storage.access_mut(task_id);\n+            let mut ready = true;\n+            if matches!(category, TaskDataCategory::Data | TaskDataCategory::All)\n+                && !task.state().is_restored(TaskDataCategory::Data)\n+            {\n+                tasks_to_restore_for_data.push(task_id);\n+                tasks_to_restore_for_data_indicies.push(i);\n+                ready = false;\n+            }\n+            if matches!(category, TaskDataCategory::Meta | TaskDataCategory::All)\n+                && !task.state().is_restored(TaskDataCategory::Meta)\n+            {\n+                tasks_to_restore_for_meta.push(task_id);\n+                tasks_to_restore_for_meta_indicies.push(i);\n+                ready = false;\n+            }\n+            if ready {\n+                prepared_task_callback(self, task_id, category, task);\n+            }\n+            #[cfg(debug_assertions)]\n+            self.active_task_locks.fetch_sub(1, Ordering::AcqRel);\n+        }\n+        if tasks_to_restore_for_meta.is_empty() && tasks_to_restore_for_data.is_empty() {\n+            return;\n+        }\n+\n+        match tasks_to_restore_for_data.len() {\n+            0 => {}\n+            1 => {\n+                let task_id = tasks_to_restore_for_data[0];\n+                let data = self.restore_task_data(task_id, TaskDataCategory::Data);\n+                let idx = tasks_to_restore_for_data_indicies[0];\n+                tasks[idx].2 = Some(data);\n+            }\n+            _ => {\n+                if let Some(data) =\n+                    self.restore_task_data_batch(&tasks_to_restore_for_data, TaskDataCategory::Data)\n+                {\n+                    data.into_iter()\n+                        .zip(tasks_to_restore_for_data_indicies)\n+                        .for_each(|(items, idx)| {\n+                            tasks[idx].2 = Some(items);\n+                        });\n+                } else {\n+                    for idx in tasks_to_restore_for_data_indicies {\n+                        tasks[idx].2 = Some(Vec::new());\n+                    }\n+                }\n+            }\n+        }\n+        match tasks_to_restore_for_meta.len() {\n+            0 => {}\n+            1 => {\n+                let task_id = tasks_to_restore_for_meta[0];\n+                let data = self.restore_task_data(task_id, TaskDataCategory::Meta);\n+                let idx = tasks_to_restore_for_meta_indicies[0];\n+                tasks[idx].3 = Some(data);\n+            }\n+            _ => {\n+                if let Some(data) =\n+                    self.restore_task_data_batch(&tasks_to_restore_for_meta, TaskDataCategory::Meta)\n+                {\n+                    data.into_iter()\n+                        .zip(tasks_to_restore_for_meta_indicies)\n+                        .for_each(|(items, idx)| {\n+                            tasks[idx].3 = Some(items);\n+                        });\n+                } else {\n+                    for idx in tasks_to_restore_for_meta_indicies {\n+                        tasks[idx].3 = Some(Vec::new());\n+                    }\n+                }\n+            }\n+        }\n+\n+        for (task_id, category, items_for_data, items_for_meta) in tasks {\n+            if items_for_data.is_none() && items_for_meta.is_none() {\n+                continue;\n+            }\n+            #[cfg(debug_assertions)]\n+            if self.active_task_locks.fetch_add(1, Ordering::AcqRel) != 0 {\n+                panic!(\n+                    \"Concurrent task lock acquisition detected. This is not allowed and indicates \\\n+                     a bug. It can lead to deadlocks.\"\n+                );\n+            }\n+\n+            let mut task = self.backend.storage.access_mut(task_id);\n+            if let Some(items) = items_for_data\n+                && !task.state().is_restored(TaskDataCategory::Data)\n+            {\n+                // TODO store items groups by type to be able to use extend here\n+                for item in items {\n+                    task.add(item);\n+                }\n+                task.state_mut().set_restored(TaskDataCategory::Data);\n+            }\n+            if let Some(items) = items_for_meta\n+                && !task.state().is_restored(TaskDataCategory::Meta)\n+            {\n+                // TODO store items groups by type to be able to use extend here\n+                for item in items {\n+                    task.add(item);\n+                }\n+                task.state_mut().set_restored(TaskDataCategory::Meta);\n+            }\n+            prepared_task_callback(self, task_id, category, task);\n+            #[cfg(debug_assertions)]\n+            self.active_task_locks.fetch_sub(1, Ordering::AcqRel);\n+        }\n+    }\n }\n \n impl<'e, 'tx, B: BackingStorage> ExecuteContext<'e> for ExecuteContextImpl<'e, 'tx, B>\n@@ -220,6 +447,38 @@ where\n         }\n     }\n \n+    fn prepare_tasks(&mut self, task_ids: impl IntoIterator<Item = (TaskId, TaskDataCategory)>) {\n+        self.prepare_tasks_with_callback(task_ids, false, |_, _, _, _| {});\n+    }\n+\n+    fn for_each_task(\n+        &mut self,\n+        task_ids: impl IntoIterator<Item = (TaskId, TaskDataCategory)>,\n+        mut func: impl FnMut(Self::TaskGuardImpl, &mut Self),\n+    ) {\n+        let backend = self.backend;\n+        #[cfg(debug_assertions)]\n+        let active_task_locks = self.active_task_locks.clone();\n+        self.prepare_tasks_with_callback(task_ids, true, |this, task_id, _category, task| {\n+            // The prepare_tasks_with_callback already increased the active_task_locks count and\n+            // checked for concurrent access but it will also decrement it again, so we\n+            // need to increase it again here as Drop will decrement it\n+            #[cfg(debug_assertions)]\n+            active_task_locks.fetch_add(1, Ordering::AcqRel);\n+\n+            let guard: TaskGuardImpl<'_, B> = TaskGuardImpl {\n+                task,\n+                task_id,\n+                backend,\n+                #[cfg(debug_assertions)]\n+                category: _category,\n+                #[cfg(debug_assertions)]\n+                active_task_locks: active_task_locks.clone(),\n+            };\n+            func(guard, this);\n+        });\n+    }\n+\n     fn is_once_task(&self, task_id: TaskId) -> bool {\n         if !task_id.is_transient() {\n             return false;\n@@ -301,14 +560,7 @@ where\n         self.schedule_task(task);\n     }\n \n-    fn schedule_task(&self, mut task: Self::TaskGuardImpl) {\n-        if let Some(tasks_to_prefetch) = task.prefetch() {\n-            self.turbo_tasks\n-                .schedule_backend_background_job(TurboTasksBackendJob::Prefetch {\n-                    data: Arc::new(tasks_to_prefetch),\n-                    range: None,\n-                });\n-        }\n+    fn schedule_task(&self, task: Self::TaskGuardImpl) {\n         self.turbo_tasks.schedule(task.id());\n     }\n \n@@ -350,7 +602,7 @@ impl<'e, B: BackingStorage> ChildExecuteContext<'e> for ChildExecuteContextImpl<\n             _operation_guard: None,\n             transaction: TransactionState::None,\n             #[cfg(debug_assertions)]\n-            active_task_locks: Arc::new(std::sync::atomic::AtomicU8::new(0)),\n+            active_task_locks: std::sync::Arc::new(std::sync::atomic::AtomicU8::new(0)),\n         }\n     }\n }\n@@ -406,7 +658,10 @@ pub trait TaskGuard: Debug {\n     where\n         F: for<'a> FnMut(CachedDataItemKey, CachedDataItemValueRef<'a>) -> bool + 'l;\n     fn invalidate_serialization(&mut self);\n-    fn prefetch(&mut self) -> Option<FxIndexMap<TaskId, bool>>;\n+    /// Determine which tasks to prefetch for a task.\n+    /// Only returns Some once per task.\n+    /// It returns a set of tasks and which info is needed.\n+    fn prefetch(&mut self) -> Option<FxIndexMap<TaskId, TaskDataCategory>>;\n     fn is_immutable(&self) -> bool;\n     fn is_dirty(&self) -> bool {\n         get!(self, Dirty).is_some_and(|dirtyness| match dirtyness {\n@@ -503,7 +758,7 @@ pub struct TaskGuardImpl<'a, B: BackingStorage> {\n     #[cfg(debug_assertions)]\n     category: TaskDataCategory,\n     #[cfg(debug_assertions)]\n-    active_task_locks: Arc<std::sync::atomic::AtomicU8>,\n+    active_task_locks: std::sync::Arc<std::sync::atomic::AtomicU8>,\n }\n \n #[cfg(debug_assertions)]\n@@ -740,18 +995,16 @@ impl<B: BackingStorage> TaskGuard for TaskGuardImpl<'_, B> {\n         }\n     }\n \n-    fn prefetch(&mut self) -> Option<FxIndexMap<TaskId, bool>> {\n-        if !self.task.state().prefetched() {\n-            self.task.state_mut().set_prefetched(true);\n-            let map = iter_many!(self, OutputDependency { target } => (target, false))\n-                .chain(iter_many!(self, CellDependency { target } => (target.task, true)))\n-                .chain(iter_many!(self, CollectiblesDependency { target } => (target.task, true)))\n-                .collect::<FxIndexMap<_, _>>();\n-            if map.len() > 16 {\n-                return Some(map);\n-            }\n+    fn prefetch(&mut self) -> Option<FxIndexMap<TaskId, TaskDataCategory>> {\n+        if self.task.state().prefetched() {\n+            return None;\n         }\n-        None\n+        self.task.state_mut().set_prefetched(true);\n+        let map = iter_many!(self, OutputDependency { target } => (target, TaskDataCategory::Meta))\n+            .chain(iter_many!(self, CellDependency { target } => (target.task, TaskDataCategory::All)))\n+            .chain(iter_many!(self, CollectiblesDependency { target } => (target.task, TaskDataCategory::All)))\n+            .collect::<FxIndexMap<_, _>>();\n+        (map.len() > 1).then_some(map)\n     }\n \n     fn is_immutable(&self) -> bool {"
        },
        {
            "sha": "e5408cb1f1c5f68edc0d6012ba83b5a1435f802e",
            "filename": "turbopack/crates/turbo-tasks-backend/src/backend/operation/update_cell.rs",
            "status": "modified",
            "additions": 14,
            "deletions": 7,
            "changes": 21,
            "blob_url": "https://github.com/vercel/next.js/blob/1d09f42590ef47dfde7061f3a01621654f2a14df/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Foperation%2Fupdate_cell.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/1d09f42590ef47dfde7061f3a01621654f2a14df/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Foperation%2Fupdate_cell.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Foperation%2Fupdate_cell.rs?ref=1d09f42590ef47dfde7061f3a01621654f2a14df",
            "patch": "@@ -15,7 +15,7 @@ use crate::{\n             AggregationUpdateQueue, ExecuteContext, Operation, TaskGuard,\n             invalidate::make_task_dirty_internal,\n         },\n-        storage::{get_many, remove},\n+        storage::{iter_many, remove},\n     },\n     data::{CachedDataItem, CachedDataItemKey, CellRef},\n };\n@@ -96,12 +96,17 @@ impl UpdateCellOperation {\n             // When not recomputing, we need to notify dependent tasks if the content actually\n             // changes.\n \n-            let dependent_tasks: SmallVec<[TaskId; 4]> = get_many!(\n+            let dependent_tasks: SmallVec<[TaskId; 4]> = iter_many!(\n                 task,\n                 CellDependent { cell: dependent_cell, task }\n                 if dependent_cell == cell\n                 => task\n-            );\n+            )\n+            .filter(|&dependent_task_id| {\n+                // once tasks are never invalidated\n+                !ctx.is_once_task(dependent_task_id)\n+            })\n+            .collect();\n \n             if !dependent_tasks.is_empty() {\n                 // Slow path: We need to invalidate tasks depending on this cell.\n@@ -125,6 +130,12 @@ impl UpdateCellOperation {\n                 drop(task);\n                 drop(old_content);\n \n+                ctx.prepare_tasks(\n+                    dependent_tasks\n+                        .iter()\n+                        .map(|&id| (id, TaskDataCategory::All)),\n+                );\n+\n                 UpdateCellOperation::InvalidateWhenCellDependency {\n                     is_serializable_cell_content,\n                     cell_ref: CellRef {\n@@ -197,10 +208,6 @@ impl Operation for UpdateCellOperation {\n                     ref mut queue,\n                 } => {\n                     if let Some(dependent_task_id) = dependent_tasks.pop() {\n-                        if ctx.is_once_task(dependent_task_id) {\n-                            // once tasks are never invalidated\n-                            continue;\n-                        }\n                         let mut make_stale = true;\n                         let dependent = ctx.task(dependent_task_id, TaskDataCategory::All);\n                         if dependent.has_key(&CachedDataItemKey::OutdatedCellDependency {"
        },
        {
            "sha": "88fae40857afd7ae2014efa67c5782fa83871d0d",
            "filename": "turbopack/crates/turbo-tasks-backend/src/backing_storage.rs",
            "status": "modified",
            "additions": 35,
            "deletions": 0,
            "changes": 35,
            "blob_url": "https://github.com/vercel/next.js/blob/1d09f42590ef47dfde7061f3a01621654f2a14df/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbacking_storage.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/1d09f42590ef47dfde7061f3a01621654f2a14df/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbacking_storage.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbacking_storage.rs?ref=1d09f42590ef47dfde7061f3a01621654f2a14df",
            "patch": "@@ -87,6 +87,23 @@ pub trait BackingStorageSealed: 'static + Send + Sync {\n         task_id: TaskId,\n         category: TaskDataCategory,\n     ) -> Result<Vec<CachedDataItem>>;\n+    /// # Safety\n+    ///\n+    /// `tx` must be a transaction from this BackingStorage instance.\n+    unsafe fn batch_lookup_data(\n+        &self,\n+        tx: Option<&Self::ReadTransaction<'_>>,\n+        task_ids: &[TaskId],\n+        category: TaskDataCategory,\n+    ) -> Result<Vec<Vec<CachedDataItem>>> {\n+        let mut results = Vec::with_capacity(task_ids.len());\n+        for &task_id in task_ids {\n+            // TODO more efficient batch implementation\n+            let data = unsafe { self.lookup_data(tx, task_id, category)? };\n+            results.push(data);\n+        }\n+        Ok(results)\n+    }\n \n     fn shutdown(&self) -> Result<()> {\n         Ok(())\n@@ -204,6 +221,24 @@ where\n         }\n     }\n \n+    unsafe fn batch_lookup_data(\n+        &self,\n+        tx: Option<&Self::ReadTransaction<'_>>,\n+        task_ids: &[TaskId],\n+        category: TaskDataCategory,\n+    ) -> Result<Vec<Vec<CachedDataItem>>> {\n+        match self {\n+            Either::Left(this) => {\n+                let tx = tx.map(|tx| read_transaction_left_or_panic(tx.as_ref()));\n+                unsafe { this.batch_lookup_data(tx, task_ids, category) }\n+            }\n+            Either::Right(this) => {\n+                let tx = tx.map(|tx| read_transaction_right_or_panic(tx.as_ref()));\n+                unsafe { this.batch_lookup_data(tx, task_ids, category) }\n+            }\n+        }\n+    }\n+\n     fn shutdown(&self) -> Result<()> {\n         either::for_both!(self, this => this.shutdown())\n     }"
        },
        {
            "sha": "3466fb94e3f1749343cb2574dc895fd051ded9ab",
            "filename": "turbopack/crates/turbo-tasks-backend/src/database/key_value_database.rs",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/vercel/next.js/blob/1d09f42590ef47dfde7061f3a01621654f2a14df/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fdatabase%2Fkey_value_database.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/1d09f42590ef47dfde7061f3a01621654f2a14df/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fdatabase%2Fkey_value_database.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fdatabase%2Fkey_value_database.rs?ref=1d09f42590ef47dfde7061f3a01621654f2a14df",
            "patch": "@@ -35,6 +35,20 @@ pub trait KeyValueDatabase {\n         key: &[u8],\n     ) -> Result<Option<Self::ValueBuffer<'l>>>;\n \n+    fn batch_get<'l, 'db: 'l>(\n+        &'l self,\n+        transaction: &'l Self::ReadTransaction<'db>,\n+        key_space: KeySpace,\n+        keys: &[&[u8]],\n+    ) -> Result<Vec<Option<Self::ValueBuffer<'l>>>> {\n+        let mut results = Vec::with_capacity(keys.len());\n+        for key in keys {\n+            let value = self.get(transaction, key_space, key)?;\n+            results.push(value);\n+        }\n+        Ok(results)\n+    }\n+\n     type SerialWriteBatch<'l>: SerialWriteBatch<'l>\n         = UnimplementedWriteBatch\n     where"
        },
        {
            "sha": "f70c1284cbe096fdea62cc2f2aa56c5b4c57ba67",
            "filename": "turbopack/crates/turbo-tasks-backend/src/database/turbo/mod.rs",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/vercel/next.js/blob/1d09f42590ef47dfde7061f3a01621654f2a14df/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fdatabase%2Fturbo%2Fmod.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/1d09f42590ef47dfde7061f3a01621654f2a14df/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fdatabase%2Fturbo%2Fmod.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fdatabase%2Fturbo%2Fmod.rs?ref=1d09f42590ef47dfde7061f3a01621654f2a14df",
            "patch": "@@ -84,6 +84,15 @@ impl KeyValueDatabase for TurboKeyValueDatabase {\n         self.db.get(key_space as usize, &key)\n     }\n \n+    fn batch_get<'l, 'db: 'l>(\n+        &'l self,\n+        _transaction: &'l Self::ReadTransaction<'db>,\n+        key_space: KeySpace,\n+        keys: &[&[u8]],\n+    ) -> Result<Vec<Option<Self::ValueBuffer<'l>>>> {\n+        self.db.batch_get(key_space as usize, keys)\n+    }\n+\n     type ConcurrentWriteBatch<'l>\n         = TurboWriteBatch<'l>\n     where"
        },
        {
            "sha": "a84949b3b4100dd978da7e20a3a7001193467c31",
            "filename": "turbopack/crates/turbo-tasks-backend/src/kv_backing_storage.rs",
            "status": "modified",
            "additions": 55,
            "deletions": 5,
            "changes": 60,
            "blob_url": "https://github.com/vercel/next.js/blob/1d09f42590ef47dfde7061f3a01621654f2a14df/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fkv_backing_storage.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/1d09f42590ef47dfde7061f3a01621654f2a14df/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fkv_backing_storage.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fkv_backing_storage.rs?ref=1d09f42590ef47dfde7061f3a01621654f2a14df",
            "patch": "@@ -523,11 +523,7 @@ impl<T: KeyValueDatabase + Send + Sync + 'static> BackingStorageSealed\n         ) -> Result<Vec<CachedDataItem>> {\n             let Some(bytes) = database.get(\n                 tx,\n-                match category {\n-                    TaskDataCategory::Meta => KeySpace::TaskMeta,\n-                    TaskDataCategory::Data => KeySpace::TaskData,\n-                    TaskDataCategory::All => unreachable!(),\n-                },\n+                category_to_key_space(category),\n                 IntKey::new(*task_id).as_ref(),\n             )?\n             else {\n@@ -541,6 +537,52 @@ impl<T: KeyValueDatabase + Send + Sync + 'static> BackingStorageSealed\n             .with_context(|| format!(\"Looking up data for {task_id} from database failed\"))\n     }\n \n+    unsafe fn batch_lookup_data(\n+        &self,\n+        tx: Option<&Self::ReadTransaction<'_>>,\n+        task_ids: &[TaskId],\n+        category: TaskDataCategory,\n+    ) -> Result<Vec<Vec<CachedDataItem>>> {\n+        let inner = &*self.inner;\n+        fn lookup<D: KeyValueDatabase>(\n+            database: &D,\n+            tx: &D::ReadTransaction<'_>,\n+            task_ids: &[TaskId],\n+            category: TaskDataCategory,\n+        ) -> Result<Vec<Vec<CachedDataItem>>> {\n+            let int_keys: Vec<_> = task_ids.iter().map(|&id| IntKey::new(*id)).collect();\n+            let keys = int_keys.iter().map(|k| k.as_ref()).collect::<Vec<_>>();\n+            let bytes = database.batch_get(\n+                tx,\n+                match category {\n+                    TaskDataCategory::Meta => KeySpace::TaskMeta,\n+                    TaskDataCategory::Data => KeySpace::TaskData,\n+                    TaskDataCategory::All => unreachable!(),\n+                },\n+                &keys,\n+            )?;\n+            bytes\n+                .into_iter()\n+                .map(|opt_bytes| {\n+                    if let Some(bytes) = opt_bytes {\n+                        let result: Vec<CachedDataItem> = turbo_bincode_decode(bytes.borrow())?;\n+                        Ok(result)\n+                    } else {\n+                        Ok(Vec::new())\n+                    }\n+                })\n+                .collect::<Result<Vec<_>>>()\n+        }\n+        inner\n+            .with_tx(tx, |tx| lookup(&inner.database, tx, task_ids, category))\n+            .with_context(|| {\n+                format!(\n+                    \"Looking up data for {} tasks from database failed\",\n+                    task_ids.len()\n+                )\n+            })\n+    }\n+\n     fn shutdown(&self) -> Result<()> {\n         self.inner.database.shutdown()\n     }\n@@ -765,3 +807,11 @@ fn encode_task_data(task: TaskId, data: &Vec<CachedDataItem>) -> Result<TurboBin\n     })\n     .with_context(|| format!(\"Unable to serialize data items for {task}: {filtered_data:#?}\"))\n }\n+\n+fn category_to_key_space(category: TaskDataCategory) -> KeySpace {\n+    match category {\n+        TaskDataCategory::Meta => KeySpace::TaskMeta,\n+        TaskDataCategory::Data => KeySpace::TaskData,\n+        TaskDataCategory::All => unreachable!(),\n+    }\n+}"
        },
        {
            "sha": "4315aafee2351d87d0cf97e4eaa95a315b5b742d",
            "filename": "turbopack/crates/turbopack-core/src/module_graph/binding_usage_info.rs",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/vercel/next.js/blob/1d09f42590ef47dfde7061f3a01621654f2a14df/turbopack%2Fcrates%2Fturbopack-core%2Fsrc%2Fmodule_graph%2Fbinding_usage_info.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/1d09f42590ef47dfde7061f3a01621654f2a14df/turbopack%2Fcrates%2Fturbopack-core%2Fsrc%2Fmodule_graph%2Fbinding_usage_info.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbopack-core%2Fsrc%2Fmodule_graph%2Fbinding_usage_info.rs?ref=1d09f42590ef47dfde7061f3a01621654f2a14df",
            "patch": "@@ -87,7 +87,7 @@ pub async fn compute_binding_usage_info(\n     remove_unused_imports: bool,\n ) -> Result<Vc<BindingUsageInfo>> {\n     let span_outer = tracing::info_span!(\n-        \"compute bindung usage info\",\n+        \"compute binding usage info\",\n         visit_count = tracing::field::Empty,\n         unused_reference_count = tracing::field::Empty\n     );"
        }
    ],
    "stats": {
        "total": 1273,
        "additions": 1150,
        "deletions": 123
    }
}