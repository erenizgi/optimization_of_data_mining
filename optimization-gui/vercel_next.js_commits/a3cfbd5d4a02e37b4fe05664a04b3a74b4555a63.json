{
    "author": "bgw",
    "message": "fix(turbo-persistence): Fix `verify_sst_content` feature, improve unit test performance (#78388)\n\n## Why?\nI'm trying to run https://github.com/boshen/cargo-shear over our workspace to remove unused dependencies. It runs `cargo expand` with `--all-features`, which was breaking on this crate.\n\n## Changes Summary\n\n- Add `opt-level = 1` in dev for this crate.\n- Reduce the iterations used in tests. If we really want tons of iterations, we should use benchmarks or write a fuzzer utility. Unit tests should be small and fast.\n- Increase the test for blobs to use a 65KiB blob size and assert that's larger than the blob threshold.\n- Cleanup: Use the more-correct `u32` type for the `family` field in a few more places instead of `usize` and make the numeric representations of the `KeySpace` enum a bit more explicit.\n\n## Prior to these changes\n- Building with `--features turbo-persistence/verify_sst_content` would fail with compilation errors\n- Unit tests would take multiple minutes to run\n- The blob test didn't appear to actually test blobs (10KiB < 64KiB)\n\n## After these changes\n- The `verify_sst_content` feature works and passes unit tests\n- Unit tests finish in about a minute with `cargo test` and `cargo nextest`, this is still slow, but much better than it was.\n- Unit tests actually test blobs as intended.\n\n![Screenshot 2025-04-21 at 2.12.32â€¯PM.png](https://graphite-user-uploaded-assets-prod.s3.amazonaws.com/HAZVitxRNnZz8QMiPn4a/ff7aa104-c193-4514-9fde-1a6fcc11acf8.png)\n\n## Potential future work\nIt would be useful to be able to adjust the medium and large (blob) size thresholds to allow us to lower those thresholds in tests.",
    "sha": "a3cfbd5d4a02e37b4fe05664a04b3a74b4555a63",
    "files": [
        {
            "sha": "f61d9d3a7677c746b172d8a4e7c76fc26162ca75",
            "filename": "Cargo.toml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/vercel/next.js/blob/a3cfbd5d4a02e37b4fe05664a04b3a74b4555a63/Cargo.toml",
            "raw_url": "https://github.com/vercel/next.js/raw/a3cfbd5d4a02e37b4fe05664a04b3a74b4555a63/Cargo.toml",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/Cargo.toml?ref=a3cfbd5d4a02e37b4fe05664a04b3a74b4555a63",
            "patch": "@@ -22,7 +22,7 @@ exclude = [\n too_many_arguments = \"allow\"\n \n # This crate is particularly sensitive to compiler optimizations\n-[profile.dev.package.turbo-tasks-memory]\n+[profile.dev.package.turbo-persistence]\n opt-level = 1\n \n # Set the options for dependencies (not crates in the workspace), this mostly impacts cold builds"
        },
        {
            "sha": "43e6668b767f2d198b488fc813e8a080550ad2f3",
            "filename": "turbopack/crates/turbo-persistence/src/tests.rs",
            "status": "modified",
            "additions": 23,
            "deletions": 16,
            "changes": 39,
            "blob_url": "https://github.com/vercel/next.js/blob/a3cfbd5d4a02e37b4fe05664a04b3a74b4555a63/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Ftests.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/a3cfbd5d4a02e37b4fe05664a04b3a74b4555a63/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Ftests.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Ftests.rs?ref=a3cfbd5d4a02e37b4fe05664a04b3a74b4555a63",
            "patch": "@@ -3,7 +3,7 @@ use std::time::Instant;\n use anyhow::Result;\n use rayon::iter::{IntoParallelIterator, ParallelIterator};\n \n-use crate::{db::TurboPersistence, write_batch::WriteBatch};\n+use crate::{constants::MAX_MEDIUM_VALUE_SIZE, db::TurboPersistence, write_batch::WriteBatch};\n \n #[test]\n fn full_cycle() -> Result<()> {\n@@ -53,7 +53,7 @@ fn full_cycle() -> Result<()> {\n         \"Families\",\n         |batch| {\n             for i in 0..16u8 {\n-                batch.put(i as usize, vec![i], vec![i].into())?;\n+                batch.put(u32::from(i), vec![i], vec![i].into())?;\n             }\n             Ok(())\n         },\n@@ -89,41 +89,46 @@ fn full_cycle() -> Result<()> {\n         },\n     );\n \n+    const BLOB_SIZE: usize = 65 * 1024 * 1024;\n+    #[expect(clippy::assertions_on_constants)]\n+    {\n+        assert!(BLOB_SIZE > MAX_MEDIUM_VALUE_SIZE);\n+    }\n     test_case(\n         &mut test_cases,\n         \"Large keys and values (blob files)\",\n         |batch| {\n-            for i in 0..20u8 {\n-                batch.put(\n-                    0,\n-                    vec![i; 10 * 1024 * 1024],\n-                    vec![i; 10 * 1024 * 1024].into(),\n-                )?;\n+            for i in 0..2u8 {\n+                batch.put(0, vec![i; BLOB_SIZE], vec![i; BLOB_SIZE].into())?;\n             }\n             Ok(())\n         },\n         |db| {\n-            for i in 0..20u8 {\n-                let Some(value) = db.get(0, &vec![i; 10 * 1024 * 1024])? else {\n+            for i in 0..2u8 {\n+                let key_and_value = vec![i; BLOB_SIZE];\n+                let Some(value) = db.get(0, &key_and_value)? else {\n                     panic!(\"Value not found\");\n                 };\n-                assert_eq!(&*value, &vec![i; 10 * 1024 * 1024]);\n+                assert_eq!(&*value, &key_and_value);\n             }\n             Ok(())\n         },\n     );\n \n+    fn different_sizes_range() -> impl Iterator<Item = u8> {\n+        (10..20).map(|value| value * 10)\n+    }\n     test_case(\n         &mut test_cases,\n         \"Different sizes keys and values\",\n         |batch| {\n-            for i in 100..200u8 {\n+            for i in different_sizes_range() {\n                 batch.put(0, vec![i; i as usize], vec![i; i as usize].into())?;\n             }\n             Ok(())\n         },\n         |db| {\n-            for i in 100..200u8 {\n+            for i in different_sizes_range() {\n                 let Some(value) = db.get(0, &vec![i; i as usize])? else {\n                     panic!(\"Value not found\");\n                 };\n@@ -351,18 +356,20 @@ fn persist_changes() -> Result<()> {\n     let tempdir = tempfile::tempdir()?;\n     let path = tempdir.path();\n \n+    const READ_COUNT: u32 = 2_000; // we'll read every 10th value, so writes are 10x this value\n     fn put(b: &WriteBatch<(u8, [u8; 4]), 1>, key: u8, value: u8) -> Result<()> {\n-        for i in 0..2000000u32 {\n+        for i in 0..(READ_COUNT * 10) {\n             b.put(0, (key, i.to_be_bytes()), vec![value].into())?;\n         }\n         Ok(())\n     }\n     fn check(db: &TurboPersistence, key: u8, value: u8) -> Result<()> {\n-        for i in 0..200000u32 {\n+        for i in 0..READ_COUNT {\n+            // read every 10th item\n             let i = i * 10;\n             assert_eq!(\n                 db.get(0, &(key, i.to_be_bytes()))?.as_deref(),\n-                Some(&[value][..])\n+                Some(&[value][..]),\n             );\n         }\n         Ok(())"
        },
        {
            "sha": "b18f93d8481bc8cc682c0e79745f775bd0ee7610",
            "filename": "turbopack/crates/turbo-persistence/src/write_batch.rs",
            "status": "modified",
            "additions": 51,
            "deletions": 23,
            "changes": 74,
            "blob_url": "https://github.com/vercel/next.js/blob/a3cfbd5d4a02e37b4fe05664a04b3a74b4555a63/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fwrite_batch.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/a3cfbd5d4a02e37b4fe05664a04b3a74b4555a63/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fwrite_batch.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fwrite_batch.rs?ref=a3cfbd5d4a02e37b4fe05664a04b3a74b4555a63",
            "patch": "@@ -22,7 +22,11 @@ use crate::{\n     key::StoreKey, static_sorted_file_builder::StaticSortedFileBuilder, ValueBuffer,\n };\n \n-/// The thread local state of a `WriteBatch`.\n+/// The thread local state of a `WriteBatch`. `FAMILIES` should fit within a `u32`.\n+//\n+// NOTE: This type *must* use `usize`, even though the real type used in storage is `u32` because\n+// there's no way to cast a `u32` to `usize` when declaring an array without the nightly\n+// `min_generic_const_args` feature.\n struct ThreadLocalState<K: StoreKey + Send, const FAMILIES: usize> {\n     /// The collectors for each family.\n     collectors: [Option<Collector<K>>; FAMILIES],\n@@ -54,7 +58,9 @@ pub struct WriteBatch<K: StoreKey + Send, const FAMILIES: usize> {\n impl<K: StoreKey + Send + Sync, const FAMILIES: usize> WriteBatch<K, FAMILIES> {\n     /// Creates a new write batch for a database.\n     pub(crate) fn new(path: PathBuf, current: u32) -> Self {\n-        assert!(FAMILIES <= u32::MAX as usize);\n+        const {\n+            assert!(FAMILIES <= usize_from_u32(u32::MAX));\n+        };\n         Self {\n             path,\n             current_sequence_number: AtomicU32::new(current),\n@@ -88,10 +94,11 @@ impl<K: StoreKey + Send + Sync, const FAMILIES: usize> WriteBatch<K, FAMILIES> {\n     fn collector_mut<'l>(\n         &self,\n         state: &'l mut ThreadLocalState<K, FAMILIES>,\n-        family: usize,\n+        family: u32,\n     ) -> Result<&'l mut Collector<K>> {\n-        debug_assert!(family < FAMILIES);\n-        let collector = state.collectors[family].get_or_insert_with(|| {\n+        let family_idx = usize_from_u32(family);\n+        debug_assert!(family_idx < FAMILIES);\n+        let collector = state.collectors[family_idx].get_or_insert_with(|| {\n             self.idle_collectors\n                 .lock()\n                 .pop()\n@@ -106,7 +113,7 @@ impl<K: StoreKey + Send + Sync, const FAMILIES: usize> WriteBatch<K, FAMILIES> {\n     }\n \n     /// Puts a key-value pair into the write batch.\n-    pub fn put(&self, family: usize, key: K, value: ValueBuffer<'_>) -> Result<()> {\n+    pub fn put(&self, family: u32, key: K, value: ValueBuffer<'_>) -> Result<()> {\n         let state = self.thread_local_state();\n         let collector = self.collector_mut(state, family)?;\n         if value.len() <= MAX_MEDIUM_VALUE_SIZE {\n@@ -120,7 +127,7 @@ impl<K: StoreKey + Send + Sync, const FAMILIES: usize> WriteBatch<K, FAMILIES> {\n     }\n \n     /// Puts a delete operation into the write batch.\n-    pub fn delete(&self, family: usize, key: K) -> Result<()> {\n+    pub fn delete(&self, family: u32, key: K) -> Result<()> {\n         let state = self.thread_local_state();\n         let collector = self.collector_mut(state, family)?;\n         collector.delete(key);\n@@ -151,7 +158,7 @@ impl<K: StoreKey + Send + Sync, const FAMILIES: usize> WriteBatch<K, FAMILIES> {\n             fn handle_done_collector<'scope, K: StoreKey + Send + Sync, const FAMILIES: usize>(\n                 this: &'scope WriteBatch<K, FAMILIES>,\n                 scope: &Scope<'scope>,\n-                family: usize,\n+                family: u32,\n                 mut collector: Collector<K>,\n                 shared_new_sst_files: &'scope Mutex<&mut Vec<(u32, File)>>,\n                 shared_error: &'scope Mutex<Result<()>>,\n@@ -173,7 +180,8 @@ impl<K: StoreKey + Send + Sync, const FAMILIES: usize> WriteBatch<K, FAMILIES> {\n             all_collectors\n                 .into_par_iter()\n                 .enumerate()\n-                .for_each(|(family, collectors)| {\n+                .for_each(|(family_idx, collectors)| {\n+                    let family = u32::try_from(family_idx).unwrap();\n                     let final_collector = collectors.into_par_iter().reduce(\n                         || None,\n                         |a, b| match (a, b) {\n@@ -250,14 +258,14 @@ impl<K: StoreKey + Send + Sync, const FAMILIES: usize> WriteBatch<K, FAMILIES> {\n     /// Creates a new SST file with the given collector data.\n     fn create_sst_file(\n         &self,\n-        family: usize,\n+        family: u32,\n         collector_data: (&[CollectorEntry<K>], usize, usize),\n     ) -> Result<(u32, File)> {\n         let (entries, total_key_size, total_value_size) = collector_data;\n         let seq = self.current_sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n \n         let builder =\n-            StaticSortedFileBuilder::new(family as u32, entries, total_key_size, total_value_size)?;\n+            StaticSortedFileBuilder::new(family, entries, total_key_size, total_value_size)?;\n \n         let path = self.path.join(format!(\"{:08}.sst\", seq));\n         let file = builder\n@@ -272,6 +280,7 @@ impl<K: StoreKey + Send + Sync, const FAMILIES: usize> WriteBatch<K, FAMILIES> {\n                 collector_entry::CollectorEntryValue,\n                 key::hash_key,\n                 static_sorted_file::{AqmfCache, BlockCache, LookupResult, StaticSortedFile},\n+                static_sorted_file_builder::Entry,\n             };\n \n             file.sync_all()?;\n@@ -297,24 +306,33 @@ impl<K: StoreKey + Send + Sync, const FAMILIES: usize> WriteBatch<K, FAMILIES> {\n                 Default::default(),\n                 Default::default(),\n             );\n+            let mut key_buf = Vec::new();\n             for entry in entries {\n-                let mut key = Vec::with_capacity(entry.key.len());\n-                entry.key.write_to(&mut key);\n+                entry.write_key_to(&mut key_buf);\n                 let result = sst\n-                    .lookup(hash_key(&key), &key, &cache1, &cache2, &cache3)\n+                    .lookup(\n+                        family,\n+                        hash_key(&key_buf),\n+                        &key_buf,\n+                        &cache1,\n+                        &cache2,\n+                        &cache3,\n+                    )\n                     .expect(\"key found\");\n+                key_buf.clear();\n                 match result {\n                     LookupResult::Deleted => {}\n-                    LookupResult::Small { value: val } => {\n-                        if let EntryValue::Small { value } | EntryValue::Medium { value } =\n-                            entry.value\n-                        {\n-                            assert_eq!(&*val, &*value);\n-                        } else {\n-                            panic!(\"Unexpected value\");\n-                        }\n+                    LookupResult::Slice {\n+                        value: lookup_value,\n+                    } => {\n+                        let expected_value_slice = match &entry.value {\n+                            CollectorEntryValue::Small { value } => &**value,\n+                            CollectorEntryValue::Medium { value } => &**value,\n+                            _ => panic!(\"Unexpected value\"),\n+                        };\n+                        assert_eq!(*lookup_value, *expected_value_slice);\n                     }\n-                    LookupResult::Blob { sequence_number } => {}\n+                    LookupResult::Blob { sequence_number: _ } => {}\n                     LookupResult::QuickFilterMiss => panic!(\"aqmf must include\"),\n                     LookupResult::RangeMiss => panic!(\"Index must cover\"),\n                     LookupResult::KeyMiss => panic!(\"All keys must exist\"),\n@@ -325,3 +343,13 @@ impl<K: StoreKey + Send + Sync, const FAMILIES: usize> WriteBatch<K, FAMILIES> {\n         Ok((seq, file))\n     }\n }\n+\n+#[inline(always)]\n+const fn usize_from_u32(value: u32) -> usize {\n+    // This should always be true, as we assume at least a 32-bit width architecture for Turbopack.\n+    // Since this is a const expression, we expect it to be compiled away.\n+    const {\n+        assert!(u32::BITS < usize::BITS);\n+    };\n+    value as usize\n+}"
        },
        {
            "sha": "f8f435edef497f912ab9cda82d006801d70896e0",
            "filename": "turbopack/crates/turbo-tasks-backend/src/database/key_value_database.rs",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/vercel/next.js/blob/a3cfbd5d4a02e37b4fe05664a04b3a74b4555a63/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fdatabase%2Fkey_value_database.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/a3cfbd5d4a02e37b4fe05664a04b3a74b4555a63/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fdatabase%2Fkey_value_database.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fdatabase%2Fkey_value_database.rs?ref=a3cfbd5d4a02e37b4fe05664a04b3a74b4555a63",
            "patch": "@@ -6,11 +6,11 @@ use crate::database::write_batch::{\n \n #[derive(Debug, Clone, Copy)]\n pub enum KeySpace {\n-    Infra,\n-    TaskMeta,\n-    TaskData,\n-    ForwardTaskCache,\n-    ReverseTaskCache,\n+    Infra = 0,\n+    TaskMeta = 1,\n+    TaskData = 2,\n+    ForwardTaskCache = 3,\n+    ReverseTaskCache = 4,\n }\n \n pub trait KeyValueDatabase {"
        },
        {
            "sha": "511f9f23ee6363ae54180ca995e14c792bef7c58",
            "filename": "turbopack/crates/turbo-tasks-backend/src/database/turbo.rs",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/vercel/next.js/blob/a3cfbd5d4a02e37b4fe05664a04b3a74b4555a63/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fdatabase%2Fturbo.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/a3cfbd5d4a02e37b4fe05664a04b3a74b4555a63/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fdatabase%2Fturbo.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fdatabase%2Fturbo.rs?ref=a3cfbd5d4a02e37b4fe05664a04b3a74b4555a63",
            "patch": "@@ -138,11 +138,11 @@ impl<'a> BaseWriteBatch<'a> for TurboWriteBatch<'a> {\n impl<'a> ConcurrentWriteBatch<'a> for TurboWriteBatch<'a> {\n     fn put(&self, key_space: KeySpace, key: WriteBuffer<'_>, value: WriteBuffer<'_>) -> Result<()> {\n         self.batch\n-            .put(key_space as usize, key.into_static(), value.into())\n+            .put(key_space as u32, key.into_static(), value.into())\n     }\n \n     fn delete(&self, key_space: KeySpace, key: WriteBuffer<'_>) -> Result<()> {\n-        self.batch.delete(key_space as usize, key.into_static())\n+        self.batch.delete(key_space as u32, key.into_static())\n     }\n }\n "
        }
    ],
    "stats": {
        "total": 129,
        "additions": 82,
        "deletions": 47
    }
}