{
    "author": "sokra",
    "message": "Turbopack: allow to customize the parallel execution of turbo-persistence (#82668)\n\n<!-- Thanks for opening a PR! Your contribution is much appreciated.\nTo make sure your PR is handled as smoothly as possible we request that you follow the checklist sections below.\nChoose the right checklist for the change(s) that you're making:\n\n## For Contributors\n\n### Improving Documentation\n\n- Run `pnpm prettier-fix` to fix formatting issues before opening the PR.\n- Read the Docs Contribution Guide to ensure your contribution follows the docs guidelines: https://nextjs.org/docs/community/contribution-guide\n\n### Adding or Updating Examples\n\n- The \"examples guidelines\" are followed from our contributing doc https://github.com/vercel/next.js/blob/canary/contributing/examples/adding-examples.md\n- Make sure the linting passes by running `pnpm build && pnpm lint`. See https://github.com/vercel/next.js/blob/canary/contributing/repository/linting.md\n\n### Fixing a bug\n\n- Related issues linked using `fixes #number`\n- Tests added. See: https://github.com/vercel/next.js/blob/canary/contributing/core/testing.md#writing-tests-for-nextjs\n- Errors have a helpful link attached, see https://github.com/vercel/next.js/blob/canary/contributing.md\n\n### Adding a feature\n\n- Implements an existing feature request or RFC. Make sure the feature request has been accepted for implementation before opening a PR. (A discussion must be opened, see https://github.com/vercel/next.js/discussions/new?category=ideas)\n- Related issues/discussions are linked using `fixes #number`\n- e2e tests added (https://github.com/vercel/next.js/blob/canary/contributing/core/testing.md#writing-tests-for-nextjs)\n- Documentation added\n- Telemetry added. In case of a feature if it's used or not.\n- Errors have a helpful link attached, see https://github.com/vercel/next.js/blob/canary/contributing.md\n\n\n## For Maintainers\n\n- Minimal description (aim for explaining to someone not on the team to understand the PR)\n- When linking to a Slack thread, you might want to share details of the conclusion\n- Link both the Linear (Fixes NEXT-xxx) and the GitHub issues\n- Add review comments if necessary to explain to the reviewer the logic behind a change\n\n### What?\n\n### Why?\n\n### How?\n\nCloses NEXT-\nFixes #\n\n-->",
    "sha": "450b07e8a445660c8c36a6a0103d38297a0754f7",
    "files": [
        {
            "sha": "6a384bae92ab4e4071521e5b373d55aa44267204",
            "filename": "turbopack/crates/turbo-persistence-tools/src/main.rs",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/vercel/next.js/blob/450b07e8a445660c8c36a6a0103d38297a0754f7/turbopack%2Fcrates%2Fturbo-persistence-tools%2Fsrc%2Fmain.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/450b07e8a445660c8c36a6a0103d38297a0754f7/turbopack%2Fcrates%2Fturbo-persistence-tools%2Fsrc%2Fmain.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence-tools%2Fsrc%2Fmain.rs?ref=450b07e8a445660c8c36a6a0103d38297a0754f7",
            "patch": "@@ -3,7 +3,7 @@\n use std::path::PathBuf;\n \n use anyhow::{Context, Result, bail};\n-use turbo_persistence::{MetaFileEntryInfo, TurboPersistence};\n+use turbo_persistence::{MetaFileEntryInfo, SerialScheduler, TurboPersistence};\n \n fn main() -> Result<()> {\n     // Get CLI argument\n@@ -16,7 +16,7 @@ fn main() -> Result<()> {\n         bail!(\"The provided path does not exist: {}\", path.display());\n     }\n \n-    let db = TurboPersistence::open_read_only(path)?;\n+    let db: TurboPersistence<SerialScheduler> = TurboPersistence::open_read_only(path)?;\n     let meta_info = db\n         .meta_info()\n         .context(\"Failed to retrieve meta information\")?;"
        },
        {
            "sha": "ac4e7e5cc45f3b45597dc08257054ac0d192cafd",
            "filename": "turbopack/crates/turbo-persistence/Cargo.toml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/vercel/next.js/blob/450b07e8a445660c8c36a6a0103d38297a0754f7/turbopack%2Fcrates%2Fturbo-persistence%2FCargo.toml",
            "raw_url": "https://github.com/vercel/next.js/raw/450b07e8a445660c8c36a6a0103d38297a0754f7/turbopack%2Fcrates%2Fturbo-persistence%2FCargo.toml",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2FCargo.toml?ref=450b07e8a445660c8c36a6a0103d38297a0754f7",
            "patch": "@@ -22,7 +22,6 @@ memmap2 = \"0.9.5\"\n parking_lot = { workspace = true }\n qfilter = { version = \"0.2.4\", features = [\"serde\"] }\n quick_cache = { workspace = true }\n-rayon = { workspace = true }\n rustc-hash = { workspace = true }\n smallvec = { workspace = true }\n thread_local = { workspace = true }\n@@ -32,6 +31,7 @@ zstd = { version = \"0.13.2\", features = [\"zdict_builder\"] }\n \n [dev-dependencies]\n rand = { workspace = true, features = [\"small_rng\"] }\n+rayon = { workspace = true }\n tempfile = { workspace = true }\n \n [lints]"
        },
        {
            "sha": "6637ea2c13e3ce0db4174e9fe1b1090b733b152f",
            "filename": "turbopack/crates/turbo-persistence/src/collector.rs",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/vercel/next.js/blob/450b07e8a445660c8c36a6a0103d38297a0754f7/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fcollector.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/450b07e8a445660c8c36a6a0103d38297a0754f7/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fcollector.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fcollector.rs?ref=450b07e8a445660c8c36a6a0103d38297a0754f7",
            "patch": "@@ -1,3 +1,5 @@\n+use std::mem::take;\n+\n use crate::{\n     ValueBuffer,\n     collector_entry::{CollectorEntry, CollectorEntryValue, EntryKey},\n@@ -111,4 +113,11 @@ impl<K: StoreKey, const SIZE_SHIFT: usize> Collector<K, SIZE_SHIFT> {\n         self.total_value_size = 0;\n         self.entries.drain(..)\n     }\n+\n+    /// Clears the collector and drops the capacity\n+    pub fn drop_contents(&mut self) {\n+        drop(take(&mut self.entries));\n+        self.total_key_size = 0;\n+        self.total_value_size = 0;\n+    }\n }"
        },
        {
            "sha": "b6c91e3e388046436aa1e1bfd66cf7d88c5cd968",
            "filename": "turbopack/crates/turbo-persistence/src/db.rs",
            "status": "modified",
            "additions": 347,
            "deletions": 329,
            "changes": 676,
            "blob_url": "https://github.com/vercel/next.js/blob/450b07e8a445660c8c36a6a0103d38297a0754f7/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fdb.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/450b07e8a445660c8c36a6a0103d38297a0754f7/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fdb.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fdb.rs?ref=450b07e8a445660c8c36a6a0103d38297a0754f7",
            "patch": "@@ -18,8 +18,6 @@ use jiff::Timestamp;\n use lzzzz::lz4::decompress;\n use memmap2::Mmap;\n use parking_lot::{Mutex, RwLock};\n-use rayon::iter::{IndexedParallelIterator, IntoParallelIterator, ParallelIterator};\n-use tracing::Span;\n \n pub use crate::compaction::selector::CompactConfig;\n use crate::{\n@@ -36,6 +34,7 @@ use crate::{\n     merge_iter::MergeIter,\n     meta_file::{AmqfCache, MetaFile, MetaLookupResult, StaticSortedFileRange},\n     meta_file_builder::MetaFileBuilder,\n+    parallel_scheduler::ParallelScheduler,\n     sst_filter::SstFilter,\n     static_sorted_file::{BlockCache, SstLookupResult},\n     static_sorted_file_builder::{StaticSortedFileBuilderMeta, write_static_stored_file},\n@@ -108,7 +107,8 @@ struct TrackedStats {\n \n /// TurboPersistence is a persistent key-value store. It is limited to a single writer at a time\n /// using a single write batch. It allows for concurrent reads.\n-pub struct TurboPersistence {\n+pub struct TurboPersistence<S: ParallelScheduler> {\n+    parallel_scheduler: S,\n     /// The path to the directory where the database is stored\n     path: PathBuf,\n     /// If true, the database is opened in read-only mode. In this mode, no writes are allowed and\n@@ -148,9 +148,26 @@ pub struct CommitOptions {\n     keys_written: u64,\n }\n \n-impl TurboPersistence {\n-    fn new(path: PathBuf, read_only: bool) -> Self {\n+impl<S: ParallelScheduler + Default> TurboPersistence<S> {\n+    /// Open a TurboPersistence database at the given path.\n+    /// This will read the directory and might performance cleanup when the database was not closed\n+    /// properly. Cleanup only requires to read a few bytes from a few files and to delete\n+    /// files, so it's fast.\n+    pub fn open(path: PathBuf) -> Result<Self> {\n+        Self::open_with_parallel_scheduler(path, Default::default())\n+    }\n+\n+    /// Open a TurboPersistence database at the given path in read only mode.\n+    /// This will read the directory. No Cleanup is performed.\n+    pub fn open_read_only(path: PathBuf) -> Result<Self> {\n+        Self::open_read_only_with_parallel_scheduler(path, Default::default())\n+    }\n+}\n+\n+impl<S: ParallelScheduler> TurboPersistence<S> {\n+    fn new(path: PathBuf, read_only: bool, parallel_scheduler: S) -> Self {\n         Self {\n+            parallel_scheduler,\n             path,\n             read_only,\n             inner: RwLock::new(Inner {\n@@ -188,16 +205,19 @@ impl TurboPersistence {\n     /// This will read the directory and might performance cleanup when the database was not closed\n     /// properly. Cleanup only requires to read a few bytes from a few files and to delete\n     /// files, so it's fast.\n-    pub fn open(path: PathBuf) -> Result<Self> {\n-        let mut db = Self::new(path, false);\n+    pub fn open_with_parallel_scheduler(path: PathBuf, parallel_scheduler: S) -> Result<Self> {\n+        let mut db = Self::new(path, false, parallel_scheduler);\n         db.open_directory(false)?;\n         Ok(db)\n     }\n \n     /// Open a TurboPersistence database at the given path in read only mode.\n     /// This will read the directory. No Cleanup is performed.\n-    pub fn open_read_only(path: PathBuf) -> Result<Self> {\n-        let mut db = Self::new(path, true);\n+    pub fn open_read_only_with_parallel_scheduler(\n+        path: PathBuf,\n+        parallel_scheduler: S,\n+    ) -> Result<Self> {\n+        let mut db = Self::new(path, true, parallel_scheduler);\n         db.open_directory(false)?;\n         Ok(db)\n     }\n@@ -341,16 +361,12 @@ impl TurboPersistence {\n \n         meta_files.retain(|seq| !deleted_files.contains(seq));\n         meta_files.sort_unstable();\n-        let span = Span::current();\n-        let mut meta_files = meta_files\n-            .into_par_iter()\n-            .with_min_len(1)\n-            .map(|seq| {\n-                let _span = span.enter();\n+        let mut meta_files = self\n+            .parallel_scheduler\n+            .parallel_map_collect::<_, _, Result<Vec<MetaFile>>>(&meta_files, |&seq| {\n                 let meta_file = MetaFile::open(&self.path, seq)?;\n                 Ok(meta_file)\n-            })\n-            .collect::<Result<Vec<MetaFile>>>()?;\n+            })?;\n \n         let mut sst_filter = SstFilter::new();\n         for meta_file in meta_files.iter_mut().rev() {\n@@ -398,7 +414,7 @@ impl TurboPersistence {\n     /// This data will only become visible after the WriteBatch is committed.\n     pub fn write_batch<K: StoreKey + Send + Sync + 'static, const FAMILIES: usize>(\n         &self,\n-    ) -> Result<WriteBatch<K, FAMILIES>> {\n+    ) -> Result<WriteBatch<K, S, FAMILIES>> {\n         if self.read_only {\n             bail!(\"Cannot write to a read-only database\");\n         }\n@@ -413,7 +429,11 @@ impl TurboPersistence {\n             );\n         }\n         let current = self.inner.read().current_sequence_number;\n-        Ok(WriteBatch::new(self.path.clone(), current))\n+        Ok(WriteBatch::new(\n+            self.path.clone(),\n+            current,\n+            self.parallel_scheduler.clone(),\n+        ))\n     }\n \n     fn open_log(&self) -> Result<BufWriter<File>> {\n@@ -432,7 +452,7 @@ impl TurboPersistence {\n     /// visible to readers.\n     pub fn commit_write_batch<K: StoreKey + Send + Sync + 'static, const FAMILIES: usize>(\n         &self,\n-        mut write_batch: WriteBatch<K, FAMILIES>,\n+        mut write_batch: WriteBatch<K, S, FAMILIES>,\n     ) -> Result<()> {\n         if self.read_only {\n             unreachable!(\"It's not possible to create a write batch for a read-only database\");\n@@ -475,15 +495,13 @@ impl TurboPersistence {\n \n         new_meta_files.sort_unstable_by_key(|(seq, _)| *seq);\n \n-        let mut new_meta_files = new_meta_files\n-            .into_par_iter()\n-            .with_min_len(1)\n-            .map(|(seq, file)| {\n+        let mut new_meta_files = self\n+            .parallel_scheduler\n+            .parallel_map_collect_owned::<_, _, Result<Vec<_>>>(new_meta_files, |(seq, file)| {\n                 file.sync_all()?;\n                 let meta_file = MetaFile::open(&self.path, seq)?;\n                 Ok(meta_file)\n-            })\n-            .collect::<Result<Vec<_>>>()?;\n+            })?;\n \n         let mut sst_filter = SstFilter::new();\n         for meta_file in new_meta_files.iter_mut().rev() {\n@@ -778,7 +796,6 @@ impl TurboPersistence {\n         let path = &self.path;\n \n         let log_mutex = Mutex::new(());\n-        let span = Span::current();\n \n         struct PartialResultPerFamily {\n             new_meta_file: Option<(u32, File)>,\n@@ -790,336 +807,337 @@ impl TurboPersistence {\n \n         let mut compact_config = compact_config.clone();\n         let merge_jobs = sst_by_family\n-            .iter()\n-            .map(|ssts_with_ranges| {\n+            .into_iter()\n+            .enumerate()\n+            .filter_map(|(family, ssts_with_ranges)| {\n                 if compact_config.max_merge_segment_count == 0 {\n-                    return Vec::new();\n+                    return None;\n                 }\n                 let (merge_jobs, real_merge_job_size) =\n-                    get_merge_segments(ssts_with_ranges, &compact_config);\n+                    get_merge_segments(&ssts_with_ranges, &compact_config);\n                 compact_config.max_merge_segment_count -= real_merge_job_size;\n-                merge_jobs\n+                Some((family, ssts_with_ranges, merge_jobs))\n             })\n             .collect::<Vec<_>>();\n \n-        let result = sst_by_family\n-            .into_par_iter()\n-            .zip(merge_jobs.into_par_iter())\n-            .with_min_len(1)\n-            .enumerate()\n-            .map(|(family, (ssts_with_ranges, merge_jobs))| {\n-                let family = family as u32;\n-                let _span = span.clone().entered();\n-\n-                if merge_jobs.is_empty() {\n-                    return Ok(PartialResultPerFamily {\n-                        new_meta_file: None,\n-                        new_sst_files: Vec::new(),\n-                        sst_seq_numbers_to_delete: Vec::new(),\n-                        blob_seq_numbers_to_delete: Vec::new(),\n-                        keys_written: 0,\n-                    });\n-                }\n-\n-                {\n-                    let metrics = compute_metrics(&ssts_with_ranges, 0..=u64::MAX);\n-                    let guard = log_mutex.lock();\n-                    let mut log = self.open_log()?;\n-                    writeln!(\n-                        log,\n-                        \"Compaction for family {family} (coverage: {}, overlap: {}, duplication: \\\n-                         {} / {} MiB):\",\n-                        metrics.coverage,\n-                        metrics.overlap,\n-                        metrics.duplication,\n-                        metrics.duplicated_size / 1024 / 1024\n-                    )?;\n-                    for job in merge_jobs.iter() {\n-                        writeln!(log, \"  merge\")?;\n-                        for i in job.iter() {\n-                            let seq = ssts_with_ranges[*i].seq;\n-                            let (min, max) = ssts_with_ranges[*i].range().into_inner();\n-                            writeln!(log, \"    {seq:08} {min:016x}-{max:016x}\")?;\n-                        }\n+        let result = self\n+            .parallel_scheduler\n+            .parallel_map_collect_owned::<_, _, Result<Vec<_>>>(\n+                merge_jobs,\n+                |(family, ssts_with_ranges, merge_jobs)| {\n+                    let family = family as u32;\n+\n+                    if merge_jobs.is_empty() {\n+                        return Ok(PartialResultPerFamily {\n+                            new_meta_file: None,\n+                            new_sst_files: Vec::new(),\n+                            sst_seq_numbers_to_delete: Vec::new(),\n+                            blob_seq_numbers_to_delete: Vec::new(),\n+                            keys_written: 0,\n+                        });\n                     }\n-                    drop(guard);\n-                }\n \n-                // Later we will remove the merged files\n-                let sst_seq_numbers_to_delete = merge_jobs\n-                    .iter()\n-                    .filter(|l| l.len() > 1)\n-                    .flat_map(|l| l.iter().copied())\n-                    .map(|index| ssts_with_ranges[index].seq)\n-                    .collect::<Vec<_>>();\n-\n-                // Merge SST files\n-                let span = tracing::trace_span!(\"merge files\");\n-                enum PartialMergeResult<'l> {\n-                    Merged {\n-                        new_sst_files: Vec<(u32, File, StaticSortedFileBuilderMeta<'static>)>,\n-                        blob_seq_numbers_to_delete: Vec<u32>,\n-                        keys_written: u64,\n-                    },\n-                    Move {\n-                        seq: u32,\n-                        meta: StaticSortedFileBuilderMeta<'l>,\n-                    },\n-                }\n-                let merge_result = merge_jobs\n-                    .into_par_iter()\n-                    .with_min_len(1)\n-                    .map(|indices| {\n-                        let _span = span.clone().entered();\n-                        if indices.len() == 1 {\n-                            // If we only have one file, we can just move it\n-                            let index = indices[0];\n-                            let meta_index = ssts_with_ranges[index].meta_index;\n-                            let index_in_meta = ssts_with_ranges[index].index_in_meta;\n-                            let meta_file = &meta_files[meta_index];\n-                            let entry = meta_file.entry(index_in_meta);\n-                            let amqf = Cow::Borrowed(entry.raw_amqf(meta_file.amqf_data()));\n-                            let meta = StaticSortedFileBuilderMeta {\n-                                min_hash: entry.min_hash(),\n-                                max_hash: entry.max_hash(),\n-                                amqf,\n-                                key_compression_dictionary_length: entry\n-                                    .key_compression_dictionary_length(),\n-                                value_compression_dictionary_length: entry\n-                                    .value_compression_dictionary_length(),\n-                                block_count: entry.block_count(),\n-                                size: entry.size(),\n-                                entries: 0,\n-                            };\n-                            return Ok(PartialMergeResult::Move {\n-                                seq: entry.sequence_number(),\n-                                meta,\n-                            });\n+                    {\n+                        let metrics = compute_metrics(&ssts_with_ranges, 0..=u64::MAX);\n+                        let guard = log_mutex.lock();\n+                        let mut log = self.open_log()?;\n+                        writeln!(\n+                            log,\n+                            \"Compaction for family {family} (coverage: {}, overlap: {}, \\\n+                             duplication: {} / {} MiB):\",\n+                            metrics.coverage,\n+                            metrics.overlap,\n+                            metrics.duplication,\n+                            metrics.duplicated_size / 1024 / 1024\n+                        )?;\n+                        for job in merge_jobs.iter() {\n+                            writeln!(log, \"  merge\")?;\n+                            for i in job.iter() {\n+                                let seq = ssts_with_ranges[*i].seq;\n+                                let (min, max) = ssts_with_ranges[*i].range().into_inner();\n+                                writeln!(log, \"    {seq:08} {min:016x}-{max:016x}\")?;\n+                            }\n                         }\n+                        drop(guard);\n+                    }\n \n-                        fn create_sst_file(\n-                            entries: &[LookupEntry],\n-                            total_key_size: usize,\n-                            total_value_size: usize,\n-                            path: &Path,\n+                    // Later we will remove the merged files\n+                    let sst_seq_numbers_to_delete = merge_jobs\n+                        .iter()\n+                        .filter(|l| l.len() > 1)\n+                        .flat_map(|l| l.iter().copied())\n+                        .map(|index| ssts_with_ranges[index].seq)\n+                        .collect::<Vec<_>>();\n+\n+                    // Merge SST files\n+                    let span = tracing::trace_span!(\"merge files\");\n+                    enum PartialMergeResult<'l> {\n+                        Merged {\n+                            new_sst_files: Vec<(u32, File, StaticSortedFileBuilderMeta<'static>)>,\n+                            blob_seq_numbers_to_delete: Vec<u32>,\n+                            keys_written: u64,\n+                        },\n+                        Move {\n                             seq: u32,\n-                        ) -> Result<(u32, File, StaticSortedFileBuilderMeta<'static>)>\n-                        {\n-                            let _span = tracing::trace_span!(\"write merged sst file\").entered();\n-                            let (meta, file) = write_static_stored_file(\n-                                entries,\n-                                total_key_size,\n-                                total_value_size,\n-                                &path.join(format!(\"{seq:08}.sst\")),\n-                            )?;\n-                            Ok((seq, file, meta))\n-                        }\n-\n-                        let mut new_sst_files = Vec::new();\n-\n-                        // Iterate all SST files\n-                        let iters = indices\n-                            .iter()\n-                            .map(|&index| {\n+                            meta: StaticSortedFileBuilderMeta<'l>,\n+                        },\n+                    }\n+                    let merge_result = self\n+                        .parallel_scheduler\n+                        .parallel_map_collect_owned::<_, _, Result<Vec<_>>>(merge_jobs, |indices| {\n+                            let _span = span.clone().entered();\n+                            if indices.len() == 1 {\n+                                // If we only have one file, we can just move it\n+                                let index = indices[0];\n                                 let meta_index = ssts_with_ranges[index].meta_index;\n                                 let index_in_meta = ssts_with_ranges[index].index_in_meta;\n-                                let meta = &meta_files[meta_index];\n-                                meta.entry(index_in_meta)\n-                                    .sst(meta)?\n-                                    .iter(key_block_cache, value_block_cache)\n-                            })\n-                            .collect::<Result<Vec<_>>>()?;\n-\n-                        let iter = MergeIter::new(iters.into_iter())?;\n-\n-                        // TODO figure out how to delete blobs when they are no longer\n-                        // referenced\n-                        let blob_seq_numbers_to_delete: Vec<u32> = Vec::new();\n-\n-                        let mut keys_written = 0;\n-\n-                        let mut total_key_size = 0;\n-                        let mut total_value_size = 0;\n-                        let mut current: Option<LookupEntry> = None;\n-                        let mut entries = Vec::new();\n-                        let mut last_entries = Vec::new();\n-                        let mut last_entries_total_sizes = (0, 0);\n-                        for entry in iter {\n-                            let entry = entry?;\n-\n-                            // Remove duplicates\n-                            if let Some(current) = current.take() {\n-                                if current.key != entry.key {\n-                                    let key_size = current.key.len();\n-                                    let value_size = current.value.size_in_sst();\n-                                    total_key_size += key_size;\n-                                    total_value_size += value_size;\n-\n-                                    if total_key_size + total_value_size\n-                                        > DATA_THRESHOLD_PER_COMPACTED_FILE\n-                                        || entries.len() >= MAX_ENTRIES_PER_COMPACTED_FILE\n-                                    {\n-                                        let (selected_total_key_size, selected_total_value_size) =\n-                                            last_entries_total_sizes;\n-                                        swap(&mut entries, &mut last_entries);\n-                                        last_entries_total_sizes = (\n-                                            total_key_size - key_size,\n-                                            total_value_size - value_size,\n-                                        );\n-                                        total_key_size = key_size;\n-                                        total_value_size = value_size;\n-\n-                                        if !entries.is_empty() {\n-                                            let seq =\n-                                                sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n-\n-                                            keys_written += entries.len() as u64;\n-                                            new_sst_files.push(create_sst_file(\n-                                                &entries,\n+                                let meta_file = &meta_files[meta_index];\n+                                let entry = meta_file.entry(index_in_meta);\n+                                let amqf = Cow::Borrowed(entry.raw_amqf(meta_file.amqf_data()));\n+                                let meta = StaticSortedFileBuilderMeta {\n+                                    min_hash: entry.min_hash(),\n+                                    max_hash: entry.max_hash(),\n+                                    amqf,\n+                                    key_compression_dictionary_length: entry\n+                                        .key_compression_dictionary_length(),\n+                                    value_compression_dictionary_length: entry\n+                                        .value_compression_dictionary_length(),\n+                                    block_count: entry.block_count(),\n+                                    size: entry.size(),\n+                                    entries: 0,\n+                                };\n+                                return Ok(PartialMergeResult::Move {\n+                                    seq: entry.sequence_number(),\n+                                    meta,\n+                                });\n+                            }\n+\n+                            fn create_sst_file(\n+                                entries: &[LookupEntry],\n+                                total_key_size: usize,\n+                                total_value_size: usize,\n+                                path: &Path,\n+                                seq: u32,\n+                            ) -> Result<(u32, File, StaticSortedFileBuilderMeta<'static>)>\n+                            {\n+                                let _span = tracing::trace_span!(\"write merged sst file\").entered();\n+                                let (meta, file) = write_static_stored_file(\n+                                    entries,\n+                                    total_key_size,\n+                                    total_value_size,\n+                                    &path.join(format!(\"{seq:08}.sst\")),\n+                                )?;\n+                                Ok((seq, file, meta))\n+                            }\n+\n+                            let mut new_sst_files = Vec::new();\n+\n+                            // Iterate all SST files\n+                            let iters = indices\n+                                .iter()\n+                                .map(|&index| {\n+                                    let meta_index = ssts_with_ranges[index].meta_index;\n+                                    let index_in_meta = ssts_with_ranges[index].index_in_meta;\n+                                    let meta = &meta_files[meta_index];\n+                                    meta.entry(index_in_meta)\n+                                        .sst(meta)?\n+                                        .iter(key_block_cache, value_block_cache)\n+                                })\n+                                .collect::<Result<Vec<_>>>()?;\n+\n+                            let iter = MergeIter::new(iters.into_iter())?;\n+\n+                            // TODO figure out how to delete blobs when they are no longer\n+                            // referenced\n+                            let blob_seq_numbers_to_delete: Vec<u32> = Vec::new();\n+\n+                            let mut keys_written = 0;\n+\n+                            let mut total_key_size = 0;\n+                            let mut total_value_size = 0;\n+                            let mut current: Option<LookupEntry> = None;\n+                            let mut entries = Vec::new();\n+                            let mut last_entries = Vec::new();\n+                            let mut last_entries_total_sizes = (0, 0);\n+                            for entry in iter {\n+                                let entry = entry?;\n+\n+                                // Remove duplicates\n+                                if let Some(current) = current.take() {\n+                                    if current.key != entry.key {\n+                                        let key_size = current.key.len();\n+                                        let value_size = current.value.size_in_sst();\n+                                        total_key_size += key_size;\n+                                        total_value_size += value_size;\n+\n+                                        if total_key_size + total_value_size\n+                                            > DATA_THRESHOLD_PER_COMPACTED_FILE\n+                                            || entries.len() >= MAX_ENTRIES_PER_COMPACTED_FILE\n+                                        {\n+                                            let (\n                                                 selected_total_key_size,\n                                                 selected_total_value_size,\n-                                                path,\n-                                                seq,\n-                                            )?);\n-\n-                                            entries.clear();\n+                                            ) = last_entries_total_sizes;\n+                                            swap(&mut entries, &mut last_entries);\n+                                            last_entries_total_sizes = (\n+                                                total_key_size - key_size,\n+                                                total_value_size - value_size,\n+                                            );\n+                                            total_key_size = key_size;\n+                                            total_value_size = value_size;\n+\n+                                            if !entries.is_empty() {\n+                                                let seq = sequence_number\n+                                                    .fetch_add(1, Ordering::SeqCst)\n+                                                    + 1;\n+\n+                                                keys_written += entries.len() as u64;\n+                                                new_sst_files.push(create_sst_file(\n+                                                    &entries,\n+                                                    selected_total_key_size,\n+                                                    selected_total_value_size,\n+                                                    path,\n+                                                    seq,\n+                                                )?);\n+\n+                                                entries.clear();\n+                                            }\n                                         }\n-                                    }\n \n-                                    entries.push(current);\n-                                } else {\n-                                    // Override value\n+                                        entries.push(current);\n+                                    } else {\n+                                        // Override value\n+                                    }\n                                 }\n+                                current = Some(entry);\n+                            }\n+                            if let Some(entry) = current {\n+                                total_key_size += entry.key.len();\n+                                total_value_size += entry.value.size_in_sst();\n+                                entries.push(entry);\n                             }\n-                            current = Some(entry);\n-                        }\n-                        if let Some(entry) = current {\n-                            total_key_size += entry.key.len();\n-                            total_value_size += entry.value.size_in_sst();\n-                            entries.push(entry);\n-                        }\n \n-                        // If we have one set of entries left, write them to a new SST file\n-                        if last_entries.is_empty() && !entries.is_empty() {\n-                            let seq = sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n-\n-                            keys_written += entries.len() as u64;\n-                            new_sst_files.push(create_sst_file(\n-                                &entries,\n-                                total_key_size,\n-                                total_value_size,\n-                                path,\n-                                seq,\n-                            )?);\n-                        } else\n-                        // If we have two sets of entries left, merge them and\n-                        // split it into two SST files, to avoid having a\n-                        // single SST file that is very small.\n-                        if !last_entries.is_empty() {\n-                            last_entries.append(&mut entries);\n-\n-                            last_entries_total_sizes.0 += total_key_size;\n-                            last_entries_total_sizes.1 += total_value_size;\n-\n-                            let (part1, part2) = last_entries.split_at(last_entries.len() / 2);\n-\n-                            let seq1 = sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n-                            let seq2 = sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n-\n-                            keys_written += part1.len() as u64;\n-                            new_sst_files.push(create_sst_file(\n-                                part1,\n-                                // We don't know the exact sizes so we estimate them\n-                                last_entries_total_sizes.0 / 2,\n-                                last_entries_total_sizes.1 / 2,\n-                                path,\n-                                seq1,\n-                            )?);\n-\n-                            keys_written += part2.len() as u64;\n-                            new_sst_files.push(create_sst_file(\n-                                part2,\n-                                last_entries_total_sizes.0 / 2,\n-                                last_entries_total_sizes.1 / 2,\n-                                path,\n-                                seq2,\n-                            )?);\n-                        }\n-                        Ok(PartialMergeResult::Merged {\n-                            new_sst_files,\n-                            blob_seq_numbers_to_delete,\n-                            keys_written,\n+                            // If we have one set of entries left, write them to a new SST file\n+                            if last_entries.is_empty() && !entries.is_empty() {\n+                                let seq = sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n+\n+                                keys_written += entries.len() as u64;\n+                                new_sst_files.push(create_sst_file(\n+                                    &entries,\n+                                    total_key_size,\n+                                    total_value_size,\n+                                    path,\n+                                    seq,\n+                                )?);\n+                            } else\n+                            // If we have two sets of entries left, merge them and\n+                            // split it into two SST files, to avoid having a\n+                            // single SST file that is very small.\n+                            if !last_entries.is_empty() {\n+                                last_entries.append(&mut entries);\n+\n+                                last_entries_total_sizes.0 += total_key_size;\n+                                last_entries_total_sizes.1 += total_value_size;\n+\n+                                let (part1, part2) = last_entries.split_at(last_entries.len() / 2);\n+\n+                                let seq1 = sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n+                                let seq2 = sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n+\n+                                keys_written += part1.len() as u64;\n+                                new_sst_files.push(create_sst_file(\n+                                    part1,\n+                                    // We don't know the exact sizes so we estimate them\n+                                    last_entries_total_sizes.0 / 2,\n+                                    last_entries_total_sizes.1 / 2,\n+                                    path,\n+                                    seq1,\n+                                )?);\n+\n+                                keys_written += part2.len() as u64;\n+                                new_sst_files.push(create_sst_file(\n+                                    part2,\n+                                    last_entries_total_sizes.0 / 2,\n+                                    last_entries_total_sizes.1 / 2,\n+                                    path,\n+                                    seq2,\n+                                )?);\n+                            }\n+                            Ok(PartialMergeResult::Merged {\n+                                new_sst_files,\n+                                blob_seq_numbers_to_delete,\n+                                keys_written,\n+                            })\n                         })\n-                    })\n-                    .collect::<Result<Vec<_>>>()\n-                    .with_context(|| {\n-                        format!(\"Failed to merge database files for family {family}\")\n-                    })?;\n-\n-                let Some((sst_files_len, blob_delete_len)) = merge_result\n-                    .iter()\n-                    .map(|r| {\n-                        if let PartialMergeResult::Merged {\n-                            new_sst_files,\n-                            blob_seq_numbers_to_delete,\n-                            keys_written: _,\n-                        } = r\n-                        {\n-                            (new_sst_files.len(), blob_seq_numbers_to_delete.len())\n-                        } else {\n-                            (0, 0)\n-                        }\n-                    })\n-                    .reduce(|(a1, a2), (b1, b2)| (a1 + b1, a2 + b2))\n-                else {\n-                    unreachable!()\n-                };\n-\n-                let mut new_sst_files = Vec::with_capacity(sst_files_len);\n-                let mut blob_seq_numbers_to_delete = Vec::with_capacity(blob_delete_len);\n-\n-                let mut meta_file_builder = MetaFileBuilder::new(family);\n-\n-                let mut keys_written = 0;\n-                for result in merge_result {\n-                    match result {\n-                        PartialMergeResult::Merged {\n-                            new_sst_files: merged_new_sst_files,\n-                            blob_seq_numbers_to_delete: merged_blob_seq_numbers_to_delete,\n-                            keys_written: merged_keys_written,\n-                        } => {\n-                            for (seq, file, meta) in merged_new_sst_files {\n+                        .with_context(|| {\n+                            format!(\"Failed to merge database files for family {family}\")\n+                        })?;\n+\n+                    let Some((sst_files_len, blob_delete_len)) = merge_result\n+                        .iter()\n+                        .map(|r| {\n+                            if let PartialMergeResult::Merged {\n+                                new_sst_files,\n+                                blob_seq_numbers_to_delete,\n+                                keys_written: _,\n+                            } = r\n+                            {\n+                                (new_sst_files.len(), blob_seq_numbers_to_delete.len())\n+                            } else {\n+                                (0, 0)\n+                            }\n+                        })\n+                        .reduce(|(a1, a2), (b1, b2)| (a1 + b1, a2 + b2))\n+                    else {\n+                        unreachable!()\n+                    };\n+\n+                    let mut new_sst_files = Vec::with_capacity(sst_files_len);\n+                    let mut blob_seq_numbers_to_delete = Vec::with_capacity(blob_delete_len);\n+\n+                    let mut meta_file_builder = MetaFileBuilder::new(family);\n+\n+                    let mut keys_written = 0;\n+                    for result in merge_result {\n+                        match result {\n+                            PartialMergeResult::Merged {\n+                                new_sst_files: merged_new_sst_files,\n+                                blob_seq_numbers_to_delete: merged_blob_seq_numbers_to_delete,\n+                                keys_written: merged_keys_written,\n+                            } => {\n+                                for (seq, file, meta) in merged_new_sst_files {\n+                                    meta_file_builder.add(seq, meta);\n+                                    new_sst_files.push((seq, file));\n+                                }\n+                                blob_seq_numbers_to_delete\n+                                    .extend(merged_blob_seq_numbers_to_delete);\n+                                keys_written += merged_keys_written;\n+                            }\n+                            PartialMergeResult::Move { seq, meta } => {\n                                 meta_file_builder.add(seq, meta);\n-                                new_sst_files.push((seq, file));\n                             }\n-                            blob_seq_numbers_to_delete.extend(merged_blob_seq_numbers_to_delete);\n-                            keys_written += merged_keys_written;\n-                        }\n-                        PartialMergeResult::Move { seq, meta } => {\n-                            meta_file_builder.add(seq, meta);\n                         }\n                     }\n-                }\n \n-                for &seq in sst_seq_numbers_to_delete.iter() {\n-                    meta_file_builder.add_obsolete_sst_file(seq);\n-                }\n+                    for &seq in sst_seq_numbers_to_delete.iter() {\n+                        meta_file_builder.add_obsolete_sst_file(seq);\n+                    }\n \n-                let seq = sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n-                let meta_file = {\n-                    let _span = tracing::trace_span!(\"write meta file\").entered();\n-                    meta_file_builder.write(&self.path, seq)?\n-                };\n-\n-                Ok(PartialResultPerFamily {\n-                    new_meta_file: Some((seq, meta_file)),\n-                    new_sst_files,\n-                    sst_seq_numbers_to_delete,\n-                    blob_seq_numbers_to_delete,\n-                    keys_written,\n-                })\n-            })\n-            .collect::<Result<Vec<_>>>()?;\n+                    let seq = sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n+                    let meta_file = {\n+                        let _span = tracing::trace_span!(\"write meta file\").entered();\n+                        meta_file_builder.write(&self.path, seq)?\n+                    };\n+\n+                    Ok(PartialResultPerFamily {\n+                        new_meta_file: Some((seq, meta_file)),\n+                        new_sst_files,\n+                        sst_seq_numbers_to_delete,\n+                        blob_seq_numbers_to_delete,\n+                        keys_written,\n+                    })\n+                },\n+            )?;\n \n         for PartialResultPerFamily {\n             new_meta_file: inner_new_meta_file,"
        },
        {
            "sha": "f944e4b4d120246981aa17c3e405df9035204e19",
            "filename": "turbopack/crates/turbo-persistence/src/lib.rs",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/vercel/next.js/blob/450b07e8a445660c8c36a6a0103d38297a0754f7/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Flib.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/450b07e8a445660c8c36a6a0103d38297a0754f7/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Flib.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Flib.rs?ref=450b07e8a445660c8c36a6a0103d38297a0754f7",
            "patch": "@@ -13,19 +13,21 @@ mod db;\n mod key;\n mod lookup_entry;\n mod merge_iter;\n+mod meta_file;\n+mod meta_file_builder;\n+mod parallel_scheduler;\n+mod sst_filter;\n mod static_sorted_file;\n mod static_sorted_file_builder;\n+mod value_buf;\n mod write_batch;\n \n-mod meta_file;\n-mod meta_file_builder;\n-mod sst_filter;\n #[cfg(test)]\n mod tests;\n-mod value_buf;\n \n pub use arc_slice::ArcSlice;\n pub use db::{CompactConfig, MetaFileEntryInfo, MetaFileInfo, TurboPersistence};\n pub use key::{KeyBase, QueryKey, StoreKey};\n+pub use parallel_scheduler::{ParallelScheduler, SerialScheduler};\n pub use value_buf::ValueBuffer;\n pub use write_batch::WriteBatch;"
        },
        {
            "sha": "52a9d626090fc672b53317e8dc0cf42074419dfb",
            "filename": "turbopack/crates/turbo-persistence/src/parallel_scheduler.rs",
            "status": "added",
            "additions": 137,
            "deletions": 0,
            "changes": 137,
            "blob_url": "https://github.com/vercel/next.js/blob/450b07e8a445660c8c36a6a0103d38297a0754f7/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fparallel_scheduler.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/450b07e8a445660c8c36a6a0103d38297a0754f7/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fparallel_scheduler.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fparallel_scheduler.rs?ref=450b07e8a445660c8c36a6a0103d38297a0754f7",
            "patch": "@@ -0,0 +1,137 @@\n+pub trait ParallelScheduler: Clone + Sync + Send {\n+    fn parallel_for_each<T>(&self, items: &[T], f: impl Fn(&T) + Send + Sync)\n+    where\n+        T: Sync;\n+\n+    fn try_parallel_for_each<'l, T, E>(\n+        &self,\n+        items: &'l [T],\n+        f: impl (Fn(&'l T) -> Result<(), E>) + Send + Sync,\n+    ) -> Result<(), E>\n+    where\n+        T: Sync,\n+        E: Send + 'static;\n+\n+    fn try_parallel_for_each_mut<'l, T, E>(\n+        &self,\n+        items: &'l mut [T],\n+        f: impl (Fn(&'l mut T) -> Result<(), E>) + Send + Sync,\n+    ) -> Result<(), E>\n+    where\n+        T: Send + Sync,\n+        E: Send + 'static;\n+\n+    fn try_parallel_for_each_owned<T, E>(\n+        &self,\n+        items: Vec<T>,\n+        f: impl (Fn(T) -> Result<(), E>) + Send + Sync,\n+    ) -> Result<(), E>\n+    where\n+        T: Send + Sync,\n+        E: Send + 'static;\n+\n+    fn parallel_map_collect<'l, Item, PerItemResult, Result>(\n+        &self,\n+        items: &'l [Item],\n+        f: impl Fn(&'l Item) -> PerItemResult + Send + Sync,\n+    ) -> Result\n+    where\n+        Item: Sync,\n+        PerItemResult: Send + Sync + 'l,\n+        Result: FromIterator<PerItemResult>;\n+\n+    fn parallel_map_collect_owned<Item, PerItemResult, Result>(\n+        &self,\n+        items: Vec<Item>,\n+        f: impl Fn(Item) -> PerItemResult + Send + Sync,\n+    ) -> Result\n+    where\n+        Item: Send + Sync,\n+        PerItemResult: Send + Sync,\n+        Result: FromIterator<PerItemResult>;\n+}\n+\n+#[derive(Clone, Copy, Default)]\n+pub struct SerialScheduler;\n+\n+impl ParallelScheduler for SerialScheduler {\n+    fn parallel_for_each<T>(&self, items: &[T], f: impl Fn(&T) + Send + Sync)\n+    where\n+        T: Sync,\n+    {\n+        for item in items {\n+            f(item);\n+        }\n+    }\n+\n+    fn try_parallel_for_each<'l, T, E>(\n+        &self,\n+        items: &'l [T],\n+        f: impl (Fn(&'l T) -> Result<(), E>) + Send + Sync,\n+    ) -> Result<(), E>\n+    where\n+        T: Sync,\n+        E: Send,\n+    {\n+        for item in items {\n+            f(item)?;\n+        }\n+        Ok(())\n+    }\n+\n+    fn try_parallel_for_each_mut<'l, T, E>(\n+        &self,\n+        items: &'l mut [T],\n+        f: impl (Fn(&'l mut T) -> Result<(), E>) + Send + Sync,\n+    ) -> Result<(), E>\n+    where\n+        T: Sync,\n+        E: Send,\n+    {\n+        for item in items {\n+            f(item)?;\n+        }\n+        Ok(())\n+    }\n+\n+    fn try_parallel_for_each_owned<T, E>(\n+        &self,\n+        items: Vec<T>,\n+        f: impl (Fn(T) -> Result<(), E>) + Send + Sync,\n+    ) -> Result<(), E>\n+    where\n+        T: Sync,\n+        E: Send,\n+    {\n+        for item in items {\n+            f(item)?;\n+        }\n+        Ok(())\n+    }\n+\n+    fn parallel_map_collect<'l, Item, PerItemResult, Result>(\n+        &self,\n+        items: &'l [Item],\n+        f: impl Fn(&'l Item) -> PerItemResult + Send + Sync,\n+    ) -> Result\n+    where\n+        Item: Sync,\n+        PerItemResult: Send + Sync + 'l,\n+        Result: FromIterator<PerItemResult>,\n+    {\n+        items.iter().map(f).collect()\n+    }\n+\n+    fn parallel_map_collect_owned<Item, PerItemResult, Result>(\n+        &self,\n+        items: Vec<Item>,\n+        f: impl Fn(Item) -> PerItemResult + Send + Sync,\n+    ) -> Result\n+    where\n+        Item: Send + Sync,\n+        PerItemResult: Send + Sync,\n+        Result: FromIterator<PerItemResult>,\n+    {\n+        items.into_iter().map(f).collect()\n+    }\n+}"
        },
        {
            "sha": "6e0b42b92fe78ba77cb69d2e131500cf3dc632c5",
            "filename": "turbopack/crates/turbo-persistence/src/tests.rs",
            "status": "modified",
            "additions": 188,
            "deletions": 31,
            "changes": 219,
            "blob_url": "https://github.com/vercel/next.js/blob/450b07e8a445660c8c36a6a0103d38297a0754f7/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Ftests.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/450b07e8a445660c8c36a6a0103d38297a0754f7/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Ftests.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Ftests.rs?ref=450b07e8a445660c8c36a6a0103d38297a0754f7",
            "patch": "@@ -6,28 +6,116 @@ use rayon::iter::{IntoParallelIterator, ParallelIterator};\n use crate::{\n     constants::MAX_MEDIUM_VALUE_SIZE,\n     db::{CompactConfig, TurboPersistence},\n+    parallel_scheduler::ParallelScheduler,\n     write_batch::WriteBatch,\n };\n \n+#[derive(Clone, Copy)]\n+struct RayonParallelScheduler;\n+\n+impl ParallelScheduler for RayonParallelScheduler {\n+    fn parallel_for_each<T>(&self, items: &[T], f: impl Fn(&T) + Send + Sync)\n+    where\n+        T: Sync,\n+    {\n+        items.into_par_iter().for_each(f);\n+    }\n+\n+    fn try_parallel_for_each<'l, T, E>(\n+        &self,\n+        items: &'l [T],\n+        f: impl (Fn(&'l T) -> Result<(), E>) + Send + Sync,\n+    ) -> Result<(), E>\n+    where\n+        T: Sync,\n+        E: Send,\n+    {\n+        items.into_par_iter().try_for_each(f)\n+    }\n+\n+    fn try_parallel_for_each_mut<'l, T, E>(\n+        &self,\n+        items: &'l mut [T],\n+        f: impl (Fn(&'l mut T) -> Result<(), E>) + Send + Sync,\n+    ) -> Result<(), E>\n+    where\n+        T: Send + Sync,\n+        E: Send,\n+    {\n+        items.into_par_iter().try_for_each(f)\n+    }\n+\n+    fn try_parallel_for_each_owned<T, E>(\n+        &self,\n+        items: Vec<T>,\n+        f: impl (Fn(T) -> Result<(), E>) + Send + Sync,\n+    ) -> Result<(), E>\n+    where\n+        T: Send + Sync,\n+        E: Send,\n+    {\n+        items.into_par_iter().try_for_each(f)\n+    }\n+\n+    fn parallel_map_collect<'l, Item, PerItemResult, Result>(\n+        &self,\n+        items: &'l [Item],\n+        f: impl Fn(&'l Item) -> PerItemResult + Send + Sync,\n+    ) -> Result\n+    where\n+        Item: Sync,\n+        PerItemResult: Send + Sync,\n+        Result: FromIterator<PerItemResult>,\n+    {\n+        items\n+            .into_par_iter()\n+            .map(f)\n+            .collect_vec_list()\n+            .into_iter()\n+            .flatten()\n+            .collect()\n+    }\n+\n+    fn parallel_map_collect_owned<Item, PerItemResult, Result>(\n+        &self,\n+        items: Vec<Item>,\n+        f: impl Fn(Item) -> PerItemResult + Send + Sync,\n+    ) -> Result\n+    where\n+        Item: Send + Sync,\n+        PerItemResult: Send + Sync,\n+        Result: FromIterator<PerItemResult>,\n+    {\n+        items\n+            .into_par_iter()\n+            .map(f)\n+            .collect_vec_list()\n+            .into_iter()\n+            .flatten()\n+            .collect()\n+    }\n+}\n+\n #[test]\n fn full_cycle() -> Result<()> {\n     let mut test_cases = Vec::new();\n     type TestCases = Vec<(\n         &'static str,\n-        Box<dyn Fn(&mut WriteBatch<Vec<u8>, 16>) -> Result<()>>,\n-        Box<dyn Fn(&TurboPersistence) -> Result<()>>,\n+        Box<dyn Fn(&mut WriteBatch<Vec<u8>, RayonParallelScheduler, 16>) -> Result<()>>,\n+        Box<dyn Fn(&TurboPersistence<RayonParallelScheduler>) -> Result<()>>,\n     )>;\n \n     fn test_case(\n         test_cases: &mut TestCases,\n         name: &'static str,\n-        write: impl Fn(&mut WriteBatch<Vec<u8>, 16>) -> Result<()> + 'static,\n-        read: impl Fn(&TurboPersistence) -> Result<()> + 'static,\n+        write: impl Fn(&mut WriteBatch<Vec<u8>, RayonParallelScheduler, 16>) -> Result<()> + 'static,\n+        read: impl Fn(&TurboPersistence<RayonParallelScheduler>) -> Result<()> + 'static,\n     ) {\n         test_cases.push((\n             name,\n-            Box::new(write) as Box<dyn Fn(&mut WriteBatch<Vec<u8>, 16>) -> Result<()>>,\n-            Box::new(read) as Box<dyn Fn(&TurboPersistence) -> Result<()>>,\n+            Box::new(write)\n+                as Box<dyn Fn(&mut WriteBatch<Vec<u8>, RayonParallelScheduler, 16>) -> Result<()>>,\n+            Box::new(read) as Box<dyn Fn(&TurboPersistence<RayonParallelScheduler>) -> Result<()>>,\n         ));\n     }\n \n@@ -215,7 +303,10 @@ fn full_cycle() -> Result<()> {\n \n         {\n             let start = Instant::now();\n-            let db = TurboPersistence::open(path.to_path_buf())?;\n+            let db = TurboPersistence::open_with_parallel_scheduler(\n+                path.to_path_buf(),\n+                RayonParallelScheduler,\n+            )?;\n             let mut batch = db.write_batch()?;\n             write(&mut batch)?;\n             db.commit_write_batch(batch)?;\n@@ -231,7 +322,10 @@ fn full_cycle() -> Result<()> {\n         }\n         {\n             let start = Instant::now();\n-            let db = TurboPersistence::open(path.to_path_buf())?;\n+            let db = TurboPersistence::open_with_parallel_scheduler(\n+                path.to_path_buf(),\n+                RayonParallelScheduler,\n+            )?;\n             println!(\"{name} restore time: {:?}\", start.elapsed());\n             let start = Instant::now();\n             read(&db)?;\n@@ -257,7 +351,10 @@ fn full_cycle() -> Result<()> {\n         }\n         {\n             let start = Instant::now();\n-            let db = TurboPersistence::open(path.to_path_buf())?;\n+            let db = TurboPersistence::open_with_parallel_scheduler(\n+                path.to_path_buf(),\n+                RayonParallelScheduler,\n+            )?;\n             println!(\"{name} restore time after compact: {:?}\", start.elapsed());\n             let start = Instant::now();\n             read(&db)?;\n@@ -291,7 +388,10 @@ fn full_cycle() -> Result<()> {\n \n         {\n             let start = Instant::now();\n-            let db = TurboPersistence::open(path.to_path_buf())?;\n+            let db = TurboPersistence::open_with_parallel_scheduler(\n+                path.to_path_buf(),\n+                RayonParallelScheduler,\n+            )?;\n             let mut batch = db.write_batch()?;\n             for (_, write, _) in test_cases.iter() {\n                 write(&mut batch)?;\n@@ -311,7 +411,10 @@ fn full_cycle() -> Result<()> {\n         }\n         {\n             let start = Instant::now();\n-            let db = TurboPersistence::open(path.to_path_buf())?;\n+            let db = TurboPersistence::open_with_parallel_scheduler(\n+                path.to_path_buf(),\n+                RayonParallelScheduler,\n+            )?;\n             println!(\"All restore time: {:?}\", start.elapsed());\n             for (name, _, read) in test_cases.iter() {\n                 let start = Instant::now();\n@@ -343,7 +446,10 @@ fn full_cycle() -> Result<()> {\n \n         {\n             let start = Instant::now();\n-            let db = TurboPersistence::open(path.to_path_buf())?;\n+            let db = TurboPersistence::open_with_parallel_scheduler(\n+                path.to_path_buf(),\n+                RayonParallelScheduler,\n+            )?;\n             println!(\"All restore time after compact: {:?}\", start.elapsed());\n \n             for (name, _, read) in test_cases.iter() {\n@@ -383,13 +489,17 @@ fn persist_changes() -> Result<()> {\n     let path = tempdir.path();\n \n     const READ_COUNT: u32 = 2_000; // we'll read every 10th value, so writes are 10x this value\n-    fn put(b: &WriteBatch<(u8, [u8; 4]), 1>, key: u8, value: u8) -> Result<()> {\n+    fn put(\n+        b: &WriteBatch<(u8, [u8; 4]), RayonParallelScheduler, 1>,\n+        key: u8,\n+        value: u8,\n+    ) -> Result<()> {\n         for i in 0..(READ_COUNT * 10) {\n             b.put(0, (key, i.to_be_bytes()), vec![value].into())?;\n         }\n         Ok(())\n     }\n-    fn check(db: &TurboPersistence, key: u8, value: u8) -> Result<()> {\n+    fn check(db: &TurboPersistence<RayonParallelScheduler>, key: u8, value: u8) -> Result<()> {\n         for i in 0..READ_COUNT {\n             // read every 10th item\n             let i = i * 10;\n@@ -402,7 +512,10 @@ fn persist_changes() -> Result<()> {\n     }\n \n     {\n-        let db = TurboPersistence::open(path.to_path_buf())?;\n+        let db = TurboPersistence::open_with_parallel_scheduler(\n+            path.to_path_buf(),\n+            RayonParallelScheduler,\n+        )?;\n         let b = db.write_batch::<_, 1>()?;\n         put(&b, 1, 11)?;\n         put(&b, 2, 21)?;\n@@ -418,7 +531,10 @@ fn persist_changes() -> Result<()> {\n \n     println!(\"---\");\n     {\n-        let db = TurboPersistence::open(path.to_path_buf())?;\n+        let db = TurboPersistence::open_with_parallel_scheduler(\n+            path.to_path_buf(),\n+            RayonParallelScheduler,\n+        )?;\n         let b = db.write_batch::<_, 1>()?;\n         put(&b, 1, 12)?;\n         put(&b, 2, 22)?;\n@@ -432,7 +548,10 @@ fn persist_changes() -> Result<()> {\n     }\n \n     {\n-        let db = TurboPersistence::open(path.to_path_buf())?;\n+        let db = TurboPersistence::open_with_parallel_scheduler(\n+            path.to_path_buf(),\n+            RayonParallelScheduler,\n+        )?;\n         let b = db.write_batch::<_, 1>()?;\n         put(&b, 1, 13)?;\n         db.commit_write_batch(b)?;\n@@ -446,7 +565,10 @@ fn persist_changes() -> Result<()> {\n \n     println!(\"---\");\n     {\n-        let db = TurboPersistence::open(path.to_path_buf())?;\n+        let db = TurboPersistence::open_with_parallel_scheduler(\n+            path.to_path_buf(),\n+            RayonParallelScheduler,\n+        )?;\n \n         check(&db, 1, 13)?;\n         check(&db, 2, 22)?;\n@@ -457,7 +579,10 @@ fn persist_changes() -> Result<()> {\n \n     println!(\"---\");\n     {\n-        let db = TurboPersistence::open(path.to_path_buf())?;\n+        let db = TurboPersistence::open_with_parallel_scheduler(\n+            path.to_path_buf(),\n+            RayonParallelScheduler,\n+        )?;\n \n         db.compact(&CompactConfig {\n             optimal_merge_count: 4,\n@@ -475,7 +600,10 @@ fn persist_changes() -> Result<()> {\n \n     println!(\"---\");\n     {\n-        let db = TurboPersistence::open(path.to_path_buf())?;\n+        let db = TurboPersistence::open_with_parallel_scheduler(\n+            path.to_path_buf(),\n+            RayonParallelScheduler,\n+        )?;\n \n         check(&db, 1, 13)?;\n         check(&db, 2, 22)?;\n@@ -493,13 +621,17 @@ fn partial_compaction() -> Result<()> {\n     let path = tempdir.path();\n \n     const READ_COUNT: u32 = 2_000; // we'll read every 10th value, so writes are 10x this value\n-    fn put(b: &WriteBatch<(u8, [u8; 4]), 1>, key: u8, value: u8) -> Result<()> {\n+    fn put(\n+        b: &WriteBatch<(u8, [u8; 4]), RayonParallelScheduler, 1>,\n+        key: u8,\n+        value: u8,\n+    ) -> Result<()> {\n         for i in 0..(READ_COUNT * 10) {\n             b.put(0, (key, i.to_be_bytes()), vec![value].into())?;\n         }\n         Ok(())\n     }\n-    fn check(db: &TurboPersistence, key: u8, value: u8) -> Result<()> {\n+    fn check(db: &TurboPersistence<RayonParallelScheduler>, key: u8, value: u8) -> Result<()> {\n         for i in 0..READ_COUNT {\n             // read every 10th item\n             let i = i * 10;\n@@ -516,7 +648,10 @@ fn partial_compaction() -> Result<()> {\n         println!(\"--- Iteration {i} ---\");\n         println!(\"Add more entries\");\n         {\n-            let db = TurboPersistence::open(path.to_path_buf())?;\n+            let db = TurboPersistence::open_with_parallel_scheduler(\n+                path.to_path_buf(),\n+                RayonParallelScheduler,\n+            )?;\n             let b = db.write_batch::<_, 1>()?;\n             put(&b, i, i)?;\n             put(&b, i + 1, i)?;\n@@ -535,7 +670,10 @@ fn partial_compaction() -> Result<()> {\n \n         println!(\"Compaction\");\n         {\n-            let db = TurboPersistence::open(path.to_path_buf())?;\n+            let db = TurboPersistence::open_with_parallel_scheduler(\n+                path.to_path_buf(),\n+                RayonParallelScheduler,\n+            )?;\n \n             db.compact(&CompactConfig {\n                 optimal_merge_count: 4,\n@@ -556,7 +694,10 @@ fn partial_compaction() -> Result<()> {\n \n         println!(\"Restore check\");\n         {\n-            let db = TurboPersistence::open(path.to_path_buf())?;\n+            let db = TurboPersistence::open_with_parallel_scheduler(\n+                path.to_path_buf(),\n+                RayonParallelScheduler,\n+            )?;\n \n             for j in 0..i {\n                 check(&db, j, j)?;\n@@ -580,7 +721,11 @@ fn merge_file_removal() -> Result<()> {\n     let _ = fs::remove_dir_all(path);\n \n     const READ_COUNT: u32 = 2_000; // we'll read every 10th value, so writes are 10x this value\n-    fn put(b: &WriteBatch<(u8, [u8; 4]), 1>, key: u8, value: u32) -> Result<()> {\n+    fn put(\n+        b: &WriteBatch<(u8, [u8; 4]), RayonParallelScheduler, 1>,\n+        key: u8,\n+        value: u32,\n+    ) -> Result<()> {\n         for i in 0..(READ_COUNT * 10) {\n             b.put(\n                 0,\n@@ -590,7 +735,7 @@ fn merge_file_removal() -> Result<()> {\n         }\n         Ok(())\n     }\n-    fn check(db: &TurboPersistence, key: u8, value: u32) -> Result<()> {\n+    fn check(db: &TurboPersistence<RayonParallelScheduler>, key: u8, value: u32) -> Result<()> {\n         for i in 0..READ_COUNT {\n             // read every 10th item\n             let i = i * 10;\n@@ -608,7 +753,10 @@ fn merge_file_removal() -> Result<()> {\n \n     {\n         println!(\"--- Init ---\");\n-        let db = TurboPersistence::open(path.to_path_buf())?;\n+        let db = TurboPersistence::open_with_parallel_scheduler(\n+            path.to_path_buf(),\n+            RayonParallelScheduler,\n+        )?;\n         let b = db.write_batch::<_, 1>()?;\n         for j in 0..=255 {\n             put(&b, j, 0)?;\n@@ -624,7 +772,10 @@ fn merge_file_removal() -> Result<()> {\n         let i = i * 37;\n         println!(\"Add more entries\");\n         {\n-            let db = TurboPersistence::open(path.to_path_buf())?;\n+            let db = TurboPersistence::open_with_parallel_scheduler(\n+                path.to_path_buf(),\n+                RayonParallelScheduler,\n+            )?;\n             let b = db.write_batch::<_, 1>()?;\n             for j in iter_bits(i) {\n                 println!(\"Put {j} = {i}\");\n@@ -642,7 +793,10 @@ fn merge_file_removal() -> Result<()> {\n \n         println!(\"Compaction\");\n         {\n-            let db = TurboPersistence::open(path.to_path_buf())?;\n+            let db = TurboPersistence::open_with_parallel_scheduler(\n+                path.to_path_buf(),\n+                RayonParallelScheduler,\n+            )?;\n \n             db.compact(&CompactConfig {\n                 optimal_merge_count: 4,\n@@ -660,7 +814,10 @@ fn merge_file_removal() -> Result<()> {\n \n         println!(\"Restore check\");\n         {\n-            let db = TurboPersistence::open(path.to_path_buf())?;\n+            let db = TurboPersistence::open_with_parallel_scheduler(\n+                path.to_path_buf(),\n+                RayonParallelScheduler,\n+            )?;\n \n             for j in 0..32 {\n                 check(&db, j, expected_values[j as usize])?;"
        },
        {
            "sha": "81a954d0ef18eff11ee2ec0b4ea0390b084a84d6",
            "filename": "turbopack/crates/turbo-persistence/src/write_batch.rs",
            "status": "modified",
            "additions": 79,
            "deletions": 75,
            "changes": 154,
            "blob_url": "https://github.com/vercel/next.js/blob/450b07e8a445660c8c36a6a0103d38297a0754f7/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fwrite_batch.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/450b07e8a445660c8c36a6a0103d38297a0754f7/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fwrite_batch.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fwrite_batch.rs?ref=450b07e8a445660c8c36a6a0103d38297a0754f7",
            "patch": "@@ -9,15 +9,11 @@ use std::{\n \n use anyhow::{Context, Result};\n use byteorder::{BE, WriteBytesExt};\n+use either::Either;\n use lzzzz::lz4::{self, ACC_LEVEL_DEFAULT};\n use parking_lot::Mutex;\n-use rayon::{\n-    iter::{Either, IndexedParallelIterator, IntoParallelIterator, ParallelIterator},\n-    scope,\n-};\n use smallvec::SmallVec;\n use thread_local::ThreadLocal;\n-use tracing::Span;\n \n use crate::{\n     ValueBuffer,\n@@ -26,6 +22,7 @@ use crate::{\n     constants::{MAX_MEDIUM_VALUE_SIZE, THREAD_LOCAL_SIZE_SHIFT},\n     key::StoreKey,\n     meta_file_builder::MetaFileBuilder,\n+    parallel_scheduler::ParallelScheduler,\n     static_sorted_file_builder::{StaticSortedFileBuilderMeta, write_static_stored_file},\n };\n \n@@ -68,7 +65,9 @@ enum GlobalCollectorState<K: StoreKey + Send> {\n }\n \n /// A write batch.\n-pub struct WriteBatch<K: StoreKey + Send, const FAMILIES: usize> {\n+pub struct WriteBatch<K: StoreKey + Send, S: ParallelScheduler, const FAMILIES: usize> {\n+    /// Parallel scheduler\n+    parallel_scheduler: S,\n     /// The database path\n     db_path: PathBuf,\n     /// The current sequence number counter. Increased for every new SST file or blob file.\n@@ -84,13 +83,16 @@ pub struct WriteBatch<K: StoreKey + Send, const FAMILIES: usize> {\n     new_sst_files: Mutex<Vec<(u32, File)>>,\n }\n \n-impl<K: StoreKey + Send + Sync, const FAMILIES: usize> WriteBatch<K, FAMILIES> {\n+impl<K: StoreKey + Send + Sync, S: ParallelScheduler, const FAMILIES: usize>\n+    WriteBatch<K, S, FAMILIES>\n+{\n     /// Creates a new write batch for a database.\n-    pub(crate) fn new(path: PathBuf, current: u32) -> Self {\n+    pub(crate) fn new(path: PathBuf, current: u32, parallel_scheduler: S) -> Self {\n         const {\n             assert!(FAMILIES <= usize_from_u32(u32::MAX));\n         };\n         Self {\n+            parallel_scheduler,\n             db_path: path,\n             current_sequence_number: AtomicU32::new(current),\n             thread_locals: ThreadLocal::new(),\n@@ -223,13 +225,12 @@ impl<K: StoreKey + Send + Sync, const FAMILIES: usize> WriteBatch<K, FAMILIES> {\n             }\n         }\n \n-        let span = Span::current();\n-        collectors.into_par_iter().try_for_each(|mut collector| {\n-            let _span = span.clone().entered();\n-            self.flush_thread_local_collector(family, &mut collector)?;\n-            drop(collector);\n-            anyhow::Ok(())\n-        })?;\n+        self.parallel_scheduler\n+            .try_parallel_for_each_owned(collectors, |mut collector| {\n+                self.flush_thread_local_collector(family, &mut collector)?;\n+                drop(collector);\n+                anyhow::Ok(())\n+            })?;\n \n         // Now we flush the global collector(s).\n         let mut collector_state = self.collectors[usize_from_u32(family)].lock();\n@@ -242,22 +243,22 @@ impl<K: StoreKey + Send + Sync, const FAMILIES: usize> WriteBatch<K, FAMILIES> {\n                 }\n             }\n             GlobalCollectorState::Sharded(_) => {\n-                let GlobalCollectorState::Sharded(shards) = replace(\n+                let GlobalCollectorState::Sharded(mut shards) = replace(\n                     &mut *collector_state,\n                     GlobalCollectorState::Unsharded(Collector::new()),\n                 ) else {\n                     unreachable!();\n                 };\n-                shards.into_par_iter().try_for_each(|mut collector| {\n-                    let _span = span.clone().entered();\n-                    if !collector.is_empty() {\n-                        let sst = self.create_sst_file(family, collector.sorted())?;\n-                        collector.clear();\n-                        self.new_sst_files.lock().push(sst);\n-                        drop(collector);\n-                    }\n-                    anyhow::Ok(())\n-                })?;\n+                self.parallel_scheduler\n+                    .try_parallel_for_each_mut(&mut shards, |collector| {\n+                        if !collector.is_empty() {\n+                            let sst = self.create_sst_file(family, collector.sorted())?;\n+                            collector.clear();\n+                            self.new_sst_files.lock().push(sst);\n+                            collector.drop_contents();\n+                        }\n+                        anyhow::Ok(())\n+                    })?;\n             }\n         }\n \n@@ -269,10 +270,9 @@ impl<K: StoreKey + Send + Sync, const FAMILIES: usize> WriteBatch<K, FAMILIES> {\n     #[tracing::instrument(level = \"trace\", skip(self))]\n     pub(crate) fn finish(&mut self) -> Result<FinishResult> {\n         let mut new_blob_files = Vec::new();\n-        let shared_error = Mutex::new(Ok(()));\n \n         // First, we flush all thread local collectors to the global collectors.\n-        scope(|scope| {\n+        {\n             let _span = tracing::trace_span!(\"flush thread local collectors\").entered();\n             let mut collectors = [const { Vec::new() }; FAMILIES];\n             for cell in self.thread_locals.iter_mut() {\n@@ -286,23 +286,24 @@ impl<K: StoreKey + Send + Sync, const FAMILIES: usize> WriteBatch<K, FAMILIES> {\n                     }\n                 }\n             }\n-            for (family, thread_local_collectors) in collectors.into_iter().enumerate() {\n-                for mut collector in thread_local_collectors {\n-                    let this = &self;\n-                    let shared_error = &shared_error;\n-                    let span = Span::current();\n-                    scope.spawn(move |_| {\n-                        let _span = span.entered();\n-                        if let Err(err) =\n-                            this.flush_thread_local_collector(family as u32, &mut collector)\n-                        {\n-                            *shared_error.lock() = Err(err);\n-                        }\n-                        drop(collector);\n-                    });\n-                }\n-            }\n-        });\n+            let to_flush = collectors\n+                .into_iter()\n+                .enumerate()\n+                .flat_map(|(family, collector)| {\n+                    collector\n+                        .into_iter()\n+                        .map(move |collector| (family as u32, collector))\n+                })\n+                .collect::<Vec<_>>();\n+            self.parallel_scheduler.try_parallel_for_each_owned(\n+                to_flush,\n+                |(family, mut collector)| {\n+                    self.flush_thread_local_collector(family, &mut collector)?;\n+                    drop(collector);\n+                    anyhow::Ok(())\n+                },\n+            )?;\n+        }\n \n         let _span = tracing::trace_span!(\"flush collectors\").entered();\n \n@@ -313,25 +314,24 @@ impl<K: StoreKey + Send + Sync, const FAMILIES: usize> WriteBatch<K, FAMILIES> {\n         let new_collectors =\n             [(); FAMILIES].map(|_| Mutex::new(GlobalCollectorState::Unsharded(Collector::new())));\n         let collectors = replace(&mut self.collectors, new_collectors);\n-        let span = Span::current();\n-        collectors\n-            .into_par_iter()\n+        let collectors = collectors\n+            .into_iter()\n             .enumerate()\n             .flat_map(|(family, state)| {\n                 let collector = state.into_inner();\n                 match collector {\n                     GlobalCollectorState::Unsharded(collector) => {\n-                        Either::Left([(family, collector)].into_par_iter())\n+                        Either::Left([(family, collector)].into_iter())\n+                    }\n+                    GlobalCollectorState::Sharded(shards) => {\n+                        Either::Right(shards.into_iter().map(move |collector| (family, collector)))\n                     }\n-                    GlobalCollectorState::Sharded(shards) => Either::Right(\n-                        shards\n-                            .into_par_iter()\n-                            .map(move |collector| (family, collector)),\n-                    ),\n                 }\n             })\n-            .try_for_each(|(family, mut collector)| {\n-                let _span = span.clone().entered();\n+            .collect::<Vec<_>>();\n+        self.parallel_scheduler.try_parallel_for_each_owned(\n+            collectors,\n+            |(family, mut collector)| {\n                 let family = family as u32;\n                 if !collector.is_empty() {\n                     let sst = self.create_sst_file(family, collector.sorted())?;\n@@ -340,33 +340,37 @@ impl<K: StoreKey + Send + Sync, const FAMILIES: usize> WriteBatch<K, FAMILIES> {\n                     shared_new_sst_files.lock().push(sst);\n                 }\n                 anyhow::Ok(())\n-            })?;\n-\n-        shared_error.into_inner()?;\n+            },\n+        )?;\n \n         // Not we need to write the new meta files.\n         let new_meta_collectors = [(); FAMILIES].map(|_| Mutex::new(Vec::new()));\n         let meta_collectors = replace(&mut self.meta_collectors, new_meta_collectors);\n         let keys_written = AtomicU64::new(0);\n-        let new_meta_files = meta_collectors\n-            .into_par_iter()\n+        let file_to_write = meta_collectors\n+            .into_iter()\n             .map(|mutex| mutex.into_inner())\n             .enumerate()\n             .filter(|(_, sst_files)| !sst_files.is_empty())\n-            .map(|(family, sst_files)| {\n-                let family = family as u32;\n-                let mut entries = 0;\n-                let mut builder = MetaFileBuilder::new(family);\n-                for (seq, sst) in sst_files {\n-                    entries += sst.entries;\n-                    builder.add(seq, sst);\n-                }\n-                keys_written.fetch_add(entries, Ordering::Relaxed);\n-                let seq = self.current_sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n-                let file = builder.write(&self.db_path, seq)?;\n-                Ok((seq, file))\n-            })\n-            .collect::<Result<Vec<_>>>()?;\n+            .collect::<Vec<_>>();\n+        let new_meta_files = self\n+            .parallel_scheduler\n+            .parallel_map_collect_owned::<_, _, Result<Vec<_>>>(\n+                file_to_write,\n+                |(family, sst_files)| {\n+                    let family = family as u32;\n+                    let mut entries = 0;\n+                    let mut builder = MetaFileBuilder::new(family);\n+                    for (seq, sst) in sst_files {\n+                        entries += sst.entries;\n+                        builder.add(seq, sst);\n+                    }\n+                    keys_written.fetch_add(entries, Ordering::Relaxed);\n+                    let seq = self.current_sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n+                    let file = builder.write(&self.db_path, seq)?;\n+                    Ok((seq, file))\n+                },\n+            )?;\n \n         // Finally we return the new files and sequence number.\n         let seq = self.current_sequence_number.load(Ordering::SeqCst);"
        },
        {
            "sha": "a3347c355fa3bb46a665e630e3fa2b220a217e1b",
            "filename": "turbopack/crates/turbo-tasks-backend/src/database/turbo/mod.rs",
            "status": "renamed",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/vercel/next.js/blob/450b07e8a445660c8c36a6a0103d38297a0754f7/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fdatabase%2Fturbo%2Fmod.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/450b07e8a445660c8c36a6a0103d38297a0754f7/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fdatabase%2Fturbo%2Fmod.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fdatabase%2Fturbo%2Fmod.rs?ref=450b07e8a445660c8c36a6a0103d38297a0754f7",
            "patch": "@@ -9,9 +9,12 @@ use turbo_tasks::{JoinHandle, message_queue::TimingEvent, spawn, turbo_tasks};\n \n use crate::database::{\n     key_value_database::{KeySpace, KeyValueDatabase},\n+    turbo::parallel_scheduler::TurboTasksParallelScheduler,\n     write_batch::{BaseWriteBatch, ConcurrentWriteBatch, WriteBatch, WriteBuffer},\n };\n \n+mod parallel_scheduler;\n+\n const MB: u64 = 1024 * 1024;\n const COMPACT_CONFIG: CompactConfig = CompactConfig {\n     min_merge_count: 3,\n@@ -24,7 +27,7 @@ const COMPACT_CONFIG: CompactConfig = CompactConfig {\n };\n \n pub struct TurboKeyValueDatabase {\n-    db: Arc<TurboPersistence>,\n+    db: Arc<TurboPersistence<TurboTasksParallelScheduler>>,\n     compact_join_handle: Mutex<Option<JoinHandle<Result<()>>>>,\n     is_ci: bool,\n     is_short_session: bool,\n@@ -116,7 +119,7 @@ impl KeyValueDatabase for TurboKeyValueDatabase {\n }\n \n fn do_compact(\n-    db: &TurboPersistence,\n+    db: &TurboPersistence<TurboTasksParallelScheduler>,\n     message: &'static str,\n     max_merge_segment_count: usize,\n ) -> Result<()> {\n@@ -135,8 +138,8 @@ fn do_compact(\n }\n \n pub struct TurboWriteBatch<'a> {\n-    batch: turbo_persistence::WriteBatch<WriteBuffer<'static>, 5>,\n-    db: &'a Arc<TurboPersistence>,\n+    batch: turbo_persistence::WriteBatch<WriteBuffer<'static>, TurboTasksParallelScheduler, 5>,\n+    db: &'a Arc<TurboPersistence<TurboTasksParallelScheduler>>,\n     compact_join_handle: Option<&'a Mutex<Option<JoinHandle<Result<()>>>>>,\n }\n ",
            "previous_filename": "turbopack/crates/turbo-tasks-backend/src/database/turbo.rs"
        },
        {
            "sha": "dd78022c3640bec3159bb3138874868533cf843d",
            "filename": "turbopack/crates/turbo-tasks-backend/src/database/turbo/parallel_scheduler.rs",
            "status": "added",
            "additions": 76,
            "deletions": 0,
            "changes": 76,
            "blob_url": "https://github.com/vercel/next.js/blob/450b07e8a445660c8c36a6a0103d38297a0754f7/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fdatabase%2Fturbo%2Fparallel_scheduler.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/450b07e8a445660c8c36a6a0103d38297a0754f7/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fdatabase%2Fturbo%2Fparallel_scheduler.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fdatabase%2Fturbo%2Fparallel_scheduler.rs?ref=450b07e8a445660c8c36a6a0103d38297a0754f7",
            "patch": "@@ -0,0 +1,76 @@\n+use turbo_persistence::ParallelScheduler;\n+use turbo_tasks::parallel;\n+\n+#[derive(Clone, Copy, Default)]\n+pub struct TurboTasksParallelScheduler;\n+\n+impl ParallelScheduler for TurboTasksParallelScheduler {\n+    fn parallel_for_each<T>(&self, items: &[T], f: impl Fn(&T) + Send + Sync)\n+    where\n+        T: Sync,\n+    {\n+        parallel::for_each(items, f);\n+    }\n+\n+    fn try_parallel_for_each<'l, T, E>(\n+        &self,\n+        items: &'l [T],\n+        f: impl (Fn(&'l T) -> Result<(), E>) + Send + Sync,\n+    ) -> Result<(), E>\n+    where\n+        T: Sync,\n+        E: Send + 'static,\n+    {\n+        parallel::try_for_each(items, f)\n+    }\n+\n+    fn try_parallel_for_each_mut<'l, T, E>(\n+        &self,\n+        items: &'l mut [T],\n+        f: impl (Fn(&'l mut T) -> Result<(), E>) + Send + Sync,\n+    ) -> Result<(), E>\n+    where\n+        T: Send + Sync,\n+        E: Send + 'static,\n+    {\n+        parallel::try_for_each_mut(items, f)\n+    }\n+\n+    fn try_parallel_for_each_owned<T, E>(\n+        &self,\n+        items: Vec<T>,\n+        f: impl (Fn(T) -> Result<(), E>) + Send + Sync,\n+    ) -> Result<(), E>\n+    where\n+        T: Send + Sync,\n+        E: Send + 'static,\n+    {\n+        parallel::try_for_each_owned(items, f)\n+    }\n+\n+    fn parallel_map_collect<'l, T, I, R>(\n+        &self,\n+        items: &'l [T],\n+        f: impl Fn(&'l T) -> I + Send + Sync,\n+    ) -> R\n+    where\n+        T: Sync,\n+        I: Send + Sync + 'l,\n+        R: FromIterator<I>,\n+    {\n+        parallel::map_collect(items, f)\n+    }\n+\n+    fn parallel_map_collect_owned<T, I, R>(\n+        &self,\n+        items: Vec<T>,\n+        f: impl Fn(T) -> I + Send + Sync,\n+    ) -> R\n+    where\n+        T: Send + Sync,\n+        I: Send + Sync,\n+        R: FromIterator<I>,\n+    {\n+        parallel::map_collect_owned(items, f)\n+    }\n+}"
        }
    ],
    "stats": {
        "total": 1298,
        "additions": 852,
        "deletions": 446
    }
}