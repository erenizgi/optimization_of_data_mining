{
    "author": "sokra",
    "message": "Turbopack: avoid using rayon in favor of tokio tasks (#82256)\n\n### What?\n\nAvoid mixing rayon and tokio. Only use tokio. Implemented parallel for_each and map on top of tokio.\n\nCloses PACK-5158",
    "sha": "823b9f50cd21214260cc03890d20b81364fc22ce",
    "files": [
        {
            "sha": "dee3ceec04c7d5bf285645c58daf31e8d2187e4c",
            "filename": "Cargo.lock",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/Cargo.lock",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/Cargo.lock",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/Cargo.lock?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -9253,7 +9253,6 @@ dependencies = [\n  \"parking_lot\",\n  \"pot\",\n  \"rand 0.9.0\",\n- \"rayon\",\n  \"regex\",\n  \"ringmap\",\n  \"rstest\","
        },
        {
            "sha": "6a384bae92ab4e4071521e5b373d55aa44267204",
            "filename": "turbopack/crates/turbo-persistence-tools/src/main.rs",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-persistence-tools%2Fsrc%2Fmain.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-persistence-tools%2Fsrc%2Fmain.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence-tools%2Fsrc%2Fmain.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -3,7 +3,7 @@\n use std::path::PathBuf;\n \n use anyhow::{Context, Result, bail};\n-use turbo_persistence::{MetaFileEntryInfo, TurboPersistence};\n+use turbo_persistence::{MetaFileEntryInfo, SerialScheduler, TurboPersistence};\n \n fn main() -> Result<()> {\n     // Get CLI argument\n@@ -16,7 +16,7 @@ fn main() -> Result<()> {\n         bail!(\"The provided path does not exist: {}\", path.display());\n     }\n \n-    let db = TurboPersistence::open_read_only(path)?;\n+    let db: TurboPersistence<SerialScheduler> = TurboPersistence::open_read_only(path)?;\n     let meta_info = db\n         .meta_info()\n         .context(\"Failed to retrieve meta information\")?;"
        },
        {
            "sha": "ac4e7e5cc45f3b45597dc08257054ac0d192cafd",
            "filename": "turbopack/crates/turbo-persistence/Cargo.toml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-persistence%2FCargo.toml",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-persistence%2FCargo.toml",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2FCargo.toml?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -22,7 +22,6 @@ memmap2 = \"0.9.5\"\n parking_lot = { workspace = true }\n qfilter = { version = \"0.2.4\", features = [\"serde\"] }\n quick_cache = { workspace = true }\n-rayon = { workspace = true }\n rustc-hash = { workspace = true }\n smallvec = { workspace = true }\n thread_local = { workspace = true }\n@@ -32,6 +31,7 @@ zstd = { version = \"0.13.2\", features = [\"zdict_builder\"] }\n \n [dev-dependencies]\n rand = { workspace = true, features = [\"small_rng\"] }\n+rayon = { workspace = true }\n tempfile = { workspace = true }\n \n [lints]"
        },
        {
            "sha": "6637ea2c13e3ce0db4174e9fe1b1090b733b152f",
            "filename": "turbopack/crates/turbo-persistence/src/collector.rs",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fcollector.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fcollector.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fcollector.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -1,3 +1,5 @@\n+use std::mem::take;\n+\n use crate::{\n     ValueBuffer,\n     collector_entry::{CollectorEntry, CollectorEntryValue, EntryKey},\n@@ -111,4 +113,11 @@ impl<K: StoreKey, const SIZE_SHIFT: usize> Collector<K, SIZE_SHIFT> {\n         self.total_value_size = 0;\n         self.entries.drain(..)\n     }\n+\n+    /// Clears the collector and drops the capacity\n+    pub fn drop_contents(&mut self) {\n+        drop(take(&mut self.entries));\n+        self.total_key_size = 0;\n+        self.total_value_size = 0;\n+    }\n }"
        },
        {
            "sha": "9936c9349cc8b72f53d81b5ef37befd8aa611d69",
            "filename": "turbopack/crates/turbo-persistence/src/db.rs",
            "status": "modified",
            "additions": 347,
            "deletions": 329,
            "changes": 676,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fdb.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fdb.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fdb.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -18,8 +18,6 @@ use jiff::Timestamp;\n use lzzzz::lz4::decompress;\n use memmap2::Mmap;\n use parking_lot::{Mutex, RwLock};\n-use rayon::iter::{IndexedParallelIterator, IntoParallelIterator, ParallelIterator};\n-use tracing::Span;\n \n pub use crate::compaction::selector::CompactConfig;\n use crate::{\n@@ -36,6 +34,7 @@ use crate::{\n     merge_iter::MergeIter,\n     meta_file::{AmqfCache, MetaFile, MetaLookupResult, StaticSortedFileRange},\n     meta_file_builder::MetaFileBuilder,\n+    parallel_scheduler::ParallelScheduler,\n     sst_filter::SstFilter,\n     static_sorted_file::{BlockCache, SstLookupResult},\n     static_sorted_file_builder::{StaticSortedFileBuilderMeta, write_static_stored_file},\n@@ -108,7 +107,8 @@ struct TrackedStats {\n \n /// TurboPersistence is a persistent key-value store. It is limited to a single writer at a time\n /// using a single write batch. It allows for concurrent reads.\n-pub struct TurboPersistence {\n+pub struct TurboPersistence<S: ParallelScheduler> {\n+    parallel_scheduler: S,\n     /// The path to the directory where the database is stored\n     path: PathBuf,\n     /// If true, the database is opened in read-only mode. In this mode, no writes are allowed and\n@@ -148,9 +148,26 @@ pub struct CommitOptions {\n     keys_written: u64,\n }\n \n-impl TurboPersistence {\n-    fn new(path: PathBuf, read_only: bool) -> Self {\n+impl<S: ParallelScheduler + Default> TurboPersistence<S> {\n+    /// Open a TurboPersistence database at the given path.\n+    /// This will read the directory and might performance cleanup when the database was not closed\n+    /// properly. Cleanup only requires to read a few bytes from a few files and to delete\n+    /// files, so it's fast.\n+    pub fn open(path: PathBuf) -> Result<Self> {\n+        Self::open_with_parallel_scheduler(path, Default::default())\n+    }\n+\n+    /// Open a TurboPersistence database at the given path in read only mode.\n+    /// This will read the directory. No Cleanup is performed.\n+    pub fn open_read_only(path: PathBuf) -> Result<Self> {\n+        Self::open_read_only_with_parallel_scheduler(path, Default::default())\n+    }\n+}\n+\n+impl<S: ParallelScheduler> TurboPersistence<S> {\n+    fn new(path: PathBuf, read_only: bool, parallel_scheduler: S) -> Self {\n         Self {\n+            parallel_scheduler,\n             path,\n             read_only,\n             inner: RwLock::new(Inner {\n@@ -188,16 +205,19 @@ impl TurboPersistence {\n     /// This will read the directory and might performance cleanup when the database was not closed\n     /// properly. Cleanup only requires to read a few bytes from a few files and to delete\n     /// files, so it's fast.\n-    pub fn open(path: PathBuf) -> Result<Self> {\n-        let mut db = Self::new(path, false);\n+    pub fn open_with_parallel_scheduler(path: PathBuf, parallel_scheduler: S) -> Result<Self> {\n+        let mut db = Self::new(path, false, parallel_scheduler);\n         db.open_directory(false)?;\n         Ok(db)\n     }\n \n     /// Open a TurboPersistence database at the given path in read only mode.\n     /// This will read the directory. No Cleanup is performed.\n-    pub fn open_read_only(path: PathBuf) -> Result<Self> {\n-        let mut db = Self::new(path, true);\n+    pub fn open_read_only_with_parallel_scheduler(\n+        path: PathBuf,\n+        parallel_scheduler: S,\n+    ) -> Result<Self> {\n+        let mut db = Self::new(path, true, parallel_scheduler);\n         db.open_directory(false)?;\n         Ok(db)\n     }\n@@ -341,16 +361,12 @@ impl TurboPersistence {\n \n         meta_files.retain(|seq| !deleted_files.contains(seq));\n         meta_files.sort_unstable();\n-        let span = Span::current();\n-        let mut meta_files = meta_files\n-            .into_par_iter()\n-            .with_min_len(1)\n-            .map(|seq| {\n-                let _span = span.enter();\n+        let mut meta_files = self\n+            .parallel_scheduler\n+            .parallel_map_collect::<_, _, Result<Vec<MetaFile>>>(&meta_files, |&seq| {\n                 let meta_file = MetaFile::open(&self.path, seq)?;\n                 Ok(meta_file)\n-            })\n-            .collect::<Result<Vec<MetaFile>>>()?;\n+            })?;\n \n         let mut sst_filter = SstFilter::new();\n         for meta_file in meta_files.iter_mut().rev() {\n@@ -398,7 +414,7 @@ impl TurboPersistence {\n     /// This data will only become visible after the WriteBatch is committed.\n     pub fn write_batch<K: StoreKey + Send + Sync + 'static, const FAMILIES: usize>(\n         &self,\n-    ) -> Result<WriteBatch<K, FAMILIES>> {\n+    ) -> Result<WriteBatch<K, S, FAMILIES>> {\n         if self.read_only {\n             bail!(\"Cannot write to a read-only database\");\n         }\n@@ -413,7 +429,11 @@ impl TurboPersistence {\n             );\n         }\n         let current = self.inner.read().current_sequence_number;\n-        Ok(WriteBatch::new(self.path.clone(), current))\n+        Ok(WriteBatch::new(\n+            self.path.clone(),\n+            current,\n+            self.parallel_scheduler.clone(),\n+        ))\n     }\n \n     fn open_log(&self) -> Result<BufWriter<File>> {\n@@ -432,7 +452,7 @@ impl TurboPersistence {\n     /// visible to readers.\n     pub fn commit_write_batch<K: StoreKey + Send + Sync + 'static, const FAMILIES: usize>(\n         &self,\n-        mut write_batch: WriteBatch<K, FAMILIES>,\n+        mut write_batch: WriteBatch<K, S, FAMILIES>,\n     ) -> Result<()> {\n         if self.read_only {\n             unreachable!(\"It's not possible to create a write batch for a read-only database\");\n@@ -475,15 +495,13 @@ impl TurboPersistence {\n \n         new_meta_files.sort_unstable_by_key(|(seq, _)| *seq);\n \n-        let mut new_meta_files = new_meta_files\n-            .into_par_iter()\n-            .with_min_len(1)\n-            .map(|(seq, file)| {\n+        let mut new_meta_files = self\n+            .parallel_scheduler\n+            .parallel_map_collect_owned::<_, _, Result<Vec<_>>>(new_meta_files, |(seq, file)| {\n                 file.sync_all()?;\n                 let meta_file = MetaFile::open(&self.path, seq)?;\n                 Ok(meta_file)\n-            })\n-            .collect::<Result<Vec<_>>>()?;\n+            })?;\n \n         let mut sst_filter = SstFilter::new();\n         for meta_file in new_meta_files.iter_mut().rev() {\n@@ -777,7 +795,6 @@ impl TurboPersistence {\n         let path = &self.path;\n \n         let log_mutex = Mutex::new(());\n-        let span = Span::current();\n \n         struct PartialResultPerFamily {\n             new_meta_file: Option<(u32, File)>,\n@@ -789,336 +806,337 @@ impl TurboPersistence {\n \n         let mut compact_config = compact_config.clone();\n         let merge_jobs = sst_by_family\n-            .iter()\n-            .map(|ssts_with_ranges| {\n+            .into_iter()\n+            .enumerate()\n+            .filter_map(|(family, ssts_with_ranges)| {\n                 if compact_config.max_merge_segment_count == 0 {\n-                    return Vec::new();\n+                    return None;\n                 }\n                 let (merge_jobs, real_merge_job_size) =\n-                    get_merge_segments(ssts_with_ranges, &compact_config);\n+                    get_merge_segments(&ssts_with_ranges, &compact_config);\n                 compact_config.max_merge_segment_count -= real_merge_job_size;\n-                merge_jobs\n+                Some((family, ssts_with_ranges, merge_jobs))\n             })\n             .collect::<Vec<_>>();\n \n-        let result = sst_by_family\n-            .into_par_iter()\n-            .zip(merge_jobs.into_par_iter())\n-            .with_min_len(1)\n-            .enumerate()\n-            .map(|(family, (ssts_with_ranges, merge_jobs))| {\n-                let family = family as u32;\n-                let _span = span.clone().entered();\n-\n-                if merge_jobs.is_empty() {\n-                    return Ok(PartialResultPerFamily {\n-                        new_meta_file: None,\n-                        new_sst_files: Vec::new(),\n-                        sst_seq_numbers_to_delete: Vec::new(),\n-                        blob_seq_numbers_to_delete: Vec::new(),\n-                        keys_written: 0,\n-                    });\n-                }\n-\n-                {\n-                    let metrics = compute_metrics(&ssts_with_ranges, 0..=u64::MAX);\n-                    let guard = log_mutex.lock();\n-                    let mut log = self.open_log()?;\n-                    writeln!(\n-                        log,\n-                        \"Compaction for family {family} (coverage: {}, overlap: {}, duplication: \\\n-                         {} / {} MiB):\",\n-                        metrics.coverage,\n-                        metrics.overlap,\n-                        metrics.duplication,\n-                        metrics.duplicated_size / 1024 / 1024\n-                    )?;\n-                    for job in merge_jobs.iter() {\n-                        writeln!(log, \"  merge\")?;\n-                        for i in job.iter() {\n-                            let seq = ssts_with_ranges[*i].seq;\n-                            let (min, max) = ssts_with_ranges[*i].range().into_inner();\n-                            writeln!(log, \"    {seq:08} {min:016x}-{max:016x}\")?;\n-                        }\n+        let result = self\n+            .parallel_scheduler\n+            .parallel_map_collect_owned::<_, _, Result<Vec<_>>>(\n+                merge_jobs,\n+                |(family, ssts_with_ranges, merge_jobs)| {\n+                    let family = family as u32;\n+\n+                    if merge_jobs.is_empty() {\n+                        return Ok(PartialResultPerFamily {\n+                            new_meta_file: None,\n+                            new_sst_files: Vec::new(),\n+                            sst_seq_numbers_to_delete: Vec::new(),\n+                            blob_seq_numbers_to_delete: Vec::new(),\n+                            keys_written: 0,\n+                        });\n                     }\n-                    drop(guard);\n-                }\n \n-                // Later we will remove the merged files\n-                let sst_seq_numbers_to_delete = merge_jobs\n-                    .iter()\n-                    .filter(|l| l.len() > 1)\n-                    .flat_map(|l| l.iter().copied())\n-                    .map(|index| ssts_with_ranges[index].seq)\n-                    .collect::<Vec<_>>();\n-\n-                // Merge SST files\n-                let span = tracing::trace_span!(\"merge files\");\n-                enum PartialMergeResult<'l> {\n-                    Merged {\n-                        new_sst_files: Vec<(u32, File, StaticSortedFileBuilderMeta<'static>)>,\n-                        blob_seq_numbers_to_delete: Vec<u32>,\n-                        keys_written: u64,\n-                    },\n-                    Move {\n-                        seq: u32,\n-                        meta: StaticSortedFileBuilderMeta<'l>,\n-                    },\n-                }\n-                let merge_result = merge_jobs\n-                    .into_par_iter()\n-                    .with_min_len(1)\n-                    .map(|indices| {\n-                        let _span = span.clone().entered();\n-                        if indices.len() == 1 {\n-                            // If we only have one file, we can just move it\n-                            let index = indices[0];\n-                            let meta_index = ssts_with_ranges[index].meta_index;\n-                            let index_in_meta = ssts_with_ranges[index].index_in_meta;\n-                            let meta_file = &meta_files[meta_index];\n-                            let entry = meta_file.entry(index_in_meta);\n-                            let amqf = Cow::Borrowed(entry.raw_amqf(meta_file.amqf_data()));\n-                            let meta = StaticSortedFileBuilderMeta {\n-                                min_hash: entry.min_hash(),\n-                                max_hash: entry.max_hash(),\n-                                amqf,\n-                                key_compression_dictionary_length: entry\n-                                    .key_compression_dictionary_length(),\n-                                value_compression_dictionary_length: entry\n-                                    .value_compression_dictionary_length(),\n-                                block_count: entry.block_count(),\n-                                size: entry.size(),\n-                                entries: 0,\n-                            };\n-                            return Ok(PartialMergeResult::Move {\n-                                seq: entry.sequence_number(),\n-                                meta,\n-                            });\n+                    {\n+                        let metrics = compute_metrics(&ssts_with_ranges, 0..=u64::MAX);\n+                        let guard = log_mutex.lock();\n+                        let mut log = self.open_log()?;\n+                        writeln!(\n+                            log,\n+                            \"Compaction for family {family} (coverage: {}, overlap: {}, \\\n+                             duplication: {} / {} MiB):\",\n+                            metrics.coverage,\n+                            metrics.overlap,\n+                            metrics.duplication,\n+                            metrics.duplicated_size / 1024 / 1024\n+                        )?;\n+                        for job in merge_jobs.iter() {\n+                            writeln!(log, \"  merge\")?;\n+                            for i in job.iter() {\n+                                let seq = ssts_with_ranges[*i].seq;\n+                                let (min, max) = ssts_with_ranges[*i].range().into_inner();\n+                                writeln!(log, \"    {seq:08} {min:016x}-{max:016x}\")?;\n+                            }\n                         }\n+                        drop(guard);\n+                    }\n \n-                        fn create_sst_file(\n-                            entries: &[LookupEntry],\n-                            total_key_size: usize,\n-                            total_value_size: usize,\n-                            path: &Path,\n+                    // Later we will remove the merged files\n+                    let sst_seq_numbers_to_delete = merge_jobs\n+                        .iter()\n+                        .filter(|l| l.len() > 1)\n+                        .flat_map(|l| l.iter().copied())\n+                        .map(|index| ssts_with_ranges[index].seq)\n+                        .collect::<Vec<_>>();\n+\n+                    // Merge SST files\n+                    let span = tracing::trace_span!(\"merge files\");\n+                    enum PartialMergeResult<'l> {\n+                        Merged {\n+                            new_sst_files: Vec<(u32, File, StaticSortedFileBuilderMeta<'static>)>,\n+                            blob_seq_numbers_to_delete: Vec<u32>,\n+                            keys_written: u64,\n+                        },\n+                        Move {\n                             seq: u32,\n-                        ) -> Result<(u32, File, StaticSortedFileBuilderMeta<'static>)>\n-                        {\n-                            let _span = tracing::trace_span!(\"write merged sst file\").entered();\n-                            let (meta, file) = write_static_stored_file(\n-                                entries,\n-                                total_key_size,\n-                                total_value_size,\n-                                &path.join(format!(\"{seq:08}.sst\")),\n-                            )?;\n-                            Ok((seq, file, meta))\n-                        }\n-\n-                        let mut new_sst_files = Vec::new();\n-\n-                        // Iterate all SST files\n-                        let iters = indices\n-                            .iter()\n-                            .map(|&index| {\n+                            meta: StaticSortedFileBuilderMeta<'l>,\n+                        },\n+                    }\n+                    let merge_result = self\n+                        .parallel_scheduler\n+                        .parallel_map_collect_owned::<_, _, Result<Vec<_>>>(merge_jobs, |indices| {\n+                            let _span = span.clone().entered();\n+                            if indices.len() == 1 {\n+                                // If we only have one file, we can just move it\n+                                let index = indices[0];\n                                 let meta_index = ssts_with_ranges[index].meta_index;\n                                 let index_in_meta = ssts_with_ranges[index].index_in_meta;\n-                                let meta = &meta_files[meta_index];\n-                                meta.entry(index_in_meta)\n-                                    .sst(meta)?\n-                                    .iter(key_block_cache, value_block_cache)\n-                            })\n-                            .collect::<Result<Vec<_>>>()?;\n-\n-                        let iter = MergeIter::new(iters.into_iter())?;\n-\n-                        // TODO figure out how to delete blobs when they are no longer\n-                        // referenced\n-                        let blob_seq_numbers_to_delete: Vec<u32> = Vec::new();\n-\n-                        let mut keys_written = 0;\n-\n-                        let mut total_key_size = 0;\n-                        let mut total_value_size = 0;\n-                        let mut current: Option<LookupEntry> = None;\n-                        let mut entries = Vec::new();\n-                        let mut last_entries = Vec::new();\n-                        let mut last_entries_total_sizes = (0, 0);\n-                        for entry in iter {\n-                            let entry = entry?;\n-\n-                            // Remove duplicates\n-                            if let Some(current) = current.take() {\n-                                if current.key != entry.key {\n-                                    let key_size = current.key.len();\n-                                    let value_size = current.value.size_in_sst();\n-                                    total_key_size += key_size;\n-                                    total_value_size += value_size;\n-\n-                                    if total_key_size + total_value_size\n-                                        > DATA_THRESHOLD_PER_COMPACTED_FILE\n-                                        || entries.len() >= MAX_ENTRIES_PER_COMPACTED_FILE\n-                                    {\n-                                        let (selected_total_key_size, selected_total_value_size) =\n-                                            last_entries_total_sizes;\n-                                        swap(&mut entries, &mut last_entries);\n-                                        last_entries_total_sizes = (\n-                                            total_key_size - key_size,\n-                                            total_value_size - value_size,\n-                                        );\n-                                        total_key_size = key_size;\n-                                        total_value_size = value_size;\n-\n-                                        if !entries.is_empty() {\n-                                            let seq =\n-                                                sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n-\n-                                            keys_written += entries.len() as u64;\n-                                            new_sst_files.push(create_sst_file(\n-                                                &entries,\n+                                let meta_file = &meta_files[meta_index];\n+                                let entry = meta_file.entry(index_in_meta);\n+                                let amqf = Cow::Borrowed(entry.raw_amqf(meta_file.amqf_data()));\n+                                let meta = StaticSortedFileBuilderMeta {\n+                                    min_hash: entry.min_hash(),\n+                                    max_hash: entry.max_hash(),\n+                                    amqf,\n+                                    key_compression_dictionary_length: entry\n+                                        .key_compression_dictionary_length(),\n+                                    value_compression_dictionary_length: entry\n+                                        .value_compression_dictionary_length(),\n+                                    block_count: entry.block_count(),\n+                                    size: entry.size(),\n+                                    entries: 0,\n+                                };\n+                                return Ok(PartialMergeResult::Move {\n+                                    seq: entry.sequence_number(),\n+                                    meta,\n+                                });\n+                            }\n+\n+                            fn create_sst_file(\n+                                entries: &[LookupEntry],\n+                                total_key_size: usize,\n+                                total_value_size: usize,\n+                                path: &Path,\n+                                seq: u32,\n+                            ) -> Result<(u32, File, StaticSortedFileBuilderMeta<'static>)>\n+                            {\n+                                let _span = tracing::trace_span!(\"write merged sst file\").entered();\n+                                let (meta, file) = write_static_stored_file(\n+                                    entries,\n+                                    total_key_size,\n+                                    total_value_size,\n+                                    &path.join(format!(\"{seq:08}.sst\")),\n+                                )?;\n+                                Ok((seq, file, meta))\n+                            }\n+\n+                            let mut new_sst_files = Vec::new();\n+\n+                            // Iterate all SST files\n+                            let iters = indices\n+                                .iter()\n+                                .map(|&index| {\n+                                    let meta_index = ssts_with_ranges[index].meta_index;\n+                                    let index_in_meta = ssts_with_ranges[index].index_in_meta;\n+                                    let meta = &meta_files[meta_index];\n+                                    meta.entry(index_in_meta)\n+                                        .sst(meta)?\n+                                        .iter(key_block_cache, value_block_cache)\n+                                })\n+                                .collect::<Result<Vec<_>>>()?;\n+\n+                            let iter = MergeIter::new(iters.into_iter())?;\n+\n+                            // TODO figure out how to delete blobs when they are no longer\n+                            // referenced\n+                            let blob_seq_numbers_to_delete: Vec<u32> = Vec::new();\n+\n+                            let mut keys_written = 0;\n+\n+                            let mut total_key_size = 0;\n+                            let mut total_value_size = 0;\n+                            let mut current: Option<LookupEntry> = None;\n+                            let mut entries = Vec::new();\n+                            let mut last_entries = Vec::new();\n+                            let mut last_entries_total_sizes = (0, 0);\n+                            for entry in iter {\n+                                let entry = entry?;\n+\n+                                // Remove duplicates\n+                                if let Some(current) = current.take() {\n+                                    if current.key != entry.key {\n+                                        let key_size = current.key.len();\n+                                        let value_size = current.value.size_in_sst();\n+                                        total_key_size += key_size;\n+                                        total_value_size += value_size;\n+\n+                                        if total_key_size + total_value_size\n+                                            > DATA_THRESHOLD_PER_COMPACTED_FILE\n+                                            || entries.len() >= MAX_ENTRIES_PER_COMPACTED_FILE\n+                                        {\n+                                            let (\n                                                 selected_total_key_size,\n                                                 selected_total_value_size,\n-                                                path,\n-                                                seq,\n-                                            )?);\n-\n-                                            entries.clear();\n+                                            ) = last_entries_total_sizes;\n+                                            swap(&mut entries, &mut last_entries);\n+                                            last_entries_total_sizes = (\n+                                                total_key_size - key_size,\n+                                                total_value_size - value_size,\n+                                            );\n+                                            total_key_size = key_size;\n+                                            total_value_size = value_size;\n+\n+                                            if !entries.is_empty() {\n+                                                let seq = sequence_number\n+                                                    .fetch_add(1, Ordering::SeqCst)\n+                                                    + 1;\n+\n+                                                keys_written += entries.len() as u64;\n+                                                new_sst_files.push(create_sst_file(\n+                                                    &entries,\n+                                                    selected_total_key_size,\n+                                                    selected_total_value_size,\n+                                                    path,\n+                                                    seq,\n+                                                )?);\n+\n+                                                entries.clear();\n+                                            }\n                                         }\n-                                    }\n \n-                                    entries.push(current);\n-                                } else {\n-                                    // Override value\n+                                        entries.push(current);\n+                                    } else {\n+                                        // Override value\n+                                    }\n                                 }\n+                                current = Some(entry);\n+                            }\n+                            if let Some(entry) = current {\n+                                total_key_size += entry.key.len();\n+                                total_value_size += entry.value.size_in_sst();\n+                                entries.push(entry);\n                             }\n-                            current = Some(entry);\n-                        }\n-                        if let Some(entry) = current {\n-                            total_key_size += entry.key.len();\n-                            total_value_size += entry.value.size_in_sst();\n-                            entries.push(entry);\n-                        }\n \n-                        // If we have one set of entries left, write them to a new SST file\n-                        if last_entries.is_empty() && !entries.is_empty() {\n-                            let seq = sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n-\n-                            keys_written += entries.len() as u64;\n-                            new_sst_files.push(create_sst_file(\n-                                &entries,\n-                                total_key_size,\n-                                total_value_size,\n-                                path,\n-                                seq,\n-                            )?);\n-                        } else\n-                        // If we have two sets of entries left, merge them and\n-                        // split it into two SST files, to avoid having a\n-                        // single SST file that is very small.\n-                        if !last_entries.is_empty() {\n-                            last_entries.append(&mut entries);\n-\n-                            last_entries_total_sizes.0 += total_key_size;\n-                            last_entries_total_sizes.1 += total_value_size;\n-\n-                            let (part1, part2) = last_entries.split_at(last_entries.len() / 2);\n-\n-                            let seq1 = sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n-                            let seq2 = sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n-\n-                            keys_written += part1.len() as u64;\n-                            new_sst_files.push(create_sst_file(\n-                                part1,\n-                                // We don't know the exact sizes so we estimate them\n-                                last_entries_total_sizes.0 / 2,\n-                                last_entries_total_sizes.1 / 2,\n-                                path,\n-                                seq1,\n-                            )?);\n-\n-                            keys_written += part2.len() as u64;\n-                            new_sst_files.push(create_sst_file(\n-                                part2,\n-                                last_entries_total_sizes.0 / 2,\n-                                last_entries_total_sizes.1 / 2,\n-                                path,\n-                                seq2,\n-                            )?);\n-                        }\n-                        Ok(PartialMergeResult::Merged {\n-                            new_sst_files,\n-                            blob_seq_numbers_to_delete,\n-                            keys_written,\n+                            // If we have one set of entries left, write them to a new SST file\n+                            if last_entries.is_empty() && !entries.is_empty() {\n+                                let seq = sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n+\n+                                keys_written += entries.len() as u64;\n+                                new_sst_files.push(create_sst_file(\n+                                    &entries,\n+                                    total_key_size,\n+                                    total_value_size,\n+                                    path,\n+                                    seq,\n+                                )?);\n+                            } else\n+                            // If we have two sets of entries left, merge them and\n+                            // split it into two SST files, to avoid having a\n+                            // single SST file that is very small.\n+                            if !last_entries.is_empty() {\n+                                last_entries.append(&mut entries);\n+\n+                                last_entries_total_sizes.0 += total_key_size;\n+                                last_entries_total_sizes.1 += total_value_size;\n+\n+                                let (part1, part2) = last_entries.split_at(last_entries.len() / 2);\n+\n+                                let seq1 = sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n+                                let seq2 = sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n+\n+                                keys_written += part1.len() as u64;\n+                                new_sst_files.push(create_sst_file(\n+                                    part1,\n+                                    // We don't know the exact sizes so we estimate them\n+                                    last_entries_total_sizes.0 / 2,\n+                                    last_entries_total_sizes.1 / 2,\n+                                    path,\n+                                    seq1,\n+                                )?);\n+\n+                                keys_written += part2.len() as u64;\n+                                new_sst_files.push(create_sst_file(\n+                                    part2,\n+                                    last_entries_total_sizes.0 / 2,\n+                                    last_entries_total_sizes.1 / 2,\n+                                    path,\n+                                    seq2,\n+                                )?);\n+                            }\n+                            Ok(PartialMergeResult::Merged {\n+                                new_sst_files,\n+                                blob_seq_numbers_to_delete,\n+                                keys_written,\n+                            })\n                         })\n-                    })\n-                    .collect::<Result<Vec<_>>>()\n-                    .with_context(|| {\n-                        format!(\"Failed to merge database files for family {family}\")\n-                    })?;\n-\n-                let Some((sst_files_len, blob_delete_len)) = merge_result\n-                    .iter()\n-                    .map(|r| {\n-                        if let PartialMergeResult::Merged {\n-                            new_sst_files,\n-                            blob_seq_numbers_to_delete,\n-                            keys_written: _,\n-                        } = r\n-                        {\n-                            (new_sst_files.len(), blob_seq_numbers_to_delete.len())\n-                        } else {\n-                            (0, 0)\n-                        }\n-                    })\n-                    .reduce(|(a1, a2), (b1, b2)| (a1 + b1, a2 + b2))\n-                else {\n-                    unreachable!()\n-                };\n-\n-                let mut new_sst_files = Vec::with_capacity(sst_files_len);\n-                let mut blob_seq_numbers_to_delete = Vec::with_capacity(blob_delete_len);\n-\n-                let mut meta_file_builder = MetaFileBuilder::new(family);\n-\n-                let mut keys_written = 0;\n-                for result in merge_result {\n-                    match result {\n-                        PartialMergeResult::Merged {\n-                            new_sst_files: merged_new_sst_files,\n-                            blob_seq_numbers_to_delete: merged_blob_seq_numbers_to_delete,\n-                            keys_written: merged_keys_written,\n-                        } => {\n-                            for (seq, file, meta) in merged_new_sst_files {\n+                        .with_context(|| {\n+                            format!(\"Failed to merge database files for family {family}\")\n+                        })?;\n+\n+                    let Some((sst_files_len, blob_delete_len)) = merge_result\n+                        .iter()\n+                        .map(|r| {\n+                            if let PartialMergeResult::Merged {\n+                                new_sst_files,\n+                                blob_seq_numbers_to_delete,\n+                                keys_written: _,\n+                            } = r\n+                            {\n+                                (new_sst_files.len(), blob_seq_numbers_to_delete.len())\n+                            } else {\n+                                (0, 0)\n+                            }\n+                        })\n+                        .reduce(|(a1, a2), (b1, b2)| (a1 + b1, a2 + b2))\n+                    else {\n+                        unreachable!()\n+                    };\n+\n+                    let mut new_sst_files = Vec::with_capacity(sst_files_len);\n+                    let mut blob_seq_numbers_to_delete = Vec::with_capacity(blob_delete_len);\n+\n+                    let mut meta_file_builder = MetaFileBuilder::new(family);\n+\n+                    let mut keys_written = 0;\n+                    for result in merge_result {\n+                        match result {\n+                            PartialMergeResult::Merged {\n+                                new_sst_files: merged_new_sst_files,\n+                                blob_seq_numbers_to_delete: merged_blob_seq_numbers_to_delete,\n+                                keys_written: merged_keys_written,\n+                            } => {\n+                                for (seq, file, meta) in merged_new_sst_files {\n+                                    meta_file_builder.add(seq, meta);\n+                                    new_sst_files.push((seq, file));\n+                                }\n+                                blob_seq_numbers_to_delete\n+                                    .extend(merged_blob_seq_numbers_to_delete);\n+                                keys_written += merged_keys_written;\n+                            }\n+                            PartialMergeResult::Move { seq, meta } => {\n                                 meta_file_builder.add(seq, meta);\n-                                new_sst_files.push((seq, file));\n                             }\n-                            blob_seq_numbers_to_delete.extend(merged_blob_seq_numbers_to_delete);\n-                            keys_written += merged_keys_written;\n-                        }\n-                        PartialMergeResult::Move { seq, meta } => {\n-                            meta_file_builder.add(seq, meta);\n                         }\n                     }\n-                }\n \n-                for &seq in sst_seq_numbers_to_delete.iter() {\n-                    meta_file_builder.add_obsolete_sst_file(seq);\n-                }\n+                    for &seq in sst_seq_numbers_to_delete.iter() {\n+                        meta_file_builder.add_obsolete_sst_file(seq);\n+                    }\n \n-                let seq = sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n-                let meta_file = {\n-                    let _span = tracing::trace_span!(\"write meta file\").entered();\n-                    meta_file_builder.write(&self.path, seq)?\n-                };\n-\n-                Ok(PartialResultPerFamily {\n-                    new_meta_file: Some((seq, meta_file)),\n-                    new_sst_files,\n-                    sst_seq_numbers_to_delete,\n-                    blob_seq_numbers_to_delete,\n-                    keys_written,\n-                })\n-            })\n-            .collect::<Result<Vec<_>>>()?;\n+                    let seq = sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n+                    let meta_file = {\n+                        let _span = tracing::trace_span!(\"write meta file\").entered();\n+                        meta_file_builder.write(&self.path, seq)?\n+                    };\n+\n+                    Ok(PartialResultPerFamily {\n+                        new_meta_file: Some((seq, meta_file)),\n+                        new_sst_files,\n+                        sst_seq_numbers_to_delete,\n+                        blob_seq_numbers_to_delete,\n+                        keys_written,\n+                    })\n+                },\n+            )?;\n \n         for PartialResultPerFamily {\n             new_meta_file: inner_new_meta_file,"
        },
        {
            "sha": "f944e4b4d120246981aa17c3e405df9035204e19",
            "filename": "turbopack/crates/turbo-persistence/src/lib.rs",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Flib.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Flib.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Flib.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -13,19 +13,21 @@ mod db;\n mod key;\n mod lookup_entry;\n mod merge_iter;\n+mod meta_file;\n+mod meta_file_builder;\n+mod parallel_scheduler;\n+mod sst_filter;\n mod static_sorted_file;\n mod static_sorted_file_builder;\n+mod value_buf;\n mod write_batch;\n \n-mod meta_file;\n-mod meta_file_builder;\n-mod sst_filter;\n #[cfg(test)]\n mod tests;\n-mod value_buf;\n \n pub use arc_slice::ArcSlice;\n pub use db::{CompactConfig, MetaFileEntryInfo, MetaFileInfo, TurboPersistence};\n pub use key::{KeyBase, QueryKey, StoreKey};\n+pub use parallel_scheduler::{ParallelScheduler, SerialScheduler};\n pub use value_buf::ValueBuffer;\n pub use write_batch::WriteBatch;"
        },
        {
            "sha": "52a9d626090fc672b53317e8dc0cf42074419dfb",
            "filename": "turbopack/crates/turbo-persistence/src/parallel_scheduler.rs",
            "status": "added",
            "additions": 137,
            "deletions": 0,
            "changes": 137,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fparallel_scheduler.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fparallel_scheduler.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fparallel_scheduler.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -0,0 +1,137 @@\n+pub trait ParallelScheduler: Clone + Sync + Send {\n+    fn parallel_for_each<T>(&self, items: &[T], f: impl Fn(&T) + Send + Sync)\n+    where\n+        T: Sync;\n+\n+    fn try_parallel_for_each<'l, T, E>(\n+        &self,\n+        items: &'l [T],\n+        f: impl (Fn(&'l T) -> Result<(), E>) + Send + Sync,\n+    ) -> Result<(), E>\n+    where\n+        T: Sync,\n+        E: Send + 'static;\n+\n+    fn try_parallel_for_each_mut<'l, T, E>(\n+        &self,\n+        items: &'l mut [T],\n+        f: impl (Fn(&'l mut T) -> Result<(), E>) + Send + Sync,\n+    ) -> Result<(), E>\n+    where\n+        T: Send + Sync,\n+        E: Send + 'static;\n+\n+    fn try_parallel_for_each_owned<T, E>(\n+        &self,\n+        items: Vec<T>,\n+        f: impl (Fn(T) -> Result<(), E>) + Send + Sync,\n+    ) -> Result<(), E>\n+    where\n+        T: Send + Sync,\n+        E: Send + 'static;\n+\n+    fn parallel_map_collect<'l, Item, PerItemResult, Result>(\n+        &self,\n+        items: &'l [Item],\n+        f: impl Fn(&'l Item) -> PerItemResult + Send + Sync,\n+    ) -> Result\n+    where\n+        Item: Sync,\n+        PerItemResult: Send + Sync + 'l,\n+        Result: FromIterator<PerItemResult>;\n+\n+    fn parallel_map_collect_owned<Item, PerItemResult, Result>(\n+        &self,\n+        items: Vec<Item>,\n+        f: impl Fn(Item) -> PerItemResult + Send + Sync,\n+    ) -> Result\n+    where\n+        Item: Send + Sync,\n+        PerItemResult: Send + Sync,\n+        Result: FromIterator<PerItemResult>;\n+}\n+\n+#[derive(Clone, Copy, Default)]\n+pub struct SerialScheduler;\n+\n+impl ParallelScheduler for SerialScheduler {\n+    fn parallel_for_each<T>(&self, items: &[T], f: impl Fn(&T) + Send + Sync)\n+    where\n+        T: Sync,\n+    {\n+        for item in items {\n+            f(item);\n+        }\n+    }\n+\n+    fn try_parallel_for_each<'l, T, E>(\n+        &self,\n+        items: &'l [T],\n+        f: impl (Fn(&'l T) -> Result<(), E>) + Send + Sync,\n+    ) -> Result<(), E>\n+    where\n+        T: Sync,\n+        E: Send,\n+    {\n+        for item in items {\n+            f(item)?;\n+        }\n+        Ok(())\n+    }\n+\n+    fn try_parallel_for_each_mut<'l, T, E>(\n+        &self,\n+        items: &'l mut [T],\n+        f: impl (Fn(&'l mut T) -> Result<(), E>) + Send + Sync,\n+    ) -> Result<(), E>\n+    where\n+        T: Sync,\n+        E: Send,\n+    {\n+        for item in items {\n+            f(item)?;\n+        }\n+        Ok(())\n+    }\n+\n+    fn try_parallel_for_each_owned<T, E>(\n+        &self,\n+        items: Vec<T>,\n+        f: impl (Fn(T) -> Result<(), E>) + Send + Sync,\n+    ) -> Result<(), E>\n+    where\n+        T: Sync,\n+        E: Send,\n+    {\n+        for item in items {\n+            f(item)?;\n+        }\n+        Ok(())\n+    }\n+\n+    fn parallel_map_collect<'l, Item, PerItemResult, Result>(\n+        &self,\n+        items: &'l [Item],\n+        f: impl Fn(&'l Item) -> PerItemResult + Send + Sync,\n+    ) -> Result\n+    where\n+        Item: Sync,\n+        PerItemResult: Send + Sync + 'l,\n+        Result: FromIterator<PerItemResult>,\n+    {\n+        items.iter().map(f).collect()\n+    }\n+\n+    fn parallel_map_collect_owned<Item, PerItemResult, Result>(\n+        &self,\n+        items: Vec<Item>,\n+        f: impl Fn(Item) -> PerItemResult + Send + Sync,\n+    ) -> Result\n+    where\n+        Item: Send + Sync,\n+        PerItemResult: Send + Sync,\n+        Result: FromIterator<PerItemResult>,\n+    {\n+        items.into_iter().map(f).collect()\n+    }\n+}"
        },
        {
            "sha": "6e0b42b92fe78ba77cb69d2e131500cf3dc632c5",
            "filename": "turbopack/crates/turbo-persistence/src/tests.rs",
            "status": "modified",
            "additions": 188,
            "deletions": 31,
            "changes": 219,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Ftests.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Ftests.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Ftests.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -6,28 +6,116 @@ use rayon::iter::{IntoParallelIterator, ParallelIterator};\n use crate::{\n     constants::MAX_MEDIUM_VALUE_SIZE,\n     db::{CompactConfig, TurboPersistence},\n+    parallel_scheduler::ParallelScheduler,\n     write_batch::WriteBatch,\n };\n \n+#[derive(Clone, Copy)]\n+struct RayonParallelScheduler;\n+\n+impl ParallelScheduler for RayonParallelScheduler {\n+    fn parallel_for_each<T>(&self, items: &[T], f: impl Fn(&T) + Send + Sync)\n+    where\n+        T: Sync,\n+    {\n+        items.into_par_iter().for_each(f);\n+    }\n+\n+    fn try_parallel_for_each<'l, T, E>(\n+        &self,\n+        items: &'l [T],\n+        f: impl (Fn(&'l T) -> Result<(), E>) + Send + Sync,\n+    ) -> Result<(), E>\n+    where\n+        T: Sync,\n+        E: Send,\n+    {\n+        items.into_par_iter().try_for_each(f)\n+    }\n+\n+    fn try_parallel_for_each_mut<'l, T, E>(\n+        &self,\n+        items: &'l mut [T],\n+        f: impl (Fn(&'l mut T) -> Result<(), E>) + Send + Sync,\n+    ) -> Result<(), E>\n+    where\n+        T: Send + Sync,\n+        E: Send,\n+    {\n+        items.into_par_iter().try_for_each(f)\n+    }\n+\n+    fn try_parallel_for_each_owned<T, E>(\n+        &self,\n+        items: Vec<T>,\n+        f: impl (Fn(T) -> Result<(), E>) + Send + Sync,\n+    ) -> Result<(), E>\n+    where\n+        T: Send + Sync,\n+        E: Send,\n+    {\n+        items.into_par_iter().try_for_each(f)\n+    }\n+\n+    fn parallel_map_collect<'l, Item, PerItemResult, Result>(\n+        &self,\n+        items: &'l [Item],\n+        f: impl Fn(&'l Item) -> PerItemResult + Send + Sync,\n+    ) -> Result\n+    where\n+        Item: Sync,\n+        PerItemResult: Send + Sync,\n+        Result: FromIterator<PerItemResult>,\n+    {\n+        items\n+            .into_par_iter()\n+            .map(f)\n+            .collect_vec_list()\n+            .into_iter()\n+            .flatten()\n+            .collect()\n+    }\n+\n+    fn parallel_map_collect_owned<Item, PerItemResult, Result>(\n+        &self,\n+        items: Vec<Item>,\n+        f: impl Fn(Item) -> PerItemResult + Send + Sync,\n+    ) -> Result\n+    where\n+        Item: Send + Sync,\n+        PerItemResult: Send + Sync,\n+        Result: FromIterator<PerItemResult>,\n+    {\n+        items\n+            .into_par_iter()\n+            .map(f)\n+            .collect_vec_list()\n+            .into_iter()\n+            .flatten()\n+            .collect()\n+    }\n+}\n+\n #[test]\n fn full_cycle() -> Result<()> {\n     let mut test_cases = Vec::new();\n     type TestCases = Vec<(\n         &'static str,\n-        Box<dyn Fn(&mut WriteBatch<Vec<u8>, 16>) -> Result<()>>,\n-        Box<dyn Fn(&TurboPersistence) -> Result<()>>,\n+        Box<dyn Fn(&mut WriteBatch<Vec<u8>, RayonParallelScheduler, 16>) -> Result<()>>,\n+        Box<dyn Fn(&TurboPersistence<RayonParallelScheduler>) -> Result<()>>,\n     )>;\n \n     fn test_case(\n         test_cases: &mut TestCases,\n         name: &'static str,\n-        write: impl Fn(&mut WriteBatch<Vec<u8>, 16>) -> Result<()> + 'static,\n-        read: impl Fn(&TurboPersistence) -> Result<()> + 'static,\n+        write: impl Fn(&mut WriteBatch<Vec<u8>, RayonParallelScheduler, 16>) -> Result<()> + 'static,\n+        read: impl Fn(&TurboPersistence<RayonParallelScheduler>) -> Result<()> + 'static,\n     ) {\n         test_cases.push((\n             name,\n-            Box::new(write) as Box<dyn Fn(&mut WriteBatch<Vec<u8>, 16>) -> Result<()>>,\n-            Box::new(read) as Box<dyn Fn(&TurboPersistence) -> Result<()>>,\n+            Box::new(write)\n+                as Box<dyn Fn(&mut WriteBatch<Vec<u8>, RayonParallelScheduler, 16>) -> Result<()>>,\n+            Box::new(read) as Box<dyn Fn(&TurboPersistence<RayonParallelScheduler>) -> Result<()>>,\n         ));\n     }\n \n@@ -215,7 +303,10 @@ fn full_cycle() -> Result<()> {\n \n         {\n             let start = Instant::now();\n-            let db = TurboPersistence::open(path.to_path_buf())?;\n+            let db = TurboPersistence::open_with_parallel_scheduler(\n+                path.to_path_buf(),\n+                RayonParallelScheduler,\n+            )?;\n             let mut batch = db.write_batch()?;\n             write(&mut batch)?;\n             db.commit_write_batch(batch)?;\n@@ -231,7 +322,10 @@ fn full_cycle() -> Result<()> {\n         }\n         {\n             let start = Instant::now();\n-            let db = TurboPersistence::open(path.to_path_buf())?;\n+            let db = TurboPersistence::open_with_parallel_scheduler(\n+                path.to_path_buf(),\n+                RayonParallelScheduler,\n+            )?;\n             println!(\"{name} restore time: {:?}\", start.elapsed());\n             let start = Instant::now();\n             read(&db)?;\n@@ -257,7 +351,10 @@ fn full_cycle() -> Result<()> {\n         }\n         {\n             let start = Instant::now();\n-            let db = TurboPersistence::open(path.to_path_buf())?;\n+            let db = TurboPersistence::open_with_parallel_scheduler(\n+                path.to_path_buf(),\n+                RayonParallelScheduler,\n+            )?;\n             println!(\"{name} restore time after compact: {:?}\", start.elapsed());\n             let start = Instant::now();\n             read(&db)?;\n@@ -291,7 +388,10 @@ fn full_cycle() -> Result<()> {\n \n         {\n             let start = Instant::now();\n-            let db = TurboPersistence::open(path.to_path_buf())?;\n+            let db = TurboPersistence::open_with_parallel_scheduler(\n+                path.to_path_buf(),\n+                RayonParallelScheduler,\n+            )?;\n             let mut batch = db.write_batch()?;\n             for (_, write, _) in test_cases.iter() {\n                 write(&mut batch)?;\n@@ -311,7 +411,10 @@ fn full_cycle() -> Result<()> {\n         }\n         {\n             let start = Instant::now();\n-            let db = TurboPersistence::open(path.to_path_buf())?;\n+            let db = TurboPersistence::open_with_parallel_scheduler(\n+                path.to_path_buf(),\n+                RayonParallelScheduler,\n+            )?;\n             println!(\"All restore time: {:?}\", start.elapsed());\n             for (name, _, read) in test_cases.iter() {\n                 let start = Instant::now();\n@@ -343,7 +446,10 @@ fn full_cycle() -> Result<()> {\n \n         {\n             let start = Instant::now();\n-            let db = TurboPersistence::open(path.to_path_buf())?;\n+            let db = TurboPersistence::open_with_parallel_scheduler(\n+                path.to_path_buf(),\n+                RayonParallelScheduler,\n+            )?;\n             println!(\"All restore time after compact: {:?}\", start.elapsed());\n \n             for (name, _, read) in test_cases.iter() {\n@@ -383,13 +489,17 @@ fn persist_changes() -> Result<()> {\n     let path = tempdir.path();\n \n     const READ_COUNT: u32 = 2_000; // we'll read every 10th value, so writes are 10x this value\n-    fn put(b: &WriteBatch<(u8, [u8; 4]), 1>, key: u8, value: u8) -> Result<()> {\n+    fn put(\n+        b: &WriteBatch<(u8, [u8; 4]), RayonParallelScheduler, 1>,\n+        key: u8,\n+        value: u8,\n+    ) -> Result<()> {\n         for i in 0..(READ_COUNT * 10) {\n             b.put(0, (key, i.to_be_bytes()), vec![value].into())?;\n         }\n         Ok(())\n     }\n-    fn check(db: &TurboPersistence, key: u8, value: u8) -> Result<()> {\n+    fn check(db: &TurboPersistence<RayonParallelScheduler>, key: u8, value: u8) -> Result<()> {\n         for i in 0..READ_COUNT {\n             // read every 10th item\n             let i = i * 10;\n@@ -402,7 +512,10 @@ fn persist_changes() -> Result<()> {\n     }\n \n     {\n-        let db = TurboPersistence::open(path.to_path_buf())?;\n+        let db = TurboPersistence::open_with_parallel_scheduler(\n+            path.to_path_buf(),\n+            RayonParallelScheduler,\n+        )?;\n         let b = db.write_batch::<_, 1>()?;\n         put(&b, 1, 11)?;\n         put(&b, 2, 21)?;\n@@ -418,7 +531,10 @@ fn persist_changes() -> Result<()> {\n \n     println!(\"---\");\n     {\n-        let db = TurboPersistence::open(path.to_path_buf())?;\n+        let db = TurboPersistence::open_with_parallel_scheduler(\n+            path.to_path_buf(),\n+            RayonParallelScheduler,\n+        )?;\n         let b = db.write_batch::<_, 1>()?;\n         put(&b, 1, 12)?;\n         put(&b, 2, 22)?;\n@@ -432,7 +548,10 @@ fn persist_changes() -> Result<()> {\n     }\n \n     {\n-        let db = TurboPersistence::open(path.to_path_buf())?;\n+        let db = TurboPersistence::open_with_parallel_scheduler(\n+            path.to_path_buf(),\n+            RayonParallelScheduler,\n+        )?;\n         let b = db.write_batch::<_, 1>()?;\n         put(&b, 1, 13)?;\n         db.commit_write_batch(b)?;\n@@ -446,7 +565,10 @@ fn persist_changes() -> Result<()> {\n \n     println!(\"---\");\n     {\n-        let db = TurboPersistence::open(path.to_path_buf())?;\n+        let db = TurboPersistence::open_with_parallel_scheduler(\n+            path.to_path_buf(),\n+            RayonParallelScheduler,\n+        )?;\n \n         check(&db, 1, 13)?;\n         check(&db, 2, 22)?;\n@@ -457,7 +579,10 @@ fn persist_changes() -> Result<()> {\n \n     println!(\"---\");\n     {\n-        let db = TurboPersistence::open(path.to_path_buf())?;\n+        let db = TurboPersistence::open_with_parallel_scheduler(\n+            path.to_path_buf(),\n+            RayonParallelScheduler,\n+        )?;\n \n         db.compact(&CompactConfig {\n             optimal_merge_count: 4,\n@@ -475,7 +600,10 @@ fn persist_changes() -> Result<()> {\n \n     println!(\"---\");\n     {\n-        let db = TurboPersistence::open(path.to_path_buf())?;\n+        let db = TurboPersistence::open_with_parallel_scheduler(\n+            path.to_path_buf(),\n+            RayonParallelScheduler,\n+        )?;\n \n         check(&db, 1, 13)?;\n         check(&db, 2, 22)?;\n@@ -493,13 +621,17 @@ fn partial_compaction() -> Result<()> {\n     let path = tempdir.path();\n \n     const READ_COUNT: u32 = 2_000; // we'll read every 10th value, so writes are 10x this value\n-    fn put(b: &WriteBatch<(u8, [u8; 4]), 1>, key: u8, value: u8) -> Result<()> {\n+    fn put(\n+        b: &WriteBatch<(u8, [u8; 4]), RayonParallelScheduler, 1>,\n+        key: u8,\n+        value: u8,\n+    ) -> Result<()> {\n         for i in 0..(READ_COUNT * 10) {\n             b.put(0, (key, i.to_be_bytes()), vec![value].into())?;\n         }\n         Ok(())\n     }\n-    fn check(db: &TurboPersistence, key: u8, value: u8) -> Result<()> {\n+    fn check(db: &TurboPersistence<RayonParallelScheduler>, key: u8, value: u8) -> Result<()> {\n         for i in 0..READ_COUNT {\n             // read every 10th item\n             let i = i * 10;\n@@ -516,7 +648,10 @@ fn partial_compaction() -> Result<()> {\n         println!(\"--- Iteration {i} ---\");\n         println!(\"Add more entries\");\n         {\n-            let db = TurboPersistence::open(path.to_path_buf())?;\n+            let db = TurboPersistence::open_with_parallel_scheduler(\n+                path.to_path_buf(),\n+                RayonParallelScheduler,\n+            )?;\n             let b = db.write_batch::<_, 1>()?;\n             put(&b, i, i)?;\n             put(&b, i + 1, i)?;\n@@ -535,7 +670,10 @@ fn partial_compaction() -> Result<()> {\n \n         println!(\"Compaction\");\n         {\n-            let db = TurboPersistence::open(path.to_path_buf())?;\n+            let db = TurboPersistence::open_with_parallel_scheduler(\n+                path.to_path_buf(),\n+                RayonParallelScheduler,\n+            )?;\n \n             db.compact(&CompactConfig {\n                 optimal_merge_count: 4,\n@@ -556,7 +694,10 @@ fn partial_compaction() -> Result<()> {\n \n         println!(\"Restore check\");\n         {\n-            let db = TurboPersistence::open(path.to_path_buf())?;\n+            let db = TurboPersistence::open_with_parallel_scheduler(\n+                path.to_path_buf(),\n+                RayonParallelScheduler,\n+            )?;\n \n             for j in 0..i {\n                 check(&db, j, j)?;\n@@ -580,7 +721,11 @@ fn merge_file_removal() -> Result<()> {\n     let _ = fs::remove_dir_all(path);\n \n     const READ_COUNT: u32 = 2_000; // we'll read every 10th value, so writes are 10x this value\n-    fn put(b: &WriteBatch<(u8, [u8; 4]), 1>, key: u8, value: u32) -> Result<()> {\n+    fn put(\n+        b: &WriteBatch<(u8, [u8; 4]), RayonParallelScheduler, 1>,\n+        key: u8,\n+        value: u32,\n+    ) -> Result<()> {\n         for i in 0..(READ_COUNT * 10) {\n             b.put(\n                 0,\n@@ -590,7 +735,7 @@ fn merge_file_removal() -> Result<()> {\n         }\n         Ok(())\n     }\n-    fn check(db: &TurboPersistence, key: u8, value: u32) -> Result<()> {\n+    fn check(db: &TurboPersistence<RayonParallelScheduler>, key: u8, value: u32) -> Result<()> {\n         for i in 0..READ_COUNT {\n             // read every 10th item\n             let i = i * 10;\n@@ -608,7 +753,10 @@ fn merge_file_removal() -> Result<()> {\n \n     {\n         println!(\"--- Init ---\");\n-        let db = TurboPersistence::open(path.to_path_buf())?;\n+        let db = TurboPersistence::open_with_parallel_scheduler(\n+            path.to_path_buf(),\n+            RayonParallelScheduler,\n+        )?;\n         let b = db.write_batch::<_, 1>()?;\n         for j in 0..=255 {\n             put(&b, j, 0)?;\n@@ -624,7 +772,10 @@ fn merge_file_removal() -> Result<()> {\n         let i = i * 37;\n         println!(\"Add more entries\");\n         {\n-            let db = TurboPersistence::open(path.to_path_buf())?;\n+            let db = TurboPersistence::open_with_parallel_scheduler(\n+                path.to_path_buf(),\n+                RayonParallelScheduler,\n+            )?;\n             let b = db.write_batch::<_, 1>()?;\n             for j in iter_bits(i) {\n                 println!(\"Put {j} = {i}\");\n@@ -642,7 +793,10 @@ fn merge_file_removal() -> Result<()> {\n \n         println!(\"Compaction\");\n         {\n-            let db = TurboPersistence::open(path.to_path_buf())?;\n+            let db = TurboPersistence::open_with_parallel_scheduler(\n+                path.to_path_buf(),\n+                RayonParallelScheduler,\n+            )?;\n \n             db.compact(&CompactConfig {\n                 optimal_merge_count: 4,\n@@ -660,7 +814,10 @@ fn merge_file_removal() -> Result<()> {\n \n         println!(\"Restore check\");\n         {\n-            let db = TurboPersistence::open(path.to_path_buf())?;\n+            let db = TurboPersistence::open_with_parallel_scheduler(\n+                path.to_path_buf(),\n+                RayonParallelScheduler,\n+            )?;\n \n             for j in 0..32 {\n                 check(&db, j, expected_values[j as usize])?;"
        },
        {
            "sha": "81a954d0ef18eff11ee2ec0b4ea0390b084a84d6",
            "filename": "turbopack/crates/turbo-persistence/src/write_batch.rs",
            "status": "modified",
            "additions": 79,
            "deletions": 75,
            "changes": 154,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fwrite_batch.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fwrite_batch.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fwrite_batch.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -9,15 +9,11 @@ use std::{\n \n use anyhow::{Context, Result};\n use byteorder::{BE, WriteBytesExt};\n+use either::Either;\n use lzzzz::lz4::{self, ACC_LEVEL_DEFAULT};\n use parking_lot::Mutex;\n-use rayon::{\n-    iter::{Either, IndexedParallelIterator, IntoParallelIterator, ParallelIterator},\n-    scope,\n-};\n use smallvec::SmallVec;\n use thread_local::ThreadLocal;\n-use tracing::Span;\n \n use crate::{\n     ValueBuffer,\n@@ -26,6 +22,7 @@ use crate::{\n     constants::{MAX_MEDIUM_VALUE_SIZE, THREAD_LOCAL_SIZE_SHIFT},\n     key::StoreKey,\n     meta_file_builder::MetaFileBuilder,\n+    parallel_scheduler::ParallelScheduler,\n     static_sorted_file_builder::{StaticSortedFileBuilderMeta, write_static_stored_file},\n };\n \n@@ -68,7 +65,9 @@ enum GlobalCollectorState<K: StoreKey + Send> {\n }\n \n /// A write batch.\n-pub struct WriteBatch<K: StoreKey + Send, const FAMILIES: usize> {\n+pub struct WriteBatch<K: StoreKey + Send, S: ParallelScheduler, const FAMILIES: usize> {\n+    /// Parallel scheduler\n+    parallel_scheduler: S,\n     /// The database path\n     db_path: PathBuf,\n     /// The current sequence number counter. Increased for every new SST file or blob file.\n@@ -84,13 +83,16 @@ pub struct WriteBatch<K: StoreKey + Send, const FAMILIES: usize> {\n     new_sst_files: Mutex<Vec<(u32, File)>>,\n }\n \n-impl<K: StoreKey + Send + Sync, const FAMILIES: usize> WriteBatch<K, FAMILIES> {\n+impl<K: StoreKey + Send + Sync, S: ParallelScheduler, const FAMILIES: usize>\n+    WriteBatch<K, S, FAMILIES>\n+{\n     /// Creates a new write batch for a database.\n-    pub(crate) fn new(path: PathBuf, current: u32) -> Self {\n+    pub(crate) fn new(path: PathBuf, current: u32, parallel_scheduler: S) -> Self {\n         const {\n             assert!(FAMILIES <= usize_from_u32(u32::MAX));\n         };\n         Self {\n+            parallel_scheduler,\n             db_path: path,\n             current_sequence_number: AtomicU32::new(current),\n             thread_locals: ThreadLocal::new(),\n@@ -223,13 +225,12 @@ impl<K: StoreKey + Send + Sync, const FAMILIES: usize> WriteBatch<K, FAMILIES> {\n             }\n         }\n \n-        let span = Span::current();\n-        collectors.into_par_iter().try_for_each(|mut collector| {\n-            let _span = span.clone().entered();\n-            self.flush_thread_local_collector(family, &mut collector)?;\n-            drop(collector);\n-            anyhow::Ok(())\n-        })?;\n+        self.parallel_scheduler\n+            .try_parallel_for_each_owned(collectors, |mut collector| {\n+                self.flush_thread_local_collector(family, &mut collector)?;\n+                drop(collector);\n+                anyhow::Ok(())\n+            })?;\n \n         // Now we flush the global collector(s).\n         let mut collector_state = self.collectors[usize_from_u32(family)].lock();\n@@ -242,22 +243,22 @@ impl<K: StoreKey + Send + Sync, const FAMILIES: usize> WriteBatch<K, FAMILIES> {\n                 }\n             }\n             GlobalCollectorState::Sharded(_) => {\n-                let GlobalCollectorState::Sharded(shards) = replace(\n+                let GlobalCollectorState::Sharded(mut shards) = replace(\n                     &mut *collector_state,\n                     GlobalCollectorState::Unsharded(Collector::new()),\n                 ) else {\n                     unreachable!();\n                 };\n-                shards.into_par_iter().try_for_each(|mut collector| {\n-                    let _span = span.clone().entered();\n-                    if !collector.is_empty() {\n-                        let sst = self.create_sst_file(family, collector.sorted())?;\n-                        collector.clear();\n-                        self.new_sst_files.lock().push(sst);\n-                        drop(collector);\n-                    }\n-                    anyhow::Ok(())\n-                })?;\n+                self.parallel_scheduler\n+                    .try_parallel_for_each_mut(&mut shards, |collector| {\n+                        if !collector.is_empty() {\n+                            let sst = self.create_sst_file(family, collector.sorted())?;\n+                            collector.clear();\n+                            self.new_sst_files.lock().push(sst);\n+                            collector.drop_contents();\n+                        }\n+                        anyhow::Ok(())\n+                    })?;\n             }\n         }\n \n@@ -269,10 +270,9 @@ impl<K: StoreKey + Send + Sync, const FAMILIES: usize> WriteBatch<K, FAMILIES> {\n     #[tracing::instrument(level = \"trace\", skip(self))]\n     pub(crate) fn finish(&mut self) -> Result<FinishResult> {\n         let mut new_blob_files = Vec::new();\n-        let shared_error = Mutex::new(Ok(()));\n \n         // First, we flush all thread local collectors to the global collectors.\n-        scope(|scope| {\n+        {\n             let _span = tracing::trace_span!(\"flush thread local collectors\").entered();\n             let mut collectors = [const { Vec::new() }; FAMILIES];\n             for cell in self.thread_locals.iter_mut() {\n@@ -286,23 +286,24 @@ impl<K: StoreKey + Send + Sync, const FAMILIES: usize> WriteBatch<K, FAMILIES> {\n                     }\n                 }\n             }\n-            for (family, thread_local_collectors) in collectors.into_iter().enumerate() {\n-                for mut collector in thread_local_collectors {\n-                    let this = &self;\n-                    let shared_error = &shared_error;\n-                    let span = Span::current();\n-                    scope.spawn(move |_| {\n-                        let _span = span.entered();\n-                        if let Err(err) =\n-                            this.flush_thread_local_collector(family as u32, &mut collector)\n-                        {\n-                            *shared_error.lock() = Err(err);\n-                        }\n-                        drop(collector);\n-                    });\n-                }\n-            }\n-        });\n+            let to_flush = collectors\n+                .into_iter()\n+                .enumerate()\n+                .flat_map(|(family, collector)| {\n+                    collector\n+                        .into_iter()\n+                        .map(move |collector| (family as u32, collector))\n+                })\n+                .collect::<Vec<_>>();\n+            self.parallel_scheduler.try_parallel_for_each_owned(\n+                to_flush,\n+                |(family, mut collector)| {\n+                    self.flush_thread_local_collector(family, &mut collector)?;\n+                    drop(collector);\n+                    anyhow::Ok(())\n+                },\n+            )?;\n+        }\n \n         let _span = tracing::trace_span!(\"flush collectors\").entered();\n \n@@ -313,25 +314,24 @@ impl<K: StoreKey + Send + Sync, const FAMILIES: usize> WriteBatch<K, FAMILIES> {\n         let new_collectors =\n             [(); FAMILIES].map(|_| Mutex::new(GlobalCollectorState::Unsharded(Collector::new())));\n         let collectors = replace(&mut self.collectors, new_collectors);\n-        let span = Span::current();\n-        collectors\n-            .into_par_iter()\n+        let collectors = collectors\n+            .into_iter()\n             .enumerate()\n             .flat_map(|(family, state)| {\n                 let collector = state.into_inner();\n                 match collector {\n                     GlobalCollectorState::Unsharded(collector) => {\n-                        Either::Left([(family, collector)].into_par_iter())\n+                        Either::Left([(family, collector)].into_iter())\n+                    }\n+                    GlobalCollectorState::Sharded(shards) => {\n+                        Either::Right(shards.into_iter().map(move |collector| (family, collector)))\n                     }\n-                    GlobalCollectorState::Sharded(shards) => Either::Right(\n-                        shards\n-                            .into_par_iter()\n-                            .map(move |collector| (family, collector)),\n-                    ),\n                 }\n             })\n-            .try_for_each(|(family, mut collector)| {\n-                let _span = span.clone().entered();\n+            .collect::<Vec<_>>();\n+        self.parallel_scheduler.try_parallel_for_each_owned(\n+            collectors,\n+            |(family, mut collector)| {\n                 let family = family as u32;\n                 if !collector.is_empty() {\n                     let sst = self.create_sst_file(family, collector.sorted())?;\n@@ -340,33 +340,37 @@ impl<K: StoreKey + Send + Sync, const FAMILIES: usize> WriteBatch<K, FAMILIES> {\n                     shared_new_sst_files.lock().push(sst);\n                 }\n                 anyhow::Ok(())\n-            })?;\n-\n-        shared_error.into_inner()?;\n+            },\n+        )?;\n \n         // Not we need to write the new meta files.\n         let new_meta_collectors = [(); FAMILIES].map(|_| Mutex::new(Vec::new()));\n         let meta_collectors = replace(&mut self.meta_collectors, new_meta_collectors);\n         let keys_written = AtomicU64::new(0);\n-        let new_meta_files = meta_collectors\n-            .into_par_iter()\n+        let file_to_write = meta_collectors\n+            .into_iter()\n             .map(|mutex| mutex.into_inner())\n             .enumerate()\n             .filter(|(_, sst_files)| !sst_files.is_empty())\n-            .map(|(family, sst_files)| {\n-                let family = family as u32;\n-                let mut entries = 0;\n-                let mut builder = MetaFileBuilder::new(family);\n-                for (seq, sst) in sst_files {\n-                    entries += sst.entries;\n-                    builder.add(seq, sst);\n-                }\n-                keys_written.fetch_add(entries, Ordering::Relaxed);\n-                let seq = self.current_sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n-                let file = builder.write(&self.db_path, seq)?;\n-                Ok((seq, file))\n-            })\n-            .collect::<Result<Vec<_>>>()?;\n+            .collect::<Vec<_>>();\n+        let new_meta_files = self\n+            .parallel_scheduler\n+            .parallel_map_collect_owned::<_, _, Result<Vec<_>>>(\n+                file_to_write,\n+                |(family, sst_files)| {\n+                    let family = family as u32;\n+                    let mut entries = 0;\n+                    let mut builder = MetaFileBuilder::new(family);\n+                    for (seq, sst) in sst_files {\n+                        entries += sst.entries;\n+                        builder.add(seq, sst);\n+                    }\n+                    keys_written.fetch_add(entries, Ordering::Relaxed);\n+                    let seq = self.current_sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n+                    let file = builder.write(&self.db_path, seq)?;\n+                    Ok((seq, file))\n+                },\n+            )?;\n \n         // Finally we return the new files and sequence number.\n         let seq = self.current_sequence_number.load(Ordering::SeqCst);"
        },
        {
            "sha": "3554ce1f31b20b6ec207e43b07a76982c023b7b2",
            "filename": "turbopack/crates/turbo-tasks-backend/Cargo.toml",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2FCargo.toml",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2FCargo.toml",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2FCargo.toml?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -40,7 +40,6 @@ once_cell = { workspace = true }\n parking_lot = { workspace = true }\n pot = \"3.0.0\"\n rand = { workspace = true }\n-rayon = { workspace = true }\n ringmap = { workspace = true, features = [\"serde\"] }\n rustc-hash = { workspace = true }\n serde = { workspace = true }"
        },
        {
            "sha": "fbba6b1ab13d4ef8c16eb17656d4c3143d48fbc7",
            "filename": "turbopack/crates/turbo-tasks-backend/src/backend/mod.rs",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Fmod.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Fmod.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Fmod.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -1267,7 +1267,6 @@ impl<B: BackingStorage> TurboTasksBackendInner<B> {\n             return task_id;\n         }\n \n-        self.track_cache_miss(&task_type);\n         let tx = self\n             .should_restore()\n             .then(|| self.backing_storage.start_read_transaction())\n@@ -1279,6 +1278,7 @@ impl<B: BackingStorage> TurboTasksBackendInner<B> {\n                     .forward_lookup_task_cache(tx.as_ref(), &task_type)\n                     .expect(\"Failed to lookup task id\")\n             } {\n+                self.track_cache_hit(&task_type);\n                 let _ = self.task_cache.try_insert(Arc::new(task_type), task_id);\n                 task_id\n             } else {\n@@ -1287,12 +1287,14 @@ impl<B: BackingStorage> TurboTasksBackendInner<B> {\n                 let task_id = if let Err(existing_task_id) =\n                     self.task_cache.try_insert(task_type.clone(), task_id)\n                 {\n+                    self.track_cache_hit(&task_type);\n                     // Safety: We just created the id and failed to insert it.\n                     unsafe {\n                         self.persisted_task_id_factory.reuse(task_id);\n                     }\n                     existing_task_id\n                 } else {\n+                    self.track_cache_miss(&task_type);\n                     task_id\n                 };\n                 if let Some(log) = &self.persisted_task_cache_log {\n@@ -1327,10 +1329,10 @@ impl<B: BackingStorage> TurboTasksBackendInner<B> {\n             return task_id;\n         }\n \n-        self.track_cache_miss(&task_type);\n         let task_type = Arc::new(task_type);\n         let task_id = self.transient_task_id_factory.get();\n-        if let Err(existing_task_id) = self.task_cache.try_insert(task_type, task_id) {\n+        if let Err(existing_task_id) = self.task_cache.try_insert(task_type.clone(), task_id) {\n+            self.track_cache_hit(&task_type);\n             // Safety: We just created the id and failed to insert it.\n             unsafe {\n                 self.transient_task_id_factory.reuse(task_id);\n@@ -1339,6 +1341,7 @@ impl<B: BackingStorage> TurboTasksBackendInner<B> {\n             return existing_task_id;\n         }\n \n+        self.track_cache_miss(&task_type);\n         self.connect_child(parent_task, task_id, turbo_tasks);\n \n         task_id"
        },
        {
            "sha": "ba83f1da5a209dc0a78cc9dbc985ddc290a6bb1b",
            "filename": "turbopack/crates/turbo-tasks-backend/src/backend/storage.rs",
            "status": "modified",
            "additions": 33,
            "deletions": 39,
            "changes": 72,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Fstorage.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Fstorage.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Fstorage.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -6,9 +6,8 @@ use std::{\n };\n \n use bitfield::bitfield;\n-use rayon::iter::{IndexedParallelIterator, IntoParallelRefIterator, ParallelIterator};\n use smallvec::SmallVec;\n-use turbo_tasks::{FxDashMap, TaskId};\n+use turbo_tasks::{FxDashMap, TaskId, parallel};\n \n use crate::{\n     backend::dynamic_storage::DynamicStorage,\n@@ -664,48 +663,43 @@ impl Storage {\n \n         // The number of shards is much larger than the number of threads, so the effect of the\n         // locks held is negligible.\n-        self.modified\n-            .shards()\n-            .par_iter()\n-            .with_max_len(1)\n-            .map(|shard| {\n-                let mut direct_snapshots: Vec<(TaskId, Box<InnerStorageSnapshot>)> = Vec::new();\n-                let mut modified: SmallVec<[TaskId; 4]> = SmallVec::new();\n-                {\n-                    // Take the snapshots from the modified map\n-                    let guard = shard.write();\n-                    // Safety: guard must outlive the iterator.\n-                    for bucket in unsafe { guard.iter() } {\n-                        // Safety: the guard guarantees that the bucket is not removed and the ptr\n-                        // is valid.\n-                        let (key, shared_value) = unsafe { bucket.as_mut() };\n-                        let modified_state = shared_value.get_mut();\n-                        match modified_state {\n-                            ModifiedState::Modified => {\n-                                modified.push(*key);\n-                            }\n-                            ModifiedState::Snapshot(snapshot) => {\n-                                if let Some(snapshot) = snapshot.take() {\n-                                    direct_snapshots.push((*key, snapshot));\n-                                }\n+        parallel::map_collect::<_, _, Vec<_>>(self.modified.shards(), |shard| {\n+            let mut direct_snapshots: Vec<(TaskId, Box<InnerStorageSnapshot>)> = Vec::new();\n+            let mut modified: SmallVec<[TaskId; 4]> = SmallVec::new();\n+            {\n+                // Take the snapshots from the modified map\n+                let guard = shard.write();\n+                // Safety: guard must outlive the iterator.\n+                for bucket in unsafe { guard.iter() } {\n+                    // Safety: the guard guarantees that the bucket is not removed and the ptr\n+                    // is valid.\n+                    let (key, shared_value) = unsafe { bucket.as_mut() };\n+                    let modified_state = shared_value.get_mut();\n+                    match modified_state {\n+                        ModifiedState::Modified => {\n+                            modified.push(*key);\n+                        }\n+                        ModifiedState::Snapshot(snapshot) => {\n+                            if let Some(snapshot) = snapshot.take() {\n+                                direct_snapshots.push((*key, snapshot));\n                             }\n                         }\n                     }\n-                    // Safety: guard must outlive the iterator.\n-                    drop(guard);\n                 }\n+                // Safety: guard must outlive the iterator.\n+                drop(guard);\n+            }\n \n-                SnapshotShard {\n-                    direct_snapshots,\n-                    modified,\n-                    storage: self,\n-                    guard: Some(guard.clone()),\n-                    process,\n-                    preprocess,\n-                    process_snapshot,\n-                }\n-            })\n-            .collect::<Vec<_>>()\n+            SnapshotShard {\n+                direct_snapshots,\n+                modified,\n+                storage: self,\n+                guard: Some(guard.clone()),\n+                process,\n+                preprocess,\n+                process_snapshot,\n+            }\n+        })\n     }\n \n     /// Start snapshot mode."
        },
        {
            "sha": "493965c4bd1e9a58bd1a97787ad7a51772c6a665",
            "filename": "turbopack/crates/turbo-tasks-backend/src/database/turbo/mod.rs",
            "status": "renamed",
            "additions": 20,
            "deletions": 13,
            "changes": 33,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fdatabase%2Fturbo%2Fmod.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fdatabase%2Fturbo%2Fmod.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fdatabase%2Fturbo%2Fmod.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -1,21 +1,24 @@\n-use std::{\n-    cmp::max,\n-    path::PathBuf,\n-    sync::Arc,\n-    thread::{JoinHandle, available_parallelism, spawn},\n-};\n+use std::{cmp::max, path::PathBuf, sync::Arc, thread::available_parallelism};\n \n use anyhow::Result;\n use parking_lot::Mutex;\n+use tokio::{\n+    runtime::Handle,\n+    spawn,\n+    task::{JoinHandle, block_in_place},\n+};\n use turbo_persistence::{\n     ArcSlice, CompactConfig, KeyBase, StoreKey, TurboPersistence, ValueBuffer,\n };\n \n use crate::database::{\n     key_value_database::{KeySpace, KeyValueDatabase},\n+    turbo::parallel_scheduler::TurboTasksParallelScheduler,\n     write_batch::{BaseWriteBatch, ConcurrentWriteBatch, WriteBatch, WriteBuffer},\n };\n \n+mod parallel_scheduler;\n+\n const MB: u64 = 1024 * 1024;\n const COMPACT_CONFIG: CompactConfig = CompactConfig {\n     min_merge_count: 3,\n@@ -28,7 +31,7 @@ const COMPACT_CONFIG: CompactConfig = CompactConfig {\n };\n \n pub struct TurboKeyValueDatabase {\n-    db: Arc<TurboPersistence>,\n+    db: Arc<TurboPersistence<TurboTasksParallelScheduler>>,\n     compact_join_handle: Mutex<Option<JoinHandle<Result<()>>>>,\n     is_ci: bool,\n     is_short_session: bool,\n@@ -45,7 +48,7 @@ impl TurboKeyValueDatabase {\n         };\n         // start compaction in background if the database is not empty\n         if !db.is_empty() {\n-            let handle = spawn(move || {\n+            let handle = spawn(async move {\n                 db.compact(&CompactConfig {\n                     max_merge_segment_count: available_parallelism()\n                         .map_or(4, |c| max(4, c.get() / 4)),\n@@ -96,7 +99,7 @@ impl KeyValueDatabase for TurboKeyValueDatabase {\n     ) -> Result<WriteBatch<'_, Self::SerialWriteBatch<'_>, Self::ConcurrentWriteBatch<'_>>> {\n         // Wait for the compaction to finish\n         if let Some(join_handle) = self.compact_join_handle.lock().take() {\n-            join_handle.join().unwrap()?;\n+            join(join_handle)?;\n         }\n         // Start a new write batch\n         Ok(WriteBatch::concurrent(TurboWriteBatch {\n@@ -112,7 +115,7 @@ impl KeyValueDatabase for TurboKeyValueDatabase {\n     fn shutdown(&self) -> Result<()> {\n         // Wait for the compaction to finish\n         if let Some(join_handle) = self.compact_join_handle.lock().take() {\n-            join_handle.join().unwrap()?;\n+            join(join_handle)?;\n         }\n         // Compact the database on shutdown\n         self.db.compact(&CompactConfig {\n@@ -130,8 +133,8 @@ impl KeyValueDatabase for TurboKeyValueDatabase {\n }\n \n pub struct TurboWriteBatch<'a> {\n-    batch: turbo_persistence::WriteBatch<WriteBuffer<'static>, 5>,\n-    db: &'a Arc<TurboPersistence>,\n+    batch: turbo_persistence::WriteBatch<WriteBuffer<'static>, TurboTasksParallelScheduler, 5>,\n+    db: &'a Arc<TurboPersistence<TurboTasksParallelScheduler>>,\n     compact_join_handle: Option<&'a Mutex<Option<JoinHandle<Result<()>>>>>,\n }\n \n@@ -156,7 +159,7 @@ impl<'a> BaseWriteBatch<'a> for TurboWriteBatch<'a> {\n         if let Some(compact_join_handle) = self.compact_join_handle {\n             // Start a new compaction in the background\n             let db = self.db.clone();\n-            let handle = spawn(move || {\n+            let handle = spawn(async move {\n                 db.compact(&CompactConfig {\n                     max_merge_segment_count: available_parallelism()\n                         .map_or(4, |c| max(4, c.get() / 2)),\n@@ -232,3 +235,7 @@ impl<'l> From<WriteBuffer<'l>> for ValueBuffer<'l> {\n         }\n     }\n }\n+\n+fn join(handle: JoinHandle<Result<()>>) -> Result<()> {\n+    block_in_place(|| Handle::current().block_on(handle))?\n+}",
            "previous_filename": "turbopack/crates/turbo-tasks-backend/src/database/turbo.rs"
        },
        {
            "sha": "dd78022c3640bec3159bb3138874868533cf843d",
            "filename": "turbopack/crates/turbo-tasks-backend/src/database/turbo/parallel_scheduler.rs",
            "status": "added",
            "additions": 76,
            "deletions": 0,
            "changes": 76,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fdatabase%2Fturbo%2Fparallel_scheduler.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fdatabase%2Fturbo%2Fparallel_scheduler.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fdatabase%2Fturbo%2Fparallel_scheduler.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -0,0 +1,76 @@\n+use turbo_persistence::ParallelScheduler;\n+use turbo_tasks::parallel;\n+\n+#[derive(Clone, Copy, Default)]\n+pub struct TurboTasksParallelScheduler;\n+\n+impl ParallelScheduler for TurboTasksParallelScheduler {\n+    fn parallel_for_each<T>(&self, items: &[T], f: impl Fn(&T) + Send + Sync)\n+    where\n+        T: Sync,\n+    {\n+        parallel::for_each(items, f);\n+    }\n+\n+    fn try_parallel_for_each<'l, T, E>(\n+        &self,\n+        items: &'l [T],\n+        f: impl (Fn(&'l T) -> Result<(), E>) + Send + Sync,\n+    ) -> Result<(), E>\n+    where\n+        T: Sync,\n+        E: Send + 'static,\n+    {\n+        parallel::try_for_each(items, f)\n+    }\n+\n+    fn try_parallel_for_each_mut<'l, T, E>(\n+        &self,\n+        items: &'l mut [T],\n+        f: impl (Fn(&'l mut T) -> Result<(), E>) + Send + Sync,\n+    ) -> Result<(), E>\n+    where\n+        T: Send + Sync,\n+        E: Send + 'static,\n+    {\n+        parallel::try_for_each_mut(items, f)\n+    }\n+\n+    fn try_parallel_for_each_owned<T, E>(\n+        &self,\n+        items: Vec<T>,\n+        f: impl (Fn(T) -> Result<(), E>) + Send + Sync,\n+    ) -> Result<(), E>\n+    where\n+        T: Send + Sync,\n+        E: Send + 'static,\n+    {\n+        parallel::try_for_each_owned(items, f)\n+    }\n+\n+    fn parallel_map_collect<'l, T, I, R>(\n+        &self,\n+        items: &'l [T],\n+        f: impl Fn(&'l T) -> I + Send + Sync,\n+    ) -> R\n+    where\n+        T: Sync,\n+        I: Send + Sync + 'l,\n+        R: FromIterator<I>,\n+    {\n+        parallel::map_collect(items, f)\n+    }\n+\n+    fn parallel_map_collect_owned<T, I, R>(\n+        &self,\n+        items: Vec<T>,\n+        f: impl Fn(T) -> I + Send + Sync,\n+    ) -> R\n+    where\n+        T: Send + Sync,\n+        I: Send + Sync,\n+        R: FromIterator<I>,\n+    {\n+        parallel::map_collect_owned(items, f)\n+    }\n+}"
        },
        {
            "sha": "3127a15ab70667c46cb4d9e4743e0038e176e1ed",
            "filename": "turbopack/crates/turbo-tasks-backend/src/kv_backing_storage.rs",
            "status": "modified",
            "additions": 94,
            "deletions": 117,
            "changes": 211,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fkv_backing_storage.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fkv_backing_storage.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fkv_backing_storage.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -1,21 +1,18 @@\n use std::{\n     borrow::Borrow,\n-    cmp::max,\n     env,\n     path::PathBuf,\n     sync::{Arc, LazyLock, Mutex, PoisonError, Weak},\n };\n \n use anyhow::{Context, Result, anyhow};\n-use rayon::iter::{IndexedParallelIterator, IntoParallelIterator, ParallelIterator};\n use serde::{Deserialize, Serialize};\n use smallvec::SmallVec;\n-use tracing::Span;\n use turbo_tasks::{\n     SessionId, TaskId,\n     backend::CachedTaskType,\n     panic_hooks::{PanicHookGuard, register_panic_hook},\n-    turbo_tasks_scope,\n+    parallel,\n };\n \n use crate::{\n@@ -331,14 +328,15 @@ impl<T: KeyValueDatabase + Send + Sync + 'static> BackingStorageSealed\n                     let _span = tracing::trace_span!(\"update task data\").entered();\n                     process_task_data(snapshots, Some(batch))?;\n                     let span = tracing::trace_span!(\"flush task data\").entered();\n-                    [KeySpace::TaskMeta, KeySpace::TaskData]\n-                        .into_par_iter()\n-                        .try_for_each(|key_space| {\n+                    parallel::try_for_each(\n+                        &[KeySpace::TaskMeta, KeySpace::TaskData],\n+                        |&key_space| {\n                             let _span = span.clone().entered();\n                             // Safety: We already finished all processing of the task data and task\n                             // meta\n                             unsafe { batch.flush(key_space) }\n-                        })?;\n+                        },\n+                    )?;\n                 }\n \n                 let mut next_task_id = get_next_free_task_id::<\n@@ -352,10 +350,9 @@ impl<T: KeyValueDatabase + Send + Sync + 'static> BackingStorageSealed\n                         items = task_cache_updates.iter().map(|m| m.len()).sum::<usize>()\n                     )\n                     .entered();\n-                    let result = task_cache_updates\n-                        .into_par_iter()\n-                        .with_max_len(1)\n-                        .map(|updates| {\n+                    let result = parallel::map_collect_owned::<_, _, Result<Vec<_>>>(\n+                        task_cache_updates,\n+                        |updates| {\n                             let _span = _span.clone().entered();\n                             let mut max_task_id = 0;\n \n@@ -390,15 +387,11 @@ impl<T: KeyValueDatabase + Send + Sync + 'static> BackingStorageSealed\n                             }\n \n                             Ok(max_task_id)\n-                        })\n-                        .reduce(\n-                            || Ok(0),\n-                            |a, b| -> anyhow::Result<_> {\n-                                let a_max = a?;\n-                                let b_max = b?;\n-                                Ok(max(a_max, b_max))\n-                            },\n-                        )?;\n+                        },\n+                    )?\n+                    .into_iter()\n+                    .max()\n+                    .unwrap_or(0);\n                     next_task_id = next_task_id.max(result);\n                 }\n \n@@ -410,64 +403,11 @@ impl<T: KeyValueDatabase + Send + Sync + 'static> BackingStorageSealed\n                 )?;\n             }\n             WriteBatch::Serial(batch) => {\n-                let mut task_items_result = Ok(Vec::new());\n-                turbo_tasks::scope(|s| {\n-                    s.spawn(|_| {\n-                        task_items_result =\n-                            process_task_data(snapshots, None::<&T::ConcurrentWriteBatch<'_>>);\n-                    });\n-\n-                    let mut next_task_id =\n-                        get_next_free_task_id::<\n-                            T::SerialWriteBatch<'_>,\n-                            T::ConcurrentWriteBatch<'_>,\n-                        >(&mut WriteBatchRef::serial(batch))?;\n-\n-                    {\n-                        let _span = tracing::trace_span!(\n-                            \"update task cache\",\n-                            items = task_cache_updates.iter().map(|m| m.len()).sum::<usize>()\n-                        )\n-                        .entered();\n-                        let mut task_type_bytes = Vec::new();\n-                        for (task_type, task_id) in task_cache_updates.into_iter().flatten() {\n-                            let task_id = *task_id;\n-                            serialize_task_type(&task_type, &mut task_type_bytes, task_id)?;\n-\n-                            batch\n-                                .put(\n-                                    KeySpace::ForwardTaskCache,\n-                                    WriteBuffer::Borrowed(&task_type_bytes),\n-                                    WriteBuffer::Borrowed(&task_id.to_le_bytes()),\n-                                )\n-                                .with_context(|| {\n-                                    anyhow!(\"Unable to write task cache {task_type:?} => {task_id}\")\n-                                })?;\n-                            batch\n-                                .put(\n-                                    KeySpace::ReverseTaskCache,\n-                                    WriteBuffer::Borrowed(IntKey::new(task_id).as_ref()),\n-                                    WriteBuffer::Borrowed(&task_type_bytes),\n-                                )\n-                                .with_context(|| {\n-                                    anyhow!(\"Unable to write task cache {task_id} => {task_type:?}\")\n-                                })?;\n-                            next_task_id = next_task_id.max(task_id + 1);\n-                        }\n-                    }\n-\n-                    save_infra::<T::SerialWriteBatch<'_>, T::ConcurrentWriteBatch<'_>>(\n-                        &mut WriteBatchRef::serial(batch),\n-                        next_task_id,\n-                        session_id,\n-                        operations,\n-                    )?;\n-                    anyhow::Ok(())\n-                })?;\n-\n                 {\n                     let _span = tracing::trace_span!(\"update tasks\").entered();\n-                    for (task_id, meta, data) in task_items_result?.into_iter().flatten() {\n+                    let task_items =\n+                        process_task_data(snapshots, None::<&T::ConcurrentWriteBatch<'_>>)?;\n+                    for (task_id, meta, data) in task_items.into_iter().flatten() {\n                         let key = IntKey::new(*task_id);\n                         let key = key.as_ref();\n                         if let Some(meta) = meta {\n@@ -485,7 +425,54 @@ impl<T: KeyValueDatabase + Send + Sync + 'static> BackingStorageSealed\n                                 })?;\n                         }\n                     }\n+                    batch.flush(KeySpace::TaskMeta)?;\n+                    batch.flush(KeySpace::TaskData)?;\n+                }\n+\n+                let mut next_task_id = get_next_free_task_id::<\n+                    T::SerialWriteBatch<'_>,\n+                    T::ConcurrentWriteBatch<'_>,\n+                >(&mut WriteBatchRef::serial(batch))?;\n+\n+                {\n+                    let _span = tracing::trace_span!(\n+                        \"update task cache\",\n+                        items = task_cache_updates.iter().map(|m| m.len()).sum::<usize>()\n+                    )\n+                    .entered();\n+                    let mut task_type_bytes = Vec::new();\n+                    for (task_type, task_id) in task_cache_updates.into_iter().flatten() {\n+                        let task_id = *task_id;\n+                        serialize_task_type(&task_type, &mut task_type_bytes, task_id)?;\n+\n+                        batch\n+                            .put(\n+                                KeySpace::ForwardTaskCache,\n+                                WriteBuffer::Borrowed(&task_type_bytes),\n+                                WriteBuffer::Borrowed(&task_id.to_le_bytes()),\n+                            )\n+                            .with_context(|| {\n+                                anyhow!(\"Unable to write task cache {task_type:?} => {task_id}\")\n+                            })?;\n+                        batch\n+                            .put(\n+                                KeySpace::ReverseTaskCache,\n+                                WriteBuffer::Borrowed(IntKey::new(task_id).as_ref()),\n+                                WriteBuffer::Borrowed(&task_type_bytes),\n+                            )\n+                            .with_context(|| {\n+                                anyhow!(\"Unable to write task cache {task_id} => {task_type:?}\")\n+                            })?;\n+                        next_task_id = next_task_id.max(task_id + 1);\n+                    }\n                 }\n+\n+                save_infra::<T::SerialWriteBatch<'_>, T::ConcurrentWriteBatch<'_>>(\n+                    &mut WriteBatchRef::serial(batch),\n+                    next_task_id,\n+                    session_id,\n+                    operations,\n+                )?;\n             }\n         }\n \n@@ -703,48 +690,38 @@ where\n         > + Send\n         + Sync,\n {\n-    let span = Span::current();\n-    let turbo_tasks = turbo_tasks::turbo_tasks();\n-    let handle = tokio::runtime::Handle::current();\n-    tasks\n-        .into_par_iter()\n-        .map(|tasks| {\n-            let _span = span.clone().entered();\n-            let _guard = handle.clone().enter();\n-            turbo_tasks_scope(turbo_tasks.clone(), || {\n-                let mut result = Vec::new();\n-                for (task_id, meta, data) in tasks {\n-                    if let Some(batch) = batch {\n-                        let key = IntKey::new(*task_id);\n-                        let key = key.as_ref();\n-                        if let Some(meta) = meta {\n-                            batch.put(\n-                                KeySpace::TaskMeta,\n-                                WriteBuffer::Borrowed(key),\n-                                WriteBuffer::SmallVec(meta),\n-                            )?;\n-                        }\n-                        if let Some(data) = data {\n-                            batch.put(\n-                                KeySpace::TaskData,\n-                                WriteBuffer::Borrowed(key),\n-                                WriteBuffer::SmallVec(data),\n-                            )?;\n-                        }\n-                    } else {\n-                        // Store the new task data\n-                        result.push((\n-                            task_id,\n-                            meta.map(WriteBuffer::SmallVec),\n-                            data.map(WriteBuffer::SmallVec),\n-                        ));\n-                    }\n+    parallel::map_collect_owned::<_, _, Result<Vec<_>>>(tasks, |tasks| {\n+        let mut result = Vec::new();\n+        for (task_id, meta, data) in tasks {\n+            if let Some(batch) = batch {\n+                let key = IntKey::new(*task_id);\n+                let key = key.as_ref();\n+                if let Some(meta) = meta {\n+                    batch.put(\n+                        KeySpace::TaskMeta,\n+                        WriteBuffer::Borrowed(key),\n+                        WriteBuffer::SmallVec(meta),\n+                    )?;\n+                }\n+                if let Some(data) = data {\n+                    batch.put(\n+                        KeySpace::TaskData,\n+                        WriteBuffer::Borrowed(key),\n+                        WriteBuffer::SmallVec(data),\n+                    )?;\n                 }\n+            } else {\n+                // Store the new task data\n+                result.push((\n+                    task_id,\n+                    meta.map(WriteBuffer::SmallVec),\n+                    data.map(WriteBuffer::SmallVec),\n+                ));\n+            }\n+        }\n \n-                Ok(result)\n-            })\n-        })\n-        .collect::<Result<Vec<_>>>()\n+        Ok(result)\n+    })\n }\n \n fn serialize(task: TaskId, data: &Vec<CachedDataItem>) -> Result<SmallVec<[u8; 16]>> {"
        },
        {
            "sha": "add31c32ecd358cb6f50aaac62de0921b96d58a8",
            "filename": "turbopack/crates/turbo-tasks-backend/tests/all_in_one.rs",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fall_in_one.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fall_in_one.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fall_in_one.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -9,7 +9,7 @@ use turbo_tasks_testing::{Registration, register, run};\n \n static REGISTRATION: Registration = register!();\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn all_in_one() {\n     run(&REGISTRATION, || async {\n         let a: Vc<u32> = Vc::cell(4242);"
        },
        {
            "sha": "a22cb96ade456c3284206a7f67ea181eda9a3652",
            "filename": "turbopack/crates/turbo-tasks-backend/tests/basic.rs",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fbasic.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fbasic.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fbasic.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -8,7 +8,7 @@ use turbo_tasks_testing::{Registration, register, run};\n \n static REGISTRATION: Registration = register!();\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn basic() {\n     run(&REGISTRATION, || async {\n         let output1 = func_without_args();"
        },
        {
            "sha": "5d225bdb8c48e8c3af7cdf74316a69e846e78631",
            "filename": "turbopack/crates/turbo-tasks-backend/tests/bug.rs",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fbug.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fbug.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fbug.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -24,7 +24,7 @@ struct TaskSpec {\n #[turbo_tasks::value(transparent)]\n struct TasksSpec(Vec<TaskSpec>);\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn graph_bug() {\n     // see https://github.com/vercel/next.js/pull/79451\n     run(&REGISTRATION, || async {"
        },
        {
            "sha": "a1495eeeca91bae792374014b11611cb8c797224",
            "filename": "turbopack/crates/turbo-tasks-backend/tests/bug2.rs",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fbug2.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fbug2.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fbug2.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -33,7 +33,7 @@ pub struct TaskSpec {\n #[turbo_tasks::value(transparent)]\n struct Iteration(State<usize>);\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn graph_bug() {\n     run(&REGISTRATION, move || async move {\n         let spec = vec!["
        },
        {
            "sha": "f06430ada2bd0ce6afc525bcffca794e270f24eb",
            "filename": "turbopack/crates/turbo-tasks-backend/tests/call_types.rs",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fcall_types.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fcall_types.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fcall_types.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -8,7 +8,7 @@ use turbo_tasks_testing::{Registration, register, run};\n \n static REGISTRATION: Registration = register!();\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn functions() {\n     run(&REGISTRATION, || async {\n         assert_eq!(*fn_plain().await?, 42);\n@@ -53,7 +53,7 @@ async fn async_fn_vc_arg(n: Vc<u32>) -> Result<Vc<u32>> {\n     Ok(Vc::cell(*n.await?))\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn methods() {\n     run(&REGISTRATION, || async {\n         assert_eq!(*Value::static_method().await?, 42);\n@@ -106,7 +106,7 @@ impl Value {\n     }\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn trait_methods() {\n     run(&REGISTRATION, || async {\n         assert_eq!(*Value::static_trait_method().await?, 42);"
        },
        {
            "sha": "945845a86e3a2490b5f833c6f080bf4b01aa4fef",
            "filename": "turbopack/crates/turbo-tasks-backend/tests/collectibles.rs",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fcollectibles.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fcollectibles.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fcollectibles.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -14,7 +14,7 @@ use turbo_tasks_testing::{Registration, register, run};\n \n static REGISTRATION: Registration = register!();\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn transitive_emitting() {\n     run(&REGISTRATION, || async {\n         let result_op = my_transitive_emitting_function(rcstr!(\"\"), rcstr!(\"\"));\n@@ -32,7 +32,7 @@ async fn transitive_emitting() {\n     .unwrap()\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn transitive_emitting_indirect() {\n     run(&REGISTRATION, || async {\n         let result_op = my_transitive_emitting_function(rcstr!(\"\"), rcstr!(\"\"));\n@@ -50,7 +50,7 @@ async fn transitive_emitting_indirect() {\n     .unwrap()\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn multi_emitting() {\n     run(&REGISTRATION, || async {\n         let result_op = my_multi_emitting_function();\n@@ -68,7 +68,7 @@ async fn multi_emitting() {\n     .unwrap()\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn taking_collectibles() {\n     run(&REGISTRATION, || async {\n         let result_op = my_collecting_function();\n@@ -84,7 +84,7 @@ async fn taking_collectibles() {\n     .unwrap()\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn taking_collectibles_extra_layer() {\n     run(&REGISTRATION, || async {\n         let result_op = my_collecting_function_indirect();\n@@ -100,7 +100,7 @@ async fn taking_collectibles_extra_layer() {\n     .unwrap()\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn taking_collectibles_parallel() {\n     run(&REGISTRATION, || async {\n         let result_op = my_transitive_emitting_function(rcstr!(\"\"), rcstr!(\"a\"));\n@@ -142,7 +142,7 @@ async fn taking_collectibles_parallel() {\n     .unwrap()\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn taking_collectibles_with_resolve() {\n     run(&REGISTRATION, || async {\n         let result_op = my_transitive_emitting_function_with_resolve(rcstr!(\"resolve\"));"
        },
        {
            "sha": "ccc833eeb85d8b820c31fb12a565a8a8d397c3ea",
            "filename": "turbopack/crates/turbo-tasks-backend/tests/debug.rs",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fdebug.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fdebug.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fdebug.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -9,7 +9,7 @@ use turbo_tasks_testing::{Registration, register, run};\n \n static REGISTRATION: Registration = register!();\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn primitive_debug() {\n     run(&REGISTRATION, || async {\n         let a: Vc<u32> = Vc::cell(42);\n@@ -20,7 +20,7 @@ async fn primitive_debug() {\n     .unwrap()\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn transparent_debug() {\n     run(&REGISTRATION, || async {\n         let a: Vc<Transparent> = Transparent(42).cell();\n@@ -32,7 +32,7 @@ async fn transparent_debug() {\n     .unwrap()\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn enum_none_debug() {\n     run(&REGISTRATION, || async {\n         let a: Vc<Enum> = Enum::None.cell();\n@@ -44,7 +44,7 @@ async fn enum_none_debug() {\n     .unwrap()\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn enum_transparent_debug() {\n     run(&REGISTRATION, || async {\n         let a: Vc<Enum> = Enum::Transparent(Transparent(42).resolved_cell()).cell();\n@@ -60,7 +60,7 @@ async fn enum_transparent_debug() {\n     .unwrap()\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn enum_inner_vc_debug() {\n     run(&REGISTRATION, || async {\n         let a: Vc<Enum> = Enum::Enum(Enum::None.resolved_cell()).cell();\n@@ -76,7 +76,7 @@ async fn enum_inner_vc_debug() {\n     .unwrap()\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn struct_unit_debug() {\n     run(&REGISTRATION, || async {\n         let a: Vc<StructUnit> = StructUnit.cell();\n@@ -87,7 +87,7 @@ async fn struct_unit_debug() {\n     .unwrap()\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn struct_transparent_debug() {\n     run(&REGISTRATION, || async {\n         let a: Vc<StructWithTransparent> = StructWithTransparent {\n@@ -106,7 +106,7 @@ async fn struct_transparent_debug() {\n     .unwrap()\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn struct_vec_debug() {\n     run(&REGISTRATION, || async {\n         let a: Vc<StructWithVec> = StructWithVec { vec: vec![] }.cell();\n@@ -135,7 +135,7 @@ async fn struct_vec_debug() {\n     .unwrap()\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn struct_ignore_debug() {\n     run(&REGISTRATION, || async {\n         let a: Vc<StructWithIgnore> = StructWithIgnore {"
        },
        {
            "sha": "b1c80929fad6aab584f41dca42c620f4b85e5b2a",
            "filename": "turbopack/crates/turbo-tasks-backend/tests/detached.rs",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fdetached.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fdetached.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fdetached.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -15,7 +15,7 @@ use turbo_tasks_testing::{Registration, register, run};\n \n static REGISTRATION: Registration = register!();\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn test_spawns_detached() -> anyhow::Result<()> {\n     run(&REGISTRATION, || async {\n         // HACK: The watch channel we use has an incorrect implementation of `TraceRawVcs`, just\n@@ -82,7 +82,7 @@ async fn spawns_detached(\n     Vc::cell(())\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn test_spawns_detached_changing() -> anyhow::Result<()> {\n     run(&REGISTRATION, || async {\n         // HACK: The watch channel we use has an incorrect implementation of `TraceRawVcs`"
        },
        {
            "sha": "89aa8998fae808a4b3e0d9a26aee3251875567dd",
            "filename": "turbopack/crates/turbo-tasks-backend/tests/dirty_in_progress.rs",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fdirty_in_progress.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fdirty_in_progress.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fdirty_in_progress.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -11,7 +11,7 @@ use turbo_tasks_testing::{Registration, register, run};\n \n static REGISTRATION: Registration = register!();\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn dirty_in_progress() {\n     run(&REGISTRATION, || async {\n         let cases = ["
        },
        {
            "sha": "87c2d6672e468160242e2406a327e42eec227fd1",
            "filename": "turbopack/crates/turbo-tasks-backend/tests/emptied_cells.rs",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Femptied_cells.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Femptied_cells.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Femptied_cells.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -8,7 +8,7 @@ use turbo_tasks_testing::{Registration, register, run};\n \n static REGISTRATION: Registration = register!();\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn recompute() {\n     run(&REGISTRATION, || async {\n         let input = ChangingInput {"
        },
        {
            "sha": "31933821102152421aec2f84e5e21cd30f75a4a8",
            "filename": "turbopack/crates/turbo-tasks-backend/tests/filter_unused_args.rs",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Ffilter_unused_args.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Ffilter_unused_args.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Ffilter_unused_args.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -8,7 +8,7 @@ use turbo_tasks_testing::{Registration, register, run};\n \n static REGISTRATION: Registration = register!();\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn filtered_trait_method_args() -> Result<()> {\n     run(&REGISTRATION, || async {\n         let uses_arg = UsesArg.cell();"
        },
        {
            "sha": "0c716c7544744d3717c7bdd0bf63aa1e895d460f",
            "filename": "turbopack/crates/turbo-tasks-backend/tests/immutable.rs",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fimmutable.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fimmutable.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fimmutable.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -8,7 +8,7 @@ use turbo_tasks_testing::{Registration, register, run};\n \n static REGISTRATION: Registration = register!();\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn hidden_mutate() {\n     run(&REGISTRATION, || async {\n         let input = create_input().resolve().await?;"
        },
        {
            "sha": "f66363d374635e8db5c672736382823195c43f75",
            "filename": "turbopack/crates/turbo-tasks-backend/tests/local_tasks.rs",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Flocal_tasks.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Flocal_tasks.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Flocal_tasks.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -8,7 +8,7 @@ use turbo_tasks_testing::{Registration, register, run};\n \n static REGISTRATION: Registration = register!();\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn test_local_task_id() -> Result<()> {\n     run(&REGISTRATION, || async {\n         let local_vc = get_local_task_id();"
        },
        {
            "sha": "457971d0667c7d893ae9ede21f3d595f65a81c8c",
            "filename": "turbopack/crates/turbo-tasks-backend/tests/operation_vc.rs",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Foperation_vc.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Foperation_vc.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Foperation_vc.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -26,7 +26,7 @@ fn use_operations() -> Vc<i32> {\n     forty_two.connect()\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn test_use_operations() -> Result<()> {\n     run(&REGISTRATION, || async {\n         assert_eq!(*use_operations().await?, 42);"
        },
        {
            "sha": "8b9458ab4f046d08fce33af8f1b75d3a292d5881",
            "filename": "turbopack/crates/turbo-tasks-backend/tests/panics.rs",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fpanics.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fpanics.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fpanics.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -25,7 +25,7 @@ static FILE_PATH_REGEX: LazyLock<Regex> =\n //\n // This test depends on the process-wide global panic handler. This test must be run in its own\n // process in isolation of any other tests.\n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn test_panic_hook() {\n     let prev_hook = take_hook();\n     set_hook(Box::new(move |info| {"
        },
        {
            "sha": "13b76582af6330c1c27bdc2c6be812dce68cdf64",
            "filename": "turbopack/crates/turbo-tasks-backend/tests/performance.rs",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fperformance.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fperformance.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fperformance.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -142,7 +142,7 @@ fn check_skip() -> bool {\n     false\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn many_calls_to_many_children() {\n     if check_skip() {\n         return;\n@@ -157,7 +157,7 @@ async fn many_calls_to_many_children() {\n     .unwrap();\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn many_calls_to_uncached_many_children() {\n     if check_skip() {\n         return;\n@@ -189,7 +189,7 @@ fn run_big_graph_test(counts: Vec<u32>) -> impl Future<Output = Result<()>> + Se\n     )\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn many_calls_to_big_graph_1() {\n     if check_skip() {\n         return;\n@@ -199,7 +199,7 @@ async fn many_calls_to_big_graph_1() {\n         .unwrap();\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn many_calls_to_big_graph_2() {\n     if check_skip() {\n         return;\n@@ -211,7 +211,7 @@ async fn many_calls_to_big_graph_2() {\n     .unwrap();\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn many_calls_to_big_graph_3() {\n     if check_skip() {\n         return;\n@@ -221,7 +221,7 @@ async fn many_calls_to_big_graph_3() {\n         .unwrap();\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn many_calls_to_big_graph_4() {\n     if check_skip() {\n         return;\n@@ -231,7 +231,7 @@ async fn many_calls_to_big_graph_4() {\n         .unwrap();\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn many_calls_to_big_graph_5() {\n     if check_skip() {\n         return;\n@@ -243,7 +243,7 @@ async fn many_calls_to_big_graph_5() {\n     .unwrap();\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn many_calls_to_big_graph_6() {\n     if check_skip() {\n         return;\n@@ -255,7 +255,7 @@ async fn many_calls_to_big_graph_6() {\n     .unwrap();\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn many_calls_to_big_graph_7() {\n     if check_skip() {\n         return;\n@@ -270,7 +270,7 @@ async fn many_calls_to_big_graph_7() {\n     .unwrap();\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn many_calls_to_big_graph_8() {\n     if check_skip() {\n         return;\n@@ -282,7 +282,7 @@ async fn many_calls_to_big_graph_8() {\n     .unwrap();\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn many_calls_to_big_graph_9() {\n     if check_skip() {\n         return;"
        },
        {
            "sha": "089490ab1c79ce60ff94ec0a0d231a52fca966f5",
            "filename": "turbopack/crates/turbo-tasks-backend/tests/random_change.rs",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Frandom_change.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Frandom_change.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Frandom_change.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -9,7 +9,7 @@ use turbo_tasks_testing::{Registration, register, run};\n \n static REGISTRATION: Registration = register!();\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn random_change() {\n     run(&REGISTRATION, || async {\n         let state = make_state();"
        },
        {
            "sha": "66c51c9e4f1ad094a436a91f31c64f9d78b74bcb",
            "filename": "turbopack/crates/turbo-tasks-backend/tests/read_ref_cell.rs",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fread_ref_cell.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fread_ref_cell.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fread_ref_cell.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -10,7 +10,7 @@ use turbo_tasks_testing::{Registration, register, run};\n \n static REGISTRATION: Registration = register!();\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn read_ref() {\n     run(&REGISTRATION, || async {\n         let counter = Counter::cell(Counter {"
        },
        {
            "sha": "dcad783b06e08c5da6b9c85243e84bdccd355e89",
            "filename": "turbopack/crates/turbo-tasks-backend/tests/recompute.rs",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Frecompute.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Frecompute.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Frecompute.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -8,7 +8,7 @@ use turbo_tasks_testing::{Registration, register, run};\n \n static REGISTRATION: Registration = register!();\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn recompute() {\n     run(&REGISTRATION, || async {\n         let input = ChangingInput {\n@@ -58,7 +58,7 @@ async fn recompute() {\n     .unwrap()\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn immutable_analysis() {\n     run(&REGISTRATION, || async {\n         let input = ChangingInput {"
        },
        {
            "sha": "d7c0be301ac7077e8ccc3326e44fd57c4826c44d",
            "filename": "turbopack/crates/turbo-tasks-backend/tests/recompute_collectibles.rs",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Frecompute_collectibles.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Frecompute_collectibles.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Frecompute_collectibles.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -9,7 +9,7 @@ use turbo_tasks_testing::{Registration, register, run};\n \n static REGISTRATION: Registration = register!();\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn recompute() {\n     run(&REGISTRATION, || async {\n         let input = ChangingInput::new(1).resolve().await?;"
        },
        {
            "sha": "a0b9914b7f8bb9802678e235b72f4360614abd9f",
            "filename": "turbopack/crates/turbo-tasks-backend/tests/resolved_vc.rs",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fresolved_vc.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fresolved_vc.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fresolved_vc.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -23,7 +23,7 @@ fn assert_resolved(input: ResolvedVc<u32>) {\n     assert!(input_vc.is_resolved());\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn test_conversion() -> Result<()> {\n     run(&REGISTRATION, || async {\n         let unresolved: Vc<u32> = Vc::cell(42);\n@@ -38,7 +38,7 @@ async fn test_conversion() -> Result<()> {\n     .await\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn test_cell_construction() -> Result<()> {\n     run(&REGISTRATION, || async {\n         let a: ResolvedVc<u32> = ResolvedVc::cell(42);\n@@ -50,7 +50,7 @@ async fn test_cell_construction() -> Result<()> {\n     .await\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn test_resolved_vc_as_arg() -> Result<()> {\n     run(&REGISTRATION, || async {\n         let unresolved: Vc<u32> = returns_int(42);\n@@ -62,7 +62,7 @@ async fn test_resolved_vc_as_arg() -> Result<()> {\n     .await\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn test_into_future() -> Result<()> {\n     run(&REGISTRATION, || async {\n         let mut resolved = ResolvedVc::cell(42);\n@@ -78,7 +78,7 @@ async fn test_into_future() -> Result<()> {\n     .await\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn test_sidecast() -> Result<()> {\n     run(&REGISTRATION, || async {\n         let concrete_value = ImplementsAAndB.resolved_cell();"
        },
        {
            "sha": "dc82e82174de5f216f9755fd0c76f8ddda5a5162",
            "filename": "turbopack/crates/turbo-tasks-backend/tests/shrink_to_fit.rs",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fshrink_to_fit.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fshrink_to_fit.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Fshrink_to_fit.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -11,7 +11,7 @@ static REGISTRATION: Registration = register!();\n #[turbo_tasks::value(transparent)]\n struct Wrapper(Vec<u32>);\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn test_shrink_to_fit() -> Result<()> {\n     run(&REGISTRATION, || async {\n         // `Vec::shrink_to_fit` is implicitly called when a cell is constructed."
        },
        {
            "sha": "869c944bcb5c7c8e16d05590fc647ccb00168034",
            "filename": "turbopack/crates/turbo-tasks-backend/tests/task_statistics.rs",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Ftask_statistics.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Ftask_statistics.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Ftask_statistics.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -13,7 +13,7 @@ use turbo_tasks_testing::{Registration, register, run_without_cache_check};\n \n static REGISTRATION: Registration = register!();\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn test_simple_task() -> Result<()> {\n     run_without_cache_check(&REGISTRATION, async move {\n         enable_stats();\n@@ -39,7 +39,7 @@ async fn test_simple_task() -> Result<()> {\n     .await\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn test_await_same_vc_multiple_times() -> Result<()> {\n     run_without_cache_check(&REGISTRATION, async move {\n         enable_stats();\n@@ -61,7 +61,7 @@ async fn test_await_same_vc_multiple_times() -> Result<()> {\n     .await\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn test_vc_receiving_task() -> Result<()> {\n     run_without_cache_check(&REGISTRATION, async move {\n         enable_stats();\n@@ -93,7 +93,7 @@ async fn test_vc_receiving_task() -> Result<()> {\n     .await\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn test_trait_methods() -> Result<()> {\n     run_without_cache_check(&REGISTRATION, async move {\n         enable_stats();\n@@ -130,7 +130,7 @@ async fn test_trait_methods() -> Result<()> {\n     .await\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn test_dyn_trait_methods() -> Result<()> {\n     run_without_cache_check(&REGISTRATION, async move {\n         enable_stats();\n@@ -174,7 +174,7 @@ async fn test_dyn_trait_methods() -> Result<()> {\n }\n \n // creates Vcs, but doesn't ever execute them\n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn test_no_execution() -> Result<()> {\n     run_without_cache_check(&REGISTRATION, async move {\n         enable_stats();"
        },
        {
            "sha": "f553a83a52c5b96a38d42846f41545698353b9ef",
            "filename": "turbopack/crates/turbo-tasks-backend/tests/trace_transient.rs",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Ftrace_transient.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Ftrace_transient.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Ftrace_transient.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -18,7 +18,7 @@ Adder::add_method (read cell of type turbo-tasks@TODO::::primitives::u64)\n     unknown transient task (read cell of type turbo-tasks@TODO::::primitives::u16)\n     unknown transient task (read cell of type turbo-tasks@TODO::::primitives::u32)\";\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn test_trace_transient() {\n     let result = run_without_cache_check(&REGISTRATION, async {\n         read_incorrect_task_input_operation(IncorrectTaskInput("
        },
        {
            "sha": "237294730336008f7a66fcf9145bf94f48b141ea",
            "filename": "turbopack/crates/turbo-tasks-backend/tests/trait_ref_cell.rs",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Ftrait_ref_cell.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Ftrait_ref_cell.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Ftrait_ref_cell.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -10,7 +10,7 @@ use turbo_tasks_testing::{Registration, register, run};\n \n static REGISTRATION: Registration = register!();\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn trait_ref() {\n     run(&REGISTRATION, || async {\n         let counter = Counter::cell(Counter {"
        },
        {
            "sha": "3b8d1cb15c02acc2b39f49546105c4a03b0f7a2b",
            "filename": "turbopack/crates/turbo-tasks-backend/tests/trait_ref_cell_mode.rs",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Ftrait_ref_cell_mode.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Ftrait_ref_cell_mode.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Ftrait_ref_cell_mode.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -9,7 +9,7 @@ static REGISTRATION: Registration = register!();\n \n // Test that with `cell = \"shared\"`, the cell will be re-used as long as the\n // value is equal.\n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn test_trait_ref_shared_cell_mode() {\n     run(&REGISTRATION, || async {\n         let input = CellIdSelector {\n@@ -44,7 +44,7 @@ async fn test_trait_ref_shared_cell_mode() {\n \n // Test that with `cell = \"new\"`, the cell will is never re-used, even if the\n // value is equal.\n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn test_trait_ref_new_cell_mode() {\n     run(&REGISTRATION, || async {\n         let input = CellIdSelector {"
        },
        {
            "sha": "b144319ed4763725cf8265bca4ec9cccc25ff5d7",
            "filename": "turbopack/crates/turbo-tasks-backend/tests/transient_collectible.rs",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Ftransient_collectible.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Ftransient_collectible.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Ftransient_collectible.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -10,7 +10,7 @@ static REGISTRATION: Registration = register!();\n const EXPECTED_MSG: &str =\n     \"Collectible is transient, transient collectibles cannot be emitted from persistent tasks\";\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn test_transient_emit_from_persistent() {\n     let result = run_without_cache_check(&REGISTRATION, async {\n         emit_incorrect_task_input_operation(IncorrectTaskInput(U32Wrapper(123).resolved_cell()))"
        },
        {
            "sha": "100008c755c5cb2673fd3bcdc9f2078c812cd903",
            "filename": "turbopack/crates/turbo-tasks-backend/tests/transient_vc.rs",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Ftransient_vc.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Ftransient_vc.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Ftransient_vc.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -7,7 +7,7 @@ use turbo_tasks_testing::{Registration, register, run_without_cache_check};\n \n static REGISTRATION: Registration = register!();\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn test_transient_vc() -> Result<()> {\n     run_without_cache_check(&REGISTRATION, async {\n         test_transient_operation(TransientValue::new(123))"
        },
        {
            "sha": "b44f2a3a00522d3cef9e57023d89454d8b207f51",
            "filename": "turbopack/crates/turbo-tasks-fetch/tests/fetch.rs",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-fetch%2Ftests%2Ffetch.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-fetch%2Ftests%2Ffetch.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-fetch%2Ftests%2Ffetch.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -18,7 +18,7 @@ static REGISTRATION: Registration = register!(turbo_tasks_fetch::register);\n /// acquire and hold this lock to prevent potential flakiness.\n static GLOBAL_TEST_LOCK: TokioMutex<()> = TokioMutex::const_new(());\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn basic_get() {\n     let _guard = GLOBAL_TEST_LOCK.lock().await;\n     run(&REGISTRATION, || async {\n@@ -49,7 +49,7 @@ async fn basic_get() {\n     .unwrap()\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn sends_user_agent() {\n     let _guard = GLOBAL_TEST_LOCK.lock().await;\n     run(&REGISTRATION, || async {\n@@ -85,7 +85,7 @@ async fn sends_user_agent() {\n \n // This is temporary behavior.\n // TODO: Implement invalidation that respects Cache-Control headers.\n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn invalidation_does_not_invalidate() {\n     let _guard = GLOBAL_TEST_LOCK.lock().await;\n     run(&REGISTRATION, || async {\n@@ -130,7 +130,7 @@ fn get_issue_context() -> Vc<FileSystemPath> {\n     DiskFileSystem::new(rcstr!(\"root\"), rcstr!(\"/\")).root()\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn errors_on_failed_connection() {\n     let _guard = GLOBAL_TEST_LOCK.lock().await;\n     run(&REGISTRATION, || async {\n@@ -161,7 +161,7 @@ async fn errors_on_failed_connection() {\n     .unwrap()\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn errors_on_404() {\n     let _guard = GLOBAL_TEST_LOCK.lock().await;\n     run(&REGISTRATION, || async {\n@@ -196,7 +196,7 @@ async fn errors_on_404() {\n     .unwrap()\n }\n \n-#[tokio::test]\n+#[tokio::test(flavor = \"multi_thread\")]\n async fn client_cache() {\n     // a simple fetch that should always succeed\n     async fn simple_fetch(path: &str, client: FetchClient) -> anyhow::Result<()> {"
        },
        {
            "sha": "2ae2dc3036f0c7752938051bac37cf68d56c1843",
            "filename": "turbopack/crates/turbo-tasks-fs/src/lib.rs",
            "status": "modified",
            "additions": 14,
            "deletions": 24,
            "changes": 38,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-fs%2Fsrc%2Flib.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-fs%2Fsrc%2Flib.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-fs%2Fsrc%2Flib.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -46,7 +46,6 @@ use dunce::simplified;\n use indexmap::IndexSet;\n use jsonc_parser::{ParseOptions, parse_to_serde_value};\n use mime::Mime;\n-use rayon::iter::{IntoParallelIterator, ParallelIterator};\n use rustc_hash::FxHashSet;\n use serde::{Deserialize, Serialize};\n use serde_json::Value;\n@@ -56,7 +55,7 @@ use turbo_rcstr::{RcStr, rcstr};\n use turbo_tasks::{\n     ApplyEffectsContext, Completion, InvalidationReason, Invalidator, NonLocalValue, ReadRef,\n     ResolvedVc, TaskInput, ValueToString, Vc, debug::ValueDebugFormat, effect,\n-    mark_session_dependent, mark_stateful, trace::TraceRawVcs,\n+    mark_session_dependent, mark_stateful, parallel, trace::TraceRawVcs,\n };\n use turbo_tasks_hash::{DeterministicHash, DeterministicHasher, hash_xxh3_hash64};\n use turbo_unix_path::{\n@@ -309,19 +308,14 @@ impl DiskFileSystemInner {\n \n     fn invalidate(&self) {\n         let _span = tracing::info_span!(\"invalidate filesystem\", name = &*self.root).entered();\n-        let span = tracing::Span::current();\n-        let handle = tokio::runtime::Handle::current();\n         let invalidator_map = take(&mut *self.invalidator_map.lock().unwrap());\n         let dir_invalidator_map = take(&mut *self.dir_invalidator_map.lock().unwrap());\n-        let iter = invalidator_map\n-            .into_par_iter()\n-            .chain(dir_invalidator_map.into_par_iter())\n-            .flat_map(|(_, invalidators)| invalidators.into_par_iter());\n-        iter.for_each(|(i, _)| {\n-            let _span = span.clone().entered();\n-            let _guard = handle.enter();\n-            i.invalidate()\n-        });\n+        let invalidators = invalidator_map\n+            .into_iter()\n+            .chain(dir_invalidator_map)\n+            .flat_map(|(_, invalidators)| invalidators.into_keys())\n+            .collect::<Vec<_>>();\n+        parallel::for_each_owned(invalidators, |invalidator| invalidator.invalidate());\n     }\n \n     /// Invalidates every tracked file in the filesystem.\n@@ -332,23 +326,19 @@ impl DiskFileSystemInner {\n         reason: impl Fn(&Path) -> R + Sync,\n     ) {\n         let _span = tracing::info_span!(\"invalidate filesystem\", name = &*self.root).entered();\n-        let span = tracing::Span::current();\n-        let handle = tokio::runtime::Handle::current();\n         let invalidator_map = take(&mut *self.invalidator_map.lock().unwrap());\n         let dir_invalidator_map = take(&mut *self.dir_invalidator_map.lock().unwrap());\n-        let iter = invalidator_map\n-            .into_par_iter()\n-            .chain(dir_invalidator_map.into_par_iter())\n+        let invalidators = invalidator_map\n+            .into_iter()\n+            .chain(dir_invalidator_map)\n             .flat_map(|(path, invalidators)| {\n-                let _span = span.clone().entered();\n                 let reason_for_path = reason(&path);\n                 invalidators\n-                    .into_par_iter()\n+                    .into_keys()\n                     .map(move |i| (reason_for_path.clone(), i))\n-            });\n-        iter.for_each(|(reason, (invalidator, _))| {\n-            let _span = span.clone().entered();\n-            let _guard = handle.enter();\n+            })\n+            .collect::<Vec<_>>();\n+        parallel::for_each_owned(invalidators, |(reason, invalidator)| {\n             invalidator.invalidate_with_reason(reason)\n         });\n     }"
        },
        {
            "sha": "a9f58e0f2d6ca2eb47afa756a5ca67520c5dd0b7",
            "filename": "turbopack/crates/turbo-tasks-fs/src/watcher.rs",
            "status": "modified",
            "additions": 20,
            "deletions": 31,
            "changes": 51,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-fs%2Fsrc%2Fwatcher.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks-fs%2Fsrc%2Fwatcher.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-fs%2Fsrc%2Fwatcher.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -16,13 +16,12 @@ use notify::{\n     Config, EventKind, PollWatcher, RecommendedWatcher, RecursiveMode, Watcher,\n     event::{MetadataKind, ModifyKind, RenameMode},\n };\n-use rayon::iter::{IntoParallelIterator, ParallelIterator};\n use rustc_hash::FxHashSet;\n use serde::{Deserialize, Serialize};\n use tracing::instrument;\n use turbo_rcstr::RcStr;\n use turbo_tasks::{\n-    FxIndexSet, InvalidationReason, InvalidationReasonKind, Invalidator, spawn_thread,\n+    FxIndexSet, InvalidationReason, InvalidationReasonKind, Invalidator, parallel, spawn_thread,\n     util::StaticOrArc,\n };\n \n@@ -397,40 +396,30 @@ impl DiskWatcher {\n         //\n         // Best is to start_watching before starting to read\n         {\n-            let span = tracing::info_span!(\"invalidate filesystem\");\n-            let _span = span.clone().entered();\n+            let _span = tracing::info_span!(\"invalidate filesystem\").entered();\n             let invalidator_map = take(&mut *fs_inner.invalidator_map.lock().unwrap());\n             let dir_invalidator_map = take(&mut *fs_inner.dir_invalidator_map.lock().unwrap());\n-            let iter = invalidator_map\n-                .into_par_iter()\n-                .chain(dir_invalidator_map.into_par_iter());\n-            let handle = tokio::runtime::Handle::current();\n+            let iter = invalidator_map.into_iter().chain(dir_invalidator_map);\n             if report_invalidation_reason {\n-                iter.flat_map(|(path, invalidators)| {\n-                    let _span = span.clone().entered();\n-                    let reason = WatchStart {\n-                        name: fs_inner.name.clone(),\n-                        // this path is just used for display purposes\n-                        path: RcStr::from(path.to_string_lossy()),\n-                    };\n-                    invalidators\n-                        .into_par_iter()\n-                        .map(move |i| (reason.clone(), i))\n-                })\n-                .for_each(|(reason, (invalidator, _))| {\n-                    let _span = span.clone().entered();\n-                    let _guard = handle.enter();\n-                    invalidator.invalidate_with_reason(reason)\n+                let invalidators = iter\n+                    .flat_map(|(path, invalidators)| {\n+                        let reason = WatchStart {\n+                            name: fs_inner.name.clone(),\n+                            // this path is just used for display purposes\n+                            path: RcStr::from(path.to_string_lossy()),\n+                        };\n+                        invalidators.into_iter().map(move |i| (reason.clone(), i))\n+                    })\n+                    .collect::<Vec<_>>();\n+                parallel::for_each_owned(invalidators, |(reason, (invalidator, _))| {\n+                    invalidator.invalidate_with_reason(reason);\n                 });\n             } else {\n-                iter.flat_map(|(_, invalidators)| {\n-                    let _span = span.clone().entered();\n-                    invalidators.into_par_iter().map(move |i| i)\n-                })\n-                .for_each(|(invalidator, _)| {\n-                    let _span = span.clone().entered();\n-                    let _guard = handle.enter();\n-                    invalidator.invalidate()\n+                let invalidators = iter\n+                    .flat_map(|(_, invalidators)| invalidators.into_keys())\n+                    .collect::<Vec<_>>();\n+                parallel::for_each_owned(invalidators, |invalidator| {\n+                    invalidator.invalidate();\n                 });\n             }\n         }"
        },
        {
            "sha": "fa7714840597ebe06e71ab6d602dbc5e5b18c09d",
            "filename": "turbopack/crates/turbo-tasks/src/lib.rs",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Flib.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Flib.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Flib.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -37,6 +37,8 @@\n #![feature(never_type)]\n #![feature(downcast_unchecked)]\n #![feature(ptr_metadata)]\n+#![feature(sync_unsafe_cell)]\n+#![feature(vec_into_raw_parts)]\n \n pub mod backend;\n mod capture_future;\n@@ -64,13 +66,14 @@ mod no_move_vec;\n mod once_map;\n mod output;\n pub mod panic_hooks;\n+pub mod parallel;\n pub mod persisted_graph;\n pub mod primitives;\n mod raw_vc;\n mod read_options;\n mod read_ref;\n pub mod registry;\n-mod scope;\n+pub mod scope;\n mod serialization_invalidation;\n pub mod small_duration;\n mod state;\n@@ -115,7 +118,6 @@ pub use raw_vc::{CellId, RawVc, ReadRawVcFuture, ResolveTypeError};\n pub use read_options::ReadCellOptions;\n pub use read_ref::ReadRef;\n use rustc_hash::FxHasher;\n-pub use scope::scope;\n pub use serialization_invalidation::SerializationInvalidator;\n pub use shrink_to_fit::ShrinkToFit;\n pub use state::{State, TransientState};"
        },
        {
            "sha": "82be6345e6a3d8a70273ff4e131f6a96dd92fd64",
            "filename": "turbopack/crates/turbo-tasks/src/manager.rs",
            "status": "modified",
            "additions": 25,
            "deletions": 18,
            "changes": 43,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Fmanager.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Fmanager.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Fmanager.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -1060,27 +1060,30 @@ impl<B: Backend + 'static> TurboTasks<B> {\n     }\n \n     pub async fn stop_and_wait(&self) {\n-        self.backend.stopping(self);\n-        self.stopped.store(true, Ordering::Release);\n-        {\n-            let listener = self\n-                .event\n-                .listen_with_note(|| || \"wait for stop\".to_string());\n-            if self.currently_scheduled_tasks.load(Ordering::Acquire) != 0 {\n-                listener.await;\n+        turbo_tasks_future_scope(self.pin(), async move {\n+            self.backend.stopping(self);\n+            self.stopped.store(true, Ordering::Release);\n+            {\n+                let listener = self\n+                    .event\n+                    .listen_with_note(|| || \"wait for stop\".to_string());\n+                if self.currently_scheduled_tasks.load(Ordering::Acquire) != 0 {\n+                    listener.await;\n+                }\n             }\n-        }\n-        {\n-            let listener = self.event_background.listen();\n-            if self\n-                .currently_scheduled_background_jobs\n-                .load(Ordering::Acquire)\n-                != 0\n             {\n-                listener.await;\n+                let listener = self.event_background.listen();\n+                if self\n+                    .currently_scheduled_background_jobs\n+                    .load(Ordering::Acquire)\n+                    != 0\n+                {\n+                    listener.await;\n+                }\n             }\n-        }\n-        self.backend.stop(self);\n+            self.backend.stop(self);\n+        })\n+        .await;\n     }\n \n     #[track_caller]\n@@ -1677,6 +1680,10 @@ pub fn turbo_tasks() -> Arc<dyn TurboTasksApi> {\n     TURBO_TASKS.with(|arc| arc.clone())\n }\n \n+pub fn try_turbo_tasks() -> Option<Arc<dyn TurboTasksApi>> {\n+    TURBO_TASKS.try_with(|arc| arc.clone()).ok()\n+}\n+\n pub fn with_turbo_tasks<T>(func: impl FnOnce(&Arc<dyn TurboTasksApi>) -> T) -> T {\n     TURBO_TASKS.with(|arc| func(arc))\n }"
        },
        {
            "sha": "e20b67bf678924e17e81878eb7f07eec39e99e63",
            "filename": "turbopack/crates/turbo-tasks/src/parallel.rs",
            "status": "added",
            "additions": 308,
            "deletions": 0,
            "changes": 308,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Fparallel.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Fparallel.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Fparallel.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -0,0 +1,308 @@\n+//! Parallel for each and map using tokio tasks.\n+//!\n+//! This avoid the problem of sleeping threads with mimalloc when using rayon in combination with\n+//! tokio. It also avoid having multiple thread pools.\n+//! see also https://pwy.io/posts/mimalloc-cigarette/\n+\n+use std::{sync::LazyLock, thread::available_parallelism};\n+\n+use crate::{scope::scope_and_block, util::into_chunks};\n+\n+/// Calculates a good chunk size for parallel processing based on the number of available threads.\n+/// This is used to ensure that the workload is evenly distributed across the threads.\n+fn good_chunk_size(len: usize) -> usize {\n+    static GOOD_CHUNK_COUNT: LazyLock<usize> =\n+        LazyLock::new(|| available_parallelism().map_or(16, |c| c.get() * 4));\n+    let min_chunk_count = *GOOD_CHUNK_COUNT;\n+    len.div_ceil(min_chunk_count)\n+}\n+\n+pub fn for_each<'l, T, F>(items: &'l [T], f: F)\n+where\n+    T: Sync,\n+    F: Fn(&'l T) + Send + Sync,\n+{\n+    let len = items.len();\n+    if len <= 1 {\n+        for item in items {\n+            f(item);\n+        }\n+        return;\n+    }\n+    let chunk_size = good_chunk_size(len);\n+    let f = &f;\n+    let _results = scope_and_block(len.div_ceil(chunk_size), |scope| {\n+        for chunk in items.chunks(chunk_size) {\n+            scope.spawn(async move {\n+                for item in chunk {\n+                    f(item);\n+                }\n+            })\n+        }\n+    });\n+}\n+\n+pub fn for_each_owned<T>(items: Vec<T>, f: impl Fn(T) + Send + Sync)\n+where\n+    T: Send + Sync,\n+{\n+    let len = items.len();\n+    if len <= 1 {\n+        for item in items {\n+            f(item);\n+        }\n+        return;\n+    }\n+    let chunk_size = good_chunk_size(len);\n+    let f = &f;\n+    let _results = scope_and_block(len.div_ceil(chunk_size), |scope| {\n+        for chunk in into_chunks(items, chunk_size) {\n+            scope.spawn(async move {\n+                // SAFETY: Even when f() panics we drop all items in the chunk.\n+                for item in chunk {\n+                    f(item);\n+                }\n+            })\n+        }\n+    });\n+}\n+\n+pub fn try_for_each<'l, T, E>(\n+    items: &'l [T],\n+    f: impl (Fn(&'l T) -> Result<(), E>) + Send + Sync,\n+) -> Result<(), E>\n+where\n+    T: Sync,\n+    E: Send + 'static,\n+{\n+    let len = items.len();\n+    if len <= 1 {\n+        for item in items {\n+            f(item)?;\n+        }\n+        return Ok(());\n+    }\n+    let chunk_size = good_chunk_size(len);\n+    let f = &f;\n+    scope_and_block(len.div_ceil(chunk_size), |scope| {\n+        for chunk in items.chunks(chunk_size) {\n+            scope.spawn(async move {\n+                for item in chunk {\n+                    f(item)?;\n+                }\n+                Ok(())\n+            })\n+        }\n+    })\n+    .collect::<Result<(), E>>()\n+}\n+\n+pub fn try_for_each_mut<'l, T, E>(\n+    items: &'l mut [T],\n+    f: impl (Fn(&'l mut T) -> Result<(), E>) + Send + Sync,\n+) -> Result<(), E>\n+where\n+    T: Send + Sync,\n+    E: Send + 'static,\n+{\n+    let len = items.len();\n+    if len <= 1 {\n+        for item in items {\n+            f(item)?;\n+        }\n+        return Ok(());\n+    }\n+    let chunk_size = good_chunk_size(len);\n+    let f = &f;\n+    scope_and_block(len.div_ceil(chunk_size), |scope| {\n+        for chunk in items.chunks_mut(chunk_size) {\n+            scope.spawn(async move {\n+                for item in chunk {\n+                    f(item)?;\n+                }\n+                Ok(())\n+            })\n+        }\n+    })\n+    .collect::<Result<(), E>>()\n+}\n+\n+pub fn try_for_each_owned<T, E>(\n+    items: Vec<T>,\n+    f: impl (Fn(T) -> Result<(), E>) + Send + Sync,\n+) -> Result<(), E>\n+where\n+    T: Send + Sync,\n+    E: Send + 'static,\n+{\n+    let len = items.len();\n+    if len <= 1 {\n+        for item in items {\n+            f(item)?;\n+        }\n+        return Ok(());\n+    }\n+    let chunk_size = good_chunk_size(len);\n+    let f = &f;\n+    scope_and_block(len.div_ceil(chunk_size), |scope| {\n+        for chunk in into_chunks(items, chunk_size) {\n+            scope.spawn(async move {\n+                for item in chunk {\n+                    f(item)?;\n+                }\n+                Ok(())\n+            })\n+        }\n+    })\n+    .collect::<Result<(), E>>()\n+}\n+\n+pub fn map_collect<'l, Item, PerItemResult, Result>(\n+    items: &'l [Item],\n+    f: impl Fn(&'l Item) -> PerItemResult + Send + Sync,\n+) -> Result\n+where\n+    Item: Sync,\n+    PerItemResult: Send + Sync + 'l,\n+    Result: FromIterator<PerItemResult>,\n+{\n+    let len = items.len();\n+    if len == 0 {\n+        return Result::from_iter(std::iter::empty()); // No items to process, return empty\n+        // collection\n+    }\n+    let chunk_size = good_chunk_size(len);\n+    let f = &f;\n+    scope_and_block(len.div_ceil(chunk_size), |scope| {\n+        for chunk in items.chunks(chunk_size) {\n+            scope.spawn(async move { chunk.iter().map(f).collect::<Vec<_>>() })\n+        }\n+    })\n+    .flatten()\n+    .collect()\n+}\n+\n+pub fn map_collect_owned<'l, Item, PerItemResult, Result>(\n+    items: Vec<Item>,\n+    f: impl Fn(Item) -> PerItemResult + Send + Sync,\n+) -> Result\n+where\n+    Item: Send + Sync,\n+    PerItemResult: Send + Sync + 'l,\n+    Result: FromIterator<PerItemResult>,\n+{\n+    let len = items.len();\n+    if len == 0 {\n+        return Result::from_iter(std::iter::empty()); // No items to process, return empty\n+        // collection;\n+    }\n+    let chunk_size = good_chunk_size(len);\n+    let f = &f;\n+    scope_and_block(len.div_ceil(chunk_size), |scope| {\n+        for chunk in into_chunks(items, chunk_size) {\n+            scope.spawn(async move { chunk.map(f).collect::<Vec<_>>() })\n+        }\n+    })\n+    .flatten()\n+    .collect()\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use std::{\n+        panic::{AssertUnwindSafe, catch_unwind},\n+        sync::atomic::{AtomicI32, Ordering},\n+    };\n+\n+    use super::*;\n+\n+    #[tokio::test(flavor = \"multi_thread\")]\n+    async fn test_parallel_for_each() {\n+        let input = vec![1, 2, 3, 4, 5];\n+        let sum = AtomicI32::new(0);\n+        for_each(&input, |&x| {\n+            sum.fetch_add(x, Ordering::SeqCst);\n+        });\n+        assert_eq!(sum.load(Ordering::SeqCst), 15);\n+    }\n+\n+    #[tokio::test(flavor = \"multi_thread\")]\n+    async fn test_parallel_try_for_each() {\n+        let input = vec![1, 2, 3, 4, 5];\n+        let result = try_for_each(&input, |&x| {\n+            if x % 2 == 0 {\n+                Ok(())\n+            } else {\n+                Err(format!(\"Odd number {x} encountered\"))\n+            }\n+        });\n+        assert!(result.is_err());\n+        assert_eq!(result.unwrap_err(), \"Odd number 1 encountered\");\n+    }\n+\n+    #[tokio::test(flavor = \"multi_thread\")]\n+    async fn test_parallel_try_for_each_mut() {\n+        let mut input = vec![1, 2, 3, 4, 5];\n+        let result = try_for_each_mut(&mut input, |x| {\n+            *x += 10;\n+            if *x % 2 == 0 {\n+                Ok(())\n+            } else {\n+                Err(format!(\"Odd number {} encountered\", *x))\n+            }\n+        });\n+        assert!(result.is_err());\n+        assert_eq!(result.unwrap_err(), \"Odd number 11 encountered\");\n+        assert_eq!(input, vec![11, 12, 13, 14, 15]);\n+    }\n+\n+    #[tokio::test(flavor = \"multi_thread\")]\n+    async fn test_parallel_for_each_owned() {\n+        let input = vec![1, 2, 3, 4, 5];\n+        let sum = AtomicI32::new(0);\n+        for_each_owned(input, |x| {\n+            sum.fetch_add(x, Ordering::SeqCst);\n+        });\n+        assert_eq!(sum.load(Ordering::SeqCst), 15);\n+    }\n+\n+    #[tokio::test(flavor = \"multi_thread\")]\n+    async fn test_parallel_map_collect() {\n+        let input = vec![1, 2, 3, 4, 5];\n+        let result: Vec<_> = map_collect(&input, |&x| x * 2);\n+        assert_eq!(result, vec![2, 4, 6, 8, 10]);\n+    }\n+\n+    #[tokio::test(flavor = \"multi_thread\")]\n+    async fn test_parallel_map_collect_owned() {\n+        let input = vec![1, 2, 3, 4, 5];\n+        let result: Vec<_> = map_collect_owned(input, |x| x * 2);\n+        assert_eq!(result, vec![2, 4, 6, 8, 10]);\n+    }\n+\n+    #[tokio::test(flavor = \"multi_thread\")]\n+    async fn test_parallel_map_collect_owned_many() {\n+        let input = vec![1; 1000];\n+        let result: Vec<_> = map_collect_owned(input, |x| x * 2);\n+        assert_eq!(result, vec![2; 1000]);\n+    }\n+\n+    #[tokio::test(flavor = \"multi_thread\")]\n+    async fn test_panic_in_scope() {\n+        let result = catch_unwind(AssertUnwindSafe(|| {\n+            let mut input = vec![1; 1000];\n+            input[744] = 2;\n+            for_each(&input, |x| {\n+                if *x == 2 {\n+                    panic!(\"Intentional panic\");\n+                }\n+            });\n+            panic!(\"Should not get here\")\n+        }));\n+        assert!(result.is_err());\n+        assert_eq!(\n+            result.unwrap_err().downcast_ref::<&str>(),\n+            Some(&\"Intentional panic\")\n+        );\n+    }\n+}"
        },
        {
            "sha": "4c474b35f22a352a37155e9de9e9779123d6c7ed",
            "filename": "turbopack/crates/turbo-tasks/src/scope.rs",
            "status": "modified",
            "additions": 273,
            "deletions": 36,
            "changes": 309,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Fscope.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Fscope.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Fscope.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -1,52 +1,289 @@\n-use std::sync::Arc;\n+//! A scoped tokio spawn implementation that allow a non-'static lifetime for tasks.\n \n-use crate::{TurboTasksApi, turbo_tasks, turbo_tasks_scope};\n+use std::{\n+    any::Any,\n+    marker::PhantomData,\n+    panic::{self, AssertUnwindSafe, catch_unwind},\n+    pin::Pin,\n+    sync::{\n+        Arc,\n+        atomic::{AtomicUsize, Ordering},\n+    },\n+    thread::{self, Thread},\n+};\n \n-/// A wrapper around [`rayon::Scope`] that preserves the [`turbo_tasks_scope`].\n-pub struct Scope<'scope, 'a> {\n-    scope: &'a rayon::Scope<'scope>,\n-    handle: tokio::runtime::Handle,\n-    turbo_tasks: Arc<dyn TurboTasksApi>,\n-    span: tracing::Span,\n+use futures::FutureExt;\n+use parking_lot::Mutex;\n+use tokio::{runtime::Handle, task::block_in_place};\n+use tracing::{Instrument, Span, info_span};\n+\n+use crate::{\n+    TurboTasksApi,\n+    manager::{try_turbo_tasks, turbo_tasks_future_scope},\n+};\n+\n+struct ScopeInner {\n+    main_thread: Thread,\n+    remaining_tasks: AtomicUsize,\n+    /// The first panic that occurred in the tasks, by task index.\n+    /// The usize value is the index of the task.\n+    panic: Mutex<Option<(Box<dyn Any + Send + 'static>, usize)>>,\n }\n \n-impl<'scope> Scope<'scope, '_> {\n-    pub fn spawn<Body>(&self, body: Body)\n+impl ScopeInner {\n+    fn on_task_finished(&self, panic: Option<(Box<dyn Any + Send + 'static>, usize)>) {\n+        if let Some((err, index)) = panic {\n+            let mut old_panic = self.panic.lock();\n+            if old_panic.as_ref().is_none_or(|&(_, i)| i > index) {\n+                *old_panic = Some((err, index));\n+            }\n+        }\n+        if self.remaining_tasks.fetch_sub(1, Ordering::Release) == 1 {\n+            self.main_thread.unpark();\n+        }\n+    }\n+\n+    fn wait(&self) {\n+        let _span = info_span!(\"blocking\").entered();\n+        while self.remaining_tasks.load(Ordering::Acquire) != 0 {\n+            thread::park();\n+        }\n+        if let Some((err, _)) = self.panic.lock().take() {\n+            panic::resume_unwind(err);\n+        }\n+    }\n+}\n+\n+/// Scope to allow spawning tasks with a limited lifetime.\n+///\n+/// Dropping this Scope will wait for all tasks to complete.\n+pub struct Scope<'scope, 'env: 'scope, R: Send + 'env> {\n+    results: &'scope [Mutex<Option<R>>],\n+    index: AtomicUsize,\n+    inner: Arc<ScopeInner>,\n+    handle: Handle,\n+    turbo_tasks: Option<Arc<dyn TurboTasksApi>>,\n+    span: Span,\n+    /// Invariance over 'env, to make sure 'env cannot shrink,\n+    /// which is necessary for soundness.\n+    ///\n+    /// see https://doc.rust-lang.org/src/std/thread/scoped.rs.html#12-29\n+    env: PhantomData<&'env mut &'env ()>,\n+}\n+\n+impl<'scope, 'env: 'scope, R: Send + 'env> Scope<'scope, 'env, R> {\n+    /// Creates a new scope.\n+    ///\n+    /// # Safety\n+    ///\n+    /// The caller must ensure `Scope` is dropped and not forgotten.\n+    unsafe fn new(results: &'scope [Mutex<Option<R>>]) -> Self {\n+        Self {\n+            results,\n+            index: AtomicUsize::new(0),\n+            inner: Arc::new(ScopeInner {\n+                main_thread: thread::current(),\n+                remaining_tasks: AtomicUsize::new(0),\n+                panic: Mutex::new(None),\n+            }),\n+            handle: Handle::current(),\n+            turbo_tasks: try_turbo_tasks(),\n+            span: Span::current(),\n+            env: PhantomData,\n+        }\n+    }\n+\n+    /// Spawns a new task in the scope.\n+    pub fn spawn<F>(&self, f: F)\n     where\n-        Body: FnOnce(&Scope<'scope, '_>) + Send + 'scope,\n+        F: Future<Output = R> + Send + 'env,\n     {\n-        let span = self.span.clone();\n-        let handle = self.handle.clone();\n+        let index = self.index.fetch_add(1, Ordering::Relaxed);\n+        assert!(index < self.results.len(), \"Too many tasks spawned\");\n+        let result_cell: &Mutex<Option<R>> = &self.results[index];\n+\n+        let f: Box<dyn Future<Output = ()> + Send + 'scope> = Box::new(async move {\n+            let result = f.await;\n+            *result_cell.lock() = Some(result);\n+        });\n+        let f: *mut (dyn Future<Output = ()> + Send + 'scope) = Box::into_raw(f);\n+        // SAFETY: Scope ensures (e. g. in Drop) that spawned tasks is awaited before the\n+        // lifetime `'env` ends.\n+        #[allow(\n+            clippy::unnecessary_cast,\n+            reason = \"Clippy thinks this is unnecessary, but it actually changes the lifetime\"\n+        )]\n+        let f = f as *mut (dyn Future<Output = ()> + Send + 'static);\n+        // SAFETY: We just called `Box::into_raw`.\n+        let f = unsafe { Box::from_raw(f) };\n+        // We pin the future in the box in memory to be able to await it.\n+        let f = Pin::from(f);\n+\n         let turbo_tasks = self.turbo_tasks.clone();\n-        self.scope.spawn(|scope| {\n-            let _span = span.clone().entered();\n-            let _guard = handle.enter();\n-            turbo_tasks_scope(turbo_tasks.clone(), || {\n-                body(&Scope {\n-                    scope,\n-                    span,\n-                    handle,\n-                    turbo_tasks,\n-                })\n-            })\n+        let span = self.span.clone();\n+\n+        let inner = self.inner.clone();\n+        inner.remaining_tasks.fetch_add(1, Ordering::Relaxed);\n+        self.handle.spawn(async move {\n+            let result = AssertUnwindSafe(\n+                async move {\n+                    if let Some(turbo_tasks) = turbo_tasks {\n+                        // Ensure that the turbo tasks context is maintained across the task.\n+                        turbo_tasks_future_scope(turbo_tasks, f).await;\n+                    } else {\n+                        // If no turbo tasks context is available, just run the future.\n+                        f.await;\n+                    }\n+                }\n+                .instrument(span),\n+            )\n+            .catch_unwind()\n+            .await;\n+            let panic = result.err().map(|e| (e, index));\n+            inner.on_task_finished(panic);\n         });\n     }\n }\n \n-/// A wrapper around [`rayon::in_place_scope`] that preserves the [`turbo_tasks_scope`].\n-pub fn scope<'scope, Op, R>(op: Op) -> R\n+impl<'scope, 'env: 'scope, R: Send + 'env> Drop for Scope<'scope, 'env, R> {\n+    fn drop(&mut self) {\n+        self.inner.wait();\n+    }\n+}\n+\n+/// Helper method to spawn tasks in parallel, ensuring that all tasks are awaited and errors are\n+/// handled. Also ensures turbo tasks and tracing context are maintained across the tasks.\n+///\n+/// Be aware that although this function avoids starving other independently spawned tasks, any\n+/// other code running concurrently in the same task will be suspended during the call to\n+/// block_in_place. This can happen e.g. when using the `join!` macro. To avoid this issue, call\n+/// `scope_and_block` in `spawn_blocking`.\n+pub fn scope_and_block<'env, F, R>(number_of_tasks: usize, f: F) -> impl Iterator<Item = R>\n where\n-    Op: FnOnce(&Scope<'scope, '_>) -> R,\n+    R: Send + 'env,\n+    F: for<'scope> FnOnce(&'scope Scope<'scope, 'env, R>) + 'env,\n {\n-    let span = tracing::Span::current();\n-    let handle = tokio::runtime::Handle::current();\n-    let turbo_tasks = turbo_tasks();\n-    rayon::in_place_scope(|scope| {\n-        op(&Scope {\n-            scope,\n-            span,\n-            handle,\n-            turbo_tasks,\n+    block_in_place(|| {\n+        let mut results = Vec::with_capacity(number_of_tasks);\n+        for _ in 0..number_of_tasks {\n+            results.push(Mutex::new(None));\n+        }\n+        let results = results.into_boxed_slice();\n+        let result = {\n+            // SAFETY: We drop the Scope later.\n+            let scope = unsafe { Scope::new(&results) };\n+            catch_unwind(AssertUnwindSafe(|| f(&scope)))\n+        };\n+        if let Err(panic) = result {\n+            panic::resume_unwind(panic);\n+        }\n+        results.into_iter().map(|mutex| {\n+            mutex\n+                .into_inner()\n+                .expect(\"All values are set when the scope returns without panic\")\n         })\n     })\n }\n+\n+#[cfg(test)]\n+mod tests {\n+    use std::panic::{AssertUnwindSafe, catch_unwind};\n+\n+    use super::*;\n+\n+    #[tokio::test(flavor = \"multi_thread\")]\n+    async fn test_scope() {\n+        let results = scope_and_block(1000, |scope| {\n+            for i in 0..1000 {\n+                scope.spawn(async move { i });\n+            }\n+        });\n+        results.enumerate().for_each(|(i, result)| {\n+            assert_eq!(result, i);\n+        });\n+    }\n+\n+    #[tokio::test(flavor = \"multi_thread\")]\n+    async fn test_empty_scope() {\n+        let results = scope_and_block(0, |scope| {\n+            if false {\n+                scope.spawn(async move { 42 });\n+            }\n+        });\n+        assert_eq!(results.count(), 0);\n+    }\n+\n+    #[tokio::test(flavor = \"multi_thread\")]\n+    async fn test_single_task() {\n+        let results = scope_and_block(1, |scope| {\n+            scope.spawn(async move { 42 });\n+        })\n+        .collect::<Vec<_>>();\n+        assert_eq!(results, vec![42]);\n+    }\n+\n+    #[tokio::test(flavor = \"multi_thread\")]\n+    async fn test_task_finish_before_scope() {\n+        let results = scope_and_block(1, |scope| {\n+            scope.spawn(async move { 42 });\n+            thread::sleep(std::time::Duration::from_millis(100));\n+        })\n+        .collect::<Vec<_>>();\n+        assert_eq!(results, vec![42]);\n+    }\n+\n+    #[tokio::test(flavor = \"multi_thread\")]\n+    async fn test_task_finish_after_scope() {\n+        let results = scope_and_block(1, |scope| {\n+            scope.spawn(async move {\n+                thread::sleep(std::time::Duration::from_millis(100));\n+                42\n+            });\n+        })\n+        .collect::<Vec<_>>();\n+        assert_eq!(results, vec![42]);\n+    }\n+\n+    #[tokio::test(flavor = \"multi_thread\")]\n+    async fn test_panic_in_scope_factory() {\n+        let result = catch_unwind(AssertUnwindSafe(|| {\n+            let _results = scope_and_block(1000, |scope| {\n+                for i in 0..500 {\n+                    scope.spawn(async move { i });\n+                }\n+                panic!(\"Intentional panic\");\n+            });\n+            unreachable!();\n+        }));\n+        assert!(result.is_err());\n+        assert_eq!(\n+            result.unwrap_err().downcast_ref::<&str>(),\n+            Some(&\"Intentional panic\")\n+        );\n+    }\n+\n+    #[tokio::test(flavor = \"multi_thread\")]\n+    async fn test_panic_in_scope_task() {\n+        let result = catch_unwind(AssertUnwindSafe(|| {\n+            let _results = scope_and_block(1000, |scope| {\n+                for i in 0..1000 {\n+                    scope.spawn(async move {\n+                        if i == 500 {\n+                            panic!(\"Intentional panic\");\n+                        } else if i == 501 {\n+                            panic!(\"Wrong intentional panic\");\n+                        } else {\n+                            i\n+                        }\n+                    });\n+                }\n+            });\n+            unreachable!();\n+        }));\n+        assert!(result.is_err());\n+        assert_eq!(\n+            result.unwrap_err().downcast_ref::<&str>(),\n+            Some(&\"Intentional panic\")\n+        );\n+    }\n+}"
        },
        {
            "sha": "ecc57de367173f26cf0f65ebfa5d7a0f2bb902ce",
            "filename": "turbopack/crates/turbo-tasks/src/util.rs",
            "status": "modified",
            "additions": 123,
            "deletions": 0,
            "changes": 123,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Futil.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Futil.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Futil.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -1,8 +1,10 @@\n use std::{\n+    cell::SyncUnsafeCell,\n     error::Error as StdError,\n     fmt::{Debug, Display},\n     future::Future,\n     hash::{Hash, Hasher},\n+    mem::ManuallyDrop,\n     ops::Deref,\n     pin::Pin,\n     sync::Arc,\n@@ -259,3 +261,124 @@ impl<F: Future, W: for<'a> Fn(Pin<&mut F>, &mut Context<'a>) -> Poll<F::Output>>\n         (this.wrapper)(this.future, cx)\n     }\n }\n+\n+/// Similar to slice::chunks but for owned data. Chunks are Send and Sync to allow to use it for\n+/// parallelism.\n+pub fn into_chunks<T>(data: Vec<T>, chunk_size: usize) -> IntoChunks<T> {\n+    let (ptr, length, capacity) = data.into_raw_parts();\n+    // SAFETY: changing a pointer from T to SyncUnsafeCell<ManuallyDrop<..>> is safe as both types\n+    // have repr(transparent).\n+    let ptr = ptr as *mut SyncUnsafeCell<ManuallyDrop<T>>;\n+    // SAFETY: The ptr, length and capacity were from into_raw_parts(). This is the only place where\n+    // we use ptr.\n+    let data =\n+        unsafe { Vec::<SyncUnsafeCell<ManuallyDrop<T>>>::from_raw_parts(ptr, length, capacity) };\n+    IntoChunks {\n+        data: Arc::new(data),\n+        index: 0,\n+        chunk_size,\n+    }\n+}\n+\n+pub struct IntoChunks<T> {\n+    data: Arc<Vec<SyncUnsafeCell<ManuallyDrop<T>>>>,\n+    index: usize,\n+    chunk_size: usize,\n+}\n+\n+impl<T> Iterator for IntoChunks<T> {\n+    type Item = Chunk<T>;\n+\n+    fn next(&mut self) -> Option<Self::Item> {\n+        if self.index < self.data.len() {\n+            let end = self.data.len().min(self.index + self.chunk_size);\n+            let item = Chunk {\n+                data: Arc::clone(&self.data),\n+                index: self.index,\n+                end,\n+            };\n+            self.index = end;\n+            Some(item)\n+        } else {\n+            None\n+        }\n+    }\n+}\n+\n+impl<T> IntoChunks<T> {\n+    fn next_item(&mut self) -> Option<T> {\n+        if self.index < self.data.len() {\n+            // SAFETY: We are the only owner of this chunk of data and we make sure that this item\n+            // is no longer dropped by moving the index\n+            let item = unsafe { ManuallyDrop::take(&mut *self.data[self.index].get()) };\n+            self.index += 1;\n+            Some(item)\n+        } else {\n+            None\n+        }\n+    }\n+}\n+\n+impl<T> Drop for IntoChunks<T> {\n+    fn drop(&mut self) {\n+        // To avoid leaking memory we need to drop the remaining items\n+        while self.next_item().is_some() {}\n+    }\n+}\n+\n+pub struct Chunk<T> {\n+    data: Arc<Vec<SyncUnsafeCell<ManuallyDrop<T>>>>,\n+    index: usize,\n+    end: usize,\n+}\n+\n+impl<T> Iterator for Chunk<T> {\n+    type Item = T;\n+\n+    fn next(&mut self) -> Option<Self::Item> {\n+        if self.index < self.end {\n+            // SAFETY: We are the only owner of this chunk of data and we make sure that this item\n+            // is no longer dropped by moving the index\n+            let item = unsafe { ManuallyDrop::take(&mut *self.data[self.index].get()) };\n+            self.index += 1;\n+            Some(item)\n+        } else {\n+            None\n+        }\n+    }\n+}\n+\n+impl<T> Drop for Chunk<T> {\n+    fn drop(&mut self) {\n+        // To avoid leaking memory we need to drop the remaining items\n+        while self.next().is_some() {}\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use super::*;\n+\n+    #[test]\n+    fn test_chunk_iterator() {\n+        let data = [(); 10]\n+            .into_iter()\n+            .enumerate()\n+            .map(|(i, _)| Arc::new(i))\n+            .collect::<Vec<_>>();\n+        let mut chunks = into_chunks(data.clone(), 3);\n+        let mut first_chunk = chunks.next().unwrap();\n+        let second_chunk = chunks.next().unwrap();\n+        drop(chunks);\n+        assert_eq!(\n+            second_chunk.into_iter().map(|a| *a).collect::<Vec<_>>(),\n+            vec![3, 4, 5]\n+        );\n+        assert_eq!(*first_chunk.next().unwrap(), 0);\n+        assert_eq!(*first_chunk.next().unwrap(), 1);\n+        drop(first_chunk);\n+        for arc in data {\n+            assert_eq!(Arc::strong_count(&arc), 1);\n+        }\n+    }\n+}"
        },
        {
            "sha": "8f4b0aa3d00be02b4e51b7428603f9af34d6792d",
            "filename": "turbopack/crates/turbopack/tests/node-file-trace.rs",
            "status": "modified",
            "additions": 14,
            "deletions": 39,
            "changes": 53,
            "blob_url": "https://github.com/vercel/next.js/blob/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbopack%2Ftests%2Fnode-file-trace.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/823b9f50cd21214260cc03890d20b81364fc22ce/turbopack%2Fcrates%2Fturbopack%2Ftests%2Fnode-file-trace.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbopack%2Ftests%2Fnode-file-trace.rs?ref=823b9f50cd21214260cc03890d20b81364fc22ce",
            "patch": "@@ -272,7 +272,7 @@ fn test_cases() {}\n \n #[apply(test_cases)]\n fn node_file_trace_noop_backing_storage(#[case] input: CaseInput) {\n-    node_file_trace(input, \"noop_backing_storage\", false, 1, 120, |_| {\n+    node_file_trace(input, \"noop_backing_storage\", 1, 120, |_| {\n         TurboTasks::new(TurboTasksBackend::new(\n             turbo_tasks_backend::BackendOptions::default(),\n             turbo_tasks_backend::noop_backing_storage(),\n@@ -282,7 +282,7 @@ fn node_file_trace_noop_backing_storage(#[case] input: CaseInput) {\n \n #[apply(test_cases)]\n fn node_file_trace_persistent(#[case] input: CaseInput) {\n-    node_file_trace(input, \"persistent_cache\", false, 2, 240, |directory_path| {\n+    node_file_trace(input, \"persistent_cache\", 2, 240, |directory_path| {\n         TurboTasks::new(TurboTasksBackend::new(\n             turbo_tasks_backend::BackendOptions::default(),\n             turbo_tasks_backend::default_backing_storage(\n@@ -302,31 +302,18 @@ fn node_file_trace_persistent(#[case] input: CaseInput) {\n \n #[cfg(feature = \"bench_against_node_nft\")]\n #[apply(test_cases)]\n-fn bench_against_node_nft_st(#[case] input: CaseInput) {\n-    bench_against_node_nft_inner(input, false);\n+fn bench_against_node_nft(#[case] input: CaseInput) {\n+    bench_against_node_nft_inner(input);\n }\n \n #[cfg(feature = \"bench_against_node_nft\")]\n-#[apply(test_cases)]\n-fn bench_against_node_nft_mt(#[case] input: CaseInput) {\n-    bench_against_node_nft_inner(input, true);\n-}\n-\n-#[cfg(feature = \"bench_against_node_nft\")]\n-fn bench_against_node_nft_inner(input: CaseInput, multi_threaded: bool) {\n-    node_file_trace(\n-        input,\n-        \"noop_backing_storage\",\n-        multi_threaded,\n-        1,\n-        120,\n-        |_| {\n-            TurboTasks::new(TurboTasksBackend::new(\n-                turbo_tasks_backend::BackendOptions::default(),\n-                turbo_tasks_backend::noop_backing_storage(),\n-            ))\n-        },\n-    );\n+fn bench_against_node_nft_inner(input: CaseInput) {\n+    node_file_trace(input, \"noop_backing_storage\", 1, 120, |_| {\n+        TurboTasks::new(TurboTasksBackend::new(\n+            turbo_tasks_backend::BackendOptions::default(),\n+            turbo_tasks_backend::noop_backing_storage(),\n+        ))\n+    });\n }\n \n #[turbo_tasks::function(operation)]\n@@ -401,7 +388,6 @@ fn node_file_trace<B: Backend + 'static>(\n         expected_stderr,\n     }: CaseInput,\n     mode: &str,\n-    multi_threaded: bool,\n     run_count: i32,\n     timeout_len: u64,\n     create_turbo_tasks: impl Fn(&Path) -> Arc<TurboTasks<B>>,\n@@ -410,15 +396,9 @@ fn node_file_trace<B: Backend + 'static>(\n         LazyLock::new(|| Arc::new(Mutex::new(Vec::new())));\n \n     let r = &mut {\n-        let mut builder = if multi_threaded {\n-            tokio::runtime::Builder::new_multi_thread()\n-        } else {\n-            tokio::runtime::Builder::new_current_thread()\n-        };\n+        let mut builder = tokio::runtime::Builder::new_multi_thread();\n         builder.enable_all();\n-        if !multi_threaded {\n-            builder.max_blocking_threads(20);\n-        }\n+        builder.max_blocking_threads(20);\n         builder.build().unwrap()\n     };\n     r.block_on(async move {\n@@ -490,12 +470,7 @@ fn node_file_trace<B: Backend + 'static>(\n                         bench_suites_lock.push(BenchSuite {\n                             suite: input\n                                 .trim_start_matches(\"node-file-trace/integration/\")\n-                                .to_string()\n-                                + (if multi_threaded {\n-                                    \" (multi-threaded)\"\n-                                } else {\n-                                    \"\"\n-                                }),\n+                                .to_string(),\n                             is_faster,\n                             rust_duration,\n                             node_duration,"
        }
    ],
    "stats": {
        "total": 2689,
        "additions": 1851,
        "deletions": 838
    }
}