{
    "author": "wyattjoh",
    "message": "perf(router): replace LRU cache with optimized doubly-linked list implementation (#82633)\n\n### What?\n\nReplace LRU cache implementation with an optimized doubly-linked list\nalgorithm and move from server-only to shared library location.\n\n### Why?\n\nThe previous Map-based LRU cache had suboptimal eviction performance for\nroute matching operations under high load. Route matching is a critical\nperformance path in Next.js that benefits significantly from true O(1)\ncache operations.\n\n### How?\n\n- Implement doubly-linked list with sentinel nodes for true O(1)\nget/set/delete operations\n- Add comprehensive test coverage including size-based eviction\nscenarios\n- Update import paths across 14 files throughout the codebase\n- Add defensive null checking for edge cases discovered during static\nanalysis\n- Replace deprecated `keys()` iteration with modern `Symbol.iterator`\npattern",
    "sha": "8c0737d76db690aeb79113f46a81917ffa1ebf28",
    "files": [
        {
            "sha": "d9dc98eb0beb12330a3ab51cc4170d3bec4589f6",
            "filename": "packages/next/errors.json",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/vercel/next.js/blob/8c0737d76db690aeb79113f46a81917ffa1ebf28/packages%2Fnext%2Ferrors.json",
            "raw_url": "https://github.com/vercel/next.js/raw/8c0737d76db690aeb79113f46a81917ffa1ebf28/packages%2Fnext%2Ferrors.json",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/packages%2Fnext%2Ferrors.json?ref=8c0737d76db690aeb79113f46a81917ffa1ebf28",
            "patch": "@@ -778,5 +778,6 @@\n   \"777\": \"Invariant: failed to find source route %s for prerender %s\",\n   \"778\": \"`prerenderAndAbortInSequentialTasksWithStages` should not be called in edge runtime.\",\n   \"779\": \"Route %s used \\\"searchParams\\\" inside \\\"use cache\\\". Accessing dynamic request data inside a cache scope is not supported. If you need some search params inside a cached function await \\\"searchParams\\\" outside of the cached function and pass only the required search params as arguments to the cached function. See more info here: https://nextjs.org/docs/messages/next-request-in-use-cache\",\n-  \"780\": \"Invariant: failed to find parent dynamic route for notFound route %s\"\n+  \"780\": \"Invariant: failed to find parent dynamic route for notFound route %s\",\n+  \"781\": \"No LRU node to remove\"\n }"
        },
        {
            "sha": "7d77129ec0f57c6e46e44861b89a1c7fb27f0190",
            "filename": "packages/next/src/build/analysis/get-page-static-info.ts",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/vercel/next.js/blob/8c0737d76db690aeb79113f46a81917ffa1ebf28/packages%2Fnext%2Fsrc%2Fbuild%2Fanalysis%2Fget-page-static-info.ts",
            "raw_url": "https://github.com/vercel/next.js/raw/8c0737d76db690aeb79113f46a81917ffa1ebf28/packages%2Fnext%2Fsrc%2Fbuild%2Fanalysis%2Fget-page-static-info.ts",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/packages%2Fnext%2Fsrc%2Fbuild%2Fanalysis%2Fget-page-static-info.ts?ref=8c0737d76db690aeb79113f46a81917ffa1ebf28",
            "patch": "@@ -416,7 +416,7 @@ function warnAboutExperimentalEdge(apiRoute: string | null) {\n     return\n   }\n \n-  if (apiRouteWarnings.has(apiRoute)) {\n+  if (apiRoute && apiRouteWarnings.has(apiRoute)) {\n     return\n   }\n \n@@ -425,7 +425,10 @@ function warnAboutExperimentalEdge(apiRoute: string | null) {\n       ? `${apiRoute} provided runtime 'experimental-edge'. It can be updated to 'edge' instead.`\n       : `You are using an experimental edge runtime, the API might change.`\n   )\n-  apiRouteWarnings.set(apiRoute, 1)\n+\n+  if (apiRoute) {\n+    apiRouteWarnings.set(apiRoute, 1)\n+  }\n }\n \n let hadUnsupportedValue = false"
        },
        {
            "sha": "552603a5c00d5cdaabd3ad3ca9d92b71f9423d55",
            "filename": "packages/next/src/server/lib/dev-bundler-service.ts",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/vercel/next.js/blob/8c0737d76db690aeb79113f46a81917ffa1ebf28/packages%2Fnext%2Fsrc%2Fserver%2Flib%2Fdev-bundler-service.ts",
            "raw_url": "https://github.com/vercel/next.js/raw/8c0737d76db690aeb79113f46a81917ffa1ebf28/packages%2Fnext%2Fsrc%2Fserver%2Flib%2Fdev-bundler-service.ts",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/packages%2Fnext%2Fsrc%2Fserver%2Flib%2Fdev-bundler-service.ts?ref=8c0737d76db690aeb79113f46a81917ffa1ebf28",
            "patch": "@@ -11,7 +11,7 @@ import { HMR_ACTIONS_SENT_TO_BROWSER } from '../dev/hot-reloader-types'\n  * bundler while in development.\n  */\n export class DevBundlerService {\n-  public appIsrManifestInner: InstanceType<typeof LRUCache>\n+  public appIsrManifestInner: InstanceType<typeof LRUCache<boolean>>\n \n   constructor(\n     private readonly bundler: DevBundler,\n@@ -87,9 +87,10 @@ export class DevBundlerService {\n   public get appIsrManifest() {\n     const serializableManifest: Record<string, boolean> = {}\n \n-    for (const key of this.appIsrManifestInner.keys() as string[]) {\n-      serializableManifest[key] = this.appIsrManifestInner.get(key) as boolean\n+    for (const [key, value] of this.appIsrManifestInner) {\n+      serializableManifest[key] = value\n     }\n+\n     return serializableManifest\n   }\n "
        },
        {
            "sha": "3be335736925019df7ad2cebd459ebedab024f16",
            "filename": "packages/next/src/server/lib/lru-cache.test.ts",
            "status": "added",
            "additions": 229,
            "deletions": 0,
            "changes": 229,
            "blob_url": "https://github.com/vercel/next.js/blob/8c0737d76db690aeb79113f46a81917ffa1ebf28/packages%2Fnext%2Fsrc%2Fserver%2Flib%2Flru-cache.test.ts",
            "raw_url": "https://github.com/vercel/next.js/raw/8c0737d76db690aeb79113f46a81917ffa1ebf28/packages%2Fnext%2Fsrc%2Fserver%2Flib%2Flru-cache.test.ts",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/packages%2Fnext%2Fsrc%2Fserver%2Flib%2Flru-cache.test.ts?ref=8c0737d76db690aeb79113f46a81917ffa1ebf28",
            "patch": "@@ -0,0 +1,229 @@\n+import { LRUCache } from './lru-cache'\n+\n+describe('LRUCache', () => {\n+  describe('Basic Operations', () => {\n+    let cache: LRUCache<string>\n+\n+    beforeEach(() => {\n+      cache = new LRUCache<string>(3)\n+    })\n+\n+    it('should set and get values', () => {\n+      cache.set('key1', 'value1')\n+      expect(cache.get('key1')).toBe('value1')\n+    })\n+\n+    it('should return undefined for non-existent keys', () => {\n+      expect(cache.get('nonexistent')).toBeUndefined()\n+    })\n+\n+    it('should check if key exists with has()', () => {\n+      cache.set('key1', 'value1')\n+      expect(cache.has('key1')).toBe(true)\n+      expect(cache.has('nonexistent')).toBe(false)\n+    })\n+\n+    it('should update existing keys', () => {\n+      cache.set('key1', 'value1')\n+      cache.set('key1', 'value2')\n+      expect(cache.get('key1')).toBe('value2')\n+      expect(cache.size).toBe(1)\n+    })\n+\n+    it('should track cache size correctly', () => {\n+      expect(cache.size).toBe(0)\n+      cache.set('key1', 'value1')\n+      expect(cache.size).toBe(1)\n+      cache.set('key2', 'value2')\n+      expect(cache.size).toBe(2)\n+    })\n+  })\n+\n+  describe('LRU Eviction Behavior', () => {\n+    let cache: LRUCache<string>\n+\n+    beforeEach(() => {\n+      cache = new LRUCache<string>(3)\n+    })\n+\n+    it('should evict least recently used item when capacity exceeded', () => {\n+      cache.set('key1', 'value1')\n+      cache.set('key2', 'value2')\n+      cache.set('key3', 'value3')\n+      cache.set('key4', 'value4') // should evict key1\n+\n+      expect(cache.has('key1')).toBe(false)\n+      expect(cache.has('key2')).toBe(true)\n+      expect(cache.has('key3')).toBe(true)\n+      expect(cache.has('key4')).toBe(true)\n+      expect(cache.size).toBe(3)\n+    })\n+\n+    it('should update LRU order when accessing items', () => {\n+      cache.set('key1', 'value1')\n+      cache.set('key2', 'value2')\n+      cache.set('key3', 'value3')\n+\n+      cache.get('key1') // key1 becomes most recently used\n+      cache.set('key4', 'value4') // should evict key2 (least recently used)\n+\n+      expect(cache.has('key1')).toBe(true)\n+      expect(cache.has('key2')).toBe(false)\n+      expect(cache.has('key3')).toBe(true)\n+      expect(cache.has('key4')).toBe(true)\n+    })\n+\n+    it('should maintain correct order with mixed operations', () => {\n+      cache.set('a', '1')\n+      cache.set('b', '2')\n+      cache.set('c', '3')\n+\n+      cache.get('a') // a becomes most recent\n+      cache.get('b') // b becomes most recent\n+      cache.set('d', '4') // should evict c\n+\n+      expect(cache.has('a')).toBe(true)\n+      expect(cache.has('b')).toBe(true)\n+      expect(cache.has('c')).toBe(false)\n+      expect(cache.has('d')).toBe(true)\n+    })\n+  })\n+\n+  describe('Size-based Eviction', () => {\n+    it('should use custom size calculation', () => {\n+      const cache = new LRUCache<string>(10, (value) => value.length)\n+\n+      cache.set('key1', 'abc') // size 3\n+      cache.set('key2', 'defgh') // size 5\n+      cache.set('key3', 'ij') // size 2, total = 10\n+      cache.set('key4', 'k') // size 1, total = 11, should evict key1\n+\n+      expect(cache.has('key1')).toBe(false)\n+      expect(cache.has('key2')).toBe(true)\n+      expect(cache.has('key3')).toBe(true)\n+      expect(cache.has('key4')).toBe(true)\n+      expect(cache.currentSize).toBe(8) // 5 + 2 + 1\n+    })\n+\n+    it('should handle items larger than max size', () => {\n+      const consoleSpy = jest.spyOn(console, 'warn').mockImplementation()\n+      const cache = new LRUCache<string>(5, (value) => value.length)\n+\n+      cache.set('key1', 'toolarge') // size 8 > maxSize 5\n+\n+      expect(cache.has('key1')).toBe(false)\n+      expect(cache.size).toBe(0)\n+      expect(consoleSpy).toHaveBeenCalledWith(\n+        'Single item size exceeds maxSize'\n+      )\n+\n+      consoleSpy.mockRestore()\n+    })\n+\n+    it('should update size when overwriting existing keys', () => {\n+      const cache = new LRUCache<string>(10, (value) => value.length)\n+\n+      cache.set('key1', 'abc') // size 3\n+      expect(cache.currentSize).toBe(3)\n+\n+      cache.set('key1', 'defghij') // size 7\n+      expect(cache.currentSize).toBe(7)\n+      expect(cache.size).toBe(1)\n+    })\n+\n+    it('should evict multiple items if necessary', () => {\n+      const cache = new LRUCache<string>(10, (value) => value.length)\n+\n+      cache.set('key1', 'ab') // size 2\n+      cache.set('key2', 'cd') // size 2\n+      cache.set('key3', 'ef') // size 2, total = 6\n+      cache.set('key4', 'ghijklmno') // size 9, should evict key1, key2, key3\n+\n+      expect(cache.has('key1')).toBe(false)\n+      expect(cache.has('key2')).toBe(false)\n+      expect(cache.has('key3')).toBe(false)\n+      expect(cache.has('key4')).toBe(true)\n+      expect(cache.currentSize).toBe(9)\n+      expect(cache.size).toBe(1)\n+    })\n+  })\n+\n+  describe('Cache Management', () => {\n+    let cache: LRUCache<string>\n+\n+    beforeEach(() => {\n+      cache = new LRUCache<string>(3)\n+      cache.set('key1', 'value1')\n+      cache.set('key2', 'value2')\n+    })\n+\n+    it('should remove specific keys', () => {\n+      cache.remove('key1')\n+      expect(cache.has('key1')).toBe(false)\n+      expect(cache.has('key2')).toBe(true)\n+      expect(cache.size).toBe(1)\n+    })\n+\n+    it('should handle removing non-existent keys', () => {\n+      cache.remove('nonexistent')\n+      expect(cache.size).toBe(2)\n+    })\n+\n+    it('should track current size correctly after operations', () => {\n+      const sizeCache = new LRUCache<string>(10, (value) => value.length)\n+\n+      sizeCache.set('key1', 'abc') // size 3\n+      sizeCache.set('key2', 'de') // size 2\n+      expect(sizeCache.currentSize).toBe(5)\n+\n+      sizeCache.remove('key1')\n+      expect(sizeCache.currentSize).toBe(2)\n+    })\n+  })\n+\n+  describe('Edge Cases', () => {\n+    it('should handle zero max size', () => {\n+      const cache = new LRUCache<string>(0)\n+      cache.set('key1', 'value1')\n+      expect(cache.has('key1')).toBe(false)\n+      expect(cache.size).toBe(0)\n+    })\n+\n+    it('should handle single item capacity', () => {\n+      const cache = new LRUCache<string>(1)\n+\n+      cache.set('key1', 'value1')\n+      expect(cache.has('key1')).toBe(true)\n+\n+      cache.set('key2', 'value2')\n+      expect(cache.has('key1')).toBe(false)\n+      expect(cache.has('key2')).toBe(true)\n+      expect(cache.size).toBe(1)\n+    })\n+\n+    it('should work with different value types', () => {\n+      const numberCache = new LRUCache<number>(2)\n+      const objectCache = new LRUCache<{ id: number }>(2)\n+\n+      numberCache.set('num', 42)\n+      expect(numberCache.get('num')).toBe(42)\n+\n+      const obj = { id: 1 }\n+      objectCache.set('obj', obj)\n+      expect(objectCache.get('obj')).toBe(obj)\n+    })\n+\n+    it('should maintain integrity with rapid operations', () => {\n+      const cache = new LRUCache<number>(100)\n+\n+      // Rapid insertions\n+      for (let i = 0; i < 150; i++) {\n+        cache.set(`key${i}`, i)\n+      }\n+\n+      expect(cache.size).toBe(100)\n+      expect(cache.has('key0')).toBe(false) // early keys evicted\n+      expect(cache.has('key149')).toBe(true) // recent keys retained\n+    })\n+  })\n+})"
        },
        {
            "sha": "663bc71ab4264b2def3fa4114ecda45f0186efe7",
            "filename": "packages/next/src/server/lib/lru-cache.ts",
            "status": "modified",
            "additions": 183,
            "deletions": 76,
            "changes": 259,
            "blob_url": "https://github.com/vercel/next.js/blob/8c0737d76db690aeb79113f46a81917ffa1ebf28/packages%2Fnext%2Fsrc%2Fserver%2Flib%2Flru-cache.ts",
            "raw_url": "https://github.com/vercel/next.js/raw/8c0737d76db690aeb79113f46a81917ffa1ebf28/packages%2Fnext%2Fsrc%2Fserver%2Flib%2Flru-cache.ts",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/packages%2Fnext%2Fsrc%2Fserver%2Flib%2Flru-cache.ts?ref=8c0737d76db690aeb79113f46a81917ffa1ebf28",
            "patch": "@@ -1,112 +1,219 @@\n+/**\n+ * Node in the doubly-linked list used for LRU tracking.\n+ * Each node represents a cache entry with bidirectional pointers.\n+ */\n+class LRUNode<T> {\n+  public readonly key: string\n+  public data: T\n+  public size: number\n+  public prev: LRUNode<T> | SentinelNode<T> | null = null\n+  public next: LRUNode<T> | SentinelNode<T> | null = null\n+\n+  constructor(key: string, data: T, size: number) {\n+    this.key = key\n+    this.data = data\n+    this.size = size\n+  }\n+}\n+\n+/**\n+ * Sentinel node used for head/tail boundaries.\n+ * These nodes don't contain actual cache data but simplify list operations.\n+ */\n+class SentinelNode<T> {\n+  public prev: LRUNode<T> | SentinelNode<T> | null = null\n+  public next: LRUNode<T> | SentinelNode<T> | null = null\n+}\n+\n+/**\n+ * LRU (Least Recently Used) Cache implementation using a doubly-linked list\n+ * and hash map for O(1) operations.\n+ *\n+ * Algorithm:\n+ * - Uses a doubly-linked list to maintain access order (most recent at head)\n+ * - Hash map provides O(1) key-to-node lookup\n+ * - Sentinel head/tail nodes simplify edge case handling\n+ * - Size-based eviction supports custom size calculation functions\n+ *\n+ * Data Structure Layout:\n+ * HEAD <-> [most recent] <-> ... <-> [least recent] <-> TAIL\n+ *\n+ * Operations:\n+ * - get(): Move accessed node to head (mark as most recent)\n+ * - set(): Add new node at head, evict from tail if over capacity\n+ * - Eviction: Remove least recent node (tail.prev) when size exceeds limit\n+ */\n export class LRUCache<T> {\n-  private cache: Map<string, T>\n-  private sizes: Map<string, number>\n-  private totalSize: number\n-  private maxSize: number\n-  private calculateSize: (value: T) => number\n+  private readonly cache: Map<string, LRUNode<T>> = new Map()\n+  private readonly head: SentinelNode<T>\n+  private readonly tail: SentinelNode<T>\n+  private totalSize: number = 0\n+  private readonly maxSize: number\n+  private readonly calculateSize: ((value: T) => number) | undefined\n \n   constructor(maxSize: number, calculateSize?: (value: T) => number) {\n-    this.cache = new Map()\n-    this.sizes = new Map()\n-    this.totalSize = 0\n     this.maxSize = maxSize\n-    this.calculateSize = calculateSize || (() => 1)\n+    this.calculateSize = calculateSize\n+\n+    // Create sentinel nodes to simplify doubly-linked list operations\n+    // HEAD <-> TAIL (empty list)\n+    this.head = new SentinelNode<T>()\n+    this.tail = new SentinelNode<T>()\n+    this.head.next = this.tail\n+    this.tail.prev = this.head\n   }\n \n-  set(key?: string | null, value?: T): void {\n-    if (!key || !value) return\n-\n-    const size = this.calculateSize(value)\n-\n-    if (size > this.maxSize) {\n-      console.warn('Single item size exceeds maxSize')\n-      return\n-    }\n-\n-    if (this.cache.has(key)) {\n-      this.totalSize -= this.sizes.get(key) || 0\n-    }\n-\n-    this.cache.set(key, value)\n-    this.sizes.set(key, size)\n-    this.totalSize += size\n-\n-    this.touch(key)\n+  /**\n+   * Adds a node immediately after the head (marks as most recently used).\n+   * Used when inserting new items or when an item is accessed.\n+   * PRECONDITION: node must be disconnected (prev/next should be null)\n+   */\n+  private addToHead(node: LRUNode<T>): void {\n+    node.prev = this.head\n+    node.next = this.head.next\n+    // head.next is always non-null (points to tail or another node)\n+    this.head.next!.prev = node\n+    this.head.next = node\n   }\n \n-  has(key?: string | null): boolean {\n-    if (!key) return false\n+  /**\n+   * Removes a node from its current position in the doubly-linked list.\n+   * Updates the prev/next pointers of adjacent nodes to maintain list integrity.\n+   * PRECONDITION: node must be connected (prev/next are non-null)\n+   */\n+  private removeNode(node: LRUNode<T>): void {\n+    // Connected nodes always have non-null prev/next\n+    node.prev!.next = node.next\n+    node.next!.prev = node.prev\n+  }\n \n-    this.touch(key)\n-    return Boolean(this.cache.get(key))\n+  /**\n+   * Moves an existing node to the head position (marks as most recently used).\n+   * This is the core LRU operation - accessed items become most recent.\n+   */\n+  private moveToHead(node: LRUNode<T>): void {\n+    this.removeNode(node)\n+    this.addToHead(node)\n   }\n \n-  get(key?: string | null): T | undefined {\n-    if (!key) return\n+  /**\n+   * Removes and returns the least recently used node (the one before tail).\n+   * This is called during eviction when the cache exceeds capacity.\n+   * PRECONDITION: cache is not empty (ensured by caller)\n+   */\n+  private removeTail(): LRUNode<T> {\n+    const lastNode = this.tail.prev as LRUNode<T>\n+    // tail.prev is always non-null and always LRUNode when cache is not empty\n+    this.removeNode(lastNode)\n+    return lastNode\n+  }\n \n-    const value = this.cache.get(key)\n-    if (value === undefined) {\n-      return undefined\n+  /**\n+   * Sets a key-value pair in the cache.\n+   * If the key exists, updates the value and moves to head.\n+   * If new, adds at head and evicts from tail if necessary.\n+   *\n+   * Time Complexity:\n+   * - O(1) for uniform item sizes\n+   * - O(k) where k is the number of items evicted (can be O(N) for variable sizes)\n+   */\n+  public set(key: string, value: T): void {\n+    const size = this.calculateSize?.(value) ?? 1\n+    if (size > this.maxSize) {\n+      console.warn('Single item size exceeds maxSize')\n+      return\n     }\n \n-    this.touch(key)\n-    return value\n-  }\n-\n-  private touch(key: string): void {\n-    const value = this.cache.get(key)\n-    if (value !== undefined) {\n-      this.cache.delete(key)\n-      this.cache.set(key, value)\n-      this.evictIfNecessary()\n+    const existing = this.cache.get(key)\n+    if (existing) {\n+      // Update existing node: adjust size and move to head (most recent)\n+      existing.data = value\n+      this.totalSize = this.totalSize - existing.size + size\n+      existing.size = size\n+      this.moveToHead(existing)\n+    } else {\n+      // Add new node at head (most recent position)\n+      const newNode = new LRUNode(key, value, size)\n+      this.cache.set(key, newNode)\n+      this.addToHead(newNode)\n+      this.totalSize += size\n     }\n-  }\n \n-  private evictIfNecessary(): void {\n+    // Evict least recently used items until under capacity\n     while (this.totalSize > this.maxSize && this.cache.size > 0) {\n-      this.evictLeastRecentlyUsed()\n+      const tail = this.removeTail()\n+      this.cache.delete(tail.key)\n+      this.totalSize -= tail.size\n     }\n   }\n \n-  private evictLeastRecentlyUsed(): void {\n-    const lruKey = this.cache.keys().next().value\n-    if (lruKey !== undefined) {\n-      const lruSize = this.sizes.get(lruKey) || 0\n-      this.totalSize -= lruSize\n-      this.cache.delete(lruKey)\n-      this.sizes.delete(lruKey)\n-    }\n+  /**\n+   * Checks if a key exists in the cache.\n+   * This is a pure query operation - does NOT update LRU order.\n+   *\n+   * Time Complexity: O(1)\n+   */\n+  public has(key: string): boolean {\n+    return this.cache.has(key)\n   }\n \n-  reset() {\n-    this.cache.clear()\n-    this.sizes.clear()\n-    this.totalSize = 0\n-  }\n+  /**\n+   * Retrieves a value by key and marks it as most recently used.\n+   * Moving to head maintains the LRU property for future evictions.\n+   *\n+   * Time Complexity: O(1)\n+   */\n+  public get(key: string): T | undefined {\n+    const node = this.cache.get(key)\n+    if (!node) return undefined\n+\n+    // Mark as most recently used by moving to head\n+    this.moveToHead(node)\n \n-  keys() {\n-    return [...this.cache.keys()]\n+    return node.data\n   }\n \n-  remove(key: string): void {\n-    if (this.cache.has(key)) {\n-      this.totalSize -= this.sizes.get(key) || 0\n-      this.cache.delete(key)\n-      this.sizes.delete(key)\n+  /**\n+   * Returns an iterator over the cache entries. The order is outputted in the\n+   * order of most recently used to least recently used.\n+   */\n+  public *[Symbol.iterator](): IterableIterator<[string, T]> {\n+    let current = this.head.next\n+    while (current && current !== this.tail) {\n+      // Between head and tail, current is always LRUNode\n+      const node = current as LRUNode<T>\n+      yield [node.key, node.data]\n+      current = current.next\n     }\n   }\n \n-  clear(): void {\n-    this.cache.clear()\n-    this.sizes.clear()\n-    this.totalSize = 0\n+  /**\n+   * Removes a specific key from the cache.\n+   * Updates both the hash map and doubly-linked list.\n+   *\n+   * Time Complexity: O(1)\n+   */\n+  public remove(key: string): void {\n+    const node = this.cache.get(key)\n+    if (!node) return\n+\n+    this.removeNode(node)\n+    this.cache.delete(key)\n+    this.totalSize -= node.size\n   }\n \n-  get size(): number {\n+  /**\n+   * Returns the number of items in the cache.\n+   */\n+  public get size(): number {\n     return this.cache.size\n   }\n \n-  get currentSize(): number {\n+  /**\n+   * Returns the current total size of all cached items.\n+   * This uses the custom size calculation if provided.\n+   */\n+  public get currentSize(): number {\n     return this.totalSize\n   }\n }"
        },
        {
            "sha": "38c43f70da8f264ab7624c0a62d4f486fc478a4e",
            "filename": "run-tests.js",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/vercel/next.js/blob/8c0737d76db690aeb79113f46a81917ffa1ebf28/run-tests.js",
            "raw_url": "https://github.com/vercel/next.js/raw/8c0737d76db690aeb79113f46a81917ffa1ebf28/run-tests.js",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/run-tests.js?ref=8c0737d76db690aeb79113f46a81917ffa1ebf28",
            "patch": "@@ -33,6 +33,8 @@ let argv = require('yargs/yargs')(process.argv.slice(2))\n   .number('c')\n   .boolean('related')\n   .boolean('dry')\n+  .boolean('print-tests')\n+  .describe('print-tests', 'Prints the test files that will be run')\n   .boolean('local')\n   .alias('r', 'related')\n   .alias('c', 'concurrency').argv\n@@ -74,15 +76,9 @@ const TIMINGS_API_HEADERS = {\n }\n \n const testFilters = {\n-  development: new RegExp(\n-    '^(test/(development|e2e)|packages/.*/src/.*|packages/next-codemod/.*)/.*\\\\.test\\\\.(js|jsx|ts|tsx)$'\n-  ),\n-  production: new RegExp(\n-    '^(test/(production|e2e))/.*\\\\.test\\\\.(js|jsx|ts|tsx)$'\n-  ),\n-  unit: new RegExp(\n-    '^test/unit|packages/.*/src/.*/.*\\\\.test\\\\.(js|jsx|ts|tsx)$'\n-  ),\n+  development: new RegExp('^(test/(development|e2e))'),\n+  production: new RegExp('^(test/(production|e2e))'),\n+  unit: new RegExp('^(test/unit|packages/.*/src|packages/next-codemod)'),\n   examples: 'examples/',\n   integration: 'test/integration/',\n   e2e: 'test/e2e/',\n@@ -217,6 +213,7 @@ async function main() {\n     retries: argv.retries ?? DEFAULT_NUM_RETRIES,\n     dry: argv.dry ?? false,\n     local: argv.local ?? false,\n+    printTests: argv.printTests ?? false,\n   }\n   let numRetries = options.retries\n   const hideOutput = !options.debug && !options.dry\n@@ -301,6 +298,8 @@ async function main() {\n         file,\n         excludedCases: [],\n       }))\n+\n+    //\n   }\n \n   if (options.timings && options.group) {\n@@ -406,6 +405,10 @@ ${tests.map((t) => t.file).join('\\n')}\n ${ENDGROUP}`)\n   console.log(`total: ${tests.length}`)\n \n+  if (options.printTests) {\n+    await cleanUpAndExit(0)\n+  }\n+\n   if (\n     !options.dry &&\n     process.env.NEXT_TEST_MODE !== 'deploy' &&"
        }
    ],
    "stats": {
        "total": 526,
        "additions": 435,
        "deletions": 91
    }
}