{
    "author": "feedthejim",
    "message": "perf: improve stats action reliability and reduce CI noise (#87945)\n\n## Summary\n\nComprehensive improvements to the PR stats action for more reliable benchmarks and reduced CI noise.\n\n### Vercel KV Integration\n- Add `@vercel/kv` for historical data persistence\n- Track metrics over time with `loadHistory()` / `saveToHistory()`\n- Display trend sparklines in comments when history is available\n\n### Comment Generation Refactor\n- New `METRIC_LABELS` and `METRIC_GROUPS` configuration system\n- Organized metrics by category (Dev Server, Production Builds, Production Runtime)\n- Better formatting utilities (`prettifyTime`, `formatChange`, `generateTrendBar`)\n- Bundle group totals for KV persistence\n\n### Noise Reduction\n- Significance thresholds: `50ms AND 10%` for time, `1KB AND 1%` for size\n- Additional `<2%` filter for long-running operations (builds)\n- Filters typical CI variance while catching real regressions\n\n### Stats Config Updates\n- Add `measureDevBoot: true` to enable dev boot benchmarks\n- Fix `appDevCommand` to include `dev` subcommand\n- Add `--webpack` flag to `appBuildCommand`\n- Rename \"Client Bundles (main, webpack)\" to \"Client Bundles (main)\"\n- Add `turbopack: {}` to test configs\n\n### GitHub Actions Updates\n- New `action.yml` with bundler input parameter\n- Workflow improvements in `pull_request_stats.yml`\n\n### New Files\n- `scripts/test-stats-benchmark.sh` - Local testing script\n- `.github/actions/next-stats-action/src/util/stats.js` - Shared stats utilities\n- `.github/actions/next-stats-action/test-local.js` - Local comment testing\n\n## Test Plan\n```bash\ncd .github/actions/next-stats-action\nnode test-local.js              # Basic test\nnode test-local.js --with-history  # Test with trend sparklines\n```",
    "sha": "4cd3ca26d71fc7904ea7030ee8355071efc9502d",
    "files": [
        {
            "sha": "724ccb611b96974824de9736a18208a0c3829983",
            "filename": ".github/actions/next-stats-action/action.yml",
            "status": "added",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/vercel/next.js/blob/4cd3ca26d71fc7904ea7030ee8355071efc9502d/.github%2Factions%2Fnext-stats-action%2Faction.yml",
            "raw_url": "https://github.com/vercel/next.js/raw/4cd3ca26d71fc7904ea7030ee8355071efc9502d/.github%2Factions%2Fnext-stats-action%2Faction.yml",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/.github%2Factions%2Fnext-stats-action%2Faction.yml?ref=4cd3ca26d71fc7904ea7030ee8355071efc9502d",
            "patch": "@@ -0,0 +1,10 @@\n+name: 'Next.js PR Stats'\n+description: 'Compare PR stats with canary'\n+inputs:\n+  bundler:\n+    description: 'Bundler to benchmark (webpack|turbopack|both)'\n+    default: 'both'\n+    required: false\n+runs:\n+  using: 'docker'\n+  image: 'Dockerfile'"
        },
        {
            "sha": "b6d2ede0dd065f6c68793cd79421653fb8237860",
            "filename": ".github/actions/next-stats-action/package.json",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/vercel/next.js/blob/4cd3ca26d71fc7904ea7030ee8355071efc9502d/.github%2Factions%2Fnext-stats-action%2Fpackage.json",
            "raw_url": "https://github.com/vercel/next.js/raw/4cd3ca26d71fc7904ea7030ee8355071efc9502d/.github%2Factions%2Fnext-stats-action%2Fpackage.json",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/.github%2Factions%2Fnext-stats-action%2Fpackage.json?ref=4cd3ca26d71fc7904ea7030ee8355071efc9502d",
            "patch": "@@ -2,6 +2,7 @@\n   \"private\": true,\n   \"main\": \"src/index.js\",\n   \"dependencies\": {\n+    \"@vercel/kv\": \"^1.0.1\",\n     \"async-sema\": \"^3.1.0\",\n     \"execa\": \"2.0.3\",\n     \"get-port\": \"^5.0.0\","
        },
        {
            "sha": "fa532bc3aaa43d31522a537598baaca17aed2b8a",
            "filename": ".github/actions/next-stats-action/src/add-comment.js",
            "status": "modified",
            "additions": 1012,
            "deletions": 178,
            "changes": 1190,
            "blob_url": "https://github.com/vercel/next.js/blob/4cd3ca26d71fc7904ea7030ee8355071efc9502d/.github%2Factions%2Fnext-stats-action%2Fsrc%2Fadd-comment.js",
            "raw_url": "https://github.com/vercel/next.js/raw/4cd3ca26d71fc7904ea7030ee8355071efc9502d/.github%2Factions%2Fnext-stats-action%2Fsrc%2Fadd-comment.js",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/.github%2Factions%2Fnext-stats-action%2Fsrc%2Fadd-comment.js?ref=4cd3ca26d71fc7904ea7030ee8355071efc9502d",
            "patch": "@@ -6,11 +6,213 @@ const logger = require('./util/logger')\n const prettyBytes = require('pretty-bytes')\n const { benchTitle } = require('./constants')\n \n-const gzipIgnoreRegex = new RegExp(`(General|^Serverless|${benchTitle})`)\n+// Try to load Vercel KV - may not be available in all environments\n+let kv = null\n+async function getKV() {\n+  if (kv) return kv\n+  if (!process.env.KV_REST_API_URL || !process.env.KV_REST_API_TOKEN) {\n+    return null\n+  }\n+  try {\n+    const { createClient } = require('@vercel/kv')\n+    kv = createClient({\n+      url: process.env.KV_REST_API_URL,\n+      token: process.env.KV_REST_API_TOKEN,\n+    })\n+    return kv\n+  } catch (e) {\n+    logger.error('Failed to initialize Vercel KV:', e)\n+    return null\n+  }\n+}\n+\n+const KV_STATS_KEY = 'next-stats-history'\n+const MAX_HISTORY_ENTRIES = 100\n+\n+// ============================================================================\n+// Metric Configuration\n+// ============================================================================\n+\n+// Human-readable labels for metrics\n+const METRIC_LABELS = {\n+  // Dev boot metrics - Turbopack (Listen = port listening, First Request = HTTP responding)\n+  nextDevColdListenDurationTurbo: 'Cold (Listen)',\n+  nextDevColdReadyDurationTurbo: 'Cold (First Request)',\n+  nextDevWarmListenDurationTurbo: 'Warm (Listen)',\n+  nextDevWarmReadyDurationTurbo: 'Warm (First Request)',\n+  // Dev boot metrics - Webpack\n+  nextDevColdListenDurationWebpack: 'Webpack Cold (Listen)',\n+  nextDevColdReadyDurationWebpack: 'Webpack Cold (First Request)',\n+  nextDevWarmListenDurationWebpack: 'Webpack Warm (Listen)',\n+  nextDevWarmReadyDurationWebpack: 'Webpack Warm (First Request)',\n+  // Production metrics\n+  nextStartReadyDuration: 'Prod Start',\n+  // Build metrics - Webpack\n+  buildDurationWebpack: 'Webpack Build Time',\n+  buildDurationCachedWebpack: 'Webpack Build Time (cached)',\n+  // Build metrics - Turbopack\n+  buildDurationTurbo: 'Turbo Build Time',\n+  buildDurationCachedTurbo: 'Turbo Build Time (cached)',\n+  // General metrics\n+  nodeModulesSize: 'node_modules Size',\n+}\n+\n+// Group configuration for organizing the comment\n+const METRIC_GROUPS = {\n+  'Dev Server': {\n+    icon: '‚ö°',\n+    description:\n+      'Boot time for `next dev` (Turbopack). Cold = fresh build, Warm = with cache.',\n+    metrics: [\n+      {\n+        label: 'Cold (Listen)',\n+        key: 'nextDevColdListenDurationTurbo',\n+        description: 'Time until TCP port accepts connections',\n+      },\n+      {\n+        label: 'Cold (First Request)',\n+        key: 'nextDevColdReadyDurationTurbo',\n+        description: 'Time until first HTTP request succeeds',\n+      },\n+      {\n+        label: 'Warm (Listen)',\n+        key: 'nextDevWarmListenDurationTurbo',\n+        description: 'Time until TCP port accepts connections (cached)',\n+      },\n+      {\n+        label: 'Warm (First Request)',\n+        key: 'nextDevWarmReadyDurationTurbo',\n+        description: 'Time until first HTTP request succeeds (cached)',\n+      },\n+    ],\n+    webpackGroup: 'Dev Server (Webpack)',\n+  },\n+  'Dev Server (Webpack)': {\n+    icon: 'üì¶',\n+    isLegacy: true,\n+    description:\n+      'Boot time for `next dev` (Webpack). Cold = fresh build, Warm = with cache.',\n+    metrics: [\n+      {\n+        label: 'Cold (Listen)',\n+        key: 'nextDevColdListenDurationWebpack',\n+        description: 'Time until TCP port accepts connections',\n+      },\n+      {\n+        label: 'Cold (First Request)',\n+        key: 'nextDevColdReadyDurationWebpack',\n+        description: 'Time until first HTTP request succeeds',\n+      },\n+      {\n+        label: 'Warm (Listen)',\n+        key: 'nextDevWarmListenDurationWebpack',\n+        description: 'Time until TCP port accepts connections (cached)',\n+      },\n+      {\n+        label: 'Warm (First Request)',\n+        key: 'nextDevWarmReadyDurationWebpack',\n+        description: 'Time until first HTTP request succeeds (cached)',\n+      },\n+    ],\n+  },\n+  'Production Builds': {\n+    icon: '‚ö°',\n+    description: 'Time for `next build` (Turbopack).',\n+    metrics: [\n+      {\n+        label: 'Fresh Build',\n+        key: 'buildDurationTurbo',\n+      },\n+      {\n+        label: 'Cached Build',\n+        key: 'buildDurationCachedTurbo',\n+      },\n+    ],\n+    webpackGroup: 'Production Builds (Webpack)',\n+  },\n+  'Production Builds (Webpack)': {\n+    icon: 'üì¶',\n+    isLegacy: true,\n+    description: 'Time for `next build --webpack`.',\n+    metrics: [\n+      {\n+        label: 'Fresh Build',\n+        key: 'buildDurationWebpack',\n+      },\n+      {\n+        label: 'Cached Build',\n+        key: 'buildDurationCachedWebpack',\n+      },\n+      {\n+        label: 'node_modules Size',\n+        key: 'nodeModulesSize',\n+        type: 'bytes',\n+      },\n+    ],\n+  },\n+  'Production Runtime': {\n+    icon: 'üöÄ',\n+    description: 'Boot time for `next start` (bundler-agnostic).',\n+    metrics: [\n+      {\n+        label: 'Start (First Request)',\n+        key: 'nextStartReadyDuration',\n+      },\n+    ],\n+  },\n+}\n+\n+// ============================================================================\n+// Historical Data (Vercel KV)\n+// ============================================================================\n+\n+async function loadHistory() {\n+  const kvClient = await getKV()\n+  if (!kvClient) {\n+    logger('KV not configured - historical trends unavailable')\n+    return { entries: [] }\n+  }\n+\n+  try {\n+    const data = await kvClient.lrange(KV_STATS_KEY, -MAX_HISTORY_ENTRIES, -1)\n+    const entries = data.map((d) => (typeof d === 'string' ? JSON.parse(d) : d))\n+    logger(`Loaded ${entries.length} historical entries from KV`)\n+    return { entries }\n+  } catch (e) {\n+    logger.error('Failed to load history from KV:', e)\n+    return { entries: [] }\n+  }\n+}\n+\n+async function saveToHistory(entry) {\n+  const kvClient = await getKV()\n+  if (!kvClient) return\n+\n+  try {\n+    await kvClient.rpush(KV_STATS_KEY, JSON.stringify(entry))\n+    // Trim to keep only last N entries\n+    await kvClient.ltrim(KV_STATS_KEY, -MAX_HISTORY_ENTRIES, -1)\n+    logger('Saved stats to KV history')\n+  } catch (e) {\n+    logger.error('Failed to save to KV:', e)\n+  }\n+}\n+\n+// ============================================================================\n+// Formatting Utilities\n+// ============================================================================\n+\n+function prettifyTime(ms) {\n+  if (typeof ms !== 'number') return 'N/A'\n+  if (ms < 1000) {\n+    return `${Math.round(ms)}ms`\n+  }\n+  return `${(ms / 1000).toFixed(3)}s`\n+}\n \n const prettify = (val, type = 'bytes') => {\n   if (typeof val !== 'number') return 'N/A'\n-  return type === 'bytes' ? prettyBytes(val) : prettyMs(val)\n+  return type === 'bytes' ? prettyBytes(val) : prettifyTime(val)\n }\n \n const round = (num, places) => {\n@@ -23,202 +225,781 @@ const shortenLabel = (itemKey) =>\n     ? `${itemKey.slice(0, 12)}..${itemKey.slice(-12)}`\n     : itemKey\n \n-const twoMB = 2 * 1024 * 1024\n-const ONE_HUNDRED_BYTES = 100\n-const ONE_HUNDRED_MS = 100\n+function getMetricLabel(key) {\n+  return METRIC_LABELS[key] || shortenLabel(key)\n+}\n \n-module.exports = async function addComment(\n-  results = [],\n-  actionInfo,\n-  statsConfig\n+function formatChange(mainVal, diffVal, type = 'bytes') {\n+  if (typeof mainVal !== 'number' || typeof diffVal !== 'number') {\n+    return { text: '-', significant: false, improved: false, regression: false }\n+  }\n+\n+  const diff = diffVal - mainVal\n+  const percentChange = mainVal > 0 ? (diff / mainVal) * 100 : 0\n+\n+  // Thresholds: filter CI noise while catching real regressions\n+  // For time:\n+  //   - (<50ms AND <10%) = insignificant for short ops (dev boot ~300ms)\n+  //   - OR <2% = insignificant for long ops (builds ~13s where 70ms is noise)\n+  // For size: <1KB AND <1% = not worth mentioning\n+  // If mainVal is 0 and diff is non-zero, always consider it significant\n+  const isInsignificant =\n+    mainVal === 0 && diff !== 0\n+      ? false\n+      : type === 'ms'\n+        ? (Math.abs(diff) < 50 && Math.abs(percentChange) < 10) ||\n+          Math.abs(percentChange) < 2\n+        : Math.abs(diff) < 1024 && Math.abs(percentChange) < 1\n+\n+  if (isInsignificant) {\n+    return { text: '‚úì', significant: false, improved: false, regression: false }\n+  }\n+\n+  const improved = diff < 0\n+  const regression = diff > 0\n+  // Clear icons: üî¥ regression, üü¢ improvement\n+  const icon = improved ? 'üü¢' : 'üî¥'\n+  const sign = diff > 0 ? '+' : ''\n+  const formatted = prettify(Math.abs(diff), type)\n+  const pct = `(${percentChange > 0 ? '+' : ''}${Math.round(percentChange)}%)`\n+\n+  return {\n+    text: `${icon} ${sign}${formatted} ${pct}`.trim(),\n+    significant: true,\n+    improved,\n+    regression,\n+  }\n+}\n+\n+function generateTrendBar(values) {\n+  if (!values || values.length < 2) return ''\n+\n+  const min = Math.min(...values)\n+  const max = Math.max(...values)\n+  const range = max - min\n+\n+  if (range === 0) return '‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ' // All values the same\n+\n+  // Unicode bar characters from short to tall\n+  const bars = '‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà'\n+\n+  // Take last 5 values for a compact trend\n+  const recent = values.slice(-5)\n+\n+  return recent\n+    .map((v) => {\n+      const normalized = (v - min) / range\n+      const index = Math.min(\n+        Math.floor(normalized * bars.length),\n+        bars.length - 1\n+      )\n+      return bars[index]\n+    })\n+    .join('')\n+}\n+\n+function getHistoricalValues(history, metricKey, limit = 15) {\n+  if (!history?.entries?.length) return []\n+  return history.entries\n+    .slice(-limit)\n+    .map((e) => e.metrics?.[metricKey])\n+    .filter((v) => typeof v === 'number')\n+}\n+\n+// ============================================================================\n+// Bundle Size Aggregation\n+// ============================================================================\n+\n+/**\n+ * Compute aggregate totals for each bundle group for KV persistence.\n+ * This allows tracking bundle size trends over time even when individual\n+ * files can't be matched (Turbopack uses content-hash filenames).\n+ */\n+function computeBundleGroupTotals(stats) {\n+  const totals = {}\n+\n+  for (const [groupKey, groupStats] of Object.entries(stats)) {\n+    if (groupKey === 'General' || groupKey === benchTitle) continue\n+    if (!groupStats || typeof groupStats !== 'object') continue\n+\n+    // Sum gzip values for each group\n+    let total = 0\n+    for (const [key, value] of Object.entries(groupStats)) {\n+      if (key.endsWith(' gzip') && typeof value === 'number') {\n+        total += value\n+      }\n+    }\n+\n+    // Create stable key from group name\n+    // \"Client Bundles (main) (Turbopack)\" ‚Üí \"clientBundlesMainTurbopackTotal\"\n+    const stableKey =\n+      groupKey\n+        .replace(/[^a-zA-Z0-9]/g, ' ')\n+        .trim()\n+        .split(/\\s+/)\n+        .filter((w) => w.length > 0)\n+        .map((w, i) =>\n+          i === 0\n+            ? w.toLowerCase()\n+            : w.charAt(0).toUpperCase() + w.slice(1).toLowerCase()\n+        )\n+        .join('') + 'Total'\n+\n+    totals[stableKey] = total\n+  }\n+\n+  return totals\n+}\n+\n+// ============================================================================\n+// Comment Generation\n+// ============================================================================\n+\n+// Determine if a metric is time-based (ms) or size-based (bytes)\n+function getMetricType(metricKey) {\n+  if (metricKey.includes('Duration')) {\n+    return 'ms'\n+  }\n+  if (metricKey.includes('Size')) {\n+    return 'bytes'\n+  }\n+  return 'ms' // default to ms for performance metrics\n+}\n+\n+function generateChangeSummary(mainStats, diffStats, history) {\n+  // Collect all significant changes across all metrics\n+  const changes = []\n+\n+  // Check General metrics\n+  const mainGeneral = mainStats?.General || {}\n+  const diffGeneral = diffStats?.General || {}\n+\n+  for (const key of Object.keys({ ...mainGeneral, ...diffGeneral })) {\n+    const mainVal = mainGeneral[key]\n+    const diffVal = diffGeneral[key]\n+    const type = getMetricType(key)\n+    const change = formatChange(mainVal, diffVal, type)\n+\n+    if (change.significant) {\n+      const histValues = getHistoricalValues(history, key)\n+      changes.push({\n+        metric: getMetricLabel(key),\n+        mainVal: prettify(mainVal, type),\n+        diffVal: prettify(diffVal, type),\n+        change: change.text,\n+        trend: generateTrendBar(histValues),\n+        improved: change.improved,\n+        regression: change.regression,\n+      })\n+    }\n+  }\n+\n+  if (changes.length === 0) {\n+    return `### ‚úÖ No significant changes detected\\n\\n`\n+  }\n+\n+  // Sort: regressions first, then improvements\n+  changes.sort((a, b) => {\n+    if (a.regression !== b.regression) return a.regression ? -1 : 1\n+    return 0\n+  })\n+\n+  const regressions = changes.filter((c) => c.regression)\n+  const improvements = changes.filter((c) => c.improved)\n+\n+  // Clear headline showing regressions\n+  let headline = ''\n+  if (regressions.length > 0) {\n+    headline = `### üî¥ ${regressions.length} regression${regressions.length > 1 ? 's' : ''}`\n+    if (improvements.length > 0) {\n+      headline += `, ${improvements.length} improvement${improvements.length > 1 ? 's' : ''}`\n+    }\n+  } else {\n+    headline = `### üü¢ ${improvements.length} improvement${improvements.length > 1 ? 's' : ''}`\n+  }\n+\n+  const hasTrends = changes.some((c) => c.trend)\n+  let summary = `${headline}\\n\\n`\n+\n+  if (hasTrends) {\n+    summary += `| Metric | Canary | PR | Change | Trend |\\n`\n+    summary += `|:-------|-------:|---:|-------:|:-----:|\\n`\n+    for (const c of changes) {\n+      summary += `| ${c.metric} | ${c.mainVal} | ${c.diffVal} | ${c.change} | ${c.trend} |\\n`\n+    }\n+  } else {\n+    summary += `| Metric | Canary | PR | Change |\\n`\n+    summary += `|:-------|-------:|---:|-------:|\\n`\n+    for (const c of changes) {\n+      summary += `| ${c.metric} | ${c.mainVal} | ${c.diffVal} | ${c.change} |\\n`\n+    }\n+  }\n+\n+  return summary + '\\n'\n+}\n+\n+function generateGlossary() {\n+  return `<details>\n+<summary><strong>üìñ Metrics Glossary</strong></summary>\n+\n+**Dev Server Metrics:**\n+- **Listen** = TCP port starts accepting connections\n+- **First Request** = HTTP server returns successful response\n+- **Cold** = Fresh build (no cache)\n+- **Warm** = With cached build artifacts\n+\n+**Build Metrics:**\n+- **Fresh** = Clean build (no .next directory)\n+- **Cached** = With existing .next directory\n+\n+**Change Thresholds:**\n+- Time: Changes < 50ms AND < 10%, OR < 2% are insignificant\n+- Size: Changes < 1KB AND < 1% are insignificant\n+- All other changes are flagged to catch regressions\n+\n+</details>\n+\n+`\n+}\n+\n+function generateMetricsTable(\n+  groupName,\n+  config,\n+  mainGroup,\n+  diffGroup,\n+  history,\n+  isCollapsed = false\n ) {\n-  let comment = `# ${\n-    actionInfo.isRelease\n-      ? statsConfig.commentReleaseHeading || 'Stats from current release'\n-      : statsConfig.commentHeading || 'Stats from current PR'\n-  }\\n\\n`\n+  // Pre-compute all row data to check if we have any trends\n+  const rowData = []\n+  let hasTrends = false\n \n-  const tableHead = `|  | ${statsConfig.mainRepo} ${statsConfig.mainBranch} ${\n-    actionInfo.lastStableTag || ''\n-  } | ${actionInfo.prRepo} ${actionInfo.prRef} | Change |\\n| - | - | - | - |\\n`\n+  for (const metricDef of config.metrics) {\n+    const mainVal = mainGroup[metricDef.key]\n+    const diffVal = diffGroup[metricDef.key]\n \n-  for (let i = 0; i < results.length; i++) {\n-    const result = results[i]\n-    const isLastResult = i === results.length - 1\n-    let resultHasIncrease = false\n-    let resultHasDecrease = false\n-    let resultContent = ''\n-\n-    Object.keys(result.mainRepoStats).forEach((groupKey) => {\n-      const isBenchmark = groupKey === benchTitle\n-      const mainRepoGroup = result.mainRepoStats[groupKey]\n-      const diffRepoGroup = result.diffRepoStats[groupKey]\n-      const itemKeys = new Set([\n-        ...Object.keys(mainRepoGroup),\n-        ...Object.keys(diffRepoGroup),\n-      ])\n-      let groupTable = tableHead\n-      let mainRepoTotal = 0\n-      let diffRepoTotal = 0\n-      let totalChange = 0\n-\n-      itemKeys.forEach((itemKey) => {\n-        const prettyType = itemKey.match(/(length|duration)/i) ? 'ms' : 'bytes'\n-        const isGzipItem = itemKey.endsWith('gzip')\n-        const mainItemVal = mainRepoGroup[itemKey]\n-        const diffItemVal = diffRepoGroup[itemKey]\n-        const useRawValue = isBenchmark && prettyType !== 'ms'\n-        const mainItemStr = useRawValue\n-          ? mainItemVal\n-          : prettify(mainItemVal, prettyType)\n-\n-        const diffItemStr = useRawValue\n-          ? diffItemVal\n-          : prettify(diffItemVal, prettyType)\n-\n-        let change = '‚úì'\n-\n-        // Don't show gzip values for serverless as they aren't\n-        // deterministic currently\n-        if (groupKey.startsWith('Serverless') && isGzipItem) return\n-        // otherwise only show gzip values\n-        else if (!isGzipItem && !groupKey.match(gzipIgnoreRegex)) return\n-\n-        // calculate the change\n-        if (mainItemVal !== diffItemVal) {\n-          if (\n-            typeof mainItemVal === 'number' &&\n-            typeof diffItemVal === 'number'\n-          ) {\n-            const roundedValue = round(diffItemVal - mainItemVal, 2)\n-\n-            // check if there is still a change after rounding\n-            if (\n-              roundedValue !== 0 &&\n-              ((prettyType === 'ms' && roundedValue > ONE_HUNDRED_MS) ||\n-                (prettyType === 'bytes' && roundedValue > ONE_HUNDRED_BYTES))\n-            ) {\n-              change = roundedValue\n-              const absChange = Math.abs(change)\n-              const warnIfNegative = isBenchmark && itemKey.match(/req\\/sec/)\n-              const warn = warnIfNegative\n-                ? change < 0\n-                  ? '‚ö†Ô∏è '\n-                  : ''\n-                : change > 0\n-                  ? '‚ö†Ô∏è '\n-                  : ''\n-              change = `${warn}${change < 0 ? '-' : '+'}${\n-                useRawValue ? absChange : prettify(absChange, prettyType)\n-              }`\n-            } else {\n-              change = 'N/A'\n-            }\n-          } else {\n-            change = 'N/A'\n-          }\n-        }\n+    if (mainVal === undefined && diffVal === undefined) continue\n+\n+    const metricType = metricDef.type || 'ms'\n+    const mainStr = prettify(mainVal, metricType)\n+    const diffStr = prettify(diffVal, metricType)\n+    const change = formatChange(mainVal, diffVal, metricType)\n+    const histValues = getHistoricalValues(history, metricDef.key)\n+    const sparkline = generateTrendBar(histValues)\n+\n+    if (sparkline) hasTrends = true\n+\n+    rowData.push({\n+      label: metricDef.label,\n+      mainStr,\n+      diffStr,\n+      changeText: change.text,\n+      sparkline,\n+    })\n+  }\n+\n+  if (rowData.length === 0) return ''\n+\n+  // Build rows with or without trend column\n+  let rows = ''\n+  for (const row of rowData) {\n+    if (hasTrends) {\n+      rows += `| ${row.label} | ${row.mainStr} | ${row.diffStr} | ${row.changeText} | ${row.sparkline} |\\n`\n+    } else {\n+      rows += `| ${row.label} | ${row.mainStr} | ${row.diffStr} | ${row.changeText} |\\n`\n+    }\n+  }\n+\n+  const header = hasTrends\n+    ? `| Metric | Canary | PR | Change | Trend |\n+|:-------|-------:|---:|-------:|:-----:|`\n+    : `| Metric | Canary | PR | Change |\n+|:-------|-------:|---:|-------:|`\n+\n+  const table = `### ${config.icon} ${groupName}\n+\n+${header}\n+${rows}\n+`\n+\n+  // Wrap legacy/webpack tables in collapsible details\n+  if (isCollapsed) {\n+    return `<details>\n+<summary><strong>${config.icon} ${groupName} (Legacy)</strong></summary>\n+\n+${table}\n+</details>\n+\n+`\n+  }\n+\n+  return table\n+}\n+\n+function generatePerformanceSection(mainStats, diffStats, history) {\n+  let content = ''\n+\n+  content += generateGlossary()\n+\n+  const mainGroup = mainStats?.General || {}\n+  const diffGroup = diffStats?.General || {}\n+\n+  // Render groups in order: show Turbopack tables, then collapse Webpack tables\n+  for (const [groupName, config] of Object.entries(METRIC_GROUPS)) {\n+    // Skip legacy groups - they'll be rendered after their corresponding Turbopack group\n+    if (config.isLegacy) continue\n+\n+    // Render Turbopack/main group prominently\n+    content += generateMetricsTable(\n+      groupName,\n+      config,\n+      mainGroup,\n+      diffGroup,\n+      history,\n+      false\n+    )\n+\n+    // If this group has a corresponding Webpack group, render it collapsed\n+    if (config.webpackGroup) {\n+      const webpackConfig = METRIC_GROUPS[config.webpackGroup]\n+      if (webpackConfig) {\n+        content += generateMetricsTable(\n+          config.webpackGroup,\n+          webpackConfig,\n+          mainGroup,\n+          diffGroup,\n+          history,\n+          true // collapsed\n+        )\n+      }\n+    }\n+  }\n+\n+  return content\n+}\n \n-        if (\n-          (change !== 'N/A' && !itemKey.startsWith('buildDuration')) ||\n-          (isBenchmark && itemKey.match(/req\\/sec/))\n-        ) {\n-          if (typeof mainItemVal === 'number') mainRepoTotal += mainItemVal\n-          if (typeof diffItemVal === 'number') diffRepoTotal += diffItemVal\n+// Base group names (without bundler suffix)\n+const BASE_BUNDLE_GROUPS = {\n+  client: [\n+    'Client Bundles (main)',\n+    'Client Pages',\n+    'Legacy Client Bundles (polyfills)',\n+  ],\n+  server: ['Edge SSR bundle Size', 'Middleware size'],\n+  // Next Runtimes are built independently of Turbopack/Webpack\n+  shared: ['Next Runtimes'],\n+  other: ['Client Build Manifests', 'Rendered Page Sizes', 'build cache'],\n+}\n+\n+// Bundler configuration\n+const BUNDLERS_CONFIG = [\n+  { name: 'Webpack', icon: 'üì¶' },\n+  { name: 'Turbopack', icon: '‚ö°' },\n+]\n+\n+// Helper to check if a group name belongs to a bundler category\n+function getBundlerFromGroupKey(groupKey) {\n+  for (const bundler of BUNDLERS_CONFIG) {\n+    if (groupKey.endsWith(`(${bundler.name})`)) {\n+      return bundler\n+    }\n+  }\n+  return null\n+}\n+\n+// Helper to get the base group name (without bundler suffix)\n+function getBaseGroupName(groupKey) {\n+  return groupKey.replace(/ \\((Webpack|Turbopack)\\)$/, '')\n+}\n+\n+function generateBundleGroup(groupKey, result, tableHead) {\n+  const gzipIgnoreRegex = new RegExp(`(General|^Serverless|${benchTitle})`)\n+  const mainRepoGroup = result.mainRepoStats[groupKey] || {}\n+  const diffRepoGroup = result.diffRepoStats[groupKey] || {}\n+  const itemKeys = new Set([\n+    ...Object.keys(mainRepoGroup),\n+    ...Object.keys(diffRepoGroup),\n+  ])\n+\n+  // Detect pure-hash filenames (Turbopack uses content-hash only, can't be matched)\n+  // Pattern: 16 hex chars followed by .js or .css (with optional .map or gzip suffix)\n+  const pureHashPattern = /^[0-9a-f]{16}\\.(js|css)/\n+  let pureHashCount = 0\n+  let totalGzipItems = 0\n+\n+  itemKeys.forEach((itemKey) => {\n+    if (itemKey.endsWith('gzip')) {\n+      totalGzipItems++\n+      // Extract base filename (remove ' gzip' suffix)\n+      const baseName = itemKey.replace(/ gzip$/, '')\n+      if (pureHashPattern.test(baseName)) {\n+        pureHashCount++\n+      }\n+    }\n+  })\n+\n+  // If more than 50% of items are pure-hash files, show totals only\n+  const isPureHashGroup =\n+    totalGzipItems > 0 && pureHashCount / totalGzipItems > 0.5\n+\n+  let groupTable = tableHead\n+  let mainRepoTotal = 0\n+  let diffRepoTotal = 0\n+  let hasItems = false\n+  let matchedItems = 0\n+  let unmatchedItems = 0\n+\n+  itemKeys.forEach((itemKey) => {\n+    const isGzipItem = itemKey.endsWith('gzip')\n+    const mainItemVal = mainRepoGroup[itemKey]\n+    const diffItemVal = diffRepoGroup[itemKey]\n+\n+    // Skip non-gzip for most groups, skip gzip for serverless\n+    if (groupKey.startsWith('Serverless') && isGzipItem) return\n+    if (!isGzipItem && !groupKey.match(gzipIgnoreRegex)) return\n+\n+    hasItems = true\n+\n+    // Track matched vs unmatched items\n+    if (typeof mainItemVal === 'number' && typeof diffItemVal === 'number') {\n+      matchedItems++\n+    } else {\n+      unmatchedItems++\n+    }\n+\n+    if (typeof mainItemVal === 'number') mainRepoTotal += mainItemVal\n+    if (typeof diffItemVal === 'number') diffRepoTotal += diffItemVal\n+\n+    // Only add individual rows if not a pure-hash group\n+    if (!isPureHashGroup) {\n+      const mainItemStr = prettify(mainItemVal, 'bytes')\n+      const diffItemStr = prettify(diffItemVal, 'bytes')\n+      const change = formatChange(mainItemVal, diffItemVal, 'bytes')\n+      groupTable += `| ${shortenLabel(itemKey)} | ${mainItemStr} | ${diffItemStr} | ${change.text} |\\n`\n+    }\n+  })\n+\n+  if (!hasItems) return null\n+\n+  const totalChange = diffRepoTotal - mainRepoTotal\n+  let totalChangeStr = '‚úì'\n+\n+  if (totalChange !== 0) {\n+    const icon = totalChange > 0 ? '‚ö†Ô∏è' : '‚úÖ'\n+    const sign = totalChange > 0 ? '+' : '-'\n+    totalChangeStr = `${icon} ${sign}${prettyBytes(Math.abs(totalChange))}`\n+  }\n+\n+  // Friendly names for groups\n+  const friendlyNames = {\n+    'Client Bundles (main)': 'Main Bundles',\n+    'Legacy Client Bundles (polyfills)': 'Polyfills',\n+    'Client Pages': 'Pages',\n+    'Client Build Manifests': 'Build Manifests',\n+    'Rendered Page Sizes': 'HTML Output',\n+    'Edge SSR bundle Size': 'Edge SSR',\n+    'Middleware size': 'Middleware',\n+    'Next Runtimes': 'Runtimes',\n+    'build cache': 'Build Cache',\n+  }\n+\n+  const baseGroupName = getBaseGroupName(groupKey)\n+  const displayName = friendlyNames[baseGroupName] || groupKey\n+\n+  // For pure-hash groups, show a simplified view with just totals\n+  if (isPureHashGroup) {\n+    return `<details>\n+<summary>${displayName}: **${prettyBytes(mainRepoTotal)}** ‚Üí **${prettyBytes(diffRepoTotal)}** ${totalChangeStr}</summary>\n+\n+*${totalGzipItems} files with content-based hashes (individual files not comparable between builds)*\n+\n+</details>\n+`\n+  }\n+\n+  groupTable += `| **Total** | **${prettyBytes(mainRepoTotal)}** | **${prettyBytes(diffRepoTotal)}** | ${totalChangeStr} |\\n`\n+\n+  return `<details>\n+<summary>${displayName}</summary>\n+\n+${groupTable}\n+</details>\n+`\n+}\n+\n+function generateBundleSizeSection(result, tableHead) {\n+  let content = ''\n+\n+  // Collect all group keys from the result\n+  const allGroupKeys = new Set([\n+    ...Object.keys(result.mainRepoStats || {}),\n+    ...Object.keys(result.diffRepoStats || {}),\n+  ])\n+\n+  // Organize groups by bundler and category\n+  const bundlerGroups = {}\n+  const nonBundlerGroups = { client: [], server: [], shared: [], other: [] }\n+\n+  for (const groupKey of allGroupKeys) {\n+    if (groupKey === 'General' || groupKey === benchTitle) continue\n+\n+    const bundler = getBundlerFromGroupKey(groupKey)\n+    const baseGroup = getBaseGroupName(groupKey)\n+\n+    if (bundler) {\n+      if (!bundlerGroups[bundler.name]) {\n+        bundlerGroups[bundler.name] = { icon: bundler.icon, groups: [] }\n+      }\n+      bundlerGroups[bundler.name].groups.push(groupKey)\n+    } else {\n+      // Categorize non-bundler groups\n+      if (BASE_BUNDLE_GROUPS.client.includes(baseGroup)) {\n+        nonBundlerGroups.client.push(groupKey)\n+      } else if (BASE_BUNDLE_GROUPS.server.includes(baseGroup)) {\n+        nonBundlerGroups.server.push(groupKey)\n+      } else if (BASE_BUNDLE_GROUPS.shared.includes(baseGroup)) {\n+        nonBundlerGroups.shared.push(groupKey)\n+      } else {\n+        nonBundlerGroups.other.push(groupKey)\n+      }\n+    }\n+  }\n+\n+  // Generate content for bundler-specific groups\n+  for (const [bundlerName, bundlerData] of Object.entries(bundlerGroups)) {\n+    let bundlerContent = ''\n+    let hasAny = false\n+\n+    // Organize bundler groups by category\n+    // Skip shared groups - they'll be rendered in their own section\n+    const categorizedGroups = { client: [], server: [], other: [] }\n+    for (const groupKey of bundlerData.groups) {\n+      const baseGroup = getBaseGroupName(groupKey)\n+      // Skip shared groups (like Next Runtimes) - they're bundler-independent\n+      if (BASE_BUNDLE_GROUPS.shared.includes(baseGroup)) {\n+        // Move to nonBundlerGroups.shared for unified rendering\n+        if (!nonBundlerGroups.shared.includes(groupKey)) {\n+          nonBundlerGroups.shared.push(groupKey)\n         }\n+        continue\n+      }\n+      if (BASE_BUNDLE_GROUPS.client.includes(baseGroup)) {\n+        categorizedGroups.client.push(groupKey)\n+      } else if (BASE_BUNDLE_GROUPS.server.includes(baseGroup)) {\n+        categorizedGroups.server.push(groupKey)\n+      } else {\n+        categorizedGroups.other.push(groupKey)\n+      }\n+    }\n \n-        groupTable += `| ${\n-          isBenchmark ? itemKey : shortenLabel(itemKey)\n-        } | ${mainItemStr} | ${diffItemStr} | ${change} |\\n`\n-      })\n-      let groupTotalChange = ''\n-\n-      totalChange = diffRepoTotal - mainRepoTotal\n-\n-      if (totalChange !== 0) {\n-        if (totalChange < 0) {\n-          resultHasDecrease = true\n-          groupTotalChange = ` Overall decrease ${isBenchmark ? '‚ö†Ô∏è' : '‚úì'}`\n-        } else {\n-          if (\n-            (groupKey !== 'General' && totalChange > 5) ||\n-            totalChange > twoMB\n-          ) {\n-            resultHasIncrease = true\n-          }\n-          groupTotalChange = ` Overall increase ${isBenchmark ? '‚úì' : '‚ö†Ô∏è'}`\n+    // Client bundles\n+    if (categorizedGroups.client.length > 0) {\n+      let clientContent = ''\n+      for (const groupKey of categorizedGroups.client) {\n+        const groupContent = generateBundleGroup(groupKey, result, tableHead)\n+        if (groupContent) {\n+          hasAny = true\n+          clientContent += groupContent\n         }\n       }\n+      if (clientContent) {\n+        bundlerContent += `**Client**\\n${clientContent}\\n`\n+      }\n+    }\n \n-      if (groupKey !== 'General' && groupKey !== benchTitle) {\n-        let totalChangeSign = ''\n+    // Server bundles\n+    if (categorizedGroups.server.length > 0) {\n+      let serverContent = ''\n+      for (const groupKey of categorizedGroups.server) {\n+        const groupContent = generateBundleGroup(groupKey, result, tableHead)\n+        if (groupContent) {\n+          hasAny = true\n+          serverContent += groupContent\n+        }\n+      }\n+      if (serverContent) {\n+        bundlerContent += `**Server**\\n${serverContent}\\n`\n+      }\n+    }\n \n-        if (totalChange === 0) {\n-          totalChange = '‚úì'\n-        } else {\n-          totalChangeSign = totalChange < 0 ? '-' : '‚ö†Ô∏è +'\n+    // Other bundles (collapsed)\n+    if (categorizedGroups.other.length > 0) {\n+      let otherContent = ''\n+      for (const groupKey of categorizedGroups.other) {\n+        const groupContent = generateBundleGroup(groupKey, result, tableHead)\n+        if (groupContent) {\n+          hasAny = true\n+          otherContent += groupContent\n         }\n-        totalChange = `${totalChangeSign}${\n-          typeof totalChange === 'number'\n-            ? prettify(Math.abs(totalChange))\n-            : totalChange\n-        }`\n-        groupTable += `| Overall change | ${prettyBytes(\n-          round(mainRepoTotal, 2)\n-        )} | ${prettyBytes(round(diffRepoTotal, 2))} | ${totalChange} |\\n`\n       }\n+      if (otherContent) {\n+        bundlerContent += `<details>\\n<summary><strong>Build Details</strong></summary>\\n\\n${otherContent}</details>\\n\\n`\n+      }\n+    }\n+\n+    if (hasAny) {\n+      content += `### ${bundlerData.icon} ${bundlerName}\\n\\n${bundlerContent}`\n+    }\n+  }\n+\n+  // Handle any non-bundler-specific groups\n+  for (const [categoryKey, groups] of Object.entries(nonBundlerGroups)) {\n+    if (groups.length === 0) continue\n+\n+    let categoryContent = ''\n+    let hasAny = false\n+\n+    // For shared groups, deduplicate by base name (e.g., \"Next Runtimes (Turbopack)\"\n+    // and \"Next Runtimes (Webpack)\" should only show once)\n+    const seenBaseGroups = new Set()\n+\n+    for (const groupKey of groups) {\n+      const baseGroup = getBaseGroupName(groupKey)\n \n-      if (itemKeys.size > 0) {\n-        resultContent += `<details>\\n`\n-        resultContent += `<summary><strong>${groupKey}</strong>${groupTotalChange}</summary>\\n\\n`\n-        resultContent += groupTable\n-        resultContent += `\\n</details>\\n\\n`\n+      // Skip if we've already rendered this base group\n+      if (categoryKey === 'shared' && seenBaseGroups.has(baseGroup)) {\n+        continue\n       }\n-    })\n+      seenBaseGroups.add(baseGroup)\n \n-    // add diffs\n-    if (result.diffs) {\n-      let diffContent = ''\n-\n-      Object.keys(result.diffs).forEach((itemKey) => {\n-        const curDiff = result.diffs[itemKey]\n-        diffContent += `<details>\\n`\n-        diffContent += `<summary>Diff for <strong>${shortenLabel(\n-          itemKey\n-        )}</strong></summary>\\n\\n`\n-\n-        if (curDiff.length > 36 * 1000) {\n-          diffContent += 'Diff too large to display'\n-        } else {\n-          diffContent += `\\`\\`\\`diff\\n${curDiff}\\n\\`\\`\\``\n-        }\n-        diffContent += `\\n</details>\\n`\n-      })\n+      const groupContent = generateBundleGroup(groupKey, result, tableHead)\n+      if (groupContent) {\n+        hasAny = true\n+        categoryContent += groupContent\n+      }\n+    }\n \n-      if (diffContent.length > 0) {\n-        resultContent += `<details>\\n`\n-        resultContent += `<summary><strong>Diff details</strong></summary>\\n\\n`\n-        resultContent += diffContent\n-        resultContent += `\\n</details>\\n\\n`\n+    if (hasAny) {\n+      const titles = {\n+        client: 'üì¶ Client',\n+        server: 'üñ•Ô∏è Server',\n+        shared: 'üîÑ Shared (bundler-independent)',\n+        other: 'üîß Other',\n       }\n+      content += `### ${titles[categoryKey]}\\n\\n${categoryContent}`\n     }\n-    let increaseDecreaseNote = ''\n+  }\n+\n+  return content ? `## Bundle Sizes\\n\\n${content}` : ''\n+}\n+\n+function generateDiffsSection(result) {\n+  if (!result.diffs || Object.keys(result.diffs).length === 0) return ''\n+\n+  const diffKeys = Object.keys(result.diffs)\n+  const diffCount = diffKeys.length\n+\n+  // Just show count and list of changed files, keep diffs collapsed\n+  let content = `<details>\\n<summary><strong>üìù Changed Files</strong> (${diffCount} file${diffCount === 1 ? '' : 's'})</summary>\\n\\n`\n+\n+  // List files that changed\n+  content += '**Files with changes:**\\n'\n+  for (const itemKey of diffKeys.slice(0, 20)) {\n+    content += `- \\`${shortenLabel(itemKey)}\\`\\n`\n+  }\n+  if (diffKeys.length > 20) {\n+    content += `- ... and ${diffKeys.length - 20} more\\n`\n+  }\n \n-    if (resultHasIncrease) {\n-      increaseDecreaseNote = ' (Increase detected ‚ö†Ô∏è)'\n-    } else if (resultHasDecrease) {\n-      increaseDecreaseNote = ' (Decrease detected ‚úì)'\n+  // Show actual diffs in nested collapsed sections\n+  content += '\\n<details>\\n<summary>View diffs</summary>\\n\\n'\n+  for (const [itemKey, diff] of Object.entries(result.diffs)) {\n+    content += `<details>\\n<summary>${shortenLabel(itemKey)}</summary>\\n\\n`\n+    if (diff.length > 36000) {\n+      content += 'Diff too large to display'\n+    } else {\n+      content += `\\`\\`\\`diff\\n${diff}\\n\\`\\`\\``\n     }\n+    content += '\\n</details>\\n'\n+  }\n+  content += '</details>\\n'\n+\n+  content += '</details>\\n\\n'\n+  return content\n+}\n \n-    comment += `<details open>\\n`\n-    comment += `<summary><strong>${result.title}</strong>${increaseDecreaseNote}</summary>\\n\\n<br/>\\n\\n`\n-    comment += resultContent\n-    comment += '</details>\\n'\n+// ============================================================================\n+// Main Export\n+// ============================================================================\n+\n+// Hidden marker to identify stats comments (invisible in rendered markdown)\n+const STATS_COMMENT_MARKER = '<!-- __NEXT_STATS_COMMENT__ -->'\n+\n+module.exports = async function addComment(\n+  results = [],\n+  actionInfo,\n+  statsConfig\n+) {\n+  // Load historical data\n+  const history = await loadHistory()\n+\n+  // Build the comment with hidden marker for identification\n+  let comment = `${STATS_COMMENT_MARKER}\\n## ${\n+    actionInfo.isRelease\n+      ? statsConfig.commentReleaseHeading || 'Stats from current release'\n+      : statsConfig.commentHeading || 'Stats from current PR'\n+  }\\n\\n`\n+\n+  const tableHead = `| | Canary | PR | Change |\\n|:--|--:|--:|--:|\\n`\n+\n+  for (let i = 0; i < results.length; i++) {\n+    const result = results[i]\n+    const isLastResult = i === results.length - 1\n+\n+    // Add summary showing only significant changes (not collapsed)\n+    if (i === 0) {\n+      comment += generateChangeSummary(\n+        result.mainRepoStats,\n+        result.diffRepoStats,\n+        history\n+      )\n+    }\n+\n+    // Add performance section (collapsed by default)\n+    const perfSection = generatePerformanceSection(\n+      result.mainRepoStats,\n+      result.diffRepoStats,\n+      history\n+    )\n+    if (perfSection) {\n+      comment += `<details>\\n<summary><strong>üìä All Metrics</strong></summary>\\n\\n${perfSection}</details>\\n\\n`\n+    }\n+\n+    // Add bundle sizes (collapsed by default)\n+    const bundleSection = generateBundleSizeSection(result, tableHead)\n+    if (bundleSection) {\n+      comment += `<details>\\n<summary><strong>üì¶ Bundle Sizes</strong></summary>\\n\\n${bundleSection}</details>\\n\\n`\n+    }\n+\n+    // Add diffs (already collapsed)\n+    comment += generateDiffsSection(result)\n \n     if (!isLastResult) {\n-      comment += `<hr/>\\n`\n+      comment += '<hr/>\\n\\n'\n     }\n   }\n+\n+  // Save canary stats to history (only for releases, not PR comparisons)\n+  // This ensures we only track official canary metrics, not PR-specific data\n+  if (results.length > 0 && actionInfo.isRelease && actionInfo.commitId) {\n+    const mainStats = results[0].mainRepoStats\n+    if (mainStats?.General) {\n+      // Compute aggregate totals for each bundle group\n+      const bundleTotals = computeBundleGroupTotals(mainStats)\n+\n+      const entry = {\n+        commitId: actionInfo.commitId,\n+        timestamp: new Date().toISOString(),\n+        metrics: {\n+          ...mainStats.General, // Performance metrics\n+          ...bundleTotals, // Bundle size totals\n+        },\n+      }\n+      await saveToHistory(entry)\n+    }\n+  }\n+\n+  // Output locally or post to GitHub\n   if (process.env.LOCAL_STATS) {\n     const statsPath = path.resolve('pr-stats.md')\n     await fs.writeFile(statsPath, comment)\n@@ -231,8 +1012,6 @@ module.exports = async function addComment(\n     actionInfo.customCommentEndpoint ||\n     (actionInfo.githubToken && actionInfo.commentEndpoint)\n   ) {\n-    logger(`Posting results to ${actionInfo.commentEndpoint}`)\n-\n     const body = {\n       body: comment,\n       ...(!actionInfo.githubToken\n@@ -249,8 +1028,61 @@ module.exports = async function addComment(\n     }\n \n     try {\n-      const res = await fetch(actionInfo.commentEndpoint, {\n-        method: 'POST',\n+      // Try to find existing stats comment to update\n+      let existingCommentId = null\n+      const commentHeading =\n+        statsConfig.commentHeading || 'Stats from current PR'\n+\n+      if (actionInfo.githubToken && actionInfo.commentEndpoint) {\n+        try {\n+          const existingRes = await fetch(actionInfo.commentEndpoint, {\n+            headers: {\n+              Authorization: `bearer ${actionInfo.githubToken}`,\n+            },\n+          })\n+\n+          if (existingRes.ok) {\n+            const comments = await existingRes.json()\n+            // Find comment with our hidden marker, or fall back to heading match\n+            // The marker ensures we only update our own stats comments\n+            const existing = comments.find(\n+              (c) =>\n+                c.body &&\n+                (c.body.includes(STATS_COMMENT_MARKER) ||\n+                  // Legacy fallback: match by heading (for comments before marker was added)\n+                  ((c.body.startsWith(`## ${commentHeading}`) ||\n+                    c.body.startsWith(`# ${commentHeading}`)) &&\n+                    c.body.includes('Canary') &&\n+                    c.body.includes('Change')))\n+            )\n+            if (existing) {\n+              existingCommentId = existing.id\n+              logger(`Found existing comment ${existingCommentId} to update`)\n+            }\n+          }\n+        } catch (e) {\n+          logger.error('Failed to fetch existing comments:', e)\n+        }\n+      }\n+\n+      // Update existing or create new\n+      let endpoint = actionInfo.commentEndpoint\n+      let method = 'POST'\n+\n+      if (existingCommentId && actionInfo.githubToken) {\n+        // GitHub API: PATCH /repos/{owner}/{repo}/issues/comments/{comment_id}\n+        endpoint = actionInfo.commentEndpoint.replace(\n+          /\\/issues\\/\\d+\\/comments$/,\n+          `/issues/comments/${existingCommentId}`\n+        )\n+        method = 'PATCH'\n+        logger(`Updating existing comment at ${endpoint}`)\n+      } else {\n+        logger(`Creating new comment at ${endpoint}`)\n+      }\n+\n+      const res = await fetch(endpoint, {\n+        method,\n         headers: {\n           ...(actionInfo.githubToken\n             ? {\n@@ -264,14 +1096,16 @@ module.exports = async function addComment(\n       })\n \n       if (!res.ok) {\n-        logger.error(`Failed to post results ${res.status}`)\n+        logger.error(`Failed to ${method} results ${res.status}`)\n         try {\n           logger.error(await res.text())\n         } catch (_) {\n           /* no-op */\n         }\n       } else {\n-        logger('Successfully posted results')\n+        logger(\n+          `Successfully ${method === 'PATCH' ? 'updated' : 'posted'} results`\n+        )\n       }\n     } catch (err) {\n       logger.error(`Error occurred posting results`, err)"
        },
        {
            "sha": "3cf4ea8bc9efbba66f58acbeb3ef8c8b0e88bb7e",
            "filename": ".github/actions/next-stats-action/src/aggregate-results.js",
            "status": "added",
            "additions": 127,
            "deletions": 0,
            "changes": 127,
            "blob_url": "https://github.com/vercel/next.js/blob/4cd3ca26d71fc7904ea7030ee8355071efc9502d/.github%2Factions%2Fnext-stats-action%2Fsrc%2Faggregate-results.js",
            "raw_url": "https://github.com/vercel/next.js/raw/4cd3ca26d71fc7904ea7030ee8355071efc9502d/.github%2Factions%2Fnext-stats-action%2Fsrc%2Faggregate-results.js",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/.github%2Factions%2Fnext-stats-action%2Fsrc%2Faggregate-results.js?ref=4cd3ca26d71fc7904ea7030ee8355071efc9502d",
            "patch": "@@ -0,0 +1,127 @@\n+#!/usr/bin/env node\n+/**\n+ * Aggregates results from sharded stats runs and posts combined comment\n+ *\n+ * Usage: node aggregate-results.js <results-dir>\n+ *\n+ * Expects JSON files named pr-stats-*.json in the results directory\n+ */\n+\n+const path = require('path')\n+const fs = require('fs/promises')\n+const { existsSync } = require('fs')\n+const addComment = require('./add-comment')\n+const logger = require('./util/logger')\n+\n+async function main() {\n+  const resultsDir = process.argv[2] || process.cwd()\n+\n+  logger(`Aggregating results from: ${resultsDir}`)\n+\n+  // Find all pr-stats-*.json files\n+  const files = await fs.readdir(resultsDir)\n+  const statsFiles = files.filter(\n+    (f) => f.startsWith('pr-stats-') && f.endsWith('.json')\n+  )\n+\n+  if (statsFiles.length === 0) {\n+    // This can happen for docs-only changes where stats jobs are skipped\n+    logger('No pr-stats-*.json files found - this may be a docs-only change')\n+    process.exit(0)\n+  }\n+\n+  logger(`Found ${statsFiles.length} results files: ${statsFiles.join(', ')}`)\n+\n+  // Load all results\n+  const allData = []\n+  for (const file of statsFiles) {\n+    const filePath = path.join(resultsDir, file)\n+    try {\n+      const content = await fs.readFile(filePath, 'utf8')\n+      const data = JSON.parse(content)\n+      allData.push(data)\n+      logger(`Loaded ${file} successfully`)\n+    } catch (err) {\n+      logger(`Warning: Failed to load ${file}: ${err.message}`)\n+    }\n+  }\n+\n+  if (allData.length === 0) {\n+    logger('No valid results files could be loaded')\n+    process.exit(1)\n+  }\n+\n+  // Use the first file's actionInfo and statsConfig\n+  const { actionInfo, statsConfig } = allData[0]\n+\n+  // Re-inject the GitHub token from env (it's excluded from JSON serialization for security)\n+  actionInfo.githubToken = process.env.PR_STATS_COMMENT_TOKEN\n+\n+  // Merge results from all files\n+  // Each file has results array with {title, mainRepoStats, diffRepoStats, diffs}\n+  // We need to merge stats objects by combining their keys\n+  const mergedResults = []\n+\n+  // Assume all files have the same number of configs with same titles\n+  const numConfigs = allData[0].results.length\n+\n+  for (let i = 0; i < numConfigs; i++) {\n+    const title = allData[0].results[i].title\n+    const mergedMainRepoStats = {}\n+    const mergedDiffRepoStats = {}\n+    let mergedDiffs = null\n+\n+    for (const data of allData) {\n+      const result = data.results[i]\n+\n+      // Merge mainRepoStats\n+      if (result.mainRepoStats) {\n+        for (const [key, value] of Object.entries(result.mainRepoStats)) {\n+          if (!mergedMainRepoStats[key]) {\n+            mergedMainRepoStats[key] = {}\n+          }\n+          Object.assign(mergedMainRepoStats[key], value)\n+        }\n+      }\n+\n+      // Merge diffRepoStats\n+      if (result.diffRepoStats) {\n+        for (const [key, value] of Object.entries(result.diffRepoStats)) {\n+          if (!mergedDiffRepoStats[key]) {\n+            mergedDiffRepoStats[key] = {}\n+          }\n+          Object.assign(mergedDiffRepoStats[key], value)\n+        }\n+      }\n+\n+      // Merge diffs (just combine all diff objects)\n+      if (result.diffs) {\n+        if (!mergedDiffs) {\n+          mergedDiffs = {}\n+        }\n+        Object.assign(mergedDiffs, result.diffs)\n+      }\n+    }\n+\n+    mergedResults.push({\n+      title,\n+      mainRepoStats: mergedMainRepoStats,\n+      diffRepoStats: mergedDiffRepoStats,\n+      diffs: mergedDiffs,\n+    })\n+  }\n+\n+  logger(\n+    `Merged ${allData.length} result sets into ${mergedResults.length} configs`\n+  )\n+\n+  // Post the combined comment\n+  await addComment(mergedResults, actionInfo, statsConfig)\n+\n+  logger('Aggregation complete')\n+}\n+\n+main().catch((err) => {\n+  console.error('Error aggregating results:', err)\n+  process.exit(1)\n+})"
        },
        {
            "sha": "c35f49384f3d7eaa9c1f715aca4b20b2f9caa7dc",
            "filename": ".github/actions/next-stats-action/src/index.js",
            "status": "modified",
            "additions": 33,
            "deletions": 2,
            "changes": 35,
            "blob_url": "https://github.com/vercel/next.js/blob/4cd3ca26d71fc7904ea7030ee8355071efc9502d/.github%2Factions%2Fnext-stats-action%2Fsrc%2Findex.js",
            "raw_url": "https://github.com/vercel/next.js/raw/4cd3ca26d71fc7904ea7030ee8355071efc9502d/.github%2Factions%2Fnext-stats-action%2Fsrc%2Findex.js",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/.github%2Factions%2Fnext-stats-action%2Fsrc%2Findex.js?ref=4cd3ca26d71fc7904ea7030ee8355071efc9502d",
            "patch": "@@ -13,6 +13,14 @@ const { cloneRepo, mergeBranch, getCommitId, linkPackages, getLastStable } =\n \n const allowedActions = new Set(['synchronize', 'opened'])\n \n+// Get bundler filter from action input (set by GitHub Actions as INPUT_BUNDLER)\n+const bundlerInput = (process.env.INPUT_BUNDLER || 'both').toLowerCase()\n+const isShardedRun = bundlerInput !== 'both'\n+\n+if (isShardedRun) {\n+  logger(`Running in sharded mode for bundler: ${bundlerInput}`)\n+}\n+\n if (!allowedActions.has(actionInfo.actionName) && !actionInfo.isRelease) {\n   logger(\n     `Not running for ${actionInfo.actionName} event action on repo: ${actionInfo.prRepo} and ref ${actionInfo.prRef}`\n@@ -141,14 +149,37 @@ if (!allowedActions.has(actionInfo.actionName) && !actionInfo.isRelease) {\n       else diffRepoPkgPaths = pkgPaths\n     }\n \n-    // run the configs and post the comment\n+    // run the configs and collect results\n     const results = await runConfigs(statsConfig.configs, {\n       statsConfig,\n       mainRepoPkgPaths,\n       diffRepoPkgPaths,\n       relativeStatsAppDir,\n+      bundlerFilter: isShardedRun ? bundlerInput : null,\n     })\n-    await addComment(results, actionInfo, statsConfig)\n+\n+    if (isShardedRun) {\n+      // In sharded mode, save results to JSON for later aggregation\n+      const resultsPath = path.join(\n+        process.env.GITHUB_WORKSPACE || process.cwd(),\n+        `pr-stats-${bundlerInput}.json`\n+      )\n+      // Exclude sensitive fields (githubToken) before serializing to JSON\n+      const { githubToken, ...safeActionInfo } = actionInfo\n+      await fs.writeFile(\n+        resultsPath,\n+        JSON.stringify(\n+          { results, actionInfo: safeActionInfo, statsConfig },\n+          null,\n+          2\n+        )\n+      )\n+      logger(`Saved results to ${resultsPath}`)\n+    } else {\n+      // In non-sharded mode, post comment directly\n+      await addComment(results, actionInfo, statsConfig)\n+    }\n+\n     logger('finished')\n     process.exit(0)\n   } catch (err) {"
        },
        {
            "sha": "725cf4411044b52825d06afbb685d4fa8578f328",
            "filename": ".github/actions/next-stats-action/src/run/collect-stats.js",
            "status": "modified",
            "additions": 394,
            "deletions": 35,
            "changes": 429,
            "blob_url": "https://github.com/vercel/next.js/blob/4cd3ca26d71fc7904ea7030ee8355071efc9502d/.github%2Factions%2Fnext-stats-action%2Fsrc%2Frun%2Fcollect-stats.js",
            "raw_url": "https://github.com/vercel/next.js/raw/4cd3ca26d71fc7904ea7030ee8355071efc9502d/.github%2Factions%2Fnext-stats-action%2Fsrc%2Frun%2Fcollect-stats.js",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/.github%2Factions%2Fnext-stats-action%2Fsrc%2Frun%2Fcollect-stats.js?ref=4cd3ca26d71fc7904ea7030ee8355071efc9502d",
            "patch": "@@ -1,14 +1,179 @@\n const path = require('path')\n+const net = require('net')\n const fs = require('fs/promises')\n+const { existsSync } = require('fs')\n const getPort = require('get-port')\n const fetch = require('node-fetch')\n const glob = require('../util/glob')\n const gzipSize = require('gzip-size')\n const logger = require('../util/logger')\n+const exec = require('../util/exec')\n const { spawn } = require('../util/exec')\n const { parse: urlParse } = require('url')\n const benchmarkUrl = require('./benchmark-url')\n const { statsAppDir, diffingDir, benchTitle } = require('../constants')\n+const { calcStats } = require('../util/stats')\n+\n+// Number of iterations for timing benchmarks to get stable median\n+const BENCHMARK_ITERATIONS = 9\n+\n+// Check if a port is accepting TCP connections\n+function checkPort(port, timeout = 100) {\n+  return new Promise((resolve) => {\n+    const socket = new net.Socket()\n+    socket.setTimeout(timeout)\n+    socket.once('connect', () => {\n+      socket.destroy()\n+      resolve(true)\n+    })\n+    socket.once('timeout', () => {\n+      socket.destroy()\n+      resolve(false)\n+    })\n+    socket.once('error', () => {\n+      socket.destroy()\n+      resolve(false)\n+    })\n+    socket.connect(port, 'localhost')\n+  })\n+}\n+\n+// Wait for port to start accepting TCP connections\n+async function waitForPort(port, timeoutMs = 60000) {\n+  const start = Date.now()\n+  while (Date.now() - start < timeoutMs) {\n+    if (await checkPort(port)) {\n+      return Date.now() - start\n+    }\n+    await new Promise((r) => setTimeout(r, 50))\n+  }\n+  return null\n+}\n+\n+// Wait for HTTP server to respond\n+async function waitForHttp(port, timeoutMs = 60000) {\n+  const start = Date.now()\n+  while (Date.now() - start < timeoutMs) {\n+    try {\n+      const res = await fetch(`http://localhost:${port}/`, { timeout: 2000 })\n+      if (res.ok) {\n+        return Date.now() - start\n+      }\n+    } catch (e) {\n+      // Server not ready yet\n+    }\n+    await new Promise((r) => setTimeout(r, 50))\n+  }\n+  return null\n+}\n+\n+// Run a single dev server boot benchmark\n+async function benchmarkDevBoot(appDevCommand, curDir, port, cleanBuild) {\n+  // Clean .next directory for cold start\n+  if (cleanBuild) {\n+    const nextDir = path.join(curDir, '.next')\n+    await fs.rm(nextDir, { recursive: true, force: true })\n+  }\n+\n+  const startTime = Date.now()\n+  const devChild = spawn(appDevCommand, {\n+    cwd: curDir,\n+    env: {\n+      PORT: port,\n+    },\n+    stdio: 'pipe',\n+  })\n+\n+  let exited = false\n+  devChild.on('exit', () => {\n+    exited = true\n+  })\n+\n+  // Capture output for debugging\n+  devChild.stdout.on('data', (data) => process.stdout.write(data))\n+  devChild.stderr.on('data', (data) => process.stderr.write(data))\n+\n+  // Measure time to port listening (TCP level)\n+  const listenTime = await waitForPort(port, 60000)\n+\n+  // Measure time to HTTP ready\n+  let readyTime = null\n+  if (listenTime !== null && !exited) {\n+    readyTime = await waitForHttp(port, 60000)\n+  }\n+\n+  devChild.kill()\n+\n+  // Wait for process to fully exit to avoid port conflicts on subsequent runs\n+  if (!exited) {\n+    await new Promise((resolve) => {\n+      devChild.on('exit', resolve)\n+      // Timeout after 5 seconds in case process doesn't exit cleanly\n+      setTimeout(resolve, 5000)\n+    })\n+  }\n+\n+  return {\n+    listenTime,\n+    readyTime,\n+  }\n+}\n+\n+// Run multiple iterations of dev boot benchmark and return median times\n+async function benchmarkDevBootWithIterations(\n+  appDevCommand,\n+  curDir,\n+  port,\n+  cleanBuild,\n+  label\n+) {\n+  const listenTimes = []\n+  const readyTimes = []\n+\n+  for (let i = 0; i < BENCHMARK_ITERATIONS; i++) {\n+    // For cold start benchmarks, clean before EVERY iteration to get true cold times\n+    logger(`  ${label} iteration ${i + 1}/${BENCHMARK_ITERATIONS}...`)\n+\n+    const result = await benchmarkDevBoot(\n+      appDevCommand,\n+      curDir,\n+      port,\n+      cleanBuild\n+    )\n+\n+    if (result.listenTime !== null) {\n+      listenTimes.push(result.listenTime)\n+      logger(`    Boot: ${result.listenTime}ms`)\n+    }\n+    if (result.readyTime !== null) {\n+      readyTimes.push(result.readyTime)\n+      logger(`    Ready: ${result.readyTime}ms`)\n+    }\n+\n+    // Small delay between iterations to let system settle\n+    await new Promise((r) => setTimeout(r, 500))\n+  }\n+\n+  const listenStats = calcStats(listenTimes)\n+  const readyStats = calcStats(readyTimes)\n+\n+  // Log detailed stats for debugging\n+  if (listenStats) {\n+    logger(\n+      `  ${label} Boot: median=${listenStats.median}ms, range=${listenStats.min}-${listenStats.max}ms, CV=${listenStats.cv}%`\n+    )\n+  }\n+  if (readyStats) {\n+    logger(\n+      `  ${label} Ready: median=${readyStats.median}ms, range=${readyStats.min}-${readyStats.max}ms, CV=${readyStats.cv}%`\n+    )\n+  }\n+\n+  return {\n+    listenTime: listenStats?.median ?? null,\n+    readyTime: readyStats?.median ?? null,\n+  }\n+}\n \n async function defaultGetRequiredFiles(nextAppDir, fileName) {\n   return [fileName]\n@@ -17,7 +182,10 @@ async function defaultGetRequiredFiles(nextAppDir, fileName) {\n module.exports = async function collectStats(\n   runConfig = {},\n   statsConfig = {},\n-  fromDiff = false\n+  fromDiff = false,\n+  bundlerSuffix = null,\n+  benchmarkOnly = false,\n+  bundlerFilter = null\n ) {\n   const stats = {\n     [benchTitle]: {},\n@@ -27,18 +195,95 @@ module.exports = async function collectStats(\n   }\n   const curDir = fromDiff ? diffingDir : statsAppDir\n \n+  // If bundlerSuffix is provided, we're collecting file sizes only (skip benchmarks)\n+  // If benchmarkOnly is true, we're running benchmarks only (skip file sizes)\n+  const collectFileSizes = !benchmarkOnly\n+  const runBenchmarks = !bundlerSuffix\n+\n   const hasPagesToFetch =\n     Array.isArray(runConfig.pagesToFetch) && runConfig.pagesToFetch.length > 0\n \n   const hasPagesToBench =\n     Array.isArray(runConfig.pagesToBench) && runConfig.pagesToBench.length > 0\n \n+  // Run production start benchmark FIRST (before dev benchmark which cleans .next)\n+  // Only run benchmarks when not collecting bundler-specific file sizes\n+  // Skip production start in sharded mode (bundlerFilter set) as these metrics don't vary by bundler\n   if (\n+    runBenchmarks &&\n     !fromDiff &&\n+    !bundlerFilter &&\n     statsConfig.appStartCommand &&\n     (hasPagesToFetch || hasPagesToBench)\n   ) {\n     const port = await getPort()\n+    const readyTimes = []\n+\n+    // Helper to run a single production start and measure time\n+    async function runProdStartTiming() {\n+      const startTime = Date.now()\n+      const child = spawn(statsConfig.appStartCommand, {\n+        cwd: curDir,\n+        env: {\n+          PORT: port,\n+        },\n+        stdio: 'pipe',\n+      })\n+\n+      let serverReadyResolve\n+      let serverReadyResolved = false\n+      const serverReadyPromise = new Promise((resolve) => {\n+        serverReadyResolve = resolve\n+      })\n+\n+      child.stdout.on('data', (data) => {\n+        if (data.toString().includes('- Local:') && !serverReadyResolved) {\n+          serverReadyResolved = true\n+          serverReadyResolve()\n+        }\n+      })\n+\n+      child.on('exit', () => {\n+        if (!serverReadyResolved) {\n+          serverReadyResolve()\n+          serverReadyResolved = true\n+        }\n+      })\n+\n+      await serverReadyPromise\n+      const readyTime = Date.now() - startTime\n+      child.kill()\n+      await new Promise((r) => setTimeout(r, 300)) // Let port release\n+      return readyTime\n+    }\n+\n+    // Run multiple timing iterations for stable median\n+    logger(\n+      `=== Production Start Benchmark (${BENCHMARK_ITERATIONS} iterations) ===`\n+    )\n+    for (let i = 0; i < BENCHMARK_ITERATIONS; i++) {\n+      logger(`  Prod Start iteration ${i + 1}/${BENCHMARK_ITERATIONS}...`)\n+      const readyTime = await runProdStartTiming()\n+      readyTimes.push(readyTime)\n+      logger(`    Ready: ${readyTime}ms`)\n+    }\n+\n+    const readyStats = calcStats(readyTimes)\n+    if (readyStats) {\n+      logger(\n+        `  Prod Start: median=${readyStats.median}ms, range=${readyStats.min}-${readyStats.max}ms, CV=${readyStats.cv}%`\n+      )\n+\n+      if (!orderedStats['General']) {\n+        orderedStats['General'] = {}\n+      }\n+      orderedStats['General']['nextStartReadyDuration'] = readyStats.median\n+    } else {\n+      logger(`  Prod Start: Failed to collect timing data`)\n+    }\n+\n+    // Now run one more time to do page fetching/benchmarking\n+    logger('=== Production Start for Page Fetching ===')\n     const startTime = Date.now()\n     const child = spawn(statsConfig.appStartCommand, {\n       cwd: curDir,\n@@ -74,11 +319,6 @@ module.exports = async function collectStats(\n     })\n \n     await serverReadyPromise\n-    if (!orderedStats['General']) {\n-      orderedStats['General'] = {}\n-    }\n-    orderedStats['General']['nextStartReadyDuration (ms)'] =\n-      Date.now() - startTime\n \n     if (exitCode !== null) {\n       throw new Error(\n@@ -139,43 +379,162 @@ module.exports = async function collectStats(\n     child.kill()\n   }\n \n-  for (const fileGroup of runConfig.filesToTrack) {\n-    const {\n-      getRequiredFiles = defaultGetRequiredFiles,\n-      name,\n-      globs,\n-    } = fileGroup\n-    const groupStats = {}\n-    const curFiles = new Set()\n+  // Measure dev server boot time if configured\n+  // Runs full matrix: (Turbopack + Webpack) x (Cold + Warm) x (Boot + Ready)\n+  // Each timing uses median of BENCHMARK_ITERATIONS runs for stability\n+  // NOTE: This runs AFTER the production start benchmark because it cleans the .next directory\n+  // Only run benchmarks when not collecting bundler-specific file sizes\n+  if (\n+    runBenchmarks &&\n+    !fromDiff &&\n+    statsConfig.appDevCommand &&\n+    statsConfig.measureDevBoot\n+  ) {\n+    const devPort = await getPort()\n \n-    for (const pattern of globs) {\n-      const results = await glob(pattern, { cwd: curDir, nodir: true })\n-      results.forEach((result) => curFiles.add(result))\n+    if (!orderedStats['General']) {\n+      orderedStats['General'] = {}\n     }\n \n-    for (const file of curFiles) {\n-      const fileKey = path.basename(file)\n-      try {\n-        let parsedSizeSum = 0\n-        let gzipSizeSum = 0\n-        for (const requiredFile of await getRequiredFiles(curDir, file)) {\n-          const absPath = path.join(curDir, requiredFile)\n-          const fileInfo = await fs.stat(absPath)\n-          parsedSizeSum += fileInfo.size\n-          gzipSizeSum += await gzipSize.file(absPath)\n+    // Run benchmarks for selected bundler(s)\n+    // Default is now turbopack, so we need --webpack for webpack\n+    const allBundlers = [\n+      { name: 'Turbopack', flag: '', suffix: 'Turbo' },\n+      { name: 'Webpack', flag: '--webpack', suffix: 'Webpack' },\n+    ]\n+    const bundlers = bundlerFilter\n+      ? allBundlers.filter((b) => b.name.toLowerCase() === bundlerFilter)\n+      : allBundlers\n+\n+    for (const bundler of bundlers) {\n+      logger(`\\n=== ${bundler.name} Dev Server Benchmarks ===`)\n+\n+      // Build the command with the bundler flag\n+      const devCommand = bundler.flag\n+        ? `${statsConfig.appDevCommand} ${bundler.flag}`\n+        : statsConfig.appDevCommand\n+\n+      // 1. Cold start benchmark (clean .next directory, multiple iterations)\n+      logger(\n+        `=== ${bundler.name} Cold Start (${BENCHMARK_ITERATIONS} iterations) ===`\n+      )\n+      const coldResult = await benchmarkDevBootWithIterations(\n+        devCommand,\n+        curDir,\n+        devPort,\n+        true, // clean .next before each iteration\n+        `${bundler.name} Cold`\n+      )\n+\n+      if (coldResult.listenTime !== null) {\n+        orderedStats['General'][`nextDevColdListenDuration${bundler.suffix}`] =\n+          coldResult.listenTime\n+      }\n+      if (coldResult.readyTime !== null) {\n+        orderedStats['General'][`nextDevColdReadyDuration${bundler.suffix}`] =\n+          coldResult.readyTime\n+      }\n+\n+      // 2. Warm up bytecode cache by running server for ~10 seconds\n+      if (coldResult.readyTime !== null) {\n+        logger(`=== ${bundler.name} Warming up bytecode cache (10s) ===`)\n+        const warmupChild = spawn(devCommand, {\n+          cwd: curDir,\n+          env: {\n+            PORT: devPort,\n+          },\n+          stdio: 'pipe',\n+        })\n+\n+        let warmupExited = false\n+        warmupChild.on('exit', () => {\n+          warmupExited = true\n+        })\n+\n+        // Wait for server to be ready\n+        await waitForHttp(devPort, 60000)\n+\n+        // Let it run for 10 seconds to warm bytecode cache\n+        await new Promise((r) => setTimeout(r, 10000))\n+\n+        warmupChild.kill()\n+\n+        // Wait for warmup server to fully exit to avoid port conflicts\n+        if (!warmupExited) {\n+          await new Promise((resolve) => {\n+            warmupChild.on('exit', resolve)\n+            // Timeout after 5 seconds in case process doesn't exit cleanly\n+            setTimeout(resolve, 5000)\n+          })\n+        }\n+\n+        // 3. Warm start benchmark (keep .next directory, multiple iterations)\n+        logger(\n+          `=== ${bundler.name} Warm Start (${BENCHMARK_ITERATIONS} iterations) ===`\n+        )\n+        const warmResult = await benchmarkDevBootWithIterations(\n+          devCommand,\n+          curDir,\n+          devPort,\n+          false, // keep build\n+          `${bundler.name} Warm`\n+        )\n+\n+        if (warmResult.listenTime !== null) {\n+          orderedStats['General'][\n+            `nextDevWarmListenDuration${bundler.suffix}`\n+          ] = warmResult.listenTime\n+        }\n+        if (warmResult.readyTime !== null) {\n+          orderedStats['General'][`nextDevWarmReadyDuration${bundler.suffix}`] =\n+            warmResult.readyTime\n         }\n-        groupStats[fileKey] = parsedSizeSum\n-        groupStats[`${fileKey} gzip`] = gzipSizeSum\n-      } catch (err) {\n-        logger.error('Failed to get file stats', err)\n       }\n     }\n-    stats[name] = groupStats\n+\n+    logger('\\n=== Dev Boot Benchmark Complete ===')\n   }\n \n-  for (const fileGroup of runConfig.filesToTrack) {\n-    const { name } = fileGroup\n-    orderedStats[name] = stats[name]\n+  // Collect file sizes only when not in benchmark-only mode\n+  if (collectFileSizes) {\n+    for (const fileGroup of runConfig.filesToTrack) {\n+      const {\n+        getRequiredFiles = defaultGetRequiredFiles,\n+        name,\n+        globs,\n+      } = fileGroup\n+      const groupStats = {}\n+      const curFiles = new Set()\n+\n+      for (const pattern of globs) {\n+        const results = await glob(pattern, { cwd: curDir, nodir: true })\n+        results.forEach((result) => curFiles.add(result))\n+      }\n+\n+      for (const file of curFiles) {\n+        const fileKey = path.basename(file)\n+        try {\n+          let parsedSizeSum = 0\n+          let gzipSizeSum = 0\n+          for (const requiredFile of await getRequiredFiles(curDir, file)) {\n+            const absPath = path.join(curDir, requiredFile)\n+            const fileInfo = await fs.stat(absPath)\n+            parsedSizeSum += fileInfo.size\n+            gzipSizeSum += await gzipSize.file(absPath)\n+          }\n+          groupStats[fileKey] = parsedSizeSum\n+          groupStats[`${fileKey} gzip`] = gzipSizeSum\n+        } catch (err) {\n+          logger.error('Failed to get file stats', err)\n+        }\n+      }\n+      stats[name] = groupStats\n+    }\n+\n+    for (const fileGroup of runConfig.filesToTrack) {\n+      const { name } = fileGroup\n+      orderedStats[name] = stats[name]\n+    }\n   }\n \n   if (stats[benchTitle]) {"
        },
        {
            "sha": "89e261d0807b5d138cea459fbf2608cd5376a242",
            "filename": ".github/actions/next-stats-action/src/run/index.js",
            "status": "modified",
            "additions": 132,
            "deletions": 30,
            "changes": 162,
            "blob_url": "https://github.com/vercel/next.js/blob/4cd3ca26d71fc7904ea7030ee8355071efc9502d/.github%2Factions%2Fnext-stats-action%2Fsrc%2Frun%2Findex.js",
            "raw_url": "https://github.com/vercel/next.js/raw/4cd3ca26d71fc7904ea7030ee8355071efc9502d/.github%2Factions%2Fnext-stats-action%2Fsrc%2Frun%2Findex.js",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/.github%2Factions%2Fnext-stats-action%2Fsrc%2Frun%2Findex.js?ref=4cd3ca26d71fc7904ea7030ee8355071efc9502d",
            "patch": "@@ -7,12 +7,43 @@ const getDirSize = require('./get-dir-size')\n const collectStats = require('./collect-stats')\n const collectDiffs = require('./collect-diffs')\n const { statsAppDir, diffRepoDir } = require('../constants')\n+const { calcStats } = require('../util/stats')\n+\n+// Number of iterations for build benchmarks to get stable median\n+const BUILD_BENCHMARK_ITERATIONS = 5\n+\n+// Bundler configurations for dual-bundler benchmarking\n+const BUNDLERS = [\n+  { name: 'Webpack', flag: '--webpack', suffix: 'Webpack' },\n+  { name: 'Turbopack', flag: '', suffix: 'Turbo' },\n+]\n \n async function runConfigs(\n   configs = [],\n-  { statsConfig, relativeStatsAppDir, mainRepoPkgPaths, diffRepoPkgPaths },\n+  {\n+    statsConfig,\n+    relativeStatsAppDir,\n+    mainRepoPkgPaths,\n+    diffRepoPkgPaths,\n+    bundlerFilter = null,\n+  },\n   diffing = false\n ) {\n+  // Filter bundlers based on input\n+  const bundlersToRun = bundlerFilter\n+    ? BUNDLERS.filter((b) => b.name.toLowerCase() === bundlerFilter)\n+    : BUNDLERS\n+\n+  if (bundlerFilter && bundlersToRun.length === 0) {\n+    throw new Error(\n+      `Invalid bundler filter: ${bundlerFilter}. Must be 'webpack' or 'turbopack'`\n+    )\n+  }\n+\n+  logger(\n+    `Running benchmarks for bundlers: ${bundlersToRun.map((b) => b.name).join(', ')}`\n+  )\n+\n   const results = []\n \n   for (const config of configs) {\n@@ -25,8 +56,6 @@ async function runConfigs(\n     for (const pkgPaths of [mainRepoPkgPaths, diffRepoPkgPaths]) {\n       let curStats = {\n         General: {\n-          buildDuration: null,\n-          buildDurationCached: null,\n           nodeModulesSize: null,\n         },\n       }\n@@ -56,31 +85,109 @@ async function runConfigs(\n         )\n       }\n \n-      const buildStart = Date.now()\n-      console.log(\n-        await exec(`cd ${statsAppDir} && ${statsConfig.appBuildCommand}`, false)\n-      )\n-      curStats.General.buildDuration = Date.now() - buildStart\n-\n-      // apply renames to get deterministic output names\n-      for (const rename of config.renames) {\n-        const results = await glob(rename.srcGlob, { cwd: statsAppDir })\n-        for (const result of results) {\n-          let dest = rename.removeHash\n-            ? result.replace(/(\\.|-)[0-9a-f]{16}(\\.|-)/g, '$1HASH$2')\n-            : rename.dest\n-          if (result === dest) continue\n-          await fs.rename(\n-            path.join(statsAppDir, result),\n-            path.join(statsAppDir, dest)\n+      // Run builds for selected bundler(s) and collect stats separately\n+      for (const bundler of bundlersToRun) {\n+        logger(`\\n=== ${bundler.name} Production Build ===`)\n+\n+        // Build base command without --webpack flag (we add it per bundler)\n+        const baseBuildCommand = statsConfig.appBuildCommand.replace(\n+          / --webpack/g,\n+          ''\n+        )\n+        const buildCommand = bundler.flag\n+          ? `${baseBuildCommand} ${bundler.flag}`\n+          : baseBuildCommand\n+\n+        // Run multiple fresh build iterations for stable timing\n+        const freshBuildTimes = []\n+        logger(`  Fresh build (${BUILD_BENCHMARK_ITERATIONS} iterations)...`)\n+        for (let i = 0; i < BUILD_BENCHMARK_ITERATIONS; i++) {\n+          // Clean .next directory for fresh build\n+          await fs.rm(path.join(statsAppDir, '.next'), {\n+            recursive: true,\n+            force: true,\n+          })\n+\n+          const buildStart = Date.now()\n+          console.log(await exec(`cd ${statsAppDir} && ${buildCommand}`, false))\n+          const buildDuration = Date.now() - buildStart\n+          freshBuildTimes.push(buildDuration)\n+          logger(`    Iteration ${i + 1}: ${buildDuration}ms`)\n+        }\n+\n+        const freshStats = calcStats(freshBuildTimes)\n+        logger(\n+          `  Fresh build: median=${freshStats.median}ms, range=${freshStats.min}-${freshStats.max}ms`\n+        )\n+        curStats.General[`buildDuration${bundler.suffix}`] = freshStats.median\n+\n+        // Run cached build iterations BEFORE renames (renames invalidate cache)\n+        const cachedBuildTimes = []\n+        logger(`  Cached build (${BUILD_BENCHMARK_ITERATIONS} iterations)...`)\n+        for (let i = 0; i < BUILD_BENCHMARK_ITERATIONS; i++) {\n+          const buildStart = Date.now()\n+          console.log(await exec(`cd ${statsAppDir} && ${buildCommand}`, false))\n+          const buildDuration = Date.now() - buildStart\n+          cachedBuildTimes.push(buildDuration)\n+          logger(`    Iteration ${i + 1}: ${buildDuration}ms`)\n+        }\n+\n+        const cachedStats = calcStats(cachedBuildTimes)\n+        logger(\n+          `  Cached build: median=${cachedStats.median}ms, range=${cachedStats.min}-${cachedStats.max}ms`\n+        )\n+        curStats.General[`buildDurationCached${bundler.suffix}`] =\n+          cachedStats.median\n+\n+        // Apply renames to get deterministic output names (after cached builds)\n+        for (const rename of config.renames) {\n+          const renameResults = await glob(rename.srcGlob, { cwd: statsAppDir })\n+          for (const result of renameResults) {\n+            let dest = rename.removeHash\n+              ? result.replace(/(\\.|-)[0-9a-f]{16}(\\.|-)/g, '$1HASH$2')\n+              : rename.dest\n+            if (result === dest) continue\n+            try {\n+              await fs.rename(\n+                path.join(statsAppDir, result),\n+                path.join(statsAppDir, dest)\n+              )\n+            } catch (e) {\n+              // File may not exist for this bundler\n+            }\n+          }\n+        }\n+\n+        // Collect file stats for this bundler (after renames for deterministic names)\n+        const collectedStats = await collectStats(\n+          config,\n+          statsConfig,\n+          false,\n+          bundler.suffix\n+        )\n+\n+        for (const key of Object.keys(collectedStats)) {\n+          // Prefix group names with bundler suffix (except General which is shared)\n+          const groupKey = key === 'General' ? key : `${key} (${bundler.name})`\n+          curStats[groupKey] = Object.assign(\n+            {},\n+            curStats[groupKey],\n+            collectedStats[key]\n           )\n         }\n       }\n \n-      const collectedStats = await collectStats(config, statsConfig)\n-\n-      for (const key of Object.keys(collectedStats)) {\n-        curStats[key] = Object.assign({}, curStats[key], collectedStats[key])\n+      // Run benchmarks for selected bundler(s) - dev boot and prod start\n+      const benchmarkStats = await collectStats(\n+        config,\n+        statsConfig,\n+        false,\n+        null,\n+        true,\n+        bundlerFilter\n+      )\n+      for (const key of Object.keys(benchmarkStats)) {\n+        curStats[key] = Object.assign({}, curStats[key], benchmarkStats[key])\n       }\n \n       const applyRenames = (renames, stats) => {\n@@ -130,6 +237,7 @@ async function runConfigs(\n                   mainRepoPkgPaths,\n                   diffRepoPkgPaths,\n                   relativeStatsAppDir,\n+                  bundlerFilter,\n                 },\n                 true\n               )\n@@ -150,12 +258,6 @@ async function runConfigs(\n         /* eslint-disable-next-line */\n         mainRepoStats = curStats\n       }\n-\n-      const secondBuildStart = Date.now()\n-      console.log(\n-        await exec(`cd ${statsAppDir} && ${statsConfig.appBuildCommand}`, false)\n-      )\n-      curStats.General.buildDurationCached = Date.now() - secondBuildStart\n     }\n \n     logger(`Finished running: ${config.title}`)"
        },
        {
            "sha": "31a22c7360f192c52216301bd6446d81f9e19559",
            "filename": ".github/actions/next-stats-action/src/util/stats.js",
            "status": "added",
            "additions": 35,
            "deletions": 0,
            "changes": 35,
            "blob_url": "https://github.com/vercel/next.js/blob/4cd3ca26d71fc7904ea7030ee8355071efc9502d/.github%2Factions%2Fnext-stats-action%2Fsrc%2Futil%2Fstats.js",
            "raw_url": "https://github.com/vercel/next.js/raw/4cd3ca26d71fc7904ea7030ee8355071efc9502d/.github%2Factions%2Fnext-stats-action%2Fsrc%2Futil%2Fstats.js",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/.github%2Factions%2Fnext-stats-action%2Fsrc%2Futil%2Fstats.js?ref=4cd3ca26d71fc7904ea7030ee8355071efc9502d",
            "patch": "@@ -0,0 +1,35 @@\n+/**\n+ * Shared statistics utilities for benchmark measurements\n+ */\n+\n+/**\n+ * Calculate statistical summary for an array of numbers\n+ * @param {number[]} arr - Array of numeric values\n+ * @returns {Object|null} Stats object with median, min, max, mean, stddev, cv or null if empty\n+ */\n+function calcStats(arr) {\n+  if (arr.length === 0) return null\n+\n+  const sorted = [...arr].sort((a, b) => a - b)\n+  const mid = Math.floor(sorted.length / 2)\n+  const median =\n+    sorted.length % 2 ? sorted[mid] : (sorted[mid - 1] + sorted[mid]) / 2\n+  const min = sorted[0]\n+  const max = sorted[sorted.length - 1]\n+  const mean = arr.reduce((a, b) => a + b, 0) / arr.length\n+  const variance =\n+    arr.reduce((sum, val) => sum + Math.pow(val - mean, 2), 0) / arr.length\n+  const stddev = Math.sqrt(variance)\n+  const cv = mean > 0 ? (stddev / mean) * 100 : 0 // coefficient of variation as %\n+\n+  return {\n+    median,\n+    min,\n+    max,\n+    mean: Math.round(mean),\n+    stddev: Math.round(stddev),\n+    cv: Math.round(cv),\n+  }\n+}\n+\n+module.exports = { calcStats }"
        },
        {
            "sha": "00f41f7a1e0a75ab3634c0899e3c229792d331b3",
            "filename": ".github/actions/next-stats-action/test-local.js",
            "status": "added",
            "additions": 121,
            "deletions": 0,
            "changes": 121,
            "blob_url": "https://github.com/vercel/next.js/blob/4cd3ca26d71fc7904ea7030ee8355071efc9502d/.github%2Factions%2Fnext-stats-action%2Ftest-local.js",
            "raw_url": "https://github.com/vercel/next.js/raw/4cd3ca26d71fc7904ea7030ee8355071efc9502d/.github%2Factions%2Fnext-stats-action%2Ftest-local.js",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/.github%2Factions%2Fnext-stats-action%2Ftest-local.js?ref=4cd3ca26d71fc7904ea7030ee8355071efc9502d",
            "patch": "@@ -0,0 +1,121 @@\n+#!/usr/bin/env node\n+/**\n+ * Test the stats action comment formatting locally\n+ *\n+ * Usage: node test-local.js [--with-history]\n+ *\n+ * This generates a sample PR comment using mock data so you can\n+ * quickly verify formatting changes without running the full action.\n+ *\n+ * Options:\n+ *   --with-history  Simulate KV history data to test trend sparklines\n+ */\n+\n+const addComment = require('./src/add-comment')\n+\n+const withHistory = process.argv.includes('--with-history')\n+\n+// Mock data simulating real benchmark results\n+const mockResults = [\n+  {\n+    title: 'Default Build',\n+    mainRepoStats: {\n+      General: {\n+        nextDevColdListenDurationTurbo: 280,\n+        nextDevColdReadyDurationTurbo: 450,\n+        nextDevWarmListenDurationTurbo: 180,\n+        nextDevWarmReadyDurationTurbo: 320,\n+        nextDevColdListenDurationWebpack: 350,\n+        nextDevColdReadyDurationWebpack: 1200,\n+        nextDevWarmListenDurationWebpack: 250,\n+        nextDevWarmReadyDurationWebpack: 800,\n+        nextStartReadyDuration: 150,\n+        buildDurationTurbo: 4500,\n+        buildDurationCachedTurbo: 4200,\n+        buildDurationWebpack: 14000,\n+        buildDurationCachedWebpack: 13500,\n+        nodeModulesSize: 250000000,\n+      },\n+    },\n+    diffRepoStats: {\n+      General: {\n+        nextDevColdListenDurationTurbo: 290, // +10ms (insignificant)\n+        nextDevColdReadyDurationTurbo: 520, // +70ms at 15% (significant regression)\n+        nextDevWarmListenDurationTurbo: 175, // -5ms (insignificant)\n+        nextDevWarmReadyDurationTurbo: 280, // -40ms at 12% (significant improvement)\n+        nextDevColdListenDurationWebpack: 360,\n+        nextDevColdReadyDurationWebpack: 1180,\n+        nextDevWarmListenDurationWebpack: 245,\n+        nextDevWarmReadyDurationWebpack: 790,\n+        nextStartReadyDuration: 145,\n+        buildDurationTurbo: 4400,\n+        buildDurationCachedTurbo: 4250,\n+        buildDurationWebpack: 13800,\n+        buildDurationCachedWebpack: 13600,\n+        nodeModulesSize: 251000000,\n+      },\n+    },\n+    diffs: null,\n+  },\n+]\n+\n+const mockActionInfo = {\n+  isRelease: false,\n+  commitId: 'abc123',\n+  issueId: 12345,\n+}\n+\n+const mockStatsConfig = {\n+  commentHeading: 'Stats from current PR',\n+}\n+\n+// Run with LOCAL_STATS to output to file instead of posting\n+process.env.LOCAL_STATS = 'true'\n+\n+// Mock the KV module to provide fake history if --with-history is passed\n+if (withHistory) {\n+  // Generate mock history entries showing a trend\n+  const mockHistory = []\n+  for (let i = 0; i < 10; i++) {\n+    mockHistory.push({\n+      commitId: `commit-${i}`,\n+      timestamp: new Date(Date.now() - i * 86400000).toISOString(),\n+      metrics: {\n+        nextDevColdReadyDurationTurbo: 400 + Math.random() * 100, // varies 400-500\n+        nextDevWarmReadyDurationTurbo: 300 + Math.random() * 50,\n+        buildDurationTurbo: 4000 + Math.random() * 1000,\n+      },\n+    })\n+  }\n+\n+  // Inject mock KV client\n+  process.env.KV_REST_API_URL = 'mock://kv'\n+  process.env.KV_REST_API_TOKEN = 'mock-token'\n+\n+  // Patch the require to intercept @vercel/kv\n+  const Module = require('module')\n+  const originalRequire = Module.prototype.require\n+  Module.prototype.require = function (id) {\n+    if (id === '@vercel/kv') {\n+      return {\n+        createClient: () => ({\n+          lrange: async () => mockHistory,\n+          rpush: async () => {},\n+          ltrim: async () => {},\n+        }),\n+      }\n+    }\n+    return originalRequire.apply(this, arguments)\n+  }\n+\n+  console.log('Running with mock history data (10 entries)')\n+}\n+\n+addComment(mockResults, mockActionInfo, mockStatsConfig)\n+  .then(() =>\n+    console.log(\n+      '\\nGenerated pr-stats.md - open it to see the comment' +\n+        (withHistory ? ' (with trend sparklines)' : '')\n+    )\n+  )\n+  .catch(console.error)"
        },
        {
            "sha": "f785e950d619b4e0a835af712463815abb9cc986",
            "filename": ".github/pnpm-lock.yaml",
            "status": "modified",
            "additions": 1253,
            "deletions": 826,
            "changes": 2079,
            "blob_url": "https://github.com/vercel/next.js/blob/4cd3ca26d71fc7904ea7030ee8355071efc9502d/.github%2Fpnpm-lock.yaml",
            "raw_url": "https://github.com/vercel/next.js/raw/4cd3ca26d71fc7904ea7030ee8355071efc9502d/.github%2Fpnpm-lock.yaml",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/.github%2Fpnpm-lock.yaml?ref=4cd3ca26d71fc7904ea7030ee8355071efc9502d"
        },
        {
            "sha": "e3844178c486407a53c98035dd957088699c7b87",
            "filename": ".github/workflows/pull_request_stats.yml",
            "status": "modified",
            "additions": 53,
            "deletions": 4,
            "changes": 57,
            "blob_url": "https://github.com/vercel/next.js/blob/4cd3ca26d71fc7904ea7030ee8355071efc9502d/.github%2Fworkflows%2Fpull_request_stats.yml",
            "raw_url": "https://github.com/vercel/next.js/raw/4cd3ca26d71fc7904ea7030ee8355071efc9502d/.github%2Fworkflows%2Fpull_request_stats.yml",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/.github%2Fworkflows%2Fpull_request_stats.yml?ref=4cd3ca26d71fc7904ea7030ee8355071efc9502d",
            "patch": "@@ -36,9 +36,13 @@ jobs:\n       uploadAnalyzerArtifacts: 'yes'\n \n   stats:\n-    name: PR Stats\n+    name: PR Stats (${{ matrix.bundler }})\n     needs: build\n     timeout-minutes: 25\n+    strategy:\n+      fail-fast: false\n+      matrix:\n+        bundler: [webpack, turbopack]\n     runs-on:\n       - 'self-hosted'\n       - 'linux'\n@@ -64,7 +68,52 @@ jobs:\n \n       - uses: ./.github/actions/next-stats-action\n         if: ${{ steps.docs-change.outputs.DOCS_CHANGE == 'nope' }}\n+        with:\n+          bundler: ${{ matrix.bundler }}\n+        env:\n+          PR_STATS_COMMENT_TOKEN: ${{ secrets.PR_STATS_COMMENT_TOKEN }}\n+          TURBO_TEAM: ${{ env.TURBO_TEAM }}\n+          TURBO_TOKEN: ${{ secrets.TURBO_TOKEN }}\n+          TURBO_CACHE: ${{ env.TURBO_CACHE }}\n+\n+      - name: Upload stats results\n+        if: ${{ steps.docs-change.outputs.DOCS_CHANGE == 'nope' }}\n+        uses: actions/upload-artifact@v4\n+        with:\n+          name: pr-stats-${{ matrix.bundler }}\n+          path: pr-stats-${{ matrix.bundler }}.json\n+          retention-days: 1\n+\n+  stats-aggregate:\n+    name: Aggregate PR Stats\n+    needs: stats\n+    if: always() && needs.stats.result != 'cancelled'\n+    runs-on: ubuntu-latest\n+    steps:\n+      - uses: actions/checkout@v4\n+        with:\n+          fetch-depth: 1\n+\n+      - name: Download all stats artifacts\n+        uses: actions/download-artifact@v4\n+        with:\n+          pattern: pr-stats-*\n+          path: stats-results\n+          merge-multiple: true\n+\n+      - name: Setup Node.js\n+        uses: actions/setup-node@v4\n+        with:\n+          node-version: ${{ env.NODE_LTS_VERSION }}\n+\n+      - name: Install dependencies\n+        working-directory: .github/actions/next-stats-action\n+        run: npm install\n+\n+      - name: Aggregate and post results\n+        working-directory: .github/actions/next-stats-action\n+        run: node src/aggregate-results.js ${{ github.workspace }}/stats-results\n         env:\n-          # This uses the webpack bundle analyzer and for consistent results we need to use webpack.\n-          # Once there is an equivalent analyzer for turbopack, we can remove this.\n-          IS_WEBPACK_TEST: 1\n+          PR_STATS_COMMENT_TOKEN: ${{ secrets.PR_STATS_COMMENT_TOKEN }}\n+          KV_REST_API_URL: ${{ secrets.KV_REST_API_URL }}\n+          KV_REST_API_TOKEN: ${{ secrets.KV_REST_API_TOKEN }}"
        },
        {
            "sha": "51c3550905b3609bef892865a03930fd99e1f2e7",
            "filename": "scripts/test-stats-benchmark.sh",
            "status": "added",
            "additions": 113,
            "deletions": 0,
            "changes": 113,
            "blob_url": "https://github.com/vercel/next.js/blob/4cd3ca26d71fc7904ea7030ee8355071efc9502d/scripts%2Ftest-stats-benchmark.sh",
            "raw_url": "https://github.com/vercel/next.js/raw/4cd3ca26d71fc7904ea7030ee8355071efc9502d/scripts%2Ftest-stats-benchmark.sh",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/scripts%2Ftest-stats-benchmark.sh?ref=4cd3ca26d71fc7904ea7030ee8355071efc9502d",
            "patch": "@@ -0,0 +1,113 @@\n+#!/bin/bash\n+# Quick sanity check for stats benchmark config\n+# Tests that the dev server can start with the new config\n+set -e\n+\n+REPO_ROOT=\"$(cd \"$(dirname \"$0\")/..\" && pwd)\"\n+cd \"$REPO_ROOT\"\n+\n+echo \"=== Stats Benchmark Config Test ===\"\n+\n+# Check Next.js is built\n+if [ ! -d \"packages/next/dist\" ]; then\n+  echo \"ERROR: Next.js not built. Run 'pnpm build' first.\"\n+  exit 1\n+fi\n+\n+# Create temp test app\n+WORK_DIR=$(mktemp -d)\n+trap \"rm -rf $WORK_DIR\" EXIT\n+\n+echo \"Setting up test app in $WORK_DIR...\"\n+cp -r test/.stats-app/* \"$WORK_DIR/\"\n+cd \"$WORK_DIR\"\n+\n+# Write the config that stats-config.js would write (with turbopack: {})\n+cat > next.config.js << 'EOF'\n+module.exports = {\n+  generateBuildId: () => 'BUILD_ID',\n+  turbopack: {},\n+}\n+EOF\n+\n+echo \"Config:\"\n+cat next.config.js\n+echo \"\"\n+\n+# Link local Next.js\n+node -e \"\n+const fs = require('fs');\n+const pkg = JSON.parse(fs.readFileSync('package.json'));\n+pkg.dependencies.next = 'file:$REPO_ROOT/packages/next';\n+fs.writeFileSync('package.json', JSON.stringify(pkg, null, 2));\n+\"\n+echo \"Installing dependencies...\"\n+pnpm install --ignore-scripts 2>/dev/null\n+\n+# Test Turbopack dev (the failing scenario)\n+echo \"\"\n+echo \"=== Test 1: Turbopack dev (default, no flag) ===\"\n+PORT=$(python3 -c \"import socket; s=socket.socket(); s.bind(('',0)); print(s.getsockname()[1]); s.close()\")\n+\n+rm -rf .next\n+NEXT_TELEMETRY_DISABLED=1 timeout 30 pnpm next dev --port $PORT > /tmp/turbo.log 2>&1 &\n+PID=$!\n+sleep 12\n+\n+if kill -0 $PID 2>/dev/null; then\n+  echo \"OK: Turbopack dev server is running\"\n+  kill $PID 2>/dev/null || true\n+  wait $PID 2>/dev/null || true\n+else\n+  wait $PID 2>/dev/null || CODE=$?\n+  if [ \"$CODE\" = \"1\" ]; then\n+    echo \"FAIL: Turbopack dev crashed (exit 1)\"\n+    echo \"Output:\"\n+    cat /tmp/turbo.log\n+    exit 1\n+  fi\n+  echo \"OK: Process exited (timeout or normal)\"\n+fi\n+\n+# Test Webpack dev\n+echo \"\"\n+echo \"=== Test 2: Webpack dev (--webpack flag) ===\"\n+PORT=$(python3 -c \"import socket; s=socket.socket(); s.bind(('',0)); print(s.getsockname()[1]); s.close()\")\n+\n+rm -rf .next\n+NEXT_TELEMETRY_DISABLED=1 timeout 30 pnpm next dev --webpack --port $PORT > /tmp/webpack.log 2>&1 &\n+PID=$!\n+sleep 12\n+\n+if kill -0 $PID 2>/dev/null; then\n+  echo \"OK: Webpack dev server is running\"\n+  kill $PID 2>/dev/null || true\n+  wait $PID 2>/dev/null || true\n+else\n+  echo \"OK: Process exited (timeout or normal)\"\n+fi\n+\n+# Test Turbopack build\n+echo \"\"\n+echo \"=== Test 3: Turbopack build ===\"\n+rm -rf .next\n+if NEXT_TELEMETRY_DISABLED=1 pnpm next build 2>&1 | head -20; then\n+  echo \"OK: Turbopack build completed\"\n+else\n+  echo \"FAIL: Turbopack build failed\"\n+  exit 1\n+fi\n+\n+# Test Webpack build\n+echo \"\"\n+echo \"=== Test 4: Webpack build (--webpack flag) ===\"\n+rm -rf .next\n+if NEXT_TELEMETRY_DISABLED=1 pnpm next build --webpack 2>&1 | head -20; then\n+  echo \"OK: Webpack build completed\"\n+else\n+  echo \"FAIL: Webpack build failed\"\n+  exit 1\n+fi\n+\n+echo \"\"\n+echo \"=== All tests passed ===\""
        },
        {
            "sha": "574af9030b20b39ec17e5be807321ef0d83d5e8b",
            "filename": "test/.stats-app/stats-config.js",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/vercel/next.js/blob/4cd3ca26d71fc7904ea7030ee8355071efc9502d/test%2F.stats-app%2Fstats-config.js",
            "raw_url": "https://github.com/vercel/next.js/raw/4cd3ca26d71fc7904ea7030ee8355071efc9502d/test%2F.stats-app%2Fstats-config.js",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/test%2F.stats-app%2Fstats-config.js?ref=4cd3ca26d71fc7904ea7030ee8355071efc9502d",
            "patch": "@@ -3,7 +3,7 @@ const path = require('path')\n \n const clientGlobs = [\n   {\n-    name: 'Client Bundles (main, webpack)',\n+    name: 'Client Bundles (main)',\n     globs: [\n       '.next/static/runtime/+(main|webpack)-*',\n       '.next/static/chunks/!(polyfills*)',\n@@ -109,9 +109,10 @@ const renames = [\n module.exports = {\n   commentHeading: 'Stats from current PR',\n   commentReleaseHeading: 'Stats from current release',\n-  appBuildCommand: 'NEXT_TELEMETRY_DISABLED=1 pnpm next build',\n+  appBuildCommand: 'NEXT_TELEMETRY_DISABLED=1 pnpm next build --webpack',\n   appStartCommand: 'NEXT_TELEMETRY_DISABLED=1 pnpm next start --port $PORT',\n-  appDevCommand: 'NEXT_TELEMETRY_DISABLED=1 pnpm next --port $PORT',\n+  appDevCommand: 'NEXT_TELEMETRY_DISABLED=1 pnpm next dev --port $PORT',\n+  measureDevBoot: true,\n   mainRepo: 'vercel/next.js',\n   mainBranch: 'canary',\n   autoMergeMain: true,\n@@ -125,6 +126,7 @@ module.exports = {\n           content: `\n             module.exports = {\n               generateBuildId: () => 'BUILD_ID',\n+              turbopack: {},\n               webpack(config) {\n                 config.optimization.minimize = false\n                 config.optimization.minimizer = undefined\n@@ -141,7 +143,8 @@ module.exports = {\n           path: 'next.config.js',\n           content: `\n           module.exports = {\n-              generateBuildId: () => 'BUILD_ID'\n+              generateBuildId: () => 'BUILD_ID',\n+              turbopack: {},\n             }\n           `,\n         },"
        }
    ],
    "stats": {
        "total": 4370,
        "additions": 3291,
        "deletions": 1079
    }
}