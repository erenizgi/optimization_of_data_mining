{
    "author": "lukesandberg",
    "message": "[turbopack] Track task durations in the task_statistics file (#83522)\n\n# Track task execution duration in TaskStatistics\n\n## What?\nThis PR adds tracking of task execution duration in Turbopack's task system, enabling better performance analysis and optimization.\n\n## Why?\nUnderstanding how long tasks take to execute helps identify optimization opportunities, especially for determining whether caching is beneficial for specific tasks.\n\n## How?\n- Added `track_task_duration` method to record execution time for tasks\n- Updated task statistics to track execution count and duration\n- Added a Python script `analyze_cache_effectiveness.py` to identify tasks that would benefit from removing caching\n- Updated tests to account for the new statistics fields\n\nSample output from the script\n```\nTasks ranked by estimated time savings from removing caching layer\n\nSavings    Hit Rate Exec Time  Operations Task Name\n---------------------------------------------------\n2.33s      39.5%    765ns      661,176    turbopack-ecmascript@turbopack_ecmascript::references::esm::base::EsmAssetReference::ChunkableModuleReference::chunking_type\n2.29s      18.5%    1.6μs      490,488    turbopack-ecmascript@turbopack_ecmascript::references::esm::base::EsmAssetReference::ChunkableModuleReference::export_usage\n1.99s      9.0%     9.8μs      430,149    turbopack@turbopack::ModuleAssetContext::AssetContext::resolve_asset\n1.17s      51.7%    1.2μs      462,164    turbopack-ecmascript@turbopack_ecmascript::EcmascriptModuleAsset::ResolveOrigin::get_inner_asset\n1.10s      54.1%    1.2μs      462,387    turbopack-core@turbopack_core::resolve::ModuleResolveResult::is_unresolvable\n916.01ms   0.0%     19.2μs     152,669    turbopack@turbopack::apply_module_type\n807.37ms   74.5%    1.1μs      722,106    turbopack-ecmascript@turbopack_ecmascript::references::esm::base::ReferencedAsset::from_resolve_result\n782.16ms   69.7%    1.5μs      680,828    turbopack-core@turbopack_core::resolve::ModuleResolveResult::primary_modules\n749.54ms   4.0%     80ns       129,625    turbopack-core@turbopack_core::ident::AssetIdent::new_inner\n717.59ms   94.5%    5ns        887,040    turbopack-ecmascript@turbopack_ecmascript::EcmascriptModuleAsset::ResolveOrigin::asset_context\n522.31ms   30.2%    1.7μs      136,180    turbopack-core@turbopack_core::resolve::ResolveResult::is_unresolvable\n452.88ms   0.0%     5.2μs      75,484     next-core@next_core::next_server::resolve::ExternalCjsModulesResolvePlugin::AfterResolvePlugin::after_resolve\n415.54ms   45.2%    937ns      134,377    turbopack-core@turbopack_core::resolve::pattern::Pattern::new_internal\n388.03ms   0.0%     191.1μs    64,672     turbopack-ecmascript@turbopack_ecmascript::parse::parse\n```\n\nThe script analyzes task statistics to find tasks where the overhead of caching exceeds the benefit, providing recommendations for optimization based on execution patterns.  It leverages data from the overhead.rs benchmark which is also enhanced to provide an estimate on the delta between the measured duration and the actual duration.\n\n## Conclusions?\n\nThere are a few items of low hanging fruit but the real issue is `trait` items.  We need to provide more flexibility to `value_trait` items to make it possible to have non-turbotask items that are `async`",
    "sha": "7cc4990f49f57576e50b2d62be65e0d1cd460b40",
    "files": [
        {
            "sha": "203488caa9a705c19ae386991fd7b199090ec301",
            "filename": "turbopack/crates/turbo-tasks-backend/benches/overhead.rs",
            "status": "modified",
            "additions": 36,
            "deletions": 1,
            "changes": 37,
            "blob_url": "https://github.com/vercel/next.js/blob/7cc4990f49f57576e50b2d62be65e0d1cd460b40/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fbenches%2Foverhead.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/7cc4990f49f57576e50b2d62be65e0d1cd460b40/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fbenches%2Foverhead.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fbenches%2Foverhead.rs?ref=7cc4990f49f57576e50b2d62be65e0d1cd460b40",
            "patch": "@@ -3,7 +3,7 @@ use std::time::{Duration, Instant};\n use criterion::{BenchmarkId, Criterion, black_box};\n use futures::{FutureExt, StreamExt, stream::FuturesUnordered};\n use tokio::spawn;\n-use turbo_tasks::TurboTasks;\n+use turbo_tasks::{TurboTasks, TurboTasksApi};\n use turbo_tasks_backend::{BackendOptions, TurboTasksBackend, noop_backing_storage};\n \n #[global_allocator]\n@@ -79,6 +79,15 @@ pub fn overhead(c: &mut Criterion) {\n                 run_turbo::<Uncached>(&rt, b, d, false);\n             },\n         );\n+        // Same as turbo-uncached but reports the time as measured by turbotasks itself\n+        // This allows us to understand the cost of the indirection within turbotasks\n+        group.bench_with_input(\n+            BenchmarkId::new(\"turbo-uncached-stats\", micros),\n+            &duration,\n+            |b, &d| {\n+                run_turbo_stats(&rt, b, d);\n+            },\n+        );\n \n         group.bench_with_input(\n             BenchmarkId::new(\"turbo-cached-same-keys\", micros),\n@@ -215,3 +224,29 @@ fn run_turbo<Mode: TurboMode>(\n         }\n     });\n }\n+\n+fn run_turbo_stats(rt: &tokio::runtime::Runtime, b: &mut criterion::Bencher<'_>, d: Duration) {\n+    b.to_async(rt).iter_custom(|iters| {\n+        // It is important to create the tt instance here to ensure the cache is not shared across\n+        // iterations.\n+        let tt = TurboTasks::new(TurboTasksBackend::new(\n+            BackendOptions {\n+                storage_mode: None,\n+                ..Default::default()\n+            },\n+            noop_backing_storage(),\n+        ));\n+        let stats = tt.task_statistics().enable().clone();\n+\n+        async move {\n+            tt.run_once(async move {\n+                for i in 0..iters {\n+                    black_box(busy_turbo(i, black_box(d)).await?);\n+                }\n+                Ok(stats.get(&BUSY_TURBO_FUNCTION).duration)\n+            })\n+            .await\n+            .unwrap()\n+        }\n+    });\n+}"
        },
        {
            "sha": "ff0199390220b91095f61b009f3f46a04220e3be",
            "filename": "turbopack/crates/turbo-tasks-backend/src/backend/mod.rs",
            "status": "modified",
            "additions": 12,
            "deletions": 2,
            "changes": 14,
            "blob_url": "https://github.com/vercel/next.js/blob/7cc4990f49f57576e50b2d62be65e0d1cd460b40/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Fmod.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/7cc4990f49f57576e50b2d62be65e0d1cd460b40/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Fmod.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Fmod.rs?ref=7cc4990f49f57576e50b2d62be65e0d1cd460b40",
            "patch": "@@ -388,6 +388,14 @@ impl<B: BackingStorage> TurboTasksBackendInner<B> {\n         self.task_statistics\n             .map(|stats| stats.increment_cache_miss(task_type.native_fn));\n     }\n+\n+    fn track_task_duration(&self, task_id: TaskId, duration: std::time::Duration) {\n+        self.task_statistics.map(|stats| {\n+            if let Some(task_type) = self.task_cache.lookup_reverse(&task_id) {\n+                stats.increment_execution_duration(task_type.native_fn, duration);\n+            }\n+        });\n+    }\n }\n \n pub(crate) struct OperationGuard<'a, B: BackingStorage> {\n@@ -1690,7 +1698,7 @@ impl<B: BackingStorage> TurboTasksBackendInner<B> {\n     fn task_execution_completed(\n         &self,\n         task_id: TaskId,\n-        _duration: Duration,\n+        duration: Duration,\n         _memory_usage: usize,\n         cell_counters: &AutoMap<ValueTypeId, u32, BuildHasherDefault<FxHasher>, 8>,\n         stateful: bool,\n@@ -1708,12 +1716,14 @@ impl<B: BackingStorage> TurboTasksBackendInner<B> {\n         // ok, since the dirty flag won't be removed until step 3 and step 4 is only affecting the\n         // in-memory representation.\n \n-        // The task might be invalidated during this process, so we need to change the stale flag\n+        // The task might be invalidated during this process, so we need to check the stale flag\n         // at the start of every step.\n \n         let span = tracing::trace_span!(\"task execution completed\", immutable = Empty).entered();\n         let mut ctx = self.execute_context(turbo_tasks);\n \n+        self.track_task_duration(task_id, duration);\n+\n         //// STEP 1 ////\n \n         let mut task = ctx.task(task_id, TaskDataCategory::All);"
        },
        {
            "sha": "0eea4c56bc7331d16002e28617462f3d271d31e7",
            "filename": "turbopack/crates/turbo-tasks-backend/tests/task_statistics.rs",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/vercel/next.js/blob/7cc4990f49f57576e50b2d62be65e0d1cd460b40/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Ftask_statistics.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/7cc4990f49f57576e50b2d62be65e0d1cd460b40/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Ftask_statistics.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Ftests%2Ftask_statistics.rs?ref=7cc4990f49f57576e50b2d62be65e0d1cd460b40",
            "patch": "@@ -254,16 +254,23 @@ fn enable_stats() {\n \n fn stats_json() -> serde_json::Value {\n     let tt = turbo_tasks::turbo_tasks();\n-    remove_crate(serde_json::to_value(tt.task_statistics().get()).unwrap())\n+    make_stats_deterministic(serde_json::to_value(tt.task_statistics().get()).unwrap())\n }\n \n // Global task identifiers can contain the crate name, remove it to simplify test assertions\n-fn remove_crate(mut json: serde_json::Value) -> serde_json::Value {\n+fn make_stats_deterministic(mut json: serde_json::Value) -> serde_json::Value {\n     static HASH_RE: Lazy<Regex> = Lazy::new(|| Regex::new(\"^[^:@]+@[^:]+:+\").unwrap());\n     match &mut json {\n         serde_json::Value::Object(map) => {\n             let old_map = std::mem::take(map);\n             for (k, v) in old_map {\n+                // Replace `duration` with a fixed value to simplify test assertions\n+                let mut v = v.clone();\n+                let object = v.as_object_mut().unwrap();\n+                // These are only populated after the task has finalized execution so it racy to\n+                // assert on it.\n+                object.remove(\"duration\");\n+                object.remove(\"executions\");\n                 map.insert(HASH_RE.replace(&k, \"\").into_owned(), v);\n             }\n         }"
        },
        {
            "sha": "83f71ff97f8603dead5dcedb6881771f281234ab",
            "filename": "turbopack/crates/turbo-tasks/src/task_statistics.rs",
            "status": "modified",
            "additions": 24,
            "deletions": 9,
            "changes": 33,
            "blob_url": "https://github.com/vercel/next.js/blob/7cc4990f49f57576e50b2d62be65e0d1cd460b40/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Ftask_statistics.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/7cc4990f49f57576e50b2d62be65e0d1cd460b40/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Ftask_statistics.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Ftask_statistics.rs?ref=7cc4990f49f57576e50b2d62be65e0d1cd460b40",
            "patch": "@@ -19,17 +19,13 @@ impl TaskStatisticsApi {\n         })\n     }\n \n-    pub fn is_enabled(&self) -> bool {\n-        self.inner.get().is_some()\n-    }\n-\n     // Calls `func` if statistics have been enabled (via\n     // [`TaskStatisticsApi::enable`]).\n     pub fn map<T>(&self, func: impl FnOnce(&Arc<TaskStatistics>) -> T) -> Option<T> {\n         self.get().map(func)\n     }\n \n-    // Calls `func` if statistics have been enabled (via\n+    // Returns the statistics if they have been enabled (via\n     // [`TaskStatisticsApi::enable`]).\n     pub fn get(&self) -> Option<&Arc<TaskStatistics>> {\n         self.inner.get()\n@@ -50,20 +46,39 @@ impl TaskStatistics {\n         self.with_task_type_statistics(native_fn, |stats| stats.cache_miss += 1)\n     }\n \n+    pub fn increment_execution_duration(\n+        &self,\n+        native_fn: &'static NativeFunction,\n+        duration: std::time::Duration,\n+    ) {\n+        self.with_task_type_statistics(native_fn, |stats| {\n+            stats.executions += 1;\n+            stats.duration += duration\n+        })\n+    }\n+\n     fn with_task_type_statistics(\n         &self,\n         native_fn: &'static NativeFunction,\n         func: impl Fn(&mut TaskFunctionStatistics),\n     ) {\n         func(self.inner.entry(native_fn).or_default().value_mut())\n     }\n+\n+    pub fn get(&self, f: &'static NativeFunction) -> TaskFunctionStatistics {\n+        self.inner.get(f).unwrap().value().clone()\n+    }\n }\n \n /// Statistics for an individual function.\n-#[derive(Default, Serialize)]\n-struct TaskFunctionStatistics {\n-    cache_hit: u32,\n-    cache_miss: u32,\n+#[derive(Default, Serialize, Clone)]\n+pub struct TaskFunctionStatistics {\n+    pub cache_hit: u32,\n+    pub cache_miss: u32,\n+    // Generally executions == cache_miss, however they can diverge when there are invalidations.\n+    // The caller gets one cache miss but we might execute multiple times.\n+    pub executions: u32,\n+    pub duration: std::time::Duration,\n }\n \n impl Serialize for TaskStatistics {"
        },
        {
            "sha": "dedaba213cdfc917261d8dad05c39a22766c5641",
            "filename": "turbopack/scripts/analyze_cache_effectiveness.py",
            "status": "added",
            "additions": 199,
            "deletions": 0,
            "changes": 199,
            "blob_url": "https://github.com/vercel/next.js/blob/7cc4990f49f57576e50b2d62be65e0d1cd460b40/turbopack%2Fscripts%2Fanalyze_cache_effectiveness.py",
            "raw_url": "https://github.com/vercel/next.js/raw/7cc4990f49f57576e50b2d62be65e0d1cd460b40/turbopack%2Fscripts%2Fanalyze_cache_effectiveness.py",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fscripts%2Fanalyze_cache_effectiveness.py?ref=7cc4990f49f57576e50b2d62be65e0d1cd460b40",
            "patch": "@@ -0,0 +1,199 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+Cache Effectiveness Analysis Script\n+\n+This script analyzes task statistics to identify which tasks are not getting\n+significant benefit from caching and would be candidates for removing the\n+caching layer.\n+\n+To use this script, run: a build with `NEXT_TURBOPACK_TASK_STATISTICS=path/to/stats.json` set\n+\n+Then run this script with the path to the stats.json file to get a report on optimization opportunities.\n+\n+Based on benchmarking data from the `turbopack/crates/turbo-tasks-backend/benches/overhead.rs` benchmark we have the following estimates:\n+- Cache hit cost: 200-500ns\n+- Execution overhead: 4-6us\n+- Measurement overhead: 260ns-750ns\n+\n+This script assumes the best case scenario and reports on the potential time savings from removing the caching layer.\n+\"\"\"\n+\n+import json\n+import sys\n+from typing import Dict, List, Tuple\n+from dataclasses import dataclass\n+\n+\n+@dataclass\n+class TaskStats:\n+    name: str\n+    cache_hit: int\n+    cache_miss: int\n+    executions: int\n+    duration_ns: int\n+\n+    @property\n+    def total_operations(self) -> int:\n+        return self.cache_hit + self.cache_miss\n+\n+    @property\n+    def cache_hit_rate(self) -> float:\n+        if self.total_operations == 0:\n+            return 0.0\n+        return self.cache_hit / self.total_operations\n+\n+    @property\n+    def avg_execution_time_ns(self) -> int:\n+        MEASUREMENT_OVERHEAD =   750 # OVerhead implicit in the reported duration\n+        if self.executions == 0:\n+            return 0\n+        return max(0, (self.duration_ns  - MEASUREMENT_OVERHEAD * self.executions) // self.executions)\n+\n+\n+def parse_duration(duration_dict: Dict) -> int:\n+    \"\"\"Convert duration dict to nanoseconds.\"\"\"\n+    return duration_dict.get(\"secs\", 0) * 1_000_000_000 + duration_dict.get(\"nanos\", 0)\n+\n+\n+def load_task_stats(file_path: str) -> List[TaskStats]:\n+    \"\"\"Load and parse task statistics from JSON file.\"\"\"\n+    with open(file_path, 'r') as f:\n+        data = json.load(f)\n+\n+    tasks = []\n+    for task_name, stats in data.items():\n+        duration_ns = parse_duration(stats[\"duration\"])\n+        task = TaskStats(\n+            name=task_name,\n+            cache_hit=stats[\"cache_hit\"],\n+            cache_miss=stats[\"cache_miss\"],\n+            executions=stats[\"executions\"],\n+            duration_ns=duration_ns\n+        )\n+        tasks.append(task)\n+\n+    return tasks\n+\n+\n+def calculate_cache_effectiveness(task: TaskStats) -> float:\n+    \"\"\"\n+    Calculate the effectiveness of caching for a task.\n+\n+    Returns:\n+        Time savings from removing caching (negative means caching is beneficial)\n+    \"\"\"\n+    # Constants based on benchmarking\n+    # These are optimistic estimates\n+    CACHE_HIT_COST_NS = 500  # Average of 200-500ns\n+    EXECUTION_OVERHEAD_NS = 6000  # Average of 4-6us (caching layer overhead)\n+    MEASUREMENT_OVERHEAD =   750 # OVerhead implicit in the reported duration\n+\n+    if task.total_operations == 0:\n+        return 0.0\n+\n+    # Current cost with caching\n+    # Cache hits: just the cache lookup cost\n+    # Cache misses: cache overhead + actual execution time\n+    cache_hit_cost = task.cache_hit * CACHE_HIT_COST_NS\n+    cache_miss_cost = task.cache_miss * (EXECUTION_OVERHEAD_NS + task.avg_execution_time_ns)\n+    current_total_cost = cache_hit_cost + cache_miss_cost\n+\n+    # Cost without caching (all operations would be direct executions, no overhead)\n+    no_cache_cost = task.total_operations * task.avg_execution_time_ns\n+\n+    # Time savings from removing caching (positive means we save time by removing cache)\n+    time_savings = current_total_cost - no_cache_cost\n+\n+    return time_savings\n+\n+\n+def analyze_tasks(tasks: List[TaskStats]) -> List[Tuple[TaskStats, float]]:\n+    \"\"\"Analyze all tasks and return sorted by potential time savings.\"\"\"\n+    results = []\n+\n+    for task in tasks:\n+        results.append((task, calculate_cache_effectiveness(task)))\n+\n+    # Sort by time savings (descending - highest savings first)\n+    results.sort(key=lambda x: x[1], reverse=True)\n+\n+    return results\n+\n+\n+def format_time(nanoseconds: float) -> str:\n+    \"\"\"Format time in appropriate units (ns, μs, ms, s).\"\"\"\n+    sign = \"-\" if nanoseconds < 0 else \"\"\n+    nanoseconds = abs(nanoseconds)\n+    if nanoseconds >= 1_000_000_000:  # >= 1 second\n+        return f\"{sign}{nanoseconds / 1_000_000_000:.2f}s\"\n+    elif nanoseconds >= 1_000_000:  # >= 1 millisecond\n+        return f\"{sign}{nanoseconds / 1_000_000:.2f}ms\"\n+    elif nanoseconds >= 1_000:  # >= 1 microsecond\n+        return f\"{sign}{nanoseconds / 1_000:.1f}μs\"\n+    else:  # nanoseconds\n+        return f\"{sign}{nanoseconds:.0f}ns\"\n+\n+\n+def print_analysis(results: List[Tuple[TaskStats, float]]):\n+    \"\"\"Print the analysis results.\"\"\"\n+    print(\"Tasks ranked by estimated time savings from removing caching layer\")\n+    print()\n+\n+    if not results:\n+        print(\"No tasks would benefit from removing caching.\")\n+        return\n+    # Print header\n+    header = (f\"{'Savings':<10} {'Hit Rate':<8} {'Exec Time':<10} \"\n+             f\"{'Operations':<10} {'Task Name'}\")\n+    print(header)\n+    print(\"-\" * len(header))\n+\n+    # Print results\n+    for (task, time_savings) in results:\n+        savings_str = format_time(time_savings)\n+        hit_rate_str = f\"{task.cache_hit_rate:.1%}\"\n+        exec_time_str = format_time(task.avg_execution_time_ns)\n+        operations_str = f\"{task.total_operations:,}\"\n+\n+        print(f\"{savings_str:<10} {hit_rate_str:<8} {exec_time_str:<10} \"\n+              f\"{operations_str:<10} {task.name}\")\n+\n+    # Print summary\n+    total_savings = sum(time_savings if time_savings > 0 else 0 for _, time_savings in results)\n+    print()\n+    print(f\"Summary: {sum(1 if time_savings > 0 else 0 for _, time_savings in results)} tasks would benefit from removing caching\")\n+    print(f\"Total potential savings: {format_time(total_savings)}\")\n+    print()\n+    print(\"Legend:\")\n+    print(\"- Savings: Time saved by removing caching layer\")\n+    print(\"- Hit Rate: Percentage of operations that were cache hits\")\n+    print(\"- Exec Time: Average execution time per operation\")\n+    print(\"- Operations: Total number of cache hits + misses\")\n+\n+\n+\n+def main():\n+    if len(sys.argv) != 2:\n+        print(\"Usage: python analyze_cache_effectiveness.py <stats-durations.json>\")\n+        sys.exit(1)\n+\n+    file_path = sys.argv[1]\n+\n+    try:\n+        tasks = load_task_stats(file_path)\n+        results = analyze_tasks(tasks)\n+        print_analysis(results)\n+\n+    except FileNotFoundError:\n+        print(f\"Error: File '{file_path}' not found\")\n+        sys.exit(1)\n+    except json.JSONDecodeError as e:\n+        print(f\"Error parsing JSON: {e}\")\n+        sys.exit(1)\n+    except Exception as e:\n+        print(f\"Error: {e}\")\n+        sys.exit(1)\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        }
    ],
    "stats": {
        "total": 294,
        "additions": 280,
        "deletions": 14
    }
}