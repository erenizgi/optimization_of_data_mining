{
    "author": "sokra",
    "message": "Turbopack: split meta data and AQMF into separate files (#79659)\n\n### What?\n\nSplit the SST meta data and AQMF into a separate `*.meta` file. A single meta file can contain meta data and AQMFs of multiple SST files.\n\nOn database startup only the meta data files need to be read and SST files are lazily mmapped and read only when the AQM-Filter is successful. This ensures that only SST files with a high likelyhood need to be read at all.\n\nCurrently this has not much benefit as keys are randomly distributed in SST files, so all files need to be read anyway.\nThere is a small benefit after a bunch of incremental builds, when some SST files become unused as they have been overwritten.\n\nIn future we want to split keys into recently used and not recently used keys and place them in separate SST files. This ensures that less SST files need to be read. This change is a pre-requirement to that change.",
    "sha": "f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83",
    "files": [
        {
            "sha": "87a7561166498232ea76a6d8a8e63774958e07ac",
            "filename": "Cargo.lock",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/vercel/next.js/blob/f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83/Cargo.lock",
            "raw_url": "https://github.com/vercel/next.js/raw/f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83/Cargo.lock",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/Cargo.lock?ref=f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83",
            "patch": "@@ -9492,6 +9492,7 @@ version = \"0.1.0\"\n dependencies = [\n  \"anyhow\",\n  \"byteorder\",\n+ \"either\",\n  \"jiff\",\n  \"lzzzz\",\n  \"memmap2 0.9.5\","
        },
        {
            "sha": "b97a1165003769ef873232638adaeed192d33301",
            "filename": "turbopack/crates/turbo-persistence/Cargo.toml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/vercel/next.js/blob/f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83/turbopack%2Fcrates%2Fturbo-persistence%2FCargo.toml",
            "raw_url": "https://github.com/vercel/next.js/raw/f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83/turbopack%2Fcrates%2Fturbo-persistence%2FCargo.toml",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2FCargo.toml?ref=f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83",
            "patch": "@@ -5,13 +5,15 @@ edition = \"2024\"\n license = \"MIT\"\n \n [features]\n+default = []\n verify_sst_content = []\n strict_checks = []\n stats = [\"quick_cache/stats\"]\n print_stats = [\"stats\"]\n \n [dependencies]\n anyhow = { workspace = true }\n+either = { workspace = true }\n pot = \"3.0.0\"\n byteorder = \"1.5.0\"\n jiff = \"0.2.10\""
        },
        {
            "sha": "e86b69a1c9be75052890f10a10fb15a1a5efe2d9",
            "filename": "turbopack/crates/turbo-persistence/README.md",
            "status": "modified",
            "additions": 28,
            "deletions": 11,
            "changes": 39,
            "blob_url": "https://github.com/vercel/next.js/blob/f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83/turbopack%2Fcrates%2Fturbo-persistence%2FREADME.md",
            "raw_url": "https://github.com/vercel/next.js/raw/f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83/turbopack%2Fcrates%2Fturbo-persistence%2FREADME.md",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2FREADME.md?ref=f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83",
            "patch": "@@ -16,10 +16,12 @@ There is a single `CURRENT` file which stores the latest committed sequence numb\n \n All other files have a sequence number as file name, e. g. `0000123.sst`. All files are immutable once there sequence number is <= the committed sequence number. But they might be deleted when they are superseeded by other committed files.\n \n-There are two different file types:\n+There are four different file types:\n \n - Static Sorted Table (SST, `*.sst`): These files contain key value pairs.\n - Blob files (`*.blob`): These files contain large values.\n+- Delete files (`*.del`): These files contain a list of sequence numbers of files that should be considered as deleted.\n+- Meta files (`*.meta`): These files contain metadata about the SST files. They contains the hash range and a AQMF for quick filtering.\n \n Therefore there are there value types:\n \n@@ -29,18 +31,33 @@ Therefore there are there value types:\n - Future:\n   - MERGE: An application specific update operation that is applied on the old value.\n \n-### SST file\n+### Meta file\n+\n+A meta file can contain metadata about multiple SST files. The metadata is stored in a single file to avoid having too many small files.\n \n-- Headers\n-  - 4 bytes magic number and version\n+- Header\n+  - 4 bytes magic number (0xFE4ADA4A)\n   - 4 bytes key family\n-  - 8 bytes min hash\n-  - 8 bytes max hash\n-  - 3 bytes AQMF length\n-  - 2 bytes key Compression Dictionary length\n-  - 2 bytes value Compression Dictionary length\n-  - 2 bytes block count\n-- serialized AQMF\n+  - 4 bytes count of obsolete SST files\n+  - foreach obsolete SST file\n+    - 4 bytes sequence number of the obsolete SST file\n+  - 4 bytes count of described SST files\n+  - foreach described SST file\n+    - 4 bytes sequence number of the SST file\n+    - 2 bytes key Compression Dictionary length\n+    - 2 bytes value Compression Dictionary length\n+    - 2 bytes block count\n+    - 8 bytes min hash\n+    - 8 bytes max hash\n+    - 8 bytes SST file size\n+    - 4 bytes end of AQMF offset relative to start of all AQMF data\n+- foreach described SST file\n+  - serialized AQMF\n+\n+### SST file\n+\n+The SST file contains only data without any header.\n+\n - serialized key Compression Dictionary\n - serialized value Compression Dictionary\n - foreach block"
        },
        {
            "sha": "5f6218426b63b24d79fc5fa36215fe886266ad49",
            "filename": "turbopack/crates/turbo-persistence/src/compaction/selector.rs",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/vercel/next.js/blob/f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fcompaction%2Fselector.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fcompaction%2Fselector.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fcompaction%2Fselector.rs?ref=f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83",
            "patch": "@@ -21,7 +21,7 @@ pub trait Compactable {\n     fn range(&self) -> Range;\n \n     /// Returns the size of the compactable.\n-    fn size(&self) -> usize;\n+    fn size(&self) -> u64;\n }\n \n fn is_overlapping(a: &Range, b: &Range) -> bool {\n@@ -65,7 +65,7 @@ pub struct CompactConfig {\n     pub max_merge: usize,\n \n     /// The maximum size of all files to merge at once.\n-    pub max_merge_size: usize,\n+    pub max_merge_size: u64,\n }\n \n /// For a list of compactables, computes merge and move jobs that are expected to perform best.\n@@ -228,23 +228,23 @@ mod tests {\n \n     struct TestCompactable {\n         range: Range,\n-        size: usize,\n+        size: u64,\n     }\n \n     impl Compactable for TestCompactable {\n         fn range(&self) -> Range {\n             self.range\n         }\n \n-        fn size(&self) -> usize {\n+        fn size(&self) -> u64 {\n             self.size\n         }\n     }\n \n     fn compact<const N: usize>(\n         ranges: [(u64, u64); N],\n         max_merge: usize,\n-        max_merge_size: usize,\n+        max_merge_size: u64,\n     ) -> CompactionJobs {\n         let compactables = ranges\n             .iter()\n@@ -277,7 +277,7 @@ mod tests {\n                 (30, 40),\n             ],\n             3,\n-            usize::MAX,\n+            u64::MAX,\n         );\n         assert_eq!(merge_jobs, vec![vec![0, 1, 2], vec![4, 5, 6]]);\n         assert_eq!(move_jobs, vec![3, 8]);\n@@ -341,7 +341,7 @@ mod tests {\n                 let config = CompactConfig {\n                     max_merge: 4,\n                     min_merge: 2,\n-                    max_merge_size: usize::MAX,\n+                    max_merge_size: u64::MAX,\n                 };\n                 let jobs = get_compaction_jobs(&containers, &config);\n                 if !jobs.is_empty() {\n@@ -387,8 +387,8 @@ mod tests {\n             (self.keys[0], *self.keys.last().unwrap())\n         }\n \n-        fn size(&self) -> usize {\n-            self.keys.len()\n+        fn size(&self) -> u64 {\n+            self.keys.len() as u64\n         }\n     }\n "
        },
        {
            "sha": "637f52a0deaebb731f4d32f438c892eedd729dc0",
            "filename": "turbopack/crates/turbo-persistence/src/db.rs",
            "status": "modified",
            "additions": 388,
            "deletions": 258,
            "changes": 646,
            "blob_url": "https://github.com/vercel/next.js/blob/f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fdb.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fdb.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fdb.rs?ref=f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83",
            "patch": "@@ -32,12 +32,13 @@ use crate::{\n         VALUE_BLOCK_CACHE_SIZE,\n     },\n     key::{StoreKey, hash_key},\n-    lookup_entry::LookupEntry,\n+    lookup_entry::{LookupEntry, LookupValue},\n     merge_iter::MergeIter,\n-    static_sorted_file::{\n-        AqmfCache, BlockCache, LookupResult, StaticSortedFile, StaticSortedFileRange,\n-    },\n-    static_sorted_file_builder::StaticSortedFileBuilder,\n+    meta_file::{AqmfCache, MetaFile, MetaLookupResult, StaticSortedFileRange},\n+    meta_file_builder::MetaFileBuilder,\n+    sst_filter::SstFilter,\n+    static_sorted_file::{BlockCache, SstLookupResult},\n+    static_sorted_file_builder::{StaticSortedFileBuilder, StaticSortedFileBuilderMeta},\n     write_batch::{FinishResult, WriteBatch},\n };\n \n@@ -79,12 +80,14 @@ impl CacheStatistics {\n #[cfg(feature = \"stats\")]\n #[derive(Debug)]\n pub struct Statistics {\n+    pub meta_files: usize,\n     pub sst_files: usize,\n     pub key_block_cache: CacheStatistics,\n     pub value_block_cache: CacheStatistics,\n     pub aqmf_cache: CacheStatistics,\n     pub hits: u64,\n     pub misses: u64,\n+    pub miss_family: u64,\n     pub miss_range: u64,\n     pub miss_aqmf: u64,\n     pub miss_key: u64,\n@@ -96,6 +99,7 @@ struct TrackedStats {\n     hits_deleted: std::sync::atomic::AtomicU64,\n     hits_small: std::sync::atomic::AtomicU64,\n     hits_blob: std::sync::atomic::AtomicU64,\n+    miss_family: std::sync::atomic::AtomicU64,\n     miss_range: std::sync::atomic::AtomicU64,\n     miss_aqmf: std::sync::atomic::AtomicU64,\n     miss_key: std::sync::atomic::AtomicU64,\n@@ -128,12 +132,22 @@ pub struct TurboPersistence {\n \n /// The inner state of the database.\n struct Inner {\n-    /// The list of SST files in the database in order.\n-    static_sorted_files: Vec<StaticSortedFile>,\n+    /// The list of meta files in the database. This is used to derive the SST files.\n+    meta_files: Vec<MetaFile>,\n     /// The current sequence number for the database.\n     current_sequence_number: u32,\n }\n \n+pub struct CommitOptions {\n+    new_meta_files: Vec<(u32, File)>,\n+    new_sst_files: Vec<(u32, File)>,\n+    new_blob_files: Vec<(u32, File)>,\n+    sst_seq_numbers_to_delete: Vec<u32>,\n+    blob_seq_numbers_to_delete: Vec<u32>,\n+    sequence_number: u32,\n+    keys_written: u64,\n+}\n+\n impl TurboPersistence {\n     /// Open a TurboPersistence database at the given path.\n     /// This will read the directory and might performance cleanup when the database was not closed\n@@ -143,7 +157,7 @@ impl TurboPersistence {\n         let mut db = Self {\n             path,\n             inner: RwLock::new(Inner {\n-                static_sorted_files: Vec::new(),\n+                meta_files: Vec::new(),\n                 current_sequence_number: 0,\n             }),\n             idle_write_batch: Mutex::new(None),\n@@ -217,7 +231,7 @@ impl TurboPersistence {\n \n     /// Loads an existing database directory and performs cleanup if necessary.\n     fn load_directory(&mut self, entries: ReadDir) -> Result<bool> {\n-        let mut sst_files = Vec::new();\n+        let mut meta_files = Vec::new();\n         let mut current_file = match File::open(self.path.join(\"CURRENT\")) {\n             Ok(file) => file,\n             Err(e) => {\n@@ -249,8 +263,8 @@ impl TurboPersistence {\n                     fs::remove_file(&path)?;\n                 } else {\n                     match ext {\n-                        \"sst\" => {\n-                            sst_files.push(seq);\n+                        \"meta\" => {\n+                            meta_files.push(seq);\n                         }\n                         \"del\" => {\n                             let mut content = &*fs::read(&path)?;\n@@ -259,8 +273,9 @@ impl TurboPersistence {\n                                 let seq = content.read_u32::<BE>()?;\n                                 deleted_files.insert(seq);\n                                 let sst_file = self.path.join(format!(\"{seq:08}.sst\"));\n+                                let meta_file = self.path.join(format!(\"{seq:08}.meta\"));\n                                 let blob_file = self.path.join(format!(\"{seq:08}.blob\"));\n-                                for path in [sst_file, blob_file] {\n+                                for path in [sst_file, meta_file, blob_file] {\n                                     if fs::exists(&path)? {\n                                         fs::remove_file(path)?;\n                                         no_existing_files = false;\n@@ -271,8 +286,8 @@ impl TurboPersistence {\n                                 fs::remove_file(&path)?;\n                             }\n                         }\n-                        \"blob\" => {\n-                            // ignore blobs, they are read when needed\n+                        \"blob\" | \"sst\" => {\n+                            // ignore blobs and sst, they are read when needed\n                         }\n                         _ => {\n                             if !path\n@@ -304,43 +319,30 @@ impl TurboPersistence {\n             }\n         }\n \n-        sst_files.retain(|seq| !deleted_files.contains(seq));\n-        sst_files.sort_unstable();\n-        let sst_files = sst_files\n-            .into_iter()\n-            .map(|seq| self.open_sst(seq))\n-            .collect::<Result<Vec<StaticSortedFile>>>()?;\n-        #[cfg(feature = \"print_stats\")]\n-        {\n-            for sst in sst_files.iter() {\n-                let crate::static_sorted_file::StaticSortedFileRange {\n-                    family,\n-                    min_hash,\n-                    max_hash,\n-                } = sst.range()?;\n-                println!(\n-                    \"SST {}  {} {:016x} - {:016x}  {:016x}\",\n-                    sst.sequence_number(),\n-                    family,\n-                    min_hash,\n-                    max_hash,\n-                    max_hash - min_hash\n-                );\n-            }\n+        meta_files.retain(|seq| !deleted_files.contains(seq));\n+        meta_files.sort_unstable();\n+        let span = Span::current();\n+        let mut meta_files = meta_files\n+            .into_par_iter()\n+            .with_min_len(1)\n+            .map(|seq| {\n+                let _span = span.enter();\n+                let meta_file = MetaFile::open(&self.path, seq)?;\n+                Ok(meta_file)\n+            })\n+            .collect::<Result<Vec<MetaFile>>>()?;\n+\n+        let mut sst_filter = SstFilter::new();\n+        for meta_file in meta_files.iter_mut() {\n+            sst_filter.apply_filter(meta_file);\n         }\n+\n         let inner = self.inner.get_mut();\n-        inner.static_sorted_files = sst_files;\n+        inner.meta_files = meta_files;\n         inner.current_sequence_number = current;\n         Ok(true)\n     }\n \n-    /// Opens a single SST file. This memory maps the file, but doesn't read it yet.\n-    fn open_sst(&self, seq: u32) -> Result<StaticSortedFile> {\n-        let path = self.path.join(format!(\"{seq:08}.sst\"));\n-        StaticSortedFile::open(seq, path)\n-            .with_context(|| format!(\"Unable to open sst file {seq:08}.sst\"))\n-    }\n-\n     /// Reads and decompresses a blob file. This is not backed by any cache.\n     fn read_blob(&self, seq: u32) -> Result<ArcSlice<u8>> {\n         let path = self.path.join(format!(\"{seq:08}.blob\"));\n@@ -367,7 +369,7 @@ impl TurboPersistence {\n \n     /// Returns true if the database is empty.\n     pub fn is_empty(&self) -> bool {\n-        self.inner.read().static_sorted_files.is_empty()\n+        self.inner.read().meta_files.is_empty()\n     }\n \n     /// Starts a new WriteBatch for the database. Only a single write operation is allowed at a\n@@ -415,10 +417,20 @@ impl TurboPersistence {\n     ) -> Result<()> {\n         let FinishResult {\n             sequence_number,\n+            new_meta_files,\n             new_sst_files,\n             new_blob_files,\n+            keys_written,\n         } = write_batch.finish()?;\n-        self.commit(new_sst_files, new_blob_files, vec![], sequence_number)?;\n+        self.commit(CommitOptions {\n+            new_meta_files,\n+            new_sst_files,\n+            new_blob_files,\n+            sst_seq_numbers_to_delete: vec![],\n+            blob_seq_numbers_to_delete: vec![],\n+            sequence_number,\n+            keys_written,\n+        })?;\n         self.active_write_operation.store(false, Ordering::Release);\n         self.idle_write_batch.lock().replace((\n             TypeId::of::<WriteBatch<K, FAMILIES>>(),\n@@ -431,61 +443,103 @@ impl TurboPersistence {\n     /// new files.\n     fn commit(\n         &self,\n-        mut new_sst_files: Vec<(u32, File)>,\n-        new_blob_files: Vec<(u32, File)>,\n-        mut indicies_to_delete: Vec<usize>,\n-        mut seq: u32,\n+        CommitOptions {\n+            mut new_meta_files,\n+            mut new_sst_files,\n+            mut new_blob_files,\n+            mut sst_seq_numbers_to_delete,\n+            mut blob_seq_numbers_to_delete,\n+            sequence_number: mut seq,\n+            keys_written,\n+        }: CommitOptions,\n     ) -> Result<(), anyhow::Error> {\n         let time = Timestamp::now();\n \n-        new_sst_files.sort_unstable_by_key(|(seq, _)| *seq);\n+        new_meta_files.sort_unstable_by_key(|(seq, _)| *seq);\n \n-        let mut new_sst_files = new_sst_files\n-            .into_iter()\n+        let mut new_meta_files = new_meta_files\n+            .into_par_iter()\n+            .with_min_len(1)\n             .map(|(seq, file)| {\n                 file.sync_all()?;\n-                self.open_sst(seq)\n+                let meta_file = MetaFile::open(&self.path, seq)?;\n+                Ok(meta_file)\n             })\n             .collect::<Result<Vec<_>>>()?;\n \n+        let mut sst_filter = SstFilter::new();\n+        for meta_file in new_meta_files.iter_mut() {\n+            sst_filter.apply_filter(meta_file);\n+        }\n+\n+        for (_, file) in new_sst_files.iter() {\n+            file.sync_all()?;\n+        }\n         for (_, file) in new_blob_files.iter() {\n             file.sync_all()?;\n         }\n \n-        let new_sst_info = new_sst_files\n+        let new_meta_info = new_meta_files\n             .iter()\n-            .map(|sst| {\n-                let seq = sst.sequence_number();\n-                let range = sst.range()?;\n-                let size = sst.size();\n-                Ok((seq, range.family, range.min_hash, range.max_hash, size))\n+            .map(|meta| {\n+                let ssts = meta\n+                    .entries()\n+                    .iter()\n+                    .map(|entry| {\n+                        let seq = entry.sequence_number();\n+                        let range = entry.range();\n+                        let size = entry.size();\n+                        (seq, range.min_hash, range.max_hash, size)\n+                    })\n+                    .collect::<Vec<_>>();\n+                (meta.sequence_number(), meta.family(), ssts)\n             })\n-            .collect::<Result<Vec<_>>>()?;\n-\n-        if !indicies_to_delete.is_empty() {\n-            seq += 1;\n-        }\n+            .collect::<Vec<_>>();\n \n-        let removed_ssts;\n+        let has_delete_file;\n+        let mut meta_seq_numbers_to_delete = Vec::new();\n \n         {\n             let mut inner = self.inner.write();\n+            for meta_file in inner.meta_files.iter_mut() {\n+                sst_filter.apply_filter(meta_file);\n+            }\n+            inner.meta_files.append(&mut new_meta_files);\n+            inner.meta_files.retain(|meta| {\n+                if sst_filter.apply_and_get_remove(meta) {\n+                    meta_seq_numbers_to_delete.push(meta.sequence_number());\n+                    false\n+                } else {\n+                    true\n+                }\n+            });\n+            has_delete_file = !sst_seq_numbers_to_delete.is_empty()\n+                || !blob_seq_numbers_to_delete.is_empty()\n+                || !meta_seq_numbers_to_delete.is_empty();\n+            if has_delete_file {\n+                seq += 1;\n+            }\n             inner.current_sequence_number = seq;\n-            indicies_to_delete.sort_unstable();\n-            removed_ssts = remove_indicies(&mut inner.static_sorted_files, &indicies_to_delete);\n-            inner.static_sorted_files.append(&mut new_sst_files);\n         }\n \n-        let mut removed_ssts = removed_ssts\n-            .into_iter()\n-            .map(|sst| sst.sequence_number())\n-            .collect::<Vec<_>>();\n-        removed_ssts.sort_unstable();\n-\n-        if !indicies_to_delete.is_empty() {\n+        if has_delete_file {\n+            sst_seq_numbers_to_delete.sort_unstable();\n+            meta_seq_numbers_to_delete.sort_unstable();\n+            blob_seq_numbers_to_delete.sort_unstable();\n             // Write *.del file, marking the selected files as to delete\n-            let mut buf = Vec::with_capacity(removed_ssts.len() * 4);\n-            for seq in removed_ssts.iter() {\n+            let mut buf = Vec::with_capacity(\n+                (sst_seq_numbers_to_delete.len()\n+                    + meta_seq_numbers_to_delete.len()\n+                    + blob_seq_numbers_to_delete.len())\n+                    * size_of::<u32>(),\n+            );\n+            for seq in sst_seq_numbers_to_delete.iter() {\n+                buf.write_u32::<BE>(*seq)?;\n+            }\n+            for seq in meta_seq_numbers_to_delete.iter() {\n+                buf.write_u32::<BE>(*seq)?;\n+            }\n+            for seq in blob_seq_numbers_to_delete.iter() {\n                 buf.write_u32::<BE>(*seq)?;\n             }\n             let mut file = File::create(self.path.join(format!(\"{seq:08}.del\")))?;\n@@ -501,31 +555,47 @@ impl TurboPersistence {\n         current_file.write_u32::<BE>(seq)?;\n         current_file.sync_all()?;\n \n-        for seq in removed_ssts {\n+        for seq in sst_seq_numbers_to_delete.iter() {\n             fs::remove_file(self.path.join(format!(\"{seq:08}.sst\")))?;\n         }\n+        for seq in meta_seq_numbers_to_delete.iter() {\n+            fs::remove_file(self.path.join(format!(\"{seq:08}.meta\")))?;\n+        }\n+        for seq in blob_seq_numbers_to_delete.iter() {\n+            fs::remove_file(self.path.join(format!(\"{seq:08}.blob\")))?;\n+        }\n \n         {\n             let mut log = self.open_log()?;\n             writeln!(log, \"Time {time}\")?;\n             let span = time.until(Timestamp::now())?;\n-            writeln!(log, \"Commit {seq:08} {span:#}\")?;\n-            for (index, family, min, max, size) in new_sst_info.iter() {\n-                writeln!(\n-                    log,\n-                    \"{:08} SST family:{} {:016x}-{:016x} {} MiB\",\n-                    index,\n-                    family,\n-                    min,\n-                    max,\n-                    size / 1024 / 1024\n-                )?;\n+            writeln!(log, \"Commit {seq:08} {keys_written} keys in {span:#}\")?;\n+            for (seq, family, ssts) in new_meta_info {\n+                writeln!(log, \"{seq:08} META family:{family}\",)?;\n+                for (seq, min, max, size) in ssts {\n+                    writeln!(\n+                        log,\n+                        \"  {seq:08} SST  {min:016x}-{max:016x} {} MiB\",\n+                        size / 1024 / 1024\n+                    )?;\n+                }\n+            }\n+            new_sst_files.sort_unstable_by_key(|(seq, _)| *seq);\n+            for (seq, _) in new_sst_files.iter() {\n+                writeln!(log, \"{seq:08} NEW SST\")?;\n             }\n+            new_blob_files.sort_unstable_by_key(|(seq, _)| *seq);\n             for (seq, _) in new_blob_files.iter() {\n-                writeln!(log, \"{seq:08} BLOB\")?;\n+                writeln!(log, \"{seq:08} NEW BLOB\")?;\n+            }\n+            for seq in sst_seq_numbers_to_delete.iter() {\n+                writeln!(log, \"{seq:08} SST DELETED\")?;\n             }\n-            for index in indicies_to_delete.iter() {\n-                writeln!(log, \"{index:08} DELETED\")?;\n+            for seq in meta_seq_numbers_to_delete.iter() {\n+                writeln!(log, \"{seq:08} META DELETED\")?;\n+            }\n+            for seq in blob_seq_numbers_to_delete.iter() {\n+                writeln!(log, \"{seq:08} BLOB DELETED\")?;\n             }\n         }\n \n@@ -535,7 +605,7 @@ impl TurboPersistence {\n     /// Runs a full compaction on the database. This will rewrite all SST files, removing all\n     /// duplicate keys and separating all key ranges into unique files.\n     pub fn full_compact(&self) -> Result<()> {\n-        self.compact(0.0, usize::MAX, usize::MAX)?;\n+        self.compact(0.0, usize::MAX, u64::MAX)?;\n         Ok(())\n     }\n \n@@ -547,7 +617,7 @@ impl TurboPersistence {\n         &self,\n         max_coverage: f32,\n         max_merge_sequence: usize,\n-        max_merge_size: usize,\n+        max_merge_size: u64,\n     ) -> Result<()> {\n         let _span = tracing::info_span!(\"compact database\").entered();\n         if self\n@@ -562,30 +632,39 @@ impl TurboPersistence {\n         }\n \n         let mut sequence_number;\n+        let mut new_meta_files = Vec::new();\n         let mut new_sst_files = Vec::new();\n-        let mut indicies_to_delete = Vec::new();\n+        let mut sst_seq_numbers_to_delete = Vec::new();\n+        let mut blob_seq_numbers_to_delete = Vec::new();\n+        let mut keys_written = 0;\n \n         {\n             let inner = self.inner.read();\n             sequence_number = AtomicU32::new(inner.current_sequence_number);\n             self.compact_internal(\n-                &inner.static_sorted_files,\n+                &inner.meta_files,\n                 &sequence_number,\n+                &mut new_meta_files,\n                 &mut new_sst_files,\n-                &mut indicies_to_delete,\n+                &mut sst_seq_numbers_to_delete,\n+                &mut blob_seq_numbers_to_delete,\n+                &mut keys_written,\n                 max_coverage,\n                 max_merge_sequence,\n                 max_merge_size,\n             )?;\n         }\n \n-        if !new_sst_files.is_empty() {\n-            self.commit(\n+        if !new_meta_files.is_empty() {\n+            self.commit(CommitOptions {\n+                new_meta_files,\n                 new_sst_files,\n-                Vec::new(),\n-                indicies_to_delete,\n-                *sequence_number.get_mut(),\n-            )?;\n+                new_blob_files: Vec::new(),\n+                sst_seq_numbers_to_delete,\n+                blob_seq_numbers_to_delete,\n+                sequence_number: *sequence_number.get_mut(),\n+                keys_written,\n+            })?;\n         }\n \n         self.active_write_operation.store(false, Ordering::Release);\n@@ -596,43 +675,53 @@ impl TurboPersistence {\n     /// Internal function to perform a compaction.\n     fn compact_internal(\n         &self,\n-        static_sorted_files: &[StaticSortedFile],\n+        meta_files: &[MetaFile],\n         sequence_number: &AtomicU32,\n+        new_meta_files: &mut Vec<(u32, File)>,\n         new_sst_files: &mut Vec<(u32, File)>,\n-        indicies_to_delete: &mut Vec<usize>,\n+        sst_seq_numbers_to_delete: &mut Vec<u32>,\n+        blob_seq_numbers_to_delete: &mut Vec<u32>,\n+        keys_written: &mut u64,\n         max_coverage: f32,\n         max_merge_sequence: usize,\n-        max_merge_size: usize,\n+        max_merge_size: u64,\n     ) -> Result<()> {\n-        if static_sorted_files.is_empty() {\n+        if meta_files.is_empty() {\n             return Ok(());\n         }\n \n         struct SstWithRange {\n-            index: usize,\n+            meta_index: usize,\n+            index_in_meta: u32,\n+            seq: u32,\n             range: StaticSortedFileRange,\n-            size: usize,\n+            size: u64,\n         }\n \n         impl Compactable for SstWithRange {\n             fn range(&self) -> (u64, u64) {\n                 (self.range.min_hash, self.range.max_hash)\n             }\n \n-            fn size(&self) -> usize {\n+            fn size(&self) -> u64 {\n                 self.size\n             }\n         }\n \n-        let ssts_with_ranges = static_sorted_files\n+        let ssts_with_ranges = meta_files\n             .iter()\n             .enumerate()\n-            .flat_map(|(index, sst)| {\n-                sst.range().ok().map(|range| SstWithRange {\n-                    index,\n-                    range,\n-                    size: sst.size(),\n-                })\n+            .flat_map(|(meta_index, meta)| {\n+                meta.entries()\n+                    .iter()\n+                    .enumerate()\n+                    .map(move |(index_in_meta, entry)| SstWithRange {\n+                        meta_index,\n+                        index_in_meta: index_in_meta as u32,\n+                        seq: entry.sequence_number(),\n+                        range: entry.range(),\n+                        size: entry.size(),\n+                    })\n             })\n             .collect::<Vec<_>>();\n \n@@ -656,15 +745,31 @@ impl TurboPersistence {\n \n         let log_mutex = Mutex::new(());\n         let span = Span::current();\n+\n+        struct PartialResultPerFamily {\n+            new_meta_file: Option<(u32, File)>,\n+            new_sst_files: Vec<(u32, File)>,\n+            sst_seq_numbers_to_delete: Vec<u32>,\n+            blob_seq_numbers_to_delete: Vec<u32>,\n+            keys_written: u64,\n+        }\n+\n         let result = sst_by_family\n             .into_par_iter()\n             .with_min_len(1)\n             .enumerate()\n             .map(|(family, ssts_with_ranges)| {\n+                let family = family as u32;\n                 let _span = span.clone().entered();\n                 let coverage = total_coverage(&ssts_with_ranges, (0, u64::MAX));\n                 if coverage <= max_coverage {\n-                    return Ok((Vec::new(), Vec::new()));\n+                    return Ok(PartialResultPerFamily {\n+                        new_meta_file: None,\n+                        new_sst_files: Vec::new(),\n+                        sst_seq_numbers_to_delete: Vec::new(),\n+                        blob_seq_numbers_to_delete: Vec::new(),\n+                        keys_written: 0,\n+                    });\n                 }\n \n                 let CompactionJobs {\n@@ -689,53 +794,53 @@ impl TurboPersistence {\n                     for job in merge_jobs.iter() {\n                         writeln!(log, \"  merge\")?;\n                         for i in job.iter() {\n-                            let index = ssts_with_ranges[*i].index;\n+                            let seq = ssts_with_ranges[*i].seq;\n                             let (min, max) = ssts_with_ranges[*i].range();\n-                            writeln!(log, \"    {index:08} {min:016x}-{max:016x}\")?;\n-                        }\n-                    }\n-                    if !move_jobs.is_empty() {\n-                        writeln!(log, \"  move\")?;\n-                        for i in move_jobs.iter() {\n-                            let index = ssts_with_ranges[*i].index;\n-                            let (min, max) = ssts_with_ranges[*i].range();\n-                            writeln!(log, \"    {index:08} {min:016x}-{max:016x}\")?;\n+                            writeln!(log, \"    {seq:08} {min:016x}-{max:016x}\")?;\n                         }\n                     }\n                     drop(guard);\n                 }\n \n-                // Later we will remove the merged and moved files\n-                let indicies_to_delete = merge_jobs\n+                // Later we will remove the merged files\n+                let sst_seq_numbers_to_delete = merge_jobs\n                     .iter()\n                     .flat_map(|l| l.iter().copied())\n-                    .chain(move_jobs.iter().copied())\n-                    .map(|index| ssts_with_ranges[index].index)\n+                    .map(|index| ssts_with_ranges[index].seq)\n                     .collect::<Vec<_>>();\n \n+                let meta_file_builder = Mutex::new(MetaFileBuilder::new(family));\n+\n                 // Merge SST files\n                 let span = tracing::trace_span!(\"merge files\");\n+                struct PartialMergeResult {\n+                    new_sst_files: Vec<(u32, File)>,\n+                    blob_seq_numbers_to_delete: Vec<u32>,\n+                    keys_written: u64,\n+                }\n                 let merge_result = merge_jobs\n                     .into_par_iter()\n                     .with_min_len(1)\n                     .map(|indicies| {\n                         let _span = span.clone().entered();\n                         fn create_sst_file(\n-                            family: u32,\n                             entries: &[LookupEntry],\n                             total_key_size: usize,\n                             total_value_size: usize,\n                             path: &Path,\n                             seq: u32,\n+                            meta_file_builder: &Mutex<MetaFileBuilder>,\n                         ) -> Result<(u32, File)> {\n                             let _span = tracing::trace_span!(\"write merged sst file\").entered();\n                             let builder = StaticSortedFileBuilder::new(\n-                                family,\n                                 entries,\n                                 total_key_size,\n                                 total_value_size,\n                             )?;\n-                            Ok((seq, builder.write(&path.join(format!(\"{seq:08}.sst\")))?))\n+                            let (meta, file) =\n+                                builder.write(&path.join(format!(\"{seq:08}.sst\")))?;\n+                            meta_file_builder.lock().add(seq, meta);\n+                            Ok((seq, file))\n                         }\n \n                         let mut new_sst_files = Vec::new();\n@@ -744,14 +849,23 @@ impl TurboPersistence {\n                         let iters = indicies\n                             .iter()\n                             .map(|&index| {\n-                                let index = ssts_with_ranges[index].index;\n-                                let sst = &static_sorted_files[index];\n-                                sst.iter(key_block_cache, value_block_cache)\n+                                let meta_index = ssts_with_ranges[index].meta_index;\n+                                let index_in_meta = ssts_with_ranges[index].index_in_meta;\n+                                let meta = &meta_files[meta_index];\n+                                meta.entry(index_in_meta)\n+                                    .sst(&self.path)?\n+                                    .iter(key_block_cache, value_block_cache)\n                             })\n                             .collect::<Result<Vec<_>>>()?;\n \n                         let iter = MergeIter::new(iters.into_iter())?;\n \n+                        // TODO figure out how to delete blobs when they are no longer\n+                        // referenced\n+                        let blob_seq_numbers_to_delete: Vec<u32> = Vec::new();\n+\n+                        let mut keys_written = 0;\n+\n                         let mut total_key_size = 0;\n                         let mut total_value_size = 0;\n                         let mut current: Option<LookupEntry> = None;\n@@ -787,13 +901,14 @@ impl TurboPersistence {\n                                             let seq =\n                                                 sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n \n+                                            keys_written += entries.len() as u64;\n                                             new_sst_files.push(create_sst_file(\n-                                                family as u32,\n                                                 &entries,\n                                                 selected_total_key_size,\n                                                 selected_total_value_size,\n                                                 path,\n                                                 seq,\n+                                                &meta_file_builder,\n                                             )?);\n \n                                             entries.clear();\n@@ -817,13 +932,14 @@ impl TurboPersistence {\n                         if last_entries.is_empty() && !entries.is_empty() {\n                             let seq = sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n \n+                            keys_written += entries.len() as u64;\n                             new_sst_files.push(create_sst_file(\n-                                family as u32,\n                                 &entries,\n                                 total_key_size,\n                                 total_value_size,\n                                 path,\n                                 seq,\n+                                &meta_file_builder,\n                             )?);\n                         } else\n                         // If we have two sets of entries left, merge them and\n@@ -840,63 +956,129 @@ impl TurboPersistence {\n                             let seq1 = sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n                             let seq2 = sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n \n+                            keys_written += part1.len() as u64;\n                             new_sst_files.push(create_sst_file(\n-                                family as u32,\n                                 part1,\n                                 // We don't know the exact sizes so we estimate them\n                                 last_entries_total_sizes.0 / 2,\n                                 last_entries_total_sizes.1 / 2,\n                                 path,\n                                 seq1,\n+                                &meta_file_builder,\n                             )?);\n \n+                            keys_written += part2.len() as u64;\n                             new_sst_files.push(create_sst_file(\n-                                family as u32,\n                                 part2,\n                                 last_entries_total_sizes.0 / 2,\n                                 last_entries_total_sizes.1 / 2,\n                                 path,\n                                 seq2,\n+                                &meta_file_builder,\n                             )?);\n                         }\n-                        Ok(new_sst_files)\n+                        Ok(PartialMergeResult {\n+                            new_sst_files,\n+                            blob_seq_numbers_to_delete,\n+                            keys_written,\n+                        })\n                     })\n                     .collect::<Result<Vec<_>>>()?;\n \n-                let move_jobs = move_jobs\n-                    .into_iter()\n-                    .map(|index| {\n-                        let seq = sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n-                        (index, seq)\n-                    })\n-                    .collect::<Vec<_>>();\n+                let mut meta_file_builder = meta_file_builder.into_inner();\n \n-                // Move SST files\n-                let span = tracing::trace_span!(\"move files\");\n-                let mut new_sst_files = move_jobs\n-                    .into_par_iter()\n-                    .with_min_len(1)\n-                    .map(|(index, seq)| {\n-                        let _span = span.clone().entered();\n-                        let index = ssts_with_ranges[index].index;\n-                        let sst = &static_sorted_files[index];\n-                        let src_path = self.path.join(format!(\"{:08}.sst\", sst.sequence_number()));\n-                        let dst_path = self.path.join(format!(\"{seq:08}.sst\"));\n-                        if fs::hard_link(&src_path, &dst_path).is_err() {\n-                            fs::copy(src_path, &dst_path)?;\n-                        }\n-                        Ok((seq, File::open(dst_path)?))\n-                    })\n-                    .collect::<Result<Vec<_>>>()?;\n+                for &seq in sst_seq_numbers_to_delete.iter() {\n+                    meta_file_builder.add_obsolete_sst_file(seq);\n+                }\n \n-                new_sst_files.extend(merge_result.into_iter().flatten());\n-                Ok((new_sst_files, indicies_to_delete))\n+                // Move SST files\n+                let span = tracing::trace_span!(\"query moved sst files\").entered();\n+                for index in move_jobs {\n+                    let meta_index = ssts_with_ranges[index].meta_index;\n+                    let index_in_meta = ssts_with_ranges[index].index_in_meta;\n+                    let meta_file = &meta_files[meta_index];\n+                    let entry = meta_file.entry(index_in_meta);\n+                    let aqmf = entry.aqmf(meta_file.aqmf_data()).to_vec();\n+                    let meta = StaticSortedFileBuilderMeta {\n+                        min_hash: entry.min_hash(),\n+                        max_hash: entry.max_hash(),\n+                        aqmf,\n+                        key_compression_dictionary_length: entry\n+                            .key_compression_dictionary_length(),\n+                        value_compression_dictionary_length: entry\n+                            .value_compression_dictionary_length(),\n+                        block_count: entry.block_count(),\n+                        size: entry.size(),\n+                        entries: 0,\n+                    };\n+                    meta_file_builder.add(entry.sequence_number(), meta);\n+                }\n+                drop(span);\n+\n+                let span = tracing::trace_span!(\"write meta file\").entered();\n+                let seq = sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n+                let meta_file =\n+                    meta_file_builder.write(&self.path.join(format!(\"{seq:08}.meta\")))?;\n+                drop(span);\n+\n+                let mut new_sst_files = Vec::with_capacity(\n+                    merge_result\n+                        .iter()\n+                        .map(\n+                            |PartialMergeResult {\n+                                 new_sst_files: v,\n+                                 blob_seq_numbers_to_delete: _,\n+                                 keys_written: _,\n+                             }| v.len(),\n+                        )\n+                        .sum(),\n+                );\n+                let mut blob_seq_numbers_to_delete = Vec::with_capacity(\n+                    merge_result\n+                        .iter()\n+                        .map(\n+                            |PartialMergeResult {\n+                                 new_sst_files: _,\n+                                 blob_seq_numbers_to_delete: v,\n+                                 keys_written: _,\n+                             }| v.len(),\n+                        )\n+                        .sum(),\n+                );\n+                let mut keys_written = 0;\n+                for PartialMergeResult {\n+                    new_sst_files: merged_new_sst_files,\n+                    blob_seq_numbers_to_delete: merged_blob_seq_numbers_to_delete,\n+                    keys_written: merged_keys_written,\n+                } in merge_result\n+                {\n+                    new_sst_files.extend(merged_new_sst_files);\n+                    blob_seq_numbers_to_delete.extend(merged_blob_seq_numbers_to_delete);\n+                    keys_written += merged_keys_written;\n+                }\n+                Ok(PartialResultPerFamily {\n+                    new_meta_file: Some((seq, meta_file)),\n+                    new_sst_files,\n+                    sst_seq_numbers_to_delete,\n+                    blob_seq_numbers_to_delete,\n+                    keys_written,\n+                })\n             })\n             .collect::<Result<Vec<_>>>()?;\n \n-        for (mut inner_new_sst_files, mut inner_indicies_to_delete) in result {\n+        for PartialResultPerFamily {\n+            new_meta_file: inner_new_meta_file,\n+            new_sst_files: mut inner_new_sst_files,\n+            sst_seq_numbers_to_delete: mut inner_sst_seq_numbers_to_delete,\n+            blob_seq_numbers_to_delete: mut inner_blob_seq_numbers_to_delete,\n+            keys_written: inner_keys_written,\n+        } in result\n+        {\n+            new_meta_files.extend(inner_new_meta_file);\n             new_sst_files.append(&mut inner_new_sst_files);\n-            indicies_to_delete.append(&mut inner_indicies_to_delete);\n+            sst_seq_numbers_to_delete.append(&mut inner_sst_seq_numbers_to_delete);\n+            blob_seq_numbers_to_delete.append(&mut inner_blob_seq_numbers_to_delete);\n+            *keys_written += inner_keys_written;\n         }\n \n         Ok(())\n@@ -907,43 +1089,51 @@ impl TurboPersistence {\n     pub fn get<K: QueryKey>(&self, family: usize, key: &K) -> Result<Option<ArcSlice<u8>>> {\n         let hash = hash_key(key);\n         let inner = self.inner.read();\n-        for sst in inner.static_sorted_files.iter().rev() {\n-            match sst.lookup(\n+        for meta in inner.meta_files.iter().rev() {\n+            match meta.lookup(\n                 family as u32,\n                 hash,\n                 key,\n                 &self.aqmf_cache,\n                 &self.key_block_cache,\n                 &self.value_block_cache,\n             )? {\n-                LookupResult::Deleted => {\n+                MetaLookupResult::FamilyMiss => {\n                     #[cfg(feature = \"stats\")]\n-                    self.stats.hits_deleted.fetch_add(1, Ordering::Relaxed);\n-                    return Ok(None);\n+                    self.stats.miss_family.fetch_add(1, Ordering::Relaxed);\n                 }\n-                LookupResult::Slice { value } => {\n-                    #[cfg(feature = \"stats\")]\n-                    self.stats.hits_small.fetch_add(1, Ordering::Relaxed);\n-                    return Ok(Some(value));\n-                }\n-                LookupResult::Blob { sequence_number } => {\n-                    #[cfg(feature = \"stats\")]\n-                    self.stats.hits_blob.fetch_add(1, Ordering::Relaxed);\n-                    let blob = self.read_blob(sequence_number)?;\n-                    return Ok(Some(blob));\n-                }\n-                LookupResult::RangeMiss => {\n+                MetaLookupResult::RangeMiss => {\n                     #[cfg(feature = \"stats\")]\n                     self.stats.miss_range.fetch_add(1, Ordering::Relaxed);\n                 }\n-                LookupResult::QuickFilterMiss => {\n+                MetaLookupResult::QuickFilterMiss => {\n                     #[cfg(feature = \"stats\")]\n                     self.stats.miss_aqmf.fetch_add(1, Ordering::Relaxed);\n                 }\n-                LookupResult::KeyMiss => {\n-                    #[cfg(feature = \"stats\")]\n-                    self.stats.miss_key.fetch_add(1, Ordering::Relaxed);\n-                }\n+                MetaLookupResult::SstLookup(result) => match result {\n+                    SstLookupResult::Found(result) => match result {\n+                        LookupValue::Deleted => {\n+                            #[cfg(feature = \"stats\")]\n+                            self.stats.hits_deleted.fetch_add(1, Ordering::Relaxed);\n+                            return Ok(None);\n+                        }\n+                        LookupValue::Slice { value } => {\n+                            #[cfg(feature = \"stats\")]\n+                            self.stats.hits_small.fetch_add(1, Ordering::Relaxed);\n+                            return Ok(Some(value));\n+                        }\n+                        LookupValue::Blob { sequence_number } => {\n+                            #[cfg(feature = \"stats\")]\n+                            self.stats.hits_blob.fetch_add(1, Ordering::Relaxed);\n+                            let blob = self.read_blob(sequence_number)?;\n+                            return Ok(Some(blob));\n+                        }\n+                    },\n+                    SstLookupResult::NotFound => {\n+                        #[cfg(feature = \"stats\")]\n+                        self.stats.miss_key.fetch_add(1, Ordering::Relaxed);\n+                    }\n+                },\n             }\n         }\n         #[cfg(feature = \"stats\")]\n@@ -956,14 +1146,16 @@ impl TurboPersistence {\n     pub fn statistics(&self) -> Statistics {\n         let inner = self.inner.read();\n         Statistics {\n-            sst_files: inner.static_sorted_files.len(),\n+            meta_files: inner.meta_files.len(),\n+            sst_files: inner.meta_files.iter().map(|m| m.entries().len()).sum(),\n             key_block_cache: CacheStatistics::new(&self.key_block_cache),\n             value_block_cache: CacheStatistics::new(&self.value_block_cache),\n             aqmf_cache: CacheStatistics::new(&self.aqmf_cache),\n             hits: self.stats.hits_deleted.load(Ordering::Relaxed)\n                 + self.stats.hits_small.load(Ordering::Relaxed)\n                 + self.stats.hits_blob.load(Ordering::Relaxed),\n             misses: self.stats.miss_global.load(Ordering::Relaxed),\n+            miss_family: self.stats.miss_family.load(Ordering::Relaxed),\n             miss_range: self.stats.miss_range.load(Ordering::Relaxed),\n             miss_aqmf: self.stats.miss_aqmf.load(Ordering::Relaxed),\n             miss_key: self.stats.miss_key.load(Ordering::Relaxed),\n@@ -977,65 +1169,3 @@ impl TurboPersistence {\n         Ok(())\n     }\n }\n-\n-/// Helper method to remove certain indicies from a list while keeping the order.\n-/// This is similar to the `remove` method on Vec, but it allows to remove multiple indicies at\n-/// once. It returns the removed elements in unspecified order.\n-///\n-/// Note: The `sorted_indicies` list needs to be sorted.\n-fn remove_indicies<T>(list: &mut Vec<T>, sorted_indicies: &[usize]) -> Vec<T> {\n-    let mut r = 0;\n-    let mut w = 0;\n-    let mut i = 0;\n-    while r < list.len() {\n-        if i < sorted_indicies.len() {\n-            let idx = sorted_indicies[i];\n-            if r != idx {\n-                list.swap(w, r);\n-                w += 1;\n-                r += 1;\n-            } else {\n-                r += 1;\n-                i += 1;\n-            }\n-        } else {\n-            list.swap(w, r);\n-            w += 1;\n-            r += 1;\n-        }\n-    }\n-    list.split_off(w)\n-}\n-\n-#[cfg(test)]\n-mod tests {\n-    use crate::db::remove_indicies;\n-\n-    #[test]\n-    fn test_remove_indicies() {\n-        let mut list = vec![1, 2, 3, 4, 5, 6, 7, 8, 9];\n-        let sorted_indicies = vec![1, 3, 5, 7];\n-        let removed = remove_indicies(&mut list, &sorted_indicies);\n-        assert_eq!(list, vec![1, 3, 5, 7, 9]);\n-        assert!(removed.contains(&2));\n-        assert!(removed.contains(&4));\n-        assert!(removed.contains(&6));\n-        assert!(removed.contains(&8));\n-        assert_eq!(removed.len(), 4);\n-    }\n-\n-    #[test]\n-    fn test_remove_indicies2() {\n-        let mut list = vec![1, 2, 3, 4, 5, 6, 7, 8, 9];\n-        let sorted_indicies = vec![0, 1, 2, 6, 7, 8];\n-        let removed = remove_indicies(&mut list, &sorted_indicies);\n-        assert_eq!(list, vec![4, 5, 6]);\n-        assert!(removed.contains(&1));\n-        assert!(removed.contains(&2));\n-        assert!(removed.contains(&3));\n-        assert!(removed.contains(&7));\n-        assert!(removed.contains(&8));\n-        assert!(removed.contains(&9));\n-        assert_eq!(removed.len(), 6);\n-    }\n-}"
        },
        {
            "sha": "9f6ec1fc1e9aae00c99c134f4e504296d833456f",
            "filename": "turbopack/crates/turbo-persistence/src/lib.rs",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/vercel/next.js/blob/f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Flib.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Flib.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Flib.rs?ref=f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83",
            "patch": "@@ -2,6 +2,7 @@\n #![feature(new_zeroed_alloc)]\n #![feature(get_mut_unchecked)]\n #![feature(sync_unsafe_cell)]\n+#![feature(iter_collect_into)]\n \n mod arc_slice;\n mod collector;\n@@ -16,6 +17,9 @@ mod static_sorted_file;\n mod static_sorted_file_builder;\n mod write_batch;\n \n+mod meta_file;\n+mod meta_file_builder;\n+mod sst_filter;\n #[cfg(test)]\n mod tests;\n mod value_buf;"
        },
        {
            "sha": "c459e409640bb65fe11b6f74c419486a027ce121",
            "filename": "turbopack/crates/turbo-persistence/src/meta_file.rs",
            "status": "added",
            "additions": 297,
            "deletions": 0,
            "changes": 297,
            "blob_url": "https://github.com/vercel/next.js/blob/f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fmeta_file.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fmeta_file.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fmeta_file.rs?ref=f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83",
            "patch": "@@ -0,0 +1,297 @@\n+use std::{\n+    fs::File,\n+    hash::BuildHasherDefault,\n+    io::{BufReader, Seek},\n+    path::{Path, PathBuf},\n+    sync::{Arc, OnceLock},\n+};\n+\n+use anyhow::{Context, Result, bail};\n+use byteorder::{BE, ReadBytesExt};\n+use memmap2::{Mmap, MmapOptions};\n+use quick_cache::sync::GuardResult;\n+use rustc_hash::FxHasher;\n+\n+use crate::{\n+    QueryKey,\n+    static_sorted_file::{BlockCache, SstLookupResult, StaticSortedFile, StaticSortedFileMetaData},\n+};\n+\n+#[derive(Clone, Default)]\n+pub struct AqmfWeighter;\n+\n+impl quick_cache::Weighter<u32, Arc<qfilter::Filter>> for AqmfWeighter {\n+    fn weight(&self, _key: &u32, filter: &Arc<qfilter::Filter>) -> u64 {\n+        filter.capacity() + 1\n+    }\n+}\n+\n+pub type AqmfCache =\n+    quick_cache::sync::Cache<u32, Arc<qfilter::Filter>, AqmfWeighter, BuildHasherDefault<FxHasher>>;\n+\n+pub struct MetaEntry {\n+    /// The metadata for the static sorted file.\n+    sst_data: StaticSortedFileMetaData,\n+    /// The key family of the SST file.\n+    family: u32,\n+    /// The minimum hash value of the keys in the SST file.\n+    min_hash: u64,\n+    /// The maximum hash value of the keys in the SST file.\n+    max_hash: u64,\n+    /// The size of the SST file in bytes.\n+    size: u64,\n+    /// The offset of the start of the AQMF data in the meta file relative to the end of the\n+    /// header.\n+    start_of_aqmf_data_offset: u32,\n+    /// The offset of the end of the AQMF data in the the meta file relative to the end of the\n+    /// header.\n+    end_of_aqmf_data_offset: u32,\n+    /// The AQMF filter of this file. This is only used if the range is very large. Smaller ranges\n+    /// use the AQMF cache instead.\n+    aqmf: OnceLock<qfilter::Filter>,\n+    /// The static sorted file that is lazily loaded\n+    sst: OnceLock<StaticSortedFile>,\n+}\n+\n+impl MetaEntry {\n+    pub fn sequence_number(&self) -> u32 {\n+        self.sst_data.sequence_number\n+    }\n+\n+    pub fn size(&self) -> u64 {\n+        self.size\n+    }\n+\n+    pub fn aqmf<'l>(&self, aqmf_data: &'l [u8]) -> &'l [u8] {\n+        aqmf_data\n+            .get(self.start_of_aqmf_data_offset as usize..self.end_of_aqmf_data_offset as usize)\n+            .expect(\"AQMF data out of bounds\")\n+    }\n+\n+    pub fn sst(&self, db_path: &Path) -> Result<&StaticSortedFile> {\n+        self.sst\n+            .get_or_try_init(|| StaticSortedFile::open(db_path, self.sst_data.clone()))\n+    }\n+\n+    /// Returns the key family and hash range of this file.\n+    pub fn range(&self) -> StaticSortedFileRange {\n+        StaticSortedFileRange {\n+            family: self.family,\n+            min_hash: self.min_hash,\n+            max_hash: self.max_hash,\n+        }\n+    }\n+\n+    pub fn min_hash(&self) -> u64 {\n+        self.min_hash\n+    }\n+\n+    pub fn max_hash(&self) -> u64 {\n+        self.max_hash\n+    }\n+\n+    pub fn key_compression_dictionary_length(&self) -> u16 {\n+        self.sst_data.key_compression_dictionary_length\n+    }\n+\n+    pub fn value_compression_dictionary_length(&self) -> u16 {\n+        self.sst_data.value_compression_dictionary_length\n+    }\n+\n+    pub fn block_count(&self) -> u16 {\n+        self.sst_data.block_count\n+    }\n+}\n+\n+/// The result of a lookup operation.\n+pub enum MetaLookupResult {\n+    /// The key was not found because it is from a different key family.\n+    FamilyMiss,\n+    /// The key was not found because it is out of the range of this SST file. But it was the\n+    /// correct key family.\n+    RangeMiss,\n+    /// The key was not found because it was not in the AQMF filter. But it was in the range.\n+    QuickFilterMiss,\n+    /// The key was looked up in the SST file. It was in the AQMF filter.\n+    SstLookup(SstLookupResult),\n+}\n+\n+/// The key family and hash range of an SST file.\n+#[derive(Clone, Copy)]\n+pub struct StaticSortedFileRange {\n+    pub family: u32,\n+    pub min_hash: u64,\n+    pub max_hash: u64,\n+}\n+\n+pub struct MetaFile {\n+    /// The database path\n+    db_path: PathBuf,\n+    /// The sequence number of this file.\n+    sequence_number: u32,\n+    /// The key family of the SST files in this meta file.\n+    family: u32,\n+    /// The entries of the file.\n+    entries: Vec<MetaEntry>,\n+    /// The obsolete SST files.\n+    obsolete_sst_files: Vec<u32>,\n+    /// The memory mapped file.\n+    mmap: Mmap,\n+}\n+\n+impl MetaFile {\n+    /// Opens a meta file at the given path. This memory maps the file, but does not read it yet.\n+    /// It's lazy read on demand.\n+    pub fn open(db_path: &Path, sequence_number: u32) -> Result<Self> {\n+        let filename = format!(\"{sequence_number:08}.meta\");\n+        let path = db_path.join(&filename);\n+        Self::open_internal(db_path.to_path_buf(), sequence_number, &path)\n+            .with_context(|| format!(\"Unable to open meta file {filename}\"))\n+    }\n+\n+    fn open_internal(db_path: PathBuf, sequence_number: u32, path: &Path) -> Result<Self> {\n+        let mut file = BufReader::new(File::open(path)?);\n+        let magic = file.read_u32::<BE>()?;\n+        if magic != 0xFE4ADA4A {\n+            bail!(\"Invalid magic number\");\n+        }\n+        let family = file.read_u32::<BE>()?;\n+        let obsolete_count = file.read_u32::<BE>()?;\n+        let mut obsolete_sst_files = Vec::with_capacity(obsolete_count as usize);\n+        for _ in 0..obsolete_count {\n+            let obsolete_sst = file.read_u32::<BE>()?;\n+            obsolete_sst_files.push(obsolete_sst);\n+        }\n+        let count = file.read_u32::<BE>()?;\n+        let mut entries = Vec::with_capacity(count as usize);\n+        let mut start_of_aqmf_data_offset = 0;\n+        for _ in 0..count {\n+            let entry = MetaEntry {\n+                sst_data: StaticSortedFileMetaData {\n+                    sequence_number: file.read_u32::<BE>()?,\n+                    key_compression_dictionary_length: file.read_u16::<BE>()?,\n+                    value_compression_dictionary_length: file.read_u16::<BE>()?,\n+                    block_count: file.read_u16::<BE>()?,\n+                },\n+                family,\n+                min_hash: file.read_u64::<BE>()?,\n+                max_hash: file.read_u64::<BE>()?,\n+                size: file.read_u64::<BE>()?,\n+                start_of_aqmf_data_offset,\n+                end_of_aqmf_data_offset: file.read_u32::<BE>()?,\n+                aqmf: OnceLock::new(),\n+                sst: OnceLock::new(),\n+            };\n+            start_of_aqmf_data_offset = entry.end_of_aqmf_data_offset;\n+            entries.push(entry);\n+        }\n+        let offset = file.stream_position()?;\n+        let file = file.into_inner();\n+        let mut options = MmapOptions::new();\n+        options.offset(offset);\n+        let mmap = unsafe { options.map(&file)? };\n+        #[cfg(unix)]\n+        mmap.advise(memmap2::Advice::Random)?;\n+        let file = Self {\n+            db_path,\n+            sequence_number,\n+            family,\n+            entries,\n+            obsolete_sst_files,\n+            mmap,\n+        };\n+        Ok(file)\n+    }\n+\n+    pub fn sequence_number(&self) -> u32 {\n+        self.sequence_number\n+    }\n+\n+    pub fn family(&self) -> u32 {\n+        self.family\n+    }\n+\n+    pub fn entries(&self) -> &[MetaEntry] {\n+        &self.entries\n+    }\n+\n+    pub fn entry(&self, index: u32) -> &MetaEntry {\n+        let index = index as usize;\n+        &self.entries[index]\n+    }\n+\n+    pub fn aqmf_data(&self) -> &[u8] {\n+        &self.mmap\n+    }\n+\n+    pub fn retain_entries(&mut self, mut predicate: impl FnMut(u32) -> bool) -> bool {\n+        let old_len = self.entries.len();\n+        self.entries\n+            .retain(|entry| predicate(entry.sst_data.sequence_number));\n+        old_len != self.entries.len()\n+    }\n+\n+    pub fn has_active_entries(&self) -> bool {\n+        !self.entries.is_empty()\n+    }\n+\n+    pub fn obsolete_sst_files(&self) -> &[u32] {\n+        &self.obsolete_sst_files\n+    }\n+\n+    pub fn lookup<K: QueryKey>(\n+        &self,\n+        key_family: u32,\n+        key_hash: u64,\n+        key: &K,\n+        aqmf_cache: &AqmfCache,\n+        key_block_cache: &BlockCache,\n+        value_block_cache: &BlockCache,\n+    ) -> Result<MetaLookupResult> {\n+        if key_family != self.family {\n+            return Ok(MetaLookupResult::FamilyMiss);\n+        }\n+        let mut miss_result = MetaLookupResult::RangeMiss;\n+        for entry in self.entries.iter() {\n+            if key_hash < entry.min_hash || key_hash > entry.max_hash {\n+                continue;\n+            }\n+            let use_aqmf_cache = entry.max_hash - entry.min_hash < 1 << 60;\n+            if use_aqmf_cache {\n+                let aqmf = match aqmf_cache.get_value_or_guard(&entry.sequence_number(), None) {\n+                    GuardResult::Value(aqmf) => aqmf,\n+                    GuardResult::Guard(guard) => {\n+                        let aqmf = entry.aqmf(self.aqmf_data());\n+                        let aqmf: Arc<qfilter::Filter> = Arc::new(pot::from_slice(aqmf)?);\n+                        let _ = guard.insert(aqmf.clone());\n+                        aqmf\n+                    }\n+                    GuardResult::Timeout => unreachable!(),\n+                };\n+                if !aqmf.contains_fingerprint(key_hash) {\n+                    miss_result = MetaLookupResult::QuickFilterMiss;\n+                    continue;\n+                }\n+            } else {\n+                let aqmf = entry.aqmf.get_or_try_init(|| {\n+                    let aqmf = entry.aqmf(self.aqmf_data());\n+                    anyhow::Ok(pot::from_slice(aqmf)?)\n+                })?;\n+                if !aqmf.contains_fingerprint(key_hash) {\n+                    miss_result = MetaLookupResult::QuickFilterMiss;\n+                    continue;\n+                }\n+            };\n+            let result = entry.sst(&self.db_path)?.lookup(\n+                key_hash,\n+                key,\n+                key_block_cache,\n+                value_block_cache,\n+            )?;\n+            if !matches!(result, SstLookupResult::NotFound) {\n+                return Ok(MetaLookupResult::SstLookup(result));\n+            }\n+        }\n+        Ok(miss_result)\n+    }\n+}"
        },
        {
            "sha": "9b002db9ae6bb32252e9b1dce8a060efd2370975",
            "filename": "turbopack/crates/turbo-persistence/src/meta_file_builder.rs",
            "status": "added",
            "additions": 67,
            "deletions": 0,
            "changes": 67,
            "blob_url": "https://github.com/vercel/next.js/blob/f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fmeta_file_builder.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fmeta_file_builder.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fmeta_file_builder.rs?ref=f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83",
            "patch": "@@ -0,0 +1,67 @@\n+use std::{\n+    fs::File,\n+    io::{self, BufWriter, Write},\n+    path::Path,\n+};\n+\n+use byteorder::{BE, WriteBytesExt};\n+\n+use crate::static_sorted_file_builder::StaticSortedFileBuilderMeta;\n+\n+pub struct MetaFileBuilder {\n+    family: u32,\n+    /// Entries in the meta file, tuples of (sequence_number, StaticSortedFileBuilderMetaResult)\n+    entries: Vec<(u32, StaticSortedFileBuilderMeta)>,\n+    /// Obsolete SST files, represented by their sequence numbers\n+    obsolete_sst_files: Vec<u32>,\n+}\n+\n+impl MetaFileBuilder {\n+    pub fn new(family: u32) -> Self {\n+        Self {\n+            family,\n+            entries: Vec::new(),\n+            obsolete_sst_files: Vec::new(),\n+        }\n+    }\n+\n+    pub fn add(&mut self, sequence_number: u32, sst: StaticSortedFileBuilderMeta) {\n+        self.entries.push((sequence_number, sst));\n+    }\n+\n+    pub fn add_obsolete_sst_file(&mut self, sequence_number: u32) {\n+        self.obsolete_sst_files.push(sequence_number);\n+    }\n+\n+    #[tracing::instrument(level = \"trace\", skip_all)]\n+    pub fn write(&self, file: &Path) -> io::Result<File> {\n+        let mut file = BufWriter::new(File::create(file)?);\n+        file.write_u32::<BE>(0xFE4ADA4A)?; // Magic number\n+        file.write_u32::<BE>(self.family)?;\n+\n+        file.write_u32::<BE>(self.obsolete_sst_files.len() as u32)?;\n+        for obsolete_sst in &self.obsolete_sst_files {\n+            file.write_u32::<BE>(*obsolete_sst)?;\n+        }\n+\n+        file.write_u32::<BE>(self.entries.len() as u32)?;\n+\n+        let mut aqmf_offset = 0;\n+        for (sequence_number, sst) in &self.entries {\n+            file.write_u32::<BE>(*sequence_number)?;\n+            file.write_u16::<BE>(sst.key_compression_dictionary_length)?;\n+            file.write_u16::<BE>(sst.value_compression_dictionary_length)?;\n+            file.write_u16::<BE>(sst.block_count)?;\n+            file.write_u64::<BE>(sst.min_hash)?;\n+            file.write_u64::<BE>(sst.max_hash)?;\n+            file.write_u64::<BE>(sst.size)?;\n+            aqmf_offset += sst.aqmf.len();\n+            file.write_u32::<BE>(aqmf_offset as u32)?;\n+        }\n+\n+        for (_, sst) in &self.entries {\n+            file.write_all(&sst.aqmf)?;\n+        }\n+        Ok(file.into_inner()?)\n+    }\n+}"
        },
        {
            "sha": "28f4268a0fb976ca650f13e905a05dac987855a2",
            "filename": "turbopack/crates/turbo-persistence/src/sst_filter.rs",
            "status": "added",
            "additions": 69,
            "deletions": 0,
            "changes": 69,
            "blob_url": "https://github.com/vercel/next.js/blob/f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fsst_filter.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fsst_filter.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fsst_filter.rs?ref=f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83",
            "patch": "@@ -0,0 +1,69 @@\n+use std::collections::hash_map::Entry;\n+\n+use rustc_hash::FxHashMap;\n+\n+use crate::meta_file::MetaFile;\n+\n+enum SstState {\n+    Active,\n+    UnusedObsolete,\n+    Obsolete,\n+}\n+\n+pub struct SstFilter(FxHashMap<u32, SstState>);\n+\n+impl SstFilter {\n+    pub fn new() -> Self {\n+        Self(FxHashMap::default())\n+    }\n+\n+    /// Phase 1: Apply the filter to the meta file and update the state in the filter.\n+    pub fn apply_filter(&mut self, meta: &mut MetaFile) {\n+        meta.retain_entries(|seq| match self.0.entry(seq) {\n+            Entry::Occupied(mut e) => {\n+                let state = e.get_mut();\n+                match state {\n+                    SstState::Active => false,\n+                    SstState::UnusedObsolete => {\n+                        *state = SstState::Obsolete;\n+                        false\n+                    }\n+                    SstState::Obsolete => false,\n+                }\n+            }\n+            Entry::Vacant(e) => {\n+                e.insert(SstState::Active);\n+                true\n+            }\n+        });\n+        for seq in meta.obsolete_sst_files() {\n+            self.0.entry(*seq).or_insert(SstState::UnusedObsolete);\n+        }\n+    }\n+\n+    /// Phase 2: Check if the meta file can be removed based on the filter state after phase 1.\n+    /// Updates the filter state for the next meta file. Returns true if the meta file can be\n+    /// removed.\n+    pub fn apply_and_get_remove(&mut self, meta: &MetaFile) -> bool {\n+        let mut used = false;\n+        for seq in meta.obsolete_sst_files() {\n+            match self.0.entry(*seq) {\n+                Entry::Occupied(e) => match e.get() {\n+                    SstState::Active => {}\n+                    SstState::UnusedObsolete => {\n+                        e.remove();\n+                    }\n+                    SstState::Obsolete => {\n+                        e.remove();\n+                        used = true;\n+                    }\n+                },\n+                Entry::Vacant(e) => {\n+                    e.insert(SstState::UnusedObsolete);\n+                }\n+            }\n+        }\n+\n+        !used && !meta.has_active_entries()\n+    }\n+}"
        },
        {
            "sha": "070aa5605a01e7e8f5e895eddc76395cef42bb69",
            "filename": "turbopack/crates/turbo-persistence/src/static_sorted_file.rs",
            "status": "modified",
            "additions": 105,
            "deletions": 249,
            "changes": 354,
            "blob_url": "https://github.com/vercel/next.js/blob/f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fstatic_sorted_file.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fstatic_sorted_file.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fstatic_sorted_file.rs?ref=f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83",
            "patch": "@@ -3,11 +3,12 @@ use std::{\n     fs::File,\n     hash::BuildHasherDefault,\n     mem::{MaybeUninit, transmute},\n-    path::PathBuf,\n-    sync::{Arc, OnceLock},\n+    ops::Range,\n+    path::{Path, PathBuf},\n+    sync::Arc,\n };\n \n-use anyhow::{Result, bail};\n+use anyhow::{Context, Result, bail};\n use byteorder::{BE, ReadBytesExt};\n use lzzzz::lz4::decompress_with_dict;\n use memmap2::Mmap;\n@@ -35,73 +36,16 @@ pub const KEY_BLOCK_ENTRY_TYPE_DELETED: u8 = 2;\n pub const KEY_BLOCK_ENTRY_TYPE_MEDIUM: u8 = 3;\n \n /// The result of a lookup operation.\n-pub enum LookupResult {\n-    /// The key was deleted.\n-    Deleted,\n-    /// The key was found and the value is a slice.\n-    Slice { value: ArcSlice<u8> },\n-    /// The key was found and the value is a blob.\n-    Blob { sequence_number: u32 },\n-    /// The key was not found because it is out of the range of this SST file.\n-    RangeMiss,\n-    /// The key was not found because it was not in the AQMF filter. But it was in the range.\n-    QuickFilterMiss,\n-    /// The key was not found. But it was in the range and the AQMF filter.\n-    KeyMiss,\n+pub enum SstLookupResult {\n+    /// The key was found.\n+    Found(LookupValue),\n+    /// The key was not found.\n+    NotFound,\n }\n \n-impl From<LookupValue> for LookupResult {\n+impl From<LookupValue> for SstLookupResult {\n     fn from(value: LookupValue) -> Self {\n-        match value {\n-            LookupValue::Deleted => LookupResult::Deleted,\n-            LookupValue::Slice { value } => LookupResult::Slice { value },\n-            LookupValue::Blob { sequence_number } => LookupResult::Blob { sequence_number },\n-        }\n-    }\n-}\n-\n-/// A byte range in the SST file.\n-struct LocationInFile {\n-    start: usize,\n-    end: usize,\n-}\n-\n-/// The read and parsed header of an SST file.\n-struct Header {\n-    /// The key family stored in this file.\n-    family: u32,\n-    /// The minimum hash value in this file.\n-    min_hash: u64,\n-    /// The maximum hash value in this file.\n-    max_hash: u64,\n-    /// The location of the AQMF filter in the file.\n-    aqmf: LocationInFile,\n-    /// The location of the key compression dictionary in the file.\n-    key_compression_dictionary: LocationInFile,\n-    /// The location of the value compression dictionary in the file.\n-    value_compression_dictionary: LocationInFile,\n-    /// The byte offset where the block offsets start.\n-    block_offsets_start: usize,\n-    /// The byte offset where the blocks start.\n-    blocks_start: usize,\n-    /// The number of blocks in this file.\n-    block_count: u16,\n-}\n-\n-/// The key family and hash range of an SST file.\n-#[derive(Clone, Copy)]\n-pub struct StaticSortedFileRange {\n-    pub family: u32,\n-    pub min_hash: u64,\n-    pub max_hash: u64,\n-}\n-\n-#[derive(Clone, Default)]\n-pub struct AqmfWeighter;\n-\n-impl quick_cache::Weighter<u32, Arc<qfilter::Filter>> for AqmfWeighter {\n-    fn weight(&self, _key: &u32, filter: &Arc<qfilter::Filter>) -> u64 {\n-        filter.capacity() + 1\n+        SstLookupResult::Found(value)\n     }\n }\n \n@@ -114,105 +58,70 @@ impl quick_cache::Weighter<(u32, u16), ArcSlice<u8>> for BlockWeighter {\n     }\n }\n \n-pub type AqmfCache =\n-    quick_cache::sync::Cache<u32, Arc<qfilter::Filter>, AqmfWeighter, BuildHasherDefault<FxHasher>>;\n pub type BlockCache =\n     quick_cache::sync::Cache<(u32, u16), ArcSlice<u8>, BlockWeighter, BuildHasherDefault<FxHasher>>;\n \n-/// A memory mapped SST file.\n-pub struct StaticSortedFile {\n+#[derive(Clone, Debug)]\n+pub struct StaticSortedFileMetaData {\n     /// The sequence number of this file.\n-    sequence_number: u32,\n-    /// The memory mapped file.\n-    mmap: Mmap,\n-    /// The parsed header of this file.\n-    header: OnceLock<Header>,\n-    /// The AQMF filter of this file. This is only used if the range is very large. Smaller ranges\n-    /// use the AQMF cache instead.\n-    aqmf: OnceLock<qfilter::Filter>,\n+    pub sequence_number: u32,\n+    /// The length of the key compression dictionary.\n+    pub key_compression_dictionary_length: u16,\n+    /// The length of the value compression dictionary.\n+    pub value_compression_dictionary_length: u16,\n+    /// The number of blocks in the SST file.\n+    pub block_count: u16,\n }\n \n-impl StaticSortedFile {\n-    /// The sequence number of this file.\n-    pub fn sequence_number(&self) -> u32 {\n-        self.sequence_number\n+impl StaticSortedFileMetaData {\n+    pub fn block_offsets_start(&self) -> usize {\n+        let k: usize = self.key_compression_dictionary_length.into();\n+        let v: usize = self.value_compression_dictionary_length.into();\n+        k + v\n     }\n \n-    /// The size of this file in bytes.\n-    pub fn size(&self) -> usize {\n-        self.mmap.len()\n+    pub fn blocks_start(&self) -> usize {\n+        let bc: usize = self.block_count.into();\n+        self.block_offsets_start() + bc * size_of::<u32>()\n     }\n \n-    /// Opens an SST file at the given path. This memory maps the file, but does not read it yet.\n-    /// It's lazy read on demand.\n-    pub fn open(sequence_number: u32, path: PathBuf) -> Result<Self> {\n-        let mmap = unsafe { Mmap::map(&File::open(&path)?)? };\n-        let file = Self {\n-            sequence_number,\n-            mmap,\n-            header: OnceLock::new(),\n-            aqmf: OnceLock::new(),\n-        };\n-        Ok(file)\n+    pub fn key_compression_dictionary_range(&self) -> Range<usize> {\n+        let start = 0;\n+        let end: usize = self.key_compression_dictionary_length.into();\n+        start..end\n     }\n \n-    /// Reads and parses the header of this file if it hasn't been read yet.\n-    fn header(&self) -> Result<&Header> {\n-        self.header.get_or_try_init(|| {\n-            let mut file = &*self.mmap;\n-            let magic = file.read_u32::<BE>()?;\n-            if magic != 0x53535401 {\n-                bail!(\"Invalid magic number or version\");\n-            }\n-            let family = file.read_u32::<BE>()?;\n-            let min_hash = file.read_u64::<BE>()?;\n-            let max_hash = file.read_u64::<BE>()?;\n-            let aqmf_length = file.read_u24::<BE>()? as usize;\n-            let key_compression_dictionary_length = file.read_u16::<BE>()? as usize;\n-            let value_compression_dictionary_length = file.read_u16::<BE>()? as usize;\n-            let block_count = file.read_u16::<BE>()?;\n-            const HEADER_SIZE: usize = 33;\n-            let mut current_offset = HEADER_SIZE;\n-            let aqmf = LocationInFile {\n-                start: current_offset,\n-                end: current_offset + aqmf_length,\n-            };\n-            current_offset += aqmf_length;\n-            let key_compression_dictionary = LocationInFile {\n-                start: current_offset,\n-                end: current_offset + key_compression_dictionary_length,\n-            };\n-            current_offset += key_compression_dictionary_length;\n-            let value_compression_dictionary = LocationInFile {\n-                start: current_offset,\n-                end: current_offset + value_compression_dictionary_length,\n-            };\n-            current_offset += value_compression_dictionary_length;\n-            let block_offsets_start = current_offset;\n-            let blocks_start = block_offsets_start + block_count as usize * 4;\n-\n-            Ok(Header {\n-                family,\n-                min_hash,\n-                max_hash,\n-                aqmf,\n-                key_compression_dictionary,\n-                value_compression_dictionary,\n-                block_offsets_start,\n-                blocks_start,\n-                block_count,\n-            })\n-        })\n+    pub fn value_compression_dictionary_range(&self) -> Range<usize> {\n+        let start = self.key_compression_dictionary_length as usize;\n+        let end = start + self.value_compression_dictionary_length as usize;\n+        start..end\n     }\n+}\n \n-    /// Returns the key family and hash range of this file.\n-    pub fn range(&self) -> Result<StaticSortedFileRange> {\n-        let header = self.header()?;\n-        Ok(StaticSortedFileRange {\n-            family: header.family,\n-            min_hash: header.min_hash,\n-            max_hash: header.max_hash,\n-        })\n+/// A memory mapped SST file.\n+pub struct StaticSortedFile {\n+    /// The meta file of this file.\n+    meta: StaticSortedFileMetaData,\n+    /// The memory mapped file.\n+    mmap: Mmap,\n+}\n+\n+impl StaticSortedFile {\n+    /// Opens an SST file at the given path. This memory maps the file, but does not read it yet.\n+    /// It's lazy read on demand.\n+    pub fn open(db_path: &Path, meta: StaticSortedFileMetaData) -> Result<Self> {\n+        let filename = format!(\"{:08}.sst\", meta.sequence_number);\n+        let path = db_path.join(&filename);\n+        Self::open_internal(path, meta)\n+            .with_context(|| format!(\"Unable to open static sorted file {filename}\"))\n+    }\n+\n+    fn open_internal(path: PathBuf, meta: StaticSortedFileMetaData) -> Result<Self> {\n+        let mmap = unsafe { Mmap::map(&File::open(&path)?)? };\n+        #[cfg(unix)]\n+        mmap.advise(memmap2::Advice::Random)?;\n+        let file = Self { meta, mmap };\n+        Ok(file)\n     }\n \n     /// Iterate over all entries in this file in sorted order.\n@@ -221,69 +130,36 @@ impl StaticSortedFile {\n         key_block_cache: &'l BlockCache,\n         value_block_cache: &'l BlockCache,\n     ) -> Result<StaticSortedFileIter<'l>> {\n-        let header = self.header()?;\n         let mut iter = StaticSortedFileIter {\n             this: self,\n             key_block_cache,\n             value_block_cache,\n-            header,\n             stack: Vec::new(),\n             current_key_block: None,\n         };\n-        iter.enter_block(header.block_count - 1)?;\n+        iter.enter_block(self.meta.block_count - 1)?;\n         Ok(iter)\n     }\n \n     /// Looks up a key in this file.\n     pub fn lookup<K: QueryKey>(\n         &self,\n-        key_family: u32,\n         key_hash: u64,\n         key: &K,\n-        aqmf_cache: &AqmfCache,\n         key_block_cache: &BlockCache,\n         value_block_cache: &BlockCache,\n-    ) -> Result<LookupResult> {\n-        let header = self.header()?;\n-        if key_family != header.family || key_hash < header.min_hash || key_hash > header.max_hash {\n-            return Ok(LookupResult::RangeMiss);\n-        }\n-\n-        let use_aqmf_cache = header.max_hash - header.min_hash < 1 << 62;\n-        if use_aqmf_cache {\n-            let aqmf = match aqmf_cache.get_value_or_guard(&self.sequence_number, None) {\n-                GuardResult::Value(aqmf) => aqmf,\n-                GuardResult::Guard(guard) => {\n-                    let aqmf = &self.mmap[header.aqmf.start..header.aqmf.end];\n-                    let aqmf: Arc<qfilter::Filter> = Arc::new(pot::from_slice(aqmf)?);\n-                    let _ = guard.insert(aqmf.clone());\n-                    aqmf\n-                }\n-                GuardResult::Timeout => unreachable!(),\n-            };\n-            if !aqmf.contains_fingerprint(key_hash) {\n-                return Ok(LookupResult::QuickFilterMiss);\n-            }\n-        } else {\n-            let aqmf = self.aqmf.get_or_try_init(|| {\n-                let aqmf = &self.mmap[header.aqmf.start..header.aqmf.end];\n-                anyhow::Ok(pot::from_slice(aqmf)?)\n-            })?;\n-            if !aqmf.contains_fingerprint(key_hash) {\n-                return Ok(LookupResult::QuickFilterMiss);\n-            }\n-        }\n-        let mut current_block = header.block_count - 1;\n+    ) -> Result<SstLookupResult> {\n+        let mut current_block = self.meta.block_count - 1;\n         loop {\n-            let block = self.get_key_block(header, current_block, key_block_cache)?;\n+            let block = self.get_key_block(current_block, key_block_cache)?;\n             let mut block = &block[..];\n             let block_type = block.read_u8()?;\n             match block_type {\n                 BLOCK_TYPE_INDEX => {\n                     current_block = self.lookup_index_block(block, key_hash)?;\n                 }\n                 BLOCK_TYPE_KEY => {\n-                    return self.lookup_key_block(block, key_hash, key, header, value_block_cache);\n+                    return self.lookup_key_block(block, key_hash, key, value_block_cache);\n                 }\n                 _ => {\n                     bail!(\"Invalid block type\");\n@@ -344,9 +220,8 @@ impl StaticSortedFile {\n         mut block: &[u8],\n         key_hash: u64,\n         key: &K,\n-        header: &Header,\n         value_block_cache: &BlockCache,\n-    ) -> Result<LookupResult> {\n+    ) -> Result<SstLookupResult> {\n         let entry_count = block.read_u24::<BE>()? as usize;\n         let offsets = &block[..entry_count * 4];\n         let entries = &block[entry_count * 4..];\n@@ -368,23 +243,22 @@ impl StaticSortedFile {\n                 }\n                 Ordering::Equal => {\n                     return Ok(self\n-                        .handle_key_match(ty, mid_val, header, value_block_cache)?\n+                        .handle_key_match(ty, mid_val, value_block_cache)?\n                         .into());\n                 }\n                 Ordering::Greater => {\n                     l = m + 1;\n                 }\n             }\n         }\n-        Ok(LookupResult::KeyMiss)\n+        Ok(SstLookupResult::NotFound)\n     }\n \n     /// Handles a key match by looking up the value.\n     fn handle_key_match(\n         &self,\n         ty: u8,\n         mut val: &[u8],\n-        header: &Header,\n         value_block_cache: &BlockCache,\n     ) -> Result<LookupValue> {\n         Ok(match ty {\n@@ -393,13 +267,13 @@ impl StaticSortedFile {\n                 let size = val.read_u16::<BE>()? as usize;\n                 let position = val.read_u32::<BE>()? as usize;\n                 let value = self\n-                    .get_value_block(header, block, value_block_cache)?\n+                    .get_value_block(block, value_block_cache)?\n                     .slice(position..position + size);\n                 LookupValue::Slice { value }\n             }\n             KEY_BLOCK_ENTRY_TYPE_MEDIUM => {\n                 let block = val.read_u16::<BE>()?;\n-                let value = self.read_value_block(header, block)?;\n+                let value = self.read_value_block(block)?;\n                 LookupValue::Slice { value }\n             }\n             KEY_BLOCK_ENTRY_TYPE_BLOB => {\n@@ -416,15 +290,14 @@ impl StaticSortedFile {\n     /// Gets a key block from the cache or reads it from the file.\n     fn get_key_block(\n         &self,\n-        header: &Header,\n         block: u16,\n         key_block_cache: &BlockCache,\n     ) -> Result<ArcSlice<u8>, anyhow::Error> {\n         Ok(\n-            match key_block_cache.get_value_or_guard(&(self.sequence_number, block), None) {\n+            match key_block_cache.get_value_or_guard(&(self.meta.sequence_number, block), None) {\n                 GuardResult::Value(block) => block,\n                 GuardResult::Guard(guard) => {\n-                    let block = self.read_key_block(header, block)?;\n+                    let block = self.read_key_block(block)?;\n                     let _ = guard.insert(block.clone());\n                     block\n                 }\n@@ -434,65 +307,51 @@ impl StaticSortedFile {\n     }\n \n     /// Gets a value block from the cache or reads it from the file.\n-    fn get_value_block(\n-        &self,\n-        header: &Header,\n-        block: u16,\n-        value_block_cache: &BlockCache,\n-    ) -> Result<ArcSlice<u8>> {\n-        let block = match value_block_cache.get_value_or_guard(&(self.sequence_number, block), None)\n-        {\n-            GuardResult::Value(block) => block,\n-            GuardResult::Guard(guard) => {\n-                let block = self.read_value_block(header, block)?;\n-                let _ = guard.insert(block.clone());\n-                block\n-            }\n-            GuardResult::Timeout => unreachable!(),\n-        };\n+    fn get_value_block(&self, block: u16, value_block_cache: &BlockCache) -> Result<ArcSlice<u8>> {\n+        let block =\n+            match value_block_cache.get_value_or_guard(&(self.meta.sequence_number, block), None) {\n+                GuardResult::Value(block) => block,\n+                GuardResult::Guard(guard) => {\n+                    let block = self.read_value_block(block)?;\n+                    let _ = guard.insert(block.clone());\n+                    block\n+                }\n+                GuardResult::Timeout => unreachable!(),\n+            };\n         Ok(block)\n     }\n \n     /// Reads a key block from the file.\n-    fn read_key_block(&self, header: &Header, block_index: u16) -> Result<ArcSlice<u8>> {\n+    fn read_key_block(&self, block_index: u16) -> Result<ArcSlice<u8>> {\n         self.read_block(\n-            header,\n             block_index,\n-            &self.mmap\n-                [header.key_compression_dictionary.start..header.key_compression_dictionary.end],\n+            &self.mmap[self.meta.key_compression_dictionary_range()],\n         )\n     }\n \n     /// Reads a value block from the file.\n-    fn read_value_block(&self, header: &Header, block_index: u16) -> Result<ArcSlice<u8>> {\n+    fn read_value_block(&self, block_index: u16) -> Result<ArcSlice<u8>> {\n         self.read_block(\n-            header,\n             block_index,\n-            &self.mmap[header.value_compression_dictionary.start\n-                ..header.value_compression_dictionary.end],\n+            &self.mmap[self.meta.value_compression_dictionary_range()],\n         )\n     }\n \n     /// Reads a block from the file.\n-    fn read_block(\n-        &self,\n-        header: &Header,\n-        block_index: u16,\n-        compression_dictionary: &[u8],\n-    ) -> Result<ArcSlice<u8>> {\n+    fn read_block(&self, block_index: u16, compression_dictionary: &[u8]) -> Result<ArcSlice<u8>> {\n         #[cfg(feature = \"strict_checks\")]\n-        if block_index >= header.block_count {\n+        if block_index >= self.meta.block_count {\n             bail!(\n                 \"Corrupted file seq:{} block:{} > number of blocks {} (block_offsets: {:x}, \\\n                  blocks: {:x})\",\n                 self.sequence_number,\n                 block_index,\n-                header.block_count,\n-                header.block_offsets_start,\n-                header.blocks_start\n+                self.meta.block_count,\n+                self.meta.block_offsets_start(),\n+                self.meta.blocks_start()\n             );\n         }\n-        let offset = header.block_offsets_start + block_index as usize * 4;\n+        let offset = self.meta.block_offsets_start() + block_index as usize * 4;\n         #[cfg(feature = \"strict_checks\")]\n         if offset + 4 > self.mmap.len() {\n             bail!(\n@@ -502,17 +361,17 @@ impl StaticSortedFile {\n                 block_index,\n                 offset,\n                 self.mmap.len(),\n-                header.block_offsets_start,\n-                header.blocks_start\n+                self.meta.block_offsets_start(),\n+                self.meta.blocks_start()\n             );\n         }\n         let block_start = if block_index == 0 {\n-            header.blocks_start\n+            self.meta.blocks_start()\n         } else {\n-            header.blocks_start + (&self.mmap[offset - 4..offset]).read_u32::<BE>()? as usize\n+            self.meta.blocks_start() + (&self.mmap[offset - 4..offset]).read_u32::<BE>()? as usize\n         };\n         let block_end =\n-            header.blocks_start + (&self.mmap[offset..offset + 4]).read_u32::<BE>()? as usize;\n+            self.meta.blocks_start() + (&self.mmap[offset..offset + 4]).read_u32::<BE>()? as usize;\n         #[cfg(feature = \"strict_checks\")]\n         if block_end > self.mmap.len() || block_start > self.mmap.len() {\n             bail!(\n@@ -523,8 +382,8 @@ impl StaticSortedFile {\n                 block_start,\n                 block_end,\n                 self.mmap.len(),\n-                header.block_offsets_start,\n-                header.blocks_start\n+                self.meta.block_offsets_start(),\n+                self.meta.blocks_start()\n             );\n         }\n         let uncompressed_length =\n@@ -546,7 +405,6 @@ pub struct StaticSortedFileIter<'l> {\n     this: &'l StaticSortedFile,\n     key_block_cache: &'l BlockCache,\n     value_block_cache: &'l BlockCache,\n-    header: &'l Header,\n \n     stack: Vec<CurrentIndexBlock>,\n     current_key_block: Option<CurrentKeyBlock>,\n@@ -576,9 +434,7 @@ impl Iterator for StaticSortedFileIter<'_> {\n impl StaticSortedFileIter<'_> {\n     /// Enters a block at the given index.\n     fn enter_block(&mut self, block_index: u16) -> Result<()> {\n-        let block_arc = self\n-            .this\n-            .get_key_block(self.header, block_index, self.key_block_cache)?;\n+        let block_arc = self.this.get_key_block(block_index, self.key_block_cache)?;\n         let mut block = &*block_arc;\n         let block_type = block.read_u8()?;\n         match block_type {\n@@ -623,9 +479,9 @@ impl StaticSortedFileIter<'_> {\n             {\n                 let GetKeyEntryResult { hash, key, ty, val } =\n                     get_key_entry(&offsets, &entries, entry_count, index)?;\n-                let value =\n-                    self.this\n-                        .handle_key_match(ty, val, self.header, self.value_block_cache)?;\n+                let value = self\n+                    .this\n+                    .handle_key_match(ty, val, self.value_block_cache)?;\n                 let entry = LookupEntry {\n                     hash,\n                     // Safety: The key is a valid slice of the entries."
        },
        {
            "sha": "6e1de95888ff02597cf9a16617b13fcaf0848d6f",
            "filename": "turbopack/crates/turbo-persistence/src/static_sorted_file_builder.rs",
            "status": "modified",
            "additions": 50,
            "deletions": 28,
            "changes": 78,
            "blob_url": "https://github.com/vercel/next.js/blob/f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fstatic_sorted_file_builder.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fstatic_sorted_file_builder.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fstatic_sorted_file_builder.rs?ref=f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83",
            "patch": "@@ -1,7 +1,7 @@\n use std::{\n     cmp::min,\n     fs::File,\n-    io::{self, BufWriter, Write},\n+    io::{self, BufWriter, Seek, Write},\n     path::Path,\n };\n \n@@ -71,29 +71,48 @@ pub enum EntryValue<'l> {\n     Deleted,\n }\n \n+#[derive(Debug, Clone)]\n+pub struct StaticSortedFileBuilderMeta {\n+    /// The minimum hash of the keys in the SST file\n+    pub min_hash: u64,\n+    /// The maximum hash of the keys in the SST file\n+    pub max_hash: u64,\n+    /// The AQMF data\n+    pub aqmf: Vec<u8>,\n+    /// The key compression dictionary\n+    pub key_compression_dictionary_length: u16,\n+    /// The value compression dictionary\n+    pub value_compression_dictionary_length: u16,\n+    /// The number of blocks in the SST file\n+    pub block_count: u16,\n+    /// The file size of the SST file\n+    pub size: u64,\n+    /// The number of entries in the SST file\n+    pub entries: u64,\n+}\n+\n #[derive(Debug, Default)]\n pub struct StaticSortedFileBuilder {\n-    family: u32,\n     aqmf: Vec<u8>,\n     key_compression_dictionary: Vec<u8>,\n     value_compression_dictionary: Vec<u8>,\n     blocks: Vec<(u32, Vec<u8>)>,\n     min_hash: u64,\n     max_hash: u64,\n+    entries: u64,\n }\n \n impl StaticSortedFileBuilder {\n     pub fn new<E: Entry>(\n-        family: u32,\n         entries: &[E],\n         total_key_size: usize,\n         total_value_size: usize,\n     ) -> Result<Self> {\n         debug_assert!(entries.iter().map(|e| e.key_hash()).is_sorted());\n         let mut builder = Self {\n-            family,\n             min_hash: entries.first().map(|e| e.key_hash()).unwrap_or(u64::MAX),\n             max_hash: entries.last().map(|e| e.key_hash()).unwrap_or(0),\n+            entries: entries.len() as u64,\n             ..Default::default()\n         };\n         builder.compute_aqmf(entries);\n@@ -114,6 +133,9 @@ impl StaticSortedFileBuilder {\n                 // This can't fail as we allocated enough capacity\n                 .expect(\"AQMF insert failed\");\n         }\n+        for entry in entries {\n+            debug_assert!(filter.contains_fingerprint(entry.key_hash()));\n+        }\n         self.aqmf = pot::to_vec(&filter).expect(\"AQMF serialization failed\");\n     }\n \n@@ -368,27 +390,8 @@ impl StaticSortedFileBuilder {\n \n     /// Writes the SST file.\n     #[tracing::instrument(level = \"trace\", skip_all)]\n-    pub fn write(&self, file: &Path) -> io::Result<File> {\n+    pub fn write(self, file: &Path) -> io::Result<(StaticSortedFileBuilderMeta, File)> {\n         let mut file = BufWriter::new(File::create(file)?);\n-        // magic number and version\n-        file.write_u32::<BE>(0x53535401)?;\n-        // family\n-        file.write_u32::<BE>(self.family)?;\n-        // min hash\n-        file.write_u64::<BE>(self.min_hash)?;\n-        // max hash\n-        file.write_u64::<BE>(self.max_hash)?;\n-        // AQMF length\n-        file.write_u24::<BE>(self.aqmf.len().try_into().unwrap())?;\n-        // Key compression dictionary length\n-        file.write_u16::<BE>(self.key_compression_dictionary.len().try_into().unwrap())?;\n-        // Value compression dictionary length\n-        file.write_u16::<BE>(self.value_compression_dictionary.len().try_into().unwrap())?;\n-        // Number of blocks\n-        file.write_u16::<BE>(self.blocks.len().try_into().unwrap())?;\n-\n-        // Write the AQMF\n-        file.write_all(&self.aqmf)?;\n         // Write the key compression dictionary\n         file.write_all(&self.key_compression_dictionary)?;\n         // Write the value compression dictionary\n@@ -402,13 +405,32 @@ impl StaticSortedFileBuilder {\n             offset += len;\n             file.write_u32::<BE>(offset.try_into().unwrap())?;\n         }\n-        for (uncompressed_size, block) in &self.blocks {\n+        let block_count = self.blocks.len().try_into().unwrap();\n+        for (uncompressed_size, block) in self.blocks {\n             // Uncompressed size\n-            file.write_u32::<BE>(*uncompressed_size)?;\n+            file.write_u32::<BE>(uncompressed_size)?;\n             // Compressed block\n-            file.write_all(block)?;\n+            file.write_all(&block)?;\n         }\n-        Ok(file.into_inner()?)\n+        let meta = StaticSortedFileBuilderMeta {\n+            min_hash: self.min_hash,\n+            max_hash: self.max_hash,\n+            aqmf: self.aqmf,\n+            key_compression_dictionary_length: self\n+                .key_compression_dictionary\n+                .len()\n+                .try_into()\n+                .unwrap(),\n+            value_compression_dictionary_length: self\n+                .value_compression_dictionary\n+                .len()\n+                .try_into()\n+                .unwrap(),\n+            block_count,\n+            size: file.stream_position()?,\n+            entries: self.entries,\n+        };\n+        Ok((meta, file.into_inner()?))\n     }\n }\n "
        },
        {
            "sha": "decac820add469cca4aa6d4a4d937eff873b5c4c",
            "filename": "turbopack/crates/turbo-persistence/src/tests.rs",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/vercel/next.js/blob/f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Ftests.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Ftests.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Ftests.rs?ref=f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83",
            "patch": "@@ -455,7 +455,7 @@ fn persist_changes() -> Result<()> {\n     {\n         let db = TurboPersistence::open(path.to_path_buf())?;\n \n-        db.compact(1.0, 3, usize::MAX)?;\n+        db.compact(1.0, 3, u64::MAX)?;\n \n         check(&db, 1, 13)?;\n         check(&db, 2, 22)?;"
        },
        {
            "sha": "65b5836eb579e8b2f99b17a19b3111a1afbf4ecf",
            "filename": "turbopack/crates/turbo-persistence/src/write_batch.rs",
            "status": "modified",
            "additions": 70,
            "deletions": 34,
            "changes": 104,
            "blob_url": "https://github.com/vercel/next.js/blob/f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fwrite_batch.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fwrite_batch.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-persistence%2Fsrc%2Fwrite_batch.rs?ref=f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83",
            "patch": "@@ -4,7 +4,7 @@ use std::{\n     io::Write,\n     mem::{replace, take},\n     path::PathBuf,\n-    sync::atomic::{AtomicU32, Ordering},\n+    sync::atomic::{AtomicU32, AtomicU64, Ordering},\n };\n \n use anyhow::{Context, Result};\n@@ -25,7 +25,8 @@ use crate::{\n     collector_entry::CollectorEntry,\n     constants::{MAX_MEDIUM_VALUE_SIZE, THREAD_LOCAL_SIZE_SHIFT},\n     key::StoreKey,\n-    static_sorted_file_builder::StaticSortedFileBuilder,\n+    meta_file_builder::MetaFileBuilder,\n+    static_sorted_file_builder::{StaticSortedFileBuilder, StaticSortedFileBuilderMeta},\n };\n \n /// The thread local state of a `WriteBatch`. `FAMILIES` should fit within a `u32`.\n@@ -49,9 +50,13 @@ const COLLECTOR_SHARD_SHIFT: usize =\n pub(crate) struct FinishResult {\n     pub(crate) sequence_number: u32,\n     /// Tuple of (sequence number, file).\n+    pub(crate) new_meta_files: Vec<(u32, File)>,\n+    /// Tuple of (sequence number, file).\n     pub(crate) new_sst_files: Vec<(u32, File)>,\n     /// Tuple of (sequence number, file).\n     pub(crate) new_blob_files: Vec<(u32, File)>,\n+    /// Number of keys written in this batch.\n+    pub(crate) keys_written: u64,\n }\n \n enum GlobalCollectorState<K: StoreKey + Send> {\n@@ -65,13 +70,15 @@ enum GlobalCollectorState<K: StoreKey + Send> {\n /// A write batch.\n pub struct WriteBatch<K: StoreKey + Send, const FAMILIES: usize> {\n     /// The database path\n-    path: PathBuf,\n+    db_path: PathBuf,\n     /// The current sequence number counter. Increased for every new SST file or blob file.\n     current_sequence_number: AtomicU32,\n     /// The thread local state.\n     thread_locals: ThreadLocal<SyncUnsafeCell<ThreadLocalState<K, FAMILIES>>>,\n     /// Collectors in use. The thread local collectors flush into these when they are full.\n     collectors: [Mutex<GlobalCollectorState<K>>; FAMILIES],\n+    /// Meta file builders for each family.\n+    meta_collectors: [Mutex<Vec<(u32, StaticSortedFileBuilderMeta)>>; FAMILIES],\n     /// The list of new SST files that have been created.\n     /// Tuple of (sequence number, file).\n     new_sst_files: Mutex<Vec<(u32, File)>>,\n@@ -88,11 +95,12 @@ impl<K: StoreKey + Send + Sync, const FAMILIES: usize> WriteBatch<K, FAMILIES> {\n             assert!(FAMILIES <= usize_from_u32(u32::MAX));\n         };\n         Self {\n-            path,\n+            db_path: path,\n             current_sequence_number: AtomicU32::new(current),\n             thread_locals: ThreadLocal::new(),\n             collectors: [(); FAMILIES]\n                 .map(|_| Mutex::new(GlobalCollectorState::Unsharded(Collector::new()))),\n+            meta_collectors: [(); FAMILIES].map(|_| Mutex::new(Vec::new())),\n             new_sst_files: Mutex::new(Vec::new()),\n             idle_collectors: Mutex::new(Vec::new()),\n             idle_thread_local_collectors: Mutex::new(Vec::new()),\n@@ -369,12 +377,42 @@ impl<K: StoreKey + Send + Sync, const FAMILIES: usize> WriteBatch<K, FAMILIES> {\n             })?;\n \n         shared_error.into_inner()?;\n+\n+        // Not we need to write the new meta files.\n+        let new_meta_collectors = [(); FAMILIES].map(|_| Mutex::new(Vec::new()));\n+        let meta_collectors = replace(&mut self.meta_collectors, new_meta_collectors);\n+        let keys_written = AtomicU64::new(0);\n+        let new_meta_files = meta_collectors\n+            .into_par_iter()\n+            .map(|mutex| mutex.into_inner())\n+            .enumerate()\n+            .filter(|(_, sst_files)| !sst_files.is_empty())\n+            .map(|(family, sst_files)| {\n+                let family = family as u32;\n+                let mut entries = 0;\n+                let mut builder = MetaFileBuilder::new(family);\n+                for (seq, sst) in sst_files {\n+                    entries += sst.entries;\n+                    builder.add(seq, sst);\n+                }\n+                keys_written.fetch_add(entries, Ordering::Relaxed);\n+                let seq = self.current_sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n+                let file = self.db_path.join(format!(\"{seq:08}.meta\"));\n+                let file = builder\n+                    .write(&file)\n+                    .with_context(|| format!(\"Unable to write meta file {seq:08}.meta\"))?;\n+                Ok((seq, file))\n+            })\n+            .collect::<Result<Vec<_>>>()?;\n+\n+        // Finally we return the new files and sequence number.\n         let seq = self.current_sequence_number.load(Ordering::SeqCst);\n-        new_sst_files.sort_unstable_by_key(|(seq, _)| *seq);\n         Ok(FinishResult {\n             sequence_number: seq,\n+            new_meta_files,\n             new_sst_files,\n             new_blob_files,\n+            keys_written: keys_written.into_inner(),\n         })\n     }\n \n@@ -388,7 +426,7 @@ impl<K: StoreKey + Send + Sync, const FAMILIES: usize> WriteBatch<K, FAMILIES> {\n         lz4::compress_to_vec(value, &mut buffer, ACC_LEVEL_DEFAULT)\n             .context(\"Compression of value for blob file failed\")?;\n \n-        let file = self.path.join(format!(\"{seq:08}.blob\"));\n+        let file = self.db_path.join(format!(\"{seq:08}.blob\"));\n         let mut file = File::create(&file).context(\"Unable to create blob file\")?;\n         file.write_all(&buffer)\n             .context(\"Unable to write blob file\")?;\n@@ -407,11 +445,10 @@ impl<K: StoreKey + Send + Sync, const FAMILIES: usize> WriteBatch<K, FAMILIES> {\n         let (entries, total_key_size, total_value_size) = collector_data;\n         let seq = self.current_sequence_number.fetch_add(1, Ordering::SeqCst) + 1;\n \n-        let builder =\n-            StaticSortedFileBuilder::new(family, entries, total_key_size, total_value_size)?;\n+        let builder = StaticSortedFileBuilder::new(entries, total_key_size, total_value_size)?;\n \n-        let path = self.path.join(format!(\"{seq:08}.sst\"));\n-        let file = builder\n+        let path = self.db_path.join(format!(\"{seq:08}.sst\"));\n+        let (meta, file) = builder\n             .write(&path)\n             .with_context(|| format!(\"Unable to write SST file {seq:08}.sst\"))?;\n \n@@ -422,19 +459,23 @@ impl<K: StoreKey + Send + Sync, const FAMILIES: usize> WriteBatch<K, FAMILIES> {\n             use crate::{\n                 collector_entry::CollectorEntryValue,\n                 key::hash_key,\n-                static_sorted_file::{AqmfCache, BlockCache, LookupResult, StaticSortedFile},\n+                lookup_entry::LookupValue,\n+                static_sorted_file::{\n+                    BlockCache, SstLookupResult, StaticSortedFile, StaticSortedFileMetaData,\n+                },\n                 static_sorted_file_builder::Entry,\n             };\n \n             file.sync_all()?;\n-            let sst = StaticSortedFile::open(seq, path)?;\n-            let cache1 = AqmfCache::with(\n-                10,\n-                u64::MAX,\n-                Default::default(),\n-                Default::default(),\n-                Default::default(),\n-            );\n+            let sst = StaticSortedFile::open(\n+                &self.db_path,\n+                StaticSortedFileMetaData {\n+                    sequence_number: seq,\n+                    key_compression_dictionary_length: meta.key_compression_dictionary_length,\n+                    value_compression_dictionary_length: meta.value_compression_dictionary_length,\n+                    block_count: meta.block_count,\n+                },\n+            )?;\n             let cache2 = BlockCache::with(\n                 10,\n                 u64::MAX,\n@@ -453,36 +494,31 @@ impl<K: StoreKey + Send + Sync, const FAMILIES: usize> WriteBatch<K, FAMILIES> {\n             for entry in entries {\n                 entry.write_key_to(&mut key_buf);\n                 let result = sst\n-                    .lookup(\n-                        family,\n-                        hash_key(&key_buf),\n-                        &key_buf,\n-                        &cache1,\n-                        &cache2,\n-                        &cache3,\n-                    )\n+                    .lookup(hash_key(&key_buf), &key_buf, &cache2, &cache3)\n                     .expect(\"key found\");\n                 key_buf.clear();\n                 match result {\n-                    LookupResult::Deleted => {}\n-                    LookupResult::Slice {\n+                    SstLookupResult::Found(LookupValue::Deleted) => {}\n+                    SstLookupResult::Found(LookupValue::Slice {\n                         value: lookup_value,\n-                    } => {\n+                    }) => {\n                         let expected_value_slice = match &entry.value {\n                             CollectorEntryValue::Small { value } => &**value,\n                             CollectorEntryValue::Medium { value } => &**value,\n                             _ => panic!(\"Unexpected value\"),\n                         };\n                         assert_eq!(*lookup_value, *expected_value_slice);\n                     }\n-                    LookupResult::Blob { sequence_number: _ } => {}\n-                    LookupResult::QuickFilterMiss => panic!(\"aqmf must include\"),\n-                    LookupResult::RangeMiss => panic!(\"Index must cover\"),\n-                    LookupResult::KeyMiss => panic!(\"All keys must exist\"),\n+                    SstLookupResult::Found(LookupValue::Blob { sequence_number: _ }) => {}\n+                    SstLookupResult::NotFound => panic!(\"All keys must exist\"),\n                 }\n             }\n         }\n \n+        self.meta_collectors[usize_from_u32(family)]\n+            .lock()\n+            .push((seq, meta));\n+\n         Ok((seq, file))\n     }\n }"
        },
        {
            "sha": "fa1bd13f290c5c69d49626c5dbe67cb2ea5a83c7",
            "filename": "turbopack/crates/turbo-tasks-backend/src/database/turbo.rs",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/vercel/next.js/blob/f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fdatabase%2Fturbo.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fdatabase%2Fturbo.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fdatabase%2Fturbo.rs?ref=f3a6889ea2d352f770d1a9d9d37c5973f3ba8e83",
            "patch": "@@ -15,7 +15,7 @@ use crate::database::{\n \n const COMPACT_MAX_COVERAGE: f32 = 20.0;\n const COMPACT_MAX_MERGE_SEQUENCE: usize = 64;\n-const COMPACT_MAX_MERGE_SIZE: usize = 512 * 1024 * 1024; // 512 MiB\n+const COMPACT_MAX_MERGE_SIZE: u64 = 512 * 1024 * 1024; // 512 MiB\n \n pub struct TurboKeyValueDatabase {\n     db: Arc<TurboPersistence>,"
        }
    ],
    "stats": {
        "total": 1683,
        "additions": 1092,
        "deletions": 591
    }
}