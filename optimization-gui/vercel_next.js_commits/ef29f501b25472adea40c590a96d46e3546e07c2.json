{
    "author": "sokra",
    "message": "Turbopack: increase retry duration, reduce busy looping when there is other work (#84171)\n\n### What?\n\nThe short retry timeout of 10s could lead to false panics. Increasing the retry count to 60s makes it less likely.\n\nThere is this case where the job need to wait for another job in the same queue to continue. This is handled by re-enqueuing the job at the end of the queue again. But it's not optimal to spend 1ms in busy waiting initially. So early retries will re-enqueue faster and the busy loop is only used aggressively after a bunch of retries.",
    "sha": "ef29f501b25472adea40c590a96d46e3546e07c2",
    "files": [
        {
            "sha": "58c3bc0c56bf928cd95c1661d45a78ded3fc574f",
            "filename": "turbopack/crates/turbo-tasks-backend/src/backend/operation/aggregation_update.rs",
            "status": "modified",
            "additions": 8,
            "deletions": 4,
            "changes": 12,
            "blob_url": "https://github.com/vercel/next.js/blob/ef29f501b25472adea40c590a96d46e3546e07c2/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Foperation%2Faggregation_update.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/ef29f501b25472adea40c590a96d46e3546e07c2/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Foperation%2Faggregation_update.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fbackend%2Foperation%2Faggregation_update.rs?ref=ef29f501b25472adea40c590a96d46e3546e07c2",
            "patch": "@@ -1285,7 +1285,7 @@ impl AggregationUpdateQueue {\n         let _span = trace_span!(\"lost follower (n uppers)\", uppers = upper_ids.len()).entered();\n \n         // see documentation of `retry_loop` for more information why this is needed\n-        let result = retry_loop(|| {\n+        let result = retry_loop(retry, || {\n             let mut follower = ctx.task(\n                 lost_follower_id,\n                 // For performance reasons this should stay `Meta` and not `All`\n@@ -1452,7 +1452,7 @@ impl AggregationUpdateQueue {\n         .entered();\n \n         // see documentation of `retry_loop` for more information why this is needed\n-        let result = retry_loop(|| {\n+        let result = retry_loop(retry, || {\n             swap_retain(&mut lost_follower_ids, |&mut lost_follower_id| {\n                 let mut follower = ctx.task(\n                     lost_follower_id,\n@@ -2476,7 +2476,7 @@ impl Operation for AggregationUpdateQueue {\n struct RetryTimeout;\n \n const MAX_YIELD_DURATION: Duration = Duration::from_millis(1);\n-const MAX_RETRIES: u16 = 10000;\n+const MAX_RETRIES: u16 = 60000;\n \n /// Retry the passed function for a few milliseconds, while yielding to other threads.\n /// Returns an error if the function was not able to complete and the timeout was reached.\n@@ -2492,13 +2492,17 @@ const MAX_RETRIES: u16 = 10000;\n /// successful, the update is added to the end of the queue again. This is important as the \"add\"\n /// update might even be in the current thread and in the same queue. If that's the case yielding\n /// won't help and the update need to be requeued.\n-fn retry_loop(mut f: impl FnMut() -> ControlFlow<()>) -> Result<(), RetryTimeout> {\n+fn retry_loop(mut retry: u16, mut f: impl FnMut() -> ControlFlow<()>) -> Result<(), RetryTimeout> {\n     let mut time: Option<Instant> = None;\n     loop {\n         match f() {\n             ControlFlow::Continue(()) => {}\n             ControlFlow::Break(()) => return Ok(()),\n         }\n+        if retry == 0 {\n+            return Err(RetryTimeout);\n+        }\n+        retry -= 1;\n         yield_now();\n         if let Some(t) = time {\n             if t.elapsed() > MAX_YIELD_DURATION {"
        }
    ],
    "stats": {
        "total": 12,
        "additions": 8,
        "deletions": 4
    }
}