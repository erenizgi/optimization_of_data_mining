{
    "author": "sokra",
    "message": "Turbopack: add parallel execution helpers (#82666)\n\n<!-- Thanks for opening a PR! Your contribution is much appreciated.\nTo make sure your PR is handled as smoothly as possible we request that you follow the checklist sections below.\nChoose the right checklist for the change(s) that you're making:\n\n## For Contributors\n\n### Improving Documentation\n\n- Run `pnpm prettier-fix` to fix formatting issues before opening the PR.\n- Read the Docs Contribution Guide to ensure your contribution follows the docs guidelines: https://nextjs.org/docs/community/contribution-guide\n\n### Adding or Updating Examples\n\n- The \"examples guidelines\" are followed from our contributing doc https://github.com/vercel/next.js/blob/canary/contributing/examples/adding-examples.md\n- Make sure the linting passes by running `pnpm build && pnpm lint`. See https://github.com/vercel/next.js/blob/canary/contributing/repository/linting.md\n\n### Fixing a bug\n\n- Related issues linked using `fixes #number`\n- Tests added. See: https://github.com/vercel/next.js/blob/canary/contributing/core/testing.md#writing-tests-for-nextjs\n- Errors have a helpful link attached, see https://github.com/vercel/next.js/blob/canary/contributing.md\n\n### Adding a feature\n\n- Implements an existing feature request or RFC. Make sure the feature request has been accepted for implementation before opening a PR. (A discussion must be opened, see https://github.com/vercel/next.js/discussions/new?category=ideas)\n- Related issues/discussions are linked using `fixes #number`\n- e2e tests added (https://github.com/vercel/next.js/blob/canary/contributing/core/testing.md#writing-tests-for-nextjs)\n- Documentation added\n- Telemetry added. In case of a feature if it's used or not.\n- Errors have a helpful link attached, see https://github.com/vercel/next.js/blob/canary/contributing.md\n\n\n## For Maintainers\n\n- Minimal description (aim for explaining to someone not on the team to understand the PR)\n- When linking to a Slack thread, you might want to share details of the conclusion\n- Link both the Linear (Fixes NEXT-xxx) and the GitHub issues\n- Add review comments if necessary to explain to the reviewer the logic behind a change\n\n### What?\n\n### Why?\n\n### How?\n\nCloses NEXT-\nFixes #\n\n-->",
    "sha": "b7da0c5425fd97cc57101a17cd06392b2d4380ad",
    "files": [
        {
            "sha": "aef3781b22f17cf839c24e883af1efa838135d4a",
            "filename": "turbopack/crates/turbo-tasks-backend/src/kv_backing_storage.rs",
            "status": "modified",
            "additions": 50,
            "deletions": 56,
            "changes": 106,
            "blob_url": "https://github.com/vercel/next.js/blob/b7da0c5425fd97cc57101a17cd06392b2d4380ad/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fkv_backing_storage.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/b7da0c5425fd97cc57101a17cd06392b2d4380ad/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fkv_backing_storage.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fsrc%2Fkv_backing_storage.rs?ref=b7da0c5425fd97cc57101a17cd06392b2d4380ad",
            "patch": "@@ -410,64 +410,11 @@ impl<T: KeyValueDatabase + Send + Sync + 'static> BackingStorageSealed\n                 )?;\n             }\n             WriteBatch::Serial(batch) => {\n-                let mut task_items_result = Ok(Vec::new());\n-                turbo_tasks::scope(|s| {\n-                    s.spawn(|_| {\n-                        task_items_result =\n-                            process_task_data(snapshots, None::<&T::ConcurrentWriteBatch<'_>>);\n-                    });\n-\n-                    let mut next_task_id =\n-                        get_next_free_task_id::<\n-                            T::SerialWriteBatch<'_>,\n-                            T::ConcurrentWriteBatch<'_>,\n-                        >(&mut WriteBatchRef::serial(batch))?;\n-\n-                    {\n-                        let _span = tracing::trace_span!(\n-                            \"update task cache\",\n-                            items = task_cache_updates.iter().map(|m| m.len()).sum::<usize>()\n-                        )\n-                        .entered();\n-                        let mut task_type_bytes = Vec::new();\n-                        for (task_type, task_id) in task_cache_updates.into_iter().flatten() {\n-                            let task_id = *task_id;\n-                            serialize_task_type(&task_type, &mut task_type_bytes, task_id)?;\n-\n-                            batch\n-                                .put(\n-                                    KeySpace::ForwardTaskCache,\n-                                    WriteBuffer::Borrowed(&task_type_bytes),\n-                                    WriteBuffer::Borrowed(&task_id.to_le_bytes()),\n-                                )\n-                                .with_context(|| {\n-                                    anyhow!(\"Unable to write task cache {task_type:?} => {task_id}\")\n-                                })?;\n-                            batch\n-                                .put(\n-                                    KeySpace::ReverseTaskCache,\n-                                    WriteBuffer::Borrowed(IntKey::new(task_id).as_ref()),\n-                                    WriteBuffer::Borrowed(&task_type_bytes),\n-                                )\n-                                .with_context(|| {\n-                                    anyhow!(\"Unable to write task cache {task_id} => {task_type:?}\")\n-                                })?;\n-                            next_task_id = next_task_id.max(task_id + 1);\n-                        }\n-                    }\n-\n-                    save_infra::<T::SerialWriteBatch<'_>, T::ConcurrentWriteBatch<'_>>(\n-                        &mut WriteBatchRef::serial(batch),\n-                        next_task_id,\n-                        session_id,\n-                        operations,\n-                    )?;\n-                    anyhow::Ok(())\n-                })?;\n-\n                 {\n                     let _span = tracing::trace_span!(\"update tasks\").entered();\n-                    for (task_id, meta, data) in task_items_result?.into_iter().flatten() {\n+                    let task_items =\n+                        process_task_data(snapshots, None::<&T::ConcurrentWriteBatch<'_>>)?;\n+                    for (task_id, meta, data) in task_items.into_iter().flatten() {\n                         let key = IntKey::new(*task_id);\n                         let key = key.as_ref();\n                         if let Some(meta) = meta {\n@@ -485,7 +432,54 @@ impl<T: KeyValueDatabase + Send + Sync + 'static> BackingStorageSealed\n                                 })?;\n                         }\n                     }\n+                    batch.flush(KeySpace::TaskMeta)?;\n+                    batch.flush(KeySpace::TaskData)?;\n+                }\n+\n+                let mut next_task_id = get_next_free_task_id::<\n+                    T::SerialWriteBatch<'_>,\n+                    T::ConcurrentWriteBatch<'_>,\n+                >(&mut WriteBatchRef::serial(batch))?;\n+\n+                {\n+                    let _span = tracing::trace_span!(\n+                        \"update task cache\",\n+                        items = task_cache_updates.iter().map(|m| m.len()).sum::<usize>()\n+                    )\n+                    .entered();\n+                    let mut task_type_bytes = Vec::new();\n+                    for (task_type, task_id) in task_cache_updates.into_iter().flatten() {\n+                        let task_id = *task_id;\n+                        serialize_task_type(&task_type, &mut task_type_bytes, task_id)?;\n+\n+                        batch\n+                            .put(\n+                                KeySpace::ForwardTaskCache,\n+                                WriteBuffer::Borrowed(&task_type_bytes),\n+                                WriteBuffer::Borrowed(&task_id.to_le_bytes()),\n+                            )\n+                            .with_context(|| {\n+                                anyhow!(\"Unable to write task cache {task_type:?} => {task_id}\")\n+                            })?;\n+                        batch\n+                            .put(\n+                                KeySpace::ReverseTaskCache,\n+                                WriteBuffer::Borrowed(IntKey::new(task_id).as_ref()),\n+                                WriteBuffer::Borrowed(&task_type_bytes),\n+                            )\n+                            .with_context(|| {\n+                                anyhow!(\"Unable to write task cache {task_id} => {task_type:?}\")\n+                            })?;\n+                        next_task_id = next_task_id.max(task_id + 1);\n+                    }\n                 }\n+\n+                save_infra::<T::SerialWriteBatch<'_>, T::ConcurrentWriteBatch<'_>>(\n+                    &mut WriteBatchRef::serial(batch),\n+                    next_task_id,\n+                    session_id,\n+                    operations,\n+                )?;\n             }\n         }\n "
        },
        {
            "sha": "c3aab4cfdad5452a92878f1d6407b6dfbf4a7e3c",
            "filename": "turbopack/crates/turbo-tasks/src/lib.rs",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/vercel/next.js/blob/b7da0c5425fd97cc57101a17cd06392b2d4380ad/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Flib.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/b7da0c5425fd97cc57101a17cd06392b2d4380ad/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Flib.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Flib.rs?ref=b7da0c5425fd97cc57101a17cd06392b2d4380ad",
            "patch": "@@ -37,6 +37,8 @@\n #![feature(never_type)]\n #![feature(downcast_unchecked)]\n #![feature(ptr_metadata)]\n+#![feature(sync_unsafe_cell)]\n+#![feature(vec_into_raw_parts)]\n \n pub mod backend;\n mod capture_future;\n@@ -64,13 +66,14 @@ mod no_move_vec;\n mod once_map;\n mod output;\n pub mod panic_hooks;\n+pub mod parallel;\n pub mod persisted_graph;\n pub mod primitives;\n mod raw_vc;\n mod read_options;\n mod read_ref;\n pub mod registry;\n-mod scope;\n+pub mod scope;\n mod serialization_invalidation;\n pub mod small_duration;\n mod spawn;\n@@ -115,7 +118,6 @@ pub use raw_vc::{CellId, RawVc, ReadRawVcFuture, ResolveTypeError};\n pub use read_options::ReadCellOptions;\n pub use read_ref::ReadRef;\n use rustc_hash::FxHasher;\n-pub use scope::scope;\n pub use serialization_invalidation::SerializationInvalidator;\n pub use shrink_to_fit::ShrinkToFit;\n pub use spawn::{JoinHandle, block_for_future, spawn, spawn_blocking, spawn_thread};"
        },
        {
            "sha": "393c5224da390a8285d3a6a638474a7023a6cc02",
            "filename": "turbopack/crates/turbo-tasks/src/manager.rs",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/vercel/next.js/blob/b7da0c5425fd97cc57101a17cd06392b2d4380ad/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Fmanager.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/b7da0c5425fd97cc57101a17cd06392b2d4380ad/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Fmanager.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Fmanager.rs?ref=b7da0c5425fd97cc57101a17cd06392b2d4380ad",
            "patch": "@@ -1678,6 +1678,10 @@ pub fn turbo_tasks() -> Arc<dyn TurboTasksApi> {\n     TURBO_TASKS.with(|arc| arc.clone())\n }\n \n+pub fn try_turbo_tasks() -> Option<Arc<dyn TurboTasksApi>> {\n+    TURBO_TASKS.try_with(|arc| arc.clone()).ok()\n+}\n+\n pub fn with_turbo_tasks<T>(func: impl FnOnce(&Arc<dyn TurboTasksApi>) -> T) -> T {\n     TURBO_TASKS.with(|arc| func(arc))\n }"
        },
        {
            "sha": "e20b67bf678924e17e81878eb7f07eec39e99e63",
            "filename": "turbopack/crates/turbo-tasks/src/parallel.rs",
            "status": "added",
            "additions": 308,
            "deletions": 0,
            "changes": 308,
            "blob_url": "https://github.com/vercel/next.js/blob/b7da0c5425fd97cc57101a17cd06392b2d4380ad/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Fparallel.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/b7da0c5425fd97cc57101a17cd06392b2d4380ad/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Fparallel.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Fparallel.rs?ref=b7da0c5425fd97cc57101a17cd06392b2d4380ad",
            "patch": "@@ -0,0 +1,308 @@\n+//! Parallel for each and map using tokio tasks.\n+//!\n+//! This avoid the problem of sleeping threads with mimalloc when using rayon in combination with\n+//! tokio. It also avoid having multiple thread pools.\n+//! see also https://pwy.io/posts/mimalloc-cigarette/\n+\n+use std::{sync::LazyLock, thread::available_parallelism};\n+\n+use crate::{scope::scope_and_block, util::into_chunks};\n+\n+/// Calculates a good chunk size for parallel processing based on the number of available threads.\n+/// This is used to ensure that the workload is evenly distributed across the threads.\n+fn good_chunk_size(len: usize) -> usize {\n+    static GOOD_CHUNK_COUNT: LazyLock<usize> =\n+        LazyLock::new(|| available_parallelism().map_or(16, |c| c.get() * 4));\n+    let min_chunk_count = *GOOD_CHUNK_COUNT;\n+    len.div_ceil(min_chunk_count)\n+}\n+\n+pub fn for_each<'l, T, F>(items: &'l [T], f: F)\n+where\n+    T: Sync,\n+    F: Fn(&'l T) + Send + Sync,\n+{\n+    let len = items.len();\n+    if len <= 1 {\n+        for item in items {\n+            f(item);\n+        }\n+        return;\n+    }\n+    let chunk_size = good_chunk_size(len);\n+    let f = &f;\n+    let _results = scope_and_block(len.div_ceil(chunk_size), |scope| {\n+        for chunk in items.chunks(chunk_size) {\n+            scope.spawn(async move {\n+                for item in chunk {\n+                    f(item);\n+                }\n+            })\n+        }\n+    });\n+}\n+\n+pub fn for_each_owned<T>(items: Vec<T>, f: impl Fn(T) + Send + Sync)\n+where\n+    T: Send + Sync,\n+{\n+    let len = items.len();\n+    if len <= 1 {\n+        for item in items {\n+            f(item);\n+        }\n+        return;\n+    }\n+    let chunk_size = good_chunk_size(len);\n+    let f = &f;\n+    let _results = scope_and_block(len.div_ceil(chunk_size), |scope| {\n+        for chunk in into_chunks(items, chunk_size) {\n+            scope.spawn(async move {\n+                // SAFETY: Even when f() panics we drop all items in the chunk.\n+                for item in chunk {\n+                    f(item);\n+                }\n+            })\n+        }\n+    });\n+}\n+\n+pub fn try_for_each<'l, T, E>(\n+    items: &'l [T],\n+    f: impl (Fn(&'l T) -> Result<(), E>) + Send + Sync,\n+) -> Result<(), E>\n+where\n+    T: Sync,\n+    E: Send + 'static,\n+{\n+    let len = items.len();\n+    if len <= 1 {\n+        for item in items {\n+            f(item)?;\n+        }\n+        return Ok(());\n+    }\n+    let chunk_size = good_chunk_size(len);\n+    let f = &f;\n+    scope_and_block(len.div_ceil(chunk_size), |scope| {\n+        for chunk in items.chunks(chunk_size) {\n+            scope.spawn(async move {\n+                for item in chunk {\n+                    f(item)?;\n+                }\n+                Ok(())\n+            })\n+        }\n+    })\n+    .collect::<Result<(), E>>()\n+}\n+\n+pub fn try_for_each_mut<'l, T, E>(\n+    items: &'l mut [T],\n+    f: impl (Fn(&'l mut T) -> Result<(), E>) + Send + Sync,\n+) -> Result<(), E>\n+where\n+    T: Send + Sync,\n+    E: Send + 'static,\n+{\n+    let len = items.len();\n+    if len <= 1 {\n+        for item in items {\n+            f(item)?;\n+        }\n+        return Ok(());\n+    }\n+    let chunk_size = good_chunk_size(len);\n+    let f = &f;\n+    scope_and_block(len.div_ceil(chunk_size), |scope| {\n+        for chunk in items.chunks_mut(chunk_size) {\n+            scope.spawn(async move {\n+                for item in chunk {\n+                    f(item)?;\n+                }\n+                Ok(())\n+            })\n+        }\n+    })\n+    .collect::<Result<(), E>>()\n+}\n+\n+pub fn try_for_each_owned<T, E>(\n+    items: Vec<T>,\n+    f: impl (Fn(T) -> Result<(), E>) + Send + Sync,\n+) -> Result<(), E>\n+where\n+    T: Send + Sync,\n+    E: Send + 'static,\n+{\n+    let len = items.len();\n+    if len <= 1 {\n+        for item in items {\n+            f(item)?;\n+        }\n+        return Ok(());\n+    }\n+    let chunk_size = good_chunk_size(len);\n+    let f = &f;\n+    scope_and_block(len.div_ceil(chunk_size), |scope| {\n+        for chunk in into_chunks(items, chunk_size) {\n+            scope.spawn(async move {\n+                for item in chunk {\n+                    f(item)?;\n+                }\n+                Ok(())\n+            })\n+        }\n+    })\n+    .collect::<Result<(), E>>()\n+}\n+\n+pub fn map_collect<'l, Item, PerItemResult, Result>(\n+    items: &'l [Item],\n+    f: impl Fn(&'l Item) -> PerItemResult + Send + Sync,\n+) -> Result\n+where\n+    Item: Sync,\n+    PerItemResult: Send + Sync + 'l,\n+    Result: FromIterator<PerItemResult>,\n+{\n+    let len = items.len();\n+    if len == 0 {\n+        return Result::from_iter(std::iter::empty()); // No items to process, return empty\n+        // collection\n+    }\n+    let chunk_size = good_chunk_size(len);\n+    let f = &f;\n+    scope_and_block(len.div_ceil(chunk_size), |scope| {\n+        for chunk in items.chunks(chunk_size) {\n+            scope.spawn(async move { chunk.iter().map(f).collect::<Vec<_>>() })\n+        }\n+    })\n+    .flatten()\n+    .collect()\n+}\n+\n+pub fn map_collect_owned<'l, Item, PerItemResult, Result>(\n+    items: Vec<Item>,\n+    f: impl Fn(Item) -> PerItemResult + Send + Sync,\n+) -> Result\n+where\n+    Item: Send + Sync,\n+    PerItemResult: Send + Sync + 'l,\n+    Result: FromIterator<PerItemResult>,\n+{\n+    let len = items.len();\n+    if len == 0 {\n+        return Result::from_iter(std::iter::empty()); // No items to process, return empty\n+        // collection;\n+    }\n+    let chunk_size = good_chunk_size(len);\n+    let f = &f;\n+    scope_and_block(len.div_ceil(chunk_size), |scope| {\n+        for chunk in into_chunks(items, chunk_size) {\n+            scope.spawn(async move { chunk.map(f).collect::<Vec<_>>() })\n+        }\n+    })\n+    .flatten()\n+    .collect()\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use std::{\n+        panic::{AssertUnwindSafe, catch_unwind},\n+        sync::atomic::{AtomicI32, Ordering},\n+    };\n+\n+    use super::*;\n+\n+    #[tokio::test(flavor = \"multi_thread\")]\n+    async fn test_parallel_for_each() {\n+        let input = vec![1, 2, 3, 4, 5];\n+        let sum = AtomicI32::new(0);\n+        for_each(&input, |&x| {\n+            sum.fetch_add(x, Ordering::SeqCst);\n+        });\n+        assert_eq!(sum.load(Ordering::SeqCst), 15);\n+    }\n+\n+    #[tokio::test(flavor = \"multi_thread\")]\n+    async fn test_parallel_try_for_each() {\n+        let input = vec![1, 2, 3, 4, 5];\n+        let result = try_for_each(&input, |&x| {\n+            if x % 2 == 0 {\n+                Ok(())\n+            } else {\n+                Err(format!(\"Odd number {x} encountered\"))\n+            }\n+        });\n+        assert!(result.is_err());\n+        assert_eq!(result.unwrap_err(), \"Odd number 1 encountered\");\n+    }\n+\n+    #[tokio::test(flavor = \"multi_thread\")]\n+    async fn test_parallel_try_for_each_mut() {\n+        let mut input = vec![1, 2, 3, 4, 5];\n+        let result = try_for_each_mut(&mut input, |x| {\n+            *x += 10;\n+            if *x % 2 == 0 {\n+                Ok(())\n+            } else {\n+                Err(format!(\"Odd number {} encountered\", *x))\n+            }\n+        });\n+        assert!(result.is_err());\n+        assert_eq!(result.unwrap_err(), \"Odd number 11 encountered\");\n+        assert_eq!(input, vec![11, 12, 13, 14, 15]);\n+    }\n+\n+    #[tokio::test(flavor = \"multi_thread\")]\n+    async fn test_parallel_for_each_owned() {\n+        let input = vec![1, 2, 3, 4, 5];\n+        let sum = AtomicI32::new(0);\n+        for_each_owned(input, |x| {\n+            sum.fetch_add(x, Ordering::SeqCst);\n+        });\n+        assert_eq!(sum.load(Ordering::SeqCst), 15);\n+    }\n+\n+    #[tokio::test(flavor = \"multi_thread\")]\n+    async fn test_parallel_map_collect() {\n+        let input = vec![1, 2, 3, 4, 5];\n+        let result: Vec<_> = map_collect(&input, |&x| x * 2);\n+        assert_eq!(result, vec![2, 4, 6, 8, 10]);\n+    }\n+\n+    #[tokio::test(flavor = \"multi_thread\")]\n+    async fn test_parallel_map_collect_owned() {\n+        let input = vec![1, 2, 3, 4, 5];\n+        let result: Vec<_> = map_collect_owned(input, |x| x * 2);\n+        assert_eq!(result, vec![2, 4, 6, 8, 10]);\n+    }\n+\n+    #[tokio::test(flavor = \"multi_thread\")]\n+    async fn test_parallel_map_collect_owned_many() {\n+        let input = vec![1; 1000];\n+        let result: Vec<_> = map_collect_owned(input, |x| x * 2);\n+        assert_eq!(result, vec![2; 1000]);\n+    }\n+\n+    #[tokio::test(flavor = \"multi_thread\")]\n+    async fn test_panic_in_scope() {\n+        let result = catch_unwind(AssertUnwindSafe(|| {\n+            let mut input = vec![1; 1000];\n+            input[744] = 2;\n+            for_each(&input, |x| {\n+                if *x == 2 {\n+                    panic!(\"Intentional panic\");\n+                }\n+            });\n+            panic!(\"Should not get here\")\n+        }));\n+        assert!(result.is_err());\n+        assert_eq!(\n+            result.unwrap_err().downcast_ref::<&str>(),\n+            Some(&\"Intentional panic\")\n+        );\n+    }\n+}"
        },
        {
            "sha": "4c474b35f22a352a37155e9de9e9779123d6c7ed",
            "filename": "turbopack/crates/turbo-tasks/src/scope.rs",
            "status": "modified",
            "additions": 273,
            "deletions": 36,
            "changes": 309,
            "blob_url": "https://github.com/vercel/next.js/blob/b7da0c5425fd97cc57101a17cd06392b2d4380ad/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Fscope.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/b7da0c5425fd97cc57101a17cd06392b2d4380ad/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Fscope.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Fscope.rs?ref=b7da0c5425fd97cc57101a17cd06392b2d4380ad",
            "patch": "@@ -1,52 +1,289 @@\n-use std::sync::Arc;\n+//! A scoped tokio spawn implementation that allow a non-'static lifetime for tasks.\n \n-use crate::{TurboTasksApi, turbo_tasks, turbo_tasks_scope};\n+use std::{\n+    any::Any,\n+    marker::PhantomData,\n+    panic::{self, AssertUnwindSafe, catch_unwind},\n+    pin::Pin,\n+    sync::{\n+        Arc,\n+        atomic::{AtomicUsize, Ordering},\n+    },\n+    thread::{self, Thread},\n+};\n \n-/// A wrapper around [`rayon::Scope`] that preserves the [`turbo_tasks_scope`].\n-pub struct Scope<'scope, 'a> {\n-    scope: &'a rayon::Scope<'scope>,\n-    handle: tokio::runtime::Handle,\n-    turbo_tasks: Arc<dyn TurboTasksApi>,\n-    span: tracing::Span,\n+use futures::FutureExt;\n+use parking_lot::Mutex;\n+use tokio::{runtime::Handle, task::block_in_place};\n+use tracing::{Instrument, Span, info_span};\n+\n+use crate::{\n+    TurboTasksApi,\n+    manager::{try_turbo_tasks, turbo_tasks_future_scope},\n+};\n+\n+struct ScopeInner {\n+    main_thread: Thread,\n+    remaining_tasks: AtomicUsize,\n+    /// The first panic that occurred in the tasks, by task index.\n+    /// The usize value is the index of the task.\n+    panic: Mutex<Option<(Box<dyn Any + Send + 'static>, usize)>>,\n }\n \n-impl<'scope> Scope<'scope, '_> {\n-    pub fn spawn<Body>(&self, body: Body)\n+impl ScopeInner {\n+    fn on_task_finished(&self, panic: Option<(Box<dyn Any + Send + 'static>, usize)>) {\n+        if let Some((err, index)) = panic {\n+            let mut old_panic = self.panic.lock();\n+            if old_panic.as_ref().is_none_or(|&(_, i)| i > index) {\n+                *old_panic = Some((err, index));\n+            }\n+        }\n+        if self.remaining_tasks.fetch_sub(1, Ordering::Release) == 1 {\n+            self.main_thread.unpark();\n+        }\n+    }\n+\n+    fn wait(&self) {\n+        let _span = info_span!(\"blocking\").entered();\n+        while self.remaining_tasks.load(Ordering::Acquire) != 0 {\n+            thread::park();\n+        }\n+        if let Some((err, _)) = self.panic.lock().take() {\n+            panic::resume_unwind(err);\n+        }\n+    }\n+}\n+\n+/// Scope to allow spawning tasks with a limited lifetime.\n+///\n+/// Dropping this Scope will wait for all tasks to complete.\n+pub struct Scope<'scope, 'env: 'scope, R: Send + 'env> {\n+    results: &'scope [Mutex<Option<R>>],\n+    index: AtomicUsize,\n+    inner: Arc<ScopeInner>,\n+    handle: Handle,\n+    turbo_tasks: Option<Arc<dyn TurboTasksApi>>,\n+    span: Span,\n+    /// Invariance over 'env, to make sure 'env cannot shrink,\n+    /// which is necessary for soundness.\n+    ///\n+    /// see https://doc.rust-lang.org/src/std/thread/scoped.rs.html#12-29\n+    env: PhantomData<&'env mut &'env ()>,\n+}\n+\n+impl<'scope, 'env: 'scope, R: Send + 'env> Scope<'scope, 'env, R> {\n+    /// Creates a new scope.\n+    ///\n+    /// # Safety\n+    ///\n+    /// The caller must ensure `Scope` is dropped and not forgotten.\n+    unsafe fn new(results: &'scope [Mutex<Option<R>>]) -> Self {\n+        Self {\n+            results,\n+            index: AtomicUsize::new(0),\n+            inner: Arc::new(ScopeInner {\n+                main_thread: thread::current(),\n+                remaining_tasks: AtomicUsize::new(0),\n+                panic: Mutex::new(None),\n+            }),\n+            handle: Handle::current(),\n+            turbo_tasks: try_turbo_tasks(),\n+            span: Span::current(),\n+            env: PhantomData,\n+        }\n+    }\n+\n+    /// Spawns a new task in the scope.\n+    pub fn spawn<F>(&self, f: F)\n     where\n-        Body: FnOnce(&Scope<'scope, '_>) + Send + 'scope,\n+        F: Future<Output = R> + Send + 'env,\n     {\n-        let span = self.span.clone();\n-        let handle = self.handle.clone();\n+        let index = self.index.fetch_add(1, Ordering::Relaxed);\n+        assert!(index < self.results.len(), \"Too many tasks spawned\");\n+        let result_cell: &Mutex<Option<R>> = &self.results[index];\n+\n+        let f: Box<dyn Future<Output = ()> + Send + 'scope> = Box::new(async move {\n+            let result = f.await;\n+            *result_cell.lock() = Some(result);\n+        });\n+        let f: *mut (dyn Future<Output = ()> + Send + 'scope) = Box::into_raw(f);\n+        // SAFETY: Scope ensures (e. g. in Drop) that spawned tasks is awaited before the\n+        // lifetime `'env` ends.\n+        #[allow(\n+            clippy::unnecessary_cast,\n+            reason = \"Clippy thinks this is unnecessary, but it actually changes the lifetime\"\n+        )]\n+        let f = f as *mut (dyn Future<Output = ()> + Send + 'static);\n+        // SAFETY: We just called `Box::into_raw`.\n+        let f = unsafe { Box::from_raw(f) };\n+        // We pin the future in the box in memory to be able to await it.\n+        let f = Pin::from(f);\n+\n         let turbo_tasks = self.turbo_tasks.clone();\n-        self.scope.spawn(|scope| {\n-            let _span = span.clone().entered();\n-            let _guard = handle.enter();\n-            turbo_tasks_scope(turbo_tasks.clone(), || {\n-                body(&Scope {\n-                    scope,\n-                    span,\n-                    handle,\n-                    turbo_tasks,\n-                })\n-            })\n+        let span = self.span.clone();\n+\n+        let inner = self.inner.clone();\n+        inner.remaining_tasks.fetch_add(1, Ordering::Relaxed);\n+        self.handle.spawn(async move {\n+            let result = AssertUnwindSafe(\n+                async move {\n+                    if let Some(turbo_tasks) = turbo_tasks {\n+                        // Ensure that the turbo tasks context is maintained across the task.\n+                        turbo_tasks_future_scope(turbo_tasks, f).await;\n+                    } else {\n+                        // If no turbo tasks context is available, just run the future.\n+                        f.await;\n+                    }\n+                }\n+                .instrument(span),\n+            )\n+            .catch_unwind()\n+            .await;\n+            let panic = result.err().map(|e| (e, index));\n+            inner.on_task_finished(panic);\n         });\n     }\n }\n \n-/// A wrapper around [`rayon::in_place_scope`] that preserves the [`turbo_tasks_scope`].\n-pub fn scope<'scope, Op, R>(op: Op) -> R\n+impl<'scope, 'env: 'scope, R: Send + 'env> Drop for Scope<'scope, 'env, R> {\n+    fn drop(&mut self) {\n+        self.inner.wait();\n+    }\n+}\n+\n+/// Helper method to spawn tasks in parallel, ensuring that all tasks are awaited and errors are\n+/// handled. Also ensures turbo tasks and tracing context are maintained across the tasks.\n+///\n+/// Be aware that although this function avoids starving other independently spawned tasks, any\n+/// other code running concurrently in the same task will be suspended during the call to\n+/// block_in_place. This can happen e.g. when using the `join!` macro. To avoid this issue, call\n+/// `scope_and_block` in `spawn_blocking`.\n+pub fn scope_and_block<'env, F, R>(number_of_tasks: usize, f: F) -> impl Iterator<Item = R>\n where\n-    Op: FnOnce(&Scope<'scope, '_>) -> R,\n+    R: Send + 'env,\n+    F: for<'scope> FnOnce(&'scope Scope<'scope, 'env, R>) + 'env,\n {\n-    let span = tracing::Span::current();\n-    let handle = tokio::runtime::Handle::current();\n-    let turbo_tasks = turbo_tasks();\n-    rayon::in_place_scope(|scope| {\n-        op(&Scope {\n-            scope,\n-            span,\n-            handle,\n-            turbo_tasks,\n+    block_in_place(|| {\n+        let mut results = Vec::with_capacity(number_of_tasks);\n+        for _ in 0..number_of_tasks {\n+            results.push(Mutex::new(None));\n+        }\n+        let results = results.into_boxed_slice();\n+        let result = {\n+            // SAFETY: We drop the Scope later.\n+            let scope = unsafe { Scope::new(&results) };\n+            catch_unwind(AssertUnwindSafe(|| f(&scope)))\n+        };\n+        if let Err(panic) = result {\n+            panic::resume_unwind(panic);\n+        }\n+        results.into_iter().map(|mutex| {\n+            mutex\n+                .into_inner()\n+                .expect(\"All values are set when the scope returns without panic\")\n         })\n     })\n }\n+\n+#[cfg(test)]\n+mod tests {\n+    use std::panic::{AssertUnwindSafe, catch_unwind};\n+\n+    use super::*;\n+\n+    #[tokio::test(flavor = \"multi_thread\")]\n+    async fn test_scope() {\n+        let results = scope_and_block(1000, |scope| {\n+            for i in 0..1000 {\n+                scope.spawn(async move { i });\n+            }\n+        });\n+        results.enumerate().for_each(|(i, result)| {\n+            assert_eq!(result, i);\n+        });\n+    }\n+\n+    #[tokio::test(flavor = \"multi_thread\")]\n+    async fn test_empty_scope() {\n+        let results = scope_and_block(0, |scope| {\n+            if false {\n+                scope.spawn(async move { 42 });\n+            }\n+        });\n+        assert_eq!(results.count(), 0);\n+    }\n+\n+    #[tokio::test(flavor = \"multi_thread\")]\n+    async fn test_single_task() {\n+        let results = scope_and_block(1, |scope| {\n+            scope.spawn(async move { 42 });\n+        })\n+        .collect::<Vec<_>>();\n+        assert_eq!(results, vec![42]);\n+    }\n+\n+    #[tokio::test(flavor = \"multi_thread\")]\n+    async fn test_task_finish_before_scope() {\n+        let results = scope_and_block(1, |scope| {\n+            scope.spawn(async move { 42 });\n+            thread::sleep(std::time::Duration::from_millis(100));\n+        })\n+        .collect::<Vec<_>>();\n+        assert_eq!(results, vec![42]);\n+    }\n+\n+    #[tokio::test(flavor = \"multi_thread\")]\n+    async fn test_task_finish_after_scope() {\n+        let results = scope_and_block(1, |scope| {\n+            scope.spawn(async move {\n+                thread::sleep(std::time::Duration::from_millis(100));\n+                42\n+            });\n+        })\n+        .collect::<Vec<_>>();\n+        assert_eq!(results, vec![42]);\n+    }\n+\n+    #[tokio::test(flavor = \"multi_thread\")]\n+    async fn test_panic_in_scope_factory() {\n+        let result = catch_unwind(AssertUnwindSafe(|| {\n+            let _results = scope_and_block(1000, |scope| {\n+                for i in 0..500 {\n+                    scope.spawn(async move { i });\n+                }\n+                panic!(\"Intentional panic\");\n+            });\n+            unreachable!();\n+        }));\n+        assert!(result.is_err());\n+        assert_eq!(\n+            result.unwrap_err().downcast_ref::<&str>(),\n+            Some(&\"Intentional panic\")\n+        );\n+    }\n+\n+    #[tokio::test(flavor = \"multi_thread\")]\n+    async fn test_panic_in_scope_task() {\n+        let result = catch_unwind(AssertUnwindSafe(|| {\n+            let _results = scope_and_block(1000, |scope| {\n+                for i in 0..1000 {\n+                    scope.spawn(async move {\n+                        if i == 500 {\n+                            panic!(\"Intentional panic\");\n+                        } else if i == 501 {\n+                            panic!(\"Wrong intentional panic\");\n+                        } else {\n+                            i\n+                        }\n+                    });\n+                }\n+            });\n+            unreachable!();\n+        }));\n+        assert!(result.is_err());\n+        assert_eq!(\n+            result.unwrap_err().downcast_ref::<&str>(),\n+            Some(&\"Intentional panic\")\n+        );\n+    }\n+}"
        },
        {
            "sha": "ecc57de367173f26cf0f65ebfa5d7a0f2bb902ce",
            "filename": "turbopack/crates/turbo-tasks/src/util.rs",
            "status": "modified",
            "additions": 123,
            "deletions": 0,
            "changes": 123,
            "blob_url": "https://github.com/vercel/next.js/blob/b7da0c5425fd97cc57101a17cd06392b2d4380ad/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Futil.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/b7da0c5425fd97cc57101a17cd06392b2d4380ad/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Futil.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks%2Fsrc%2Futil.rs?ref=b7da0c5425fd97cc57101a17cd06392b2d4380ad",
            "patch": "@@ -1,8 +1,10 @@\n use std::{\n+    cell::SyncUnsafeCell,\n     error::Error as StdError,\n     fmt::{Debug, Display},\n     future::Future,\n     hash::{Hash, Hasher},\n+    mem::ManuallyDrop,\n     ops::Deref,\n     pin::Pin,\n     sync::Arc,\n@@ -259,3 +261,124 @@ impl<F: Future, W: for<'a> Fn(Pin<&mut F>, &mut Context<'a>) -> Poll<F::Output>>\n         (this.wrapper)(this.future, cx)\n     }\n }\n+\n+/// Similar to slice::chunks but for owned data. Chunks are Send and Sync to allow to use it for\n+/// parallelism.\n+pub fn into_chunks<T>(data: Vec<T>, chunk_size: usize) -> IntoChunks<T> {\n+    let (ptr, length, capacity) = data.into_raw_parts();\n+    // SAFETY: changing a pointer from T to SyncUnsafeCell<ManuallyDrop<..>> is safe as both types\n+    // have repr(transparent).\n+    let ptr = ptr as *mut SyncUnsafeCell<ManuallyDrop<T>>;\n+    // SAFETY: The ptr, length and capacity were from into_raw_parts(). This is the only place where\n+    // we use ptr.\n+    let data =\n+        unsafe { Vec::<SyncUnsafeCell<ManuallyDrop<T>>>::from_raw_parts(ptr, length, capacity) };\n+    IntoChunks {\n+        data: Arc::new(data),\n+        index: 0,\n+        chunk_size,\n+    }\n+}\n+\n+pub struct IntoChunks<T> {\n+    data: Arc<Vec<SyncUnsafeCell<ManuallyDrop<T>>>>,\n+    index: usize,\n+    chunk_size: usize,\n+}\n+\n+impl<T> Iterator for IntoChunks<T> {\n+    type Item = Chunk<T>;\n+\n+    fn next(&mut self) -> Option<Self::Item> {\n+        if self.index < self.data.len() {\n+            let end = self.data.len().min(self.index + self.chunk_size);\n+            let item = Chunk {\n+                data: Arc::clone(&self.data),\n+                index: self.index,\n+                end,\n+            };\n+            self.index = end;\n+            Some(item)\n+        } else {\n+            None\n+        }\n+    }\n+}\n+\n+impl<T> IntoChunks<T> {\n+    fn next_item(&mut self) -> Option<T> {\n+        if self.index < self.data.len() {\n+            // SAFETY: We are the only owner of this chunk of data and we make sure that this item\n+            // is no longer dropped by moving the index\n+            let item = unsafe { ManuallyDrop::take(&mut *self.data[self.index].get()) };\n+            self.index += 1;\n+            Some(item)\n+        } else {\n+            None\n+        }\n+    }\n+}\n+\n+impl<T> Drop for IntoChunks<T> {\n+    fn drop(&mut self) {\n+        // To avoid leaking memory we need to drop the remaining items\n+        while self.next_item().is_some() {}\n+    }\n+}\n+\n+pub struct Chunk<T> {\n+    data: Arc<Vec<SyncUnsafeCell<ManuallyDrop<T>>>>,\n+    index: usize,\n+    end: usize,\n+}\n+\n+impl<T> Iterator for Chunk<T> {\n+    type Item = T;\n+\n+    fn next(&mut self) -> Option<Self::Item> {\n+        if self.index < self.end {\n+            // SAFETY: We are the only owner of this chunk of data and we make sure that this item\n+            // is no longer dropped by moving the index\n+            let item = unsafe { ManuallyDrop::take(&mut *self.data[self.index].get()) };\n+            self.index += 1;\n+            Some(item)\n+        } else {\n+            None\n+        }\n+    }\n+}\n+\n+impl<T> Drop for Chunk<T> {\n+    fn drop(&mut self) {\n+        // To avoid leaking memory we need to drop the remaining items\n+        while self.next().is_some() {}\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use super::*;\n+\n+    #[test]\n+    fn test_chunk_iterator() {\n+        let data = [(); 10]\n+            .into_iter()\n+            .enumerate()\n+            .map(|(i, _)| Arc::new(i))\n+            .collect::<Vec<_>>();\n+        let mut chunks = into_chunks(data.clone(), 3);\n+        let mut first_chunk = chunks.next().unwrap();\n+        let second_chunk = chunks.next().unwrap();\n+        drop(chunks);\n+        assert_eq!(\n+            second_chunk.into_iter().map(|a| *a).collect::<Vec<_>>(),\n+            vec![3, 4, 5]\n+        );\n+        assert_eq!(*first_chunk.next().unwrap(), 0);\n+        assert_eq!(*first_chunk.next().unwrap(), 1);\n+        drop(first_chunk);\n+        for arc in data {\n+            assert_eq!(Arc::strong_count(&arc), 1);\n+        }\n+    }\n+}"
        }
    ],
    "stats": {
        "total": 856,
        "additions": 762,
        "deletions": 94
    }
}