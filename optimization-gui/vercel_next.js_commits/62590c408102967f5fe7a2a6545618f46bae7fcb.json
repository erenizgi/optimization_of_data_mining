{
    "author": "ijjk",
    "message": "Update E2E tests workflow (#85485)\n\nAdds an option to allow continuing tests on error to find all failures\nin single run also marks jobs as failed if any test failed while\nrunning.",
    "sha": "62590c408102967f5fe7a2a6545618f46bae7fcb",
    "files": [
        {
            "sha": "596b8e2c41e748a04983e746020221dba3139aeb",
            "filename": ".github/workflows/integration_tests_reusable.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/vercel/next.js/blob/62590c408102967f5fe7a2a6545618f46bae7fcb/.github%2Fworkflows%2Fintegration_tests_reusable.yml",
            "raw_url": "https://github.com/vercel/next.js/raw/62590c408102967f5fe7a2a6545618f46bae7fcb/.github%2Fworkflows%2Fintegration_tests_reusable.yml",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/.github%2Fworkflows%2Fintegration_tests_reusable.yml?ref=62590c408102967f5fe7a2a6545618f46bae7fcb",
            "patch": "@@ -91,7 +91,7 @@ jobs:\n       afterBuild: |\n         # e2e and ${{ inputs.test_type }} tests with `node run-tests.js`\n \n-        export NEXT_TEST_CONTINUE_ON_ERROR=TRUE\n+        export NEXT_TEST_CONTINUE_ON_ERROR=true\n         export NEXT_TEST_MODE=${{\n           inputs.test_type == 'development' && 'dev' || 'start'\n         }}\n@@ -123,7 +123,7 @@ jobs:\n       afterBuild: |\n         # legacy integration tests with `node run-tests.js`\n \n-        export NEXT_TEST_CONTINUE_ON_ERROR=TRUE\n+        export NEXT_TEST_CONTINUE_ON_ERROR=true\n \n         # HACK: Despite the name, these environment variables are just used to\n         # gate tests, so they're applicable to both turbopack and rspack tests"
        },
        {
            "sha": "3ad1f12e29ca5dbc5c315ae4f747686700a150b9",
            "filename": ".github/workflows/test_e2e_deploy_release.yml",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/vercel/next.js/blob/62590c408102967f5fe7a2a6545618f46bae7fcb/.github%2Fworkflows%2Ftest_e2e_deploy_release.yml",
            "raw_url": "https://github.com/vercel/next.js/raw/62590c408102967f5fe7a2a6545618f46bae7fcb/.github%2Fworkflows%2Ftest_e2e_deploy_release.yml",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/.github%2Fworkflows%2Ftest_e2e_deploy_release.yml?ref=62590c408102967f5fe7a2a6545618f46bae7fcb",
            "patch": "@@ -19,6 +19,10 @@ on:\n         description: Override the proxy address to use for the test\n         default: ''\n         type: string\n+      continueOnError:\n+        description: whether the tests should continue on failure\n+        default: false\n+        type: boolean\n \n env:\n   TURBO_TEAM: 'vercel'\n@@ -81,6 +85,7 @@ jobs:\n         NEXT_E2E_TEST_TIMEOUT=240000 \\\n         NEXT_TEST_MODE=deploy \\\n         IS_WEBPACK_TEST=1 \\\n+        NEXT_TEST_CONTINUE_ON_ERROR=\"${{ github.event.inputs.continueOnError || false }}\" \\\n         NEXT_EXTERNAL_TESTS_FILTERS=\"test/deploy-tests-manifest.json\" \\\n         NEXT_TEST_VERSION=\"${{ github.event.inputs.nextVersion || needs.setup.outputs.next-version || 'canary' }}\" \\\n         VERCEL_CLI_VERSION=\"${{ github.event.inputs.vercelCliVersion || 'vercel@latest' }}\" \\"
        },
        {
            "sha": "4f893b93feecf260d9e2c210037b229b7688e86d",
            "filename": "run-tests.js",
            "status": "modified",
            "additions": 16,
            "deletions": 2,
            "changes": 18,
            "blob_url": "https://github.com/vercel/next.js/blob/62590c408102967f5fe7a2a6545618f46bae7fcb/run-tests.js",
            "raw_url": "https://github.com/vercel/next.js/raw/62590c408102967f5fe7a2a6545618f46bae7fcb/run-tests.js",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/run-tests.js?ref=62590c408102967f5fe7a2a6545618f46bae7fcb",
            "patch": "@@ -55,7 +55,9 @@ const DEFAULT_CONCURRENCY = 2\n const RESULTS_EXT = `.results.json`\n const isTestJob = !!process.env.NEXT_TEST_JOB\n // Check env to see if test should continue even if some of test fails\n-const shouldContinueTestsOnError = !!process.env.NEXT_TEST_CONTINUE_ON_ERROR\n+const shouldContinueTestsOnError =\n+  process.env.NEXT_TEST_CONTINUE_ON_ERROR === 'true'\n+\n // Check env to load a list of test paths to skip retry. This is to be used in conjunction with NEXT_TEST_CONTINUE_ON_ERROR,\n // When try to run all of the tests regardless of pass / fail and want to skip retrying `known` failed tests.\n // manifest should be a json file with an array of test paths.\n@@ -500,6 +502,7 @@ ${ENDGROUP}`)\n   )\n   let firstError = true\n   let killed = false\n+  let hadFailures = false\n \n   const runTest = (/** @type {TestFile} */ test, isFinalRun, isRetry) =>\n     new Promise((resolve, reject) => {\n@@ -756,6 +759,7 @@ ${ENDGROUP}`)\n           children.forEach((child) => child.kill())\n           cleanUpAndExit(1)\n         } else {\n+          hadFailures = true\n           console.log(\n             `CONTINUE_ON_ERROR enabled, continuing tests after ${test.file} failed`\n           )\n@@ -852,10 +856,20 @@ ${ENDGROUP}`)\n       }\n     }\n   }\n+\n+  // Return whether there were any failures\n+  return hadFailures\n }\n \n main()\n-  .then(() => cleanUpAndExit(0))\n+  .then((hadFailures) => {\n+    if (hadFailures) {\n+      console.error('Some tests failed')\n+      cleanUpAndExit(1)\n+    } else {\n+      cleanUpAndExit(0)\n+    }\n+  })\n   .catch((err) => {\n     console.error(err)\n     cleanUpAndExit(1)"
        }
    ],
    "stats": {
        "total": 27,
        "additions": 23,
        "deletions": 4
    }
}