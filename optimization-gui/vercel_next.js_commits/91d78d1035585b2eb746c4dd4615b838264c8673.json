{
    "author": "lukesandberg",
    "message": "[turbopack] Create a simple benchmark to measure the overhead of turbotasks (#82982)\n\nThis benchmark works by\n\n* creating a simple busy wait function to simulate cpu bound work\n  - This is mostly to prove that we are measuring overheads and not some feature of small tasks.  The overheads for a 1us task are ~identical to the overheads for a 10us task\n* running that function directly, as an uncached turbotask as a cached turbo task hitting one key and as a cached turbotask hitting many different cached keys\n\nThis allows us to independently estimate the error of the busy loop, the overhead of launching and executing a task and the overhead of getting a cache hit.\n\nI should note, these are _idealized_ conditions and we should expect overheads to be higher during an actual build.  This is because there is no actual contention for CPU resources (no task queueing), the internal hashmapsare small so there is less time waiting to fill caches from RAM, and there are no parallel allocations/deallocations occurring (contenting in mimalloc data structures).\n\nOn my machine i see:\n\nCache-Hit (all same key): 130ns-140ns \nCache-Hit (all different keys): 240ns-500ns\nTurbo-Task-Execution Overhead: 4-8us\n\nThe fact that the cost of a cache hit is so different depending on if we are hitting the same key or different keys is interesting.  I would assume that this this is the cost of missing cpu caches (~100ns to fill a cache line from ram), this implies we are paying for the cost of an extra 2-3 dependent ram reads (which in retrospect sounds like a DashMap to me!)\n\nThis implies a 'break even' equation for caching\n\nGiven a task that takes `Tns` of time  caching becomes worth it after this many executions `ceil((TOverhead-TCache + TTask) / (TTask - TCache))`  plugging in our values we get the following results.\n\nIn the optimistic case for caching\n * T <= 130ns:  never worth it\n * T = 1000ns (1us):  7 executions (1 actual, 7 cache hits)\n * T  >= 10μs:  2 executions (aka need at least one cache hit)\n\nIn the pessimistic case for caching\n * T <= 500ns:  never worth it\n * T = 1000ns (1us):  11 executions (1 actual, 10 cache hits)\n * T >= 10μs:  2 executions \n \nSo really we should be suspicious of all tasks that never get a hit and all tasks faster than 10us need many hits to work.",
    "sha": "91d78d1035585b2eb746c4dd4615b838264c8673",
    "files": [
        {
            "sha": "b61d054dd6cea859f120b7d46c6bf9c6f75e3916",
            "filename": "turbopack/crates/turbo-tasks-backend/benches/mod.rs",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/vercel/next.js/blob/91d78d1035585b2eb746c4dd4615b838264c8673/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fbenches%2Fmod.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/91d78d1035585b2eb746c4dd4615b838264c8673/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fbenches%2Fmod.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fbenches%2Fmod.rs?ref=91d78d1035585b2eb746c4dd4615b838264c8673",
            "patch": "@@ -3,13 +3,14 @@\n \n use criterion::{Criterion, criterion_group, criterion_main};\n \n+pub(crate) mod overhead;\n pub(crate) mod scope_stress;\n pub(crate) mod stress;\n \n criterion_group!(\n     name = turbo_tasks_backend_stress;\n     config = Criterion::default();\n-    targets = stress::fibonacci, scope_stress::scope_stress\n+    targets = stress::fibonacci, scope_stress::scope_stress, overhead::overhead\n );\n criterion_main!(turbo_tasks_backend_stress);\n "
        },
        {
            "sha": "90ecc5316323c4440fa5db971bc3945f1c70e3c0",
            "filename": "turbopack/crates/turbo-tasks-backend/benches/overhead.rs",
            "status": "added",
            "additions": 141,
            "deletions": 0,
            "changes": 141,
            "blob_url": "https://github.com/vercel/next.js/blob/91d78d1035585b2eb746c4dd4615b838264c8673/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fbenches%2Foverhead.rs",
            "raw_url": "https://github.com/vercel/next.js/raw/91d78d1035585b2eb746c4dd4615b838264c8673/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fbenches%2Foverhead.rs",
            "contents_url": "https://api.github.com/repos/vercel/next.js/contents/turbopack%2Fcrates%2Fturbo-tasks-backend%2Fbenches%2Foverhead.rs?ref=91d78d1035585b2eb746c4dd4615b838264c8673",
            "patch": "@@ -0,0 +1,141 @@\n+use std::time::{Duration, Instant};\n+\n+use criterion::{BenchmarkId, Criterion, black_box};\n+use turbo_tasks::TurboTasks;\n+use turbo_tasks_backend::{BackendOptions, TurboTasksBackend, noop_backing_storage};\n+\n+use super::register;\n+\n+// Tunable task: busy-wait for a given duration\n+#[inline(never)]\n+fn busy_task(duration: Duration) {\n+    let start = Instant::now();\n+    while start.elapsed() < duration {\n+        std::hint::spin_loop();\n+    }\n+}\n+\n+// Simulate running the task inside turbo-tasks (replace with actual turbo-tasks API)\n+#[turbo_tasks::function]\n+fn busy_turbo(key: u64, duration: Duration) {\n+    busy_task(duration);\n+    black_box(key); // consume the key, we need it to be part of the cache key.\n+}\n+\n+pub fn overhead(c: &mut Criterion) {\n+    register();\n+\n+    let mut group = c.benchmark_group(\"task_overhead\");\n+    group.sample_size(100);\n+    let rt: tokio::runtime::Runtime = tokio::runtime::Builder::new_multi_thread()\n+        .enable_all()\n+        .build()\n+        .unwrap();\n+\n+    // Test durations between 10us and 1ms.  This enables two things\n+    // 1. ensure that our busy task is working correctly, we should see uncached times scale with\n+    //    this metric\n+    // 2. see if there are effects related to how long await points take\n+    for micros in [1, 10, 100, 1000] {\n+        let duration = Duration::from_micros(micros);\n+\n+        group.bench_with_input(BenchmarkId::new(\"direct\", micros), &duration, |b, &d| {\n+            b.iter(|| busy_task(black_box(d)))\n+        });\n+\n+        group.bench_with_input(\n+            BenchmarkId::new(\"turbo-uncached\", micros),\n+            &duration,\n+            |b, &d| {\n+                run_turbo::<Uncached>(&rt, b, d);\n+            },\n+        );\n+\n+        group.bench_with_input(\n+            BenchmarkId::new(\"turbo-cached-same-keys\", micros),\n+            &duration,\n+            |b, &d| {\n+                run_turbo::<CachedSame>(&rt, b, d);\n+            },\n+        );\n+\n+        group.bench_with_input(\n+            BenchmarkId::new(\"turbo-cached-different-keys\", micros),\n+            &duration,\n+            |b, &d| {\n+                run_turbo::<CachedDifferent>(&rt, b, d);\n+            },\n+        );\n+    }\n+    group.finish();\n+}\n+\n+trait TurboMode {\n+    fn key(index: u64) -> u64;\n+    fn is_cached() -> bool;\n+}\n+struct Uncached;\n+impl TurboMode for Uncached {\n+    fn key(index: u64) -> u64 {\n+        index\n+    }\n+\n+    fn is_cached() -> bool {\n+        false\n+    }\n+}\n+struct CachedSame;\n+impl TurboMode for CachedSame {\n+    fn key(_index: u64) -> u64 {\n+        0\n+    }\n+\n+    fn is_cached() -> bool {\n+        true\n+    }\n+}\n+struct CachedDifferent;\n+impl TurboMode for CachedDifferent {\n+    fn key(index: u64) -> u64 {\n+        index\n+    }\n+\n+    fn is_cached() -> bool {\n+        true\n+    }\n+}\n+fn run_turbo<Mode: TurboMode>(\n+    rt: &tokio::runtime::Runtime,\n+    b: &mut criterion::Bencher<'_>,\n+    d: Duration,\n+) {\n+    b.to_async(rt).iter_custom(|iters| {\n+        // It is important to create the tt instance here to ensure the cache is not shared across\n+        // iterations.\n+        let tt = TurboTasks::new(TurboTasksBackend::new(\n+            BackendOptions {\n+                storage_mode: None,\n+                ..Default::default()\n+            },\n+            noop_backing_storage(),\n+        ));\n+\n+        async move {\n+            tt.run_once(async move {\n+                // If cached run once outside the loop to ensure the tasks are cached.\n+                if Mode::is_cached() {\n+                    for i in 0..iters {\n+                        black_box(busy_turbo(i, black_box(d)).await?);\n+                    }\n+                }\n+                let start = Instant::now();\n+                for i in 0..iters {\n+                    black_box(busy_turbo(Mode::key(i), black_box(d)).await?);\n+                }\n+                Ok(start.elapsed())\n+            })\n+            .await\n+            .unwrap()\n+        }\n+    });\n+}"
        }
    ],
    "stats": {
        "total": 144,
        "additions": 143,
        "deletions": 1
    }
}