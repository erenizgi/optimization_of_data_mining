{
    "author": "penpornk",
    "message": "[xla:tsl] Add `if_onednn` to guard oneDNN dependencies in XLA integration.\n\nPiperOrigin-RevId: 809047867",
    "sha": "968a096a0a8ad0d8dd9ebed5d0b80c253a974282",
    "files": [
        {
            "sha": "9c0dc3c49720c975b5b74316f529d6bf1997c0b9",
            "filename": "third_party/xla/xla/service/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/968a096a0a8ad0d8dd9ebed5d0b80c253a974282/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/968a096a0a8ad0d8dd9ebed5d0b80c253a974282/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2FBUILD?ref=968a096a0a8ad0d8dd9ebed5d0b80c253a974282",
            "patch": "@@ -19,6 +19,7 @@ load(\"//xla/stream_executor:build_defs.bzl\", \"if_cuda_or_rocm_is_configured\", \"i\n load(\"//xla/tests:build_defs.bzl\", \"xla_test\")\n load(\"//xla/tsl:tsl.bzl\", \"if_google\", \"if_libtpu\", \"internal_visibility\", \"tsl_copts\")\n load(\"//xla/tsl:tsl.default.bzl\", \"filegroup\", \"get_compatible_with_portable\")\n+load(\"//xla/tsl/mkl:build_defs.bzl\", \"if_onednn\")\n load(\n     \"//xla/tsl/platform:build_config.bzl\",\n     \"tf_proto_library\",\n@@ -5694,8 +5695,9 @@ cc_library(\n     deps = [\n         \":hlo_creation_utils\",\n         \"//xla/hlo/pass:hlo_pass\",\n+    ] + if_onednn([\n         \"//xla/service/cpu:onednn_contraction_rewriter\",\n-    ],\n+    ]),\n )\n \n cc_library("
        },
        {
            "sha": "139a43761faec47709672462b242c77cdfc91593",
            "filename": "third_party/xla/xla/service/cpu/BUILD",
            "status": "modified",
            "additions": 14,
            "deletions": 11,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/968a096a0a8ad0d8dd9ebed5d0b80c253a974282/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/968a096a0a8ad0d8dd9ebed5d0b80c253a974282/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD?ref=968a096a0a8ad0d8dd9ebed5d0b80c253a974282",
            "patch": "@@ -13,7 +13,7 @@ load(\n load(\"//xla/tests:build_defs.bzl\", \"xla_test\")\n load(\"//xla/tsl:tsl.bzl\", \"internal_visibility\", \"tsl_copts\")\n load(\"//xla/tsl:tsl.default.bzl\", \"filegroup\", \"get_compatible_with_portable\")\n-load(\"//xla/tsl/mkl:build_defs.bzl\", \"if_graph_api\")\n+load(\"//xla/tsl/mkl:build_defs.bzl\", \"if_graph_api\", \"if_onednn\")\n load(\"//xla/tsl/platform:build_config.bzl\", \"tf_proto_library\")\n load(\n     \"//xla/tsl/platform:build_config_root.bzl\",\n@@ -203,9 +203,6 @@ cc_library(\n         \":ir_emitter\",\n         \":ir_emitter2\",\n         \":metrics\",\n-        \":onednn_contraction_rewriter\",\n-        \":onednn_float_support\",\n-        \":onednn_ops_rewriter\",\n         \":parallel_task_assignment\",\n         \":runtime_symbol_generator\",\n         \":small_while_loop_hoisting_pass\",\n@@ -417,6 +414,10 @@ cc_library(\n         \"@llvm-project//llvm:SystemZCodeGen\",  # fixdeps: keep\n     ]) + if_llvm_x86_available([\n         \"@llvm-project//llvm:X86CodeGen\",  # fixdeps: keep\n+    ]) + if_onednn([\n+        \":onednn_contraction_rewriter\",\n+        \":onednn_float_support\",\n+        \":onednn_ops_rewriter\",\n     ]),\n )\n \n@@ -584,10 +585,6 @@ cc_library(\n     copts = if_enable_acl([\"-DXLA_CPU_USE_ACL=1\"]) + tsl_copts(),\n     deps = [\n         \":cpu_runtime\",\n-        \":onednn_convolution\",\n-        \":onednn_layer_norm\",\n-        \":onednn_matmul\",\n-        \":onednn_softmax\",\n         \":runtime_conv2d\",\n         \":runtime_conv2d_acl\",\n         \":runtime_conv3d\",\n@@ -610,7 +607,12 @@ cc_library(\n         \"@llvm-project//llvm:Support\",\n         \"@llvm-project//mlir:mlir_c_runner_utils\",\n         \"@local_tsl//tsl/platform:logging\",\n-    ],\n+    ] + if_onednn([\n+        \":onednn_convolution\",\n+        \":onednn_layer_norm\",\n+        \":onednn_matmul\",\n+        \":onednn_softmax\",\n+    ]),\n )\n \n cc_library(\n@@ -981,7 +983,6 @@ cc_library(\n         \"//xla/backends/cpu/runtime:topk_thunk\",\n         \"//xla/backends/cpu/runtime:while_thunk\",\n         \"//xla/backends/cpu/runtime/onednn:onednn_fusion_thunk\",\n-        \"//xla/backends/cpu/runtime/onednn:onednn_op_thunk\",\n         \"//xla/backends/cpu/runtime/xnnpack:xnn_dot_thunk\",\n         \"//xla/backends/cpu/runtime/xnnpack:xnn_fusion_thunk\",\n         \"//xla/codegen:kernel_definition\",\n@@ -1020,7 +1021,9 @@ cc_library(\n         \"@llvm-project//mlir:IR\",\n         \"@llvm-project//mlir:Support\",\n         \"@local_tsl//tsl/profiler/lib:traceme\",\n-    ],\n+    ] + if_onednn([\n+        \"//xla/backends/cpu/runtime/onednn:onednn_op_thunk\",\n+    ]),\n )\n \n cc_library("
        },
        {
            "sha": "1027ff6deecb9a2549ed54ffeea2fab9c65f6456",
            "filename": "third_party/xla/xla/tsl/mkl/build_defs.bzl",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/968a096a0a8ad0d8dd9ebed5d0b80c253a974282/third_party%2Fxla%2Fxla%2Ftsl%2Fmkl%2Fbuild_defs.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/968a096a0a8ad0d8dd9ebed5d0b80c253a974282/third_party%2Fxla%2Fxla%2Ftsl%2Fmkl%2Fbuild_defs.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftsl%2Fmkl%2Fbuild_defs.bzl?ref=968a096a0a8ad0d8dd9ebed5d0b80c253a974282",
            "patch": "@@ -41,6 +41,10 @@ def if_mkl(if_true, if_false = []):\n         \"//conditions:default\": if_false,\n     })\n \n+# Use `if_onednn` for XLA code to allow different configurations between TF and\n+# XLA in the future.\n+if_onednn = if_mkl\n+\n def if_mkl_ml(if_true, if_false = []):\n     \"\"\"Shorthand for select()'ing on whether we're building with MKL-ML.\n "
        }
    ],
    "stats": {
        "total": 33,
        "additions": 21,
        "deletions": 12
    }
}