{
    "author": "mkuperst",
    "message": "[XLA] Partition computations by directly traversing the call-graph.\n\nCurrently, the SPMD partitioner traverses the call-graph (for kControlFlow call sites) by traversing the instructions of each computation and partitioning called computations on the fly. This refactors it to explicitly walk the call-graph instead. This is not expected to have any effect when the call-graph is a tree, and is a preparatory CL for allowing general DAGs.\n\nPiperOrigin-RevId: 798204999",
    "sha": "e890c9a8b8399ecec6e222092cce9623447d62f5",
    "files": [
        {
            "sha": "c5a8e209cde87b853ee6ef4304646fb804b02225",
            "filename": "third_party/xla/xla/service/spmd/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e890c9a8b8399ecec6e222092cce9623447d62f5/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e890c9a8b8399ecec6e222092cce9623447d62f5/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2FBUILD?ref=e890c9a8b8399ecec6e222092cce9623447d62f5",
            "patch": "@@ -87,6 +87,7 @@ cc_library(\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/types:span\",\n         \"@com_google_absl//absl/utility\",\n         \"@local_tsl//tsl/platform:numbers\","
        },
        {
            "sha": "4a3019ba9d44ac8d0099a1bbdc111319aa70753f",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner.cc",
            "status": "modified",
            "additions": 75,
            "deletions": 39,
            "changes": 114,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/e890c9a8b8399ecec6e222092cce9623447d62f5/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/e890c9a8b8399ecec6e222092cce9623447d62f5/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc?ref=e890c9a8b8399ecec6e222092cce9623447d62f5",
            "patch": "@@ -37,6 +37,7 @@ limitations under the License.\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/str_format.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n #include \"xla/array.h\"\n@@ -2549,12 +2550,6 @@ absl::Status SpmdPartitioningVisitor::HandleCall(HloInstruction* hlo) {\n   for (int64_t i = 0; i < hlo->operand_count(); ++i) {\n     call_args.push_back(GetPartitionedHlo(hlo->operand(i)).hlo());\n   }\n-\n-  TF_RETURN_IF_ERROR(\n-      partitioner_\n-          ->PartitionComputation(hlo->to_apply(), hlo->sharding(),\n-                                 next_channel_id_, logger_, call_graph_)\n-          .status());\n   SetPartitionedHlo(hlo, [&] {\n     auto* call = b_.AddInstruction(HloInstruction::CreateCall(\n         MakePartitionedShape(hlo->shape(), hlo->sharding()), call_args,\n@@ -4276,19 +4271,6 @@ absl::Status SpmdPartitioningVisitor::HandleReverse(HloInstruction* hlo) {\n \n absl::Status SpmdPartitioningVisitor::HandleWhile(HloInstruction* hlo) {\n   const HloSharding& sharding = hlo->sharding();\n-  HloInstruction* cond_root = hlo->while_condition()->root_instruction();\n-  const HloSharding cond_root_sharding = cond_root->sharding();\n-  TF_RETURN_IF_ERROR(\n-      partitioner_\n-          ->PartitionComputation(hlo->while_condition(), cond_root_sharding,\n-                                 next_channel_id_, logger_, call_graph_)\n-          .status());\n-  TF_RETURN_IF_ERROR(partitioner_\n-                         ->PartitionComputation(hlo->while_body(), sharding,\n-                                                next_channel_id_, logger_,\n-                                                call_graph_)\n-                         .status());\n-\n   HloInstruction* whileOp = b_.AddInstruction(HloInstruction::CreateWhile(\n       MakePartitionedShape(hlo->shape(), sharding), hlo->while_condition(),\n       hlo->while_body(),\n@@ -4300,20 +4282,11 @@ absl::Status SpmdPartitioningVisitor::HandleWhile(HloInstruction* hlo) {\n \n absl::Status SpmdPartitioningVisitor::HandleConditional(HloInstruction* hlo) {\n   std::vector<HloInstruction*> branch_args;\n+  branch_args.reserve(hlo->branch_count());\n   for (int64_t i = 0; i < hlo->branch_count(); ++i) {\n     branch_args.push_back(GetPartitionedHlo(hlo->operand(i + 1)).hlo());\n   }\n \n-  // The root of the branch computations must follow the sharding of the\n-  // conditional instruction.\n-  for (int64_t i = 0; i < hlo->branch_count(); ++i) {\n-    HloComputation* computation = hlo->branch_computation(i);\n-    TF_RETURN_IF_ERROR(partitioner_\n-                           ->PartitionComputation(computation, hlo->sharding(),\n-                                                  next_channel_id_, logger_,\n-                                                  call_graph_)\n-                           .status());\n-  }\n   SetPartitionedHlo(hlo, [&] {\n     HloInstruction* cond = GetPartitionedHlo(hlo->operand(0)).hlo();\n     if (!hlo->operand(0)->sharding().IsManual()) {\n@@ -5519,18 +5492,81 @@ absl::StatusOr<bool> SpmdPartitioner::Run(\n                     /*disabled=*/!VLOG_IS_ON(1));\n   auto program_shape = module->entry_computation()->ComputeProgramShape();\n   int64_t next_channel_id = hlo_query::NextChannelId(*module);\n-  // Copy the root sharding since the partitioner visitor may temporarily change\n-  // the sharding to work around manual sharding.\n-  HloSharding root_sharding =\n-      module->entry_computation()->root_instruction()->sharding();\n-\n   std::unique_ptr<CallGraph> call_graph = CallGraph::Build(module);\n   CHECK(call_graph->IsFlattened());\n-  TF_ASSIGN_OR_RETURN(\n-      bool partition_changed,\n-      PartitionComputation(module->entry_computation(), root_sharding,\n-                           &next_channel_id, &logger, *call_graph));\n-  changed |= partition_changed;\n+  TF_RETURN_IF_ERROR(\n+      call_graph->VisitNodes([&](const CallGraphNode& node) -> absl::Status {\n+        HloComputation* computation = node.computation();\n+        if (computation->IsEntryComputation()) {\n+          // Copy the root sharding since the partitioner visitor may\n+          // temporarily change the sharding to work around manual sharding.\n+          HloSharding root_sharding =\n+              module->entry_computation()->root_instruction()->sharding();\n+          TF_ASSIGN_OR_RETURN(\n+              bool partition_changed,\n+              PartitionComputation(computation, root_sharding, &next_channel_id,\n+                                   &logger, *call_graph));\n+          changed |= partition_changed;\n+          return absl::OkStatus();\n+        }\n+        if (!execution_threads.empty() &&\n+            !execution_threads.contains(computation->execution_thread())) {\n+          return absl::OkStatus();\n+        }\n+        if (node.context() != CallContext::kControlFlow) {\n+          return absl::OkStatus();\n+        }\n+        if (node.caller_callsites().empty()) {\n+          return absl::OkStatus();\n+        }\n+        if (node.caller_callsites().size() > 1) {\n+          return absl::InternalError(\n+              \"Expected CFG to be flattened before SPMD partitioner.\");\n+        }\n+        HloInstruction* caller = node.caller_callsites()[0].instruction();\n+        switch (caller->opcode()) {\n+          case HloOpcode::kWhile: {\n+            bool is_body = (caller->while_body() == computation);\n+            bool is_condition = (caller->while_condition() == computation);\n+            // We don't expect the same computation to be both the body and the\n+            // condition. Bail out.\n+            if (is_body == is_condition) {\n+              return absl::InternalError(\n+                  \"Expected a computation used by kWhile to be the while's \"\n+                  \"body or condition (but not both).\");\n+            }\n+            bool partition_changed;\n+            if (is_body) {\n+              TF_ASSIGN_OR_RETURN(\n+                  partition_changed,\n+                  PartitionComputation(computation, caller->sharding(),\n+                                       &next_channel_id, &logger, *call_graph));\n+            } else {\n+              HloInstruction* cond_root = computation->root_instruction();\n+              const HloSharding cond_root_sharding = cond_root->sharding();\n+              TF_ASSIGN_OR_RETURN(\n+                  partition_changed,\n+                  PartitionComputation(computation, cond_root_sharding,\n+                                       &next_channel_id, &logger, *call_graph));\n+            }\n+            changed |= partition_changed;\n+            break;\n+          }\n+          case HloOpcode::kCall:\n+          case HloOpcode::kConditional: {\n+            TF_RETURN_IF_ERROR(\n+                PartitionComputation(computation, caller->sharding(),\n+                                     &next_channel_id, &logger, *call_graph)\n+                    .status());\n+            break;\n+          }\n+          default:\n+            return absl::InternalError(absl::StrFormat(\n+                \"Unexpected control-flow callsite in SPMD partitioner: %s\",\n+                caller->ToString()));\n+        }\n+        return absl::OkStatus();\n+      }));\n \n   // For the entry computation, make sure that the root instruction and the\n   // parameters preserve their signatures."
        }
    ],
    "stats": {
        "total": 115,
        "additions": 76,
        "deletions": 39
    }
}