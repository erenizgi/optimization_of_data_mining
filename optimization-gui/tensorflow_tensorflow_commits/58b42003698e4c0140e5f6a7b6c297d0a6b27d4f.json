{
    "author": "pschuh",
    "message": "Generalize AllocateOutputBuffersWithInputReuse onto CommonPjRtClient.\n\nPiperOrigin-RevId: 839418809",
    "sha": "58b42003698e4c0140e5f6a7b6c297d0a6b27d4f",
    "files": [
        {
            "sha": "7b88775c3c088be90968fda32f14f3df7588c148",
            "filename": "third_party/xla/xla/pjrt/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/58b42003698e4c0140e5f6a7b6c297d0a6b27d4f/third_party%2Fxla%2Fxla%2Fpjrt%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/58b42003698e4c0140e5f6a7b6c297d0a6b27d4f/third_party%2Fxla%2Fxla%2Fpjrt%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2FBUILD?ref=58b42003698e4c0140e5f6a7b6c297d0a6b27d4f",
            "patch": "@@ -149,6 +149,7 @@ cc_library(\n         \"//xla:shape_util\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n+        \"//xla/hlo/ir:hlo\",\n         \"//xla/tsl/concurrency:async_value\",\n         \"//xla/tsl/concurrency:ref_count\",\n         \"//xla/tsl/platform:errors\","
        },
        {
            "sha": "f2eb19e133db61cb45aa94bdea3f10e0905d4541",
            "filename": "third_party/xla/xla/pjrt/common_pjrt_client.cc",
            "status": "modified",
            "additions": 121,
            "deletions": 0,
            "changes": 121,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/58b42003698e4c0140e5f6a7b6c297d0a6b27d4f/third_party%2Fxla%2Fxla%2Fpjrt%2Fcommon_pjrt_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/58b42003698e4c0140e5f6a7b6c297d0a6b27d4f/third_party%2Fxla%2Fxla%2Fpjrt%2Fcommon_pjrt_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fcommon_pjrt_client.cc?ref=58b42003698e4c0140e5f6a7b6c297d0a6b27d4f",
            "patch": "@@ -39,6 +39,7 @@ limitations under the License.\n #include \"absl/synchronization/mutex.h\"\n #include \"absl/types/span.h\"\n #include \"xla/future.h\"\n+#include \"xla/hlo/ir/hlo_input_output_alias_config.h\"\n #include \"xla/layout.h\"\n #include \"xla/layout_util.h\"\n #include \"xla/literal.h\"\n@@ -425,6 +426,126 @@ void CommonPjRtClient::ScheduleRemoteSend(\n   usage_event_promise->SetError(error);\n }\n \n+absl::StatusOr<absl::InlinedVector<tsl::RCReference<CommonPjRtRawBuffer>, 4>>\n+CommonPjRtClient::AllocateOutputBuffersWithInputReuse(\n+    const Shape& output_device_shape,\n+    absl::Span<const CommonPjRtBuffer::ScopedHold> input_device_buffer_holds,\n+    const HloInputOutputAliasConfig& alias_config, PjRtDevice* device,\n+    absl::Span<const int> output_memory_space_kind_ids) {\n+  tsl::profiler::TraceMe traceme(\"AllocateOutputBuffersWithInputReuse\");\n+  VLOG(1) << \"Creating an output buffer, which may be partially donated, with \"\n+             \"shape \"\n+          << output_device_shape.ToString();\n+  absl::InlinedVector<tsl::RCReference<CommonPjRtRawBuffer>, 4> buffers;\n+  if (output_device_shape.IsTuple() &&\n+      output_device_shape.tuple_shapes().empty()) {\n+    return buffers;\n+  }\n+  int num_input_pjrt_buffers = input_device_buffer_holds.size();\n+  absl::Span<const Shape> output_leaf_shapes =\n+      output_device_shape.IsTuple()\n+          ? absl::MakeSpan(output_device_shape.tuple_shapes())\n+          : absl::MakeSpan(&output_device_shape, 1);\n+  auto get_alias = [&](int i) {\n+    return output_device_shape.IsTuple() ? alias_config.GetAliasedParameter({i})\n+                                         : alias_config.GetAliasedParameter({});\n+  };\n+  buffers.reserve(output_leaf_shapes.size());\n+\n+  auto should_allocate_new_buffer =\n+      [&](std::optional<HloInputOutputAliasConfig::Alias> alias) -> bool {\n+    if (!alias.has_value()) {\n+      return true;\n+    }\n+    int parameter_index = alias->parameter_number;\n+    // Handle \"Case 3.\" input\n+    // donation below. ^ denotes donation pair. i0,  i1^  ->   r0^ where\n+    // parameter_is_tupled_arguments=true\n+    //\n+    // e.g. For alias: {0, {1}, may-alias}\n+    // We should check the donation eligibility of the second buffer in the\n+    // input list.\n+    if (num_input_pjrt_buffers > 1 && alias->parameter_index.size() == 1) {\n+      parameter_index = alias->parameter_index[0];\n+    }\n+    return input_device_buffer_holds[parameter_index].type() !=\n+           CommonPjRtBuffer::ScopedHold::kDonation;\n+  };\n+  std::vector<size_t> output_buffer_sizes;\n+  for (int i = 0; i < output_leaf_shapes.size(); ++i) {\n+    std::optional<HloInputOutputAliasConfig::Alias> alias = get_alias(i);\n+    if (should_allocate_new_buffer(alias)) {\n+      const Shape& leaf_shape = output_leaf_shapes[i];\n+      const auto& current_anno =\n+          tsl::profiler::ScopedMemoryDebugAnnotation::CurrentAnnotation();\n+      tsl::profiler::ScopedMemoryDebugAnnotation anno(\n+          \"dummy\", current_anno.pending_region_type, 0, [&leaf_shape]() {\n+            return ShapeUtil::HumanStringWithLayout(leaf_shape);\n+          });\n+      int kind_id = output_memory_space_kind_ids[i];\n+      PjRtMemorySpace* memory_space = nullptr;\n+      for (PjRtMemorySpace* ms : device->memory_spaces()) {\n+        if (kind_id == ms->kind_id()) {\n+          memory_space = ms;\n+          break;\n+        }\n+      }\n+      if (memory_space == nullptr) {\n+        return absl::InternalError(\n+            absl::StrCat(\"No memory space found (kind_id: \", kind_id, \")\"));\n+      }\n+      TF_ASSIGN_OR_RETURN(int64_t on_device_bytes,\n+                          GetOnDeviceBytesCount(memory_space, leaf_shape));\n+      TF_ASSIGN_OR_RETURN(auto raw_buffer,\n+                          AllocateRawBuffer(memory_space, on_device_bytes,\n+                                            /*retry_on_oom=*/false,\n+                                            /*allocate_after=*/{}));\n+      buffers.push_back(std::move(raw_buffer));\n+    } else {\n+      // a tuple output element alias to input. There are 3 supported cases.\n+      // Case 1: alias a non-tuple input.\n+      // Case 2: alias a tuple input leaf while a single tuple PjRtBuffer is\n+      // passed to PjRtLoadExecutable::Execute.\n+      // Case 3: alias a tuple input leaf while individual input PjRtBuffer\n+      // leaves are passed to PjRtLoadExecutable::Execute.\n+      const ShapeIndex& shape_index = alias->parameter_index;\n+      size_t parameter_number;\n+      if (shape_index.empty()) {\n+        // Case 1: (o, i, {}) alias non-tuple input i\n+        CHECK_LT(alias->parameter_number, num_input_pjrt_buffers);\n+        parameter_number = alias->parameter_number;\n+      } else if (num_input_pjrt_buffers == 1 && shape_index.size() == 1 &&\n+                 shape_index[0] != 0) {\n+        // Case 2: (o, 0, {i}) alias a single tuple input's i-th element\n+        //  where i > 0.\n+        return Unimplemented(\"Alias %s not supported: found %d inputs.\",\n+                             alias->ToString(), num_input_pjrt_buffers);\n+      } else if (shape_index.size() == 1) {\n+        // Case 3: (o, 0, {i}) alias a single tuple input's i-th element but\n+        // the input PjRtBuffers have not been tuplized yet\n+        parameter_number = shape_index[0];\n+      } else {\n+        return Unimplemented(\"Alias %s not supported: found %d inputs.\",\n+                             alias->ToString(), num_input_pjrt_buffers);\n+      }\n+      const CommonPjRtBuffer::ScopedHold& input_hold =\n+          input_device_buffer_holds[parameter_number];\n+      buffers.push_back(input_hold.buffer()->raw_buffer());\n+    }\n+  }\n+\n+  if (VLOG_IS_ON(1)) {\n+    int64_t total_size = 0;\n+    for (const auto size : output_buffer_sizes) {\n+      total_size += size;\n+    }\n+    LOG(INFO)\n+        << \"Total size of new output buffers allocated in this execution: \"\n+        << total_size;\n+  }\n+  return std::move(buffers);\n+}\n+\n absl::StatusOr<std::unique_ptr<PjRtBuffer>>\n CommonPjRtBufferImpl::CopyToCpuMemorySpace(const xla::Shape& dst_shape,\n                                            PjRtMemorySpace* dst_memory_space) {"
        },
        {
            "sha": "985a3b61348baa1d58667967960a24141d7ccd55",
            "filename": "third_party/xla/xla/pjrt/common_pjrt_client.h",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/58b42003698e4c0140e5f6a7b6c297d0a6b27d4f/third_party%2Fxla%2Fxla%2Fpjrt%2Fcommon_pjrt_client.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/58b42003698e4c0140e5f6a7b6c297d0a6b27d4f/third_party%2Fxla%2Fxla%2Fpjrt%2Fcommon_pjrt_client.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fcommon_pjrt_client.h?ref=58b42003698e4c0140e5f6a7b6c297d0a6b27d4f",
            "patch": "@@ -235,6 +235,13 @@ class CommonPjRtClient : public PjRtClient {\n       tsl::RCReference<PjRtDeviceEventPromise> usage_event_promise,\n       Future<std::string> serialized_descriptor,\n       PjRtBuffer::RemoteSendCallback on_done);\n+\n+  absl::StatusOr<absl::InlinedVector<tsl::RCReference<CommonPjRtRawBuffer>, 4>>\n+  AllocateOutputBuffersWithInputReuse(\n+      const Shape& output_device_shape,\n+      absl::Span<const CommonPjRtBuffer::ScopedHold> input_device_buffer_holds,\n+      const HloInputOutputAliasConfig& alias_config, PjRtDevice* device,\n+      absl::Span<const int> output_memory_space_kind_ids);\n };\n \n // TODO(parkers): Merge everything here into CommonPjRtBuffer."
        }
    ],
    "stats": {
        "total": 129,
        "additions": 129,
        "deletions": 0
    }
}