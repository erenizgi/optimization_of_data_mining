{
    "author": "unknown",
    "message": "PR #31366: Update to operation_semantics\n\nImported from GitHub PR https://github.com/openxla/xla/pull/31366\n\n## Summary of Changes\n\n- **`AllGather`**\n   - Updated table to include `shard count`\n   - Added `layout` and `use_global_device_ids` to table and signature\n   - Added info on `use_global_device_ids` to list\n\n- **`AllReduce`**\n   - Updated `replica_group_ids` to `replica_groups`\n   - Added `shape_with_layout` and `use_global_device_ids` to table, signature, and list\n   - Added `shape_with_layout` and `use_global_device_ids` to list\n\n- **`CollectivePermute `**\n   - Added `channel_id`\n   - minor formating\n\n- **`CrossReplicaSum `**\n   - Added signature and table\n   - minor formating\n\n- **`CustomCall `**\n   - Removed the majority of information with redirect link to better documentation\n\n- **`Gather `**\n  - Added link to StableHLO spec\n\n- **`GetTupleElement`**\n - Added signature and table\n\n- **`ReShape`**\n - Added signature and table\n - Added Link to stableHLO spec\n\nðŸŽ¯ Justification\nDocumentation update\n\nðŸš€ Kind of Contribution\nðŸ“š Documentation\nCopybara import of the project:\n\n--\n5aa399a802a603e0734fc26250b934f83dd9d496 by Amelia Thurdekoos <athurdekoos@quansight.com>:\n\nFinal signature matching changes\n\nMerging this change closes #31366\n\nPiperOrigin-RevId: 807817656",
    "sha": "0698098c0c555d912a47631d5c475822a0b8dccf",
    "files": [
        {
            "sha": "0ba147ea41ef0e906ad6fdd96dd5230201f87162",
            "filename": "third_party/xla/docs/operation_semantics.md",
            "status": "modified",
            "additions": 94,
            "deletions": 73,
            "changes": 167,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/0698098c0c555d912a47631d5c475822a0b8dccf/third_party%2Fxla%2Fdocs%2Foperation_semantics.md",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/0698098c0c555d912a47631d5c475822a0b8dccf/third_party%2Fxla%2Fdocs%2Foperation_semantics.md",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2Foperation_semantics.md?ref=0698098c0c555d912a47631d5c475822a0b8dccf",
            "patch": "@@ -37,17 +37,26 @@ See also\n Performs concatenation across replicas.\n \n **`AllGather(operand, all_gather_dim, shard_count, replica_group_ids,\n-channel_id)`**\n-\n-| Arguments        | Type                 | Semantics                   |\n-| ---------------- | -------------------- | --------------------------- |\n-| `operand`        | `XlaOp`              | Array to concatenate across |\n-:                  :                      : replicas                    :\n-| `all_gather_dim` | `int64`              | Concatenation dimension     |\n-| `replica_groups` | vector of vectors of | Groups between which the    |\n-:                  : `int64`              : concatenation is performed  :\n-| `channel_id`     | optional `int64`     | Optional channel ID for     |\n-:                  :                      : cross-module communication  :\n+channel_id, layout, use_global_device_ids)`**\n+\n+| Arguments               | Type                 | Semantics                   |\n+| ----------------------- | -------------------- | --------------------------- |\n+| `operand`               | `XlaOp`              | Array to concatenate across |\n+:                         :                      : replicas                    :\n+| `all_gather_dim`        | `int64`              | Concatenation dimension     |\n+| `shard_count`           | `int64`              | The size of each replica    |\n+:                         :                      : group                       :\n+| `replica_groups`        | vector of vectors of | Groups between which the    |\n+:                         : `int64`              : concatenation is performed  :\n+| `channel_id`            | optional `int64`     | Optional channel ID for     |\n+:                         :                      : cross-module communication  :\n+| `layout`                | optional `Layout`    | Creates a layout pattern    |\n+:                         :                      : that will capture the       :\n+:                         :                      : matched layout in the       :\n+:                         :                      : argument                    :\n+| `use_global_device_ids` | optional `bool`      | Returns true if the ids in  |\n+:                         :                      : the ReplicaGroup config     :\n+:                         :                      : represent a global id       :\n \n -   `replica_groups` is a list of replica groups between which the concatenation\n     is performed (replica id for the current replica can be retrieved using\n@@ -61,6 +70,10 @@ channel_id)`**\n     `replica_groups` are empty.\n -   `channel_id` is used for cross-module communication: only `all-gather`\n     operations with the same `channel_id` can communicate to each other.\n+-   `use_global_device_ids` Returns true if the ids in the ReplicaGroup config\n+    represent a global id of (replica_id * partition_count + partition_id)\n+    instead of a replica id. This enables more flexible grouping of devices if\n+    this all-reduce is both cross-partition and cross-replica.\n \n The output shape is the input shape with the `all_gather_dim` made `shard_count`\n times larger. For example, if there are two replicas and the operand has the\n@@ -75,17 +88,24 @@ See also\n \n Performs a custom computation across replicas.\n \n-**`AllReduce(operand, computation, replica_group_ids, channel_id)`**\n-\n-| Arguments        | Type                 | Semantics                        |\n-| ---------------- | -------------------- | -------------------------------- |\n-| `operand`        | `XlaOp`              | Array or a non-empty tuple of    |\n-:                  :                      : arrays to reduce across replicas :\n-| `computation`    | `XlaComputation`     | Reduction computation            |\n-| `replica_groups` | vector of vectors of | Groups between which the         |\n-:                  : `int64`              : reductions are performed         :\n-| `channel_id`     | optional `int64`     | Optional channel ID for          |\n-:                  :                      : cross-module communication       :\n+**`AllReduce(operand, computation, replica_groups, channel_id,\n+shape_with_layout, use_global_device_ids)`**\n+\n+| Arguments               | Type                 | Semantics                  |\n+| ----------------------- | -------------------- | -------------------------- |\n+| `operand`               | `XlaOp`              | Array or a non-empty tuple |\n+:                         :                      : of arrays to reduce across :\n+:                         :                      : replicas                   :\n+| `computation`           | `XlaComputation`     | Reduction computation      |\n+| `replica_groups`        | vector of vectors of | Groups between which the   |\n+:                         : `int64`              : reductions are performed   :\n+| `channel_id`            | optional `int64`     | Optional channel ID for    |\n+:                         :                      : cross-module communication :\n+| `shape_with_layout`     | optional `Layout`    | Defines the layout of the  |\n+:                         :                      : data transferred           :\n+| `use_global_device_ids` | optional `bool`      | Returns true if the ids in |\n+:                         :                      : the ReplicaGroup config    :\n+:                         :                      : represent a global id      :\n \n -   When `operand` is a tuple of arrays, the all-reduce is performed on each\n     element of the tuple.\n@@ -98,6 +118,13 @@ Performs a custom computation across replicas.\n     `3`.\n -   `channel_id` is used for cross-module communication: only `all-reduce`\n     operations with the same `channel_id` can communicate to each other.\n+-   `shape_with_layout`: forces the layout of the AllReduce to the given layout.\n+    This is used to guarantee the same layout for a group of AllReduce ops\n+    compiled separately.\n+-   `use_global_device_ids` Returns true if the ids in the ReplicaGroup config\n+    represent a global id of (replica_id * partition_count + partition_id)\n+    instead of a replica id. This enables more flexible grouping of devices if\n+    this all-reduce is both cross-partition and cross-replica.\n \n The output shape is the same as the input shape. For example, if there are two\n replicas and the operand has the value `[1.0, 2.5]` and `[3.0, 5.25]`\n@@ -650,12 +677,19 @@ See also\n CollectivePermute is a collective operation that sends and receives data cross\n replicas.\n \n-**`CollectivePermute(operand, source_target_pairs)`**\n+**`CollectivePermute(operand, source_target_pairs, channel_id)`**\n \n | Arguments             | Type                    | Semantics                  |\n | --------------------- | ----------------------- | -------------------------- |\n | `operand`             | `XlaOp`                 | n dimensional input array  |\n-| `source_target_pairs` | `<int64, int64>` vector | A list of (source_replica_id, target_replica_id) pairs. For each pair, the operand is sent from source replica to target replica. |\n+| `source_target_pairs` | `<int64, int64>` vector | A list of                  |\n+:                       :                         : (source_replica_id,        :\n+:                       :                         : target_replica_id) pairs.  :\n+:                       :                         : For each pair, the operand :\n+:                       :                         : is sent from source        :\n+:                       :                         : replica to target replica. :\n+| `channel_id`          | optional `int64`        | Optional channel ID for    |\n+:                       :                         : cross-module communication :\n \n Note that there are the following restrictions on the `source_target_pair`:\n \n@@ -1129,62 +1163,28 @@ then b == f32[3]{0.0, 1.0, 2.0}\n \n Performs `AllReduce` with a summation computation.\n \n+**`CrossReplicaSum(operand, replica_groups)`**\n+\n+| Arguments        | Type                 | Semantics                        |\n+| ---------------- | -------------------- | -------------------------------- |\n+| `operand`        | `XlaOp`              | Array or a non-empty tuple of    |\n+:                  :                      : arrays to reduce across replicas :\n+| `replica_groups` | vector of vectors of | Groups between which the         |\n+:                  : `int64`              : reductions are performed         :\n+\n+Returns the sum of the operand value within each subgroup of replicas. All\n+replicas supply one input to the sum and all replicas receive the resulting sum\n+for each subgroup.\n+\n ## CustomCall\n \n See also\n [`XlaBuilder::CustomCall`](https://github.com/openxla/xla/tree/main/xla/hlo/builder/xla_builder.h).\n \n Call a user-provided function within a computation.\n \n-**`CustomCall(target_name, args..., shape)`**\n-\n-| Arguments     | Type                   | Semantics                         |\n-| ------------- | ---------------------- | --------------------------------- |\n-| `target_name` | `string`               | Name of the function. A call instruction will be emitted which targets this symbol name. |\n-| `args`        | sequence of N `XlaOp`s | N arguments of arbitrary type, which will be passed to the function. |\n-| `shape`       | `Shape`                | Output shape of the function      |\n-\n-The function signature is the same, regardless of the arity or type of args:\n-\n-```cpp\n-extern \"C\" void target_name(void* out, void** in);\n-```\n-\n-For example, if CustomCall is used as follows:\n-\n-```cpp\n-let x = f32[2] {1,2};\n-let y = f32[2x3] {{10, 20, 30}, {40, 50, 60}};\n-\n-CustomCall(\"myfunc\", {x, y}, f32[3x3])\n-```\n-\n-Here is an example of an implementation of `myfunc`:\n-\n-```cpp\n-extern \"C\" void myfunc(void* out, void** in) {\n-  float (&x)[2] = *static_cast<float(*)[2]>(in[0]);\n-  float (&y)[2][3] = *static_cast<float(*)[2][3]>(in[1]);\n-  EXPECT_EQ(1, x[0]);\n-  EXPECT_EQ(2, x[1]);\n-  EXPECT_EQ(10, y[0][0]);\n-  EXPECT_EQ(20, y[0][1]);\n-  EXPECT_EQ(30, y[0][2]);\n-  EXPECT_EQ(40, y[1][0]);\n-  EXPECT_EQ(50, y[1][1]);\n-  EXPECT_EQ(60, y[1][2]);\n-  float (&z)[3][3] = *static_cast<float(*)[3][3]>(out);\n-  z[0][0] = x[1] + y[1][0];\n-  // ...\n-}\n-```\n-\n-The user-provided function must not have side-effects and its execution must be\n-idempotent.\n-\n-> Note: The opaque nature of the user-provided function restricts optimization\n-> opportunities for the compiler. Try to express your computation in terms of\n-> native XLA ops whenever possible; only use CustomCall as a last resort.\n+CustomCall documentation is provided in \\\n+[Developer details - XLA Custom Calls](https://openxla.org/xla/custom_call)\n \n ## Dot\n \n@@ -1728,6 +1728,9 @@ CPU FFT is backed by Eigen's TensorFFT. GPU FFT uses cuFFT.\n The XLA gather operation stitches together several slices (each slice at a\n potentially different runtime offset) of an input array.\n \n+See also\n+[`Gather`](https://github.com/openxla/stablehlo/blob/main/docs/spec.md#gather)\n+\n ### General Semantics\n \n See also\n@@ -1992,6 +1995,10 @@ let element_1: s32 = gettupleelement(t, 1);  // Inferred shape matches s32.\n \n See also `tf.tuple`.\n \n+**`GetTupleElement(tuple_data, index)`** | Argument | Type | Semantics |\n+| ------------ | ----- | -------------------- | | `tuple_data` | XlaOP | The\n+tuple | | `index` | int64 | Index of tuple shape |\n+\n ## Infeed\n \n See also\n@@ -2571,6 +2578,20 @@ Reshape(f32[1x1] {{5}}, {}) == 5;\n Reshape(5, {1,1}) == f32[1x1] {{5}};\n ```\n \n+See also\n+[StabloHLO reshape](https://github.com/openxla/stablehlo/blob/main/docs/spec.md#reshape)\n+\n+### Reshape (explicit)\n+\n+**`Reshape(shape, operand)`**\n+\n+Reshape op that uses an explicit target shape.\n+\n+Arguments | Type    | Semantics\n+--------- | ------- | ----------------------\n+`shape`   | `Shape` | Output shape of type T\n+`operand` | `XlaOp` | array of type T\n+\n ## Rev (reverse)\n \n See also"
        }
    ],
    "stats": {
        "total": 167,
        "additions": 94,
        "deletions": 73
    }
}