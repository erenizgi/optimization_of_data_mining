{
    "author": "tensorflower-gardener",
    "message": "[Autotuner] Add proxy FissionBackend for GPU autotuner.\n\n- It breaks up the current combined fission backend. Autotuner users can register Fission wrapping specific codegen backends as per the need.\n- This structure seemlessly fit with the autotuner, and hence is easy to discard when we don't want to do fissions.\n- The Backend creation is a bit messy, but we can create a factory method to hide the details.\n\nPiperOrigin-RevId: 827547707",
    "sha": "d912510fb4db247f219bba63956102a9fc9d870f",
    "files": [
        {
            "sha": "bc6ee6f696378893b0abb5b97226c0ccc4083902",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/BUILD",
            "status": "modified",
            "additions": 55,
            "deletions": 0,
            "changes": 55,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d912510fb4db247f219bba63956102a9fc9d870f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d912510fb4db247f219bba63956102a9fc9d870f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2FBUILD?ref=d912510fb4db247f219bba63956102a9fc9d870f",
            "patch": "@@ -787,6 +787,61 @@ cc_library(\n     ],\n )\n \n+cc_library(\n+    name = \"fission_backend\",\n+    srcs = [\"fission_backend.cc\"],\n+    hdrs = [\"fission_backend.h\"],\n+    deps = [\n+        \":gpu_codegen_backend\",\n+        \"//xla/backends/autotuner:codegen_backend\",\n+        \"//xla/hlo/analysis:symbolic_expr\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/pass:hlo_pass_pipeline\",\n+        \"//xla/service:compiler\",\n+        \"//xla/service:hlo_cost_analysis\",\n+        \"//xla/service/gpu/transforms:priority_fusion\",\n+        \"//xla/stream_executor:stream_executor_h\",\n+        \"//xla/tools:hlo_decomposer_lib\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n+    ],\n+)\n+\n+xla_test(\n+    name = \"fission_backend_test\",\n+    srcs = [\"fission_backend_test.cc\"],\n+    backends = [\"h100\"],\n+    tags = [\"cuda-only\"],\n+    deps = [\n+        \":cublas\",\n+        \":fission_backend\",\n+        \":gpu_codegen_backend\",\n+        \"//xla/backends/autotuner:codegen_backend\",\n+        \"//xla/hlo/analysis:symbolic_expr\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/pass:hlo_pass_pipeline\",\n+        \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n+        \"//xla/service:compiler\",\n+        \"//xla/service:executable\",\n+        \"//xla/service:platform_util\",\n+        \"//xla/service/gpu:nvptx_compiler_impl\",\n+        \"//xla/service/gpu/transforms:dot_algorithm_rewriter\",\n+        \"//xla/service/gpu/transforms:gemm_rewriter\",\n+        \"//xla/stream_executor:device_description\",\n+        \"//xla/stream_executor:stream_executor_h\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_googletest//:gtest_main\",\n+        \"@llvm-project//mlir:IR\",\n+    ],\n+)\n+\n xla_cc_test(\n     name = \"legacy_cache_test\",\n     srcs = [\"legacy_cache_test.cc\"],"
        },
        {
            "sha": "83e91a231fa45b31b80c2ab60c0ee73f0e6b59e0",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission_backend.cc",
            "status": "added",
            "additions": 170,
            "deletions": 0,
            "changes": 170,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d912510fb4db247f219bba63956102a9fc9d870f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d912510fb4db247f219bba63956102a9fc9d870f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend.cc?ref=d912510fb4db247f219bba63956102a9fc9d870f",
            "patch": "@@ -0,0 +1,170 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/autotuner/fission_backend.h\"\n+\n+#include <memory>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/container/flat_hash_map.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"xla/backends/autotuner/codegen_backend.h\"\n+#include \"xla/hlo/ir/hlo_casting_utils.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/hlo/pass/hlo_pass_pipeline.h\"\n+#include \"xla/service/compiler.h\"\n+#include \"xla/service/gpu/transforms/priority_fusion.h\"\n+#include \"xla/service/hlo_cost_analysis.h\"\n+#include \"xla/tools/hlo_decomposer.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+namespace xla {\n+\n+namespace gpu {\n+\n+namespace {\n+\n+// Replaces the fusion instruction with the instructions from the fissioned\n+// computation.\n+absl::Status InlineFissionedComputation(HloInstruction* fusion_instr,\n+                                        HloComputation* fissioned_computation) {\n+  absl::flat_hash_map<const HloInstruction*, HloInstruction*>\n+      cloned_instructions;\n+  HloComputation* parent_computation = fusion_instr->parent();\n+\n+  for (HloInstruction* instruction_to_clone :\n+       fissioned_computation->MakeInstructionPostOrder()) {\n+    if (instruction_to_clone->opcode() == HloOpcode::kParameter) {\n+      cloned_instructions[instruction_to_clone] = fusion_instr->mutable_operand(\n+          instruction_to_clone->parameter_number());\n+      continue;\n+    }\n+\n+    std::vector<HloInstruction*> new_operands;\n+    for (const HloInstruction* operand : instruction_to_clone->operands()) {\n+      new_operands.push_back(cloned_instructions.at(operand));\n+    }\n+    HloInstruction* new_instruction = parent_computation->AddInstruction(\n+        instruction_to_clone->CloneWithNewOperands(\n+            instruction_to_clone->shape(), new_operands));\n+    cloned_instructions[instruction_to_clone] = new_instruction;\n+  }\n+  HloInstruction* new_root =\n+      cloned_instructions.at(fissioned_computation->root_instruction());\n+  return parent_computation->ReplaceInstruction(fusion_instr, new_root);\n+}\n+\n+}  // namespace\n+\n+absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>\n+FissionBackend::GetSupportedConfigs(const HloInstruction& instr) {\n+  if (!IsSupported(instr)) {\n+    return std::vector<std::unique_ptr<BackendConfig>>();\n+  }\n+  TF_ASSIGN_OR_RETURN(std::unique_ptr<HloModule> hlo_module,\n+                      GetFissionedAndRewrittenModule(instr));\n+  TF_ASSIGN_OR_RETURN(HloInstruction * supported_instr,\n+                      FindFirstSupportedInstruction(hlo_module.get()));\n+  return codegen_backend_->GetSupportedConfigs(*supported_instr);\n+\n+  return std::vector<std::unique_ptr<BackendConfig>>();\n+}\n+\n+absl::StatusOr<std::unique_ptr<BackendConfig>> FissionBackend::GetDefaultConfig(\n+    const HloInstruction& instr) {\n+  if (!IsSupported(instr)) {\n+    return absl::InvalidArgumentError(\"Not a fusion instruction.\");\n+  }\n+  TF_ASSIGN_OR_RETURN(std::unique_ptr<HloModule> hlo_module,\n+                      GetFissionedAndRewrittenModule(instr));\n+  TF_ASSIGN_OR_RETURN(HloInstruction * supported_instr,\n+                      FindFirstSupportedInstruction(hlo_module.get()));\n+  return codegen_backend_->GetDefaultConfig(*supported_instr);\n+\n+  return absl::InvalidArgumentError(\"No supported configs found.\");\n+}\n+\n+absl::StatusOr<std::unique_ptr<HloModule>> FissionBackend::RunHloPasses(\n+    std::unique_ptr<HloModule> hlo_module,\n+    const Compiler::CompileOptions& options) {\n+  TF_ASSIGN_OR_RETURN(\n+      std::unique_ptr<HloModule> module,\n+      codegen_backend_->RunHloPasses(std::move(hlo_module), options));\n+\n+  // Run priority fusion to fuse the fissioned HLOs.\n+  HloCostAnalysis::Options priority_fusion_options;\n+  priority_fusion_options.count_multiple_input_accesses = true;\n+  // TODO: b/407494653 - Get rid of PriorityFusion.\n+  PriorityFusion priority_fusion(\n+      /*thread_pool=*/nullptr, target_config().device_description,\n+      priority_fusion_options, symbolic_expr_context_);\n+  TF_RETURN_IF_ERROR(priority_fusion.Run(module.get()).status());\n+  return module;\n+}\n+\n+absl::Status FissionBackend::ApplyConfig(HloInstruction& instr,\n+                                         const BackendConfig& config) {\n+  TF_ASSIGN_OR_RETURN(std::unique_ptr<HloModule> hlo_module,\n+                      GetFissionedAndRewrittenModule(instr));\n+  TF_ASSIGN_OR_RETURN(HloInstruction * supported_instr,\n+                      FindFirstSupportedInstruction(hlo_module.get()));\n+  TF_RETURN_IF_ERROR(codegen_backend_->ApplyConfig(*supported_instr, config));\n+  return InlineFissionedComputation(&instr, hlo_module->entry_computation());\n+}\n+\n+bool FissionBackend::IsSupported(const HloInstruction& instr) {\n+  return instr.opcode() == HloOpcode::kFusion;\n+}\n+\n+absl::StatusOr<std::unique_ptr<HloModule>>\n+FissionBackend::GetFissionedAndRewrittenModule(\n+    const HloInstruction& fusion_instr) {\n+  const auto* fusion = Cast<HloFusionInstruction>(&fusion_instr);\n+  std::unique_ptr<HloModule> hlo_module =\n+      ExtractComputationIntoNewModule(*fusion->called_computation());\n+  TF_RETURN_IF_ERROR(rewriter_pipeline_->Run(hlo_module.get()).status());\n+  return hlo_module;\n+}\n+\n+absl::StatusOr<HloInstruction*> FissionBackend::FindFirstSupportedInstruction(\n+    const HloModule* module) {\n+  std::vector<HloInstruction*> supported_instructions;\n+  for (HloComputation* computation : module->computations()) {\n+    for (HloInstruction* instruction : computation->instructions()) {\n+      if (codegen_backend_->IsSupported(*instruction)) {\n+        supported_instructions.push_back(instruction);\n+      }\n+    }\n+  }\n+  if (supported_instructions.empty()) {\n+    return absl::InvalidArgumentError(\"No supported instructions found.\");\n+  }\n+  if (supported_instructions.size() > 1) {\n+    LOG(WARNING) << \"Backend \" << name()\n+                 << \" found multiple supported instructions found. Using the \"\n+                    \"first one.\";\n+  }\n+  return supported_instructions[0];\n+}\n+\n+}  // namespace gpu\n+\n+}  // namespace xla"
        },
        {
            "sha": "d5a51633baef6a75d6703034da6d5175c06b719a",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission_backend.h",
            "status": "added",
            "additions": 88,
            "deletions": 0,
            "changes": 88,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d912510fb4db247f219bba63956102a9fc9d870f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d912510fb4db247f219bba63956102a9fc9d870f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend.h?ref=d912510fb4db247f219bba63956102a9fc9d870f",
            "patch": "@@ -0,0 +1,88 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_BACKENDS_GPU_AUTOTUNER_FISSION_BACKEND_H_\n+#define XLA_BACKENDS_GPU_AUTOTUNER_FISSION_BACKEND_H_\n+\n+#include <memory>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"xla/backends/autotuner/codegen_backend.h\"\n+#include \"xla/backends/gpu/autotuner/gpu_codegen_backend.h\"\n+#include \"xla/hlo/analysis/symbolic_expr.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/pass/hlo_pass_pipeline.h\"\n+#include \"xla/service/compiler.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n+\n+namespace xla::gpu {\n+\n+// A proxy backend that wraps an actual codegen backend. The `rewriter_pipeline`\n+// is used to transform unfused instructions to retarget them for the underlying\n+// codegen backend.\n+// For the get/apply config operations, the proxy backend only operates on the\n+// *first* supported instruction by the underlying backend, found in the unfused\n+// and transmormed HLO.\n+// The assumption is that there is only one operation of interest in the fusion\n+// (e.g., a 'dot' in a gemm fusion).\n+class FissionBackend : public GpuCodegenBackend {\n+ public:\n+  FissionBackend(const DebugOptions* debug_options, Compiler* compiler,\n+                 const Compiler::TargetConfig* target_config,\n+                 std::unique_ptr<GpuCodegenBackend> backend,\n+                 std::unique_ptr<HloPassPipeline> rewriter_pipeline,\n+                 SymbolicExprContext* symbolic_expr_context,\n+                 stream_executor::StreamExecutor* stream_executor = nullptr)\n+      : GpuCodegenBackend(absl::StrCat(backend->name(), \"_fission\"),\n+                          debug_options, compiler, target_config,\n+                          stream_executor),\n+        rewriter_pipeline_(std::move(rewriter_pipeline)),\n+        codegen_backend_(std::move(backend)),\n+        symbolic_expr_context_(symbolic_expr_context) {}\n+  ~FissionBackend() override = default;\n+\n+  absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>\n+  GetSupportedConfigs(const HloInstruction& instr) override;\n+\n+  absl::StatusOr<std::unique_ptr<BackendConfig>> GetDefaultConfig(\n+      const HloInstruction& instr) override;\n+\n+  absl::StatusOr<std::unique_ptr<HloModule>> RunHloPasses(\n+      std::unique_ptr<HloModule> hlo_module,\n+      const Compiler::CompileOptions& options) override;\n+\n+  absl::Status ApplyConfig(HloInstruction& instr,\n+                           const BackendConfig& config) override;\n+\n+  bool IsSupported(const HloInstruction& instr) override;\n+\n+ private:\n+  absl::StatusOr<std::unique_ptr<HloModule>> GetFissionedAndRewrittenModule(\n+      const HloInstruction& fusion_instr);\n+  absl::StatusOr<HloInstruction*> FindFirstSupportedInstruction(\n+      const HloModule* module);\n+\n+  std::unique_ptr<HloPassPipeline> rewriter_pipeline_;\n+  std::unique_ptr<GpuCodegenBackend> codegen_backend_;\n+  SymbolicExprContext* symbolic_expr_context_;\n+};\n+\n+}  // namespace xla::gpu\n+\n+#endif  // XLA_BACKENDS_GPU_AUTOTUNER_FISSION_BACKEND_H_"
        },
        {
            "sha": "19a47eed06fec0f2a0313015506a11025506cafa",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/fission_backend_test.cc",
            "status": "added",
            "additions": 164,
            "deletions": 0,
            "changes": 164,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/d912510fb4db247f219bba63956102a9fc9d870f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/d912510fb4db247f219bba63956102a9fc9d870f/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Ffission_backend_test.cc?ref=d912510fb4db247f219bba63956102a9fc9d870f",
            "patch": "@@ -0,0 +1,164 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/backends/gpu/autotuner/fission_backend.h\"\n+\n+#include <memory>\n+#include <string>\n+#include <utility>\n+#include <vector>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"absl/status/status_matchers.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n+#include \"xla/backends/autotuner/codegen_backend.h\"\n+#include \"xla/backends/gpu/autotuner/cublas.h\"\n+#include \"xla/backends/gpu/autotuner/gpu_codegen_backend.h\"\n+#include \"xla/hlo/analysis/symbolic_expr.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/hlo/pass/hlo_pass_pipeline.h\"\n+#include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n+#include \"xla/service/compiler.h\"\n+#include \"xla/service/executable.h\"\n+#include \"xla/service/gpu/nvptx_compiler.h\"\n+#include \"xla/service/gpu/transforms/dot_algorithm_rewriter.h\"\n+#include \"xla/service/gpu/transforms/gemm_rewriter.h\"\n+#include \"xla/service/platform_util.h\"\n+#include \"xla/stream_executor/device_description.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+namespace xla {\n+namespace gpu {\n+\n+namespace {\n+\n+using absl_testing::IsOk;\n+using absl_testing::IsOkAndHolds;\n+using ::testing::HasSubstr;\n+\n+const char kTritonFusionHlo[] = R\"(\n+  HloModule module\n+\n+  computation {\n+    p0 = bf16[1024,1024]{1,0} parameter(0)\n+    convert0 = f32[1024,1024]{1,0} convert(p0)\n+    p1 = s8[1024,1024]{1,0} parameter(1)\n+    convert1 = f32[1024,1024]{1,0} convert(p1)\n+    ROOT dot = f32[1024,1024]{1,0} dot(convert0, convert1),\n+        lhs_contracting_dims={1}, rhs_contracting_dims={0}\n+  }\n+\n+  ENTRY main {\n+    p0 = bf16[1024,1024]{1,0} parameter(0)\n+    p1 = s8[1024,1024]{1,0} parameter(1)\n+    ROOT fusion = f32[1024,1024]{1,0} fusion(p0, p1),\n+      kind=kCustom, calls=computation,\n+      backend_config={\"fusion_backend_config\":{\"kind\":\"__triton_gemm\"}}\n+  })\";\n+\n+class CublasFissionTest : public HloHardwareIndependentTestBase {\n+ protected:\n+  DebugOptions debug_options_;\n+  NVPTXCompiler compiler_;\n+  se::StreamExecutor* stream_executor_;\n+  Compiler::TargetConfig target_config_;\n+  se::DeviceDescription device_description_;\n+  std::unique_ptr<HloPassPipeline> rewriter_pipeline_;\n+  std::unique_ptr<GpuCodegenBackend> cublas_backend_;\n+  std::unique_ptr<FissionBackend> fission_backend_;\n+  mlir::MLIRContext mlir_context_;\n+  SymbolicExprContext symbolic_expr_context_{&mlir_context_};\n+\n+  std::unique_ptr<HloPassPipeline> GetCublasRewriterPipeline() {\n+    auto pipeline = std::make_unique<HloPassPipeline>(\"fission_pipeline\");\n+    pipeline->AddPass(std::make_unique<DotAlgorithmRewriter>());\n+    for (GemmRewriterOptions::DType dtype :\n+         {GemmRewriterOptions::DType::kFp8Only,\n+          GemmRewriterOptions::DType::kNonFp8Only}) {\n+      auto gemm_rewriter = std::make_unique<GemmRewriter>(\n+          device_description_.gpu_compute_capability(),\n+          device_description_.runtime_version(), GemmRewriterOptions{dtype});\n+      pipeline->AddPass(std::move(gemm_rewriter));\n+    }\n+    return pipeline;\n+  }\n+\n+  CublasFissionTest()\n+      : stream_executor_(PlatformUtil::GetDefaultPlatform()\n+                             .value()\n+                             ->ExecutorForDevice(0)\n+                             .value()),\n+        target_config_(stream_executor_),\n+        device_description_(stream_executor_->GetDeviceDescription()),\n+        rewriter_pipeline_(GetCublasRewriterPipeline()),\n+        cublas_backend_(std::make_unique<CublasBackend>(\n+            stream_executor_, &debug_options_, &compiler_, &target_config_)),\n+        fission_backend_(std::make_unique<FissionBackend>(\n+            &debug_options_, &compiler_, &target_config_,\n+            std::move(cublas_backend_), std::move(rewriter_pipeline_),\n+            &symbolic_expr_context_, stream_executor_)) {}\n+};\n+\n+TEST_F(CublasFissionTest, CanCreateFissionBackend) {\n+  EXPECT_EQ(fission_backend_->name(), \"Cublas_fission\");\n+}\n+\n+TEST_F(CublasFissionTest, GetSupportedConfigs) {\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(kTritonFusionHlo));\n+  absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>> configs =\n+      fission_backend_->GetSupportedConfigs(\n+          (*module->entry_computation()->root_instruction()));\n+  EXPECT_THAT(configs, IsOkAndHolds(testing::SizeIs(1)));\n+}\n+\n+TEST_F(CublasFissionTest, GetDefaultConfig) {\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(kTritonFusionHlo));\n+  HloInstruction* fusion = module->entry_computation()->root_instruction();\n+  EXPECT_THAT(fission_backend_->GetDefaultConfig(*fusion), IsOk());\n+}\n+\n+TEST_F(CublasFissionTest, Compile) {\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(kTritonFusionHlo));\n+  HloInstruction* fusion = module->entry_computation()->root_instruction();\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<BackendConfig> config,\n+                          fission_backend_->GetDefaultConfig(*fusion));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Executable> executable,\n+                          fission_backend_->Compile(*fusion, *config));\n+  EXPECT_NE(executable, nullptr);\n+}\n+\n+TEST_F(CublasFissionTest, ApplyConfig) {\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,\n+                          ParseAndReturnVerifiedModule(kTritonFusionHlo));\n+  HloInstruction* fusion = module->entry_computation()->root_instruction();\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<BackendConfig> config,\n+                          fission_backend_->GetDefaultConfig(*fusion));\n+  EXPECT_THAT(fission_backend_->ApplyConfig(*fusion, *config), IsOk());\n+  std::string module_str = module->ToString();\n+  EXPECT_THAT(module_str, HasSubstr(\"custom_call_target=\\\"__cublas$gemm\\\"\"));\n+  EXPECT_THAT(module_str, HasSubstr(\"\\\"selected_algorithm\\\":\\\"-1\\\"\"));\n+}\n+\n+}  // namespace\n+}  // namespace gpu\n+}  // namespace xla"
        }
    ],
    "stats": {
        "total": 477,
        "additions": 477,
        "deletions": 0
    }
}