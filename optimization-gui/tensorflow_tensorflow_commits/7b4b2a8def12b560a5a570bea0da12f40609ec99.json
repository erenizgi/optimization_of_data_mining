{
    "author": "tensorflower-gardener",
    "message": "Add GitHub Actions job for 8xH100 GPUs.\n\nPiperOrigin-RevId: 842571530",
    "sha": "7b4b2a8def12b560a5a570bea0da12f40609ec99",
    "files": [
        {
            "sha": "4171626436f600eb1bb7a963506cfe3521627971",
            "filename": "third_party/xla/.github/workflows/ci_multi_device.yml",
            "status": "added",
            "additions": 64,
            "deletions": 0,
            "changes": 64,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b4b2a8def12b560a5a570bea0da12f40609ec99/third_party%2Fxla%2F.github%2Fworkflows%2Fci_multi_device.yml",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b4b2a8def12b560a5a570bea0da12f40609ec99/third_party%2Fxla%2F.github%2Fworkflows%2Fci_multi_device.yml",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2F.github%2Fworkflows%2Fci_multi_device.yml?ref=7b4b2a8def12b560a5a570bea0da12f40609ec99",
            "patch": "@@ -0,0 +1,64 @@\n+# Copyright 2025 The OpenXLA Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ============================================================================\n+name: Multi-Device CI\n+permissions:\n+  contents: read\n+on:\n+  workflow_dispatch:  # Allows manual triggering\n+\n+jobs:\n+  Tests:\n+    strategy:\n+      # Don't fail fast - want to see results for all builds even if one fails.\n+      fail-fast: false\n+      matrix:\n+        job_info: [\n+          {\n+            pool: \"linux-x86-a3-8g-h100-8gpu\",\n+            container: \"us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build:latest\",\n+            name: \"XLA Linux x86 GPU 8xH100\",\n+            repo: \"openxla/xla\",\n+          },\n+        ]\n+    name: ${{ matrix.job_info.name }}\n+    runs-on: ${{ matrix.job_info.pool }}\n+    container: ${{ matrix.job_info.container }}\n+    defaults:\n+      run:\n+        shell: bash\n+    timeout-minutes: 60\n+    steps:\n+      - name: \"Checking out openxla/xla\"\n+        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0\n+        with:\n+          path: \"openxla/xla\"\n+      - name: Checking out ${{ matrix.job_info.repo }}\n+        if: ${{ matrix.job_info.repo != 'openxla/xla' }}\n+        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0\n+        with:\n+          repository: ${{ matrix.job_info.repo }}\n+          path: ${{ matrix.job_info.repo }}\n+      - name: \"Wait For Connection\"\n+        uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n+        with:\n+          halt-dispatch-input: ${{ inputs.halt-for-connection }}\n+      - name: \"Run build.py\"\n+        working-directory: ${{ matrix.job_info.repo }}\n+        run: |\n+          if [[ \"${{ matrix.job_info.pool }}\" == *windows* ]]; then\n+            python $GITHUB_WORKSPACE\\\\openxla\\\\xla\\\\build_tools\\\\ci\\\\build.py --build=\"${{ matrix.job_info.name }}_github_actions\"\n+          else\n+            $GITHUB_WORKSPACE/openxla/xla/build_tools/ci/build.py --build=\"${{ matrix.job_info.name }}_github_actions\"\n+          fi"
        },
        {
            "sha": "bf4c2071dce9ce87a26835a4f0792f0370427886",
            "filename": "third_party/xla/build_tools/ci/build.py",
            "status": "modified",
            "additions": 30,
            "deletions": 15,
            "changes": 45,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b4b2a8def12b560a5a570bea0da12f40609ec99/third_party%2Fxla%2Fbuild_tools%2Fci%2Fbuild.py",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b4b2a8def12b560a5a570bea0da12f40609ec99/third_party%2Fxla%2Fbuild_tools%2Fci%2Fbuild.py",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fbuild_tools%2Fci%2Fbuild.py?ref=7b4b2a8def12b560a5a570bea0da12f40609ec99",
            "patch": "@@ -111,6 +111,7 @@ class BuildType(enum.Enum):\n   XLA_LINUX_X86_CPU_BZLMOD_GITHUB_ACTIONS = enum.auto()\n   XLA_LINUX_ARM64_CPU_GITHUB_ACTIONS = enum.auto()\n   XLA_LINUX_X86_GPU_L4_GITHUB_ACTIONS = enum.auto()\n+  XLA_LINUX_X86_GPU_8X_H100_GITHUB_ACTIONS = enum.auto()\n   XLA_LINUX_X86_GPU_ONEAPI_GITHUB_ACTIONS = enum.auto()\n \n   # Presubmit builds for regression testing.\n@@ -281,23 +282,29 @@ def _tag_filters_for_compute_capability(\n     \"-oneapi-only\",\n )\n \n-single_nvidia_gpu_filters = nvidia_gpu_filters + (\"-multi_gpu\",)\n+nvidia_single_gpu_filters = nvidia_gpu_filters + (\"-multi_gpu\",)\n+\n+nvidia_only_multi_gpu_filters = nvidia_gpu_filters + (\"multi_gpu\",)\n \n \n def nvidia_gpu_build_with_compute_capability(\n     *,\n     type_: BuildType,\n     configs: Tuple[str, ...],\n     compute_capability: int,\n+    multi_gpu: bool = False,\n ) -> Build:\n   extra_gpu_tags = _tag_filters_for_compute_capability(compute_capability)\n+  filter_tags = (\n+      nvidia_only_multi_gpu_filters if multi_gpu else nvidia_single_gpu_filters\n+  )\n   return Build(\n       type_=type_,\n       repo=\"openxla/xla\",\n       target_patterns=_XLA_DEFAULT_TARGET_PATTERNS,\n       configs=configs,\n-      test_tag_filters=single_nvidia_gpu_filters + extra_gpu_tags,\n-      build_tag_filters=single_nvidia_gpu_filters,\n+      test_tag_filters=filter_tags + extra_gpu_tags,\n+      build_tag_filters=filter_tags,\n       options={\n           \"run_under\": \"//build_tools/ci:parallel_gpu_execute\",\n           \"//xla/tsl:ci_build\": True,\n@@ -434,6 +441,14 @@ def nvidia_gpu_build_with_compute_capability(\n     type_=BuildType.XLA_LINUX_X86_GPU_L4_GITHUB_ACTIONS,\n     configs=(\"warnings\", \"rbe_linux_cuda_nvcc\", \"hermetic_cuda_umd\"),\n     compute_capability=75,\n+    multi_gpu=False,\n+)\n+\n+nvidia_gpu_build_with_compute_capability(\n+    type_=BuildType.XLA_LINUX_X86_GPU_8X_H100_GITHUB_ACTIONS,\n+    configs=(\"warnings\", \"rbe_linux_cuda_nvcc\", \"hermetic_cuda_umd\"),\n+    compute_capability=90,\n+    multi_gpu=True,\n )\n \n oneapi_build_tag_filter = (\n@@ -508,9 +523,9 @@ def nvidia_gpu_build_with_compute_capability(\n     repo=\"openxla/xla\",\n     target_patterns=_XLA_GPU_PRESUBMIT_BENCHMARKS_DEFAULT_TARGET_PATTERNS,\n     configs=(\"warnings\", \"rbe_linux_cuda_nvcc\", \"hermetic_cuda_umd\"),\n-    test_tag_filters=single_nvidia_gpu_filters\n+    test_tag_filters=nvidia_single_gpu_filters\n     + _tag_filters_for_compute_capability(compute_capability=75),\n-    build_tag_filters=single_nvidia_gpu_filters,\n+    build_tag_filters=nvidia_single_gpu_filters,\n     options={\n         \"run_under\": \"//build_tools/ci:parallel_gpu_execute\",\n         \"//xla/tsl:ci_build\": True,\n@@ -528,9 +543,9 @@ def nvidia_gpu_build_with_compute_capability(\n     repo=\"openxla/xla\",\n     target_patterns=_XLA_GPU_PRESUBMIT_BENCHMARKS_DEFAULT_TARGET_PATTERNS,\n     configs=(\"warnings\", \"rbe_linux_cuda_nvcc\", \"hermetic_cuda_umd\"),\n-    test_tag_filters=single_nvidia_gpu_filters\n+    test_tag_filters=nvidia_single_gpu_filters\n     + _tag_filters_for_compute_capability(compute_capability=75),\n-    build_tag_filters=single_nvidia_gpu_filters,\n+    build_tag_filters=nvidia_single_gpu_filters,\n     options={\n         \"run_under\": \"//build_tools/ci:parallel_gpu_execute\",\n         \"//xla/tsl:ci_build\": True,\n@@ -549,9 +564,9 @@ def nvidia_gpu_build_with_compute_capability(\n     repo=\"openxla/xla\",\n     configs=(\"warnings\", \"rbe_linux_cuda_nvcc\", \"hermetic_cuda_umd\"),\n     target_patterns=_XLA_GPU_PRESUBMIT_BENCHMARKS_DEFAULT_TARGET_PATTERNS,\n-    test_tag_filters=single_nvidia_gpu_filters\n+    test_tag_filters=nvidia_single_gpu_filters\n     + _tag_filters_for_compute_capability(compute_capability=75),\n-    build_tag_filters=single_nvidia_gpu_filters,\n+    build_tag_filters=nvidia_single_gpu_filters,\n     options={\n         \"run_under\": \"//build_tools/ci:parallel_gpu_execute\",\n         \"//xla/tsl:ci_build\": True,\n@@ -569,9 +584,9 @@ def nvidia_gpu_build_with_compute_capability(\n     repo=\"openxla/xla\",\n     configs=(\"warnings\", \"rbe_linux_cuda_nvcc\", \"hermetic_cuda_umd\"),\n     target_patterns=_XLA_GPU_PRESUBMIT_BENCHMARKS_DEFAULT_TARGET_PATTERNS,\n-    test_tag_filters=single_nvidia_gpu_filters\n+    test_tag_filters=nvidia_single_gpu_filters\n     + _tag_filters_for_compute_capability(compute_capability=75),\n-    build_tag_filters=single_nvidia_gpu_filters,\n+    build_tag_filters=nvidia_single_gpu_filters,\n     options={\n         \"run_under\": \"//build_tools/ci:parallel_gpu_execute\",\n         \"//xla/tsl:ci_build\": True,\n@@ -590,9 +605,9 @@ def nvidia_gpu_build_with_compute_capability(\n     repo=\"openxla/xla\",\n     configs=(),\n     target_patterns=_XLA_GPU_PRESUBMIT_BENCHMARKS_DEFAULT_TARGET_PATTERNS,\n-    test_tag_filters=single_nvidia_gpu_filters\n+    test_tag_filters=nvidia_single_gpu_filters\n     + _tag_filters_for_compute_capability(compute_capability=100),\n-    build_tag_filters=single_nvidia_gpu_filters,\n+    build_tag_filters=nvidia_single_gpu_filters,\n     options={\n         \"run_under\": \"//build_tools/ci:parallel_gpu_execute\",\n         # Use User Mode and Kernel Mode Drivers pre-installed on the system.\n@@ -613,9 +628,9 @@ def nvidia_gpu_build_with_compute_capability(\n     repo=\"openxla/xla\",\n     configs=(),\n     target_patterns=_XLA_GPU_PRESUBMIT_BENCHMARKS_DEFAULT_TARGET_PATTERNS,\n-    test_tag_filters=single_nvidia_gpu_filters\n+    test_tag_filters=nvidia_single_gpu_filters\n     + _tag_filters_for_compute_capability(compute_capability=100),\n-    build_tag_filters=single_nvidia_gpu_filters,\n+    build_tag_filters=nvidia_single_gpu_filters,\n     options={\n         \"run_under\": \"//build_tools/ci:parallel_gpu_execute\",\n         # Use User Mode and Kernel Mode Drivers pre-installed on the system."
        },
        {
            "sha": "3510dd02708fe8c3255b542a6dc3c28f818956b0",
            "filename": "third_party/xla/build_tools/ci/golden_commands.txt",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/7b4b2a8def12b560a5a570bea0da12f40609ec99/third_party%2Fxla%2Fbuild_tools%2Fci%2Fgolden_commands.txt",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/7b4b2a8def12b560a5a570bea0da12f40609ec99/third_party%2Fxla%2Fbuild_tools%2Fci%2Fgolden_commands.txt",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fbuild_tools%2Fci%2Fgolden_commands.txt?ref=7b4b2a8def12b560a5a570bea0da12f40609ec99",
            "patch": "@@ -53,6 +53,12 @@ parallel --ungroup --retries 3 --delay 15 --nonall -- bazel build --build_tag_fi\n bazel test --build_tag_filters=-no_oss,-gpu,-requires-gpu-nvidia,-requires-gpu-amd,-requires-gpu-intel --test_tag_filters=-no_oss,-gpu,-requires-gpu-nvidia,-requires-gpu-amd,-requires-gpu-intel --config=warnings --config=nonccl --config=rbe_linux_cpu --color=yes --test_output=errors --verbose_failures --keep_going --nobuild_tests_only --profile=profile.json.gz --flaky_test_attempts=3 --jobs=150 --bes_upload_mode=fully_async --//xla/tsl:ci_build -- //xla/... //build_tools/... @local_tsl//tsl/...\n bazel analyze-profile profile.json.gz\n # END BuildType.XLA_LINUX_X86_CPU_GITHUB_ACTIONS\n+# BEGIN BuildType.XLA_LINUX_X86_GPU_8X_H100_GITHUB_ACTIONS\n+nvidia-smi\n+parallel --ungroup --retries 3 --delay 15 --nonall -- bazel build --build_tag_filters=-no_oss,requires-gpu-nvidia,gpu,-rocm-only,-oneapi-only,multi_gpu --test_tag_filters=-no_oss,requires-gpu-nvidia,gpu,-rocm-only,-oneapi-only,multi_gpu,requires-gpu-sm90-only,requires-gpu-sm60,requires-gpu-sm70,requires-gpu-sm80,requires-gpu-sm90,-requires-gpu-sm100,-requires-gpu-sm100-only,-requires-gpu-amd,-requires-gpu-intel --config=warnings --config=rbe_linux_cuda_nvcc --config=hermetic_cuda_umd --repo_env=TF_CUDA_COMPUTE_CAPABILITIES=9.0 --run_under=//build_tools/ci:parallel_gpu_execute --//xla/tsl:ci_build --color=yes --test_output=errors --verbose_failures --keep_going --nobuild_tests_only --profile=profile.json.gz --flaky_test_attempts=3 --jobs=150 --bes_upload_mode=fully_async --nobuild -- //xla/... //build_tools/... @local_tsl//tsl/...\n+bazel test --build_tag_filters=-no_oss,requires-gpu-nvidia,gpu,-rocm-only,-oneapi-only,multi_gpu --test_tag_filters=-no_oss,requires-gpu-nvidia,gpu,-rocm-only,-oneapi-only,multi_gpu,requires-gpu-sm90-only,requires-gpu-sm60,requires-gpu-sm70,requires-gpu-sm80,requires-gpu-sm90,-requires-gpu-sm100,-requires-gpu-sm100-only,-requires-gpu-amd,-requires-gpu-intel --config=warnings --config=rbe_linux_cuda_nvcc --config=hermetic_cuda_umd --repo_env=TF_CUDA_COMPUTE_CAPABILITIES=9.0 --run_under=//build_tools/ci:parallel_gpu_execute --//xla/tsl:ci_build --color=yes --test_output=errors --verbose_failures --keep_going --nobuild_tests_only --profile=profile.json.gz --flaky_test_attempts=3 --jobs=150 --bes_upload_mode=fully_async -- //xla/... //build_tools/... @local_tsl//tsl/...\n+bazel analyze-profile profile.json.gz\n+# END BuildType.XLA_LINUX_X86_GPU_8X_H100_GITHUB_ACTIONS\n # BEGIN BuildType.XLA_LINUX_X86_GPU_A4_224_VCPU_BENCHMARK_PRESUBMIT_GITHUB_ACTIONS\n nvidia-smi\n parallel --ungroup --retries 3 --delay 15 --nonall -- bazel build --build_tag_filters=-no_oss,requires-gpu-nvidia,gpu,-rocm-only,-oneapi-only,-multi_gpu --test_tag_filters=-no_oss,requires-gpu-nvidia,gpu,-rocm-only,-oneapi-only,-multi_gpu,requires-gpu-sm100-only,requires-gpu-sm60,requires-gpu-sm70,requires-gpu-sm80,requires-gpu-sm90,requires-gpu-sm100,-requires-gpu-amd,-requires-gpu-intel --repo_env=TF_CUDA_COMPUTE_CAPABILITIES=10 --repo_env=HERMETIC_CUDA_VERSION=12.8.0 --repo_env=HERMETIC_CUDNN_VERSION=9.8.0 --run_under=//build_tools/ci:parallel_gpu_execute --//xla/tsl:ci_build --@local_config_cuda//cuda:include_cuda_libs=False --color=yes --test_output=errors --verbose_failures --keep_going --nobuild_tests_only --profile=profile.json.gz --flaky_test_attempts=3 --jobs=150 --bes_upload_mode=fully_async --nobuild -- //xla/tools/multihost_hlo_runner:hlo_runner_main_gpu //xla/tools:compute_xspace_stats_main_gpu"
        }
    ],
    "stats": {
        "total": 115,
        "additions": 100,
        "deletions": 15
    }
}