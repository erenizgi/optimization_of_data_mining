{
    "author": "unknown",
    "message": "PR #30932: [DOC] Update to operation_semantics\n\nImported from GitHub PR https://github.com/openxla/xla/pull/30932\n\n## Summary of Changes\n\n- **`CompositeCall`**\n   - Added information on decomposition\n   - Linked to StableHLO Spec\n\n- **`Concatenate`**\n   - reformatted title from `Concatenate` to `ConcatInDim (Concatenate)`\n\n- **`Conv`**\n   - Removed `ConvWithGeneralPadding` as level 2 heading\n   - Restructured description of `Conv` as the base case\n   - Appended `ConvWithGeneralPadding`, `ConvWithGeneralDimensions`, `ConvGeneral`, `ConvGeneralDilated` as level 3 headings to `Conv`\n          - Created declarations for each\n          - Created table for each\n          - Add brief description to each\n          - Added hot link to `Conv` to each\n   - Added description of `precision_config`\n   - Added description of `preferred_element_type`\n\n- **`Element-wise binary arithmetic operations`**\n   - Reformatted Table\n\n- **`OptimizationBarrier `**\n   - Linked to StableHLO Spec\n\n- **`Reduce `**\n   - Updated `dimensions` to `dimensions_to_reduce`\n\n- **`Tuple `**\n   -  Linked to StableHLO spec\n   - Added note for nuance difference between HLO and StableHLO\n\nðŸŽ¯ Justification\nDocumentation update\n\nðŸš€ Kind of Contribution\nðŸ“š Documentation\n\nCopybara import of the project:\n\n--\ndcc9a8cac2c2e0839335860458e05faac1cde172 by Amelia Thurdekoos <athurdekoos@quansight.com>:\n\nUpdates to operation_semantics\n\n--\n318df02b65d5f23809a73a6ac0c30683ab37294e by Amelia Thurdekoos <athurdekoos@quansight.com>:\n\nUpdates to operation_semantics\n\nMerging this change closes #30932\n\nPiperOrigin-RevId: 802781382",
    "sha": "80969688d27d0a181775a656b9311fca5cee9b8c",
    "files": [
        {
            "sha": "41f74de21edbaac64d3a2b97e95f0d54be375d74",
            "filename": "third_party/xla/docs/operation_semantics.md",
            "status": "modified",
            "additions": 216,
            "deletions": 30,
            "changes": 246,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/80969688d27d0a181775a656b9311fca5cee9b8c/third_party%2Fxla%2Fdocs%2Foperation_semantics.md",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/80969688d27d0a181775a656b9311fca5cee9b8c/third_party%2Fxla%2Fdocs%2Foperation_semantics.md",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fdocs%2Foperation_semantics.md?ref=80969688d27d0a181775a656b9311fca5cee9b8c",
            "patch": "@@ -508,6 +508,13 @@ frontend_attributes = {\n | `decomposition`             | `XlaComputation`       | computation of type `T_0, T_1, ..., T_{N-1} -> S` with N parameters of arbitrary type |\n | `version`                   | `int64`.               | number to version updates to semantics of the composite op                            |\n \n+An opâ€™s `decomposition` isnâ€™t a field called, but instead appears as a to_apply\n+attribute that points to the function which contains the lower-level\n+implementation, i.e. `to_apply=%funcname`\n+\n+More information on composite and decomposition can be found on\n+[StableHLO Specification](https://openxla.org/stablehlo/spec#composite)\n+\n ## Cholesky\n \n See also\n@@ -657,7 +664,7 @@ Note that there are the following restrictions on the `source_target_pair`:\n -   If a replica id is not a target in any pair, then the output on that replica\n     is a tensor consisting of 0(s) with the same shape as the input.\n \n-## Concatenate\n+## ConcatInDim (Concatenate)\n \n See also\n [`XlaBuilder::ConcatInDim`](https://github.com/openxla/xla/tree/main/xla/hlo/builder/xla_builder.h).\n@@ -765,35 +772,46 @@ type of the returned value of each `branch_computations[b]` must be the same.\n Note that only one of the `branch_computations` will be executed depending on\n the value of `branch_index`.\n \n-## Conv (convolution)\n+## Conv (Convolution)\n \n See also\n [`XlaBuilder::Conv`](https://github.com/openxla/xla/tree/main/xla/hlo/builder/xla_builder.h).\n \n-As ConvWithGeneralPadding, but the padding is specified in a short-hand way as\n-either SAME or VALID. SAME padding pads the input (`lhs`) with zeroes so that\n-the output has the same shape as the input when not taking striding into\n-account. VALID padding simply means no padding.\n-\n-## ConvWithGeneralPadding (convolution)\n-\n-See also\n-[`XlaBuilder::ConvWithGeneralPadding`](https://github.com/openxla/xla/tree/main/xla/hlo/builder/xla_builder.h).\n-\n Computes a convolution of the kind used in neural networks. Here, a convolution\n can be thought of as a n-dimensional window moving across a n-dimensional base\n area and a computation is performed for each possible position of the window.\n \n-| Arguments             | Type                     | Semantics                |\n-| --------------------- | ------------------------ | ------------------------ |\n-| `lhs`                 | `XlaOp`                  | (n+2)-dimensional array of inputs |\n-| `rhs`                 | `XlaOp`                  | (n+2)-dimensional array of kernel weights |\n-| `window_strides`      | `ArraySlice<int64>`      | n-d array of kernel strides |\n-| `padding`             | `ArraySlice< pair<int64,int64>>` | n-d array of (low, high) padding |\n-| `lhs_dilation`        | `ArraySlice<int64>`      | n-d lhs dilation factor array |\n-| `rhs_dilation`        | `ArraySlice<int64>`      | n-d rhs dilation factor array |\n-| `feature_group_count` | int64                    | the number of feature groups |\n-| `batch_group_count`   | int64                    | the number of batch groups |\n+`Conv` Enqueues a convolution instruction onto the computation, which uses the\n+default convolution dimension numbers with no dilation.\n+\n+The padding is specified in a short-hand way as either SAME or VALID. SAME\n+padding pads the input (`lhs`) with zeroes so that the output has the same shape\n+as the input when not taking striding into account. VALID padding simply means\n+no padding.\n+\n+**`Conv(lhs, rhs, window_strides, padding, feature_group_count,\n+batch_group_count, precision_config, preferred_element_type)`**\n+\n+| Arguments                | Type                | Semantics                   |\n+| ------------------------ | ------------------- | --------------------------- |\n+| `lhs`                    | `XlaOp`             | (n+2)-dimensional array of  |\n+:                          :                     : inputs                      :\n+| `rhs`                    | `XlaOp`             | (n+2)-dimensional array of  |\n+:                          :                     : kernel weights              :\n+| `window_strides`         | `ArraySlice<int64>` | n-d array of kernel strides |\n+| `padding`                | `Padding`           | enum of padding             |\n+| `feature_group_count`    | int64               | the number of feature       |\n+:                          :                     : groups                      :\n+| `batch_group_count`      | int64               | the number of batch groups  |\n+| `precision_config`       | optional            | enum for level of precision |\n+:                          : `PrecisionConfig`   :                             :\n+| `preferred_element_type` | optional            | enum of scalar element type |\n+:                          : `PrimitiveType`     :                             :\n+\n+Increasing levels of controls are available for `Conv`: -\n+[ConvWithGeneralPadding](#ConvWithGeneralPadding) -\n+[ConvWithGeneralDimensions](#ConvWithGeneralDimensions) -\n+[ConvGeneral](#ConvGeneral) - [ConvGeneralDilated](#convgeneraldilated)\n \n Let n be the number of spatial dimensions. The `lhs` argument is an\n (n+2)-dimensional array describing the base area. This is called the input,\n@@ -921,6 +939,158 @@ for (b, oz, oy, ox) {  // output coordinates\n }\n ```\n \n+`precision_config` is used to indicate the precision configuration. The level\n+dictates whether hardware should attempt to generate more machine code\n+instructions to provide more accurate dtype emulation when needed (i.e.\n+emulating f32 on a TPU that only supports bf16 matmuls). Values may be\n+`DEFAULT`, `HIGH`, `HIGHEST`. Additional details\n+[in the MXU sections](https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus).\n+\n+`preferred_element_type` is a scalar element of higher/lower precision output\n+types used for accumulation. `preferred_element_type` recommends the\n+accumulation type for the given operaiton, however it is not guaranteed. This\n+allows for some hardware backends to instead accumulate in a different type and\n+convert to the preferred output type.\n+\n+### ConvWithGeneralPadding\n+\n+**`ConvWithGeneralPadding(lhs, rhs, window_strides, padding,\n+feature_group_count, batch_group_count, precision_config,\n+preferred_element_type)`**\n+\n+See also\n+[`XlaBuilder::ConvWithGeneralPadding`](https://github.com/openxla/xla/tree/main/xla/hlo/builder/xla_builder.h).\n+\n+Same as [`Conv`](#conv-convolution) where padding configuration is explicit.\n+\n+| Arguments                | Type                | Semantics                   |\n+| ------------------------ | ------------------- | --------------------------- |\n+| `lhs`                    | `XlaOp`             | (n+2)-dimensional array of  |\n+:                          :                     : inputs                      :\n+| `rhs`                    | `XlaOp`             | (n+2)-dimensional array of  |\n+:                          :                     : kernel weights              :\n+| `window_strides`         | `ArraySlice<int64>` | n-d array of kernel strides |\n+| `padding`                | `ArraySlice<        | n-d array of (low, high)    |\n+:                          : pair<int64,int64>>` : padding                     :\n+| `feature_group_count`    | int64               | the number of feature       |\n+:                          :                     : groups                      :\n+| `batch_group_count`      | int64               | the number of batch groups  |\n+| `precision_config`       | optional            | enum for level of precision |\n+:                          : `PrecisionConfig`   :                             :\n+| `preferred_element_type` | optional            | enum of scalar element type |\n+:                          : `PrimitiveType`     :                             :\n+\n+### ConvWithGeneralDimensions\n+\n+**`ConvWithGeneralDimensions(lhs, rhs, window_strides, padding,\n+dimension_numbers, feature_group_count, batch_group_count, precision_config,\n+preferred_element_type)`**\n+\n+See also\n+[`XlaBuilder::ConvWithGeneralDimensions`](https://github.com/openxla/xla/tree/main/xla/hlo/builder/xla_builder.h).\n+\n+Same as [`Conv`](#conv-convolution) where dimension numbers are explicit.\n+\n+| Arguments                | Type                          | Semantics         |\n+| ------------------------ | ----------------------------- | ----------------- |\n+| `lhs`                    | `XlaOp`                       | (n+2)-dimensional |\n+:                          :                               : array of inputs   :\n+| `rhs`                    | `XlaOp`                       | (n+2)-dimensional |\n+:                          :                               : array of kernel   :\n+:                          :                               : weights           :\n+| `window_strides`         | `ArraySlice<int64>`           | n-d array of      |\n+:                          :                               : kernel strides    :\n+| `padding`                | `Padding`                     | enum of padding   |\n+| `dimension_numbers`      | `ConvolutionDimensionNumbers` | the number of     |\n+:                          :                               : dimensions        :\n+| `feature_group_count`    | int64                         | the number of     |\n+:                          :                               : feature groups    :\n+| `batch_group_count`      | int64                         | the number of     |\n+:                          :                               : batch groups      :\n+| `precision_config`       | optional `PrecisionConfig`    | enum for level of |\n+:                          :                               : precision         :\n+| `preferred_element_type` | optional `PrimitiveType`      | enum of scalar    |\n+:                          :                               : element type      :\n+\n+### ConvGeneral\n+\n+**`ConvGeneral(lhs, rhs, window_strides, padding, dimension_numbers,\n+feature_group_count, batch_group_count, precision_config,\n+preferred_element_type)`**\n+\n+See also\n+[`XlaBuilder::ConvGeneral`](https://github.com/openxla/xla/tree/main/xla/hlo/builder/xla_builder.h).\n+\n+Same as [`Conv`](#conv-convolution) where dimension numbers and padding\n+configuration is explicit\n+\n+| Arguments                | Type                          | Semantics         |\n+| ------------------------ | ----------------------------- | ----------------- |\n+| `lhs`                    | `XlaOp`                       | (n+2)-dimensional |\n+:                          :                               : array of inputs   :\n+| `rhs`                    | `XlaOp`                       | (n+2)-dimensional |\n+:                          :                               : array of kernel   :\n+:                          :                               : weights           :\n+| `window_strides`         | `ArraySlice<int64>`           | n-d array of      |\n+:                          :                               : kernel strides    :\n+| `padding`                | `ArraySlice<                  | n-d array of      |\n+:                          : pair<int64,int64>>`           : (low, high)       :\n+:                          :                               : padding           :\n+| `dimension_numbers`      | `ConvolutionDimensionNumbers` | the number of     |\n+:                          :                               : dimensions        :\n+| `feature_group_count`    | int64                         | the number of     |\n+:                          :                               : feature groups    :\n+| `batch_group_count`      | int64                         | the number of     |\n+:                          :                               : batch groups      :\n+| `precision_config`       | optional `PrecisionConfig`    | enum for level of |\n+:                          :                               : precision         :\n+| `preferred_element_type` | optional `PrimitiveType`      | enum of scalar    |\n+:                          :                               : element type      :\n+\n+### ConvGeneralDilated\n+\n+**`ConvGeneralDilated(lhs, rhs, window_strides, padding, lhs_dilation,\n+rhs_dilation, dimension_numbers, feature_group_count, batch_group_count,\n+precision_config, preferred_element_type, window_reversal)`**\n+\n+See also\n+[`XlaBuilder::ConvGeneralDilated`](https://github.com/openxla/xla/tree/main/xla/hlo/builder/xla_builder.h).\n+\n+Same as [`Conv`](#conv-convolution) where padding configuration, dilation\n+factors, and dimension numbers are explicit.\n+\n+| Arguments                | Type                          | Semantics         |\n+| ------------------------ | ----------------------------- | ----------------- |\n+| `lhs`                    | `XlaOp`                       | (n+2)-dimensional |\n+:                          :                               : array of inputs   :\n+| `rhs`                    | `XlaOp`                       | (n+2)-dimensional |\n+:                          :                               : array of kernel   :\n+:                          :                               : weights           :\n+| `window_strides`         | `ArraySlice<int64>`           | n-d array of      |\n+:                          :                               : kernel strides    :\n+| `padding`                | `ArraySlice<                  | n-d array of      |\n+:                          : pair<int64,int64>>`           : (low, high)       :\n+:                          :                               : padding           :\n+| `lhs_dilation`           | `ArraySlice<int64>`           | n-d lhs dilation  |\n+:                          :                               : factor array      :\n+| `rhs_dilation`           | `ArraySlice<int64>`           | n-d rhs dilation  |\n+:                          :                               : factor array      :\n+| `dimension_numbers`      | `ConvolutionDimensionNumbers` | the number of     |\n+:                          :                               : dimensions        :\n+| `feature_group_count`    | int64                         | the number of     |\n+:                          :                               : feature groups    :\n+| `batch_group_count`      | int64                         | the number of     |\n+:                          :                               : batch groups      :\n+| `precision_config`       | optional `PrecisionConfig`    | enum for level of |\n+:                          :                               : precision         :\n+| `preferred_element_type` | optional `PrimitiveType`      | enum of scalar    |\n+:                          :                               : element type      :\n+| `window_reversal`        | optional `vector<bool>`       | flag used to      |\n+:                          :                               : logically reverse :\n+:                          :                               : dimension before  :\n+:                          :                               : applying the      :\n+:                          :                               : convolution       :\n+\n ## ConvertElementType\n \n See also\n@@ -1927,6 +2097,9 @@ Blocks any optimization pass from moving computations across the barrier.\n Ensures that all inputs are evaluated before any operators that depend on the\n barrier's outputs.\n \n+See also\n+[StableHLO optimization_barrier](https://openxla.org/stablehlo/spec#optimization_barrier)\n+\n ## Pad\n \n See also\n@@ -2007,14 +2180,20 @@ See also\n \n Applies a reduction function to one or more arrays in parallel.\n \n-**`Reduce(operands..., init_values..., computation, dimensions)`**\n-\n-| Arguments     | Type                  | Semantics                        |\n-| ------------- | --------------------- | -------------------------------- |\n-| `operands`    | Sequence of N `XlaOp` | N arrays of types `T_0, ..., T_{N-1}`. |\n-| `init_values` | Sequence of N `XlaOp` | N scalars of types `T_0, ..., T_{N-1}`. |\n-| `computation` | `XlaComputation`      | computation of type `T_0, ..., T_{N-1}, T_0, ..., T_{N-1} ->` `Collate(T_0, ..., T_{N-1})`. |\n-| `dimensions`  | `int64` array         | unordered array of dimensions to reduce. |\n+**`Reduce(operands..., init_values..., computation, dimensions_to_reduce)`**\n+\n+| Arguments              | Type                  | Semantics                 |\n+| ---------------------- | --------------------- | ------------------------- |\n+| `operands`             | Sequence of N `XlaOp` | N arrays of types `T_0,   |\n+:                        :                       : ..., T_{N-1}`.            :\n+| `init_values`          | Sequence of N `XlaOp` | N scalars of types `T_0,  |\n+:                        :                       : ..., T_{N-1}`.            :\n+| `computation`          | `XlaComputation`      | computation of type `T_0, |\n+:                        :                       : ..., T_{N-1}, T_0, ...,   :\n+:                        :                       : T_{N-1} ->` `Collate(T_0, :\n+:                        :                       : ..., T_{N-1})`.           :\n+| `dimensions_to_reduce` | `int64` array         | unordered array of        |\n+:                        :                       : dimensions to reduce.     :\n \n Where:\n \n@@ -2993,6 +3172,13 @@ let t: (f32[10], s32) = tuple(v, s);\n Tuples can be deconstructed (accessed) via the [`GetTupleElement`]\n (#gettupleelement) operation.\n \n+For more information see\n+[StableHLO Tuple](https://openxla.org/stablehlo/spec#tuple)\n+\n+> **Note:** In HLO, tuples are needed for most ops that return >1 result. While\n+> in StableHLO/MLIR, variadic results can be expressed and tuples are not used,\n+> except in custom_calls/get_tuple_element.\n+\n ## While\n \n See also"
        }
    ],
    "stats": {
        "total": 246,
        "additions": 216,
        "deletions": 30
    }
}