{
    "author": "ezhulenev",
    "message": "[xla:gpu] Remove redundant Thunk::GetGpuCollectives\n\nPiperOrigin-RevId: 837910560",
    "sha": "261933ee3923574c8e5942190b895ed2cc782ee3",
    "files": [
        {
            "sha": "84c48b3bdefbe832019dced83546f44274cb4d43",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/261933ee3923574c8e5942190b895ed2cc782ee3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/261933ee3923574c8e5942190b895ed2cc782ee3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=261933ee3923574c8e5942190b895ed2cc782ee3",
            "patch": "@@ -1668,6 +1668,7 @@ cc_library(\n         \"//xla/core/collectives:rank_id\",\n         \"//xla/hlo/ir:collective_op_group_mode\",\n         \"//xla/hlo/ir:hlo\",\n+        \"//xla/runtime:device_id\",\n         \"//xla/service:buffer_assignment\",\n         \"//xla/service:collective_ops_utils\",\n         \"//xla/service:computation_placer\","
        },
        {
            "sha": "442f78b1610ecb33df5d734a908b7d1ca2bd6937",
            "filename": "third_party/xla/xla/backends/gpu/runtime/all_to_all_thunk.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/261933ee3923574c8e5942190b895ed2cc782ee3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_to_all_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/261933ee3923574c8e5942190b895ed2cc782ee3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_to_all_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fall_to_all_thunk.cc?ref=261933ee3923574c8e5942190b895ed2cc782ee3",
            "patch": "@@ -121,14 +121,11 @@ absl::Status AllToAllStartThunk::Initialize(const InitializeParams& params) {\n           << \"] Local device count : \" << device_count_;\n \n   if (is_local() && p2p_memcpy_enabled_) {\n-    TF_ASSIGN_OR_RETURN(GpuCollectives * collectives,\n-                        GetGpuCollectives(params));\n     AsyncStreamKind stream_kind = GetAsyncStreamKind();\n     TF_ASSIGN_OR_RETURN(\n         CommunicatorHandle comm_handle,\n-        GetComm(collectives, *params.collective_params,\n-                *params.collective_cliques, config().replica_groups,\n-                config().group_mode, stream_kind));\n+        GetComm(*params.collective_params, *params.collective_cliques,\n+                config().replica_groups, config().group_mode, stream_kind));\n     TF_ASSIGN_OR_RETURN(int32_t num_ranks, comm_handle.comm->NumRanks());\n     se::StreamExecutor* executor = params.executor;\n     {"
        },
        {
            "sha": "62d90f352c67919609a747d99dacaa9c67dc4862",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_thunk.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 26,
            "changes": 45,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/261933ee3923574c8e5942190b895ed2cc782ee3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/261933ee3923574c8e5942190b895ed2cc782ee3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.cc?ref=261933ee3923574c8e5942190b895ed2cc782ee3",
            "patch": "@@ -45,6 +45,7 @@ limitations under the License.\n #include \"xla/debug_options_flags.h\"\n #include \"xla/hlo/ir/collective_op_group_mode.h\"\n #include \"xla/primitive_util.h\"\n+#include \"xla/runtime/device_id.h\"\n #include \"xla/service/collective_ops_utils.h\"\n #include \"xla/service/computation_placer.h\"\n #include \"xla/service/global_device_id.h\"\n@@ -197,10 +198,12 @@ CollectiveThunk::CollectiveThunk(Kind kind, ThunkInfo thunk_info, bool is_sync,\n       async_events_(is_sync ? nullptr : std::make_shared<AsyncEvents>()) {}\n \n absl::StatusOr<GpuCliqueKey> GetGpuCliqueKey(\n-    GpuCollectives* collectives, const CollectiveParams& params,\n-    const std::vector<ReplicaGroup>& replica_groups,\n+    const CollectiveParams& params,\n+    absl::Span<const ReplicaGroup> replica_groups,\n     CollectiveOpGroupMode group_mode, AsyncStreamKind stream_kind,\n     bool use_nccl) {\n+  TF_RET_CHECK(params.collectives) << \"Collectives API is not provided\";\n+\n   GlobalDeviceId global_device_id = params.global_device_id;\n \n   if (params.device_assn == nullptr) {\n@@ -231,7 +234,7 @@ absl::StatusOr<GpuCliqueKey> GetGpuCliqueKey(\n                               *params.device_assn, replica_groups, group_mode));\n     }\n \n-    if (collectives->IsGlobalConfig() &&\n+    if (params.collectives->IsGlobalConfig() &&\n         (participants.size() != params.device_assn->replica_count())) {\n       return InvalidArgument(\n           \"Partial replica groups are not allowed when using NCCL_COMM_ID \"\n@@ -272,22 +275,18 @@ absl::StatusOr<GpuCliqueKey> GetGpuCliqueKey(\n absl::StatusOr<GpuCliqueKey> GetCollectiveGpuCliqueKey(\n     const CollectiveParams& params, const CollectiveConfig& collective_config,\n     bool use_nccl) {\n-  TF_ASSIGN_OR_RETURN(GpuCollectives * collectives,\n-                      CollectiveThunk::GetGpuCollectives(params));\n-  return GetGpuCliqueKey(collectives, params, collective_config.replica_groups,\n-                         collective_config.group_mode,\n-                         AsyncStreamKind::ASYNC_STREAM_KIND_COLLECTIVE,\n-                         use_nccl);\n+  return GetGpuCliqueKey(\n+      params, collective_config.replica_groups, collective_config.group_mode,\n+      AsyncStreamKind::ASYNC_STREAM_KIND_COLLECTIVE, use_nccl);\n }\n \n absl::StatusOr<CommunicatorHandle> GetComm(\n-    GpuCollectives* collectives, const CollectiveParams& params,\n-    const CollectiveCliques& collective_cliques,\n+    const CollectiveParams& params, const CollectiveCliques& collective_cliques,\n     const std::vector<ReplicaGroup>& replica_groups,\n     CollectiveOpGroupMode group_mode, AsyncStreamKind stream_kind) {\n-  TF_ASSIGN_OR_RETURN(GpuCliqueKey clique_key,\n-                      GetGpuCliqueKey(collectives, params, replica_groups,\n-                                      group_mode, stream_kind));\n+  TF_ASSIGN_OR_RETURN(\n+      GpuCliqueKey clique_key,\n+      GetGpuCliqueKey(params, replica_groups, group_mode, stream_kind));\n \n   std::optional<RankId> rank = clique_key.rank(params.global_device_id);\n   TF_ASSIGN_OR_RETURN(Communicator * comm,\n@@ -386,12 +385,10 @@ absl::StatusOr<se::Event*> CollectiveThunk::AsyncEvents::GetEvent(\n \n absl::Status CollectiveThunk::Prepare(const PrepareParams& params) {\n   TF_RET_CHECK(params.collective_params != nullptr);\n-  TF_ASSIGN_OR_RETURN(GpuCollectives * collectives, GetGpuCollectives(params));\n   TF_ASSIGN_OR_RETURN(\n       GpuCliqueKey clique_key,\n-      GetGpuCliqueKey(collectives, *params.collective_params,\n-                      config().replica_groups, config().group_mode,\n-                      GetAsyncStreamKind()));\n+      GetGpuCliqueKey(*params.collective_params, config().replica_groups,\n+                      config().group_mode, GetAsyncStreamKind()));\n   return params.clique_requests->RequestClique(clique_key);\n }\n \n@@ -407,12 +404,10 @@ absl::Status CollectiveThunk::ExecuteOnStream(const ExecuteParams& params) {\n       \"[%d] Starting %s %s.\", params.stream->parent()->device_ordinal(),\n       IsAsync() ? \"async\" : \"sync\", Thunk::KindToString(kind()));\n   AsyncStreamKind stream_kind = GetAsyncStreamKind();\n-  TF_ASSIGN_OR_RETURN(GpuCollectives * collectives, GetGpuCollectives(params));\n   TF_ASSIGN_OR_RETURN(\n       CommunicatorHandle comm_handle,\n-      GetComm(collectives, *params.collective_params,\n-              *params.collective_cliques, config().replica_groups,\n-              config().group_mode, stream_kind));\n+      GetComm(*params.collective_params, *params.collective_cliques,\n+              config().replica_groups, config().group_mode, stream_kind));\n   se::StreamExecutor* executor = params.stream->parent();\n   int64_t async_stream_idx = static_cast<int64_t>(stream_kind);\n \n@@ -485,12 +480,10 @@ absl::Status CollectiveThunk::ExecuteOnStream(const ExecuteParams& params) {\n absl::StatusOr<std::vector<Communicator*>> CollectiveThunk::GetCommunicators(\n     const ExecuteParams& params) const {\n   AsyncStreamKind stream_kind = GetAsyncStreamKind();\n-  TF_ASSIGN_OR_RETURN(GpuCollectives * collectives, GetGpuCollectives(params));\n   TF_ASSIGN_OR_RETURN(\n       CommunicatorHandle comm_handle,\n-      GetComm(collectives, *params.collective_params,\n-              *params.collective_cliques, config().replica_groups,\n-              config().group_mode, stream_kind));\n+      GetComm(*params.collective_params, *params.collective_cliques,\n+              config().replica_groups, config().group_mode, stream_kind));\n   return std::vector<Communicator*>{comm_handle.comm};\n }\n "
        },
        {
            "sha": "24596c7e9ca3abe8aa6e391eb16746a0e94f16ad",
            "filename": "third_party/xla/xla/backends/gpu/runtime/collective_thunk.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/261933ee3923574c8e5942190b895ed2cc782ee3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/261933ee3923574c8e5942190b895ed2cc782ee3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcollective_thunk.h?ref=261933ee3923574c8e5942190b895ed2cc782ee3",
            "patch": "@@ -1,5 +1,6 @@\n #include <cstddef>\n \n+#include \"absl/types/span.h\"\n #include \"xla/backends/gpu/runtime/collective_cliques.h\"\n #include \"xla/backends/gpu/runtime/thunk.pb.h\"\n /* Copyright 2019 The OpenXLA Authors.\n@@ -273,8 +274,8 @@ absl::Status AddOpDescription(absl::Status status, OpT op,\n //===----------------------------------------------------------------------===//\n \n absl::StatusOr<GpuCliqueKey> GetGpuCliqueKey(\n-    GpuCollectives* collectives, const CollectiveParams& params,\n-    const std::vector<ReplicaGroup>& replica_groups,\n+    const CollectiveParams& params,\n+    absl::Span<const ReplicaGroup> replica_groups,\n     CollectiveOpGroupMode group_mode, AsyncStreamKind stream_kind,\n     bool use_nccl = true);\n \n@@ -285,8 +286,7 @@ absl::StatusOr<GpuCliqueKey> GetCollectiveGpuCliqueKey(\n \n // Returns a communicator and additional information about the clique.\n absl::StatusOr<CommunicatorHandle> GetComm(\n-    GpuCollectives* collectives, const CollectiveParams& params,\n-    const CollectiveCliques& collective_cliques,\n+    const CollectiveParams& params, const CollectiveCliques& collective_cliques,\n     const std::vector<ReplicaGroup>& replica_groups,\n     CollectiveOpGroupMode group_mode, AsyncStreamKind stream_kind);\n "
        },
        {
            "sha": "d258db60cf43a435273dbfda8e3d6ea881e1dc99",
            "filename": "third_party/xla/xla/backends/gpu/runtime/command_buffer_cmd.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 27,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/261933ee3923574c8e5942190b895ed2cc782ee3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/261933ee3923574c8e5942190b895ed2cc782ee3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fcommand_buffer_cmd.cc?ref=261933ee3923574c8e5942190b895ed2cc782ee3",
            "patch": "@@ -2074,12 +2074,10 @@ CollectiveCmd::CollectiveCmd(\n \n absl::Status CollectiveCmd::Prepare(const Thunk::PrepareParams& params) {\n   TF_RET_CHECK(params.collective_params != nullptr);\n-  TF_ASSIGN_OR_RETURN(GpuCollectives * collectives,\n-                      Thunk::GetGpuCollectives(params));\n   TF_ASSIGN_OR_RETURN(\n       GpuCliqueKey clique_key,\n-      GetGpuCliqueKey(collectives, *params.collective_params,\n-                      config().replica_groups, config().group_mode,\n+      GetGpuCliqueKey(*params.collective_params, config().replica_groups,\n+                      config().group_mode,\n                       AsyncStreamKind::ASYNC_STREAM_KIND_COLLECTIVE));\n   return params.clique_requests->RequestClique(clique_key);\n }\n@@ -2151,12 +2149,9 @@ absl::StatusOr<const se::CommandBuffer::Command*> AllReduceCmd::Record(\n         \"AllReduceCmd requires collective parameters and cliques\");\n   }\n \n-  TF_ASSIGN_OR_RETURN(GpuCollectives * collectives,\n-                      Thunk::GetGpuCollectives(execute_params));\n-\n   TF_ASSIGN_OR_RETURN(\n       CommunicatorHandle comm_handle,\n-      GetComm(collectives, *execute_params.collective_params,\n+      GetComm(*execute_params.collective_params,\n               *execute_params.collective_cliques, config().replica_groups,\n               config().group_mode,\n               AsyncStreamKind::ASYNC_STREAM_KIND_COLLECTIVE));  // Use constant\n@@ -2217,12 +2212,9 @@ absl::StatusOr<const se::CommandBuffer::Command*> ReduceScatterCmd::Record(\n         \"ReduceScatterCmd requires collective parameters and cliques\");\n   }\n \n-  TF_ASSIGN_OR_RETURN(GpuCollectives * collectives,\n-                      Thunk::GetGpuCollectives(execute_params));\n-\n   TF_ASSIGN_OR_RETURN(\n       CommunicatorHandle comm_handle,\n-      GetComm(collectives, *execute_params.collective_params,\n+      GetComm(*execute_params.collective_params,\n               *execute_params.collective_cliques, config().replica_groups,\n               config().group_mode,\n               AsyncStreamKind::ASYNC_STREAM_KIND_COLLECTIVE));  // Use constant\n@@ -2284,11 +2276,9 @@ absl::StatusOr<const se::CommandBuffer::Command*> AllToAllCmd::Record(\n         \"AllToAllCmd requires collective parameters and cliques\");\n   }\n \n-  TF_ASSIGN_OR_RETURN(GpuCollectives * collectives,\n-                      Thunk::GetGpuCollectives(execute_params));\n   TF_ASSIGN_OR_RETURN(\n       CommunicatorHandle comm_handle,\n-      GetComm(collectives, *execute_params.collective_params,\n+      GetComm(*execute_params.collective_params,\n               *execute_params.collective_cliques, config().replica_groups,\n               config().group_mode,\n               AsyncStreamKind::ASYNC_STREAM_KIND_COLLECTIVE));  // Use constant\n@@ -2347,12 +2337,9 @@ absl::StatusOr<const se::CommandBuffer::Command*> AllGatherCmd::Record(\n         \"AllGatherCmd requires collective parameters and cliques\");\n   }\n \n-  TF_ASSIGN_OR_RETURN(GpuCollectives * collectives,\n-                      Thunk::GetGpuCollectives(execute_params));\n-\n   TF_ASSIGN_OR_RETURN(\n       CommunicatorHandle comm_handle,\n-      GetComm(collectives, *execute_params.collective_params,\n+      GetComm(*execute_params.collective_params,\n               *execute_params.collective_cliques, config().replica_groups,\n               config().group_mode,\n               AsyncStreamKind::ASYNC_STREAM_KIND_COLLECTIVE));  // Use constant\n@@ -2411,12 +2398,9 @@ CollectiveBroadcastCmd::Record(const Thunk::ExecuteParams& execute_params,\n         \"CollectiveBroadcastCmd requires collective parameters and cliques\");\n   }\n \n-  TF_ASSIGN_OR_RETURN(GpuCollectives * collectives,\n-                      Thunk::GetGpuCollectives(execute_params));\n-\n   TF_ASSIGN_OR_RETURN(\n       CommunicatorHandle comm_handle,\n-      GetComm(collectives, *execute_params.collective_params,\n+      GetComm(*execute_params.collective_params,\n               *execute_params.collective_cliques, config().replica_groups,\n               config().group_mode,\n               AsyncStreamKind::ASYNC_STREAM_KIND_COLLECTIVE));  // Use constant\n@@ -2476,12 +2460,9 @@ absl::StatusOr<const se::CommandBuffer::Command*> CollectivePermuteCmd::Record(\n         \"CollectivePermuteCmd requires collective parameters and cliques\");\n   }\n \n-  TF_ASSIGN_OR_RETURN(GpuCollectives * collectives,\n-                      Thunk::GetGpuCollectives(execute_params));\n-\n   TF_ASSIGN_OR_RETURN(\n       CommunicatorHandle comm_handle,\n-      GetComm(collectives, *execute_params.collective_params,\n+      GetComm(*execute_params.collective_params,\n               *execute_params.collective_cliques, config().replica_groups,\n               config().group_mode,\n               AsyncStreamKind::ASYNC_STREAM_KIND_COLLECTIVE));  // Use constant"
        },
        {
            "sha": "4c5a6d865f3570b3af37456434a5a539322a885d",
            "filename": "third_party/xla/xla/backends/gpu/runtime/nvshmem_collective_thunk.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/261933ee3923574c8e5942190b895ed2cc782ee3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_collective_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/261933ee3923574c8e5942190b895ed2cc782ee3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_collective_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fnvshmem_collective_thunk.cc?ref=261933ee3923574c8e5942190b895ed2cc782ee3",
            "patch": "@@ -91,12 +91,11 @@ absl::StatusOr<xla::gpu::GpuCollectives*> GetNvshmemCollectivesFromRegistry() {\n \n absl::Status NvshmemCollectiveThunk::Prepare(const PrepareParams& params) {\n   TF_RET_CHECK(params.collective_params != nullptr);\n-  TF_ASSIGN_OR_RETURN(GpuCollectives * collectives, GetGpuCollectives(params));\n   TF_ASSIGN_OR_RETURN(\n       GpuCliqueKey clique_key,\n-      GetGpuCliqueKey(collectives, *params.collective_params,\n-                      config().replica_groups, config().group_mode,\n-                      GetAsyncStreamKind(), /*use_nccl= */ false));\n+      GetGpuCliqueKey(*params.collective_params, config().replica_groups,\n+                      config().group_mode, GetAsyncStreamKind(),\n+                      /*use_nccl= */ false));\n   return params.clique_requests->RequestClique(clique_key);\n }\n "
        },
        {
            "sha": "5bd1e169aea492c115a8d8355491c01bd2d01131",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk.cc",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/261933ee3923574c8e5942190b895ed2cc782ee3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/261933ee3923574c8e5942190b895ed2cc782ee3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.cc?ref=261933ee3923574c8e5942190b895ed2cc782ee3",
            "patch": "@@ -309,14 +309,6 @@ ThunkMetadataListProto GetMetadataListProtoFromThunkGraph(\n   return metadata_list_proto;\n }\n \n-absl::StatusOr<GpuCollectives* absl_nonnull> Thunk::GetGpuCollectives(\n-    const CollectiveParams& params) {\n-  if (params.collectives == nullptr) {\n-    return Internal(\"Collectives API is not provided\");\n-  }\n-  return params.collectives;\n-}\n-\n ThunkInfoProto Thunk::ThunkInfo::ToProto() const {\n   ThunkInfoProto proto;\n   proto.set_profile_annotation(profile_annotation);"
        },
        {
            "sha": "dbbfa3309a27ad1115c0d55d9398da74fe6d2753",
            "filename": "third_party/xla/xla/backends/gpu/runtime/thunk.h",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/261933ee3923574c8e5942190b895ed2cc782ee3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/261933ee3923574c8e5942190b895ed2cc782ee3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fthunk.h?ref=261933ee3923574c8e5942190b895ed2cc782ee3",
            "patch": "@@ -447,22 +447,6 @@ class Thunk {\n     return absl::OkStatus();\n   }\n \n-  // A helper function to get the `GpuCollectives*` pointer from the\n-  // CollectiveParams.\n-  static absl::StatusOr<GpuCollectives* absl_nonnull> GetGpuCollectives(\n-      CollectiveParams const& params);\n-\n-  // A helper function to get the `GpuCollectives*` pointer from the\n-  // thunk parameters. Returns an error if collectives API is not provided.\n-  template <typename Params>\n-  static absl::StatusOr<GpuCollectives* absl_nonnull> GetGpuCollectives(\n-      const Params& params) {\n-    if (params.collective_params == nullptr) {\n-      return Internal(\"Collective params are not provided\");\n-    }\n-    return GetGpuCollectives(*params.collective_params);\n-  }\n-\n   // Serializes the thunk into a `ThunkProto`.\n   virtual absl::StatusOr<ThunkProto> ToProto() const;\n "
        }
    ],
    "stats": {
        "total": 127,
        "additions": 37,
        "deletions": 90
    }
}