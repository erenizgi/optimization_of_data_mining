{
    "author": "chsigg",
    "message": "[xla:gpu] Add padding to split-k to allow pipelining.\n\nLoads are required to be 16-byte aligned for Triton to apply pipelining. This change adds extra padding to both split-k rewriters so that the reduction dimensions are a multiple of 16 bytes.\n\nPiperOrigin-RevId: 820134896",
    "sha": "6ac8b2ea6e2c3acb0f9c3919e84df2c1309e7d92",
    "files": [
        {
            "sha": "ebe902cb21369227ddb52ff89d1c5b206fdccd5b",
            "filename": "third_party/xla/xla/service/gpu/split_k_gemm_rewriter.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6ac8b2ea6e2c3acb0f9c3919e84df2c1309e7d92/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fsplit_k_gemm_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6ac8b2ea6e2c3acb0f9c3919e84df2c1309e7d92/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fsplit_k_gemm_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fsplit_k_gemm_rewriter.cc?ref=6ac8b2ea6e2c3acb0f9c3919e84df2c1309e7d92",
            "patch": "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include \"xla/service/gpu/split_k_gemm_rewriter.h\"\n \n+#include <algorithm>\n #include <cmath>\n #include <cstdint>\n #include <iterator>\n@@ -109,6 +110,17 @@ absl::Status UncompilableMatmul(absl::string_view explanation) {\n   return s;\n }\n \n+// Returns the padded K dimension so that it is a multiple of split_k and 16B.\n+int64_t GetPaddedK(HloInstruction& dot, int64_t k, int64_t split_k) {\n+  const int64_t alignment_in_bits = 16 * 8;\n+  int64_t min_element_size_in_bits = alignment_in_bits;\n+  for (const HloInstruction* p : dot.parent()->parameter_instructions()) {\n+    min_element_size_in_bits = std::min(\n+        min_element_size_in_bits, ShapeUtil::ElementSizeInBits(p->shape()));\n+  }\n+  return RoundUpTo(k, split_k * alignment_in_bits / min_element_size_in_bits);\n+}\n+\n }  // namespace\n \n absl::StatusOr<HloInstruction*> MakeSplitKOperand(\n@@ -118,6 +130,8 @@ absl::StatusOr<HloInstruction*> MakeSplitKOperand(\n     std::optional<int64_t> padded_k_size = std::nullopt) {\n   HloInstruction* operand = dot.mutable_operand(operand_number);\n   const int64_t k = operand->shape().dimensions(contracting_dim_idx);\n+  padded_k_size =\n+      std::max(GetPaddedK(dot, k, config.split_k), padded_k_size.value_or(0));\n   const bool need_padding =\n       padded_k_size.has_value() ? k < *padded_k_size : k % config.split_k != 0;\n "
        },
        {
            "sha": "3d73b499a179016d23f3090ff90f0841693577ca",
            "filename": "third_party/xla/xla/service/gpu/split_k_gemm_rewriter_test.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6ac8b2ea6e2c3acb0f9c3919e84df2c1309e7d92/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fsplit_k_gemm_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6ac8b2ea6e2c3acb0f9c3919e84df2c1309e7d92/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fsplit_k_gemm_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fsplit_k_gemm_rewriter_test.cc?ref=6ac8b2ea6e2c3acb0f9c3919e84df2c1309e7d92",
            "patch": "@@ -851,10 +851,10 @@ ENTRY %entry_computation {\n       GmockMatch(m::Reduce(m::Fusion(&dot_fusion), m::ConstantScalar())));\n   EXPECT_THAT(\n       dot_fusion->called_computations()[0]->root_instruction(),\n-      GmockMatch(m::ScaledDot(m::Op().WithShape(F8E4M3FN, {16, 3, 64}),\n-                              m::Op().WithShape(F8E5M2, {32, 3, 64}),\n-                              m::Op().WithShape(F8E8M0FNU, {16, 3, 2}),\n-                              m::Op().WithShape(F8E8M0FNU, {32, 3, 2}))));\n+      GmockMatch(m::ScaledDot(m::Op().WithShape(F8E4M3FN, {16, 3, 512}),\n+                              m::Op().WithShape(F8E5M2, {32, 3, 512}),\n+                              m::Op().WithShape(F8E8M0FNU, {16, 3, 16}),\n+                              m::Op().WithShape(F8E8M0FNU, {32, 3, 16}))));\n }\n \n TEST_F(SplitKTest, ScaledDot_DifferentBlockSize) {\n@@ -888,10 +888,10 @@ ENTRY %entry_computation {\n       GmockMatch(m::Reduce(m::Fusion(&dot_fusion), m::ConstantScalar())));\n   EXPECT_THAT(\n       dot_fusion->called_computations()[0]->root_instruction(),\n-      GmockMatch(m::ScaledDot(m::Op().WithShape(F8E4M3FN, {16, 3, 64}),\n-                              m::Op().WithShape(F8E5M2, {32, 3, 64}),\n-                              m::Op().WithShape(F8E8M0FNU, {16, 3, 2}),\n-                              m::Op().WithShape(F8E8M0FNU, {32, 3, 4}))));\n+      GmockMatch(m::ScaledDot(m::Op().WithShape(F8E4M3FN, {16, 3, 512}),\n+                              m::Op().WithShape(F8E5M2, {32, 3, 512}),\n+                              m::Op().WithShape(F8E8M0FNU, {16, 3, 16}),\n+                              m::Op().WithShape(F8E8M0FNU, {32, 3, 32}))));\n }\n \n TEST_F(SplitKTest, ScaledDot_LhsOnly) {\n@@ -923,9 +923,9 @@ ENTRY %entry_computation {\n       module->entry_computation()->root_instruction(),\n       GmockMatch(m::Reduce(m::Fusion(&dot_fusion), m::ConstantScalar())));\n   EXPECT_THAT(dot_fusion->called_computations()[0]->root_instruction(),\n-              GmockMatch(m::ScaledDot(m::Op().WithShape(F8E4M3FN, {16, 3, 64}),\n-                                      m::Op().WithShape(F8E5M2, {32, 3, 64}),\n-                                      m::Op().WithShape(F8E8M0FNU, {16, 3, 2}),\n+              GmockMatch(m::ScaledDot(m::Op().WithShape(F8E4M3FN, {16, 3, 512}),\n+                                      m::Op().WithShape(F8E5M2, {32, 3, 512}),\n+                                      m::Op().WithShape(F8E8M0FNU, {16, 3, 16}),\n                                       m::Op().WithShape(F8E5M2, {}))));\n }\n \n@@ -959,10 +959,10 @@ ENTRY %entry_computation {\n       GmockMatch(m::Reduce(m::Fusion(&dot_fusion), m::ConstantScalar())));\n   EXPECT_THAT(\n       dot_fusion->called_computations()[0]->root_instruction(),\n-      GmockMatch(m::ScaledDot(m::Op().WithShape(F8E4M3FN, {16, 3, 64}),\n-                              m::Op().WithShape(F8E5M2, {32, 3, 64}),\n+      GmockMatch(m::ScaledDot(m::Op().WithShape(F8E4M3FN, {16, 3, 512}),\n+                              m::Op().WithShape(F8E5M2, {32, 3, 512}),\n                               m::Op().WithShape(F8E4M3FN, {}),\n-                              m::Op().WithShape(F8E8M0FNU, {32, 3, 2}))));\n+                              m::Op().WithShape(F8E8M0FNU, {32, 3, 16}))));\n }\n \n TEST_F(SplitKTest, ScaledDot_IncompatibleBlockSize) {"
        },
        {
            "sha": "89e0215f79da4540c485b4fa6a017f036fbe507b",
            "filename": "third_party/xla/xla/service/gpu/transforms/splitk_rewriter.cc",
            "status": "modified",
            "additions": 19,
            "deletions": 8,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6ac8b2ea6e2c3acb0f9c3919e84df2c1309e7d92/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsplitk_rewriter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6ac8b2ea6e2c3acb0f9c3919e84df2c1309e7d92/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsplitk_rewriter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsplitk_rewriter.cc?ref=6ac8b2ea6e2c3acb0f9c3919e84df2c1309e7d92",
            "patch": "@@ -219,24 +219,33 @@ HloInstruction* PadInstruction(HloInstruction* instr, int64_t dimension_idx,\n   PaddingConfig padding_config =\n       MakeNoPaddingConfig(instr->shape().dimensions().size());\n   padding_config.mutable_dimensions(dimension_idx)\n-      ->set_edge_padding_low(new_dimension_size -\n-                             instr->shape().dimensions(dimension_idx));\n+      ->set_edge_padding_high(new_dimension_size -\n+                              instr->shape().dimensions(dimension_idx));\n   Shape new_shape = instr->shape();\n   new_shape.set_dimensions(dimension_idx, new_dimension_size);\n   return computation->AddInstruction(\n       HloInstruction::CreatePad(new_shape, instr, zero, padding_config));\n }\n \n+// Returns the padded K dimension so that it is a multiple of split_k and 16B.\n+int64_t GetPaddedK(HloInstruction& dot, int64_t k, int64_t split_k) {\n+  const int64_t alignment_in_bits = 16 * 8;\n+  int64_t min_element_size_in_bits = alignment_in_bits;\n+  for (const HloInstruction* p : dot.parent()->parameter_instructions()) {\n+    min_element_size_in_bits = std::min(\n+        min_element_size_in_bits, ShapeUtil::ElementSizeInBits(p->shape()));\n+  }\n+  return RoundUpTo(k, split_k * alignment_in_bits / min_element_size_in_bits);\n+}\n+\n // The contracting dimension index becomes new batch (split) dimension, and all\n // dimensions after it are shifted by 1.\n HloInstruction* SplitKOperand(HloInstruction* operand,\n                               int64_t contracting_dimension_idx,\n-                              int64_t split_k) {\n+                              int64_t split_k, int64_t padded_k) {\n   // if the K dimension is not divisible by split_k, we need to pad it.\n   const int64_t src_k = operand->shape().dimensions(contracting_dimension_idx);\n-  const bool needs_padding = src_k % split_k != 0;\n-  if (needs_padding) {\n-    const int64_t padded_k = RoundUpTo(src_k, split_k);\n+  if (padded_k != src_k) {\n     operand = PadInstruction(operand, contracting_dimension_idx, padded_k);\n   }\n   const Shape& old_shape = operand->shape();\n@@ -296,12 +305,14 @@ absl::StatusOr<HloInstruction*> SplitKDimensionOfDot(HloDotInstruction* src_dot,\n       src_dot->dot_dimension_numbers().lhs_contracting_dimensions(0);\n   const int64_t rhs_k_idx =\n       src_dot->dot_dimension_numbers().rhs_contracting_dimensions(0);\n+  const int64_t padded_k = GetPaddedK(\n+      *src_dot, src_dot->operand(0)->shape().dimensions(lhs_k_idx), split_k);\n   // The operands' K dimension are split into [split_k, K/split_k] (shifting\n   // right all the dimensions after it).\n   HloInstruction* lhs =\n-      SplitKOperand(src_dot->mutable_operand(0), lhs_k_idx, split_k);\n+      SplitKOperand(src_dot->mutable_operand(0), lhs_k_idx, split_k, padded_k);\n   HloInstruction* rhs =\n-      SplitKOperand(src_dot->mutable_operand(1), rhs_k_idx, split_k);\n+      SplitKOperand(src_dot->mutable_operand(1), rhs_k_idx, split_k, padded_k);\n \n   // Update the dot's dimension numbers accordingly (shifting right all the\n   // dimensions starting from the K dimension and inserting new batch dims)."
        },
        {
            "sha": "8447fed691b3f928b3d690fa144d640acfa8669d",
            "filename": "third_party/xla/xla/service/gpu/transforms/splitk_rewriter_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6ac8b2ea6e2c3acb0f9c3919e84df2c1309e7d92/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsplitk_rewriter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6ac8b2ea6e2c3acb0f9c3919e84df2c1309e7d92/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsplitk_rewriter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftransforms%2Fsplitk_rewriter_test.cc?ref=6ac8b2ea6e2c3acb0f9c3919e84df2c1309e7d92",
            "patch": "@@ -94,7 +94,7 @@ TEST_F(SplitkRewriterTest, PaddingIsInserted) {\n                           rewriter_.HloModulePass::Run(module.get()));\n   EXPECT_TRUE(changed);\n   EXPECT_TRUE(RunFileCheck(module->ToString(), R\"(\n-CHECK: f32[16,102528]{1,0} pad({{.*}}), padding=0_0x127_0\n+CHECK: f32[16,102912]{1,0} pad(%lhs, %constant), padding=0_0x0_511\n     )\")\n                   .value_or(false));\n }"
        }
    ],
    "stats": {
        "total": 71,
        "additions": 48,
        "deletions": 23
    }
}