{
    "author": "sohaibiftikhar",
    "message": "[XLA:GPU]: Add triton_xla.atomic_spin_wait op\n\natomic_spin_wait takes in a pointer to an address or a tensor of addresses,\natomically loads the value of that address with the passed memory semantic and\nscope, and compares it against the given value based on the comparator.\n\nAs long as the comparison returns true the operation will spin wait.\n\nPiperOrigin-RevId: 804818109",
    "sha": "8aa068842169a74c331bed6309040f10d0c03f5b",
    "files": [
        {
            "sha": "d483cf9e5cf16d27f54e9df5a421200f3b15cfcf",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/ir/triton_xla_attrs.td",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8aa068842169a74c331bed6309040f10d0c03f5b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_attrs.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8aa068842169a74c331bed6309040f10d0c03f5b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_attrs.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_attrs.td?ref=8aa068842169a74c331bed6309040f10d0c03f5b",
            "patch": "@@ -83,4 +83,16 @@ def TTXLA_LayoutAttr : TTXLA_Attr<\"Layout\", [\n   }];\n }\n \n+def TTXLA_ComparatorEnum : I32Enum<\"Comparator\",\n+    \"A comparison operator for instructions.\",\n+    [\n+      I32EnumCase<\"EQ\", 0, \"equal_to\">,\n+      I32EnumCase<\"LT\", 1, \"less_than\">\n+      // Extend if required.\n+    ]> {\n+  let cppNamespace = \"::mlir::triton::xla\";\n+}\n+def TTXLA_ComparatorAttr : EnumAttr<XlaTritonDialect, TTXLA_ComparatorEnum, \"\">;\n+\n+\n #endif // XLA_BACKENDS_GPU_CODEGEN_TRITON_IR_TRITON_XLA_ATTRS_TD_"
        },
        {
            "sha": "07b5e94e4dab1953e500fa9b5604dc01b2dcab8c",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/ir/triton_xla_ops.td",
            "status": "modified",
            "additions": 54,
            "deletions": 0,
            "changes": 54,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8aa068842169a74c331bed6309040f10d0c03f5b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_ops.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8aa068842169a74c331bed6309040f10d0c03f5b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_ops.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fir%2Ftriton_xla_ops.td?ref=8aa068842169a74c331bed6309040f10d0c03f5b",
            "patch": "@@ -22,6 +22,7 @@ include \"mlir/IR/OpAsmInterface.td\"\n include \"mlir/Interfaces/SideEffectInterfaces.td\" // Pure\n include \"mlir/Interfaces/InferTypeOpInterface.td\" // SameOperandsAndResultType\n include \"mlir/Interfaces/ViewLikeInterface.td\" // OffsetSizeAndStrideOpInterface\n+include \"xla/backends/gpu/codegen/triton/ir/triton_xla_attrs.td\"\n include \"xla/backends/gpu/codegen/triton/ir/triton_xla_dialect.td\"\n include \"triton/Dialect/Triton/IR/TritonAttrDefs.td\" // TT_MemSemanticAttr, TT_MemSyncScopeAttr\n include \"triton/Dialect/Triton/IR/TritonInterfaces.td\"\n@@ -267,5 +268,58 @@ def TTXLA_AtomicWriteOp : TTXLA_Op<\"atomic_write\", []> {\n   }];\n }\n \n+// NB: MemoryWrite is a workaround to prevent this op from being cleaned up\n+// during dead code elimination.\n+def TTXLA_AtomicSpinWaitOp : TTXLA_Op<\"atomic_spin_wait\",\n+  [MemoryEffects<[MemWrite]>]> {\n+    let summary = \"Spin-wait while reading u32 value(s) atomically.\";\n+\n+    let description = [{\n+      Atomically read u32 value(s) from the memory location(s) specified by\n+      $ptr. The memory ordering guarantees are specified by the\n+      $mem_sync_(scope|semantic) attributes.\n+\n+      The loaded value is compared to $expected using the $comparator.\n+      As long as the comparison returns true, the spin-wait loop continues.\n+      If the comparison returns false, the spin-wait loop is finished.\n+\n+      NB: In the end only a single lane/thread within a program will\n+      write to (or read from) a given memory location so the synchronization\n+      happens between a single writing lane with a single reading lane. However,\n+      the operation will wait until all lanes within the block have synchronized\n+      so the end result is a block level memory synchronization.\n+\n+      For Memory Synchronization Scope, see comment on TTXLA_AtomicWriteOp.\n+      The following memory synchronization semantics are allowed:\n+        - relaxed: No memory ordering guarantees.\n+        - acquire: No reads or writes in the current block/program can be\n+          reordered before this wait. All writes in other programs that\n+          `release` the same memory location are visible in the current block.\n+\n+      Internally this operation expands into elementwise\n+      `ld.global.<semantic>.<scope>.u32` and a labelled loop.\n+    }];\n+\n+    let arguments = (ins\n+      Arg<TT_PtrLike, \"\", [MemRead<GlobalMemory>]>:$ptr,\n+      TT_I32Like:$expected,\n+      Optional<TT_BoolLike>:$mask,\n+      TT_MemSyncScopeAttr:$mem_sync_scope,\n+      TT_MemSemanticAttr:$mem_sync_semantic,\n+      TTXLA_ComparatorAttr:$comparator\n+    );\n+\n+    // An atomic spin-wait returns no results.\n+    let results = (outs);\n+\n+    // See comment on TTXLA_AtomicWriteOp for why we explicitly add the mem sync\n+    // attributes to the assembly format.\n+    let assemblyFormat = [{\n+      $mem_sync_scope `,` $mem_sync_semantic `,`\n+      $ptr `,` $comparator `,` $expected (`,` $mask^) ?  attr-dict `:`\n+      functional-type(operands, results)\n+    }];\n+}\n+\n #endif // XLA_BACKENDS_GPU_CODEGEN_TRITON_IR_TRITON_XLA_OPS_TD_\n "
        },
        {
            "sha": "af02ad2cbd2c68c2025a51946ad547a461d8936f",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/tests/triton_xla_atomic_spin_wait.mlir",
            "status": "added",
            "additions": 46,
            "deletions": 0,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8aa068842169a74c331bed6309040f10d0c03f5b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_atomic_spin_wait.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8aa068842169a74c331bed6309040f10d0c03f5b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_atomic_spin_wait.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftests%2Ftriton_xla_atomic_spin_wait.mlir?ref=8aa068842169a74c331bed6309040f10d0c03f5b",
            "patch": "@@ -0,0 +1,46 @@\n+// RUN: xla-opt %s --triton-xla-atomics | FileCheck %s\n+\n+// CHECK-LABEL: tt.func @nomask_kernel\n+// CHECK-SAME:    %[[ARG0:.*]]: !tt.ptr<i32>\n+// CHECK-SAME:    %[[ARG1:.*]]: i32\n+tt.func @nomask_kernel(%ptr : !tt.ptr<i32>, %expected : i32) {\n+// CHECK-NEXT:  %[[RES:.+]] = tt.elementwise_inline_asm\n+// CHECK-SAME:  .reg .pred %p<1>;\n+// CHECK-SAME:  .reg .b32 %r<1>;\n+// CHECK-SAME:  wait:\n+// CHECK-SAME:  ld.global.gpu.relaxed.u32 %r0, [$1];\n+// CHECK-SAME:  setp.eq.u32 %p0, %r0, $2;\n+// CHECK-SAME:  @%p0 bra wait;\n+// CHECK-SAME:  {constraints = \"=r,l,r\", packed_element = 1 : i32, pure = false}\n+// CHECK-SAME:  %[[ARG0]], %[[ARG1]]\n+// CHECK-SAME:  !tt.ptr<i32>, i32 -> i32\n+  triton_xla.atomic_spin_wait gpu, relaxed, %ptr, equal_to, %expected  : (!tt.ptr<i32>, i32) -> ()\n+  tt.return\n+}\n+\n+// CHECK-LABEL: tt.func @masked_kernel\n+// CHECK-SAME:    %[[ARG0:.*]]: tensor<4x!tt.ptr<i32>>\n+// CHECK-SAME:    %[[ARG1:.*]]: tensor<4xi1>\n+// CHECK-SAME:    %[[ARG2:.*]]: i32\n+tt.func @masked_kernel(\n+  %ptr: tensor<4x!tt.ptr<i32>>,\n+  %mask: tensor<4xi1>,\n+  %expected: i32\n+) {\n+// CHECK:         tt.elementwise_inline_asm\n+// CHECK-SAME:    .reg .pred %p<2>;\n+// CHECK-SAME:    .reg .b32 %r<1>;\n+// CHECK-SAME:    setp.ne.u32 %p0, $3, 0;\n+// CHECK-SAME:    @%!p0 bra done;\n+// CHECK-SAME:    wait:\n+// CHECK-SAME:    ld.global.gpu.acquire.u32 %r0, [$1];\n+// CHECK-SAME:    setp.lt.u32 %p1, %r0, $2;\n+// CHECK-SAME:    @%p1 bra wait;\n+// CHECK-SAME:    done:\n+// CHECK-SAME:    {constraints = \"=r,l,r,r\", packed_element = 1 : i32, pure = false}\n+// CHECK-SAME:    %[[ARG0]], %[[ARG2]], %[[ARG1]]\n+// CHECK-SAME:    tensor<4x!tt.ptr<i32>>, i32, tensor<4xi1> -> tensor<4xi32>\n+  triton_xla.atomic_spin_wait gpu, acquire, %ptr, less_than, %expected, %mask\n+      : (tensor<4x!tt.ptr<i32>>, i32, tensor<4xi1>) -> ()\n+  tt.return\n+}"
        },
        {
            "sha": "e3d7dde02dc39f90f930085e3a7fcace136376e3",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_lower_atomics_pass.cc",
            "status": "modified",
            "additions": 88,
            "deletions": 12,
            "changes": 100,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8aa068842169a74c331bed6309040f10d0c03f5b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_atomics_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8aa068842169a74c331bed6309040f10d0c03f5b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_atomics_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_lower_atomics_pass.cc?ref=8aa068842169a74c331bed6309040f10d0c03f5b",
            "patch": "@@ -62,11 +62,31 @@ absl::string_view GetMemSyncScopeStr(triton::MemSyncScope scope) {\n   }\n }\n \n+absl::string_view GetComparatorStr(Comparator comparator) {\n+  switch (comparator) {\n+    case Comparator::EQ:\n+      return \"eq\";\n+    case Comparator::LT:\n+      return \"lt\";\n+  }\n+}\n+\n+mlir::Type GetResultType(mlir::Type ptr_type, PatternRewriter& rewriter) {\n+  mlir::Type result_type = rewriter.getI32Type();\n+  auto ranked_tensor_type = mlir::dyn_cast<mlir::RankedTensorType>(ptr_type);\n+  // Tensor arguments must have tensor result type.\n+  if (ranked_tensor_type) {\n+    result_type = mlir::RankedTensorType::get(ranked_tensor_type.getShape(),\n+                                              rewriter.getI32Type());\n+  }\n+  return result_type;\n+}\n+\n LogicalResult LowerAtomicWriteOp(AtomicWriteOp atomic_write,\n                                  PatternRewriter& rewriter) {\n   mlir::ImplicitLocOpBuilder builder(atomic_write.getLoc(), rewriter);\n \n-  mlir::Value addr = atomic_write.getPtr();\n+  mlir::Value ptr = atomic_write.getPtr();\n   mlir::Value value = atomic_write.getValue();\n   triton::MemSemantic semantic = atomic_write.getMemSyncSemantic();\n   if (semantic != triton::MemSemantic::RELAXED &&\n@@ -91,15 +111,7 @@ LogicalResult LowerAtomicWriteOp(AtomicWriteOp atomic_write,\n     st.global.%s.%s.u32 [$1], $2;\n   )\";\n \n-  const auto i32_type = rewriter.getI32Type();\n-  mlir::Type result_type = i32_type;\n-  auto ranked_tensor_type =\n-      mlir::dyn_cast<mlir::RankedTensorType>(addr.getType());\n-  if (ranked_tensor_type) {\n-    // Tensor arguments must have tensor result type.\n-    result_type =\n-        mlir::RankedTensorType::get(ranked_tensor_type.getShape(), i32_type);\n-  }\n+  mlir::Type result_type = GetResultType(ptr.getType(), rewriter);\n   mlir::Value mask = atomic_write.getMask();\n   if (mask) {\n     const std::string atomic_write_asm_with_mask = absl::StrFormat(\n@@ -110,7 +122,7 @@ LogicalResult LowerAtomicWriteOp(AtomicWriteOp atomic_write,\n         /*constraints=*/rewriter.getStringAttr(\"=r,l,r,r\"),\n         /*pure=*/rewriter.getBoolAttr(false),\n         /*packed_element=*/rewriter.getI32IntegerAttr(1),\n-        /*args=*/mlir::ValueRange{addr, value, mask});\n+        /*args=*/mlir::ValueRange{ptr, value, mask});\n   } else {\n     const std::string atomic_write_asm =\n         absl::StrFormat(kAtomicWriteAsmTemplate, scope, memory_semantic);\n@@ -120,13 +132,76 @@ LogicalResult LowerAtomicWriteOp(AtomicWriteOp atomic_write,\n         /*constraints=*/rewriter.getStringAttr(\"=r,l,r\"),\n         /*pure=*/rewriter.getBoolAttr(false),\n         /*packed_element=*/rewriter.getI32IntegerAttr(1),\n-        /*args=*/mlir::ValueRange{addr, value});\n+        /*args=*/mlir::ValueRange{ptr, value});\n   }\n   // No results to replace; just erase the op.\n   rewriter.eraseOp(atomic_write);\n   return success();\n }\n \n+LogicalResult LowerAtomicSpinWaitOp(AtomicSpinWaitOp atomic_wait,\n+                                    PatternRewriter& rewriter) {\n+  mlir::ImplicitLocOpBuilder builder(atomic_wait.getLoc(), rewriter);\n+\n+  mlir::Value ptr = atomic_wait.getPtr();\n+  mlir::Value expected = atomic_wait.getExpected();\n+  triton::MemSemantic semantic = atomic_wait.getMemSyncSemantic();\n+  if (semantic != triton::MemSemantic::RELAXED &&\n+      semantic != triton::MemSemantic::ACQUIRE) {\n+    return rewriter.notifyMatchFailure(\n+        atomic_wait, absl::StrFormat(\"Unsupported memory semantic: %s\",\n+                                     stringifyMemSemantic(semantic)));\n+  }\n+  absl::string_view memory_semantic = GetMemorySemanticStr(semantic);\n+  absl::string_view scope = GetMemSyncScopeStr(atomic_wait.getMemSyncScope());\n+\n+  absl::string_view comparator = GetComparatorStr(atomic_wait.getComparator());\n+  constexpr absl::string_view kAtomicSpinWaitAsmTemplate = R\"(\n+    .reg .pred %%p<1>;\n+    .reg .b32 %%r<1>;\n+    wait:\n+      ld.global.%s.%s.u32 %%r0, [$1];\n+      setp.%s.u32 %%p0, %%r0, $2;\n+      @%%p0 bra wait;\n+  )\";\n+  constexpr absl::string_view kAtomicSpinWaitAsmWithMaskTemplate = R\"(\n+    .reg .pred %%p<2>;\n+    .reg .b32 %%r<1>;\n+    setp.ne.u32 %%p0, $3, 0;\n+    @%%!p0 bra done;\n+    wait:\n+      ld.global.%s.%s.u32 %%r0, [$1];\n+      setp.%s.u32 %%p1, %%r0, $2;\n+      @%%p1 bra wait;\n+    done:\n+  )\";\n+  mlir::Type result_type = GetResultType(ptr.getType(), rewriter);\n+  Value mask = atomic_wait.getMask();\n+  if (mask) {\n+    const std::string atomic_wait_asm_with_mask = absl::StrFormat(\n+        kAtomicSpinWaitAsmWithMaskTemplate, scope, memory_semantic, comparator);\n+    builder.create<triton::ElementwiseInlineAsmOp>(\n+        /*result_types=*/result_type,\n+        /*asm_string=*/rewriter.getStringAttr(atomic_wait_asm_with_mask),\n+        /*constraints=*/rewriter.getStringAttr(\"=r,l,r,r\"),\n+        /*pure=*/rewriter.getBoolAttr(false),\n+        /*packed_element=*/rewriter.getI32IntegerAttr(1),\n+        /*args=*/mlir::ValueRange{ptr, expected, mask});\n+  } else {\n+    const std::string atomic_wait_asm = absl::StrFormat(\n+        kAtomicSpinWaitAsmTemplate, scope, memory_semantic, comparator);\n+    builder.create<triton::ElementwiseInlineAsmOp>(\n+        /*result_types=*/result_type,\n+        /*asm_string=*/rewriter.getStringAttr(atomic_wait_asm),\n+        /*constraints=*/rewriter.getStringAttr(\"=r,l,r\"),\n+        /*pure=*/rewriter.getBoolAttr(false),\n+        /*packed_element=*/rewriter.getI32IntegerAttr(1),\n+        /*args=*/mlir::ValueRange{ptr, expected});\n+  }\n+  rewriter.eraseOp(atomic_wait);\n+  return success();\n+}\n+\n class TritonXLALowerAtomicsPass\n     : public impl::TritonXLALowerAtomicsPassBase<TritonXLALowerAtomicsPass> {\n  public:\n@@ -136,6 +211,7 @@ class TritonXLALowerAtomicsPass\n   void runOnOperation() override {\n     RewritePatternSet patterns(&getContext());\n     patterns.add(LowerAtomicWriteOp);\n+    patterns.add(LowerAtomicSpinWaitOp);\n     if (failed(applyPatternsGreedily(getOperation(), std::move(patterns)))) {\n       return signalPassFailure();\n     }"
        }
    ],
    "stats": {
        "total": 212,
        "additions": 200,
        "deletions": 12
    }
}