{
    "author": "EusebioDM",
    "message": "Move `GpuThunkGpuAotCompilationResult` to a new file, and rename it.\n\nMoving it out of the `gpu_compiler.cc` since that class is already huge.\n\nRenaming to legacy since it will be replaced the new result form the runtime split project, and removing the \"thunk\" part since this has nothing to do with thunks (unlike the new upcoming result).\n\nUnfortunately we had to create a `LoadExecutableFromAotResult` function in compiler.h to avoid a circular dependency in between `LegacyGpuAotCompilationResult` and `GpuCompiler`.\n\nAlso fixing some includes and braces along the way.\n\nPiperOrigin-RevId: 829362663",
    "sha": "b977bfa8de23c56017cb23cedb8f149a5d15303b",
    "files": [
        {
            "sha": "ab014eaa611de9e7aa46861f5332d5344c62b46e",
            "filename": "third_party/xla/xla/service/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b977bfa8de23c56017cb23cedb8f149a5d15303b/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b977bfa8de23c56017cb23cedb8f149a5d15303b/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2FBUILD?ref=b977bfa8de23c56017cb23cedb8f149a5d15303b",
            "patch": "@@ -1577,7 +1577,6 @@ cc_library(\n         \"//xla:shape_util\",\n         \"//xla:util\",\n         \"//xla/hlo/ir:hlo\",\n-        \"//xla/hlo/ir:hlo_module_group\",\n         \"//xla/pjrt/distributed:key_value_store_interface\",\n         \"//xla/stream_executor:device_description\",\n         \"//xla/stream_executor:device_memory_allocator\",\n@@ -1594,7 +1593,7 @@ cc_library(\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/synchronization\",\n-        \"@local_tsl//tsl/platform:protobuf\",\n+        \"@com_google_protobuf//:protobuf\",\n     ],\n )\n "
        },
        {
            "sha": "3253ca4666e11082c2f029a62a0b081101848c71",
            "filename": "third_party/xla/xla/service/compiler.h",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b977bfa8de23c56017cb23cedb8f149a5d15303b/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b977bfa8de23c56017cb23cedb8f149a5d15303b/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcompiler.h?ref=b977bfa8de23c56017cb23cedb8f149a5d15303b",
            "patch": "@@ -34,6 +34,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/synchronization/mutex.h\"\n+#include \"google/protobuf/message.h\"\n #include \"xla/debug_options_flags.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n@@ -53,7 +54,6 @@ limitations under the License.\n #include \"xla/stream_executor/stream_executor.h\"\n #include \"xla/tsl/platform/threadpool.h\"\n #include \"xla/util.h\"\n-#include \"tsl/platform/protobuf.h\"\n \n namespace mlir {\n class DialectRegistry;\n@@ -354,6 +354,13 @@ class Compiler {\n     return Unimplemented(\"DeserializeExecutable unimplemented\");\n   }\n \n+  // Creates an `Executable` based on the given `aot_result`.\n+  virtual absl::StatusOr<std::unique_ptr<Executable>>\n+  LoadExecutableFromAotResult(const AotCompilationResult& aot_result,\n+                              const se::StreamExecutor& stream_exec) {\n+    return Unimplemented(\"LoadExecutableFromAotResult unimplemented\");\n+  }\n+\n  private:\n   // Mutex that guards the platform-compiler map.\n   static absl::Mutex platform_compiler_mutex_;"
        },
        {
            "sha": "d479581259b2a04ce6ecadff0290e8045f01a5bc",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b977bfa8de23c56017cb23cedb8f149a5d15303b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b977bfa8de23c56017cb23cedb8f149a5d15303b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=b977bfa8de23c56017cb23cedb8f149a5d15303b",
            "patch": "@@ -1633,6 +1633,7 @@ cc_library(\n         \":ir_emitter_context\",\n         \":ir_emitter_unnested\",\n         \":kernel_reuse_cache\",\n+        \":legacy_gpu_aot_compilation_result\",\n         \":matmul_utils\",\n         \":metrics\",\n         \":pre_scheduling_copy_insertion_pipeline\",\n@@ -1997,6 +1998,34 @@ xla_test(\n     ],\n )\n \n+cc_library(\n+    name = \"legacy_gpu_aot_compilation_result\",\n+    srcs = [\"legacy_gpu_aot_compilation_result.cc\"],\n+    hdrs = [\"legacy_gpu_aot_compilation_result.h\"],\n+    # Explicitely restrict the visibility since this is an internal implementation detail of the\n+    # gpu_compiler.\n+    visibility = [],\n+    deps = [\n+        \":executable_proto_cc\",\n+        \":gpu_executable\",\n+        \":gpu_latency_hiding_scheduler\",\n+        \":ir_emission_utils\",\n+        \"//xla:util\",\n+        \"//xla/hlo/analysis:alias_info\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/service:buffer_assignment\",\n+        \"//xla/service:buffer_value\",\n+        \"//xla/service:compiler\",\n+        \"//xla/service:executable\",\n+        \"//xla/stream_executor:stream_executor_h\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_absl//absl/types:span\",\n+        \"@local_tsl//tsl/profiler/lib:traceme\",\n+    ],\n+)\n+\n xla_test(\n     name = \"gpu_offloading_test\",\n     srcs = [\"gpu_offloading_test.cc\"],"
        },
        {
            "sha": "4bfa8a8215c0055786eee0c2f61c6b1617a0d6e5",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.cc",
            "status": "modified",
            "additions": 108,
            "deletions": 192,
            "changes": 300,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b977bfa8de23c56017cb23cedb8f149a5d15303b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b977bfa8de23c56017cb23cedb8f149a5d15303b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.cc?ref=b977bfa8de23c56017cb23cedb8f149a5d15303b",
            "patch": "@@ -28,31 +28,25 @@ limitations under the License.\n \n #include \"absl/algorithm/container.h\"\n #include \"absl/base/call_once.h\"\n-#include \"absl/container/flat_hash_map.h\"\n-#include \"absl/container/flat_hash_set.h\"\n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n #include \"absl/status/status.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/synchronization/blocking_counter.h\"\n-#include \"absl/types/span.h\"\n-#include \"llvm/ADT/DenseMap.h\"\n #include \"llvm/ADT/SmallString.h\"\n #include \"llvm/ADT/StringRef.h\"\n #include \"llvm/AsmParser/Parser.h\"\n #include \"llvm/Bitcode/BitcodeReader.h\"\n #include \"llvm/Bitcode/BitcodeWriter.h\"\n-#include \"llvm/IR/Constants.h\"\n #include \"llvm/IR/DataLayout.h\"\n #include \"llvm/IR/DiagnosticInfo.h\"\n #include \"llvm/IR/DiagnosticPrinter.h\"\n #include \"llvm/IR/GlobalValue.h\"\n #include \"llvm/IR/LLVMContext.h\"\n #include \"llvm/IR/Module.h\"\n #include \"llvm/IR/Verifier.h\"\n-#include \"llvm/Support/Casting.h\"\n #include \"llvm/Support/Error.h\"\n #include \"llvm/Support/raw_ostream.h\"\n #include \"llvm/TargetParser/Triple.h\"\n@@ -61,7 +55,6 @@ limitations under the License.\n #include \"mlir/Support/LLVM.h\"\n #include \"google/protobuf/text_format.h\"\n #include \"xla/backends/cpu/nanort/nanort_client.h\"\n-#include \"xla/backends/cpu/nanort/nanort_executable.h\"\n #include \"xla/backends/gpu/codegen/triton/support.h\"\n #include \"xla/backends/gpu/runtime/host_execute_thunk.h\"\n #include \"xla/backends/gpu/runtime/runtime_intrinsics.h\"\n@@ -141,7 +134,6 @@ limitations under the License.\n #include \"xla/hlo/transforms/simplifiers/tuple_simplifier.h\"\n #include \"xla/hlo/transforms/simplifiers/zero_sized_hlo_elimination.h\"\n #include \"xla/hlo/transforms/while_loop_trip_count_annotator.h\"\n-#include \"xla/hlo/utils/hlo_traversal.h\"\n #include \"xla/maybe_owning.h\"\n #include \"xla/pjrt/proto/compile_options.pb.h\"\n #include \"xla/service/all_reduce_promotion.h\"\n@@ -152,7 +144,6 @@ limitations under the License.\n #include \"xla/service/buffer_assignment.h\"\n #include \"xla/service/buffer_value.h\"\n #include \"xla/service/call_inliner.h\"\n-#include \"xla/service/collective_ops_utils.h\"\n #include \"xla/service/collective_permute_decomposer.h\"\n #include \"xla/service/collective_pipeliner.h\"\n #include \"xla/service/collective_pipeliner_utils.h\"\n@@ -191,6 +182,7 @@ limitations under the License.\n #include \"xla/service/gpu/ir_emitter_context.h\"\n #include \"xla/service/gpu/ir_emitter_unnested.h\"\n #include \"xla/service/gpu/kernel_reuse_cache.h\"\n+#include \"xla/service/gpu/legacy_gpu_aot_compilation_result.h\"\n #include \"xla/service/gpu/matmul_utils.h\"\n #include \"xla/service/gpu/metrics.h\"\n #include \"xla/service/gpu/model/collective_ptable_stats_collection.h\"\n@@ -375,184 +367,7 @@ se::GpuComputeCapability GetGpuVersion(const se::StreamExecutor* stream_exec) {\n   return stream_exec->GetDeviceDescription().gpu_compute_capability();\n }\n \n-class GpuThunkAotCompilationResult : public AotCompilationResult {\n- public:\n-  static absl::StatusOr<std::unique_ptr<GpuThunkAotCompilationResult>>\n-  FromModule(const HloModule* hlo_module,\n-             const BufferAssignment* buffer_assignment,\n-             absl::string_view asm_text, absl::Span<const uint8_t> binary,\n-             const BinaryMap& dnn_compiled_graphs, int pointer_size) {\n-    tsl::profiler::TraceMe traceme(\"ResultFromModule\");\n-    CompilationResultProto proto;\n-    *proto.mutable_hlo_module_with_config() = hlo_module->ToProtoWithConfig();\n-    *proto.mutable_buffer_assignment() = buffer_assignment->ToProto();\n-    proto.set_asm_text(asm_text);\n-    proto.set_binary(binary.data(), binary.size());\n-    proto.mutable_dnn_compiled_graphs()->insert(dnn_compiled_graphs.cbegin(),\n-                                                dnn_compiled_graphs.cend());\n-    return std::unique_ptr<GpuThunkAotCompilationResult>(\n-        new GpuThunkAotCompilationResult(hlo_module->Clone(), std::move(proto),\n-                                         pointer_size));\n-  }\n-\n-  static absl::StatusOr<std::unique_ptr<GpuThunkAotCompilationResult>>\n-  FromString(const std::string& serialized, int pointer_size) {\n-    tsl::profiler::TraceMe traceme(\"ResultFromString\");\n-    CompilationResultProto proto;\n-    if (!proto.ParseFromString(serialized)) {\n-      return Internal(\n-          \"Failed to parse serialized GpuThunkAotCompilationResult.\");\n-    }\n-\n-    TF_ASSIGN_OR_RETURN(\n-        std::unique_ptr<HloModule> module,\n-        HloModule::CreateFromProtoWithConfig(proto.hlo_module_with_config()));\n-    return std::unique_ptr<GpuThunkAotCompilationResult>(\n-        new GpuThunkAotCompilationResult(std::move(module), std::move(proto),\n-                                         pointer_size));\n-  }\n-\n-  absl::StatusOr<std::string> SerializeAsString() const override {\n-    return proto_.SerializeAsString();\n-  }\n-\n-  absl::StatusOr<std::unique_ptr<Executable>> LoadExecutable(\n-      Compiler* compiler, const se::StreamExecutor* stream_exec) &&\n-      override;\n-\n-  const HloModule* optimized_module() const override { return module_.get(); }\n-  std::unique_ptr<HloModule> consume_optimized_module() override {\n-    return std::move(module_);\n-  }\n-\n-  absl::StatusOr<std::unique_ptr<BufferAssignment>> buffer_assignment()\n-      const override;\n-\n- private:\n-  GpuThunkAotCompilationResult(std::unique_ptr<HloModule> module,\n-                               CompilationResultProto proto, int pointer_size)\n-      : module_(std::move(module)),\n-        proto_(std::move(proto)),\n-        pointer_size_(pointer_size) {}\n-\n-  std::unique_ptr<HloModule> module_;\n-  CompilationResultProto proto_;\n-  int pointer_size_;\n-};\n-\n-}  // end anonymous namespace\n-\n-absl::StatusOr<std::unique_ptr<BufferAssignment>>\n-GpuThunkAotCompilationResult::buffer_assignment() const {\n-  auto buffer_size_bytes_function =\n-      [pointer_size = pointer_size_](const BufferValue& buffer) {\n-        return gpu::ShapeSizeBytesFunction(pointer_size)(buffer.shape());\n-      };\n-\n-  // Recreate BufferAssignment from proto.\n-  // Technically, we should pass the proper GpuAliasInfo, but the FromProto()\n-  // method does not actually make use of the MayAlias function. And for now, we\n-  // don't have backend-specific MustAlias rules.\n-  // TODO(b/424109294): This needs to be fixed when we implement\n-  // backend-specific MustAlias rules.\n-  AliasInfo alias_info;\n-  return BufferAssignment::FromProto(proto_.buffer_assignment(), module_.get(),\n-                                     buffer_size_bytes_function, &alias_info);\n-}\n-\n-absl::StatusOr<std::unique_ptr<Executable>>\n-GpuThunkAotCompilationResult::LoadExecutable(\n-    Compiler* compiler, const se::StreamExecutor* stream_exec) && {\n-  tsl::profiler::TraceMe traceme(\"LoadExecutable\");\n-  // Recreate HloModule+HloModuleConfig from proto.\n-  TF_ASSIGN_OR_RETURN(\n-      std::unique_ptr<HloModule> hlo_module,\n-      HloModule::CreateFromProtoWithConfig(proto_.hlo_module_with_config()));\n-\n-  ExecutionStreamAssignment execution_stream_assignment(hlo_module.get());\n-\n-  std::vector<uint8_t> binary(proto_.binary().begin(), proto_.binary().end());\n-\n-  // Build the executable, which should be a thunk sequence.\n-  TF_ASSIGN_OR_RETURN(\n-      se::Platform * platform,\n-      se::PlatformManager::PlatformWithId(compiler->PlatformId()));\n-  std::string platform_name = platform->Name();\n-  const se::DeviceDescription& gpu_device_info =\n-      stream_exec->GetDeviceDescription();\n-  llvm::LLVMContext llvm_context;\n-  auto* gpu_compiler = dynamic_cast<GpuCompiler*>(compiler);\n-  if (gpu_compiler == nullptr) {\n-    return Internal(\"Compiler is not a GpuCompiler.\");\n-  }\n-  auto llvm_module = std::make_unique<llvm::Module>(\"\", llvm_context);\n-  llvm_module->setTargetTriple(llvm::Triple(gpu_compiler->target_triple()));\n-  llvm_module->setDataLayout(gpu_compiler->data_layout());\n-\n-  // Recreate BufferAssignment from proto.\n-  std::unique_ptr<GpuAliasInfo> alias_info =\n-      gpu_compiler->GetAliasInfo(gpu_device_info);\n-  TF_ASSIGN_OR_RETURN(\n-      std::unique_ptr<BufferAssignment> buffer_assignment,\n-      BufferAssignment::FromProto(proto_.buffer_assignment(), hlo_module.get(),\n-                                  compiler->BufferSizeBytesFunction(),\n-                                  alias_info.get()));\n-\n-  IrEmitterContext ir_emitter_context(\n-      hlo_module.get(), buffer_assignment.get(), &execution_stream_assignment,\n-      platform_name, gpu_device_info, gpu_compiler->symbolic_expr_context(),\n-      llvm_module.get(),\n-      /*llvm_module_constants=*/nullptr,\n-      /*emit_kernels=*/false);\n-\n-  absl::string_view cache_file_path =\n-      hlo_module->config().debug_options().xla_gpu_kernel_cache_file();\n-  if (!cache_file_path.empty() &&\n-      hlo_module->config()\n-          .debug_options()\n-          .xla_gpu_enable_llvm_module_compilation_parallelism()) {\n-    TF_RETURN_IF_ERROR(LoadCache(ir_emitter_context, cache_file_path));\n-  }\n-\n-  auto ir_emitter = IrEmitterUnnested::Create(&ir_emitter_context);\n-  TF_RETURN_IF_ERROR(\n-      ir_emitter->EmitHloComputation(hlo_module->entry_computation()));\n-\n-  // Get all other fields required by GpuExecutable.\n-  std::vector<GpuExecutable::ConstantInfo> constants =\n-      std::move(ir_emitter_context.constants());\n-  TF_ASSIGN_OR_RETURN(auto output_info,\n-                      GetOutputInfo(*hlo_module, *buffer_assignment));\n-  ProgramShape program_shape =\n-      hlo_module->entry_computation_layout().ComputeProgramShape();\n-  *program_shape.mutable_result() = hlo_module->result_shape();\n-  DebugOptions debug_options = hlo_module->config().debug_options();\n-  std::string hlo_module_name = hlo_module->name();\n-\n-  {\n-    tsl::profiler::TraceMe traceme(\"CreateGpuExecutable\");\n-    std::unique_ptr<GpuAliasInfo> alias_info =\n-        gpu_compiler->GetAliasInfo(gpu_device_info);\n-    return GpuExecutable::Create(GpuExecutable::Params{\n-        /*asm_text=*/proto_.asm_text(),\n-        /*binary=*/binary,\n-        /*dnn_compiled_graphs=*/\n-        BinaryMap(proto_.dnn_compiled_graphs().cbegin(),\n-                  proto_.dnn_compiled_graphs().cend()),\n-        /*executable=*/ir_emitter->ConsumeThunkSequence(),\n-        /*constants=*/std::move(constants),\n-        /*output_info=*/std::move(output_info),\n-        /*module_name=*/std::move(hlo_module_name),\n-        /*program_shape=*/std::move(program_shape),\n-        /*mlir_allocations=*/std::nullopt,\n-        /*buffer_assignment=*/std::move(buffer_assignment),\n-        /*alias_info=*/std::move(alias_info),\n-        /*debug_options=*/std::move(debug_options),\n-        /*device_description=*/gpu_device_info,\n-        /*debug_module=*/std::move(hlo_module),\n-        /*enable_debug_info_manager=*/true});\n-  }\n-}\n+}  // namespace\n \n GpuCompiler::GpuCompiler(se::Platform::Id platform_id,\n                          const char* target_triple, const char* data_layout)\n@@ -2872,7 +2687,7 @@ GpuCompiler::CompileAheadOfTime(std::unique_ptr<HloModule> hlo_module,\n   // Create GpuThunkAotCompilationResult if thunk runtime is enabled.\n   TF_ASSIGN_OR_RETURN(\n       results.emplace_back(),\n-      GpuThunkAotCompilationResult::FromModule(\n+      LegacyGpuAotCompilationResult::FromModule(\n           optimized_module.get(),\n           res.compile_module_results.buffer_assignment.get(),\n           res.backend_result.asm_text, res.backend_result.binary,\n@@ -2889,9 +2704,11 @@ HloCostAnalysis::ShapeSizeFunction GpuCompiler::ShapeSizeBytesFunction() const {\n absl::StatusOr<std::unique_ptr<AotCompilationResult>> GpuCompiler::Export(\n     Executable* executable) const {\n   auto* gpu_executable = tensorflow::down_cast<GpuExecutable*>(executable);\n-  if (!gpu_executable) return Internal(\"GpuExecutable is null\");\n+  if (!gpu_executable) {\n+    return Internal(\"GpuExecutable is null\");\n+  }\n \n-  return GpuThunkAotCompilationResult::FromModule(\n+  return LegacyGpuAotCompilationResult::FromModule(\n       &gpu_executable->module(), gpu_executable->buffer_assignment(),\n       gpu_executable->text(), gpu_executable->binary(),\n       gpu_executable->dnn_compiled_graphs(), pointer_size_);\n@@ -3130,8 +2947,107 @@ absl::Status GpuCompiler::SerializeAutotuneResultsToFile(\n absl::StatusOr<std::unique_ptr<AotCompilationResult>>\n GpuCompiler::LoadAotCompilationResult(\n     const std::string& serialized_aot_result) {\n-  return GpuThunkAotCompilationResult::FromString(serialized_aot_result,\n-                                                  pointer_size_);\n+  return LegacyGpuAotCompilationResult::FromString(serialized_aot_result,\n+                                                   pointer_size_);\n+}\n+\n+absl::StatusOr<std::unique_ptr<Executable>>\n+GpuCompiler::LoadExecutableFromAotResult(\n+    const AotCompilationResult& aot_result,\n+    const se::StreamExecutor& stream_exec) {\n+  tsl::profiler::TraceMe traceme(\"LoadExecutableFromAotResult\");\n+\n+  const auto* gpu_aot_result =\n+      dynamic_cast<const LegacyGpuAotCompilationResult*>(&aot_result);\n+  if (gpu_aot_result == nullptr) {\n+    return Internal(\n+        \"AotCompilationResult is not a GpuThunkAotCompilationResult.\");\n+  }\n+  const CompilationResultProto& proto =\n+      gpu_aot_result->GetCompilationResultProto();\n+\n+  // Recreate HloModule+HloModuleConfig from proto.\n+  TF_ASSIGN_OR_RETURN(\n+      std::unique_ptr<HloModule> hlo_module,\n+      HloModule::CreateFromProtoWithConfig(proto.hlo_module_with_config()));\n+\n+  ExecutionStreamAssignment execution_stream_assignment(hlo_module.get());\n+\n+  std::vector<uint8_t> binary(proto.binary().begin(), proto.binary().end());\n+\n+  // Build the executable, which should be a thunk sequence.\n+  TF_ASSIGN_OR_RETURN(se::Platform * platform,\n+                      se::PlatformManager::PlatformWithId(PlatformId()));\n+  std::string platform_name = platform->Name();\n+\n+  const se::DeviceDescription& gpu_device_info =\n+      stream_exec.GetDeviceDescription();\n+  llvm::LLVMContext llvm_context;\n+\n+  auto llvm_module = std::make_unique<llvm::Module>(\"\", llvm_context);\n+  llvm_module->setTargetTriple(llvm::Triple(target_triple()));\n+  llvm_module->setDataLayout(data_layout());\n+\n+  // Recreate BufferAssignment from proto.\n+  std::unique_ptr<GpuAliasInfo> alias_info = GetAliasInfo(gpu_device_info);\n+  TF_ASSIGN_OR_RETURN(\n+      std::unique_ptr<BufferAssignment> buffer_assignment,\n+      BufferAssignment::FromProto(proto.buffer_assignment(), hlo_module.get(),\n+                                  BufferSizeBytesFunction(), alias_info.get()));\n+\n+  IrEmitterContext ir_emitter_context(\n+      hlo_module.get(), buffer_assignment.get(), &execution_stream_assignment,\n+      platform_name, gpu_device_info, symbolic_expr_context(),\n+      llvm_module.get(),\n+      /*llvm_module_constants=*/nullptr,\n+      /*emit_kernels=*/false);\n+\n+  absl::string_view cache_file_path =\n+      hlo_module->config().debug_options().xla_gpu_kernel_cache_file();\n+  if (!cache_file_path.empty() &&\n+      hlo_module->config()\n+          .debug_options()\n+          .xla_gpu_enable_llvm_module_compilation_parallelism()) {\n+    TF_RETURN_IF_ERROR(LoadCache(ir_emitter_context, cache_file_path));\n+  }\n+\n+  auto ir_emitter = IrEmitterUnnested::Create(&ir_emitter_context);\n+  TF_RETURN_IF_ERROR(\n+      ir_emitter->EmitHloComputation(hlo_module->entry_computation()));\n+\n+  // Get all other fields required by GpuExecutable.\n+  std::vector<GpuExecutable::ConstantInfo> constants =\n+      std::move(ir_emitter_context.constants());\n+  TF_ASSIGN_OR_RETURN(auto output_info,\n+                      GetOutputInfo(*hlo_module, *buffer_assignment));\n+  ProgramShape program_shape =\n+      hlo_module->entry_computation_layout().ComputeProgramShape();\n+  *program_shape.mutable_result() = hlo_module->result_shape();\n+  DebugOptions debug_options = hlo_module->config().debug_options();\n+  std::string hlo_module_name = hlo_module->name();\n+\n+  {\n+    tsl::profiler::TraceMe traceme(\"CreateGpuExecutable\");\n+    std::unique_ptr<GpuAliasInfo> alias_info = GetAliasInfo(gpu_device_info);\n+    return GpuExecutable::Create(GpuExecutable::Params{\n+        /*asm_text=*/proto.asm_text(),\n+        /*binary=*/binary,\n+        /*dnn_compiled_graphs=*/\n+        BinaryMap(proto.dnn_compiled_graphs().cbegin(),\n+                  proto.dnn_compiled_graphs().cend()),\n+        /*executable=*/ir_emitter->ConsumeThunkSequence(),\n+        /*constants=*/std::move(constants),\n+        /*output_info=*/std::move(output_info),\n+        /*module_name=*/std::move(hlo_module_name),\n+        /*program_shape=*/std::move(program_shape),\n+        /*mlir_allocations=*/std::nullopt,\n+        /*buffer_assignment=*/std::move(buffer_assignment),\n+        /*alias_info=*/std::move(alias_info),\n+        /*debug_options=*/std::move(debug_options),\n+        /*device_description=*/gpu_device_info,\n+        /*debug_module=*/std::move(hlo_module),\n+        /*enable_debug_info_manager=*/true});\n+  }\n }\n \n }  // namespace gpu"
        },
        {
            "sha": "c2ab6c14f810a28ded18f3f4dd4f7ee447ca0fa0",
            "filename": "third_party/xla/xla/service/gpu/gpu_compiler.h",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b977bfa8de23c56017cb23cedb8f149a5d15303b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b977bfa8de23c56017cb23cedb8f149a5d15303b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_compiler.h?ref=b977bfa8de23c56017cb23cedb8f149a5d15303b",
            "patch": "@@ -139,6 +139,10 @@ class GpuCompiler : public LLVMCompiler {\n       AlgebraicSimplifierMode mode, const DebugOptions& debug_options,\n       bool is_rocm);\n \n+  absl::StatusOr<std::unique_ptr<Executable>> LoadExecutableFromAotResult(\n+      const AotCompilationResult& aot_result,\n+      const se::StreamExecutor& stream_exec) override;\n+\n  protected:\n   struct BackendCompileResult {\n     std::string asm_text;"
        },
        {
            "sha": "3c5aee130f4856b3d4f7f9e61c194c4f260ae7f6",
            "filename": "third_party/xla/xla/service/gpu/legacy_gpu_aot_compilation_result.cc",
            "status": "added",
            "additions": 112,
            "deletions": 0,
            "changes": 112,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b977bfa8de23c56017cb23cedb8f149a5d15303b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Flegacy_gpu_aot_compilation_result.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b977bfa8de23c56017cb23cedb8f149a5d15303b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Flegacy_gpu_aot_compilation_result.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Flegacy_gpu_aot_compilation_result.cc?ref=b977bfa8de23c56017cb23cedb8f149a5d15303b",
            "patch": "@@ -0,0 +1,112 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/service/gpu/legacy_gpu_aot_compilation_result.h\"\n+\n+#include <cstdint>\n+#include <memory>\n+#include <string>\n+#include <utility>\n+\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/hlo/analysis/alias_info.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/service/buffer_assignment.h\"\n+#include \"xla/service/buffer_value.h\"\n+#include \"xla/service/compiler.h\"\n+#include \"xla/service/executable.h\"\n+#include \"xla/service/gpu/executable.pb.h\"\n+#include \"xla/service/gpu/gpu_latency_hiding_scheduler.h\"\n+#include \"xla/service/gpu/ir_emission_utils.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+#include \"xla/util.h\"\n+#include \"tsl/profiler/lib/traceme.h\"\n+\n+namespace xla {\n+namespace gpu {\n+\n+absl::StatusOr<std::unique_ptr<LegacyGpuAotCompilationResult>>\n+LegacyGpuAotCompilationResult::FromModule(\n+    const HloModule* hlo_module, const BufferAssignment* buffer_assignment,\n+    absl::string_view asm_text, absl::Span<const uint8_t> binary,\n+    const BinaryMap& dnn_compiled_graphs, int pointer_size) {\n+  tsl::profiler::TraceMe traceme(\"ResultFromModule\");\n+  CompilationResultProto proto;\n+  *proto.mutable_hlo_module_with_config() = hlo_module->ToProtoWithConfig();\n+  *proto.mutable_buffer_assignment() = buffer_assignment->ToProto();\n+  proto.set_asm_text(asm_text);\n+  proto.set_binary(binary.data(), binary.size());\n+  proto.mutable_dnn_compiled_graphs()->insert(dnn_compiled_graphs.cbegin(),\n+                                              dnn_compiled_graphs.cend());\n+  return std::unique_ptr<LegacyGpuAotCompilationResult>(\n+      new LegacyGpuAotCompilationResult(hlo_module->Clone(), std::move(proto),\n+                                        pointer_size));\n+}\n+\n+absl::StatusOr<std::unique_ptr<LegacyGpuAotCompilationResult>>\n+LegacyGpuAotCompilationResult::FromString(const std::string& serialized,\n+                                          int pointer_size) {\n+  tsl::profiler::TraceMe traceme(\"ResultFromString\");\n+  CompilationResultProto proto;\n+  if (!proto.ParseFromString(serialized)) {\n+    return Internal(\"Failed to parse serialized GpuThunkAotCompilationResult.\");\n+  }\n+\n+  TF_ASSIGN_OR_RETURN(\n+      std::unique_ptr<HloModule> module,\n+      HloModule::CreateFromProtoWithConfig(proto.hlo_module_with_config()));\n+  return std::unique_ptr<LegacyGpuAotCompilationResult>(\n+      new LegacyGpuAotCompilationResult(std::move(module), std::move(proto),\n+                                        pointer_size));\n+}\n+\n+absl::StatusOr<std::string> LegacyGpuAotCompilationResult::SerializeAsString()\n+    const {\n+  return proto_.SerializeAsString();\n+}\n+\n+absl::StatusOr<std::unique_ptr<Executable>>\n+LegacyGpuAotCompilationResult::LoadExecutable(\n+    Compiler* compiler, const se::StreamExecutor* stream_exec) && {\n+  if (stream_exec == nullptr) {\n+    return InvalidArgument(\"Stream executor is null.\");\n+  }\n+\n+  return compiler->LoadExecutableFromAotResult(*this, *stream_exec);\n+}\n+\n+absl::StatusOr<std::unique_ptr<BufferAssignment>>\n+LegacyGpuAotCompilationResult::buffer_assignment() const {\n+  auto buffer_size_bytes_function =\n+      [pointer_size = pointer_size_](const BufferValue& buffer) {\n+        return gpu::ShapeSizeBytesFunction(pointer_size)(buffer.shape());\n+      };\n+\n+  // Recreate BufferAssignment from proto.\n+  // Technically, we should pass the proper GpuAliasInfo, but the FromProto()\n+  // method does not actually make use of the MayAlias function. And for now, we\n+  // don't have backend-specific MustAlias rules.\n+  // TODO(b/424109294): This needs to be fixed when we implement\n+  // backend-specific MustAlias rules.\n+  AliasInfo alias_info;\n+  return BufferAssignment::FromProto(proto_.buffer_assignment(), module_.get(),\n+                                     buffer_size_bytes_function, &alias_info);\n+}\n+\n+}  // namespace gpu\n+}  // namespace xla"
        },
        {
            "sha": "9b785aae5ee2c2e6a5d5d9351178b15e006f7d2d",
            "filename": "third_party/xla/xla/service/gpu/legacy_gpu_aot_compilation_result.h",
            "status": "added",
            "additions": 91,
            "deletions": 0,
            "changes": 91,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b977bfa8de23c56017cb23cedb8f149a5d15303b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Flegacy_gpu_aot_compilation_result.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b977bfa8de23c56017cb23cedb8f149a5d15303b/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Flegacy_gpu_aot_compilation_result.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Flegacy_gpu_aot_compilation_result.h?ref=b977bfa8de23c56017cb23cedb8f149a5d15303b",
            "patch": "@@ -0,0 +1,91 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_SERVICE_GPU_LEGACY_GPU_AOT_COMPILATION_RESULT_H_\n+#define XLA_SERVICE_GPU_LEGACY_GPU_AOT_COMPILATION_RESULT_H_\n+\n+#include <cstdint>\n+#include <memory>\n+#include <string>\n+#include <utility>\n+\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/service/buffer_assignment.h\"\n+#include \"xla/service/compiler.h\"\n+#include \"xla/service/executable.h\"\n+#include \"xla/service/gpu/executable.pb.h\"\n+#include \"xla/service/gpu/ir_emission_utils.h\"\n+#include \"xla/stream_executor/stream_executor.h\"\n+\n+namespace xla {\n+namespace gpu {\n+\n+// Represents the legacy result of a GPU AOT compilation.\n+//\n+// This result primarily contains the optimized HLO module. Executables loaded\n+// from this result can bypass the HLO optimization passes, since this result\n+// already contains the optimized HLO.\n+//\n+// This class is considered legacy and is expected to be replaced by a\n+// new AOT result type as part of the runtime split. The new type will\n+// encapsulate the compilation up to the Thunks generation stage.\n+class LegacyGpuAotCompilationResult : public AotCompilationResult {\n+ public:\n+  static absl::StatusOr<std::unique_ptr<LegacyGpuAotCompilationResult>>\n+  FromModule(const HloModule* hlo_module,\n+             const BufferAssignment* buffer_assignment,\n+             absl::string_view asm_text, absl::Span<const uint8_t> binary,\n+             const BinaryMap& dnn_compiled_graphs, int pointer_size);\n+\n+  static absl::StatusOr<std::unique_ptr<LegacyGpuAotCompilationResult>>\n+  FromString(const std::string& serialized, int pointer_size);\n+\n+  absl::StatusOr<std::string> SerializeAsString() const override;\n+\n+  absl::StatusOr<std::unique_ptr<Executable>> LoadExecutable(\n+      Compiler* compiler, const se::StreamExecutor* stream_exec) &&\n+      override;\n+\n+  const HloModule* optimized_module() const override { return module_.get(); }\n+  std::unique_ptr<HloModule> consume_optimized_module() override {\n+    return std::move(module_);\n+  }\n+\n+  absl::StatusOr<std::unique_ptr<BufferAssignment>> buffer_assignment()\n+      const override;\n+\n+  const CompilationResultProto& GetCompilationResultProto() const {\n+    return proto_;\n+  }\n+\n+ private:\n+  LegacyGpuAotCompilationResult(std::unique_ptr<HloModule> module,\n+                                CompilationResultProto proto, int pointer_size)\n+      : module_(std::move(module)),\n+        proto_(std::move(proto)),\n+        pointer_size_(pointer_size) {}\n+\n+  std::unique_ptr<HloModule> module_;\n+  CompilationResultProto proto_;\n+  int pointer_size_;\n+};\n+\n+}  // namespace gpu\n+}  // namespace xla\n+\n+#endif  // XLA_SERVICE_GPU_LEGACY_GPU_AOT_COMPILATION_RESULT_H_"
        }
    ],
    "stats": {
        "total": 548,
        "additions": 353,
        "deletions": 195
    }
}