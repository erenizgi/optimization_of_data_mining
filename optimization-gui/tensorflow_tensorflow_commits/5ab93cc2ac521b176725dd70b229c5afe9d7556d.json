{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 805224087",
    "sha": "5ab93cc2ac521b176725dd70b229c5afe9d7556d",
    "files": [
        {
            "sha": "97dbf9845fe2f6049fc2a8c33ce5236175c2112e",
            "filename": "third_party/xla/xla/service/hlo_creation_utils.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5ab93cc2ac521b176725dd70b229c5afe9d7556d/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_creation_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5ab93cc2ac521b176725dd70b229c5afe9d7556d/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_creation_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fhlo_creation_utils.cc?ref=5ab93cc2ac521b176725dd70b229c5afe9d7556d",
            "patch": "@@ -536,7 +536,7 @@ absl::StatusOr<HloInstruction*> MakeReduceHlo(\n   CHECK_EQ(operands.size(), init_values.size());\n   auto root = reduce_computation->root_instruction();\n   if (root->shape().IsTuple()) {\n-    CHECK_EQ(root->shape().tuple_shapes_size(), operands.size());\n+    CHECK_EQ(root->shape().tuple_shapes().size(), operands.size());\n   } else {\n     CHECK_EQ(operands.size(), 1);\n   }"
        },
        {
            "sha": "6bd77349f884edef90c21a2904e6facfc40f0aea",
            "filename": "third_party/xla/xla/service/shape_inference.cc",
            "status": "modified",
            "additions": 40,
            "deletions": 37,
            "changes": 77,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5ab93cc2ac521b176725dd70b229c5afe9d7556d/third_party%2Fxla%2Fxla%2Fservice%2Fshape_inference.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5ab93cc2ac521b176725dd70b229c5afe9d7556d/third_party%2Fxla%2Fxla%2Fservice%2Fshape_inference.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fshape_inference.cc?ref=5ab93cc2ac521b176725dd70b229c5afe9d7556d",
            "patch": "@@ -2411,14 +2411,14 @@ ShapeInference::InferScalarBroadcastShape(absl::Span<const Shape> shapes) {\n       RET_CHECK_RANK(in);\n       for (int i = 0; i < fft_rank; i++) {\n         if (!IsUnboundedDynamicSize(\n-                in.dimensions(in.dimensions_size() - fft_rank + i)) &&\n-            in.dimensions(in.dimensions_size() - fft_rank + i) !=\n+                in.dimensions(in.dimensions().size() - fft_rank + i)) &&\n+            in.dimensions(in.dimensions().size() - fft_rank + i) !=\n                 fft_length[i]) {\n           return InvalidArgument(\n               \"RFFT requires innermost dimensions match fft_length but \"\n               \"dimension %d is %d and should be %d.\",\n-              in.dimensions_size() - fft_rank + i,\n-              in.dimensions(in.dimensions_size() - fft_rank + i),\n+              in.dimensions().size() - fft_rank + i,\n+              in.dimensions(in.dimensions().size() - fft_rank + i),\n               fft_length[i]);\n         }\n       }\n@@ -2442,25 +2442,26 @@ ShapeInference::InferScalarBroadcastShape(absl::Span<const Shape> shapes) {\n       RET_CHECK_RANK(in);\n       Shape result = ShapeUtil::ComplexComponentShape(in);\n       for (int i = 0; i < fft_rank - 1; i++) {\n-        if (in.dimensions(in.dimensions_size() - fft_rank + i) !=\n+        if (in.dimensions(in.dimensions().size() - fft_rank + i) !=\n             fft_length[i]) {\n           return InvalidArgument(\n               \"IRFFT requires all but one innermost dimensions match \"\n               \"fft_length, but dimension %d is %d and should be %d.\",\n-              in.dimensions_size() - fft_rank + i,\n-              in.dimensions(in.dimensions_size() - fft_rank + i),\n+              in.dimensions().size() - fft_rank + i,\n+              in.dimensions(in.dimensions().size() - fft_rank + i),\n               fft_length[i]);\n         }\n       }\n       // The size of zero-sized dimensions is preserved.\n-      int64_t last_in_dimension_size = in.dimensions(in.dimensions_size() - 1);\n+      int64_t last_in_dimension_size =\n+          in.dimensions(in.dimensions().size() - 1);\n       if ((last_in_dimension_size != 0 || fft_length[fft_rank - 1] != 0) &&\n           !IsUnboundedDynamicSize(last_in_dimension_size) &&\n           last_in_dimension_size != fft_length[fft_rank - 1] / 2 + 1) {\n         return InvalidArgument(\n             \"IRFFT requires innermost dimension matches fft_length/2+1, but \"\n             \"dimension %d is %d and should be %d.\",\n-            in.dimensions_size() - 1, last_in_dimension_size,\n+            in.dimensions().size() - 1, last_in_dimension_size,\n             fft_length[fft_rank - 1] / 2 + 1);\n       }\n       const int dim = static_cast<int>(result.dimensions().size()) - 1;\n@@ -2485,25 +2486,25 @@ ShapeInference::InferScalarBroadcastShape(absl::Span<const Shape> shapes) {\n         PrimitiveType_Name(a.element_type()),\n         PrimitiveType_Name(b.element_type()));\n   }\n-  if (a.dimensions_size() < 2) {\n+  if (a.dimensions().size() < 2) {\n     return InvalidArgument(\n         \"The 'a' argument to TriangularSolve must have rank >= 2, got shape %s\",\n         a.ToString());\n   }\n-  if (b.dimensions_size() != a.dimensions_size()) {\n+  if (b.dimensions().size() != a.dimensions().size()) {\n     return InvalidArgument(\n         \"Arguments to triangular solve must have equal rank; got %s and %s.\",\n         b.ToString(), a.ToString());\n   }\n-  if (!CompatibleDimensionSizes(a.dimensions(a.dimensions_size() - 2),\n-                                a.dimensions(a.dimensions_size() - 1))) {\n+  if (!CompatibleDimensionSizes(a.dimensions(a.dimensions().size() - 2),\n+                                a.dimensions(a.dimensions().size() - 1))) {\n     return InvalidArgument(\n         \"The two minor dimensions of 'a' must have equal size, got %s.\",\n         a.ToString());\n   }\n-  if (!CompatibleDimensionSizes(\n-          a.dimensions(a.dimensions_size() - 1),\n-          b.dimensions(b.dimensions_size() - (options.left_side() ? 2 : 1)))) {\n+  if (!CompatibleDimensionSizes(a.dimensions(a.dimensions().size() - 1),\n+                                b.dimensions(b.dimensions().size() -\n+                                             (options.left_side() ? 2 : 1)))) {\n     return InvalidArgument(\n         \"The shared dimension of 'a' and 'b' does not match, got shapes %s and \"\n         \"%s\",\n@@ -2536,13 +2537,13 @@ ShapeInference::InferScalarBroadcastShape(absl::Span<const Shape> shapes) {\n         \"Cholesky; got %s.\",\n         PrimitiveType_Name(a.element_type()));\n   }\n-  if (a.dimensions_size() < 2) {\n+  if (a.dimensions().size() < 2) {\n     return InvalidArgument(\n         \"The 'a' argument to Cholesky must have rank >= 2, got shape %s\",\n         a.ToString());\n   }\n-  if (!CompatibleDimensionSizes(a.dimensions(a.dimensions_size() - 2),\n-                                a.dimensions(a.dimensions_size() - 1))) {\n+  if (!CompatibleDimensionSizes(a.dimensions(a.dimensions().size() - 2),\n+                                a.dimensions(a.dimensions().size() - 1))) {\n     return InvalidArgument(\n         \"The two minor dimensions of 'a' must have compatible size, got %s.\",\n         a.ToString());\n@@ -2819,7 +2820,7 @@ ShapeInference::InferCollectivePermuteDoneShape(const Shape& operand_shape) {\n   // doesn't matter which one we choose.\n   const Shape& arg = *reduced_args[0];\n   for (int64_t dimension : dimensions_to_reduce) {\n-    if (dimension >= arg.dimensions_size() || dimension < 0) {\n+    if (dimension >= arg.dimensions().size() || dimension < 0) {\n       return InvalidArgument(\"Reducing out-of-bounds dimension %d in shape %s.\",\n                              dimension, ShapeUtil::HumanString(arg));\n     }\n@@ -2844,7 +2845,7 @@ ShapeInference::InferCollectivePermuteDoneShape(const Shape& operand_shape) {\n \n   std::vector<int64_t> new_dimensions;\n   std::vector<bool> new_is_dynamic;\n-  for (int i = 0; i < arg.dimensions_size(); ++i) {\n+  for (int i = 0; i < arg.dimensions().size(); ++i) {\n     if (dimensions_to_reduce_set.find(i) == dimensions_to_reduce_set.end()) {\n       new_dimensions.push_back(arg.dimensions(i));\n       new_is_dynamic.push_back(arg.is_dynamic_dimension(i));\n@@ -3116,10 +3117,10 @@ ShapeInference::InferCollectivePermuteDoneShape(const Shape& operand_shape) {\n                            starts.size(), strides.size()));\n   }\n \n-  if (starts.size() != arg.dimensions_size()) {\n+  if (starts.size() != arg.dimensions().size()) {\n     return InvalidArgument(\n         \"Slice index count does not match argument rank: %u vs %d.\",\n-        starts.size(), arg.dimensions_size());\n+        starts.size(), arg.dimensions().size());\n   }\n \n   std::vector<int64_t> sizes;\n@@ -3154,8 +3155,8 @@ ShapeInference::InferCollectivePermuteDoneShape(const Shape& operand_shape) {\n     sizes.push_back((limit_index - start_index + stride - 1) / stride);\n   }\n \n-  std::vector<bool> is_dynamic(arg.dimensions_size());\n-  for (int64_t i = 0; i < arg.dimensions_size(); ++i) {\n+  std::vector<bool> is_dynamic(arg.dimensions().size());\n+  for (int64_t i = 0; i < arg.dimensions().size(); ++i) {\n     // Slicing 1 out of a dynamic dimension eliminates the dynamic dimension.\n     if (sizes[i] == 1) {\n       continue;\n@@ -3172,8 +3173,9 @@ ShapeInference::InferCollectivePermuteDoneShape(const Shape& operand_shape) {\n   TF_RETURN_IF_ERROR(ExpectArray(operand_shape, \"operand of dynamic slice\"));\n   auto number_of_indices = start_index_shapes.size();\n   // TODO(b/118437727): Remove this path.\n-  if (!allow_scalar_indices || (number_of_indices >= 1 &&\n-                                start_index_shapes[0].dimensions_size() == 1)) {\n+  if (!allow_scalar_indices ||\n+      (number_of_indices >= 1 &&\n+       start_index_shapes[0].dimensions().size() == 1)) {\n     if (number_of_indices != 1) {\n       return InvalidArgument(\n           \"Dynamic slice should have exactly 1 index operand, has %d.\",\n@@ -3292,8 +3294,9 @@ ShapeInference::InferCollectivePermuteDoneShape(const Shape& operand_shape) {\n \n   auto number_of_indices = start_index_shapes.size();\n   // TODO(b/118437727): Remove this path.\n-  if (!allow_scalar_indices || (number_of_indices >= 1 &&\n-                                start_index_shapes[0].dimensions_size() == 1)) {\n+  if (!allow_scalar_indices ||\n+      (number_of_indices >= 1 &&\n+       start_index_shapes[0].dimensions().size() == 1)) {\n     if (number_of_indices != 1) {\n       return InvalidArgument(\n           \"Dynamic update slice should have exactly 1 index operand, has %d.\",\n@@ -3444,11 +3447,11 @@ ShapeInference::InferCollectivePermuteDoneShape(const Shape& operand_shape) {\n         ShapeUtil::HumanString(arg));\n   }\n \n-  if (index < 0 || index >= arg.tuple_shapes_size()) {\n+  if (index < 0 || index >= arg.tuple_shapes().size()) {\n     return InvalidArgument(\n         \"Cannot infer shape: attempt to index out of tuple bounds: %d \"\n         \">= %d in shape %s.\",\n-        index, arg.tuple_shapes_size(), ShapeUtil::HumanString(arg));\n+        index, arg.tuple_shapes().size(), ShapeUtil::HumanString(arg));\n   }\n \n   return arg.tuple_shapes(index);\n@@ -3581,15 +3584,15 @@ ShapeInference::InferCollectivePermuteDoneShape(const Shape& operand_shape) {\n     }\n   }\n \n-  std::vector<int64_t> dimensions(operand.dimensions_size() +\n+  std::vector<int64_t> dimensions(operand.dimensions().size() +\n                                   broadcast_sizes.size());\n   std::copy(broadcast_sizes.begin(), broadcast_sizes.end(), dimensions.begin());\n   std::copy(operand.dimensions().begin(), operand.dimensions().end(),\n             dimensions.begin() + broadcast_sizes.size());\n \n   TF_ASSIGN_OR_RETURN(Shape result, ShapeUtil::MakeValidatedShape(\n                                         operand.element_type(), dimensions));\n-  for (int64_t i = 0; i < operand.dimensions_size(); ++i) {\n+  for (int64_t i = 0; i < operand.dimensions().size(); ++i) {\n     result.set_dynamic_dimension(broadcast_sizes.size() + i,\n                                  operand.is_dynamic_dimension(i));\n   }\n@@ -3712,12 +3715,12 @@ ShapeInference::InferCollectivePermuteDoneShape(const Shape& operand_shape) {\n         ShapeUtil::HumanString(inferred_shape));\n   }\n \n-  std::vector<int64_t> indices(operand.dimensions_size());\n+  std::vector<int64_t> indices(operand.dimensions().size());\n   std::iota(indices.begin(), indices.end(), 0);\n \n   // Propagate dynamic dimension.\n   auto common_factors = CommonFactors(operand.dimensions(), dimensions);\n-  for (int64_t input_dim = 0; input_dim < operand.dimensions_size();\n+  for (int64_t input_dim = 0; input_dim < operand.dimensions().size();\n        ++input_dim) {\n     if (!operand.is_dynamic_dimension(input_dim)) {\n       continue;\n@@ -3795,7 +3798,7 @@ ShapeInference::InferCollectivePermuteDoneShape(const Shape& operand_shape) {\n       if (input_dim == 0) {\n         output_dynamic_dimension = 0;\n       }\n-      if (input_dim == operand.dimensions_size() - 1) {\n+      if (input_dim == operand.dimensions().size() - 1) {\n         output_dynamic_dimension = dimensions.size() - 1;\n       }\n \n@@ -3842,7 +3845,7 @@ ShapeInference::InferCollectivePermuteDoneShape(const Shape& operand_shape) {\n     const Shape& operand, absl::Span<const int64_t> dimensions) {\n   TF_RETURN_IF_ERROR(ExpectArray(operand, \"transpose\"));\n \n-  if (dimensions.size() != operand.dimensions_size() ||\n+  if (dimensions.size() != operand.dimensions().size() ||\n       !IsPermutation(dimensions)) {\n     return InvalidArgument(\n         \"Transpose dimensions [%s] are not a permutation of the operand \""
        }
    ],
    "stats": {
        "total": 79,
        "additions": 41,
        "deletions": 38
    }
}