{
    "author": "GleasonK",
    "message": "[StableHLO Optim] Coalesce adjacent concatenated splats\n\n`Pattern: concat(splat_a, splat_a, X) -> concat(splat_a_resize, X)`\n\nPiperOrigin-RevId: 829822831",
    "sha": "5fb239b99374207bd3d0c9bafd806265bc0c510d",
    "files": [
        {
            "sha": "419fd26060b2ff632ef978b365b2fe623fe7244b",
            "filename": "third_party/xla/third_party/stablehlo/temporary.patch",
            "status": "modified",
            "additions": 154,
            "deletions": 2,
            "changes": 156,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/5fb239b99374207bd3d0c9bafd806265bc0c510d/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/5fb239b99374207bd3d0c9bafd806265bc0c510d/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fstablehlo%2Ftemporary.patch?ref=5fb239b99374207bd3d0c9bafd806265bc0c510d",
            "patch": "@@ -512,7 +512,52 @@ diff --ruN a/stablehlo/stablehlo/tests/ops_broadcasting.mlir b/stablehlo/stableh\n diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n --- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n +++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir\n-@@ -576,16 +576,19 @@\n+@@ -473,6 +473,44 @@\n+   return %0, %1, %2, %3 : tensor<6xi32>, tensor<3xi32>, tensor<3x3xi32>, tensor<2x5xi32>\n+ }\n+ \n++// CHECK-LABEL: func.func @fold_concatenate_splat_leading\n++func.func @fold_concatenate_splat_leading(%arg0: tensor<1xi32>) -> tensor<3xi32> {\n++  // CHECK: [[CST0:%.+]] = stablehlo.constant dense<0> : tensor<2xi32>\n++  // CHECK-NEXT: stablehlo.concatenate [[CST0]], %arg0, dim = 0\n++  %cst0 = stablehlo.constant dense<0> : tensor<1xi32>\n++  %0 = stablehlo.concatenate %cst0, %cst0, %arg0, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<3xi32>\n++  return %0 : tensor<3xi32>\n++}\n++\n++// CHECK-LABEL: func.func @fold_concatenate_splat_trailing\n++func.func @fold_concatenate_splat_trailing(%arg0: tensor<2xi32>) -> tensor<6xi32> {\n++  // CHECK: [[CST0:%.+]] = stablehlo.constant dense<0> : tensor<4xi32>\n++  // CHECK-NEXT: stablehlo.concatenate %arg0, [[CST0]], dim = 0\n++  %cst0 = stablehlo.constant dense<0> : tensor<2xi32>\n++  %0 = stablehlo.concatenate %arg0, %cst0, %cst0, dim = 0 : (tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor<6xi32>\n++  return %0 : tensor<6xi32>\n++}\n++\n++// CHECK-LABEL: func.func @fold_concatenate_splat_middle\n++func.func @fold_concatenate_splat_middle(%arg0: tensor<1xi32>) -> tensor<4xi32> {\n++  // CHECK: [[CST0:%.+]] = stablehlo.constant dense<0> : tensor<2xi32>\n++  // CHECK-NEXT: stablehlo.concatenate %arg0, [[CST0]], %arg0, dim = 0\n++  %cst0 = stablehlo.constant dense<0> : tensor<1xi32>\n++  %0 = stablehlo.concatenate %arg0, %cst0, %cst0, %arg0, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<4xi32>\n++  return %0 : tensor<4xi32>\n++}\n++\n++// CHECK-LABEL: func.func @fold_concatenate_splat_multiple\n++func.func @fold_concatenate_splat_multiple(%arg0: tensor<1xi32>) -> tensor<5xi32> {\n++  // CHECK-DAG: [[CST0:%.+]] = stablehlo.constant dense<0> : tensor<2xi32>\n++  // CHECK-DAG: [[CST1:%.+]] = stablehlo.constant dense<1> : tensor<2xi32>\n++  // CHECK-NEXT: stablehlo.concatenate [[CST0]], [[CST1]], %arg0, dim = 0\n++  %cst0 = stablehlo.constant dense<0> : tensor<1xi32>\n++  %cst1 = stablehlo.constant dense<1> : tensor<1xi32>\n++  %0 = stablehlo.concatenate %cst0, %cst0, %cst1, %cst1, %arg0, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<5xi32>\n++  return %0 : tensor<5xi32>\n++}\n++\n+ // -----\n+ \n+ ////////\n+@@ -576,16 +614,19 @@\n  // ReshapeOp\n  \n  // CHECK-LABEL: func @reshape_fold\n@@ -539,6 +584,42 @@ diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.ml\n +  return %0, %1, %2 : tensor<1xf32>, tensor<2x2xi32>, tensor<3x2xcomplex<f32>>\n  }\n  \n+ // -----\n+diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_target_independent_optimization.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_target_independent_optimization.mlir\n+--- stablehlo/stablehlo/tests/transforms/stablehlo_target_independent_optimization.mlir\n++++ stablehlo/stablehlo/tests/transforms/stablehlo_target_independent_optimization.mlir\n+@@ -9,6 +9,32 @@\n+   // CHECK: stablehlo.add %arg0, %cst : tensor<f32>\n+   %1 = stablehlo.add %0, %arg0 : tensor<f32>\n+   return %1 : tensor<f32>\n++}\n++\n++// -----\n++\n++func.func @concatenate_fold_splat_flatten_integ(%arg0: tensor<8xf32>) -> tensor<64xf32> {\n++  // CHECK-DAG: [[CST0:%.+]] = stablehlo.constant dense<0.000000e+00> : tensor<8xf32>\n++  // CHECK-DAG: [[CST1:%.+]] = stablehlo.constant dense<1.000000e+00> : tensor<8xf32>\n++  // CHECK-DAG: [[CST2:%.+]] = stablehlo.constant dense<2.000000e+00> : tensor<8xf32>\n++  // CHECK-DAG: [[CST3:%.+]] = stablehlo.constant dense<3.000000e+00> : tensor<8xf32>\n++  // CHECK: stablehlo.concatenate [[CST0]], [[CST1]], [[CST2]], [[CST3]], %arg0, %arg0, %arg0, %arg0,\n++  %cst0 = stablehlo.constant dense<0.0> : tensor<f32>\n++  %cst1 = stablehlo.constant dense<1.0> : tensor<f32>\n++  %cst2 = stablehlo.constant dense<2.0> : tensor<f32>\n++  %cst3 = stablehlo.constant dense<3.0> : tensor<f32>\n++  %0 = stablehlo.reshape %cst0 : (tensor<f32>) -> tensor<1xf32>\n++  %1 = stablehlo.reshape %cst1 : (tensor<f32>) -> tensor<1xf32>\n++  %2 = stablehlo.reshape %cst2 : (tensor<f32>) -> tensor<1xf32>\n++  %3 = stablehlo.reshape %cst3 : (tensor<f32>) -> tensor<1xf32>\n++  %4 = stablehlo.concatenate %0, %0, %0, %0, %0, %0, %0, %0, dim = 0 : (tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>) -> tensor<8xf32>\n++  %5 = stablehlo.concatenate %1, %1, %1, %1, %1, %1, %1, %1, dim = 0 : (tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>) -> tensor<8xf32>\n++  %6 = stablehlo.concatenate %2, %2, %2, %2, %2, %2, %2, %2, dim = 0 : (tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>) -> tensor<8xf32>\n++  %7 = stablehlo.concatenate %3, %3, %3, %3, %3, %3, %3, %3, dim = 0 : (tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>, tensor<1xf32>) -> tensor<8xf32>\n++  %8 = stablehlo.concatenate %4, %5, %6, %7, dim = 0 : (tensor<8xf32>, tensor<8xf32>, tensor<8xf32>, tensor<8xf32>) -> tensor<32xf32>\n++  %9 = stablehlo.concatenate %arg0, %arg0, %arg0, %arg0, dim = 0 : (tensor<8xf32>, tensor<8xf32>, tensor<8xf32>, tensor<8xf32>) -> tensor<32xf32>\n++  %10 = stablehlo.concatenate %8, %9, dim = 0 : (tensor<32xf32>, tensor<32xf32>) -> tensor<64xf32>\n++  return %10 : tensor<64xf32>\n+ }\n+ \n  // -----\n diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp b/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp\n --- stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp\n@@ -912,7 +993,69 @@ diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.h b/stabl\n diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n --- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n +++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp\n-@@ -1108,7 +1108,8 @@\n+@@ -822,6 +822,61 @@\n+   int64_t foldOpElementLimit;\n+ };\n+ \n++// Pattern: concat(splat_a, splat_a, X) -> concat(splat_a_resize, X)\n++struct FoldConcatenateAdjacentSplatsOpPattern final\n++    : ShapeOpRewritePattern<mlir::stablehlo::ConcatenateOp> {\n++  using ShapeOpRewritePattern::ShapeOpRewritePattern;\n++\n++  LogicalResult matchAndRewrite(ConcatenateOp op,\n++                                PatternRewriter& rewriter) const override {\n++    SmallVector<Value> newOperands;\n++    SplatElementsAttr currSplat;\n++    for (size_t i = 0; i < op.getNumOperands(); ++i) {\n++      Value operand = op.getOperand(i);\n++      // Match a splat and look ahead for adjacent identical splats.\n++      if (matchPattern(operand, m_Constant(&currSplat)) && currSplat) {\n++        size_t j = i+1;\n++        SplatElementsAttr lookaheadSplat;\n++        int64_t nOccurrences = 1;\n++        for (; j < op.getNumOperands(); ++j) {\n++          if (matchPattern(op.getOperand(j), m_Constant(&lookaheadSplat)) &&\n++              lookaheadSplat && lookaheadSplat == currSplat) {\n++            ++nOccurrences;\n++            continue;\n++          }\n++          break;\n++        }\n++\n++        // Special case for a single occurrence, no new constants\n++        if (nOccurrences == 1) {\n++          newOperands.push_back(operand);\n++          continue;\n++        }\n++\n++        // Resize the splat and append it to the new operands.\n++        SmallVector<int64_t> newShape =\n++            llvm::to_vector(currSplat.getType().getShape());\n++        newShape[op.getDimension()] *= nOccurrences;\n++        newOperands.push_back(ConstantOp::create(\n++            rewriter, op.getLoc(),\n++            currSplat.resizeSplat(currSplat.getType().clone(newShape))));\n++\n++        // Set `i` to j-1 so that next iteration processes the next operand.\n++        i = j - 1;\n++        continue;\n++      }\n++      // Not splat, append the operand.\n++      newOperands.push_back(operand);\n++    }\n++    if (newOperands.size() == op.getNumOperands()) {\n++      return rewriter.notifyMatchFailure(op, \"No splats to fold\");\n++    }\n++    rewriter.replaceOpWithNewOp<ConcatenateOp>(op, op.getType(), newOperands,\n++                                               op.getDimension());\n++    return success();\n++  }\n++};\n++\n+ struct FoldConvertOpPattern : public ShapeOpRewritePattern<ConvertOp> {\n+   using ShapeOpRewritePattern::ShapeOpRewritePattern;\n+ \n+@@ -1108,7 +1163,8 @@\n                                  PatternRewriter& rewriter) const override {\n      auto resultType = op.getType();\n      if (failed(validateStaticShapeResult(rewriter, op, resultType)) ||\n@@ -922,4 +1065,13 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFold\n        return failure();\n  \n      DenseElementsAttr attr;\n+@@ -1923,6 +1979,8 @@\n+   patterns->add<FoldClampOpPattern>(context, options, benefit);\n+   patterns->add<FoldCompareOpPattern>(context, options, benefit);\n+   patterns->add<FoldConcatenateOpPattern>(context, options, benefit);\n++  patterns->add<FoldConcatenateAdjacentSplatsOpPattern>(context, options,\n++                                                        benefit);\n+   patterns->add<FoldConvertOpPattern>(context, options, benefit);\n+   patterns->add<FoldDivOpPattern>(context, options, benefit);\n+   patterns->add<FoldDynamicSliceOpPattern>(context, options, benefit);\n "
        }
    ],
    "stats": {
        "total": 156,
        "additions": 154,
        "deletions": 2
    }
}