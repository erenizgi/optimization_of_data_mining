{
    "author": "sergachev",
    "message": "PR #34898: [GPU] Do not float-normalize bf16 negation and abs.\n\nImported from GitHub PR https://github.com/openxla/xla/pull/34898\n\nüìù Summary of Changes\nAvoid unnecessary type casts - bf16 negation and abs are supported in PTX.\n\nüöÄ Kind of Contribution\n‚ôªÔ∏è Cleanup\n\nüß™ Unit Tests:\nyes\n\nüß™ Execution Tests:\nno\n\nCopybara import of the project:\n\n--\n867f131cccba2df2cbc61d584ebc238cb0aceeae by Ilia Sergachev <isergachev@nvidia.com>:\n\n[GPU] Do not float-normalize bf16 negation and abs.\n\nMerging this change closes #34898\n\nPiperOrigin-RevId: 842132075",
    "sha": "f90cdb544e97accf09780a2c8810cab6a3a305fb",
    "files": [
        {
            "sha": "6aa7e4b1ec1f68e2698277243a0b36e94e7359bf",
            "filename": "third_party/xla/xla/service/gpu/gpu_float_support.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f90cdb544e97accf09780a2c8810cab6a3a305fb/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_float_support.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f90cdb544e97accf09780a2c8810cab6a3a305fb/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_float_support.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_float_support.cc?ref=f90cdb544e97accf09780a2c8810cab6a3a305fb",
            "patch": "@@ -131,8 +131,10 @@ bool GpuFloatSupport::IsSupported(const HloInstruction& hlo) const {\n         return compute_capability_.IsCuda();\n       }\n       return false;\n+    case HloOpcode::kAbs:\n     case HloOpcode::kMaximum:\n     case HloOpcode::kMinimum:\n+    case HloOpcode::kNegate:\n       if (LowPrecisionType() == BF16) {\n         auto* cuda_compute_capability =\n             compute_capability_.cuda_compute_capability();"
        },
        {
            "sha": "bd88890113d1a592d1d3b55b4bc3d5b3976e1d50",
            "filename": "third_party/xla/xla/service/gpu/gpu_float_support_test.cc",
            "status": "modified",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f90cdb544e97accf09780a2c8810cab6a3a305fb/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_float_support_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f90cdb544e97accf09780a2c8810cab6a3a305fb/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_float_support_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_float_support_test.cc?ref=f90cdb544e97accf09780a2c8810cab6a3a305fb",
            "patch": "@@ -432,6 +432,35 @@ ENTRY main {\n       se::GpuComputeCapability{se::CudaComputeCapability::Volta()}, BF16, F32));\n }\n \n+class Bf16UnaryOpTest : public FloatSupportTest,\n+                        public ::testing::WithParamInterface<HloOpcode> {};\n+\n+TEST_P(Bf16UnaryOpTest, IsOnlyNormalizedPreAmpere) {\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          ParseAndReturnVerifiedModule(\n+                              absl::Substitute(R\"(\n+entry {\n+  a = bf16[] parameter(0)\n+  r = bf16[] $0(a)\n+})\",\n+                                               HloOpcodeString(GetParam()))));\n+  EXPECT_FALSE(\n+      Normalize(module.get(),\n+                se::GpuComputeCapability{se::CudaComputeCapability::Hopper()},\n+                BF16, F32));\n+  EXPECT_FALSE(\n+      Normalize(module.get(),\n+                se::GpuComputeCapability{se::CudaComputeCapability::Ampere()},\n+                BF16, F32));\n+  EXPECT_TRUE(Normalize(\n+      module.get(),\n+      se::GpuComputeCapability{se::CudaComputeCapability::Volta()}, BF16, F32));\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(Bf16UnaryOps, Bf16UnaryOpTest,\n+                         ::testing::Values(HloOpcode::kNegate,\n+                                           HloOpcode::kAbs));\n+\n TEST_F(FloatSupportTest,\n        BF16ReductionOnHopperIsOnlyNormalizedIfReducerIsUnsupported) {\n   auto cc = se::CudaComputeCapability::Hopper();"
        }
    ],
    "stats": {
        "total": 31,
        "additions": 31,
        "deletions": 0
    }
}