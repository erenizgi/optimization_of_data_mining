{
    "author": "zvikinoza",
    "message": "Hlo module splitting\n\nThis change introduces the ability to split a single HLO module into multiple modules, for the separate compilation effort.\n\nSplitting strategy assigns each computation in the call graph (starting from the entry computation) to its own separate module. Calls between computations that end up in different modules are replaced by stubs, which are implemented as custom-calls. An HloLinkingManifest is generated alongside the split modules, containing information to link the modules back together by replacing stubs with calls to the actual computations in other modules.\n\nThis change also adds helper methods to assign deterministic instruction and computation IDs, which are used to generate stable module names, and to replace called computations when inserting stubs.\n\nPiperOrigin-RevId: 829434027",
    "sha": "3f34740ef595a3d2b17f70b4286a207a64af1b33",
    "files": [
        {
            "sha": "5d1e0768431b2cc3953c202abc6482071a2701a1",
            "filename": "third_party/xla/xla/hlo/separate_compilation/BUILD",
            "status": "added",
            "additions": 72,
            "deletions": 0,
            "changes": 72,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3f34740ef595a3d2b17f70b4286a207a64af1b33/third_party%2Fxla%2Fxla%2Fhlo%2Fseparate_compilation%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3f34740ef595a3d2b17f70b4286a207a64af1b33/third_party%2Fxla%2Fxla%2Fhlo%2Fseparate_compilation%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fseparate_compilation%2FBUILD?ref=3f34740ef595a3d2b17f70b4286a207a64af1b33",
            "patch": "@@ -0,0 +1,72 @@\n+# Targets related to hlo_module_splitting, linking, and more generally\n+# incremental compilation of HLO.\n+\n+load(\"//xla/tests:build_defs.bzl\", \"xla_test\")\n+load(\"//xla/tsl:tsl.bzl\", \"internal_visibility\")\n+load(\"//xla/tsl/platform:rules_cc.bzl\", \"cc_library\")\n+\n+package(\n+    # copybara:uncomment default_applicable_licenses = [\"//tensorflow:license\"],\n+    default_visibility = internal_visibility([\"//xla:internal\"]),\n+    licenses = [\"notice\"],\n+)\n+\n+cc_library(\n+    name = \"hlo_linking_manifest\",\n+    hdrs = [\"hlo_linking_manifest.h\"],\n+    deps = [\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/service:compilation_environments\",\n+        \"//xla/service:hlo_module_config\",\n+        \"@com_google_absl//absl/base:nullability\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n+    ],\n+)\n+\n+cc_library(\n+    name = \"hlo_module_splitting\",\n+    srcs = [\"hlo_module_splitting.cc\"],\n+    hdrs = [\"hlo_module_splitting.h\"],\n+    deps = [\n+        \":hlo_linking_manifest\",\n+        \"//xla:status_macros\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/service:compilation_environments\",\n+        \"//xla/service:hlo_module_config\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/container:flat_hash_set\",\n+        \"@com_google_absl//absl/hash\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_absl//absl/types:span\",\n+    ],\n+)\n+\n+# Tests below\n+\n+xla_test(\n+    name = \"hlo_module_splitting_test\",\n+    size = \"small\",\n+    srcs = [\"hlo_module_splitting_test.cc\"],\n+    backends = [\"cpu\"],\n+    deps = [\n+        \":hlo_module_splitting\",\n+        \"//xla/hlo/ir:hlo\",\n+        \"//xla/hlo/testlib:hlo_hardware_independent_test_base\",\n+        \"//xla/service:compiler\",\n+        \"//xla/service:platform_util\",\n+        \"//xla/stream_executor:platform\",\n+        \"//xla/tsl/lib/core:status_test_util\",\n+        \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/container:flat_hash_set\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_googletest//:gtest\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)"
        },
        {
            "sha": "1558a2f6c477886b3db176a50beae414904a607d",
            "filename": "third_party/xla/xla/hlo/separate_compilation/hlo_linking_manifest.h",
            "status": "added",
            "additions": 50,
            "deletions": 0,
            "changes": 50,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3f34740ef595a3d2b17f70b4286a207a64af1b33/third_party%2Fxla%2Fxla%2Fhlo%2Fseparate_compilation%2Fhlo_linking_manifest.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3f34740ef595a3d2b17f70b4286a207a64af1b33/third_party%2Fxla%2Fxla%2Fhlo%2Fseparate_compilation%2Fhlo_linking_manifest.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fseparate_compilation%2Fhlo_linking_manifest.h?ref=3f34740ef595a3d2b17f70b4286a207a64af1b33",
            "patch": "@@ -0,0 +1,50 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_HLO_SEPARATE_COMPILATION_HLO_LINKING_MANIFEST_H_\n+#define XLA_HLO_SEPARATE_COMPILATION_HLO_LINKING_MANIFEST_H_\n+\n+#include <memory>\n+\n+#include \"absl/base/nullability.h\"\n+#include \"absl/container/flat_hash_map.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/service/compilation_environments.h\"\n+#include \"xla/service/hlo_module_config.h\"\n+\n+namespace xla::separate_compilation {\n+\n+// Metadata to guide linking process of HLO modules.\n+//\n+// Manifest contains Caller/Callee information:\n+// When splitting modules, sometimes callers and their callees end up\n+// in different sub-modules. During linking, we must be able to find\n+// the callees and replace the stubs we inserted in the caller's sub-module.\n+// Because, HLO serialization includes ids keeping this information in\n+// the module interferes with caching of artifacts, by making artifacts\n+// with the same semantics appear different when serialized. This class\n+// externalizes the information.\n+struct HloLinkingManifest {\n+  // Maps from a stub computation to the cloned computation.\n+  // Note that these `HloComputation` pointers might be from different modules.\n+  absl::flat_hash_map<const HloComputation* absl_nonnull,\n+                      const HloComputation* absl_nonnull>\n+      stub_links;\n+  std::shared_ptr<const HloModuleConfig> module_config;\n+  std::unique_ptr<CompilationEnvironments> compilation_environment;\n+};\n+\n+}  // namespace xla::separate_compilation\n+#endif  // XLA_HLO_SEPARATE_COMPILATION_HLO_LINKING_MANIFEST_H_"
        },
        {
            "sha": "ac8afaeaeaf0a296354c2fa8a7c4f799c423106a",
            "filename": "third_party/xla/xla/hlo/separate_compilation/hlo_module_splitting.cc",
            "status": "added",
            "additions": 335,
            "deletions": 0,
            "changes": 335,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3f34740ef595a3d2b17f70b4286a207a64af1b33/third_party%2Fxla%2Fxla%2Fhlo%2Fseparate_compilation%2Fhlo_module_splitting.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3f34740ef595a3d2b17f70b4286a207a64af1b33/third_party%2Fxla%2Fxla%2Fhlo%2Fseparate_compilation%2Fhlo_module_splitting.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fseparate_compilation%2Fhlo_module_splitting.cc?ref=3f34740ef595a3d2b17f70b4286a207a64af1b33",
            "patch": "@@ -0,0 +1,335 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/hlo/separate_compilation/hlo_module_splitting.h\"\n+\n+#include <cstdint>\n+#include <deque>\n+#include <memory>\n+#include <string>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/container/flat_hash_map.h\"\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/hash/hash.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/hlo/ir/hlo_clone_context.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/hlo/ir/hlo_opcode.h\"\n+#include \"xla/hlo/separate_compilation/hlo_linking_manifest.h\"\n+#include \"xla/service/compilation_environments.h\"\n+#include \"xla/service/hlo_module_config.h\"\n+#include \"xla/status_macros.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+namespace xla::separate_compilation {\n+namespace {\n+\n+constexpr absl::string_view kEntryName = \"entry\";\n+constexpr absl::string_view kModulePrefix = \"module\";\n+constexpr absl::string_view kStubPrefix = \"stub\";\n+\n+// Provide a name for the module containing the split.\n+// The name should be stable across compilations meaning that the same\n+// split should get the same name.\n+std::string GetSplitModuleName(\n+    absl::flat_hash_set<const HloComputation*> split) {\n+  // If multiple `HloComputation` elements are in a split, we have to worry\n+  // about their ordering when hashing, or use some ordering-invariant hash.\n+  CHECK(split.size() == 1) << \"The current implementation only supports \"\n+                              \"singleton splits.\";\n+  return absl::StrCat(kModulePrefix, absl::HashOf(*split.begin()));\n+}\n+\n+std::string GetStubName(int32_t callee_index) {\n+  return absl::StrCat(kStubPrefix, \".\", callee_index);\n+}\n+\n+absl::StatusOr<std::unique_ptr<HloComputation>> CreateCalleeStub(\n+    HloComputation* callee, int32_t callee_index) {\n+  // Bind to keep alive for the duration of the scope.\n+  std::string stub_name = GetStubName(callee_index);\n+  auto comp_builder = HloComputation::Builder(stub_name);\n+\n+  std::vector<HloInstruction*> operands;\n+  for (const HloInstruction* parameter : callee->parameter_instructions()) {\n+    TF_ASSIGN_OR_RETURN(\n+        HloInstruction * cloned_parameter,\n+        comp_builder.AddParameter(parameter->Clone(/*suffix=*/\"\")));\n+    operands.push_back(cloned_parameter);\n+  }\n+\n+  comp_builder.AddInstruction(HloInstruction::CreateCustomCall(\n+      callee->root_instruction()->shape(), operands,\n+      /*custom_call_target=*/kStubPrefix));\n+  return comp_builder.Build();\n+}\n+\n+// Returns all `kCall` instructions in the given computation.\n+std::vector<const HloInstruction*> CollectCallInstructions(\n+    const HloComputation* computation) {\n+  // TODO: b/419473710 - Maybe guarantee order of operand traversal?\n+  std::vector<const HloInstruction*> call_sites;\n+  for (const HloInstruction* caller : computation->MakeInstructionPostOrder()) {\n+    if (caller->opcode() == HloOpcode::kCall) {\n+      call_sites.push_back(caller);\n+    }\n+  }\n+  return call_sites;\n+}\n+\n+// Composes two maps into one. The first map's value type must be second map's\n+// key type.\n+template <typename K, typename KV, typename V>\n+absl::StatusOr<absl::flat_hash_map<K, V>> ComposeMaps(\n+    const absl::flat_hash_map<K, KV>& first,\n+    const absl::flat_hash_map<KV, V>& second) {\n+  absl::flat_hash_map<K, V> result;\n+  for (const auto [k, kv] : first) {\n+    if (auto it = second.find(kv); it != second.end()) {\n+      result.insert({k, it->second});\n+    }\n+  }\n+  return result;\n+}\n+\n+// Merges `from` into `into`. If `error_on_duplicate_key` is true, returns an\n+// error if any key is encountered more than once.\n+template <typename K, typename V>\n+absl::Status MergeMapInto(absl::flat_hash_map<K, V>& into,\n+                          const absl::flat_hash_map<K, V>& from,\n+                          bool error_on_duplicate_key = true) {\n+  for (const auto& [k, v] : from) {\n+    if (!into.insert({k, v}).second) {\n+      if (error_on_duplicate_key) {\n+        return absl::AlreadyExistsError(\"Duplicate key encountered.\");\n+      }\n+    }\n+  }\n+  return absl::OkStatus();\n+}\n+}  // namespace\n+\n+// Assigns entry & all kCall-ed computations to splits in stable way.\n+// Computations are assigned to exactly one split.\n+//\n+// Other computations, not explicitly kCall-ed, are not assigned and will be\n+// cloned with any explicitly assigned computation that depends on them. Note\n+// that this means that they might be replicated to multiple splits!\n+absl::StatusOr<std::vector<absl::flat_hash_set<const HloComputation*>>>\n+GroupComputationsForSplitting(const HloModule& module) {\n+  std::vector<absl::flat_hash_set<const HloComputation*>> result;\n+\n+  const HloComputation* entry_computation = module.entry_computation();\n+  TF_RET_CHECK(entry_computation != nullptr)\n+      << \"Module has no entry computation.\";\n+\n+  // Perform a BFS traversal of the graph along kCall edges.\n+  // All other edges are ignored.\n+  std::deque<const HloComputation*> computations_to_visit;\n+  absl::flat_hash_set<const HloComputation*> seen;\n+\n+  computations_to_visit.push_back(entry_computation);\n+  seen.insert(entry_computation);\n+\n+  while (!computations_to_visit.empty()) {\n+    const HloComputation* current_computation = computations_to_visit.front();\n+    computations_to_visit.pop_front();\n+\n+    // Each reachable computation is added as a separate split.\n+    // If grouping is possible, this logic might change.\n+    result.push_back({current_computation});\n+\n+    // Process callees.\n+    for (const HloInstruction* op : current_computation->instructions()) {\n+      if (op->opcode() == HloOpcode::kCall) {\n+        const HloComputation* callee = op->to_apply();\n+        TF_RET_CHECK(callee != nullptr)\n+            << \"HloOpcode::kCall has a null callee.\";\n+        if (!seen.contains(callee)) {\n+          seen.insert(callee);\n+          computations_to_visit.push_back(callee);\n+        }\n+      }\n+    }\n+  }\n+  return result;\n+}\n+\n+absl::StatusOr<std::unique_ptr<HloModuleSplit>> CreateHloModuleSplit(\n+    const HloModule& module, absl::flat_hash_set<const HloComputation*> split) {\n+  // If multiple `HloComputation` elements are in a split, we have to worry\n+  // about their ordering when hashing, or use some ordering-invariant hash.\n+  CHECK(split.size() == 1)\n+      << \"The current implementation supports singleton splits.\";\n+  // TODO: b/419184359 - Revisit when we reconfigure the pipeline\n+  // (global->local->global). Check what data is needed by which\n+  // set of passes.\n+  std::shared_ptr<const HloModuleConfig> sub_module_config =\n+      module.shared_config();\n+  auto sub_module_env =\n+      std::make_unique<CompilationEnvironments>(module.comp_envs());\n+  auto submodule = std::make_unique<HloModule>(\n+      GetSplitModuleName(split), sub_module_config, std::move(sub_module_env));\n+  HloCloneContext clone_context(submodule.get());\n+  // The plan is:\n+  // 1. Prepare stubs as substitutions for callees.\n+  // 2. Clone the computation(s) with replacements of calls.\n+  // 3. Set the ENTRY computation.\n+\n+  const HloComputation* computation = *split.begin();\n+  VLOG(4) << \"Splitting out: \" << computation->name();\n+  std::vector<const HloInstruction*> call_instructions =\n+      CollectCallInstructions(computation);\n+  // stub -> original callee\n+  absl::flat_hash_map<const HloComputation*, const HloComputation*> stub_map;\n+  // original computation -> split computation\n+  absl::flat_hash_map<const HloComputation*, const HloComputation*>\n+      computation_map;\n+\n+  // We want to give a unique name to every call site by inserting a unique\n+  // stub call into every call site. However, `HloCloneContext` wants a map\n+  // of callee_computation -> new_computation, and this does not allow different\n+  // call sites to replace the same `callee_computation` with a different\n+  // `new_computation_2` at a different call site.\n+  //\n+  // We create a fresh stub for every call site. Since we only handle kCall\n+  // instructions, it can have single callee (call site). We map\n+  // original callee to these new stub, even though some aliasing might\n+  // happen. Specifically, if multiple call sites refer to the same callee\n+  // that callee will map to the stub for the last encountered call site.\n+  //\n+  // We tolerate this to avoid cloning the actual callees into the new module.\n+  // After cloning we go into the cloned computation and patch callee pointers.\n+\n+  absl::flat_hash_map<const HloInstruction*, HloComputation*>\n+      callee_replacements;\n+  int32_t callee_index = 0;\n+  for (const HloInstruction* caller : call_instructions) {\n+    HloComputation* callee = caller->to_apply();\n+    // Skip callee that is part of the current split.\n+    if (split.contains(callee)) {\n+      // Remember the original callee, so that we can patch the call site later.\n+      callee_replacements[caller] = callee;\n+      continue;\n+    }\n+    TF_ASSIGN_OR_RETURN(std::unique_ptr<HloComputation> stub,\n+                        CreateCalleeStub(callee, callee_index));\n+    VLOG(4) << \"Stubbing \" << stub->name() << \" --> \" << callee->name() << \" \"\n+            << stub->ToString();\n+    HloComputation* stub_raw_ptr =\n+        submodule->AddComputationAndUnifyNamesAndIds(std::move(stub),\n+                                                     /*is_entry=*/false);\n+    clone_context.MapComputation(callee, stub_raw_ptr);\n+    callee_replacements[caller] = stub_raw_ptr;\n+    stub_map.insert({stub_raw_ptr, callee});\n+    ++callee_index;\n+  }\n+\n+  HloComputation* entry_computation = submodule->AddEntryComputationWithLayouts(\n+      computation->CloneInContext(clone_context, nullptr,\n+                                  /*extra_parameters=*/{}, /*suffix=*/\"\"));\n+\n+  // Patch call sites.\n+  for (const HloInstruction* caller : call_instructions) {\n+    HloInstruction* mapped_call_instruction =\n+        clone_context.GetInstruction(caller);\n+    // Adjust `callee_replacement` to only contain pointer to computations in\n+    // the submodule. This is currently not the case because we skipped some\n+    // callees when stubbing and remembered pointer to their original.\n+    HloComputation* replacement = callee_replacements[caller];\n+    if (replacement->parent() != submodule.get()) {\n+      replacement = clone_context.GetComputation(replacement);\n+    }\n+    mapped_call_instruction->set_to_apply(callee_replacements[caller]);\n+  }\n+\n+  entry_computation->SetAndSanitizeName(kEntryName);\n+  computation_map.insert({computation, entry_computation});\n+\n+  VLOG(3) << submodule->ToString();\n+  return std::make_unique<HloModuleSplit>(\n+      module, std::move(submodule), std::move(stub_map),\n+      std::move(computation_map), std::move(call_instructions));\n+}\n+\n+absl::StatusOr<std::unique_ptr<HloModuleSplitGroup>> CreateHloModuleSplitGroup(\n+    const HloModule& module) {\n+  absl::flat_hash_map<const HloComputation*, const HloModuleSplit*>\n+      computation_address_book;\n+  std::vector<std::unique_ptr<HloModuleSplit>> module_splits;\n+\n+  // See `HloModuleSplit::stub_map`.\n+  absl::flat_hash_map<const HloComputation*, const HloComputation*>\n+      global_stub_map;\n+  absl::flat_hash_map<const HloComputation*, const HloComputation*>\n+      global_computation_map;\n+\n+  TF_ASSIGN_OR_RETURN(\n+      std::vector<absl::flat_hash_set<const HloComputation*>> splits,\n+      GroupComputationsForSplitting(module));\n+\n+  for (const auto& split : splits) {\n+    TF_ASSIGN_OR_RETURN(auto module_split, CreateHloModuleSplit(module, split));\n+    module_splits.push_back(std::move(module_split));\n+    for (const auto* original_comp : split) {\n+      computation_address_book.insert(\n+          {original_comp, module_splits.back().get()});\n+    }\n+    TF_RETURN_IF_ERROR(\n+        MergeMapInto(global_stub_map, module_splits.back()->stub_map));\n+    TF_RETURN_IF_ERROR(MergeMapInto(global_computation_map,\n+                                    module_splits.back()->computation_map));\n+  }\n+\n+  if (VLOG_IS_ON(5)) {\n+    VLOG(5) << \"Split group:\";\n+    for (const auto& split : module_splits) {\n+      VLOG(5) << \"Split: \" << split->submodule->name();\n+      VLOG(5) << \" Stub links:\";\n+      for (const auto& [stub, comp] : split->stub_map) {\n+        VLOG(5)\n+            << \"  \" << stub->name() << \" ==>> \" << comp->name() << \"(\"\n+            << computation_address_book[comp]->computation_map.at(comp)->name()\n+            << \" @ \" << computation_address_book[comp]->submodule->name()\n+            << \")\";\n+      }\n+    }\n+  }\n+  // Compose at the end once all planned cloning operations are finished and\n+  // we know where each original computation ended up.\n+  TF_ASSIGN_OR_RETURN(auto stub_links,\n+                      ComposeMaps(global_stub_map, global_computation_map));\n+\n+  HloLinkingManifest linking_manifest{\n+      std::move(stub_links), module.shared_config(),\n+      std::make_unique<CompilationEnvironments>(module.comp_envs())};\n+\n+  return std::make_unique<HloModuleSplitGroup>(\n+      std::move(computation_address_book), std::move(module_splits),\n+      std::move(linking_manifest));\n+}\n+\n+}  // namespace xla::separate_compilation"
        },
        {
            "sha": "4569edd925e5ca7bc45802175b8c242080ad17df",
            "filename": "third_party/xla/xla/hlo/separate_compilation/hlo_module_splitting.h",
            "status": "added",
            "additions": 107,
            "deletions": 0,
            "changes": 107,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3f34740ef595a3d2b17f70b4286a207a64af1b33/third_party%2Fxla%2Fxla%2Fhlo%2Fseparate_compilation%2Fhlo_module_splitting.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3f34740ef595a3d2b17f70b4286a207a64af1b33/third_party%2Fxla%2Fxla%2Fhlo%2Fseparate_compilation%2Fhlo_module_splitting.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fseparate_compilation%2Fhlo_module_splitting.h?ref=3f34740ef595a3d2b17f70b4286a207a64af1b33",
            "patch": "@@ -0,0 +1,107 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_HLO_SEPARATE_COMPILATION_HLO_MODULE_SPLITTING_H_\n+#define XLA_HLO_SEPARATE_COMPILATION_HLO_MODULE_SPLITTING_H_\n+\n+#include <memory>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/container/flat_hash_map.h\"\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/hlo/separate_compilation/hlo_linking_manifest.h\"\n+\n+namespace xla::separate_compilation {\n+\n+// Returns a list of sets of computations that can be split into separate\n+// modules. Adjacent computations in the same set can be compiled together.\n+absl::StatusOr<std::vector<absl::flat_hash_set<const HloComputation*>>>\n+GroupComputationsForSplitting(const HloModule& module);\n+\n+// Represents one part of a module that was split into multiple parts for\n+// separate compilation.\n+struct HloModuleSplit {\n+  using StubComputation = HloComputation;\n+  using OriginalComputation = HloComputation;\n+  using ClonedComputation = HloComputation;\n+\n+  // The original `HloModule` that this split originated from.\n+  const HloModule& module;\n+  // An `HloModule` containing computations belonging to this split. If any\n+  // computation in `submodule` calls a computation which is part of another\n+  // split, that call is replaced with a call to a stub computation.\n+  std::unique_ptr<HloModule> submodule;\n+  // Maps stub computations defined in `submodule` to the original computations\n+  // in `module` which they replace.\n+  absl::flat_hash_map<const StubComputation*, const OriginalComputation*>\n+      stub_map;\n+  // Maps computations from `module` to their cloned versions in `submodule`.\n+  absl::flat_hash_map<const OriginalComputation*, const ClonedComputation*>\n+      computation_map;\n+  // All `kCall` instructions in `submodule` which originally belonged to\n+  // computations cloned into this split. This includes calls to stubbed out\n+  // computations, and calls to computations within this split.\n+  std::vector<const HloInstruction*> call_sites;\n+\n+  HloModuleSplit(\n+      const HloModule& module, std::unique_ptr<HloModule> submodule,\n+      absl::flat_hash_map<const StubComputation*, const OriginalComputation*>\n+          stub_map,\n+      absl::flat_hash_map<const OriginalComputation*, const ClonedComputation*>\n+          computation_map,\n+      std::vector<const HloInstruction*> call_sites)\n+      : module{std::move(module)},\n+        submodule{std::move(submodule)},\n+        stub_map{std::move(stub_map)},\n+        computation_map{std::move(computation_map)},\n+        call_sites{std::move(call_sites)} {}\n+};\n+\n+// Creates an `HloModule` that only contains the requested computations\n+// from the original module and, potentially, insert callee stubs.\n+absl::StatusOr<std::unique_ptr<HloModuleSplit>> CreateHloModuleSplit(\n+    const HloModule& module, absl::flat_hash_set<const HloComputation*> split);\n+\n+// Represents a group of `HloModuleSplit`s.\n+struct HloModuleSplitGroup {\n+  absl::flat_hash_map<const HloComputation*, const HloModuleSplit*>\n+      address_book;\n+  std::vector<std::unique_ptr<HloModuleSplit>> module_splits;\n+  HloLinkingManifest linking_manifest;\n+\n+  HloModuleSplitGroup(\n+      absl::flat_hash_map<const HloComputation*, const HloModuleSplit*>&&\n+          address_book,\n+      std::vector<std::unique_ptr<HloModuleSplit>>&& module_splits,\n+      HloLinkingManifest&& linking_manifest)\n+      : address_book(std::move(address_book)),\n+        module_splits(std::move(module_splits)),\n+        linking_manifest(std::move(linking_manifest)) {}\n+};\n+\n+// Split the given module. Returns a mapping from `HloComputation*` to\n+// the `ModulePartition` data where that computation was assigned. If multiple\n+// computations are assigned to the same module there are multiple keys pointing\n+// to the same `ModulePartition` structure.\n+absl::StatusOr<std::unique_ptr<HloModuleSplitGroup>> CreateHloModuleSplitGroup(\n+    const HloModule& module);\n+\n+}  // namespace xla::separate_compilation\n+#endif  // XLA_HLO_SEPARATE_COMPILATION_HLO_MODULE_SPLITTING_H_"
        },
        {
            "sha": "79f0809d66f16a71b728803ac15490dd47734b84",
            "filename": "third_party/xla/xla/hlo/separate_compilation/hlo_module_splitting_test.cc",
            "status": "added",
            "additions": 376,
            "deletions": 0,
            "changes": 376,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3f34740ef595a3d2b17f70b4286a207a64af1b33/third_party%2Fxla%2Fxla%2Fhlo%2Fseparate_compilation%2Fhlo_module_splitting_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3f34740ef595a3d2b17f70b4286a207a64af1b33/third_party%2Fxla%2Fxla%2Fhlo%2Fseparate_compilation%2Fhlo_module_splitting_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fseparate_compilation%2Fhlo_module_splitting_test.cc?ref=3f34740ef595a3d2b17f70b4286a207a64af1b33",
            "patch": "@@ -0,0 +1,376 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/hlo/separate_compilation/hlo_module_splitting.h\"\n+\n+#include <memory>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n+#include \"xla/service/compiler.h\"\n+#include \"xla/service/platform_util.h\"\n+#include \"xla/stream_executor/platform.h\"\n+#include \"xla/tsl/lib/core/status_test_util.h\"\n+#include \"xla/tsl/platform/statusor.h\"\n+\n+namespace xla::separate_compilation {\n+namespace {\n+\n+using ::testing::UnorderedElementsAreArray;\n+\n+class SplittingTest : public HloHardwareIndependentTestBase {};\n+\n+TEST_F(SplittingTest, AllComputationsInBuckets) {\n+  constexpr absl::string_view module_text = R\"(\n+HloModule simple_module\n+\n+\n+// Simple alpha equivalence examples\n+\n+%add.0 (x: f32[], y: f32[]) -> f32[] {\n+  %x = f32[] parameter(0)\n+  %y = f32[] parameter(1)\n+  ROOT %add = f32[] add(f32[] %x, f32[] %y)\n+}\n+\n+%add.1 (a: f32[], b: f32[]) -> f32[] {\n+  %a = f32[] parameter(0)\n+  %b = f32[] parameter(1)\n+  ROOT %add = f32[] add(f32[] %a, f32[] %b)\n+}\n+\n+%add.2 (a: f32[], b: f32[]) -> f32[] {\n+  %a = f32[] parameter(0)\n+  %b = f32[] parameter(1)\n+  ROOT %add = f32[] add(f32[] %a, f32[] %b)\n+}\n+\n+%add.3 (a: f32[], b: f32[]) -> f32[] {\n+  %a = f32[] parameter(0)\n+  %b = f32[] parameter(1)\n+  ROOT %add = f32[] add(f32[] %a, f32[] %b)\n+}\n+\n+ENTRY %main() -> f32[] {\n+\n+  %p = f32[] constant(3.3)\n+  %q = f32[] constant(-1.0)\n+  %r = f32[] constant(7.1)\n+  %s = f32[] constant(0.2)\n+\n+  %res.0 = call(%p, %q), to_apply=%add.0\n+  %res.1 = call(%p, %q), to_apply=%add.1\n+  %res.2 = call(%r, %s), to_apply=%add.2\n+  %res.3 = call(%p, %s), to_apply=%add.3\n+\n+  %res.01 = f32[] add(f32[] %res.0, f32[] %res.1)\n+  %res.23 = f32[] add(f32[] %res.2, f32[] %res.3)\n+\n+  ROOT %result = f32[] add(f32[] %res.01, f32[] %res.23)\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          ParseAndReturnVerifiedModule(module_text));\n+  TF_ASSERT_OK_AND_ASSIGN(auto splits, GroupComputationsForSplitting(*module));\n+\n+  absl::flat_hash_set<const HloComputation*> all_computations_in_splits;\n+  for (const auto& bucket : splits) {\n+    all_computations_in_splits.insert(bucket.begin(), bucket.end());\n+  }\n+  EXPECT_THAT(all_computations_in_splits,\n+              UnorderedElementsAreArray(module->computations()));\n+}\n+\n+TEST_F(SplittingTest, CreateModule) {\n+  constexpr absl::string_view module_text = R\"(\n+HloModule simple_module\n+\n+\n+// Simple alpha equivalence examples\n+\n+%add.0 (x: f32[], y: f32[]) -> f32[] {\n+  %x = f32[] parameter(0)\n+  %y = f32[] parameter(1)\n+  ROOT %add = f32[] add(f32[] %x, f32[] %y)\n+}\n+\n+%add.1 (a: f32[], b: f32[]) -> f32[] {\n+  %a = f32[] parameter(0)\n+  %b = f32[] parameter(1)\n+  ROOT %add = f32[] add(f32[] %a, f32[] %b)\n+}\n+\n+%add.2 (a: f32[], b: f32[]) -> f32[] {\n+  %a = f32[] parameter(0)\n+  %b = f32[] parameter(1)\n+  ROOT %add = f32[] add(f32[] %a, f32[] %b)\n+}\n+\n+%add.3 (a: f32[], b: f32[]) -> f32[] {\n+  %a = f32[] parameter(0)\n+  %b = f32[] parameter(1)\n+  ROOT %add = f32[] add(f32[] %a, f32[] %b)\n+}\n+\n+ENTRY %main() -> f32[] {\n+\n+  %p = f32[] constant(3.3)\n+  %q = f32[] constant(-1.0)\n+  %r = f32[] constant(7.1)\n+  %s = f32[] constant(0.2)\n+\n+  %res.0 = call(%p, %q), to_apply=%add.0\n+  %res.1 = call(%p, %q), to_apply=%add.1\n+  %res.2 = call(%r, %s), to_apply=%add.2\n+  %res.3 = call(%p, %s), to_apply=%add.3\n+\n+  %res.01 = f32[] add(f32[] %res.0, f32[] %res.1)\n+  %res.23 = f32[] add(f32[] %res.2, f32[] %res.3)\n+\n+  ROOT %result = f32[] add(f32[] %res.01, f32[] %res.23)\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          ParseAndReturnVerifiedModule(module_text));\n+  auto* main = FindComputation(module.get(), \"main\");\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module_split,\n+                          CreateHloModuleSplit(*module, {main}));\n+\n+  const int kMainModuleComputationCount = 5;  // the main + 4 stubs\n+  // const int kMainModuleClonedComputations = 1;\n+  const int kMainModuleCallSitesCount = 4;\n+  EXPECT_EQ(module_split->module.computation_count(),\n+            kMainModuleComputationCount);\n+  EXPECT_EQ(module_split->stub_map.size(), kMainModuleCallSitesCount);\n+  EXPECT_EQ(module_split->call_sites.size(), kMainModuleCallSitesCount);\n+}\n+\n+TEST_F(SplittingTest, SplitModule) {\n+  constexpr absl::string_view module_text = R\"(\n+HloModule simple_module\n+\n+\n+// Simple alpha equivalence examples\n+\n+%fusion.1 (x: f32[]) -> f32[] {\n+  %x = f32[] parameter(0)\n+  ROOT %add = f32[] negate(f32[] %x)\n+}\n+\n+%add.0 (x: f32[], y: f32[]) -> f32[] {\n+  %x = f32[] parameter(0)\n+  %y = f32[] parameter(1)\n+  ROOT %add = f32[] add(f32[] %x, f32[] %y)\n+}\n+\n+%add.1 (a: f32[], b: f32[]) -> f32[] {\n+  %a = f32[] parameter(0)\n+  %b = f32[] parameter(1)\n+  ROOT %add = f32[] add(f32[] %a, f32[] %b)\n+}\n+\n+%add.2 (a: f32[], b: f32[]) -> f32[] {\n+  %a = f32[] parameter(0)\n+  %b = f32[] parameter(1)\n+  ROOT %add = f32[] add(f32[] %a, f32[] %b)\n+}\n+\n+%fusion.2 (x: f32[]) -> f32[] {\n+  %x = f32[] parameter(0)\n+  ROOT %add = f32[] negate(f32[] %x)\n+}\n+\n+ENTRY %main() -> f32[] {\n+\n+  %p = f32[] constant(3.3)\n+  %q = f32[] constant(-1.0)\n+  %r = f32[] constant(7.1)\n+  %s = f32[] constant(0.2)\n+\n+  %res.0 = call(%p, %q), to_apply=%add.0\n+  %res.1 = call(%p, %q), to_apply=%add.1\n+  %res.2 = call(%r, %s), to_apply=%add.2\n+  %res.3 = call(%p, %s), to_apply=%add.2\n+  %fusion = f32[] fusion(%res.3), kind=kLoop, calls=%fusion.2\n+\n+  %res.01 = f32[] add(f32[] %res.0, f32[] %res.1)\n+  %res.23 = f32[] add(f32[] %res.2, f32[] %res.3)\n+  %res.fu23 = f32[] add(f32[] %fusion, f32[] %res.23)\n+\n+  ROOT %result = f32[] add(f32[] %res.01, f32[] %res.fu23)\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          ParseAndReturnVerifiedModule(module_text));\n+  TF_ASSERT_OK_AND_ASSIGN(auto module_split_group,\n+                          CreateHloModuleSplitGroup(*module));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(stream_executor::Platform * platform,\n+                          PlatformUtil::GetPlatform(\"cpu\"));\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Compiler> compiler,\n+                          Compiler::GetForPlatform(platform));\n+  TF_ASSERT_OK(compiler->RunHloPasses(module->Clone(), /*executor=*/nullptr,\n+                                      Compiler::CompileOptions{}));\n+  for (auto& split : module_split_group->module_splits) {\n+    TF_EXPECT_OK(compiler->RunHloPasses(split->submodule->Clone(),\n+                                        /*executor=*/nullptr,\n+                                        Compiler::CompileOptions{}));\n+  }\n+}\n+\n+TEST_F(SplittingTest, SplitDiamondGraphModule) {\n+  constexpr absl::string_view module_text = R\"(\n+    HloModule shared_callee_module\n+\n+    %y {\n+      %p = f32[] parameter(0)\n+      ROOT %result = f32[] exponential(%p)\n+    }\n+\n+    %x {\n+      %p = f32[] parameter(0)\n+      %call_y = f32[] call(%p), to_apply=%y\n+      ROOT %result = f32[] cosine(%call_y)\n+    }\n+\n+    %a {\n+      %p = f32[] parameter(0)\n+      %c = f32[] constant(5.0)\n+      %call_x = f32[] call(%p), to_apply=%x\n+      ROOT %result = f32[] add(%call_x, %c)\n+    }\n+\n+    %b {\n+      %p = f32[] parameter(0)\n+      %c = f32[] constant(10.0)\n+      %call_x = f32[] call(%p), to_apply=%x\n+      ROOT %result = f32[] subtract(%call_x, %c)\n+    }\n+\n+    ENTRY %entry {\n+      %p_entry = f32[] parameter(0)\n+      %call_a = f32[] call(%p_entry), to_apply=%a\n+      %call_b = f32[] call(%p_entry), to_apply=%b\n+      ROOT %result = f32[] add(%call_a, %call_b)\n+    }\n+  )\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          ParseAndReturnVerifiedModule(module_text));\n+  TF_ASSERT_OK_AND_ASSIGN(auto module_split_group,\n+                          CreateHloModuleSplitGroup(*module));\n+\n+  // Expect all computations to be assigned somewhere.\n+  for (auto original_comp : module->computations()) {\n+    EXPECT_TRUE(module_split_group->address_book.contains(original_comp));\n+  }\n+\n+  EXPECT_EQ(module_split_group->module_splits.size(),\n+            module->computation_count());\n+\n+  TF_ASSERT_OK_AND_ASSIGN(stream_executor::Platform * platform,\n+                          PlatformUtil::GetPlatform(\"cpu\"));\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Compiler> compiler,\n+                          Compiler::GetForPlatform(platform));\n+  TF_ASSERT_OK(compiler->RunHloPasses(module->Clone(), /*executor=*/nullptr,\n+                                      Compiler::CompileOptions{}));\n+  for (auto& split : module_split_group->module_splits) {\n+    TF_EXPECT_OK(compiler->RunHloPasses(split->submodule->Clone(),\n+                                        /*executor=*/nullptr,\n+                                        Compiler::CompileOptions{}));\n+  }\n+}\n+\n+TEST_F(SplittingTest, SplitModuleWithSharedComputations) {\n+  constexpr absl::string_view module_text = R\"(\n+HloModule simple_module\n+\n+\n+// Simple alpha equivalence examples\n+\n+%fusion.1 (x: f32[]) -> f32[] {\n+  %x = f32[] parameter(0)\n+  ROOT %add = f32[] negate(f32[] %x)\n+}\n+\n+%fusion.2 (x: f32[]) -> f32[] {\n+  %x = f32[] parameter(0)\n+  ROOT %add = f32[] negate(f32[] %x)\n+}\n+\n+%add.0 (x: f32[], y: f32[]) -> f32[] {\n+  %x = f32[] parameter(0)\n+  %y = f32[] parameter(1)\n+  ROOT %add = f32[] add(f32[] %x, f32[] %y)\n+}\n+\n+%add.1 (a: f32[], b: f32[]) -> f32[] {\n+  %a = f32[] parameter(0)\n+  %b = f32[] parameter(1)\n+  ROOT %add = f32[] add(f32[] %a, f32[] %b)\n+}\n+\n+%add.2 (a: f32[], b: f32[]) -> f32[] {\n+  %a = f32[] parameter(0)\n+  %b = f32[] parameter(1)\n+  ROOT %add = f32[] add(f32[] %a, f32[] %b)\n+}\n+\n+ENTRY %main() -> f32[] {\n+\n+  %p = f32[] constant(3.3)\n+  %q = f32[] constant(-1.0)\n+  %r = f32[] constant(7.1)\n+  %s = f32[] constant(0.2)\n+\n+  %res.0 = call(%p, %q), to_apply=%add.0\n+  %res.1 = call(%p, %q), to_apply=%add.1\n+  %res.1f = f32[] fusion(%res.1), kind=kLoop, calls=%fusion.1\n+  %res.2 = call(%r, %s), to_apply=%add.2\n+  %res.3 = call(%p, %s), to_apply=%add.2\n+  %fusion = f32[] fusion(%res.3), kind=kLoop, calls=%fusion.2\n+\n+  %res.01 = f32[] add(f32[] %res.0, f32[] %res.1f)\n+  %res.23 = f32[] add(f32[] %res.2, f32[] %res.3)\n+  %res.f = f32[] add(f32[] %fusion, f32[], %res.23)\n+\n+  ROOT %result = f32[] add(f32[] %res.01, f32[] %res.f)\n+})\";\n+\n+  TF_ASSERT_OK_AND_ASSIGN(auto module,\n+                          ParseAndReturnVerifiedModule(module_text));\n+  TF_ASSERT_OK_AND_ASSIGN(auto module_split_group,\n+                          CreateHloModuleSplitGroup(*module));\n+\n+  TF_ASSERT_OK_AND_ASSIGN(stream_executor::Platform * platform,\n+                          PlatformUtil::GetPlatform(\"cpu\"));\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Compiler> compiler,\n+                          Compiler::GetForPlatform(platform));\n+  TF_ASSERT_OK(compiler->RunHloPasses(module->Clone(), /*executor=*/nullptr,\n+                                      Compiler::CompileOptions{}));\n+  for (auto& split : module_split_group->module_splits) {\n+    TF_EXPECT_OK(compiler->RunHloPasses(split->submodule->Clone(),\n+                                        /*executor=*/nullptr,\n+                                        Compiler::CompileOptions{}));\n+  }\n+}\n+\n+}  // namespace\n+}  // namespace xla::separate_compilation"
        }
    ],
    "stats": {
        "total": 940,
        "additions": 940,
        "deletions": 0
    }
}