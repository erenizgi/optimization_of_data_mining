{
    "author": "majiddadashi",
    "message": "Add support for kTfLiteInt2 to Dequantize kernels.\n\nThis change enables the Dequantize and PerChannelDequantize operations to handle 2-bit integer inputs (`kTfLiteInt2`). It includes logic to unpack the packed 2-bit integers into int8_t before performing the dequantization and adds new test cases for both per-tensor and per-channel dequantization with kTfLiteInt2.\n\nPiperOrigin-RevId: 822207279",
    "sha": "c37a4aaa58744a5ce1fcce9ee74780ced8a9c569",
    "files": [
        {
            "sha": "682995f7fc61cbe377a4c679b50d6c4fa4a5ea88",
            "filename": "tensorflow/compiler/mlir/lite/ir/tfl_ops.td",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c37a4aaa58744a5ce1fcce9ee74780ced8a9c569/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fir%2Ftfl_ops.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c37a4aaa58744a5ce1fcce9ee74780ced8a9c569/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fir%2Ftfl_ops.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Fir%2Ftfl_ops.td?ref=c37a4aaa58744a5ce1fcce9ee74780ced8a9c569",
            "patch": "@@ -4279,7 +4279,7 @@ def TFL_DequantizeOp: TFL_Op<\"dequantize\", [NoMemoryEffect]> {\n     quantization parameters.\n   }];\n \n-  let arguments = (ins TFL_TensorOf<[QI4, QI8, QUI8, QI16, F16]>:$input);\n+  let arguments = (ins TFL_TensorOf<[QI2, QI4, QI8, QUI8, QI16, F16]>:$input);\n \n   let results = (outs TFL_FpTensor:$output);\n "
        },
        {
            "sha": "9e78274405a72db0807b9056bd894bde5c950cd1",
            "filename": "tensorflow/compiler/mlir/lite/tools/versioning/op_version.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c37a4aaa58744a5ce1fcce9ee74780ced8a9c569/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftools%2Fversioning%2Fop_version.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c37a4aaa58744a5ce1fcce9ee74780ced8a9c569/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftools%2Fversioning%2Fop_version.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftools%2Fversioning%2Fop_version.cc?ref=c37a4aaa58744a5ce1fcce9ee74780ced8a9c569",
            "patch": "@@ -499,6 +499,9 @@ int GetBuiltinOperatorVersion(const OpSignature& op_sig) {\n       return 1;\n \n     case BuiltinOperator_DEQUANTIZE:\n+      if (op_sig.inputs.at(0).type == kTfLiteInt2) {\n+        return 7;\n+      }\n       if (op_sig.inputs.at(0).type == kTfLiteInt4) {\n         return 6;\n       }"
        },
        {
            "sha": "1eebfa93a37266b37a642c9530fb4ae691dc380b",
            "filename": "tensorflow/compiler/mlir/lite/tools/versioning/op_version_test.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c37a4aaa58744a5ce1fcce9ee74780ced8a9c569/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftools%2Fversioning%2Fop_version_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c37a4aaa58744a5ce1fcce9ee74780ced8a9c569/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftools%2Fversioning%2Fop_version_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftools%2Fversioning%2Fop_version_test.cc?ref=c37a4aaa58744a5ce1fcce9ee74780ced8a9c569",
            "patch": "@@ -757,6 +757,12 @@ TEST(OpVersionTest, VersioningDequantizeTest) {\n   fake_op_sig.ext_options.dequantize.is_per_channel_quantized = true;\n   EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 5);\n \n+  fake_op_sig = {\n+      .op = BuiltinOperator_DEQUANTIZE,\n+      .inputs = CreateOpSignatureTensorSpecs(kTfLiteInt2),\n+  };\n+  EXPECT_EQ(GetBuiltinOperatorVersion(fake_op_sig), 7);\n+\n   fake_op_sig = {\n       .op = BuiltinOperator_DEQUANTIZE,\n       .inputs = CreateOpSignatureTensorSpecs(kTfLiteFloat32),"
        },
        {
            "sha": "be620b9e013da156ddde5c31d05ef75ce3658a3b",
            "filename": "tensorflow/compiler/mlir/lite/tools/versioning/runtime_version.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c37a4aaa58744a5ce1fcce9ee74780ced8a9c569/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftools%2Fversioning%2Fruntime_version.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c37a4aaa58744a5ce1fcce9ee74780ced8a9c569/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftools%2Fversioning%2Fruntime_version.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftools%2Fversioning%2Fruntime_version.cc?ref=c37a4aaa58744a5ce1fcce9ee74780ced8a9c569",
            "patch": "@@ -326,6 +326,7 @@ std::string FindMinimumRuntimeVersionForOp(tflite::BuiltinOperator op_code,\n               {{BuiltinOperator_DEQUANTIZE, 4}, \"2.2.0\"},\n               {{BuiltinOperator_DEQUANTIZE, 5}, \"2.7.0\"},\n               {{BuiltinOperator_DEQUANTIZE, 6}, \"2.18.0\"},\n+              {{BuiltinOperator_DEQUANTIZE, 7}, \"2.21.0\"},\n               {{BuiltinOperator_REVERSE_SEQUENCE, 1}, \"1.14.0\"},\n               {{BuiltinOperator_EQUAL, 1}, \"1.14.0\"},\n               {{BuiltinOperator_EQUAL, 2}, \"1.14.0\"},"
        },
        {
            "sha": "f335da091550685d129923eb0633f5490f961425",
            "filename": "tensorflow/lite/core/kernels/register.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c37a4aaa58744a5ce1fcce9ee74780ced8a9c569/tensorflow%2Flite%2Fcore%2Fkernels%2Fregister.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c37a4aaa58744a5ce1fcce9ee74780ced8a9c569/tensorflow%2Flite%2Fcore%2Fkernels%2Fregister.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fcore%2Fkernels%2Fregister.cc?ref=c37a4aaa58744a5ce1fcce9ee74780ced8a9c569",
            "patch": "@@ -179,7 +179,7 @@ BuiltinOpResolver::BuiltinOpResolver() {\n              /* max_version = */ 8);\n   AddBuiltin(BuiltinOperator_DEQUANTIZE, Register_DEQUANTIZE(),\n              /* min_version = */ 1,\n-             /* max_version = */ 6);\n+             /* max_version = */ 7);\n   AddBuiltin(BuiltinOperator_PRELU, Register_PRELU());\n   AddBuiltin(BuiltinOperator_MAXIMUM, Register_MAXIMUM(),\n              /* min_version = */ 1,"
        },
        {
            "sha": "2359d13b7edd8592523f10ca17a0d11ba71cf45f",
            "filename": "tensorflow/lite/kernels/dequantize.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c37a4aaa58744a5ce1fcce9ee74780ced8a9c569/tensorflow%2Flite%2Fkernels%2Fdequantize.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c37a4aaa58744a5ce1fcce9ee74780ced8a9c569/tensorflow%2Flite%2Fkernels%2Fdequantize.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fkernels%2Fdequantize.cc?ref=c37a4aaa58744a5ce1fcce9ee74780ced8a9c569",
            "patch": "@@ -57,7 +57,8 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n \n   TF_LITE_ENSURE(context, op_context.input != nullptr);\n \n-  TF_LITE_ENSURE(context, op_context.input->type == kTfLiteInt4 ||\n+  TF_LITE_ENSURE(context, op_context.input->type == kTfLiteInt2 ||\n+                              op_context.input->type == kTfLiteInt4 ||\n                               op_context.input->type == kTfLiteUInt8 ||\n                               op_context.input->type == kTfLiteInt8 ||\n                               op_context.input->type == kTfLiteInt16 ||"
        },
        {
            "sha": "07888d7a16d05b092438c2cdd628fd3cffc88252",
            "filename": "tensorflow/lite/kernels/dequantize.h",
            "status": "modified",
            "additions": 25,
            "deletions": 2,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c37a4aaa58744a5ce1fcce9ee74780ced8a9c569/tensorflow%2Flite%2Fkernels%2Fdequantize.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c37a4aaa58744a5ce1fcce9ee74780ced8a9c569/tensorflow%2Flite%2Fkernels%2Fdequantize.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fkernels%2Fdequantize.h?ref=c37a4aaa58744a5ce1fcce9ee74780ced8a9c569",
            "patch": "@@ -72,14 +72,24 @@ inline TfLiteStatus PerChannelDequantizeImpl(TfLiteContext* context,\n     per_channel_op_params.zero_point = zero_points.data();\n   }\n   const int8_t* input_data;\n-  const size_t bytes_unpacked = input->bytes * 2;\n+  size_t bytes_unpacked;\n+  if (input->type == kTfLiteInt2) {\n+    bytes_unpacked = input->bytes * 4;\n+  } else {\n+    bytes_unpacked = input->bytes * 2;\n+  }\n   auto unpacked_input_data = std::make_unique<int8_t[]>(bytes_unpacked);\n \n   if (input->type == kTfLiteInt4) {\n     tflite::tensor_utils::UnpackPackedIntToInt8(\n         GetTensorData<int8_t>(input), GetTensorShape(input).FlatSize(),\n         /*bit_width=*/4, unpacked_input_data.get());\n     input_data = unpacked_input_data.get();\n+  } else if (input->type == kTfLiteInt2) {\n+    tflite::tensor_utils::UnpackPackedIntToInt8(\n+        GetTensorData<int8_t>(input), GetTensorShape(input).FlatSize(),\n+        /*bit_width=*/2, unpacked_input_data.get());\n+    input_data = unpacked_input_data.get();\n   } else {\n     input_data = GetTensorData<int8_t>(input);\n   }\n@@ -91,6 +101,7 @@ inline TfLiteStatus PerChannelDequantizeImpl(TfLiteContext* context,\n           GetTensorData<uint8_t>(input), GetTensorShape(output),\n           GetTensorData<float>(output));\n       break;\n+    case kTfLiteInt2:\n     case kTfLiteInt4:\n     case kTfLiteInt8:\n       reference_ops::PerChannelDequantize<int8_t>(\n@@ -115,7 +126,12 @@ TfLiteStatus DequantizeImpl(TfLiteContext* context, TfLiteNode* node,\n   op_params.zero_point = input->params.zero_point;\n   op_params.scale = input->params.scale;\n   const int8_t* input_data;\n-  const size_t bytes_unpacked = input->bytes * 2;\n+  size_t bytes_unpacked;\n+  if (input->type == kTfLiteInt2) {\n+    bytes_unpacked = input->bytes * 4;\n+  } else {\n+    bytes_unpacked = input->bytes * 2;\n+  }\n   auto unpacked_input_data = std::make_unique<int8_t[]>(bytes_unpacked);\n \n   if (input->type == kTfLiteInt4) {\n@@ -124,6 +140,12 @@ TfLiteStatus DequantizeImpl(TfLiteContext* context, TfLiteNode* node,\n         GetTensorData<int8_t>(input), GetTensorShape(input).FlatSize(),\n         /*bit_width=*/4, unpacked_input_data.get());\n     input_data = unpacked_input_data.get();\n+  } else if (input->type == kTfLiteInt2) {\n+    // Use GetTensorShape(input).FlatSize() for num_elements.\n+    tflite::tensor_utils::UnpackPackedIntToInt8(\n+        GetTensorData<int8_t>(input), GetTensorShape(input).FlatSize(),\n+        /*bit_width=*/2, unpacked_input_data.get());\n+    input_data = unpacked_input_data.get();\n   } else {\n     input_data = GetTensorData<int8_t>(input);\n   }\n@@ -140,6 +162,7 @@ TfLiteStatus DequantizeImpl(TfLiteContext* context, TfLiteNode* node,\n             GetTensorShape(output), GetTensorData<float>(output));\n       }\n       break;\n+    case kTfLiteInt2:\n     case kTfLiteInt4:\n     case kTfLiteInt8:\n       if (kernel_type == kReference) {"
        },
        {
            "sha": "d60bebb3049b5ffa461efa5cb5cdc17e90d5e7db",
            "filename": "tensorflow/lite/kernels/dequantize_test.cc",
            "status": "modified",
            "additions": 35,
            "deletions": 4,
            "changes": 39,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c37a4aaa58744a5ce1fcce9ee74780ced8a9c569/tensorflow%2Flite%2Fkernels%2Fdequantize_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c37a4aaa58744a5ce1fcce9ee74780ced8a9c569/tensorflow%2Flite%2Fkernels%2Fdequantize_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fkernels%2Fdequantize_test.cc?ref=c37a4aaa58744a5ce1fcce9ee74780ced8a9c569",
            "patch": "@@ -19,12 +19,8 @@ limitations under the License.\n \n #include <gmock/gmock.h>\n #include <gtest/gtest.h>\n-#include \"absl/memory/memory.h\"\n #include \"Eigen/Core\"  // from @eigen_archive\n-#include \"flatbuffers/flatbuffers.h\"  // from @flatbuffers\n-#include \"tensorflow/lite/core/api/op_resolver.h\"\n #include \"tensorflow/lite/core/interpreter.h\"\n-#include \"tensorflow/lite/kernels/internal/types.h\"\n #include \"tensorflow/lite/kernels/test_util.h\"\n #include \"tensorflow/lite/schema/schema_generated.h\"\n \n@@ -75,6 +71,15 @@ class DequantizeOpModel : public SingleOpModel {\n                        data_int8.data() + data_int8.size());\n   }\n \n+  template <typename T>\n+  void SetInputInt2(int input, const std::vector<T> data) {\n+    auto non_const = *const_cast<std::vector<T>*>(&data);\n+    std::vector<int8_t> data_int8(non_const.size());\n+    std::copy(non_const.begin(), non_const.end(), data_int8.begin());\n+    PopulateTensor2bit(input, 0, data_int8.data(),\n+                       data_int8.data() + data_int8.size());\n+  }\n+\n   std::vector<float> GetOutput() { return ExtractVector<float>(output_); }\n \n  protected:\n@@ -92,6 +97,15 @@ TEST(DequantizeOpTest, Int4) {\n               ElementsAreArray(ArrayFloatNear({4, 3.5, -3, -3.5})));\n }\n \n+TEST(DequantizeOpTest, Int2) {\n+  DequantizeOpModel m(TensorType_INT2, {1, 4}, 0.5, -1, 6);\n+\n+  m.SetInputInt2<int8_t>(0, {1, 0, -1, -2});\n+  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n+  EXPECT_THAT(m.GetOutput(),\n+              ElementsAreArray(ArrayFloatNear({1.0, 0.5, 0.0, -0.5})));\n+}\n+\n TEST(DequantizeOpTest, Uint8) {\n   // [-63.5, 64] -> scale=0.5 zero_point=127 for UINT8\n   DequantizeOpModel m(TensorType_UINT8, {2, 5}, 0.5, 127, 1);\n@@ -185,5 +199,22 @@ TEST(DequantizePerChannelOpTest, Int8) {\n                   {-63.5, -63, -62.5, -62, -61.5, 62, 62.5, 63, 63.5, 64})));\n }\n \n+TEST(DequantizePerChannelOpTest, Int2) {\n+  // scales={0.5, 1.0}, zero_points={-1, 0}, channel_dim=0\n+  DequantizePerChannelOpModel m(TensorType_INT2, {2, 2}, {0.5, 1.0}, {-1, 0}, 0,\n+                                6);\n+  m.SetInputInt2<int8_t>(0, {1, 0, -1, -2});\n+  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n+  // Dequantization formula: (val - zp) * scale\n+  // Channel 0: scale=0.5, zp=-1.\n+  // val=1: (1 - (-1)) * 0.5 = 1.0\n+  // val=0: (0 - (-1)) * 0.5 = 0.5\n+  // Channel 1: scale=1.0, zp=0\n+  // val=-1: (-1 - 0) * 1.0 = -1.0\n+  // val=-2: (-2 - 0) * 1.0 = -2.0\n+  EXPECT_THAT(m.GetOutput(),\n+              ElementsAreArray(ArrayFloatNear({1.0, 0.5, -1.0, -2.0})));\n+}\n+\n }  // namespace\n }  // namespace tflite"
        },
        {
            "sha": "ded50b733705fbccd86eb889a24ae9456e7d247c",
            "filename": "tensorflow/lite/kernels/register_ref.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c37a4aaa58744a5ce1fcce9ee74780ced8a9c569/tensorflow%2Flite%2Fkernels%2Fregister_ref.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c37a4aaa58744a5ce1fcce9ee74780ced8a9c569/tensorflow%2Flite%2Fkernels%2Fregister_ref.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Flite%2Fkernels%2Fregister_ref.cc?ref=c37a4aaa58744a5ce1fcce9ee74780ced8a9c569",
            "patch": "@@ -380,7 +380,7 @@ BuiltinRefOpResolver::BuiltinRefOpResolver() {\n              /* max_version = */ 8);\n   AddBuiltin(BuiltinOperator_DEQUANTIZE, Register_DEQUANTIZE_REF(),\n              /* min_version = */ 1,\n-             /* max_version = */ 6);\n+             /* max_version = */ 7);\n   AddBuiltin(BuiltinOperator_PRELU, Register_PRELU_REF());\n   AddBuiltin(BuiltinOperator_MAXIMUM, Register_MAXIMUM_REF(),\n              /* min_version = */ 1,"
        }
    ],
    "stats": {
        "total": 85,
        "additions": 75,
        "deletions": 10
    }
}