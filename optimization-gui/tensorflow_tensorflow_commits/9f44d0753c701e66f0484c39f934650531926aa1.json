{
    "author": "ZixuanJiang",
    "message": "Export `sdy.replicated_to_unreduced` to a manual computation.\n\nThe `sdy.replicated_to_unreduced` operation is lowered to a `sdy.manual_computation`. The manual computation checks if the current device is the first one in the partition.\n* If yes, the input tensor is used\n* Otherwise, a tensor of zeros is produced.\n\nPiperOrigin-RevId: 845947531",
    "sha": "9f44d0753c701e66f0484c39f934650531926aa1",
    "files": [
        {
            "sha": "edb5c3b54457dfb2d528da74683f6078b5d43a14",
            "filename": "third_party/xla/xla/service/spmd/shardy/stablehlo_round_trip/export_manual_reduction_collectives.cc",
            "status": "modified",
            "additions": 71,
            "deletions": 20,
            "changes": 91,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f44d0753c701e66f0484c39f934650531926aa1/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fexport_manual_reduction_collectives.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f44d0753c701e66f0484c39f934650531926aa1/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fexport_manual_reduction_collectives.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fexport_manual_reduction_collectives.cc?ref=9f44d0753c701e66f0484c39f934650531926aa1",
            "patch": "@@ -19,6 +19,7 @@ limitations under the License.\n #include <cstdint>\n #include <functional>\n #include <memory>\n+#include <utility>\n \n #include \"absl/log/check.h\"\n #include \"absl/log/log.h\"\n@@ -259,6 +260,25 @@ int64_t convertReduceScatter(sdy::ReduceScatterOp op, int64_t nextChannelId,\n   return nextChannelId;\n }\n \n+std::pair<llvm::StringMap<Value>, llvm::StringMap<Value>>\n+getAxesCoordinateAndSize(OpBuilder& builder, mlir::Location loc,\n+                         MeshAttr mesh) {\n+  Value partitionId = stablehlo::PartitionIdOp::create(builder, loc);\n+  Value currentRem = stablehlo::ConvertOp::create(\n+      builder, loc, RankedTensorType::get({}, builder.getIntegerType(32)),\n+      partitionId);\n+  llvm::StringMap<Value> axisSizes, axisCoordinates;\n+  for (sdy::MeshAxisAttr axis : llvm::reverse(mesh.getAxes())) {\n+    Value axisSize = stablehlo::ConstantOp::create(\n+        builder, loc, builder.getI32IntegerAttr(axis.getSize()));\n+    axisSizes[axis.getName()] = axisSize;\n+    axisCoordinates[axis.getName()] =\n+        stablehlo::RemOp::create(builder, loc, currentRem, axisSize);\n+    currentRem = stablehlo::DivOp::create(builder, loc, currentRem, axisSize);\n+  }\n+  return {axisCoordinates, axisSizes};\n+}\n+\n void convertShardedToUnreduced(sdy::ShardedToUnreducedOp op,\n                                mlir::IRRewriter& rewriter) {\n   TensorShardingAttr outSharding = op.getOutSharding();\n@@ -289,23 +309,8 @@ void convertShardedToUnreduced(sdy::ShardedToUnreducedOp op,\n         Value broadcast = stablehlo::BroadcastOp::create(\n             blockBuilder, loc, outputType, zero, outputType.getShape());\n \n-        // Decompose partitionId into axis coordinates.\n-        Value partitionId = stablehlo::PartitionIdOp::create(blockBuilder, loc);\n-        Value currentRem = stablehlo::ConvertOp::create(\n-            blockBuilder, loc,\n-            RankedTensorType::get({}, blockBuilder.getIntegerType(32)),\n-            partitionId);\n-        llvm::StringMap<Value> axisSizes, axisCoordinates;\n-        for (sdy::MeshAxisAttr axis : llvm::reverse(mesh.getAxes())) {\n-          Value axisSize = stablehlo::ConstantOp::create(\n-              blockBuilder, loc,\n-              blockBuilder.getI32IntegerAttr(axis.getSize()));\n-          axisSizes[axis.getName()] = axisSize;\n-          axisCoordinates[axis.getName()] =\n-              stablehlo::RemOp::create(blockBuilder, loc, currentRem, axisSize);\n-          currentRem =\n-              stablehlo::DivOp::create(blockBuilder, loc, currentRem, axisSize);\n-        }\n+        auto [axisCoordinates, axisSizes] =\n+            getAxesCoordinateAndSize(blockBuilder, loc, mesh);\n \n         SmallVector<Value> offsets;\n         offsets.reserve(outputType.getRank());\n@@ -348,6 +353,48 @@ void convertShardedToUnreduced(sdy::ShardedToUnreducedOp op,\n   rewriter.replaceOp(op, manualComputation);\n }\n \n+void convertReplicatedToUnreduced(sdy::ReplicatedToUnreducedOp op,\n+                                  mlir::IRRewriter& rewriter) {\n+  TensorShardingAttr outSharding = op.getOutSharding();\n+  MeshAttr mesh = outSharding.getMesh(op);\n+\n+  mlir::Location loc = op.getLoc();\n+  rewriter.setInsertionPoint(op);\n+\n+  ManualComputationOp manualComputation = createFullyManualComputation(\n+      loc, op.getTensor(), outSharding, mesh, rewriter,\n+      [&](mlir::BlockArgument arg, OpBuilder& blockBuilder) {\n+        auto [axisCoordinates, axisSizes] =\n+            getAxesCoordinateAndSize(blockBuilder, loc, mesh);\n+        (void)axisSizes;\n+\n+        Value i32Zero = stablehlo::ConstantOp::create(\n+            blockBuilder, loc, blockBuilder.getI32IntegerAttr(0));\n+        Value pred = nullptr;\n+        for (AxisRefAttr axis : op.getAxes()) {\n+          CHECK(!axis.getSubAxisInfo()) << \"Sub-axes not supported in \"\n+                                           \"ReplicatedToUnreducedOp.\";\n+          Value coord = axisCoordinates[axis.getName()];\n+          Value isZero =\n+              stablehlo::CompareOp::create(blockBuilder, loc, coord, i32Zero,\n+                                           stablehlo::ComparisonDirection::EQ);\n+          pred = pred\n+                     ? stablehlo::AndOp::create(blockBuilder, loc, pred, isZero)\n+                     : isZero;\n+        }\n+        CHECK(pred != nullptr) << \"No replicated-to-unreduced axes.\";\n+\n+        RankedTensorType type = mlir::cast<RankedTensorType>(arg.getType());\n+        Value zeroVal = stablehlo::ConstantOp::create(\n+            blockBuilder, loc, blockBuilder.getZeroAttr(type.getElementType()));\n+        Value zeroBroadcast = stablehlo::BroadcastOp::create(\n+            blockBuilder, loc, type, zeroVal, type.getShape());\n+        return stablehlo::SelectOp::create(blockBuilder, loc, pred, arg,\n+                                           zeroBroadcast);\n+      });\n+  rewriter.replaceOp(op, manualComputation);\n+}\n+\n void syncInOutUnreducedAxes(mlir::Operation* op) {\n   Value input = op->getOperand(0);\n   TensorShardingAttr outSharding = sdy::getSharding(op->getResult(0));\n@@ -415,6 +462,9 @@ class StablehloExportManualReductionCollectivesPass\n       } else if (auto shardedToUnreduced =\n                      mlir::dyn_cast<sdy::ShardedToUnreducedOp>(op)) {\n         convertShardedToUnreduced(shardedToUnreduced, rewriter);\n+      } else if (auto replicatedToUnreduced =\n+                     mlir::dyn_cast<sdy::ReplicatedToUnreducedOp>(op)) {\n+        convertReplicatedToUnreduced(replicatedToUnreduced, rewriter);\n       }\n     });\n   }\n@@ -424,9 +474,10 @@ class StablehloExportManualReductionCollectivesPass\n   }\n \n   StringRef getDescription() const override {\n-    return \"Exports `sdy.all_reduce`, that originate from user defined \"\n-           \"shardings with unreduced axes, to `stablehlo.all_reduce` inside a \"\n-           \"fully manual `sdy.manual_computation`\";\n+    return \"Exports `sdy.all_reduce`, `sdy.reduce_scatter`, \"\n+           \"`sdy.sharded_to_unreduced` and `sdy.replicated_to_unreduced` that \"\n+           \"originate from user-defined shardings with unreduced axes. The \"\n+           \"exported ops are inside a full manual `sdy.manual_computation`.\";\n   }\n \n   void getDependentDialects(mlir::DialectRegistry& registry) const final {"
        },
        {
            "sha": "be473a371ca545e4ed0abffc2568035c43863657",
            "filename": "third_party/xla/xla/service/spmd/shardy/stablehlo_round_trip/export_manual_reduction_collectives.h",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f44d0753c701e66f0484c39f934650531926aa1/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fexport_manual_reduction_collectives.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f44d0753c701e66f0484c39f934650531926aa1/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fexport_manual_reduction_collectives.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Fstablehlo_round_trip%2Fexport_manual_reduction_collectives.h?ref=9f44d0753c701e66f0484c39f934650531926aa1",
            "patch": "@@ -23,11 +23,10 @@ limitations under the License.\n namespace xla {\n namespace sdy {\n \n-// TODO(tomnatan): mention reduce-scatter and how collectives are marked.\n-// TODO(tomnatan): mention if the shard map is fully manual or not.\n-\n-// Exports `sdy.all_reduce`, that originate from user defined shardings with\n-// unreduced axes, to `stablehlo.all_reduce` inside an `sdy.manual_computation`.\n+// Exports `sdy.all_reduce`, `sdy.reduce_scatter`, `sdy.sharded_to_unreduced`\n+// and `sdy.replicated_to_unreduced` that originate from user-defined shardings\n+// with unreduced axes. The exported ops are inside a full manual\n+// `sdy.manual_computation`.\n std::unique_ptr<mlir::Pass>\n createStablehloExportManualReductionCollectivesPass();\n "
        },
        {
            "sha": "be4e2a5243a255e24170ec8f2e5da43ee53a34fe",
            "filename": "third_party/xla/xla/service/spmd/shardy/test/stablehlo_export_manual_reduction_collectives.mlir",
            "status": "modified",
            "additions": 35,
            "deletions": 0,
            "changes": 35,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9f44d0753c701e66f0484c39f934650531926aa1/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fstablehlo_export_manual_reduction_collectives.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9f44d0753c701e66f0484c39f934650531926aa1/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fstablehlo_export_manual_reduction_collectives.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fshardy%2Ftest%2Fstablehlo_export_manual_reduction_collectives.mlir?ref=9f44d0753c701e66f0484c39f934650531926aa1",
            "patch": "@@ -371,3 +371,38 @@ func.func @sharded_to_unreduced(%arg0: tensor<16x16xf32> {sdy.sharding = #sdy.sh\n   %0 = sdy.sharded_to_unreduced [{\"x\"}, {}] %arg0 out_sharding=<@mesh, [{}, {\"y\"}], unreduced={\"x\"}> : tensor<16x16xf32>\n   return %0 : tensor<16x16xf32>\n }\n+\n+// -----\n+\n+sdy.mesh @mesh = <[\"x\"=4, \"y\"=2, \"z\"=3]>\n+\n+// CHECK-LABEL: func @replicated_to_unreduced\n+func.func @replicated_to_unreduced(%arg0: tensor<16x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}], unreduced={\"y\"}>}) -> tensor<16x16xf32> {\n+  // CHECK-NEXT: %[[MANUAL_COMP:.*]] = sdy.manual_computation(%arg0)\n+  // CHECK-SAME:     in_shardings=[<@mesh, [{}, {}], unreduced={\"y\"}>]\n+  // CHECK-SAME:     out_shardings=[<@mesh, [{}, {}], unreduced={\"x\", \"y\", \"z\"}>]\n+  // CHECK-SAME:     manual_axes={\"x\", \"y\", \"z\"} (%arg1: tensor<16x16xf32>) {\n+  // CHECK-NEXT:   %[[PID:.*]] = stablehlo.partition_id : tensor<ui32>\n+  // CHECK-NEXT:   %[[PID_I32:.*]] = stablehlo.convert %[[PID]] : (tensor<ui32>) -> tensor<i32>\n+  // CHECK-NEXT:   %[[C3:.*]] = stablehlo.constant dense<3> : tensor<i32>\n+  // CHECK-NEXT:   %[[REM_Z:.*]] = stablehlo.remainder %[[PID_I32]], %[[C3]] : tensor<i32>\n+  // CHECK-NEXT:   %[[DIV_Z:.*]] = stablehlo.divide %[[PID_I32]], %[[C3]] : tensor<i32>\n+  // CHECK-NEXT:   %[[C2:.*]] = stablehlo.constant dense<2> : tensor<i32>\n+  // CHECK-NEXT:   %[[REM_Y:.*]] = stablehlo.remainder %[[DIV_Z]], %[[C2]] : tensor<i32>\n+  // CHECK-NEXT:   %[[DIV_Y:.*]] = stablehlo.divide %[[DIV_Z]], %[[C2]] : tensor<i32>\n+  // CHECK-NEXT:   %[[C4:.*]] = stablehlo.constant dense<4> : tensor<i32>\n+  // CHECK-NEXT:   %[[REM_X:.*]] = stablehlo.remainder %[[DIV_Y]], %[[C4]] : tensor<i32>\n+  // CHECK-NEXT:   %[[DIV_X:.*]] = stablehlo.divide %[[DIV_Y]], %[[C4]] : tensor<i32>\n+  // CHECK-NEXT:   %[[C0:.*]] = stablehlo.constant dense<0> : tensor<i32>\n+  // CHECK-NEXT:   %[[CMP_X:.*]] = stablehlo.compare  EQ, %[[REM_X]], %[[C0]] : (tensor<i32>, tensor<i32>) -> tensor<i1>\n+  // CHECK-NEXT:   %[[CMP_Z:.*]] = stablehlo.compare  EQ, %[[REM_Z]], %[[C0]] : (tensor<i32>, tensor<i32>) -> tensor<i1>\n+  // CHECK-NEXT:   %[[PRED:.*]] = stablehlo.and %[[CMP_X]], %[[CMP_Z]] : tensor<i1>\n+  // CHECK-NEXT:   %[[ZERO:.*]] = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n+  // CHECK-NEXT:   %[[ZERO_BCAST:.*]] = stablehlo.broadcast %[[ZERO]], sizes = [16, 16] : (tensor<f32>) -> tensor<16x16xf32>\n+  // CHECK-NEXT:   %[[SELECT:.*]] = stablehlo.select %[[PRED]], %arg1, %[[ZERO_BCAST]] : tensor<i1>, tensor<16x16xf32>\n+  // CHECK-NEXT:   sdy.return %[[SELECT]] : tensor<16x16xf32>\n+  // CHECK-NEXT: } : (tensor<16x16xf32>) -> tensor<16x16xf32>\n+  // CHECK-NEXT: return %[[MANUAL_COMP]] : tensor<16x16xf32>\n+  %0 = sdy.replicated_to_unreduced {\"x\", \"z\"} %arg0 out_sharding=<@mesh, [{}, {}], unreduced={\"x\", \"y\", \"z\"}> : tensor<16x16xf32>\n+  return %0 : tensor<16x16xf32>\n+}"
        }
    ],
    "stats": {
        "total": 135,
        "additions": 110,
        "deletions": 25
    }
}