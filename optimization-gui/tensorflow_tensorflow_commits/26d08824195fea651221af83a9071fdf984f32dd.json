{
    "author": "beckerhe",
    "message": "Add proto serialization for GpuExecutable\n\nThis is adding `GpuExecutuable::ToProto` and `GpuExecutable::FromProto` which allow us to [de]serialize an instance of `GpuExecutable` and later reconstruct it.\n\nPiperOrigin-RevId: 826470601",
    "sha": "26d08824195fea651221af83a9071fdf984f32dd",
    "files": [
        {
            "sha": "b378592f915eaa9fea6f4be960d5ec1e18ee15c2",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/26d08824195fea651221af83a9071fdf984f32dd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/26d08824195fea651221af83a9071fdf984f32dd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=26d08824195fea651221af83a9071fdf984f32dd",
            "patch": "@@ -743,6 +743,7 @@ cc_library(\n         \"//xla/backends/gpu/runtime:thunk\",\n         \"//xla/backends/gpu/runtime:thunk_buffer_debug_pass\",\n         \"//xla/backends/gpu/runtime:thunk_pass_pipeline\",\n+        \"//xla/backends/gpu/runtime:thunk_proto_deserialization\",\n         \"//xla/core/collectives:clique_key\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/service:buffer_assignment\",\n@@ -773,6 +774,7 @@ cc_library(\n         \"//xla/tsl/platform:logging\",\n         \"//xla/tsl/platform:status\",\n         \"//xla/tsl/platform:statusor\",\n+        \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/base:nullability\",\n         \"@com_google_absl//absl/container:flat_hash_map\","
        },
        {
            "sha": "ca4c1f304dbf3969d77e0c892afc80595c196d72",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable.cc",
            "status": "modified",
            "additions": 136,
            "deletions": 3,
            "changes": 139,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/26d08824195fea651221af83a9071fdf984f32dd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/26d08824195fea651221af83a9071fdf984f32dd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc?ref=26d08824195fea651221af83a9071fdf984f32dd",
            "patch": "@@ -47,6 +47,7 @@ limitations under the License.\n #include \"xla/backends/gpu/runtime/thunk.h\"\n #include \"xla/backends/gpu/runtime/thunk_buffer_debug_pass.h\"\n #include \"xla/backends/gpu/runtime/thunk_pass_pipeline.h\"\n+#include \"xla/backends/gpu/runtime/thunk_proto_deserialization.h\"\n #include \"xla/core/collectives/clique_key.h\"\n #include \"xla/executable_run_options.h\"\n #include \"xla/hlo/ir/hlo_input_output_alias_config.h\"\n@@ -60,6 +61,7 @@ limitations under the License.\n #include \"xla/service/gpu/buffer_allocations.h\"\n #include \"xla/service/gpu/gpu_constants.h\"\n #include \"xla/service/gpu/gpu_executable_run_options.h\"\n+#include \"xla/service/gpu/ir_emission_utils.h\"\n #include \"xla/service/gpu/resource_requests.h\"\n #include \"xla/service/gpu/stream_executor_util.h\"\n #include \"xla/service/hlo_value.h\"\n@@ -1104,8 +1106,8 @@ GetOutputInfo(const HloModule& hlo_module, const BufferAssignment& assignment) {\n   return output;\n }\n \n-OutputInfoProto GpuExecutable::OutputInfo::ToProto() const {\n-  OutputInfoProto proto;\n+GpuExecutableProto::OutputInfoProto GpuExecutable::OutputInfo::ToProto() const {\n+  GpuExecutableProto::OutputInfoProto proto;\n   proto.set_allocation_index(allocation_index);\n   proto.set_passthrough(passthrough);\n \n@@ -1130,7 +1132,7 @@ OutputInfoProto GpuExecutable::OutputInfo::ToProto() const {\n }\n \n absl::StatusOr<GpuExecutable::OutputInfo> GpuExecutable::OutputInfo::FromProto(\n-    const OutputInfoProto& proto) {\n+    const GpuExecutableProto::OutputInfoProto& proto) {\n   OutputInfo output_info;\n   output_info.allocation_index = proto.allocation_index();\n   output_info.passthrough = proto.passthrough();\n@@ -1155,5 +1157,136 @@ absl::StatusOr<GpuExecutable::OutputInfo> GpuExecutable::OutputInfo::FromProto(\n   }\n   return output_info;\n }\n+\n+GpuExecutableProto::ConstantInfoProto GpuExecutable::ConstantInfo::ToProto()\n+    const {\n+  GpuExecutableProto::ConstantInfoProto proto;\n+  proto.set_symbol_name(symbol_name);\n+  *proto.mutable_content() = content.ToProto();\n+  proto.set_allocation_index(allocation_index);\n+  return proto;\n+}\n+\n+GpuExecutable::ConstantInfo GpuExecutable::ConstantInfo::FromProto(\n+    const GpuExecutableProto::ConstantInfoProto& proto) {\n+  return ConstantInfo{\n+      /*symbol_name=*/proto.symbol_name(),\n+      /*content=*/DenseDataIntermediate::FromProto(proto.content()),\n+      /*allocation_index=*/static_cast<int>(proto.allocation_index())};\n+}\n+\n+absl::StatusOr<GpuExecutableProto> GpuExecutable::ToProto() const {\n+  GpuExecutableProto proto;\n+  proto.set_binary(binary_.data(), binary_.size());\n+  proto.set_asm_text(text_);\n+  proto.mutable_dnn_compiled_graphs()->insert(dnn_compiled_graphs_.cbegin(),\n+                                              dnn_compiled_graphs_.cend());\n+\n+  *proto.mutable_gpu_compute_capability() = gpu_version_.ToProto();\n+\n+  TF_ASSIGN_OR_RETURN(*proto.mutable_thunk(), thunks_->ToProto());\n+\n+  proto.set_module_name(module_name_);\n+  *proto.mutable_program_shape() = program_shape_.ToProto();\n+\n+  absl::Span<const BufferAllocation* const> allocations = GetAllocations();\n+  proto.mutable_buffer_allocations()->Reserve(allocations.size());\n+  for (const auto& allocation : allocations) {\n+    proto.mutable_buffer_allocations()->Add(allocation->ToProto());\n+  }\n+\n+  if (hlo_module_ != nullptr) {\n+    *proto.mutable_hlo_module() = hlo_module_->ToProtoWithConfig();\n+  }\n+\n+  proto.mutable_output_info_map()->Reserve(output_info_.size());\n+  for (const auto& [shape_index, output_info] : output_info_) {\n+    auto map_entry = proto.add_output_info_map();\n+    *map_entry->mutable_shape_index() = shape_index.ToProto();\n+    *map_entry->mutable_output_info() = output_info.ToProto();\n+  }\n+\n+  proto.mutable_constants()->Reserve(constants_.size());\n+  for (const auto& constant : constants_) {\n+    *proto.add_constants() = constant.ToProto();\n+  }\n+\n+  return proto;\n+}\n+\n+absl::StatusOr<std::unique_ptr<GpuExecutable>> GpuExecutable::FromProto(\n+    const GpuExecutableProto& proto,\n+    const se::DeviceDescription& device_description) {\n+  Params params;\n+  params.enable_debug_info_manager = false;\n+  params.asm_text = proto.asm_text();\n+  const std::string& binary = proto.binary();\n+  params.binary.assign(binary.begin(), binary.end());\n+  params.buffer_assignment = nullptr;\n+  if (proto.has_hlo_module()) {\n+    TF_ASSIGN_OR_RETURN(\n+        params.debug_module,\n+        HloModule::CreateFromProtoWithConfig(proto.hlo_module()));\n+  }\n+\n+  params.mlir_allocations.emplace();\n+  params.mlir_allocations->reserve(proto.buffer_allocations().size());\n+  for (const BufferAllocationProto& allocation_proto :\n+       proto.buffer_allocations()) {\n+    params.mlir_allocations->push_back(\n+        BufferAllocation::FromProto(allocation_proto));\n+  }\n+\n+  for (const auto& [key, value] : proto.dnn_compiled_graphs()) {\n+    params.dnn_compiled_graphs.emplace(key, value);\n+  }\n+\n+  TF_ASSIGN_OR_RETURN(\n+      stream_executor::GpuComputeCapability gpu_compute_capability,\n+      stream_executor::GpuComputeCapability::FromProto(\n+          proto.gpu_compute_capability()));\n+\n+  if (gpu_compute_capability != device_description.gpu_compute_capability()) {\n+    return absl::InvalidArgumentError(absl::StrFormat(\n+        \"GPU compute capability of serialized executable doesn't match target \"\n+        \"device capability. (serialized: %s, target: %s)\",\n+        gpu_compute_capability.ToString(),\n+        device_description.gpu_compute_capability().ToString()));\n+  }\n+\n+  params.device_description = device_description;\n+\n+  TF_ASSIGN_OR_RETURN(\n+      std::unique_ptr<Thunk> thunk,\n+      DeserializeThunkProto(proto.thunk(), params.mlir_allocations.value()));\n+\n+  if (dynamic_cast<const SequentialThunk*>(thunk.get()) == nullptr) {\n+    return absl::InvalidArgumentError(\n+        \"The top-most serialized thunk in the GPU Executable is not a \"\n+        \"SequentialThunk!\");\n+  }\n+\n+  params.executable = unique_ptr_down_cast<SequentialThunk>(std::move(thunk));\n+\n+  params.constants.reserve(proto.constants().size());\n+  for (const auto& constant_proto : proto.constants()) {\n+    params.constants.push_back(ConstantInfo::FromProto(constant_proto));\n+  }\n+\n+  params.output_info.reserve(proto.output_info_map().size());\n+  for (const auto& output_info_proto : proto.output_info_map()) {\n+    ShapeIndex shape_index =\n+        ShapeIndex::FromProto(output_info_proto.shape_index());\n+    TF_ASSIGN_OR_RETURN(OutputInfo output_info,\n+                        OutputInfo::FromProto(output_info_proto.output_info()));\n+    params.output_info.emplace(std::move(shape_index), std::move(output_info));\n+  }\n+\n+  params.module_name = proto.module_name();\n+  TF_ASSIGN_OR_RETURN(params.program_shape,\n+                      ProgramShape::FromProto(proto.program_shape()));\n+\n+  return Create(std::move(params));\n+}\n }  // namespace gpu\n }  // namespace xla"
        },
        {
            "sha": "60333aa1811a56025e4aba267e7b57fd38aebad6",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable.h",
            "status": "modified",
            "additions": 14,
            "deletions": 2,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/26d08824195fea651221af83a9071fdf984f32dd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/26d08824195fea651221af83a9071fdf984f32dd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.h?ref=26d08824195fea651221af83a9071fdf984f32dd",
            "patch": "@@ -70,6 +70,11 @@ class GpuExecutable : public Executable {\n     std::string symbol_name;\n     DenseDataIntermediate content;\n     int allocation_index = -1;\n+\n+    GpuExecutableProto::ConstantInfoProto ToProto() const;\n+\n+    static ConstantInfo FromProto(\n+        const GpuExecutableProto::ConstantInfoProto& proto);\n   };\n \n   struct OutputInfo {\n@@ -83,8 +88,9 @@ class GpuExecutable : public Executable {\n     // would indicate the aliased parameter), and what kind of alias it is.\n     std::optional<HloInputOutputAliasConfig::Alias> alias_config;\n \n-    OutputInfoProto ToProto() const;\n-    static absl::StatusOr<OutputInfo> FromProto(const OutputInfoProto& proto);\n+    GpuExecutableProto::OutputInfoProto ToProto() const;\n+    static absl::StatusOr<OutputInfo> FromProto(\n+        const GpuExecutableProto::OutputInfoProto& proto);\n \n     friend bool operator==(const OutputInfo& lhs, const OutputInfo& rhs) {\n       return std::tie(lhs.allocation_index, lhs.passthrough,\n@@ -210,6 +216,12 @@ class GpuExecutable : public Executable {\n \n   absl::Status VerboseAllocationError(absl::Status s);\n \n+  static absl::StatusOr<std::unique_ptr<GpuExecutable>> FromProto(\n+      const GpuExecutableProto&,\n+      const se::DeviceDescription& device_description);\n+\n+  absl::StatusOr<GpuExecutableProto> ToProto() const;\n+\n  private:\n   // Use GpuExecutable::Create() to create an instance.\n   explicit GpuExecutable(Params params,"
        },
        {
            "sha": "4e9872efd8cb870516dc02c54a109743b3e1b510",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable.proto",
            "status": "modified",
            "additions": 71,
            "deletions": 7,
            "changes": 78,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/26d08824195fea651221af83a9071fdf984f32dd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/26d08824195fea651221af83a9071fdf984f32dd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.proto?ref=26d08824195fea651221af83a9071fdf984f32dd",
            "patch": "@@ -2,15 +2,79 @@ syntax = \"proto3\";\n \n package xla.gpu;\n \n+import \"xla/backends/gpu/runtime/thunk.proto\";\n+import \"xla/service/gpu/ir_emission_utils.proto\";\n import \"xla/service/hlo.proto\";\n+import \"xla/shape_util.proto\";\n+import \"xla/stream_executor/cuda/cuda_compute_capability.proto\";\n+import \"xla/stream_executor/device_description.proto\";\n+import \"xla/xla.proto\";\n+import \"xla/xla_data.proto\";\n \n-message OutputInfoProto {\n-  // This output is part of the following buffer allocation\n-  int64 allocation_index = 1;\n+message GpuExecutableProto {\n+  // The binary of the executable.\n+  //\n+  // For CUDA, this is a cubin binary.\n+  // For ROCm, this is a hsaco binary.\n+  bytes binary = 1;\n \n-  // True when this output is passed through from an input parameter\n-  bool passthrough = 2;\n+  // The PTX of the executable. (Only applicable to CUDA)\n+  string asm_text = 2;\n \n-  // Describes whether and how this output aliases with an input parameter\n-  optional xla.HloInputOutputAliasProto.AliasEntryProto alias_config = 3;\n+  // The DNN compiled graphs of the executable.\n+  //\n+  // The key is the DNN kernel name, and the value is the compiled graph\n+  // serialized to JSON. (Only applicable to cuDNN)\n+  map<string, string> dnn_compiled_graphs = 3;\n+\n+  // The target compute capability of the executable.\n+  stream_executor.GpuComputeCapabilityProto gpu_compute_capability = 4;\n+\n+  // The HLO module of the executable - for debugging purposes only.\n+  xla.HloModuleProtoWithConfig hlo_module = 5;\n+\n+  // The thunk tree of the executable.\n+  ThunkProto thunk = 6;\n+\n+  // The name of the HLO module - for debugging purposes only.\n+  string module_name = 7;\n+\n+  // The shape of the program (parameters and result).\n+  xla.ProgramShapeProto program_shape = 8;\n+\n+  // The buffer allocations of the executable.\n+  repeated BufferAllocationProto buffer_allocations = 9;\n+\n+  message OutputInfoProto {\n+    // This output is part of the following buffer allocation\n+    int64 allocation_index = 1;\n+\n+    // True when this output is passed through from an input parameter\n+    bool passthrough = 2;\n+\n+    // Describes whether and how this output aliases with an input parameter\n+    optional xla.HloInputOutputAliasProto.AliasEntryProto alias_config = 3;\n+  }\n+\n+  message OutputInfoMapEntry {\n+    xla.ShapeIndexProto shape_index = 1;\n+    OutputInfoProto output_info = 2;\n+  }\n+\n+  // Map from output shape index to output info.\n+  repeated OutputInfoMapEntry output_info_map = 10;\n+\n+  message ConstantInfoProto {\n+    // The name of the constant in the HLO module.\n+    string symbol_name = 1;\n+\n+    // The content of the constant - this can be large.\n+    DenseDataIntermediateProto content = 2;\n+\n+    // The index of the buffer allocation for this constant.\n+    int64 allocation_index = 3;\n+  }\n+\n+  // The constants used by the executable.\n+  repeated ConstantInfoProto constants = 11;\n }"
        },
        {
            "sha": "881065fe91985cce3942fb2f381bbaa707dbb109",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable_test.cc",
            "status": "modified",
            "additions": 54,
            "deletions": 0,
            "changes": 54,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/26d08824195fea651221af83a9071fdf984f32dd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/26d08824195fea651221af83a9071fdf984f32dd/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable_test.cc?ref=26d08824195fea651221af83a9071fdf984f32dd",
            "patch": "@@ -62,8 +62,11 @@ namespace xla::gpu {\n namespace {\n using ::testing::ElementsAre;\n using ::testing::ElementsAreArray;\n+using ::testing::Pair;\n+using ::testing::Pointee;\n using ::testing::Property;\n using ::testing::SizeIs;\n+using ::testing::UnorderedElementsAre;\n using ::tsl::proto_testing::EqualsProto;\n \n TEST(GpuExecutableTest, OuputInfoToAndFromProto) {\n@@ -433,5 +436,56 @@ TEST(GpuExecutableTest, DumpsMetadataListProto) {\n               )pb\"));\n }\n \n+TEST(GpuExecutableTest, ProtoConversion) {\n+  se::DeviceDescription device_description;\n+  device_description.set_gpu_compute_capability(\n+      se::GpuComputeCapability{se::CudaComputeCapability::Volta()});\n+  device_description.set_driver_version({12, 3, 0});\n+  device_description.set_runtime_version({12, 3, 0});\n+\n+  Thunk::ThunkInfo thunk_info;\n+  thunk_info.thunk_id = 123;\n+\n+  ThunkSequence thunk_sequence;\n+  thunk_sequence.push_back(std::make_unique<KernelThunk>(\n+      thunk_info,\n+      /*kernel_name=*/\"test_kernel\", emitters::KernelArguments({}),\n+      LaunchDimensions(),\n+      /*cluster_dim=*/std::nullopt,\n+      /*shmem_bytes=*/0, se::gpu::TmaMetadata()));\n+\n+  GpuExecutable::Params params;\n+  params.asm_text = \"test_asm_text\";\n+  params.binary = {1, 2, 3};\n+  params.dnn_compiled_graphs = {{\"test_dnn_compiled_graph\", \"test_json\"}};\n+\n+  thunk_info.thunk_id = 456;\n+  params.executable =\n+      std::make_unique<SequentialThunk>(thunk_info, std::move(thunk_sequence));\n+  params.device_description = device_description;\n+\n+  params.module_name = \"test_module\";\n+  params.enable_debug_info_manager = false;\n+  params.mlir_allocations = {BufferAllocation(0, 1024, 0)};\n+  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<GpuExecutable> reference_executable,\n+                          GpuExecutable::Create(std::move(params)));\n+  TF_ASSERT_OK_AND_ASSIGN(GpuExecutableProto proto,\n+                          reference_executable->ToProto());\n+\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      std::unique_ptr<GpuExecutable> reconstructed_executable,\n+      GpuExecutable::FromProto(proto, device_description));\n+  EXPECT_THAT(reconstructed_executable->text(), \"test_asm_text\");\n+  EXPECT_THAT(reconstructed_executable->binary(), ElementsAre(1, 2, 3));\n+  EXPECT_THAT(\n+      reconstructed_executable->dnn_compiled_graphs(),\n+      UnorderedElementsAre(Pair(\"test_dnn_compiled_graph\", \"test_json\")));\n+  EXPECT_THAT(reconstructed_executable->GetThunk().thunks(),\n+              ElementsAre(Pointee(Property(&Thunk::kind, Thunk::kKernel))));\n+  EXPECT_THAT(reconstructed_executable->GetAllocations(),\n+              ElementsAre(Pointee(Property(&BufferAllocation::size, 1024))));\n+  EXPECT_THAT(reconstructed_executable->name(), \"test_module\");\n+}\n+\n }  // namespace\n }  // namespace xla::gpu"
        }
    ],
    "stats": {
        "total": 289,
        "additions": 277,
        "deletions": 12
    }
}