{
    "author": "mdfaijul",
    "message": "PR #31116: [XLA:GPU][oneAPI] Enable SPIR-V backend codegen for Intel GPU.\n\nImported from GitHub PR https://github.com/openxla/xla/pull/31116\n\nThis PR enables LLVM SPIR-V backend code generation for Intel GPU. More PRs would follow for handling bfloat16 since SPIRV does not support bfloat16 natively.\nCopybara import of the project:\n\n--\nc2dea29d9a03d3130e5e13b455c4e3619818fa4d by mdfaijul <md.faijul.amin@intel.com>:\n\nEnable SPIR-V backend codegen for IntelGPU.\n\n--\n26b2cc85d94e33b6dd9a7394952d3bebd0fcf596 by mdfaijul <md.faijul.amin@intel.com>:\n\nFix a typo.\n\n--\n99c2859ade6dfb5a19520b4c3941db783cf0991a by mdfaijul <md.faijul.amin@intel.com>:\n\nFix linking issue on GPUToLLVMSPV transforms.\n\n--\nd39cbb54202da4e88040e1fd8be325e7d8bf4a17 by mdfaijul <md.faijul.amin@intel.com>:\n\nAdd back dependencies generated by tool\n\n--\n8ead0e05d871c85f72ecd6886585f069ffb206a6 by Akhil Goel <akhil.goel@intel.com>:\n\nAddress review comments\n\nMerging this change closes #31116\n\nPiperOrigin-RevId: 814139549",
    "sha": "c9c2fcce4d24f4522641e8ec0d5fc98396316e81",
    "files": [
        {
            "sha": "e87c4bf578ebee7ac7417025846ca421032f713b",
            "filename": "third_party/xla/third_party/llvm/setup.bzl",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c9c2fcce4d24f4522641e8ec0d5fc98396316e81/third_party%2Fxla%2Fthird_party%2Fllvm%2Fsetup.bzl",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c9c2fcce4d24f4522641e8ec0d5fc98396316e81/third_party%2Fxla%2Fthird_party%2Fllvm%2Fsetup.bzl",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fthird_party%2Fllvm%2Fsetup.bzl?ref=c9c2fcce4d24f4522641e8ec0d5fc98396316e81",
            "patch": "@@ -12,6 +12,7 @@ _LLVM_TARGETS = [\n     \"RISCV\",\n     \"SystemZ\",\n     \"X86\",\n+    \"SPIRV\",\n ]\n \n def llvm_setup(name):"
        },
        {
            "sha": "9bd265969ea0eb479994f3a63dc3da176367b579",
            "filename": "third_party/xla/xla/codegen/device_spec.h",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c9c2fcce4d24f4522641e8ec0d5fc98396316e81/third_party%2Fxla%2Fxla%2Fcodegen%2Fdevice_spec.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c9c2fcce4d24f4522641e8ec0d5fc98396316e81/third_party%2Fxla%2Fxla%2Fcodegen%2Fdevice_spec.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Fdevice_spec.h?ref=c9c2fcce4d24f4522641e8ec0d5fc98396316e81",
            "patch": "@@ -59,6 +59,11 @@ class DeviceSpec {\n            std::holds_alternative<stream_executor::CudaComputeCapability>(\n                gpu().gpu_compute_capability());\n   }\n+  bool IsIntelGpu() const {\n+    // TODO(intel-gpu): Align with CUDA and ROCM approach of detecting Intel\n+    // GPU.\n+    return absl::StrContains(gpu().name(), \"Intel\");\n+  }\n \n  private:\n   DeviceSpecType type_;"
        },
        {
            "sha": "6268da4d6c8b0ac077ee15c2f23475903f615a50",
            "filename": "third_party/xla/xla/codegen/emitters/transforms/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c9c2fcce4d24f4522641e8ec0d5fc98396316e81/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c9c2fcce4d24f4522641e8ec0d5fc98396316e81/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2FBUILD?ref=c9c2fcce4d24f4522641e8ec0d5fc98396316e81",
            "patch": "@@ -109,6 +109,7 @@ cc_library(\n         \"@llvm-project//mlir:FuncDialect\",\n         \"@llvm-project//mlir:FuncToLLVM\",\n         \"@llvm-project//mlir:GPUDialect\",\n+        \"@llvm-project//mlir:GPUToLLVMSPVTransforms\",\n         \"@llvm-project//mlir:GPUToNVVMTransforms\",\n         \"@llvm-project//mlir:GPUToROCDLTransforms\",\n         \"@llvm-project//mlir:IR\","
        },
        {
            "sha": "dd8c09757278800429ed8f1a258decc7ec64ec53",
            "filename": "third_party/xla/xla/codegen/emitters/transforms/lower_to_llvm.cc",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c9c2fcce4d24f4522641e8ec0d5fc98396316e81/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Flower_to_llvm.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c9c2fcce4d24f4522641e8ec0d5fc98396316e81/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Flower_to_llvm.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fcodegen%2Femitters%2Ftransforms%2Flower_to_llvm.cc?ref=c9c2fcce4d24f4522641e8ec0d5fc98396316e81",
            "patch": "@@ -23,6 +23,7 @@ limitations under the License.\n #include \"mlir/Conversion/ComplexToLLVM/ComplexToLLVM.h\"\n #include \"mlir/Conversion/ControlFlowToLLVM/ControlFlowToLLVM.h\"\n #include \"mlir/Conversion/FuncToLLVM/ConvertFuncToLLVM.h\"\n+#include \"mlir/Conversion/GPUToLLVMSPV/GPUToLLVMSPVPass.h\"\n #include \"mlir/Conversion/GPUToNVVM/GPUToNVVMPass.h\"\n #include \"mlir/Conversion/GPUToROCDL/GPUToROCDLPass.h\"\n #include \"mlir/Conversion/LLVMCommon/ConversionTarget.h\"\n@@ -108,6 +109,20 @@ class LowerToLLVMPass : public impl::LowerToLLVMPassBase<LowerToLLVMPass> {\n             type_converter, patterns, mlir::gpu::amd::Runtime::Unknown,\n             *maybeChipset);\n         mlir::configureGpuToROCDLConversionLegality(target);\n+      } else if (device_spec_.IsIntelGpu()) {\n+        // Add sub-group-size attribute to functions.\n+        int32_t sub_group_size = device_spec_.gpu().threads_per_warp();\n+        if (auto module_op = mlir::dyn_cast<mlir::ModuleOp>(getOperation())) {\n+          module_op.walk([sub_group_size](mlir::func::FuncOp func) {\n+            if (!func.getBody().empty()) {\n+              mlir::OpBuilder b(func.getContext());\n+              auto sub_group_attr = b.getI32IntegerAttr(sub_group_size);\n+              func->setAttr(\"intel_reqd_sub_group_size\", sub_group_attr);\n+            }\n+          });\n+        }\n+        populateGpuToLLVMSPVConversionPatterns(type_converter, patterns);\n+        populateGpuMemorySpaceAttributeConversions(type_converter);\n       } else {\n         mlir::populateGpuToNVVMConversionPatterns(type_converter, patterns);\n         mlir::configureGpuToNVVMConversionLegality(target);"
        },
        {
            "sha": "9d143998de256bc22baa1f8bdb9d0aaa5389e568",
            "filename": "third_party/xla/xla/service/BUILD",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c9c2fcce4d24f4522641e8ec0d5fc98396316e81/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c9c2fcce4d24f4522641e8ec0d5fc98396316e81/third_party%2Fxla%2Fxla%2Fservice%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2FBUILD?ref=c9c2fcce4d24f4522641e8ec0d5fc98396316e81",
            "patch": "@@ -7,6 +7,10 @@ load(\n     \"@local_config_rocm//rocm:build_defs.bzl\",\n     \"if_rocm_is_configured\",\n )\n+load(\n+    \"@local_config_sycl//sycl:build_defs.bzl\",\n+    \"if_sycl_is_configured\",\n+)\n load(\n     \"//xla:xla.default.bzl\",\n     \"xla_cc_binary\",\n@@ -1408,6 +1412,8 @@ cc_library(\n     ]) + if_rocm_is_configured([\n         \"//xla/service/gpu:amdgpu_compiler\",\n         \"//xla/stream_executor/rocm:stream_executor_rocm\",\n+    ]) + if_sycl_is_configured([\n+        \"//xla/service/gpu:intel_gpu_compiler\",\n     ]),\n )\n "
        },
        {
            "sha": "6b188176f0bef2cabf179013600ae2a7546cf57c",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c9c2fcce4d24f4522641e8ec0d5fc98396316e81/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c9c2fcce4d24f4522641e8ec0d5fc98396316e81/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=c9c2fcce4d24f4522641e8ec0d5fc98396316e81",
            "patch": "@@ -3193,6 +3193,7 @@ cc_library(\n         \":target_constants\",\n         \"//xla/hlo/analysis:hlo_dataflow_analysis\",\n         \"//xla/hlo/ir:hlo\",\n+        \"//xla/service/gpu/llvm_gpu_backend:spirv_backend\",\n         \"//xla/stream_executor:semantic_version\",\n         \"//xla/stream_executor:stream_executor_h\",\n         \"//xla/stream_executor/sycl:sycl_platform_id\","
        },
        {
            "sha": "ed4d7db90602359b259d0ee28cdcdad51d7c8003",
            "filename": "third_party/xla/xla/service/gpu/gpu_executable.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c9c2fcce4d24f4522641e8ec0d5fc98396316e81/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c9c2fcce4d24f4522641e8ec0d5fc98396316e81/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fgpu_executable.cc?ref=c9c2fcce4d24f4522641e8ec0d5fc98396316e81",
            "patch": "@@ -281,6 +281,10 @@ absl::Status ExecuteThunksImpl(\n   se::StreamExecutor* executor = main_stream->parent();\n   stream_executor::StreamPriority stream_priority =\n       stream_executor::StreamPriority::Default;\n+  // TODO(intel-tf): Enable stream priorities for sycl backend.\n+  if (executor->GetPlatform()->id() == stream_executor::sycl::kSyclPlatformId) {\n+    use_highest_priority_for_async_stream = false;\n+  }\n   if (use_highest_priority_for_async_stream) {\n     stream_priority = stream_executor::StreamPriority::Highest;\n   }"
        },
        {
            "sha": "b306fc21951c0da4abe4077e35953a76aac733ef",
            "filename": "third_party/xla/xla/service/gpu/intel_gpu_compiler.cc",
            "status": "modified",
            "additions": 21,
            "deletions": 3,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c9c2fcce4d24f4522641e8ec0d5fc98396316e81/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fintel_gpu_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c9c2fcce4d24f4522641e8ec0d5fc98396316e81/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fintel_gpu_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fintel_gpu_compiler.cc?ref=c9c2fcce4d24f4522641e8ec0d5fc98396316e81",
            "patch": "@@ -15,6 +15,9 @@ limitations under the License.\n \n #include \"xla/service/gpu/intel_gpu_compiler.h\"\n \n+#include \"xla/debug_options_flags.h\"\n+#include \"xla/service/dump.h\"\n+#include \"xla/service/gpu/llvm_gpu_backend/spirv_backend.h\"\n #include \"xla/service/gpu/target_constants.h\"\n #include \"xla/stream_executor/sycl/sycl_platform_id.h\"\n \n@@ -39,13 +42,28 @@ IntelGpuCompiler::CompileTargetBinary(\n     const stream_executor::DeviceDescription& device_description,\n     bool relocatable, const HloModule* debug_module,\n     const CompileOptions& options, std::optional<int> shard_number) {\n-  // Note: this is a stub.\n-  return BackendCompileResult{};\n+  TF_ASSIGN_OR_RETURN(\n+      auto spirv_str,\n+      spirv::CompileToSPIRV(llvm_module,\n+                            device_description.gpu_compute_capability(),\n+                            module_config.debug_options()));\n+  if (DumpingEnabledForHloModule(debug_module ? debug_module->name() : \"\",\n+                                 module_config.debug_options())) {\n+    if (debug_module) {\n+      DumpToFileInDirOrStdout(*debug_module, \"\", \"spv\", spirv_str);\n+    } else {\n+      LOG(ERROR) << \"Dumping is not implemented since the file name cannot be \"\n+                    \"inferred. Please implement (potentially MLIR) module -> \"\n+                    \"filename heuristic.\";\n+    }\n+  }\n+  std::vector<uint8_t> spirv_bin(spirv_str.begin(), spirv_str.end());\n+  return BackendCompileResult{/*asm_text=*/\"\", std::move(spirv_bin)};\n }\n \n std::vector<std::string> IntelGpuCompiler::GetLLVMCommandLineOptions(\n     const DebugOptions& debug_options) const {\n-  return {};\n+  return spirv::GetSPIRVBackendOptions(debug_options);\n }\n \n }  // namespace gpu"
        },
        {
            "sha": "50f8f5c36989fdc8dc027a5ccdc2d078b65c64c6",
            "filename": "third_party/xla/xla/service/gpu/llvm_gpu_backend/BUILD",
            "status": "modified",
            "additions": 20,
            "deletions": 0,
            "changes": 20,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c9c2fcce4d24f4522641e8ec0d5fc98396316e81/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c9c2fcce4d24f4522641e8ec0d5fc98396316e81/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2FBUILD?ref=c9c2fcce4d24f4522641e8ec0d5fc98396316e81",
            "patch": "@@ -268,3 +268,23 @@ xla_cc_test(\n         \"@local_tsl//tsl/platform:test\",\n     ],\n )\n+\n+cc_library(\n+    name = \"spirv_backend\",\n+    srcs = [\n+        \"spirv_backend.cc\",\n+    ],\n+    hdrs = [\n+        \"spirv_backend.h\",\n+    ],\n+    tags = [\n+        \"gpu\",\n+        \"oneapi-only\",\n+    ],\n+    deps = [\n+        \":llvm_gpu_backend\",\n+        \"//xla/service/llvm_ir:llvm_command_line_options\",\n+        \"@llvm-project//llvm:SPIRVCodeGen\",\n+        \"@local_tsl//tsl/platform:errors\",\n+    ],\n+)"
        },
        {
            "sha": "b444eb1f70af6763ebc5e23b77f6b1eaed9a3471",
            "filename": "third_party/xla/xla/service/gpu/llvm_gpu_backend/spirv_backend.cc",
            "status": "added",
            "additions": 166,
            "deletions": 0,
            "changes": 166,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c9c2fcce4d24f4522641e8ec0d5fc98396316e81/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Fspirv_backend.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c9c2fcce4d24f4522641e8ec0d5fc98396316e81/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Fspirv_backend.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Fspirv_backend.cc?ref=c9c2fcce4d24f4522641e8ec0d5fc98396316e81",
            "patch": "@@ -0,0 +1,166 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/service/gpu/llvm_gpu_backend/spirv_backend.h\"\n+\n+#include <memory>\n+#include <string>\n+\n+#include \"llvm/IR/Constants.h\"\n+#include \"llvm/IR/IRBuilder.h\"\n+#include \"llvm/IR/LegacyPassManager.h\"\n+#include \"llvm/Support/TargetSelect.h\"\n+#include \"llvm/Transforms/Scalar.h\"\n+#include \"llvm/Transforms/Utils/Cloning.h\"\n+#include \"llvm/lib/Target/SPIRV/SPIRVAPI.h\"\n+#include \"xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.h\"\n+#include \"xla/service/llvm_ir/llvm_command_line_options.h\"\n+#include \"tsl/platform/errors.h\"\n+\n+namespace xla::gpu::spirv {\n+\n+namespace {\n+\n+// Default inline threshold value to use in llvm.\n+const int kDefaultInlineThreshold = 1100;\n+\n+void SPIRVBackendInit() {\n+  LLVMInitializeSPIRVTargetInfo();\n+  LLVMInitializeSPIRVTarget();\n+  LLVMInitializeSPIRVTargetMC();\n+  LLVMInitializeSPIRVAsmPrinter();\n+\n+  // Initialize the LLVM optimization passes.\n+  llvm::PassRegistry* registry = llvm::PassRegistry::getPassRegistry();\n+  InitializePasses(registry);\n+}\n+\n+absl::Status SPIRVTargetModuleLinker(\n+    llvm::Module* module, stream_executor::GpuComputeCapability gpu_version,\n+    const DebugOptions& debug_options, const std::string& device_bitcode_path) {\n+  return absl::OkStatus();\n+}\n+\n+}  // namespace\n+\n+std::vector<std::string> GetSPIRVBackendOptions(\n+    const DebugOptions& debug_options) {\n+  // Feed all customized flags here, so we can override them with llvm_cl_opts\n+  // without redeploy the compiler for development purpose.\n+  std::vector<std::string> backend_llvm_opts;\n+\n+  auto backend_extra_llvm_opts = llvm_ir::ExtractXlaBackendExtraOptions(\n+      debug_options.xla_backend_extra_options());\n+  backend_llvm_opts.insert(backend_llvm_opts.end(),\n+                           backend_extra_llvm_opts.cbegin(),\n+                           backend_extra_llvm_opts.cend());\n+\n+  return backend_llvm_opts;\n+}\n+\n+absl::StatusOr<std::string> CompileToSPIRV(\n+    llvm::Module* module, stream_executor::GpuComputeCapability gpu_version,\n+    const DebugOptions& debug_options) {\n+  static absl::once_flag backend_init_flag;\n+  absl::call_once(backend_init_flag, SPIRVBackendInit);\n+  auto llvm_opts = GetSPIRVBackendOptions(debug_options);\n+  llvm_ir::LLVMCommandLineOptionsLock llvm_lock(llvm_opts);\n+\n+  // SPIRV Kernel functions expect their arguments' address spaces to be global,\n+  // i.e., addrspace(1). Here we only change kernel argument's address space if\n+  // it is addrspace(0). Then an address space cast to original is applied so\n+  // that users still have old address space.\n+  std::vector<llvm::Type*> new_arg_types;\n+  llvm::LLVMContext& context = module->getContext();\n+  llvm::SmallVector<llvm::Function*> kernel_funcs;\n+  for (auto& func : *module) {\n+    if (func.getCallingConv() == llvm::CallingConv::SPIR_KERNEL) {\n+      kernel_funcs.push_back(&func);\n+    }\n+  }\n+  for (auto old_func : kernel_funcs) {\n+    if (old_func->getCallingConv() == llvm::CallingConv::SPIR_KERNEL) {\n+      for (auto& old_arg : old_func->args()) {\n+        llvm::Type* old_arg_type = old_arg.getType();\n+        auto ptr_type = llvm::dyn_cast<llvm::PointerType>(old_arg_type);\n+        if (ptr_type->getAddressSpace() == 0) {\n+          auto new_arg_type = llvm::PointerType::get(context, 1);\n+          new_arg_types.push_back(new_arg_type);\n+        } else {\n+          new_arg_types.push_back(old_arg_type);\n+        }\n+      }\n+      auto new_func_type = llvm::FunctionType::get(\n+          old_func->getReturnType(), new_arg_types, old_func->isVarArg());\n+\n+      auto new_func = llvm::Function::Create(\n+          new_func_type, old_func->getLinkage(), old_func->getName(), module);\n+\n+      // We do not want to modify type of arguments of old func at the uses,\n+      // hence using identity map.\n+      llvm::ValueToValueMapTy identity_map;\n+      for (auto& old_arg : old_func->args()) {\n+        identity_map[&old_arg] = &old_arg;\n+      }\n+      llvm::SmallVector<llvm::ReturnInst*> returns;\n+      llvm::CloneFunctionInto(new_func, old_func, identity_map,\n+                              llvm::CloneFunctionChangeType::LocalChangesOnly,\n+                              returns);\n+\n+      llvm::IRBuilder<> builder(&(*new_func->begin()->begin()));\n+      auto new_arg_it = new_func->arg_begin();\n+      auto old_arg_it = old_func->arg_begin();\n+      for (; old_arg_it != old_func->arg_end(); ++old_arg_it, ++new_arg_it) {\n+        if (auto old_ptr_type =\n+                llvm::dyn_cast<llvm::PointerType>(old_arg_it->getType())) {\n+          auto cast = builder.CreateAddrSpaceCast(new_arg_it, old_ptr_type);\n+          old_arg_it->replaceAllUsesWith(cast);\n+        }\n+      }\n+      // TODO: Update kernel function's uses. Currently, we are assuming that\n+      // kernel function is not called by any other functions in the current\n+      // LLVM module.\n+      new_func->takeName(old_func);\n+      old_func->eraseFromParent();\n+    }\n+  }\n+\n+  llvm::Triple default_target_triple(\"spirv64-unknown-unknown\");\n+  std::unique_ptr<llvm::TargetMachine> target_machine =\n+      GetTargetMachine(default_target_triple, \"\", debug_options, \"\");\n+  TF_RETURN_IF_ERROR(LinkAndOptimizeModule(\n+      module, gpu_version, debug_options, \"\", SPIRVTargetModuleLinker,\n+      default_target_triple, target_machine.get(), kDefaultInlineThreshold));\n+\n+  // Unlike other GPU backends like NVTPTX and AMDGPU, SPIRV does not have\n+  // address inference pass in the TargetPassConfig. So we do it here\n+  // explicitly.\n+  llvm::legacy::PassManager pm;\n+  pm.add(llvm::createInferAddressSpacesPass(0));\n+  pm.run(*module);\n+\n+  std::string spirv_str;\n+  std::string spirv_err_msg;\n+  std::vector<std::string> spirv_extensions{\"all\"};\n+  std::vector<std::string> spirv_options{default_target_triple.str()};\n+  bool spirv_success = llvm::SPIRVTranslateModule(\n+      module, spirv_str, spirv_err_msg, spirv_extensions, spirv_options);\n+  if (!spirv_success) {\n+    return absl::AbortedError(\"Failed to translate LLVM module to SPIRV.\");\n+  }\n+  return spirv_str;\n+}\n+\n+}  // namespace xla::gpu::spirv"
        },
        {
            "sha": "64baa0e544e00a6362ea90055138160e4bb97712",
            "filename": "third_party/xla/xla/service/gpu/llvm_gpu_backend/spirv_backend.h",
            "status": "added",
            "additions": 42,
            "deletions": 0,
            "changes": 42,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c9c2fcce4d24f4522641e8ec0d5fc98396316e81/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Fspirv_backend.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c9c2fcce4d24f4522641e8ec0d5fc98396316e81/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Fspirv_backend.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fllvm_gpu_backend%2Fspirv_backend.h?ref=c9c2fcce4d24f4522641e8ec0d5fc98396316e81",
            "patch": "@@ -0,0 +1,42 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// LLVM-based compiler backend.\n+#ifndef XLA_SERVICE_GPU_LLVM_GPU_BACKEND_SPIRV_BACKEND_H_\n+#define XLA_SERVICE_GPU_LLVM_GPU_BACKEND_SPIRV_BACKEND_H_\n+\n+#include <functional>\n+#include <string>\n+#include <vector>\n+\n+#include \"absl/status/statusor.h\"\n+#include \"llvm/IR/Module.h\"\n+#include \"llvm/Target/TargetMachine.h\"\n+#include \"xla/stream_executor/device_description.h\"\n+#include \"xla/xla.pb.h\"\n+\n+namespace xla::gpu::spirv {\n+\n+absl::StatusOr<std::string> CompileToSPIRV(\n+    llvm::Module* module, stream_executor::GpuComputeCapability gpu_version,\n+    const DebugOptions& debug_options);\n+\n+// Returns the LLVM command line flags that we use for compilation.\n+std::vector<std::string> GetSPIRVBackendOptions(\n+    const DebugOptions& debug_options);\n+\n+}  // namespace xla::gpu::spirv\n+\n+#endif  // XLA_SERVICE_GPU_LLVM_GPU_BACKEND_SPIRV_BACKEND_H_"
        },
        {
            "sha": "7e4080de45527ccf3ba2668ecc782ba4ddace800",
            "filename": "third_party/xla/xla/service/gpu/target_constants.h",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c9c2fcce4d24f4522641e8ec0d5fc98396316e81/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftarget_constants.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c9c2fcce4d24f4522641e8ec0d5fc98396316e81/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftarget_constants.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftarget_constants.h?ref=c9c2fcce4d24f4522641e8ec0d5fc98396316e81",
            "patch": "@@ -55,19 +55,16 @@ inline const char* DataLayout() {\n }  // namespace amdgpu\n \n namespace spir {\n-// The triple that represents our target on SPIR backend.\n+// The triple that represents our target on SPIR-V backend.\n inline const char* TargetTriple() {\n-  static constexpr char kTargetTriple[] = \"spir64-unknown-unknown\";\n+  static constexpr char kTargetTriple[] = \"spirv64-unknown-unknown\";\n   return kTargetTriple;\n }\n \n // The data layout of the emitted module.\n inline const char* DataLayout() {\n-  static constexpr char kDataLayout[] =\n-      \"e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:\"\n-      \"32:32-f64:64:64-v16:16:16-v24:32:32-v32:32:32-v48:64:64-v64:64:64-v96:\"\n-      \"128:128-v128:128:128-v192:256:256-v256:256:256-v512:512:512-v1024:1024:\"\n-      \"1024\";\n+  // Let SPIR-V backend choose a default layout.\n+  static constexpr char kDataLayout[] = \"\";\n   return kDataLayout;\n }\n }  // namespace spir"
        },
        {
            "sha": "5018b455d420fae0f26000d83c5626f8e112c5bf",
            "filename": "third_party/xla/xla/service/gpu/target_util.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c9c2fcce4d24f4522641e8ec0d5fc98396316e81/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftarget_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c9c2fcce4d24f4522641e8ec0d5fc98396316e81/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftarget_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftarget_util.cc?ref=c9c2fcce4d24f4522641e8ec0d5fc98396316e81",
            "patch": "@@ -399,7 +399,7 @@ std::string ObtainDeviceFunctionName(TargetDeviceFunctionID func_id,\n     } else {\n       LOG(FATAL) << \"Unexpected type while getting device function name.\";\n     }\n-  } else if (target_triple.isSPIR()) {\n+  } else if (target_triple.isSPIROrSPIRV()) {\n     // TODO(b/370452608): Are there approximate functions we can use for BF16\n     // and F16 types?\n     if (output_type == BF16 || output_type == F16 || output_type == F32) {\n@@ -454,7 +454,7 @@ llvm::CallInst* EmitDeviceFunctionCall(\n           .getCallee());\n \n   callee->addFnAttrs(attributes);\n-  if (target_triple.isSPIR())\n+  if (target_triple.isSPIROrSPIRV())\n     callee->setCallingConv(llvm::CallingConv::SPIR_FUNC);\n \n   return b->CreateCall(callee, llvm_ir::AsArrayRef(operands), name.data());\n@@ -473,7 +473,7 @@ llvm::CallInst* EmitCallToTargetIntrinsic(\n     llvm_intrinsic_or_function = gpu_intrinsic_id.nvptx_intrinsic_or_function;\n   } else if (target_triple.getArch() == llvm::Triple::amdgcn) {\n     llvm_intrinsic_or_function = gpu_intrinsic_id.amdgpu_intrinsic_or_function;\n-  } else if (target_triple.isSPIR()) {\n+  } else if (target_triple.isSPIROrSPIRV()) {\n     llvm_intrinsic_or_function = gpu_intrinsic_id.spir_intrinsic_or_function;\n   } else {\n     LOG(FATAL) << \"Invalid triple \" << target_triple.str();\n@@ -503,7 +503,7 @@ void AnnotateFunctionAsGpuKernel(llvm::Module* module, llvm::Function* func,\n     // Attach information so AMDGPU can recognize function as a AMDGPU kernel.\n     func->setCallingConv(llvm::CallingConv::AMDGPU_KERNEL);\n     func->addFnAttr(\"uniform-work-group-size\", \"true\");\n-  } else if (target_triple.isSPIR()) {\n+  } else if (target_triple.isSPIROrSPIRV()) {\n     // Attach information so that it can be recognized as a SPIR kernel.\n     func->setCallingConv(llvm::CallingConv::SPIR_KERNEL);\n   } else {"
        },
        {
            "sha": "8df0dab0626d85c0dc34f8c65888c416d49faaff",
            "filename": "third_party/xla/xla/service/gpu/tests/gpu_codegen_test.h",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c9c2fcce4d24f4522641e8ec0d5fc98396316e81/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fgpu_codegen_test.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c9c2fcce4d24f4522641e8ec0d5fc98396316e81/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fgpu_codegen_test.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fgpu_codegen_test.h?ref=c9c2fcce4d24f4522641e8ec0d5fc98396316e81",
            "patch": "@@ -31,8 +31,9 @@ namespace gpu {\n class GpuCodegenTest : public LlvmIrGenTestBase {\n  public:\n   GpuCodegenTest()\n-      : is_built_with_rocm_(\n-            se::PlatformManager::PlatformWithName(\"ROCM\").ok()) {}\n+      : is_built_with_rocm_(se::PlatformManager::PlatformWithName(\"ROCM\").ok()),\n+        is_built_with_sycl_(\n+            se::PlatformManager::PlatformWithName(\"SYCL\").ok()) {}\n \n  protected:\n   // Converts LLVM match to be platform-specific.\n@@ -52,6 +53,7 @@ class GpuCodegenTest : public LlvmIrGenTestBase {\n       bool run_optimization_passes = true);\n \n   bool is_built_with_rocm_;\n+  bool is_built_with_sycl_;\n };\n \n }  // namespace gpu"
        },
        {
            "sha": "9cfe8a99eefdd91d8f5f660867da18f96cddcc4a",
            "filename": "third_party/xla/xla/service/gpu/tests/gpu_noalias_test.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 3,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c9c2fcce4d24f4522641e8ec0d5fc98396316e81/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fgpu_noalias_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c9c2fcce4d24f4522641e8ec0d5fc98396316e81/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fgpu_noalias_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Ftests%2Fgpu_noalias_test.cc?ref=c9c2fcce4d24f4522641e8ec0d5fc98396316e81",
            "patch": "@@ -51,12 +51,18 @@ TEST_F(GpuNoAliasTest, Concat) {\n   // - After optimizations we have \"concatenate(x, y, x)\".\n   // - We only pass the same parameters once, so the kernel will have these\n   // parameters: (x, y, output), and all of them will be noalias.\n-  auto expected_ir = is_built_with_rocm_ ? R\"(\n+  const char* expected_ir;\n+  if (is_built_with_rocm_) {\n+    expected_ir = R\"(\n CHECK: define amdgpu_kernel void @{{[a-zA-Z0-9_]+}}(ptr noalias align 16 dereferenceable(16) %{{[a-zA-Z0-9_]+}}, ptr noalias align 16 dereferenceable(16) %{{[a-zA-Z0-9_]+}}, ptr noalias align 256 dereferenceable(48) %{{[a-zA-Z0-9_]+}}) #0\n-  )\"\n-                                         : R\"(\n+  )\";\n+  } else if (is_built_with_sycl_) {\n+    expected_ir = R\"(CHECK: define spir_kernel {{.*}})\";\n+  } else {\n+    expected_ir = R\"(\n CHECK: define ptx_kernel void @{{[a-zA-Z0-9_]+}}(ptr noalias align 16 dereferenceable(16) %{{[a-zA-Z0-9_]+}}, ptr noalias align 16 dereferenceable(16) %{{[a-zA-Z0-9_]+}}, ptr noalias align 256 dereferenceable(48) %{{[a-zA-Z0-9_]+}})\n   )\";\n+  }\n   CompileAndVerifyIr(std::move(hlo_module), expected_ir,\n                      /*match_optimized_ir=*/false);\n }"
        },
        {
            "sha": "619eee214b519f9a640d86f3e078cd50ebe1c037",
            "filename": "third_party/xla/xla/tools/BUILD",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c9c2fcce4d24f4522641e8ec0d5fc98396316e81/third_party%2Fxla%2Fxla%2Ftools%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c9c2fcce4d24f4522641e8ec0d5fc98396316e81/third_party%2Fxla%2Fxla%2Ftools%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftools%2FBUILD?ref=c9c2fcce4d24f4522641e8ec0d5fc98396316e81",
            "patch": "@@ -3,6 +3,10 @@\n load(\"@bazel_skylib//rules:build_test.bzl\", \"build_test\")\n load(\"@local_config_cuda//cuda:build_defs.bzl\", \"if_cuda\")\n load(\"@local_config_rocm//rocm:build_defs.bzl\", \"if_rocm\", \"if_rocm_is_configured\")\n+load(\n+    \"@local_config_sycl//sycl:build_defs.bzl\",\n+    \"if_sycl_is_configured\",\n+)\n load(\"//xla:lit.bzl\", \"lit_test_suite\")\n load(\n     \"//xla:xla.default.bzl\",\n@@ -554,12 +558,14 @@ xla_cc_binary(\n         \"@local_tsl//tsl/platform:platform_port\",\n         \"@local_tsl//tsl/platform:status\",\n         \"@local_tsl//tsl/platform:test\",\n-    ] + if_cuda_or_rocm([\n+    ] + if_gpu_is_configured([\n         \"//xla/service:gpu_plugin\",\n     ]) + if_cuda([\n         \"//xla/stream_executor:cuda_platform\",\n     ]) + if_rocm([\n         \"//xla/stream_executor:rocm_platform\",\n+    ]) + if_sycl_is_configured([\n+        \"//xla/stream_executor:sycl_platform\",\n     ]),\n )\n "
        }
    ],
    "stats": {
        "total": 330,
        "additions": 310,
        "deletions": 20
    }
}