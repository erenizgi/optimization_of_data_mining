{
    "author": "hyeontaek",
    "message": "[IFRT] Migrate `Array::pjrt_layout()` callers to interpret `nullptr` as a default layout\n\n`Array::pjrt_layout()` will be changed to return `nullptr` to indicate a default layout, where the callers can obtain the corresponding concrete default layout by using `Client::GetDefaultPjRtLayout()`.\n\nThis change adds `nullptr` handling preemptively before the new `Array::pjrt_layout()` semantics becomes effective so that the existing code works as before.\n\nTests using `Array::pjrt_layout()` method calls are minimally updated to add a non-nullness check. They will be updated as `Array::pjrt_layout()` actually returns `nullptr`.\n\nPiperOrigin-RevId: 817893146",
    "sha": "6c026def7a4b5fc3fd8469749793430b24eba193",
    "files": [
        {
            "sha": "c51adef049b7516af122e3542e17b9384fa8d556",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/pjrt_array.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6c026def7a4b5fc3fd8469749793430b24eba193/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_array.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6c026def7a4b5fc3fd8469749793430b24eba193/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_array.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_array.cc?ref=6c026def7a4b5fc3fd8469749793430b24eba193",
            "patch": "@@ -580,6 +580,19 @@ std::string PjRtArray::DebugString() const {\n   DCHECK(this);\n   absl::StatusOr<std::shared_ptr<const xla::PjRtLayout>> layout_ptr =\n       pjrt_layout();\n+  if (layout_ptr.ok() && *layout_ptr == nullptr) {\n+    layout_ptr =\n+        [&]() -> absl::StatusOr<std::shared_ptr<const xla::PjRtLayout>> {\n+      TF_ASSIGN_OR_RETURN(xla::ifrt::Shape shard_shape,\n+                          sharding_->GetShardShape(std::get<Shape>(shape_)));\n+      TF_ASSIGN_OR_RETURN(\n+          std::shared_ptr<const xla::PjRtLayout> layout,\n+          client_->GetDefaultPjRtLayout(dtype_, shard_shape.dims(),\n+                                        sharding_->devices()->devices().front(),\n+                                        sharding_->memory_kind()));\n+      return layout;\n+    }();\n+  }\n   std::string layout_str =\n       layout_ptr.ok() ? (*layout_ptr)->ToString() : \"<unknown>\";\n "
        },
        {
            "sha": "f6ef50f4df60ba96e13cc392699b761535c94367",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/pjrt_client.cc",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6c026def7a4b5fc3fd8469749793430b24eba193/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6c026def7a4b5fc3fd8469749793430b24eba193/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Fpjrt_client.cc?ref=6c026def7a4b5fc3fd8469749793430b24eba193",
            "patch": "@@ -1387,6 +1387,16 @@ PjRtClient::CopyArraysForCrossHost(absl::Span<ArrayRef> arrays,\n                         arrays[i]->shared_ptr_sharding()->WithDeviceAssignment(\n                             dst_devices, memory_kind));\n     TF_ASSIGN_OR_RETURN(auto new_layout, arrays[i]->pjrt_layout());\n+    if (new_layout == nullptr) {\n+      TF_ASSIGN_OR_RETURN(\n+          xla::ifrt::Shape shard_shape,\n+          arrays[i]->sharding().GetShardShape(arrays[i]->shape()));\n+      TF_ASSIGN_OR_RETURN(\n+          new_layout, GetDefaultPjRtLayout(\n+                          arrays[i]->dtype(), shard_shape.dims(),\n+                          arrays[i]->sharding().devices()->devices().front(),\n+                          arrays[i]->sharding().memory_kind()));\n+    }\n     TF_ASSIGN_OR_RETURN(\n         new_arrays.emplace_back(),\n         PjRtArray::Create(this, arrays[i]->dtype(), arrays[i]->shape(),"
        },
        {
            "sha": "b7fb8305e9e96ef37a17b7185606e6e7d21483a9",
            "filename": "third_party/xla/xla/python/pjrt_ifrt/reshard_impl_test_lib.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/6c026def7a4b5fc3fd8469749793430b24eba193/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Freshard_impl_test_lib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/6c026def7a4b5fc3fd8469749793430b24eba193/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Freshard_impl_test_lib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpython%2Fpjrt_ifrt%2Freshard_impl_test_lib.cc?ref=6c026def7a4b5fc3fd8469749793430b24eba193",
            "patch": "@@ -402,6 +402,7 @@ TEST_F(ReshardTest, DifferentDestinationLayout) {\n   // Make sure that the destination layout is actually different from the source\n   // layout in order to ensure the test coverage.\n   TF_ASSERT_OK_AND_ASSIGN(const auto src_layout, src_array->pjrt_layout());\n+  ASSERT_NE(src_layout, nullptr);\n   ASSERT_NE(src_layout->xla_layout(), dst_array_spec.layout->xla_layout());\n \n   TF_ASSERT_OK_AND_ASSIGN(\n@@ -415,6 +416,7 @@ TEST_F(ReshardTest, DifferentDestinationLayout) {\n \n   // Verify that the destination array is created with the user-provided layout.\n   TF_ASSERT_OK_AND_ASSIGN(const auto dst_layout, dst_array->pjrt_layout());\n+  ASSERT_NE(dst_layout, nullptr);\n   EXPECT_EQ(dst_layout->xla_layout(), dst_array_spec.layout->xla_layout());\n \n   EXPECT_THAT(CopyArrayToLiteral(dst_array),"
        }
    ],
    "stats": {
        "total": 25,
        "additions": 25,
        "deletions": 0
    }
}