{
    "author": "protobird-git",
    "message": "Set FC's keep_num_dims to false when output dims is different from input dims after quantization.\n\nOn gemma3n with decode batch > 1, it happens when the embedding is coupled with PLE by einsum.\nThe export steps are:\n1) Initial: BMM([b,2048]x[2048,7680] -> [b,7680])\n2) FuseInputReshape_BatchMatMulWithFlattenedRhsDims: BMM([b,2048]x[2048,7680] -> [b,7680])\n3) ConvertBatchMatMulOp2FullyConnectedOp_Rank2ConstantRhs: FC([b,2048]x[2048,7680] -> [b,7680])\n4) StrictQuantizationPattern(by IsDrqTensor): FC([b,1,2048]x[2048,7680] -> [b,7680])\n\nWhen FC's keep_num_dims is false and it's followed by reshape op (like gemma3n), keep_num_dims will be set to true later with correct shapes by EnableFullyConnectedKeepNumDimsBeforeReshape.\n\nPiperOrigin-RevId: 847813526",
    "sha": "fec780d7febddc82045efa6923ed00040437de9e",
    "files": [
        {
            "sha": "c50e0a26e71c486d5f0d94c14469239eda0b78a6",
            "filename": "tensorflow/compiler/mlir/lite/transforms/quantize.cc",
            "status": "modified",
            "additions": 40,
            "deletions": 6,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/fec780d7febddc82045efa6923ed00040437de9e/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fquantize.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/fec780d7febddc82045efa6923ed00040437de9e/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fquantize.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Flite%2Ftransforms%2Fquantize.cc?ref=fec780d7febddc82045efa6923ed00040437de9e",
            "patch": "@@ -80,13 +80,13 @@ static LogicalResult IsDrqTensor(Value value, Value& fq_input) {\n   // fake quant op.\n   // This is to support the case such as:\n   // %2077 = \"vhlo.composite_v1\"(%73, %69, %2070) : (tensor<i32>, tensor<i32>,\n-  //  tensor<1x?x512xf32>) -> tensor<1x?x512xf32>\n+  //   tensor<1x?x512xf32>) -> tensor<1x?x512xf32>\n   // %2078 = \"tfl.reshape\"(%2077, %99) : (tensor<1x?x512xf32>, tensor<2xi32>) ->\n-  //  tensor<?x512xf32>\n+  //   tensor<?x512xf32>\n   // %2079 = \"tfl.pseudo_qconst\"() <{qtype = tensor<64x512x!quant.uniform<i8....\n-  // %2080 = \"tfl.dequantize\"(%2079) %2081 = \"tfl.fully_connected\"\n-  //  (%2078, %2080, %0) : (tensor<?x512xf32>, tensor<64x512xf32>, none) ->\n-  //  tensor<?x64xf32>\n+  // %2080 = \"tfl.dequantize\"(%2079)\n+  // %2081 = \"tfl.fully_connected\"(%2078, %2080, %0) : (tensor<?x512xf32>,\n+  //   tensor<64x512xf32>, none) -> tensor<?x64xf32>\n   // TODO - b/422588785: Have proper support for dynamic shaped models.\n   auto v = value;\n   if (auto reshape_op = llvm::dyn_cast_or_null<ReshapeOp>(v.getDefiningOp())) {\n@@ -228,6 +228,40 @@ class PushForwardDrqFQ : public OpRewritePattern<stablehlo::CompositeOp> {\n   }\n };\n \n+// Fixes keep_num_dims option of FC if output dims is different from input dims\n+// though keep_num_dims is true. It happens when FC's input has changed after\n+// quantization, e.g. by IsDrqTensor().\n+// Sets keep_num_dims to false if that's the case. Otherwise, it's not\n+// compatible with GPU. See CheckGpuDelegateCompatibility() in\n+// third_party/tensorflow/lite/tools/versioning/gpu_compatibility.cc.\n+// Note that if FC is followed by Reshape, the keep_num_dims will be set to true\n+// with a correct shape later by EnableFullyConnectedKeepNumDimsBeforeReshape()\n+// in optimize pass.\n+struct FixFullyConnectedKeepNumDims\n+    : public OpRewritePattern<FullyConnectedOp> {\n+  explicit FixFullyConnectedKeepNumDims(MLIRContext* context)\n+      : OpRewritePattern<TFL::FullyConnectedOp>(context, /*benefit=*/0) {}\n+\n+  LogicalResult matchAndRewrite(FullyConnectedOp fc,\n+                                PatternRewriter& rewriter) const override {\n+    if (!fc.getKeepNumDims()) return failure();\n+\n+    auto input_ty =\n+        mlir::dyn_cast_or_null<RankedTensorType>(fc.getInput().getType());\n+    auto fc_ty = mlir::dyn_cast_or_null<RankedTensorType>(fc.getType(0));\n+    if (!input_ty || !fc_ty) return failure();\n+\n+    auto input_shape = input_ty.getShape();\n+    auto fc_shape = fc_ty.getShape();\n+    if (input_shape.size() == fc_shape.size()) {\n+      return failure();\n+    }\n+\n+    fc.setKeepNumDims(false);\n+    return success();\n+  }\n+};\n+\n class StrictQuantizationPattern : public RewritePattern {\n  public:\n   using BaseType = StrictQuantizationPattern;\n@@ -764,7 +798,7 @@ void QuantizePass::runOnOperation() {\n     patterns.add<TFLFullQuantization, TFLFullQuantizationReverse>(ctx,\n                                                                   quant_params);\n   }\n-\n+  patterns.add<FixFullyConnectedKeepNumDims>(ctx);\n   (void)applyPatternsGreedily(func, std::move(patterns));\n \n   // Constant quantization is a lossy transformation, so they are applied only"
        }
    ],
    "stats": {
        "total": 46,
        "additions": 40,
        "deletions": 6
    }
}