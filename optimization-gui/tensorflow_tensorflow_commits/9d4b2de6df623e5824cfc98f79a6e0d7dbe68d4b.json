{
    "author": "vwbaker",
    "message": "Update the NativeEmitter to only work on fusions that already are going through the NativeEmitter. This allows us to autotune above fusion dispatch between native & block level emitters.\n\nPiperOrigin-RevId: 802487607",
    "sha": "9d4b2de6df623e5824cfc98f79a6e0d7dbe68d4b",
    "files": [
        {
            "sha": "f3828419f64e4b4e3ea277a989db8c11ef0dc2aa",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/native_emitter.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 3,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9d4b2de6df623e5824cfc98f79a6e0d7dbe68d4b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9d4b2de6df623e5824cfc98f79a6e0d7dbe68d4b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter.cc?ref=9d4b2de6df623e5824cfc98f79a6e0d7dbe68d4b",
            "patch": "@@ -36,10 +36,19 @@ limitations under the License.\n namespace xla {\n namespace gpu {\n \n+// Returns true if the given instruction is a fusion instruction that is\n+// supported by the native emitter backend.\n+//\n+// There is no guarantee that the native emitter backend can actually compile if\n+// it has a config for another backend, and we currently don't have an easy way\n+// to check that. Therefore, we only support fusions that are already set up to\n+// go through the native emitter.\n bool IsSupported(const HloInstruction& instr) {\n-  return instr.opcode() == HloOpcode::kFusion &&\n-         // TODO: b/440062644 - Support multi-output fusions.\n-         !Cast<HloFusionInstruction>(&instr)->IsMultiOutputFusion();\n+  if (instr.opcode() != HloOpcode::kFusion) {\n+    return false;\n+  }\n+  auto fusion_kind = Cast<HloFusionInstruction>(&instr)->fusion_kind();\n+  return fusion_kind != HloInstruction::FusionKind::kCustom;\n }\n \n absl::StatusOr<std::vector<std::unique_ptr<BackendConfig>>>"
        },
        {
            "sha": "e3547074701b6e770e748fb3660bd57b4a948332",
            "filename": "third_party/xla/xla/backends/gpu/autotuner/native_emitter_test.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 14,
            "changes": 21,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/9d4b2de6df623e5824cfc98f79a6e0d7dbe68d4b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/9d4b2de6df623e5824cfc98f79a6e0d7dbe68d4b/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fautotuner%2Fnative_emitter_test.cc?ref=9d4b2de6df623e5824cfc98f79a6e0d7dbe68d4b",
            "patch": "@@ -59,18 +59,11 @@ HloModule m\n \n ENTRY %entry_computation (p0: f32[32,4096,2048]) -> f32[32,2048] {\n   %p0 = f32[32,4096,2048]{2,1,0} parameter(0)\n-  ROOT %reduce_fusion = f32[32,2048]{1,0} fusion(%p0), kind=kCustom,\n-    calls=%fused_reduce.clone,\n-    backend_config={ \"fusion_backend_config\": {\n-      \"kind\":\"__triton\",\n-      \"block_level_fusion_config\":{\n-        \"num_warps\":\"8\",\"output_tiles\":[{\"sizes\":[\"1\",\"4\"]}],\n-        \"num_ctas\":1,\"num_stages\":1,\"is_tma_allowed\":false\n-      }\n-    }}\n+  ROOT %reduce_fusion = f32[32,2048]{1,0} fusion(%p0), kind=kInput,\n+    calls=%fused_reduce.clone\n })\";\n \n-const char kMultiOutputFusionHlo[] = R\"(\n+const char kCustomFusionHlo[] = R\"(\n HloModule m\n \n %fused_add_and_sub (p0: f32[32,16], p1: f32[32,16]) -> (f32[32,16], f32[32,16]) {\n@@ -136,15 +129,15 @@ TEST_F(NativeEmitterBackendTest, GetSupportedConfigs) {\n }\n \n TEST_F(NativeEmitterBackendTest,\n-       GetSupportedConfigsDoesNotSupportMultiOutputFusions) {\n+       GetSupportedConfigsDoesNotSupportKCustomFusions) {\n   TF_ASSERT_OK_AND_ASSIGN(auto module,\n-                          ParseAndReturnVerifiedModule(kMultiOutputFusionHlo));\n+                          ParseAndReturnVerifiedModule(kCustomFusionHlo));\n   auto fusion_instruction = module->entry_computation()->root_instruction();\n   // Call GetSupportedConfigs on the fusion instruction.\n   TF_ASSERT_OK_AND_ASSIGN(std::vector<std::unique_ptr<BackendConfig>> configs,\n                           backend_.GetSupportedConfigs(*(fusion_instruction)));\n-  // GetSupportedConfigs should return an empty vector as it doesn't support\n-  // multi-output fusions.\n+  // GetSupportedConfigs should return an empty vector as it doesn't support the\n+  // fusion.\n   ASSERT_TRUE(configs.empty());\n }\n "
        }
    ],
    "stats": {
        "total": 36,
        "additions": 19,
        "deletions": 17
    }
}