{
    "author": "akuegel",
    "message": "[XLA:GPU] Make ReductionEmitter deterministic.\n\nSo far, the output could be non-deterministic if multiple reductions are\ngrouped together. This change makes it deterministic.\n\nPiperOrigin-RevId: 824965037",
    "sha": "a12d2cfb31ddd40723457352c20128d94bb047c3",
    "files": [
        {
            "sha": "78e98d3532a6632d5fb3311c49953941e8a0e6ed",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/reduction.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a12d2cfb31ddd40723457352c20128d94bb047c3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Freduction.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a12d2cfb31ddd40723457352c20128d94bb047c3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Freduction.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Freduction.cc?ref=a12d2cfb31ddd40723457352c20128d94bb047c3",
            "patch": "@@ -195,9 +195,9 @@ PerThreadOutputs ReductionFusion::EmitterState::EmitPerThreadElements(\n   const auto& reductions = owner.reduction_heroes_[group_id];\n   absl::flat_hash_map<const HloInstruction*, int> iter_arg_starts;\n \n-  for (const auto& [reduction, init] : inits) {\n+  for (const HloInstruction* reduction : reductions) {\n     iter_arg_starts[reduction] = iter_arg_inits.size();\n-    iter_arg_inits.append(init);\n+    iter_arg_inits.append(inits.find(reduction)->second);\n   }\n \n   auto body_builder = [&](ImplicitLocOpBuilder& nested_b,"
        },
        {
            "sha": "3cca83f157acbde3b2ffda6f40663844f262c747",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/tests/reduce_row/reduction_groups_two_in_same_group.hlo",
            "status": "added",
            "additions": 39,
            "deletions": 0,
            "changes": 39,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/a12d2cfb31ddd40723457352c20128d94bb047c3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftests%2Freduce_row%2Freduction_groups_two_in_same_group.hlo",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/a12d2cfb31ddd40723457352c20128d94bb047c3/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftests%2Freduce_row%2Freduction_groups_two_in_same_group.hlo",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftests%2Freduce_row%2Freduction_groups_two_in_same_group.hlo?ref=a12d2cfb31ddd40723457352c20128d94bb047c3",
            "patch": "@@ -0,0 +1,39 @@\n+// RUN: fusion_to_mlir %s | FileCheck %s\n+// RUN: gpu_test_correctness %s\n+\n+%add_f32 {\n+  %x = f32[] parameter(0)\n+  %y = f32[] parameter(1)\n+  ROOT %add = f32[] add(%x, %y)\n+}\n+\n+%add_f32.2 {\n+  %x = f32[] parameter(0)\n+  %y = f32[] parameter(1)\n+  ROOT %add = f32[] add(%x, %y)\n+}\n+\n+fusion {\n+  param_1.23864 = f32[1,1,144,256,32]{4,3,2,1,0} parameter(1)\n+  constant_8203_2_clone_1 = f32[] constant(0.844827533)\n+  broadcast.6321.5.clone.1 = f32[1,1,144,256,32]{4,3,2,1,0} broadcast(constant_8203_2_clone_1), dimensions={}\n+  mul.770.5.clone.1 = f32[1,1,144,256,32]{4,3,2,1,0} multiply(param_1.23864, broadcast.6321.5.clone.1)\n+  bitcast.125.3.clone.1 = f32[1,144,256,32]{3,2,1,0} bitcast(mul.770.5.clone.1)\n+  param_0.21084 = f32[1,144,256,32]{3,2,1,0} parameter(0)\n+  add_any.68.3.clone.1 = f32[1,144,256,32]{3,2,1,0} add(bitcast.125.3.clone.1, param_0.21084)\n+  constant_8190_1_clone_1 = f32[] constant(0.393919319)\n+  broadcast.6364.3.clone.1 = f32[1,144,256,32]{3,2,1,0} broadcast(constant_8190_1_clone_1), dimensions={}\n+  mul.675.1.clone.1 = f32[1,144,256,32]{3,2,1,0} multiply(add_any.68.3.clone.1, broadcast.6364.3.clone.1)\n+  bitcast.15178.1 = f32[128,288,32]{2,1,0} bitcast(mul.675.1.clone.1)\n+  constant_8186_50 = f32[] constant(0)\n+  reduce.812.1 = f32[128,32]{1,0} reduce(bitcast.15178.1, constant_8186_50), dimensions={1}, to_apply=add_f32\n+  constant_8204_2_clone_1 = f32[] constant(0.362068981)\n+  broadcast.6327.3.clone.1 = f32[1,1,144,256,32]{4,3,2,1,0} broadcast(constant_8204_2_clone_1), dimensions={}\n+  mul.771.3.clone.1 = f32[1,1,144,256,32]{4,3,2,1,0} multiply(param_1.23864, broadcast.6327.3.clone.1)\n+  bitcast.15180.1.clone.1 = f32[128,288,32]{2,1,0} bitcast(mul.771.3.clone.1)\n+  reduce.816.1.clone.1 = f32[128,32]{1,0} reduce(bitcast.15180.1.clone.1, constant_8186_50), dimensions={1}, to_apply=add_f32.2\n+  ROOT tuple.1351 = (f32[128,32]{1,0}, f32[1,144,256,32]{3,2,1,0}, f32[128,32]{1,0}) tuple(reduce.812.1, mul.675.1.clone.1, reduce.816.1.clone.1)\n+}\n+\n+// CHECK: xla.pure_call @add_f32_add\n+// CHECK: xla.pure_call @add_f32_2_add_1"
        }
    ],
    "stats": {
        "total": 43,
        "additions": 41,
        "deletions": 2
    }
}