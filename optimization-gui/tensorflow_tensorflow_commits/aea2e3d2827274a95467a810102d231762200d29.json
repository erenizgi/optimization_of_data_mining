{
    "author": "tensorflower-gardener",
    "message": "Automated Code Change\n\nPiperOrigin-RevId: 845668992",
    "sha": "aea2e3d2827274a95467a810102d231762200d29",
    "files": [
        {
            "sha": "b80045c28f08cf60f10d11191c0fd42c3ec0c257",
            "filename": "tensorflow/core/distributed_runtime/rpc_collective_executor_mgr.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Frpc_collective_executor_mgr.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Frpc_collective_executor_mgr.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fdistributed_runtime%2Frpc_collective_executor_mgr.cc?ref=aea2e3d2827274a95467a810102d231762200d29",
            "patch": "@@ -30,7 +30,7 @@ RpcCollectiveExecutorMgr::RpcCollectiveExecutorMgr(\n     std::unique_ptr<DeviceResolverDistributed> dev_resolver,\n     std::unique_ptr<CollectiveParamResolverDistributed> param_resolver,\n     std::unique_ptr<NcclCommunicatorInterface> nccl_communicator,\n-    WorkerCacheInterface* worker_cache, const string& task_name)\n+    WorkerCacheInterface* worker_cache, const std::string& task_name)\n     : CollectiveExecutorMgr(config, dev_mgr, std::move(dev_resolver),\n                             std::move(param_resolver),\n                             std::move(nccl_communicator)),\n@@ -172,7 +172,8 @@ void RpcCollectiveExecutorMgr::RetireStepId(int64_t graph_key,\n std::unique_ptr<RpcCollectiveExecutorMgr> CreateProdRpcCollectiveExecutorMgr(\n     const ConfigProto& config, const DeviceMgr* device_mgr,\n     std::unique_ptr<NcclCommunicatorInterface> nccl_communicator,\n-    WorkerCacheInterface* worker_cache, const string& default_worker_name) {\n+    WorkerCacheInterface* worker_cache,\n+    const std::string& default_worker_name) {\n   auto dev_resolver = std::make_unique<DeviceResolverDistributed>(device_mgr);\n   auto param_resolver = std::make_unique<CollectiveParamResolverDistributed>(\n       config, device_mgr, dev_resolver.get(), nccl_communicator.get(),"
        },
        {
            "sha": "aadbaf33796437f5c7ef4da585f98893b4f9706a",
            "filename": "tensorflow/core/distributed_runtime/rpc_collective_executor_mgr.h",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Frpc_collective_executor_mgr.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Frpc_collective_executor_mgr.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fdistributed_runtime%2Frpc_collective_executor_mgr.h?ref=aea2e3d2827274a95467a810102d231762200d29",
            "patch": "@@ -39,7 +39,7 @@ class RpcCollectiveExecutorMgr : public CollectiveExecutorMgr {\n       std::unique_ptr<DeviceResolverDistributed> dev_resolver,\n       std::unique_ptr<CollectiveParamResolverDistributed> param_resolver,\n       std::unique_ptr<NcclCommunicatorInterface> nccl_communicator,\n-      WorkerCacheInterface* worker_cache, const string& task_name);\n+      WorkerCacheInterface* worker_cache, const std::string& task_name);\n \n   virtual ~RpcCollectiveExecutorMgr();\n \n@@ -60,8 +60,8 @@ class RpcCollectiveExecutorMgr : public CollectiveExecutorMgr {\n   virtual CollectiveExecutor* Create(int64_t step_id) override;\n \n   WorkerCacheInterface* const worker_cache_;  // Not owned.\n-  const string task_name_;\n-  string group_leader_;\n+  const std::string task_name_;\n+  std::string group_leader_;\n   friend class RpcCollectiveExecutorMgrTest;\n \n  private:\n@@ -88,7 +88,7 @@ class RpcCollectiveExecutorMgr : public CollectiveExecutorMgr {\n std::unique_ptr<RpcCollectiveExecutorMgr> CreateProdRpcCollectiveExecutorMgr(\n     const ConfigProto& config, const DeviceMgr* device_mgr,\n     std::unique_ptr<NcclCommunicatorInterface> nccl_communicator,\n-    WorkerCacheInterface* worker_cache, const string& default_worker_name);\n+    WorkerCacheInterface* worker_cache, const std::string& default_worker_name);\n \n }  // namespace tensorflow\n #endif  // TENSORFLOW_CORE_DISTRIBUTED_RUNTIME_RPC_COLLECTIVE_EXECUTOR_MGR_H_"
        },
        {
            "sha": "55eebf621e5882df81eefa0ece6abe185a533db6",
            "filename": "tensorflow/core/distributed_runtime/rpc_collective_executor_mgr_test.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Frpc_collective_executor_mgr_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Frpc_collective_executor_mgr_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fdistributed_runtime%2Frpc_collective_executor_mgr_test.cc?ref=aea2e3d2827274a95467a810102d231762200d29",
            "patch": "@@ -39,7 +39,7 @@ namespace tensorflow {\n class RpcCollectiveExecutorMgrTest : public ::testing::Test {\n  protected:\n   RpcCollectiveExecutorMgrTest() {\n-    string task_name = \"/job:localhost/replica:0/task:0\";\n+    std::string task_name = \"/job:localhost/replica:0/task:0\";\n     SessionOptions options;\n     options.config.mutable_experimental()->set_collective_group_leader(\n         task_name);"
        },
        {
            "sha": "70816cc8a7b556484113cf313eeae9317aaa28d5",
            "filename": "tensorflow/core/distributed_runtime/rpcbench_test.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Frpcbench_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Frpcbench_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fdistributed_runtime%2Frpcbench_test.cc?ref=aea2e3d2827274a95467a810102d231762200d29",
            "patch": "@@ -42,7 +42,7 @@ static const int kWorkers = 60;\n static thread::ThreadPool* worker_threads;\n \n void MakeGRPCCluster(const SessionOptions& options, int n,\n-                     std::vector<string>* workers,\n+                     std::vector<std::string>* workers,\n                      std::vector<DeviceAttributes>* devices) {\n   CHECK_GE(n, 1);\n \n@@ -100,7 +100,7 @@ void MakeGRPCCluster(const SessionOptions& options, int n,\n \n struct Cluster {\n   SessionOptions options;\n-  std::vector<string> workers;\n+  std::vector<std::string> workers;\n   std::vector<DeviceAttributes> devices;  // One per process\n \n   Cluster() {\n@@ -153,14 +153,14 @@ GraphDef CreateGraphDef(int num_stages, int width, int tensor_size,\n   return def;\n }\n \n-string DebugString(const Tensor& x, const Tensor& y, int tensor_size) {\n+std::string DebugString(const Tensor& x, const Tensor& y, int tensor_size) {\n   CHECK_EQ(x.NumElements(), tensor_size);\n   CHECK_EQ(y.NumElements(), tensor_size);\n   auto x_flat = x.flat<float>();\n   auto y_flat = y.flat<float>();\n   // Just print the first couple of elements of each tensor\n   CHECK_GE(tensor_size, 2);\n-  return strings::Printf(\"x = [%8.6f %8.6f] y = [%8.6f %8.6f]\", x_flat(0),\n+  return absl::StrFormat(\"x = [%8.6f %8.6f] y = [%8.6f %8.6f]\", x_flat(0),\n                          x_flat(1), y_flat(0), y_flat(1));\n }\n "
        },
        {
            "sha": "d277bdab74e8352103fb3efb4d47b285a469628a",
            "filename": "tensorflow/core/distributed_runtime/scheduler.h",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Fscheduler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Fscheduler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fdistributed_runtime%2Fscheduler.h?ref=aea2e3d2827274a95467a810102d231762200d29",
            "patch": "@@ -86,7 +86,7 @@ class GreedyScheduler {\n   const CostModel* cost_model_;\n   const Graph* graph_;\n   std::vector<int64_t>* priority_;\n-  std::unordered_map<string, Sim*> device_states_;\n+  std::unordered_map<std::string, Sim*> device_states_;\n \n   GreedyScheduler(const GreedyScheduler&) = delete;\n   void operator=(const GreedyScheduler&) = delete;"
        },
        {
            "sha": "527dd49507c607db86869ddf555c8dccc1b11765",
            "filename": "tensorflow/core/distributed_runtime/server_lib.cc",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Fserver_lib.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Fserver_lib.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fdistributed_runtime%2Fserver_lib.cc?ref=aea2e3d2827274a95467a810102d231762200d29",
            "patch": "@@ -28,15 +28,15 @@ mutex* get_server_factory_lock() {\n   return &server_factory_lock;\n }\n \n-typedef std::unordered_map<string, ServerFactory*> ServerFactories;\n+typedef std::unordered_map<std::string, ServerFactory*> ServerFactories;\n ServerFactories* server_factories() {\n   static ServerFactories* factories = new ServerFactories;\n   return factories;\n }\n }  // namespace\n \n /* static */\n-void ServerFactory::Register(const string& server_type,\n+void ServerFactory::Register(const std::string& server_type,\n                              ServerFactory* factory) {\n   mutex_lock l(*get_server_factory_lock());\n   if (!server_factories()->insert({server_type, factory}).second) {\n@@ -56,7 +56,7 @@ absl::Status ServerFactory::GetFactory(const ServerDef& server_def,\n     }\n   }\n \n-  std::vector<string> server_names;\n+  std::vector<std::string> server_names;\n   for (const auto& server_factory : *server_factories()) {\n     server_names.push_back(server_factory.first);\n   }"
        },
        {
            "sha": "c49d47970b4ca08f3723ae407db15e1b32aa8b29",
            "filename": "tensorflow/core/distributed_runtime/server_lib.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Fserver_lib.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Fserver_lib.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fdistributed_runtime%2Fserver_lib.h?ref=aea2e3d2827274a95467a810102d231762200d29",
            "patch": "@@ -64,7 +64,7 @@ class ServerInterface {\n \n   // Returns a target string that can be used to connect to this server using\n   // `tensorflow::NewSession()`.\n-  virtual const string target() const = 0;\n+  virtual const std::string target() const = 0;\n \n   virtual WorkerEnv* worker_env() = 0;\n   virtual MasterEnv* master_env() = 0;\n@@ -77,7 +77,7 @@ class ServerInterface {\n   // Add master eager context to local eager service in order to handle enqueue\n   // requests from remote workers.\n   virtual absl::Status AddMasterEagerContextToEagerService(\n-      const tensorflow::uint64 context_id, EagerContext* context) = 0;\n+      const uint64_t context_id, EagerContext* context) = 0;\n   // Set coordination service agent instance to coordination service RPC handler\n   virtual absl::Status SetCoordinationServiceAgentInstance(\n       tsl::CoordinationServiceAgent* agent) = 0;\n@@ -113,7 +113,7 @@ class ServerFactory {\n   // be registered by calling this method.\n   //\n   // The `server_type` must be unique to the server factory.\n-  static void Register(const string& server_type, ServerFactory* factory);\n+  static void Register(const std::string& server_type, ServerFactory* factory);\n \n   // Looks up a factory that can create a server based on the given\n   // `server_def`, and stores it in `*out_factory`. Returns OK on"
        },
        {
            "sha": "43524d19a3578888eb32cec0171d49fa51d4fcef",
            "filename": "tensorflow/core/distributed_runtime/tensor_coding.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Ftensor_coding.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Ftensor_coding.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fdistributed_runtime%2Ftensor_coding.cc?ref=aea2e3d2827274a95467a810102d231762200d29",
            "patch": "@@ -114,14 +114,14 @@ enum WireType {\n   WIRETYPE_VARINT = 0,\n   WIRETYPE_LENGTH_DELIMITED = 2,\n };\n-inline int GetTagFieldNumber(uint32 tag) { return tag >> 3; }\n-inline WireType GetTagWireType(uint32 tag) {\n+inline int GetTagFieldNumber(uint32_t tag) { return tag >> 3; }\n+inline WireType GetTagWireType(uint32_t tag) {\n   return static_cast<WireType>(tag & 0x7);\n }\n \n bool ReadVarintSizeAsInt(protobuf::io::CodedInputStream* input, int* result) {\n   protobuf_uint64 v;\n-  if (input->ReadVarint64(&v) && v <= static_cast<uint64>(INT_MAX)) {\n+  if (input->ReadVarint64(&v) && v <= static_cast<uint64_t>(INT_MAX)) {\n     *result = static_cast<int>(v);\n     return true;\n   } else {\n@@ -162,7 +162,7 @@ bool TensorResponse::ParseTensorSubmessage(\n     }\n     switch (tag) {\n       case TensorProto::kDtypeFieldNumber: {\n-        uint32 v;\n+        uint32_t v;\n         if ((wt != WIRETYPE_VARINT) || !input->ReadVarint32(&v)) return false;\n         if (seen_tensor_content) return false;\n         tensor_meta->set_dtype(static_cast<DataType>(static_cast<int>(v)));\n@@ -177,10 +177,10 @@ bool TensorResponse::ParseTensorSubmessage(\n         break;\n       }\n       case TensorProto::kVersionNumberFieldNumber: {\n-        uint32 v;\n+        uint32_t v;\n         if ((wt != WIRETYPE_VARINT) || !input->ReadVarint32(&v)) return false;\n         if (seen_tensor_content) return false;\n-        tensor_meta->set_version_number(static_cast<int32>(v));\n+        tensor_meta->set_version_number(static_cast<int32_t>(v));\n         break;\n       }\n       case TensorProto::kTensorContentFieldNumber: {\n@@ -242,7 +242,7 @@ bool TensorResponse::ParseFast(Source* source) {\n         break;\n       }\n       case RecvTensorResponse::kIsDeadFieldNumber: {\n-        uint32 v;\n+        uint32_t v;\n         if ((wt != WIRETYPE_VARINT) || !input.ReadVarint32(&v)) return false;\n         meta_.set_is_dead(v != 0);\n         break;\n@@ -260,7 +260,7 @@ bool TensorResponse::ParseFast(Source* source) {\n         break;\n       }\n       case RecvTensorResponse::kRequireAckFieldNumber: {\n-        uint32 v;\n+        uint32_t v;\n         if ((wt != WIRETYPE_VARINT) || !input.ReadVarint32(&v)) return false;\n         meta_.set_require_ack(v != 0);\n         break;"
        },
        {
            "sha": "66ba2bdce86b3a30fd8a66240d4b9720e812d10c",
            "filename": "tensorflow/core/distributed_runtime/tensor_coding_test.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Ftensor_coding_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Ftensor_coding_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fdistributed_runtime%2Ftensor_coding_test.cc?ref=aea2e3d2827274a95467a810102d231762200d29",
            "patch": "@@ -48,7 +48,7 @@ class DummyDevice : public DeviceBase {\n \n class StringSource : public TensorResponse::Source {\n  public:\n-  explicit StringSource(const string* s, int block_size)\n+  explicit StringSource(const std::string* s, int block_size)\n       : s_(s), stream_(nullptr), block_size_(block_size) {}\n   ~StringSource() override { DeleteStream(); }\n \n@@ -66,7 +66,7 @@ class StringSource : public TensorResponse::Source {\n   }\n \n  private:\n-  const string* s_;\n+  const std::string* s_;\n   protobuf::io::ArrayInputStream* stream_;\n   char space_[sizeof(protobuf::io::ArrayInputStream)];\n   int block_size_;\n@@ -83,7 +83,7 @@ class TensorResponseTest : public ::testing::Test {\n     } else {\n       src.AsProtoField(proto.mutable_tensor());\n     }\n-    string encoded;\n+    std::string encoded;\n     proto.AppendToString(&encoded);\n \n     StringSource source(&encoded, 1024);\n@@ -136,11 +136,11 @@ class TensorResponseTest : public ::testing::Test {\n TEST_F(TensorResponseTest, Simple) {\n   DoTest<float>(DT_FLOAT);\n   DoTest<double>(DT_DOUBLE);\n-  DoTest<int32>(DT_INT32);\n-  DoTest<uint16>(DT_UINT16);\n-  DoTest<uint8>(DT_UINT8);\n-  DoTest<int16>(DT_INT16);\n-  DoTest<int8>(DT_INT8);\n+  DoTest<int32_t>(DT_INT32);\n+  DoTest<uint16_t>(DT_UINT16);\n+  DoTest<uint8_t>(DT_UINT8);\n+  DoTest<int16_t>(DT_INT16);\n+  DoTest<int8_t>(DT_INT8);\n   DoTest<complex64>(DT_COMPLEX64);\n   DoTest<complex128>(DT_COMPLEX128);\n   DoTest<int64_t>(DT_INT64);\n@@ -156,27 +156,27 @@ TEST_F(TensorResponseTest, Simple) {\n \n TEST_F(TensorResponseTest, StringTensor) { DoTestForStrings(DT_STRING); }\n \n-string MakeFloatTensorTestCase(int num_elems) {\n-  std::vector<int8> v(num_elems);\n+std::string MakeFloatTensorTestCase(int num_elems) {\n+  std::vector<int8_t> v(num_elems);\n   for (int i = 0; i < num_elems; i++) {\n     v[i] = i % 10;\n   }\n   Tensor src(DT_INT8, TensorShape({1, static_cast<int64_t>(v.size())}));\n-  test::FillValues<int8>(&src, v);\n+  test::FillValues<int8_t>(&src, v);\n \n   RecvTensorResponse proto;\n   proto.set_is_dead(false);\n   proto.set_send_start_micros(123456);\n   src.AsProtoTensorContent(proto.mutable_tensor());\n-  string encoded;\n+  std::string encoded;\n   proto.AppendToString(&encoded);\n   return encoded;\n }\n \n static void BM_TensorResponse(::testing::benchmark::State& state) {\n   const int arg = state.range(0);\n \n-  string encoded = MakeFloatTensorTestCase(arg);\n+  std::string encoded = MakeFloatTensorTestCase(arg);\n   DummyDevice cpu_device(Env::Default());\n   size_t bytes = 0;\n   for (auto i : state) {"
        },
        {
            "sha": "b7316299e051c3f000b86239b6c62b7f018062e7",
            "filename": "tensorflow/core/distributed_runtime/test_utils.h",
            "status": "modified",
            "additions": 14,
            "deletions": 11,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Ftest_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Ftest_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fdistributed_runtime%2Ftest_utils.h?ref=aea2e3d2827274a95467a810102d231762200d29",
            "patch": "@@ -124,23 +124,24 @@ class TestWorkerCache : public WorkerCacheInterface {\n  public:\n   virtual ~TestWorkerCache() {}\n \n-  void AddWorker(const string& target, WorkerInterface* wi) {\n+  void AddWorker(const std::string& target, WorkerInterface* wi) {\n     workers_[target] = wi;\n   }\n \n-  void AddDevice(const string& device_name, const DeviceLocality& dev_loc) {\n+  void AddDevice(const std::string& device_name,\n+                 const DeviceLocality& dev_loc) {\n     localities_[device_name] = dev_loc;\n   }\n \n-  void ListWorkers(std::vector<string>* workers) const override {\n+  void ListWorkers(std::vector<std::string>* workers) const override {\n     workers->clear();\n     for (auto it : workers_) {\n       workers->push_back(it.first);\n     }\n   }\n \n-  void ListWorkersInJob(const string& job_name,\n-                        std::vector<string>* workers) const override {\n+  void ListWorkersInJob(const std::string& job_name,\n+                        std::vector<std::string>* workers) const override {\n     workers->clear();\n     for (auto it : workers_) {\n       DeviceNameUtils::ParsedName device_name;\n@@ -152,15 +153,16 @@ class TestWorkerCache : public WorkerCacheInterface {\n     }\n   }\n \n-  WorkerInterface* GetOrCreateWorker(const string& target) override {\n+  WorkerInterface* GetOrCreateWorker(const std::string& target) override {\n     auto it = workers_.find(target);\n     if (it != workers_.end()) {\n       return it->second;\n     }\n     return nullptr;\n   }\n \n-  void ReleaseWorker(const string& target, WorkerInterface* worker) override {}\n+  void ReleaseWorker(const std::string& target,\n+                     WorkerInterface* worker) override {}\n \n   absl::Status GetEagerClientCache(\n       std::unique_ptr<eager::EagerClientCache>* eager_client_cache) override {\n@@ -172,7 +174,7 @@ class TestWorkerCache : public WorkerCacheInterface {\n     return errors::Unimplemented(\"Unimplemented.\");\n   }\n \n-  bool GetDeviceLocalityNonBlocking(const string& device,\n+  bool GetDeviceLocalityNonBlocking(const std::string& device,\n                                     DeviceLocality* locality) override {\n     auto it = localities_.find(device);\n     if (it != localities_.end()) {\n@@ -182,7 +184,8 @@ class TestWorkerCache : public WorkerCacheInterface {\n     return false;\n   }\n \n-  void GetDeviceLocalityAsync(const string& device, DeviceLocality* locality,\n+  void GetDeviceLocalityAsync(const std::string& device,\n+                              DeviceLocality* locality,\n                               StatusCallback done) override {\n     auto it = localities_.find(device);\n     if (it != localities_.end()) {\n@@ -194,8 +197,8 @@ class TestWorkerCache : public WorkerCacheInterface {\n   }\n \n  protected:\n-  std::unordered_map<string, WorkerInterface*> workers_;\n-  std::unordered_map<string, DeviceLocality> localities_;\n+  std::unordered_map<std::string, WorkerInterface*> workers_;\n+  std::unordered_map<std::string, DeviceLocality> localities_;\n };\n \n }  // namespace tensorflow"
        },
        {
            "sha": "04b0ee20d2cc8f8a15a99415e27d5f0e0661b14d",
            "filename": "tensorflow/core/distributed_runtime/worker.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Fworker.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Fworker.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fdistributed_runtime%2Fworker.cc?ref=aea2e3d2827274a95467a810102d231762200d29",
            "patch": "@@ -251,7 +251,7 @@ void Worker::DoRunGraph(CallOptions* opts, RunGraphRequestWrapper* request,\n \n         if (s.ok()) {\n           for (const auto& p : *out) {\n-            const string& key = p.first;\n+            const std::string& key = p.first;\n             const Tensor& val = p.second;\n             response->AddRecv(key, val);\n           }\n@@ -271,7 +271,7 @@ void Worker::DoPartialRunGraph(CallOptions* opts,\n                                MutableRunGraphResponseWrapper* response,\n                                StatusCallback done) {\n   const int64_t step_id = request->step_id();\n-  const string& graph_handle = request->graph_handle();\n+  const std::string& graph_handle = request->graph_handle();\n   TRACEPRINTF(\"PartialRunGraph: %lld\", step_id);\n   absl::Status s = recent_request_ids_.TrackUnique(\n       request->request_id(), \"PartialRunGraph (Worker)\", request);\n@@ -345,7 +345,7 @@ void Worker::DoPartialRunGraph(CallOptions* opts,\n         if (s.ok()) {\n           // Construct and return the resp.\n           for (const auto& p : *out) {\n-            const string& key = p.first;\n+            const std::string& key = p.first;\n             const Tensor& val = p.second;\n             response->AddRecv(key, val);\n           }\n@@ -378,7 +378,7 @@ void Worker::CleanupGraphAsync(const CleanupGraphRequest* request,\n void Worker::CleanupAllAsync(const CleanupAllRequest* request,\n                              CleanupAllResponse* response,\n                              StatusCallback done) {\n-  std::vector<string> containers;\n+  std::vector<std::string> containers;\n   for (const auto& c : request->container()) containers.push_back(c);\n   env_->device_mgr->ClearContainers(containers);\n   done(absl::OkStatus());\n@@ -474,7 +474,7 @@ void Worker::GetStepSequenceAsync(const GetStepSequenceRequest* request,\n absl::Status Worker::PrepareRecvTensor(const Rendezvous::ParsedKey& parsed,\n                                        Device** src_dev) {\n   // Figures out which device the tensor is hosted on.\n-  string local_name = DeviceNameUtils::LocalName(parsed.src_device);\n+  std::string local_name = DeviceNameUtils::LocalName(parsed.src_device);\n   TF_RETURN_IF_ERROR(env_->device_mgr->LookupDevice(local_name, src_dev));\n \n   // Does the device have the right incarnation number we expect?"
        },
        {
            "sha": "0612a8321d3aac5b303d223d18dec9a90300c484",
            "filename": "tensorflow/core/distributed_runtime/worker_cache.h",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Fworker_cache.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Fworker_cache.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fdistributed_runtime%2Fworker_cache.h?ref=aea2e3d2827274a95467a810102d231762200d29",
            "patch": "@@ -37,22 +37,23 @@ class WorkerCacheInterface {\n \n   // Updates *workers with strings naming the remote worker tasks to\n   // which open channels have been established.\n-  virtual void ListWorkers(std::vector<string>* workers) const = 0;\n-  virtual void ListWorkersInJob(const string& job_name,\n-                                std::vector<string>* workers) const = 0;\n+  virtual void ListWorkers(std::vector<std::string>* workers) const = 0;\n+  virtual void ListWorkersInJob(const std::string& job_name,\n+                                std::vector<std::string>* workers) const = 0;\n \n   // If \"target\" names a remote task for which an RPC channel exists\n   // or can be constructed, returns a pointer to a WorkerInterface object\n   // wrapping that channel. The returned value must be destroyed by\n   // calling `this->ReleaseWorker(target, ret)`\n-  virtual WorkerInterface* GetOrCreateWorker(const string& target) = 0;\n+  virtual WorkerInterface* GetOrCreateWorker(const std::string& target) = 0;\n \n   // Release a worker previously returned by this->GetOrCreateWorker(target).\n   //\n   // TODO(jeff,sanjay): Consider moving target into WorkerInterface.\n   // TODO(jeff,sanjay): Unify all worker-cache impls and factor out a\n   //                    per-rpc-subsystem WorkerInterface creator.\n-  virtual void ReleaseWorker(const string& target, WorkerInterface* worker) {\n+  virtual void ReleaseWorker(const std::string& target,\n+                             WorkerInterface* worker) {\n     // Subclasses may override to reuse worker objects.\n     delete worker;\n   }\n@@ -61,13 +62,13 @@ class WorkerCacheInterface {\n   // within its local environment.  Returns true if *locality\n   // was set, using only locally cached data.  Returns false\n   // if status data for that device was not available.  Never blocks.\n-  virtual bool GetDeviceLocalityNonBlocking(const string& device,\n+  virtual bool GetDeviceLocalityNonBlocking(const std::string& device,\n                                             DeviceLocality* locality) = 0;\n \n   // Set *locality with the DeviceLocality of the specified remote device\n   // within its local environment.  Callback gets Status::OK if *locality\n   // was set.\n-  virtual void GetDeviceLocalityAsync(const string& device,\n+  virtual void GetDeviceLocalityAsync(const std::string& device,\n                                       DeviceLocality* locality,\n                                       StatusCallback done) = 0;\n "
        },
        {
            "sha": "5a1d3d02d4ecebf104146749a53367670a21f047",
            "filename": "tensorflow/core/distributed_runtime/worker_cache_logger.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 14,
            "changes": 26,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Fworker_cache_logger.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Fworker_cache_logger.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fdistributed_runtime%2Fworker_cache_logger.cc?ref=aea2e3d2827274a95467a810102d231762200d29",
            "patch": "@@ -68,7 +68,7 @@ bool WorkerCacheLogger::RetrieveLogs(int64_t step_id, StepStats* ss) {\n   return false;\n }\n \n-void WorkerCacheLogger::Save(const string& device, int64_t step_id,\n+void WorkerCacheLogger::Save(const std::string& device, int64_t step_id,\n                              NodeExecStats* ns) {\n   mutex_lock l(mu_);\n   StepLog* sl = &log_map_[step_id];\n@@ -84,33 +84,31 @@ void WorkerCacheLogger::Save(const string& device, int64_t step_id,\n \n void WorkerCacheLogger::RecordRecvTensor(int64_t step_id, int64_t start_usecs,\n                                          int64_t end_usecs,\n-                                         const string& tensor_name,\n-                                         const string& src_device,\n-                                         const string& dst_device,\n+                                         const std::string& tensor_name,\n+                                         const std::string& src_device,\n+                                         const std::string& dst_device,\n                                          int64_t bytes) {\n   RecordDataTransfer(step_id, start_usecs, end_usecs, tensor_name, src_device,\n                      dst_device, bytes, \"\", \"RecvTensor\");\n }\n \n-void WorkerCacheLogger::RecordDataTransfer(int64_t step_id, int64_t start_usecs,\n-                                           int64_t end_usecs,\n-                                           const string& tensor_name,\n-                                           const string& src_device,\n-                                           const string& dst_device,\n-                                           int64_t bytes, const string& details,\n-                                           const string& transfer_method_name) {\n+void WorkerCacheLogger::RecordDataTransfer(\n+    int64_t step_id, int64_t start_usecs, int64_t end_usecs,\n+    const std::string& tensor_name, const std::string& src_device,\n+    const std::string& dst_device, int64_t bytes, const std::string& details,\n+    const std::string& transfer_method_name) {\n   NodeExecStats* ns = new NodeExecStats;\n   ns->set_node_name(transfer_method_name);\n   int64_t elapsed_usecs = end_usecs - start_usecs;\n   if (details.empty()) {\n     auto byte_string = absl::StrCat(\"[\", bytes, \"B] \");\n     if (bytes >= 0.1 * 1048576.0) {\n-      byte_string = strings::Printf(\"[%.1fMB] \", bytes / 1048576.0);\n+      byte_string = absl::StrFormat(\"[%.1fMB] \", bytes / 1048576.0);\n     }\n     float mbs_rate = (8.0 * static_cast<float>(bytes)) / elapsed_usecs;\n     auto rate_string = (mbs_rate >= 1000.0)\n-                           ? strings::Printf(\"[%.1fGb/s] \", mbs_rate / 1000.0)\n-                           : strings::Printf(\"[%fMb/s] \", mbs_rate);\n+                           ? absl::StrFormat(\"[%.1fGb/s] \", mbs_rate / 1000.0)\n+                           : absl::StrFormat(\"[%fMb/s] \", mbs_rate);\n     auto label = strings::StrCat(byte_string, rate_string, tensor_name,\n                                  \" from \", src_device, \" to \", dst_device);\n     ns->set_timeline_label(label);"
        },
        {
            "sha": "e7a1ebf0c40708e2829a24927732a8a2062a7c33",
            "filename": "tensorflow/core/distributed_runtime/worker_cache_logger.h",
            "status": "modified",
            "additions": 10,
            "deletions": 8,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Fworker_cache_logger.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Fworker_cache_logger.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fdistributed_runtime%2Fworker_cache_logger.h?ref=aea2e3d2827274a95467a810102d231762200d29",
            "patch": "@@ -57,20 +57,22 @@ class WorkerCacheLogger {\n   // Generates a NodeExecStats record with the given data, and saves for\n   // later retrieval by RetrieveLogs().\n   void RecordRecvTensor(int64_t step_id, int64_t start_usecs, int64_t end_usecs,\n-                        const string& tensor_name, const string& src_device,\n-                        const string& dst_device, int64_t bytes);\n+                        const std::string& tensor_name,\n+                        const std::string& src_device,\n+                        const std::string& dst_device, int64_t bytes);\n \n   // Generates a NodeExecStats record with the given data, and saves for\n   // later retrieval by RetrieveLogs().\n   void RecordDataTransfer(int64_t step_id, int64_t start_usecs,\n-                          int64_t end_usecs, const string& tensor_name,\n-                          const string& src_device, const string& dst_device,\n-                          int64_t bytes, const string& details,\n-                          const string& transfer_method_name);\n+                          int64_t end_usecs, const std::string& tensor_name,\n+                          const std::string& src_device,\n+                          const std::string& dst_device, int64_t bytes,\n+                          const std::string& details,\n+                          const std::string& transfer_method_name);\n \n  private:\n   mutex count_mu_;\n-  int32 want_logging_count_ TF_GUARDED_BY(count_mu_) = 0;\n+  int32_t want_logging_count_ TF_GUARDED_BY(count_mu_) = 0;\n \n   struct StepLog {\n     StepStats step_stats;\n@@ -81,7 +83,7 @@ class WorkerCacheLogger {\n   LogMap log_map_ TF_GUARDED_BY(mu_);\n \n   // Records \"ns\" in log_map_ under the given device and step.\n-  void Save(const string& device, int64_t step_id, NodeExecStats* ns);\n+  void Save(const std::string& device, int64_t step_id, NodeExecStats* ns);\n \n   void ClearLogsWithLock() TF_EXCLUSIVE_LOCKS_REQUIRED(mu_);\n };"
        },
        {
            "sha": "47fdcce387297d26fa70dc1486c8ebdec3d88faf",
            "filename": "tensorflow/core/distributed_runtime/worker_cache_partial.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Fworker_cache_partial.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Fworker_cache_partial.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fdistributed_runtime%2Fworker_cache_partial.cc?ref=aea2e3d2827274a95467a810102d231762200d29",
            "patch": "@@ -27,7 +27,7 @@ limitations under the License.\n namespace tensorflow {\n \n bool WorkerCachePartial::GetDeviceLocalityNonBlocking(\n-    const string& device_name, DeviceLocality* locality) {\n+    const std::string& device_name, DeviceLocality* locality) {\n   mutex_lock lock(mu_);  // could use reader lock\n   auto iter = device_status_cache_.find(device_name);\n   if (iter != device_status_cache_.end()) {\n@@ -37,7 +37,7 @@ bool WorkerCachePartial::GetDeviceLocalityNonBlocking(\n   return false;\n }\n \n-void WorkerCachePartial::GetDeviceLocalityAsync(const string& device_name,\n+void WorkerCachePartial::GetDeviceLocalityAsync(const std::string& device_name,\n                                                 DeviceLocality* locality,\n                                                 StatusCallback done) {\n   if (!GetDeviceLocalityNonBlocking(device_name, locality)) {\n@@ -55,9 +55,9 @@ void WorkerCachePartial::GetDeviceLocalityAsync(const string& device_name,\n }\n \n absl::Status WorkerCachePartial::RefreshDeviceStatus(\n-    const string& device_name) {\n-  string task;\n-  string device;\n+    const std::string& device_name) {\n+  std::string task;\n+  std::string device;\n   absl::Status s;\n   if (!DeviceNameUtils::SplitDeviceName(device_name, &task, &device)) {\n     s = errors::InvalidArgument(\"Bad device name to RefreshDeviceStatus: \","
        },
        {
            "sha": "08e272a3bb6db67f193a2a676fd4332bae23918c",
            "filename": "tensorflow/core/distributed_runtime/worker_cache_partial.h",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Fworker_cache_partial.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Fworker_cache_partial.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fdistributed_runtime%2Fworker_cache_partial.h?ref=aea2e3d2827274a95467a810102d231762200d29",
            "patch": "@@ -31,10 +31,11 @@ namespace tensorflow {\n // device status attributes.\n class WorkerCachePartial : public WorkerCacheInterface {\n  public:\n-  bool GetDeviceLocalityNonBlocking(const string& device,\n+  bool GetDeviceLocalityNonBlocking(const std::string& device,\n                                     DeviceLocality* locality) override;\n \n-  void GetDeviceLocalityAsync(const string& device, DeviceLocality* locality,\n+  void GetDeviceLocalityAsync(const std::string& device,\n+                              DeviceLocality* locality,\n                               StatusCallback) override;\n \n   ~WorkerCachePartial() override {}\n@@ -47,9 +48,9 @@ class WorkerCachePartial : public WorkerCacheInterface {\n \n   // Initiate a GetStatusAsync to the remote task named by \"task\", and\n   // update the cache with all the DeviceAttributes reported.\n-  absl::Status RefreshDeviceStatus(const string& device_name);\n+  absl::Status RefreshDeviceStatus(const std::string& device_name);\n \n-  typedef std::unordered_map<string, DeviceAttributes> StatusMap;\n+  typedef std::unordered_map<std::string, DeviceAttributes> StatusMap;\n   StatusMap device_status_cache_ TF_GUARDED_BY(mu_);\n };\n "
        },
        {
            "sha": "8917da3825773b6a0cfa18c62d533a1780a4fd06",
            "filename": "tensorflow/core/distributed_runtime/worker_cache_wrapper.h",
            "status": "modified",
            "additions": 9,
            "deletions": 7,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Fworker_cache_wrapper.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Fworker_cache_wrapper.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fdistributed_runtime%2Fworker_cache_wrapper.h?ref=aea2e3d2827274a95467a810102d231762200d29",
            "patch": "@@ -29,19 +29,19 @@ class WorkerCacheWrapper : public WorkerCacheInterface {\n \n   // Updates *workers with strings naming the remote worker tasks to\n   // which open channels have been established.\n-  void ListWorkers(std::vector<string>* workers) const override {\n+  void ListWorkers(std::vector<std::string>* workers) const override {\n     return wrapped_->ListWorkers(workers);\n   }\n-  void ListWorkersInJob(const string& job_name,\n-                        std::vector<string>* workers) const override {\n+  void ListWorkersInJob(const std::string& job_name,\n+                        std::vector<std::string>* workers) const override {\n     return wrapped_->ListWorkersInJob(job_name, workers);\n   }\n \n   // If \"target\" names a remote task for which an RPC channel exists\n   // or can be constructed, returns a pointer to a WorkerInterface object\n   // wrapping that channel. The returned value must be destroyed by\n   // calling `this->ReleaseWorker(target, ret)`\n-  WorkerInterface* GetOrCreateWorker(const string& target) override {\n+  WorkerInterface* GetOrCreateWorker(const std::string& target) override {\n     return wrapped_->GetOrCreateWorker(target);\n   }\n \n@@ -50,7 +50,8 @@ class WorkerCacheWrapper : public WorkerCacheInterface {\n   // TODO(jeff,sanjay): Consider moving target into WorkerInterface.\n   // TODO(jeff,sanjay): Unify all worker-cache impls and factor out a\n   //                    per-rpc-subsystem WorkerInterface creator.\n-  void ReleaseWorker(const string& target, WorkerInterface* worker) override {\n+  void ReleaseWorker(const std::string& target,\n+                     WorkerInterface* worker) override {\n     return wrapped_->ReleaseWorker(target, worker);\n   }\n \n@@ -69,15 +70,16 @@ class WorkerCacheWrapper : public WorkerCacheInterface {\n   // within its local environment.  Returns true if *locality\n   // was set, using only locally cached data.  Returns false\n   // if status data for that device was not available.  Never blocks.\n-  bool GetDeviceLocalityNonBlocking(const string& device,\n+  bool GetDeviceLocalityNonBlocking(const std::string& device,\n                                     DeviceLocality* locality) override {\n     return wrapped_->GetDeviceLocalityNonBlocking(device, locality);\n   }\n \n   // Set *locality with the DeviceLocality of the specified remote device\n   // within its local environment.  Callback gets Status::OK if *locality\n   // was set.\n-  void GetDeviceLocalityAsync(const string& device, DeviceLocality* locality,\n+  void GetDeviceLocalityAsync(const std::string& device,\n+                              DeviceLocality* locality,\n                               StatusCallback done) override {\n     return wrapped_->GetDeviceLocalityAsync(device, locality, std::move(done));\n   }"
        },
        {
            "sha": "cb66a4f845f5b793da1132b836f6f0a6272b2c4e",
            "filename": "tensorflow/core/distributed_runtime/worker_session.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 11,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Fworker_session.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Fworker_session.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fdistributed_runtime%2Fworker_session.cc?ref=aea2e3d2827274a95467a810102d231762200d29",
            "patch": "@@ -43,16 +43,16 @@ class WorkerFreeListCache : public WorkerCacheInterface {\n     }\n   }\n \n-  void ListWorkers(std::vector<string>* workers) const override {\n+  void ListWorkers(std::vector<std::string>* workers) const override {\n     wrapped_->ListWorkers(workers);\n   }\n \n-  void ListWorkersInJob(const string& job_name,\n-                        std::vector<string>* workers) const override {\n+  void ListWorkersInJob(const std::string& job_name,\n+                        std::vector<std::string>* workers) const override {\n     wrapped_->ListWorkersInJob(job_name, workers);\n   }\n \n-  WorkerInterface* GetOrCreateWorker(const string& target) override {\n+  WorkerInterface* GetOrCreateWorker(const std::string& target) override {\n     {\n       // Fast path if worker has been created.\n       tf_shared_lock l(mu_);\n@@ -88,16 +88,18 @@ class WorkerFreeListCache : public WorkerCacheInterface {\n     return wrapped_->GetCoordinationClientCache(coordination_client_cache);\n   }\n \n-  void ReleaseWorker(const string& target, WorkerInterface* worker) override {\n+  void ReleaseWorker(const std::string& target,\n+                     WorkerInterface* worker) override {\n     // TODO(jeff,sanjay): Should decrement ref-count when we implement eviction.\n   }\n \n-  bool GetDeviceLocalityNonBlocking(const string& device,\n+  bool GetDeviceLocalityNonBlocking(const std::string& device,\n                                     DeviceLocality* locality) override {\n     return wrapped_->GetDeviceLocalityNonBlocking(device, locality);\n   }\n \n-  void GetDeviceLocalityAsync(const string& device, DeviceLocality* locality,\n+  void GetDeviceLocalityAsync(const std::string& device,\n+                              DeviceLocality* locality,\n                               StatusCallback done) override {\n     wrapped_->GetDeviceLocalityAsync(device, locality, done);\n   }\n@@ -121,13 +123,13 @@ class WorkerFreeListCache : public WorkerCacheInterface {\n \n   // TODO(jeff,sanjay): Eviction when the map becomes too big.\n   mutex mu_;\n-  std::unordered_map<string, WorkerState> workers_ TF_GUARDED_BY(mu_);\n+  std::unordered_map<std::string, WorkerState> workers_ TF_GUARDED_BY(mu_);\n };\n \n }  // namespace\n \n WorkerSession::WorkerSession(\n-    const string& session_name, const string& worker_name,\n+    const std::string& session_name, const std::string& worker_name,\n     std::unique_ptr<WorkerCacheInterface> worker_cache,\n     std::unique_ptr<DeviceMgr> device_mgr, std::unique_ptr<GraphMgr> graph_mgr,\n     std::unique_ptr<DynamicDeviceMgr> remote_device_mgr,\n@@ -165,7 +167,7 @@ absl::Status WorkerSession::UpdateWorkerCacheAndDevices(\n \n /* static */\n std::shared_ptr<WorkerSession> WorkerSession::CreateWithBorrowedDeviceMgr(\n-    const string& session_name, const string& worker_name,\n+    const std::string& session_name, const std::string& worker_name,\n     std::unique_ptr<WorkerCacheInterface> worker_cache,\n     DeviceMgr* borrowed_device_mgr, std::unique_ptr<GraphMgr> graph_mgr,\n     std::unique_ptr<DynamicDeviceMgr> remote_device_mgr,\n@@ -177,7 +179,7 @@ std::shared_ptr<WorkerSession> WorkerSession::CreateWithBorrowedDeviceMgr(\n }\n \n WorkerSession::WorkerSession(\n-    const string& session_name, const string& worker_name,\n+    const std::string& session_name, const std::string& worker_name,\n     std::unique_ptr<WorkerCacheInterface> worker_cache,\n     DeviceMgr* borrowed_device_mgr, std::unique_ptr<GraphMgr> graph_mgr,\n     std::unique_ptr<DynamicDeviceMgr> remote_device_mgr,"
        },
        {
            "sha": "5f8d66d93b6c69b895df20c005caeecb7e4197ed",
            "filename": "tensorflow/core/distributed_runtime/worker_session.h",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Fworker_session.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/aea2e3d2827274a95467a810102d231762200d29/tensorflow%2Fcore%2Fdistributed_runtime%2Fworker_session.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fdistributed_runtime%2Fworker_session.h?ref=aea2e3d2827274a95467a810102d231762200d29",
            "patch": "@@ -51,8 +51,8 @@ class WorkerSession {\n \n   DynamicDeviceMgr* remote_device_mgr() { return remote_device_mgr_.get(); }\n \n-  const string& session_name() const { return session_name_; }\n-  const string& worker_name() const { return worker_name_; }\n+  const std::string& session_name() const { return session_name_; }\n+  const std::string& worker_name() const { return worker_name_; }\n \n   WorkerCacheInterface* worker_cache() const {\n     tf_shared_lock l(worker_session_state_mu_);\n@@ -64,15 +64,15 @@ class WorkerSession {\n     return cluster_flr_.get();\n   }\n \n-  WorkerSession(const string& session_name, const string& worker_name,\n+  WorkerSession(const std::string& session_name, const std::string& worker_name,\n                 std::unique_ptr<WorkerCacheInterface> worker_cache,\n                 std::unique_ptr<DeviceMgr> device_mgr,\n                 std::unique_ptr<GraphMgr> graph_mgr,\n                 std::unique_ptr<DynamicDeviceMgr> remote_device_mgr,\n                 DistributedFunctionLibraryRuntimeCreator cluster_flr_creator);\n \n   static std::shared_ptr<WorkerSession> CreateWithBorrowedDeviceMgr(\n-      const string& session_name, const string& worker_name,\n+      const std::string& session_name, const std::string& worker_name,\n       std::unique_ptr<WorkerCacheInterface> worker_cache,\n       DeviceMgr* borrowed_device_mgr, std::unique_ptr<GraphMgr> graph_mgr,\n       std::unique_ptr<DynamicDeviceMgr> remote_device_mgr,\n@@ -98,18 +98,18 @@ class WorkerSession {\n   ~WorkerSession();\n \n  private:\n-  WorkerSession(const string& session_name, const string& worker_name,\n+  WorkerSession(const std::string& session_name, const std::string& worker_name,\n                 std::unique_ptr<WorkerCacheInterface> worker_cache,\n                 DeviceMgr* borrowed_device_mgr,\n                 std::unique_ptr<GraphMgr> graph_mgr,\n                 std::unique_ptr<DynamicDeviceMgr> remote_device_mgr,\n                 DistributedFunctionLibraryRuntimeCreator cluster_flr_creator);\n \n   // The name of the session.\n-  const string session_name_;\n+  const std::string session_name_;\n \n   // The name of the worker. E.g., /job:mnist/replica:0/task:1.\n-  const string worker_name_;\n+  const std::string worker_name_;\n \n   mutable mutex worker_session_state_mu_;\n   // Object from which WorkerInterface instances can be obtained."
        }
    ],
    "stats": {
        "total": 246,
        "additions": 128,
        "deletions": 118
    }
}