{
    "author": "Varcho",
    "message": "[ReplicaGroupV3][Refactor][6/n] Update rest of `spmd/` dir to use CollectiveDeviceListBase in place of vector<vector<int>> and reduce cognitive complexity in `GetDefaultCollectiveOpsCreator`.\n\nPiperOrigin-RevId: 848356290",
    "sha": "93ae7c23eacc5ca0a0820e684d0f37a4b4a41081",
    "files": [
        {
            "sha": "737fbad8e03c08cd5d022bcb2f818f26aa3acb64",
            "filename": "third_party/xla/xla/service/spmd/convolution_handler.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/93ae7c23eacc5ca0a0820e684d0f37a4b4a41081/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fconvolution_handler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/93ae7c23eacc5ca0a0820e684d0f37a4b4a41081/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fconvolution_handler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fconvolution_handler.cc?ref=93ae7c23eacc5ca0a0820e684d0f37a4b4a41081",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/ir/hlo_sharding.h\"\n+#include \"xla/hlo/ir/replica_group.h\"\n #include \"xla/hlo/utils/hlo_sharding_util.h\"\n #include \"xla/literal_util.h\"\n #include \"xla/service/dot_as_convolution_util.h\"\n@@ -512,8 +513,8 @@ PartitionConvolutionWithSpatialDimensionHaloExchangeOnRHS(\n           new_window));\n \n   auto ar = collective_ops_creator.create_cross_partition_all_reduce(\n-      b, conv, MakeBinaryAdd(original_hlo->shape().element_type(), module), {},\n-      (*lhs.state().next_channel_id)++);\n+      b, conv, MakeBinaryAdd(original_hlo->shape().element_type(), module),\n+      CollectiveDeviceList(), (*lhs.state().next_channel_id)++);\n   ar->set_sharding(HloSharding::Replicate());\n   return PartitionedHlo(ar, output_base_shape, lhs.state())\n       .Reshard(output_sharding)\n@@ -739,8 +740,8 @@ PartitionConvolutionWithSpatialDimensionHaloExchangeOnLHS(\n           new_window));\n   auto ar =\n       lhs.state().collective_ops_creator.create_cross_partition_all_reduce(\n-          b, conv, MakeBinaryAdd(output_base_shape.element_type(), module), {},\n-          (*lhs.state().next_channel_id)++);\n+          b, conv, MakeBinaryAdd(output_base_shape.element_type(), module),\n+          CollectiveDeviceList(), (*lhs.state().next_channel_id)++);\n   ar->set_sharding(HloSharding::Replicate());\n   return PartitionedHlo(ar, output_base_shape, lhs.state())\n       .Reshard(output_sharding)"
        },
        {
            "sha": "51bb55ed8c1cfd78d52dda232d820c84d5d1aff1",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner.cc",
            "status": "modified",
            "additions": 124,
            "deletions": 121,
            "changes": 245,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/93ae7c23eacc5ca0a0820e684d0f37a4b4a41081/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/93ae7c23eacc5ca0a0820e684d0f37a4b4a41081/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.cc?ref=93ae7c23eacc5ca0a0820e684d0f37a4b4a41081",
            "patch": "@@ -1610,7 +1610,7 @@ PartitionedHlo PartitionedHlo::Broadcast() const {\n       MakeBinaryAdd(shape.element_type(), state_.module);\n \n   auto result = state_.collective_ops_creator.create_cross_partition_all_reduce(\n-      state_.b, operand, reduction, {}, NewChannel());\n+      state_.b, operand, reduction, CollectiveDeviceList(), NewChannel());\n   result->set_sharding(HloSharding::Replicate());\n   return PartitionedHlo(result, base_shape_, state_);\n }\n@@ -1755,8 +1755,8 @@ PartitionedHlo PartitionedHlo::ReshardWithAllToAll(\n     // After the reshape, it is guaranteed to have at least 3 dimensions.\n     all_to_all =\n         state_.collective_ops_creator.create_cross_partition_all_to_all(\n-            state_.b, {reshape}, groups.flattened_replica_groups(),\n-            (*state_.next_channel_id)++, target_dim);\n+            state_.b, {reshape}, groups, (*state_.next_channel_id)++,\n+            target_dim);\n   }\n   CHECK_NE(all_to_all, nullptr);\n \n@@ -1942,8 +1942,7 @@ PartitionedHlo PartitionedHlo::TryMultipleSourceTargetDims(\n         temp_target, eligible_target_dims, group_sizes);\n     all_to_all =\n         state_.collective_ops_creator.create_cross_partition_all_to_all(\n-            state_.b, {reshape_1}, groups.flattened_replica_groups(),\n-            (*state_.next_channel_id)++, 0);\n+            state_.b, {reshape_1}, groups, (*state_.next_channel_id)++, 0);\n   }\n   // Step 3. Split sharding axes to multiple dimensions\n   // 1. reshape_2 (8,16,8,16,8) -> (2,4,16,8,16,8)\n@@ -4950,113 +4949,121 @@ absl::Status SpmdPartitioningVisitor::HandleRaggedDot(HloInstruction* hlo) {\n   return absl::OkStatus();\n }\n \n-SPMDCollectiveOpsCreator GetDefaultCollectiveOpsCreator(int64_t num_partitions,\n-                                                        int64_t num_replicas) {\n-  auto create_all_reduce_lists_of_lists =\n-      [num_replicas, num_partitions](\n-          SpmdBuilder* b, HloInstruction* operand, HloComputation* reduction,\n-          const std::vector<std::vector<int64_t>>& partition_subgroups,\n-          int64_t channel_id) {\n-        std::vector<ReplicaGroup> device_groups;\n-        if (partition_subgroups.size() <= 1) {\n-          device_groups.reserve(num_replicas);\n-          for (int64_t rid = 0; rid < num_replicas; ++rid) {\n-            device_groups.emplace_back();\n-            for (int64_t pid = 0; pid < num_partitions; ++pid) {\n-              device_groups.back().add_replica_ids(rid * num_partitions + pid);\n-            }\n-          }\n-        } else {\n-          device_groups.reserve(partition_subgroups.size() * num_replicas);\n-          for (int64_t rid = 0; rid < num_replicas; ++rid) {\n-            for (const auto& pgroup : partition_subgroups) {\n-              device_groups.emplace_back();\n-              for (int64_t pid : pgroup) {\n-                device_groups.back().add_replica_ids(rid * num_partitions +\n-                                                     pid);\n-              }\n-            }\n-          }\n-        }\n+HloInstruction* CreateAllReduceListsOfLists(\n+    int64_t num_replicas, int64_t num_partitions, SpmdBuilder* b,\n+    HloInstruction* operand, HloComputation* reduction,\n+    const CollectiveDeviceListBase& device_list, int64_t channel_id) {\n+  const auto& partition_subgroups = device_list.flattened_replica_groups();\n \n-        HloComputation* reduction_clone =\n-            reduction->parent()->AddComputationAndUnifyNamesAndIds(\n-                reduction->Clone(), false);\n-        HloInstruction* all_reduce =\n-            b->AddInstruction(HloInstruction::CreateAllReduce(\n-                operand->shape(), {operand}, reduction_clone,\n-                CollectiveDeviceList(device_groups),\n-                /*constrain_layout=*/false, channel_id,\n-                /*use_global_device_ids=*/true));\n-        return all_reduce;\n-      };\n-  auto create_all_to_all_list_of_lists =\n-      [](SpmdBuilder* b, absl::Span<HloInstruction* const> operands,\n-         const std::vector<std::vector<int64_t>>& partition_subgroups,\n-         int64_t channel_id, std::optional<int64_t> split_dimension) {\n-        std::vector<Shape> shapes(operands.size(), operands[0]->shape());\n-        const Shape output_shape =\n-            (shapes.size() == 1)\n-                ? shapes[0]\n-                : ShapeUtil::MakeValidatedTupleShape(shapes).value();\n-        std::vector<ReplicaGroup> groups(partition_subgroups.size());\n-        for (int64_t i = 0; i < groups.size(); ++i) {\n-          for (int64_t id : partition_subgroups[i]) {\n-            groups[i].add_replica_ids(id);\n-          }\n-        }\n-        return b->AddInstruction(HloInstruction::CreateAllToAll(\n-            output_shape, operands, CollectiveDeviceList(groups),\n-            /*constrain_layout=*/false, channel_id, split_dimension));\n-      };\n-  auto create_all_gather_list_of_lists =\n-      [num_replicas, num_partitions](\n-          SpmdBuilder* b, HloInstruction* operand, const Shape& ag_shape,\n-          const std::vector<std::vector<int64_t>>& partition_subgroups,\n-          int64_t channel_id, int64_t all_gather_dimension) {\n-        std::vector<ReplicaGroup> device_groups;\n-        device_groups.reserve(partition_subgroups.size() * num_replicas);\n-        for (int64_t i = 0; i < num_replicas; ++i) {\n-          for (const auto& pgroup : partition_subgroups) {\n-            device_groups.emplace_back();\n-            for (int64_t pid : pgroup) {\n-              device_groups.back().add_replica_ids(i * num_partitions + pid);\n-            }\n-          }\n-        }\n-        return b->AddInstruction(HloInstruction::CreateAllGather(\n-            ag_shape, {operand}, all_gather_dimension,\n-            CollectiveDeviceList(device_groups),\n-            /*constrain_layout=*/false, channel_id,\n-            /*use_global_device_ids=*/true));\n-      };\n+  std::vector<std::vector<int64_t>> normalized_subgroups = partition_subgroups;\n+  if (normalized_subgroups.size() <= 1) {\n+    normalized_subgroups.assign(1, std::vector<int64_t>(num_partitions));\n+    std::iota(normalized_subgroups[0].begin(), normalized_subgroups[0].end(),\n+              0);\n+  }\n+\n+  auto create_replica_group = [&](int64_t rid,\n+                                  const std::vector<int64_t>& pids) {\n+    ReplicaGroup group;\n+    group.mutable_replica_ids()->Reserve(pids.size());\n+    for (int64_t pid : pids) {\n+      group.add_replica_ids(rid * num_partitions + pid);\n+    }\n+    return group;\n+  };\n+\n+  std::vector<ReplicaGroup> device_groups;\n+  device_groups.reserve(num_replicas * normalized_subgroups.size());\n+  for (int64_t rid = 0; rid < num_replicas; ++rid) {\n+    for (const auto& pgroup : normalized_subgroups) {\n+      device_groups.push_back(create_replica_group(rid, pgroup));\n+    }\n+  }\n \n+  HloComputation* reduction_clone =\n+      reduction->parent()->AddComputationAndUnifyNamesAndIds(reduction->Clone(),\n+                                                             false);\n+  return b->AddInstruction(HloInstruction::CreateAllReduce(\n+      operand->shape(), {operand}, reduction_clone,\n+      CollectiveDeviceList(device_groups),\n+      /*constrain_layout=*/false, channel_id,\n+      /*use_global_device_ids=*/true));\n+}\n+\n+HloInstruction* CreateAllToAllListsOfLists(\n+    int64_t num_replicas, int64_t num_partitions, SpmdBuilder* b,\n+    absl::Span<HloInstruction* const> operands,\n+    const CollectiveDeviceListBase& device_list, int64_t channel_id,\n+    std::optional<int64_t> split_dimension) {\n+  const std::vector<std::vector<int64_t>>& partition_subgroups =\n+      device_list.flattened_replica_groups();\n+  std::vector<Shape> shapes(operands.size(), operands[0]->shape());\n+  const Shape output_shape =\n+      (shapes.size() == 1) ? shapes[0]\n+                           : ShapeUtil::MakeValidatedTupleShape(shapes).value();\n+  std::vector<ReplicaGroup> groups(partition_subgroups.size());\n+  for (int64_t i = 0; i < groups.size(); ++i) {\n+    for (int64_t id : partition_subgroups[i]) {\n+      groups[i].add_replica_ids(id);\n+    }\n+  }\n+  return b->AddInstruction(HloInstruction::CreateAllToAll(\n+      output_shape, operands, CollectiveDeviceList(groups),\n+      /*constrain_layout=*/false, channel_id, split_dimension));\n+}\n+\n+HloInstruction* CreateAllGatherListsOfLists(\n+    int64_t num_replicas, int64_t num_partitions, SpmdBuilder* b,\n+    HloInstruction* operand, const Shape& ag_shape,\n+    const CollectiveDeviceListBase& device_list, int64_t channel_id,\n+    int64_t all_gather_dimension) {\n+  const std::vector<std::vector<int64_t>>& partition_subgroups =\n+      device_list.flattened_replica_groups();\n+  std::vector<ReplicaGroup> device_groups;\n+  device_groups.reserve(partition_subgroups.size() * num_replicas);\n+  for (int64_t i = 0; i < num_replicas; ++i) {\n+    for (const auto& pgroup : partition_subgroups) {\n+      device_groups.emplace_back();\n+      for (int64_t pid : pgroup) {\n+        device_groups.back().add_replica_ids(i * num_partitions + pid);\n+      }\n+    }\n+  }\n+  return b->AddInstruction(\n+      HloInstruction::CreateAllGather(ag_shape, {operand}, all_gather_dimension,\n+                                      CollectiveDeviceList(device_groups),\n+                                      /*constrain_layout=*/false, channel_id,\n+                                      /*use_global_device_ids=*/true));\n+}\n+\n+SPMDCollectiveOpsCreator GetDefaultCollectiveOpsCreator(int64_t num_partitions,\n+                                                        int64_t num_replicas) {\n   SPMDCollectiveOpsCreator result = {\n       .create_partition_id =\n           [](SpmdBuilder* b) {\n             return b->AddInstruction(HloInstruction::CreatePartitionId());\n           },\n       .create_cross_partition_all_reduce =\n-          [create_all_reduce_lists_of_lists](\n+          [num_replicas, num_partitions](\n               SpmdBuilder* b, HloInstruction* operand,\n               HloComputation* reduction,\n-              const std::vector<std::vector<int64_t>>& partition_subgroups,\n-              int64_t channel_id) {\n-            return create_all_reduce_lists_of_lists(\n-                b, operand, reduction, partition_subgroups, channel_id);\n+              const CollectiveDeviceListBase& device_list, int64_t channel_id) {\n+            return CreateAllReduceListsOfLists(num_replicas, num_partitions, b,\n+                                               operand, reduction, device_list,\n+                                               channel_id);\n           },\n       .create_cross_partition_all_reduce_with_iota_device_list =\n-          [create_all_reduce_lists_of_lists, num_replicas, num_partitions](\n+          [num_replicas, num_partitions](\n               SpmdBuilder* b, HloInstruction* operand,\n               HloComputation* reduction,\n               const IotaReplicaGroupList& partition_group_list,\n               int64_t channel_id) {\n             // Fallback to list of lists collective creation if the partition\n             // group list does not utilize all the partitions.\n             if (partition_group_list.num_total_devices() != num_partitions) {\n-              return create_all_reduce_lists_of_lists(\n-                  b, operand, reduction,\n-                  partition_group_list.flattened_replica_groups(), channel_id);\n+              return CreateAllReduceListsOfLists(\n+                  num_replicas, num_partitions, b, operand, reduction,\n+                  partition_group_list, channel_id);\n             }\n             HloComputation* reduction_clone =\n                 reduction->parent()->AddComputationAndUnifyNamesAndIds(\n@@ -5096,24 +5103,25 @@ SPMDCollectiveOpsCreator GetDefaultCollectiveOpsCreator(int64_t num_partitions,\n                 operand->shape(), operand, src_dst_pairs, channel_id));\n           },\n       .create_cross_partition_all_to_all =\n-          [create_all_to_all_list_of_lists](\n+          [num_replicas, num_partitions](\n               SpmdBuilder* b, absl::Span<HloInstruction* const> operands,\n-              const std::vector<std::vector<int64_t>>& partition_subgroups,\n-              int64_t channel_id, std::optional<int64_t> split_dimension) {\n-            return create_all_to_all_list_of_lists(\n-                b, operands, partition_subgroups, channel_id, split_dimension);\n+              const CollectiveDeviceListBase& device_list, int64_t channel_id,\n+              std::optional<int64_t> split_dimension) {\n+            return CreateAllToAllListsOfLists(num_replicas, num_partitions, b,\n+                                              operands, device_list, channel_id,\n+                                              split_dimension);\n           },\n       .create_cross_partition_all_to_all_with_iota_device_list =\n-          [create_all_to_all_list_of_lists, num_replicas, num_partitions](\n+          [num_replicas, num_partitions](\n               SpmdBuilder* b, absl::Span<HloInstruction* const> operands,\n               const IotaReplicaGroupList& partition_group_list,\n               int64_t channel_id, std::optional<int64_t> split_dimension) {\n             // Fallback back to list of lists collective creation if the\n             // partition group list does not utilize all the partitions.\n             if (partition_group_list.num_total_devices() != num_partitions) {\n-              return create_all_to_all_list_of_lists(\n-                  b, operands, partition_group_list.flattened_replica_groups(),\n-                  channel_id, split_dimension);\n+              return CreateAllToAllListsOfLists(num_replicas, num_partitions, b,\n+                                                operands, partition_group_list,\n+                                                channel_id, split_dimension);\n             }\n             std::vector<Shape> shapes(operands.size(), operands[0]->shape());\n             const Shape output_shape = (shapes.size() == 1)\n@@ -5126,26 +5134,25 @@ SPMDCollectiveOpsCreator GetDefaultCollectiveOpsCreator(int64_t num_partitions,\n                 /*constrain_layout=*/false, channel_id, split_dimension));\n           },\n       .create_cross_partition_all_gather =\n-          [create_all_gather_list_of_lists](\n+          [num_replicas, num_partitions](\n               SpmdBuilder* b, HloInstruction* operand, const Shape& ag_shape,\n-              const std::vector<std::vector<int64_t>>& partition_subgroups,\n-              int64_t channel_id, int64_t all_gather_dimension) {\n-            return create_all_gather_list_of_lists(\n-                b, operand, ag_shape, partition_subgroups, channel_id,\n-                all_gather_dimension);\n+              const CollectiveDeviceListBase& device_list, int64_t channel_id,\n+              int64_t all_gather_dimension) {\n+            return CreateAllGatherListsOfLists(\n+                num_replicas, num_partitions, b, operand, ag_shape, device_list,\n+                channel_id, all_gather_dimension);\n           },\n       .create_cross_partition_all_gather_with_iota_device_list =\n-          [create_all_gather_list_of_lists, num_replicas, num_partitions](\n+          [num_replicas, num_partitions](\n               SpmdBuilder* b, HloInstruction* operand, const Shape& ag_shape,\n               const IotaReplicaGroupList& partition_group_list,\n               int64_t channel_id, int64_t all_gather_dimension) {\n             // Fallback to list of lists collective creation if the partition\n             // group list does not utilize all the partitions.\n             if (partition_group_list.num_total_devices() != num_partitions) {\n-              return create_all_gather_list_of_lists(\n-                  b, operand, ag_shape,\n-                  partition_group_list.flattened_replica_groups(), channel_id,\n-                  all_gather_dimension);\n+              return CreateAllGatherListsOfLists(\n+                  num_replicas, num_partitions, b, operand, ag_shape,\n+                  partition_group_list, channel_id, all_gather_dimension);\n             }\n             return b->AddInstruction(HloInstruction::CreateAllGather(\n                 ag_shape, {operand}, all_gather_dimension,\n@@ -5211,9 +5218,7 @@ SpmdPartitioner::AllGatherShardsInternal(\n             *it, result_shape.dimensions(*it) *\n                      partition_subgroups.num_devices_per_group());\n         result = collectives_creator.create_cross_partition_all_gather(\n-            b, result, result_shape,\n-            partition_subgroups.flattened_replica_groups(),\n-            (*next_channel_id)++,\n+            b, result, result_shape, partition_subgroups, (*next_channel_id)++,\n             /*all_gather_dimension=*/*it);\n       }\n     }\n@@ -5252,7 +5257,7 @@ SpmdPartitioner::AllGatherShardsInternal(\n     shape[0] *= partition_subgroups.num_devices_per_group();\n     result = collectives_creator.create_cross_partition_all_gather(\n         b, result, ShapeUtil::MakeShape(operand->shape().element_type(), shape),\n-        partition_subgroups.flattened_replica_groups(), (*next_channel_id)++,\n+        partition_subgroups, (*next_channel_id)++,\n         /*all_gather_dimension=*/0);\n   }\n   ag = result;\n@@ -5341,8 +5346,7 @@ HloInstruction* SpmdPartitioner::AllReduceAlongShardingDimsInternal(\n     auto partition_subgroups =\n         GetPartitionGroupsForReplication(sharding, selected_dims);\n     return collectives_creator.create_cross_partition_all_reduce(\n-        b, operand, reduction, partition_subgroups.flattened_replica_groups(),\n-        (*next_channel_id)++);\n+        b, operand, reduction, partition_subgroups, (*next_channel_id)++);\n   }\n \n   auto result = operand;\n@@ -5365,8 +5369,7 @@ HloInstruction* SpmdPartitioner::AllReduceAlongShardingDimsInternal(\n       auto partition_subgroups =\n           GetPartitionGroupsForReplication(sharding, {*it});\n       result = collectives_creator.create_cross_partition_all_reduce(\n-          b, result, reduction, partition_subgroups.flattened_replica_groups(),\n-          (*next_channel_id)++);\n+          b, result, reduction, partition_subgroups, (*next_channel_id)++);\n     }\n   }\n   return result;"
        },
        {
            "sha": "8d26760a15fd98ff7d6d1dcbe47464b49a87941e",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner.h",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/93ae7c23eacc5ca0a0820e684d0f37a4b4a41081/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/93ae7c23eacc5ca0a0820e684d0f37a4b4a41081/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner.h?ref=93ae7c23eacc5ca0a0820e684d0f37a4b4a41081",
            "patch": "@@ -203,8 +203,7 @@ struct SPMDCollectiveOpsCreator {\n   // Function used to create a cross-partition all-reduce HLO.\n   std::function<HloInstruction*(\n       SpmdBuilder*, HloInstruction* operand, HloComputation* reduction,\n-      const std::vector<std::vector<int64_t>>& partition_subgroups,\n-      int64_t channel_id)>\n+      const CollectiveDeviceListBase& partition_subgroups, int64_t channel_id)>\n       create_cross_partition_all_reduce;\n \n   // Function used to create a cross-partition all-reduce HLO using device list\n@@ -227,8 +226,8 @@ struct SPMDCollectiveOpsCreator {\n   // Function used to create a cross-partition all-to-all HLO.\n   std::function<HloInstruction*(\n       SpmdBuilder*, absl::Span<HloInstruction* const> operands,\n-      const std::vector<std::vector<int64_t>>& partition_subgroups,\n-      int64_t channel_id, std::optional<int64_t> split_dimension)>\n+      const CollectiveDeviceListBase& partition_subgroups, int64_t channel_id,\n+      std::optional<int64_t> split_dimension)>\n       create_cross_partition_all_to_all;\n \n   // Function used to create a cross-partition all-to-all HLO using device list\n@@ -244,8 +243,8 @@ struct SPMDCollectiveOpsCreator {\n   // if it is nullptr, the partitioner will use all-reduce instead.\n   std::function<HloInstruction*(\n       SpmdBuilder*, HloInstruction* operand, const Shape& ag_shape,\n-      const std::vector<std::vector<int64_t>>& partition_subgroups,\n-      int64_t channel_id, int64_t all_gather_dimension)>\n+      const CollectiveDeviceListBase& partition_subgroups, int64_t channel_id,\n+      int64_t all_gather_dimension)>\n       create_cross_partition_all_gather;\n \n   // Function used to create a cross-partition all-gather HLO using device list"
        },
        {
            "sha": "f9ac602d258afc3d1d2cd44e34e7b7857c2e074e",
            "filename": "third_party/xla/xla/service/spmd/spmd_partitioner_util.cc",
            "status": "modified",
            "additions": 12,
            "deletions": 13,
            "changes": 25,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/93ae7c23eacc5ca0a0820e684d0f37a4b4a41081/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/93ae7c23eacc5ca0a0820e684d0f37a4b4a41081/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fspmd%2Fspmd_partitioner_util.cc?ref=93ae7c23eacc5ca0a0820e684d0f37a4b4a41081",
            "patch": "@@ -461,14 +461,16 @@ bool IsIota(const Array<int64_t>& x) {\n \n // Expand the device groups, making each device group follow the format of the\n // partition group.\n-std::vector<std::vector<int64_t>> ExpandDeviceGroups(\n+CollectiveDeviceList ExpandDeviceGroups(\n     const DeviceGroupTileAssignment& device_groups,\n-    const std::vector<std::vector<int64_t>>& partition_subgroups) {\n+    const CollectiveDeviceListBase& collective_device_list) {\n   // Example: Given device groups of {{0,1,2,3},{4,5,6,7}} and partition\n   // subgroups of {{0,2}, {1,3}} returns device groups of {{0,2}, {1,3}, {4,6},\n   // {5,7}}\n+  const std::vector<std::vector<int64_t>>& partition_subgroups =\n+      collective_device_list.flattened_replica_groups();\n   if (partition_subgroups.empty()) {\n-    return device_groups.flattened_device_groups();\n+    return CollectiveDeviceList(device_groups.flattened_device_groups());\n   }\n   std::vector<std::vector<int64_t>> result(partition_subgroups.size() *\n                                            device_groups.num_groups());\n@@ -482,7 +484,7 @@ std::vector<std::vector<int64_t>> ExpandDeviceGroups(\n       }\n     }\n   }\n-  return result;\n+  return CollectiveDeviceList(result);\n }\n \n // Expand the device groups, making each device group follow the format of the\n@@ -552,7 +554,7 @@ CreateCrossPartitionAllReduce(\n     std::shared_ptr<const DeviceGroupTileAssignment> device_groups_ptr) {\n   return [creator, device_groups_ptr](\n              SpmdBuilder* b, HloInstruction* operand, HloComputation* reduction,\n-             const std::vector<std::vector<int64_t>>& partition_subgroups,\n+             const CollectiveDeviceListBase& partition_subgroups,\n              int64_t channel_id) {\n     return creator.create_cross_partition_all_reduce(\n         b, operand, reduction,\n@@ -577,8 +579,7 @@ CreateCrossPartitionAllReduceWithIotaDeviceList(\n     if (!expanded_iota_partition_group_list.has_value()) {\n       return creator.create_cross_partition_all_reduce(\n           b, operand, reduction,\n-          ExpandDeviceGroups(*device_groups_ptr,\n-                             partition_group_list.flattened_replica_groups()),\n+          ExpandDeviceGroups(*device_groups_ptr, partition_group_list),\n           channel_id);\n     }\n     return creator.create_cross_partition_all_reduce_with_iota_device_list(\n@@ -615,7 +616,7 @@ CreateCrossPartitionAllToAll(\n     std::shared_ptr<const DeviceGroupTileAssignment> device_groups_ptr) {\n   return [creator, device_groups_ptr](\n              SpmdBuilder* b, absl::Span<HloInstruction* const> operands,\n-             const std::vector<std::vector<int64_t>>& partition_subgroups,\n+             const CollectiveDeviceListBase& partition_subgroups,\n              int64_t channel_id, std::optional<int64_t> split_dimension) {\n     return creator.create_cross_partition_all_to_all(\n         b, operands,\n@@ -640,8 +641,7 @@ CreateCrossPartitionAllToAllWithIotaDeviceList(\n     if (!expanded_iota_partition_group_list.has_value()) {\n       return creator.create_cross_partition_all_to_all(\n           b, operands,\n-          ExpandDeviceGroups(*device_groups_ptr,\n-                             partition_group_list.flattened_replica_groups()),\n+          ExpandDeviceGroups(*device_groups_ptr, partition_group_list),\n           channel_id, split_dimension);\n     }\n     return creator.create_cross_partition_all_to_all_with_iota_device_list(\n@@ -656,7 +656,7 @@ CreateCrossPartitionAllGather(\n     std::shared_ptr<const DeviceGroupTileAssignment> device_groups_ptr) {\n   return [creator, device_groups_ptr](\n              SpmdBuilder* b, HloInstruction* operand, const Shape& ag_shape,\n-             const std::vector<std::vector<int64_t>>& partition_subgroups,\n+             const CollectiveDeviceListBase& partition_subgroups,\n              int64_t channel_id, int64_t all_gather_dimension) {\n     return creator.create_cross_partition_all_gather(\n         b, operand, ag_shape,\n@@ -682,8 +682,7 @@ CreateCrossPartitionAllGatherWithIotaDeviceList(\n     if (!expanded_iota_partition_group_list.has_value()) {\n       return creator.create_cross_partition_all_gather(\n           b, operand, ag_shape,\n-          ExpandDeviceGroups(*device_groups_ptr,\n-                             partition_group_list.flattened_replica_groups()),\n+          ExpandDeviceGroups(*device_groups_ptr, partition_group_list),\n           channel_id, all_gather_dimension);\n     }\n     return creator.create_cross_partition_all_gather_with_iota_device_list("
        }
    ],
    "stats": {
        "total": 290,
        "additions": 146,
        "deletions": 144
    }
}