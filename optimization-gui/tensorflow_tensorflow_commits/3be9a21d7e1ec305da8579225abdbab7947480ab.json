{
    "author": "alexander-shaposhnikov",
    "message": "Add initial support for offloading dots to YNNPACK.\n\nPiperOrigin-RevId: 823318539",
    "sha": "3be9a21d7e1ec305da8579225abdbab7947480ab",
    "files": [
        {
            "sha": "049d6023e3f6c964da25718d7726994bc2532887",
            "filename": "third_party/xla/xla/backends/cpu/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3be9a21d7e1ec305da8579225abdbab7947480ab/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3be9a21d7e1ec305da8579225abdbab7947480ab/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2FBUILD?ref=3be9a21d7e1ec305da8579225abdbab7947480ab",
            "patch": "@@ -157,9 +157,10 @@ cc_library(\n         \"//xla:shape_util\",\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n-        \"//xla/backends/cpu/runtime/xnnpack:xnn_interop\",\n+        \"//xla/backends/cpu/runtime:dot_lib\",\n         \"//xla/backends/cpu/runtime/ynnpack:ynn_interop\",\n         \"//xla/hlo/ir:hlo\",\n+        \"//xla/stream_executor:device_memory\",\n         \"//xla/tsl/platform:logging\",\n         \"//xla/tsl/platform:statusor\",\n         \"@XNNPACK//ynnpack\","
        },
        {
            "sha": "9c6384eb9cbe6e7cd90a6e31578f94c61bc72190",
            "filename": "third_party/xla/xla/backends/cpu/ynn_emitter.cc",
            "status": "modified",
            "additions": 126,
            "deletions": 3,
            "changes": 129,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3be9a21d7e1ec305da8579225abdbab7947480ab/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3be9a21d7e1ec305da8579225abdbab7947480ab/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_emitter.cc?ref=3be9a21d7e1ec305da8579225abdbab7947480ab",
            "patch": "@@ -15,23 +15,31 @@ limitations under the License.\n \n #include \"xla/backends/cpu/ynn_emitter.h\"\n \n+#include <array>\n #include <cstddef>\n #include <cstdint>\n #include <memory>\n+#include <numeric>\n+#include <utility>\n #include <vector>\n \n #include \"ynnpack/include/ynnpack.h\"\n #include \"absl/container/flat_hash_map.h\"\n #include \"absl/functional/any_invocable.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_format.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/backends/cpu/runtime/dot_lib.h\"\n #include \"xla/backends/cpu/runtime/ynnpack/ynn_interop.h\"\n #include \"xla/backends/cpu/ynn_support.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/literal.h\"\n+#include \"xla/primitive_util.h\"\n #include \"xla/shape.h\"\n+#include \"xla/stream_executor/device_memory.h\"\n #include \"xla/tsl/platform/logging.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n@@ -280,7 +288,113 @@ static absl::StatusOr<YnnSubgraph> EmitYnnSubgraph(\n   return subgraph;\n }\n \n-absl::StatusOr<absl::AnyInvocable<absl::StatusOr<YnnSubgraph>()>>\n+//===----------------------------------------------------------------------===//\n+// Emit YNNPACK subgraph for the given HLO dot instruction.\n+//===----------------------------------------------------------------------===//\n+\n+// TODO(ashaposhnikov): Use DefineBatchMatrixMultiply in EmitYnnSubgraph.\n+static ynn_status DefineBatchMatrixMultiply(ynn_subgraph_t subgraph,\n+                                            uint32_t input1_id,\n+                                            uint32_t input2_id,\n+                                            uint32_t output_id, size_t b_rank,\n+                                            bool transpose_b) {\n+  if (transpose_b) {\n+    uint32_t input2_id_transposed = YNN_INVALID_VALUE_ID;\n+    std::array<int32_t, YNN_MAX_TENSOR_RANK> perm;\n+    std::iota(perm.begin(), perm.end(), 0);\n+    CHECK_LT(b_rank, YNN_MAX_TENSOR_RANK);\n+    std::swap(perm[b_rank - 1], perm[b_rank - 2]);\n+    ynn_status status = ynn_define_static_transpose(\n+        subgraph,\n+        /*num_dims=*/b_rank, perm.data(), input2_id, &input2_id_transposed,\n+        /*flags=*/0);\n+    if (status != ynn_status_success) {\n+      return status;\n+    }\n+    input2_id = input2_id_transposed;\n+  }\n+\n+  return ynn_define_dot(subgraph, /*num_k_dims=*/1, input1_id, input2_id,\n+                        YNN_INVALID_VALUE_ID, &output_id, /*flags=*/0);\n+}\n+\n+static absl::StatusOr<YnnSubgraph> EmitYnnDotSubgraph(\n+    const HloDotInstruction* dot,\n+    std::vector<std::unique_ptr<Literal>>& literals,\n+    absl::Span<const se::DeviceMemoryBase> arguments_buffers,\n+    bool capture_rhs) {\n+  TF_ASSIGN_OR_RETURN(YnnSubgraph subgraph,\n+                      CreateYnnSubgraph([&](ynn_subgraph_t* subgraph) {\n+                        return ynn_create_subgraph(\n+                            /*external_value_ids=*/3,\n+                            /*flags=*/0, subgraph);\n+                      }));\n+\n+  uint32_t lhs_id = 0;\n+  uint32_t rhs_id = 1;\n+  uint32_t out_id = 2;\n+\n+  const HloInstruction* lhs = dot->operand(0);\n+  const HloInstruction* rhs = dot->operand(1);\n+\n+  const Shape& lhs_shape = lhs->shape();\n+  const Shape& rhs_shape = rhs->shape();\n+  const Shape& out_shape = dot->shape();\n+\n+  auto dims = [](absl::Span<const int64_t> dims) -> std::vector<size_t> {\n+    return {dims.begin(), dims.end()};\n+  };\n+\n+  std::vector<size_t> lhs_dims = dims(lhs_shape.dimensions());\n+  std::vector<size_t> rhs_dims = dims(rhs_shape.dimensions());\n+  std::vector<size_t> out_dims = dims(out_shape.dimensions());\n+\n+  PrimitiveType dtype = lhs->shape().element_type();\n+  if (dtype != F32 && dtype != BF16) {\n+    return InvalidArgument(\"Unsupported input data type for YnnDotThunk: %s\",\n+                           primitive_util::LowercasePrimitiveTypeName(dtype));\n+  }\n+\n+  ynn_type input_type = (dtype == F32) ? ynn_type_fp32 : ynn_type_bf16;\n+  ynn_type output_type = ynn_type_fp32;\n+\n+  const uint32_t input_tensor_flags = YNN_VALUE_FLAG_EXTERNAL_INPUT;\n+  YNN_RETURN_IF_ERROR(ynn_define_tensor_value(\n+      subgraph.get(), input_type, lhs_dims.size(), lhs_dims.data(),\n+      /*data=*/nullptr,\n+      /*zero_point_id=*/YNN_INVALID_VALUE_ID,\n+      /*scale_id=*/YNN_INVALID_VALUE_ID, input_tensor_flags, &lhs_id));\n+\n+  YNN_RETURN_IF_ERROR(ynn_define_tensor_value(\n+      subgraph.get(), input_type, rhs_dims.size(), rhs_dims.data(),\n+      capture_rhs ? arguments_buffers[1].opaque() : nullptr,\n+      /*zero_point_id=*/YNN_INVALID_VALUE_ID,\n+      /*scale_id=*/YNN_INVALID_VALUE_ID, input_tensor_flags, &rhs_id));\n+\n+  const uint32_t output_tensor_flags = YNN_VALUE_FLAG_EXTERNAL_OUTPUT;\n+  YNN_RETURN_IF_ERROR(ynn_define_tensor_value(\n+      subgraph.get(), output_type, out_dims.size(), out_dims.data(),\n+      /*data=*/nullptr,\n+      /*zero_point_id=*/YNN_INVALID_VALUE_ID,\n+      /*scale_id=*/YNN_INVALID_VALUE_ID, output_tensor_flags, &out_id));\n+\n+  DotDimensionNumbers dot_dimensions = dot->dot_dimension_numbers();\n+  TF_ASSIGN_OR_RETURN(DotShape dot_shape, GetDotShape(dot_dimensions, lhs_shape,\n+                                                      rhs_shape, out_shape));\n+\n+  TF_ASSIGN_OR_RETURN(DotCanonicalDims dot_canonical_dims,\n+                      GetDotCanonicalDims(dot_dimensions, dot_shape));\n+\n+  const size_t b_rank = rhs_shape.dimensions_size();\n+  const bool transpose_b = !dot_canonical_dims.rhs_canonical;\n+  YNN_RETURN_IF_ERROR(DefineBatchMatrixMultiply(subgraph.get(), lhs_id, rhs_id,\n+                                                out_id, b_rank, transpose_b));\n+\n+  return subgraph;\n+}\n+\n+absl::StatusOr<absl::AnyInvocable<absl::StatusOr<YnnSubgraph>(\n+    absl::Span<const se::DeviceMemoryBase> arguments_buffers)>>\n EmitYnnFusionBuilder(const HloComputation* computation) {\n   // We do not support non-array parameters for YNNPACK operations.\n   for (auto& param : computation->parameter_instructions()) {\n@@ -297,10 +411,19 @@ EmitYnnFusionBuilder(const HloComputation* computation) {\n                            computation->root_instruction()->shape().ToString());\n   }\n \n-  return [computation,\n-          literals = std::vector<std::unique_ptr<Literal>>()]() mutable {\n+  return [computation, literals = std::vector<std::unique_ptr<Literal>>()](\n+             absl::Span<const se::DeviceMemoryBase> arguments_buffers) mutable {\n     return EmitYnnSubgraph(computation, literals);\n   };\n }\n \n+absl::StatusOr<absl::AnyInvocable<absl::StatusOr<YnnSubgraph>(\n+    absl::Span<const se::DeviceMemoryBase> arguments_buffers)>>\n+EmitYnnDotBuilder(const HloDotInstruction* dot, bool capture_rhs) {\n+  return [dot, capture_rhs, literals = std::vector<std::unique_ptr<Literal>>()](\n+             absl::Span<const se::DeviceMemoryBase> arguments_buffers) mutable {\n+    return EmitYnnDotSubgraph(dot, literals, arguments_buffers, capture_rhs);\n+  };\n+}\n+\n }  // namespace xla::cpu"
        },
        {
            "sha": "280216a9a68a0d24763d36c24ce24d1f19466492",
            "filename": "third_party/xla/xla/backends/cpu/ynn_emitter.h",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3be9a21d7e1ec305da8579225abdbab7947480ab/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_emitter.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3be9a21d7e1ec305da8579225abdbab7947480ab/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_emitter.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fcpu%2Fynn_emitter.h?ref=3be9a21d7e1ec305da8579225abdbab7947480ab",
            "patch": "@@ -20,12 +20,19 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"xla/backends/cpu/runtime/ynnpack/ynn_interop.h\"\n #include \"xla/hlo/ir/hlo_computation.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/stream_executor/device_memory.h\"\n \n namespace xla::cpu {\n \n-absl::StatusOr<absl::AnyInvocable<absl::StatusOr<YnnSubgraph>()>>\n+absl::StatusOr<absl::AnyInvocable<absl::StatusOr<YnnSubgraph>(\n+    absl::Span<const se::DeviceMemoryBase> arguments_buffers)>>\n EmitYnnFusionBuilder(const HloComputation* computation);\n \n+absl::StatusOr<absl::AnyInvocable<absl::StatusOr<YnnSubgraph>(\n+    absl::Span<const se::DeviceMemoryBase> arguments_buffers)>>\n+EmitYnnDotBuilder(const HloDotInstruction* dot, bool capture_rhs);\n+\n }  // namespace xla::cpu\n \n #endif  // XLA_BACKENDS_CPU_YNN_EMITTER_H_"
        },
        {
            "sha": "7580b239321a8785f6fec8cd48f90de8971ba412",
            "filename": "third_party/xla/xla/debug_options_flags.cc",
            "status": "modified",
            "additions": 16,
            "deletions": 1,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3be9a21d7e1ec305da8579225abdbab7947480ab/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3be9a21d7e1ec305da8579225abdbab7947480ab/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fdebug_options_flags.cc?ref=3be9a21d7e1ec305da8579225abdbab7947480ab",
            "patch": "@@ -1096,14 +1096,29 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,\n           &DebugOptions::LibraryFusionType_Parse,\n           debug_options->mutable_xla_cpu_experimental_xnn_fusion_type()),\n       \"\",\n-      \"Comma-separated list of XNN fusion types to be enabled.; \"\n+      \"Comma-separated list of XNN fusion types to be enabled; \"\n       \"no whitespace around commas. Two ways to pass values:\\n\"\n       \"  1. Exact type names. This overwrites the default setting.\\n\"\n       \"  2. '+' or '-' prefix: This adds or removes a fusion type \"\n       \"from the default list. Cannot be mixed with the overwrite \"\n       \"mode. Every item must have the sign prefix.\\n\"\n       \"Available fusion types: dot, eltwise, and reduce.\\n\"\n       \"The default list is currently empty.\"));\n+  flag_list->push_back(tsl::Flag(\n+      \"xla_cpu_experimental_ynn_fusion_type\",\n+      SetterForRepeatedEnum<DebugOptions::LibraryFusionType>(\n+          \"xla_cpu_experimental_ynn_fusion_type\",\n+          /*enum_prefix=*/\"LIBRARY_FUSION_TYPE_\",\n+          &DebugOptions::LibraryFusionType_Parse,\n+          debug_options->mutable_xla_cpu_experimental_ynn_fusion_type()),\n+      \"\",\n+      \"Comma-separated list of YNN fusion types to be enabled; \"\n+      \"no whitespace around commas. Two ways to pass values:\\n\"\n+      \"  1. Exact type names. This overwrites the default setting.\\n\"\n+      \"  2. '+' or '-' prefix: This adds or removes a fusion type \"\n+      \"from the default list. Cannot be mixed with the overwrite \"\n+      \"mode. Every item must have the sign prefix.\\n\"\n+      \"The default list is currently empty.\"));\n   flag_list->push_back(tsl::Flag(\n       \"xla_cpu_experimental_xnn_graph_fusion_mode\",\n       setter_for_xla_cpu_experimental_xnn_graph_fusion_mode,"
        },
        {
            "sha": "93340d7b8ed9dc5c713f9b7ad18eacd420352fb4",
            "filename": "third_party/xla/xla/service/cpu/BUILD",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3be9a21d7e1ec305da8579225abdbab7947480ab/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3be9a21d7e1ec305da8579225abdbab7947480ab/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2FBUILD?ref=3be9a21d7e1ec305da8579225abdbab7947480ab",
            "patch": "@@ -988,12 +988,14 @@ cc_library(\n         \"//xla/service:pattern_matcher\",\n         \"//xla/service/gpu/model/experimental:symbolic_expr\",\n         \"//xla/service/llvm_ir:llvm_util\",\n+        \"//xla/stream_executor:device_memory\",\n         \"//xla/tsl/platform:env\",\n         \"//xla/tsl/platform:errors\",\n         \"//xla/tsl/platform:logging\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/functional:any_invocable\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n@@ -1010,6 +1012,7 @@ cc_library(\n     ]) + if_ynnpack([\n         \"//xla/backends/cpu:ynn_emitter\",\n         \"//xla/backends/cpu:ynn_support\",\n+        \"//xla/backends/cpu/runtime/ynnpack:ynn_interop\",\n         \"//xla/backends/cpu/runtime/ynnpack:ynn_fusion_thunk\",\n     ]),\n )"
        },
        {
            "sha": "b0fc20ada71211d937e88be2b1bb726577801d41",
            "filename": "third_party/xla/xla/service/cpu/thunk_emitter.cc",
            "status": "modified",
            "additions": 48,
            "deletions": 7,
            "changes": 55,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3be9a21d7e1ec305da8579225abdbab7947480ab/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3be9a21d7e1ec305da8579225abdbab7947480ab/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fcpu%2Fthunk_emitter.cc?ref=3be9a21d7e1ec305da8579225abdbab7947480ab",
            "patch": "@@ -24,6 +24,7 @@ limitations under the License.\n #include <vector>\n \n #include \"absl/algorithm/container.h\"\n+#include \"absl/functional/any_invocable.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/match.h\"\n@@ -107,6 +108,7 @@ limitations under the License.\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/status_macros.h\"\n+#include \"xla/stream_executor/device_memory.h\"\n #include \"xla/tsl/platform/errors.h\"\n #include \"xla/tsl/platform/logging.h\"\n #include \"xla/tsl/platform/statusor.h\"\n@@ -127,6 +129,7 @@ limitations under the License.\n \n #ifdef XLA_YNNPACK\n #include \"xla/backends/cpu/runtime/ynnpack/ynn_fusion_thunk.h\"\n+#include \"xla/backends/cpu/runtime/ynnpack/ynn_interop.h\"\n #include \"xla/backends/cpu/ynn_emitter.h\"\n #include \"xla/backends/cpu/ynn_support.h\"\n #endif  // XLA_YNNPACK\n@@ -1082,6 +1085,25 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitDotThunk(\n       TF_ASSIGN_OR_RETURN(BufferAllocation::Slice out_slice,\n                           GetAllocationSlice(instruction));\n \n+#ifdef XLA_YNNPACK\n+      const bool use_ynn = absl::c_linear_search(\n+          hlo_module_config_.debug_options()\n+              .xla_cpu_experimental_ynn_fusion_type(),\n+          DebugOptions::LIBRARY_FUSION_TYPE_INDIVIDUAL_DOT);\n+      if (use_ynn) {\n+        // TODO(ashaposhnikov): Replace IsDotSupportedByXnn with\n+        // IsDotSupportedByYnn.\n+        TF_ASSIGN_OR_RETURN(\n+            auto is_dot_supported,\n+            IsDotSupportedByXnn(dnums, lhs->shape(), rhs->shape(),\n+                                instruction->shape(), &target_machine_features_,\n+                                /*use_cost_model=*/false));\n+        if (is_dot_supported) {\n+          return EmitYnnFusionThunk(instruction);\n+        }\n+      }\n+#endif  // XLA_YNNPACK\n+\n       // Decide whether to use XNNPACK or Eigen.\n       bool use_xnn = hlo_module_config_.debug_options().xla_cpu_use_xnnpack();\n       if (use_xnn) {\n@@ -1508,8 +1530,6 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitXnnFusionThunk(\n absl::StatusOr<ThunkSequence> ThunkEmitter::EmitYnnFusionThunk(\n     const HloInstruction* instruction) {\n #ifdef XLA_YNNPACK\n-  auto* fusion = Cast<HloFusionInstruction>(instruction);\n-\n   // Collect YNNPACK fusion arguments.\n   std::vector<YnnFusionThunk::Argument> arguments;\n   for (HloInstruction* operand : instruction->operands()) {\n@@ -1530,15 +1550,36 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitYnnFusionThunk(\n     results.push_back(YnnFusionThunk::Result{slice, indexed.shape});\n   }\n \n-  const HloComputation* computation = fusion->fused_instructions_computation();\n-\n-  // Construct YNNPACK subgraph builder from the fusion computation.\n-  TF_ASSIGN_OR_RETURN(auto builder, EmitYnnFusionBuilder(computation));\n+  absl::AnyInvocable<absl::StatusOr<YnnSubgraph>(\n+      absl::Span<const se::DeviceMemoryBase> arguments_buffers)>\n+      builder;\n+  absl::Span<const int64_t> captured_arguments_ids;\n+  if (instruction->opcode() == HloOpcode::kDot) {\n+    const HloDotInstruction* dot = Cast<HloDotInstruction>(instruction);\n+    // TODO(ashaposhnikov): Revisit this if we ever get a reliable way\n+    // to determine that RHS is constant.\n+    bool capture_rhs = false;\n+    // Construct YNNPACK subgraph builder from the dot instruction.\n+    TF_ASSIGN_OR_RETURN(builder, EmitYnnDotBuilder(dot, capture_rhs));\n+    static constexpr int64_t kCapturedIds[1] = {1};\n+    if (capture_rhs) {\n+      captured_arguments_ids = kCapturedIds;\n+    }\n+  } else {\n+    auto* fusion = Cast<HloFusionInstruction>(instruction);\n+    const HloComputation* computation =\n+        fusion->fused_instructions_computation();\n+    // Construct YNNPACK subgraph builder from the fusion computation.\n+    TF_ASSIGN_OR_RETURN(builder, EmitYnnFusionBuilder(computation));\n+  }\n \n   return ThunkSequence::Of<YnnFusionThunk>(\n       YnnFusionThunk::Options{}, ThunkInfo(instruction), std::move(arguments),\n       std::move(results),\n-      [b = std::move(builder)](auto, auto) mutable { return b(); });\n+      [b = std::move(builder)](auto, auto, auto arg_buffers) mutable {\n+        return b(arg_buffers);\n+      },\n+      captured_arguments_ids);\n #else\n   return Unimplemented(\"XLA is not built with YNNPACK.\");\n #endif  // XLA_YNNPACK"
        },
        {
            "sha": "0f22f117553484de1c72bc92db6a52a035dc8f0c",
            "filename": "third_party/xla/xla/xla.proto",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3be9a21d7e1ec305da8579225abdbab7947480ab/third_party%2Fxla%2Fxla%2Fxla.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3be9a21d7e1ec305da8579225abdbab7947480ab/third_party%2Fxla%2Fxla%2Fxla.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fxla.proto?ref=3be9a21d7e1ec305da8579225abdbab7947480ab",
            "patch": "@@ -155,6 +155,7 @@ message DebugOptions {\n     LIBRARY_FUSION_TYPE_DOT = 1;  // Dot and any eltwise ops around it.\n     LIBRARY_FUSION_TYPE_ELTWISE = 2;\n     LIBRARY_FUSION_TYPE_REDUCE = 3;\n+    LIBRARY_FUSION_TYPE_INDIVIDUAL_DOT = 4;\n   }\n \n   enum XnnGraphFusionMode {\n@@ -205,15 +206,19 @@ message DebugOptions {\n   // Call oneDNN custom call thunks in the CPU backend\n   optional bool xla_cpu_experimental_onednn_custom_call = 412;\n \n-  // Stores the fusion types enabled for oneDNN in DotLibraryRewriter pass.\n+  // Stores the fusion types enabled for oneDNN in LibraryRewriter pass.\n   repeated LibraryFusionType xla_cpu_experimental_onednn_fusion_type = 399;\n \n-  // Stores the fusion types enabled for XNNPACK in DotLibraryRewriter pass.\n+  // Stores the fusion types enabled for XNNPACK in LibraryRewriter pass.\n   repeated LibraryFusionType xla_cpu_experimental_xnn_fusion_type = 400;\n \n   // Controls XnnGraphFusion HLO pass.\n   optional XnnGraphFusionMode xla_cpu_experimental_xnn_graph_fusion_mode = 365;\n \n+  // Stores the fusion types enabled for YNNPACK in LibraryRewriter pass or\n+  // for individual operations.\n+  repeated LibraryFusionType xla_cpu_experimental_ynn_fusion_type = 422;\n+\n   // When xla_cpu_enable_fast_math is true then this controls whether we forbid\n   // to use the reciprocal of an argument instead of division. Ignored when\n   // xla_cpu_enable_fast_math is false.\n@@ -1361,7 +1366,7 @@ message DebugOptions {\n   // Note: when adding a new flag, please add it to one of the hardware-specific\n   // or hardware-agnostic sections at the top of this proto message.\n \n-  // Next id: 422\n+  // Next id: 423\n \n   // Extra options to pass to the compilation backend (e.g. LLVM); specific\n   // interpretation of these values is left to the backend."
        }
    ],
    "stats": {
        "total": 227,
        "additions": 211,
        "deletions": 16
    }
}