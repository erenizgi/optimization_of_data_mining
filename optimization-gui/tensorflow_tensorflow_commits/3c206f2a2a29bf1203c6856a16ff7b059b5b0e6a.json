{
    "author": "mwhittaker",
    "message": "Added buildable copy of coordination service.\n\nThis is part of an effort to make a copy of the coordination service used\nexclusively by JAX.\n\nPiperOrigin-RevId: 839829814",
    "sha": "3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a",
    "files": [
        {
            "sha": "344d56d9c7c18b9f9ec9dfb26e245f9cac40b1ef",
            "filename": "third_party/xla/xla/pjrt/distributed/coordination/BUILD",
            "status": "added",
            "additions": 356,
            "deletions": 0,
            "changes": 356,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2FBUILD?ref=3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a",
            "patch": "@@ -0,0 +1,356 @@\n+load(\"//xla:xla.default.bzl\", \"xla_cc_test\")\n+load(\"//xla/tsl:tsl.bzl\", \"if_oss\", \"internal_visibility\")\n+load(\"//xla/tsl/platform:build_config.bzl\", \"tf_proto_library\")\n+load(\"//xla/tsl/platform:rules_cc.bzl\", \"cc_library\")\n+\n+package(\n+    # copybara:uncomment default_applicable_licenses = [\"//tensorflow:license\"],\n+    default_visibility = internal_visibility([\n+        \"//xla/tsl:internal\",\n+    ]),\n+    licenses = [\"notice\"],\n+)\n+\n+cc_library(\n+    name = \"coordination_service_error_util\",\n+    srcs = [\"coordination_service_error_util.cc\"],\n+    hdrs = [\"coordination_service_error_util.h\"],\n+    deps = [\n+        \"//xla/tsl/protobuf:coordination_service_proto_cc\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:cord\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@local_tsl//tsl/platform:regexp\",\n+    ],\n+)\n+\n+xla_cc_test(\n+    name = \"coordination_service_error_util_test\",\n+    srcs = [\"coordination_service_error_util_test.cc\"],\n+    deps = [\n+        \":coordination_service_error_util\",\n+        \"//xla/tsl/platform:test\",\n+        \"//xla/tsl/protobuf:coordination_service_proto_cc\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n+cc_library(\n+    name = \"coordination_client\",\n+    hdrs = [\"coordination_client.h\"],\n+    deps = [\n+        \"//xla/tsl/distributed_runtime:call_options\",\n+        \"//xla/tsl/platform:status\",\n+        \"//xla/tsl/protobuf:coordination_service_proto_cc\",\n+    ],\n+)\n+\n+cc_library(\n+    name = \"coordination_service\",\n+    srcs = [\"coordination_service.cc\"],\n+    hdrs = [\"coordination_service.h\"],\n+    deps = [\n+        \":coordination_client\",\n+        \":coordination_service_error_util\",\n+        \":key_value_store\",\n+        \"//xla/tsl/distributed_runtime:call_options\",\n+        \"//xla/tsl/lib/gtl:int_type\",\n+        \"//xla/tsl/platform:env\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:status\",\n+        \"//xla/tsl/protobuf:coordination_config_proto_cc\",\n+        \"//xla/tsl/protobuf:coordination_service_proto_cc\",\n+        \"//xla/tsl/util:device_name_utils\",\n+        \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/base:core_headers\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/container:flat_hash_set\",\n+        \"@com_google_absl//absl/functional:any_invocable\",\n+        \"@com_google_absl//absl/hash\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:str_format\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_absl//absl/synchronization\",\n+        \"@com_google_absl//absl/time\",\n+        \"@com_google_absl//absl/types:span\",\n+        \"@local_tsl//tsl/platform:random\",\n+    ],\n+)\n+\n+tf_proto_library(\n+    name = \"test_device_proto\",\n+    testonly = 1,\n+    srcs = [\"test_device.proto\"],\n+)\n+\n+xla_cc_test(\n+    name = \"coordination_service_test\",\n+    srcs = [\"coordination_service_test.cc\"],\n+    tags = if_oss([\n+        \"manual\",\n+        \"no_oss\",\n+    ]),  # b/169705709, no protobuf matchers in OSS.\n+    deps = [\n+        \":coordination_client\",\n+        \":coordination_service\",\n+        \":coordination_service_error_util\",\n+        \":test_device_proto_cc\",\n+        \"//xla/tsl/distributed_runtime:call_options\",\n+        \"//xla/tsl/lib/core:status_test_util\",\n+        \"//xla/tsl/platform:env\",\n+        \"//xla/tsl/platform:status\",\n+        \"//xla/tsl/platform:test\",\n+        \"//xla/tsl/platform:types\",\n+        \"//xla/tsl/protobuf:coordination_config_proto_cc\",\n+        \"//xla/tsl/protobuf:coordination_service_proto_cc\",\n+        \"//xla/tsl/util/proto:proto_matchers\",\n+        \"@com_google_absl//absl/base:core_headers\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_absl//absl/synchronization\",\n+        \"@com_google_absl//absl/time\",\n+        \"@com_google_googletest//:gtest_main\",\n+        \"@local_tsl//tsl/platform:random\",\n+    ],\n+)\n+\n+cc_library(\n+    name = \"coordination_service_agent\",\n+    srcs = [\"coordination_service_agent.cc\"],\n+    hdrs = [\"coordination_service_agent.h\"],\n+    deps = [\n+        \":coordination_client\",\n+        \":coordination_service\",\n+        \":coordination_service_error_util\",\n+        \"//xla/tsl/distributed_runtime:call_options\",\n+        \"//xla/tsl/framework:cancellation\",\n+        \"//xla/tsl/lib/monitoring:gauge\",\n+        \"//xla/tsl/platform:env\",\n+        \"//xla/tsl/platform:status\",\n+        \"//xla/tsl/protobuf:coordination_config_proto_cc\",\n+        \"//xla/tsl/protobuf:coordination_service_proto_cc\",\n+        \"@com_google_absl//absl/base:core_headers\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/container:flat_hash_set\",\n+        \"@com_google_absl//absl/functional:any_invocable\",\n+        \"@com_google_absl//absl/functional:bind_front\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:str_format\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_absl//absl/synchronization\",\n+        \"@com_google_absl//absl/time\",\n+        \"@com_google_absl//absl/types:span\",\n+        \"@local_tsl//tsl/platform:random\",\n+    ],\n+)\n+\n+xla_cc_test(\n+    name = \"coordination_service_agent_test\",\n+    srcs = [\"coordination_service_agent_test.cc\"],\n+    deps = [\n+        \":coordination_client\",\n+        \":coordination_service_agent\",\n+        \":coordination_service_error_util\",\n+        \"//xla/tsl/distributed_runtime:call_options\",\n+        \"//xla/tsl/lib/core:status_test_util\",\n+        \"//xla/tsl/platform:env\",\n+        \"//xla/tsl/platform:status\",\n+        \"//xla/tsl/platform:test\",\n+        \"//xla/tsl/protobuf:coordination_config_proto_cc_impl\",\n+        \"//xla/tsl/protobuf:coordination_service_proto_cc_impl\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/log:check\",\n+        \"@com_google_absl//absl/memory\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_absl//absl/time\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n+cc_library(\n+    name = \"coordination_service_rpc_handler\",\n+    srcs = [\"coordination_service_rpc_handler.cc\"],\n+    hdrs = [\n+        \"coordination_service_rpc_handler.h\",\n+    ],\n+    deps = [\n+        \":coordination_service\",\n+        \":coordination_service_agent\",\n+        \":coordination_service_error_util\",\n+        \"//xla/tsl/platform:status\",\n+        \"//xla/tsl/protobuf:coordination_service_proto_cc\",\n+        \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_absl//absl/synchronization\",\n+        \"@com_google_absl//absl/time\",\n+        \"@local_tsl//tsl/platform:protobuf\",\n+        \"@local_tsl//tsl/platform:thread_annotations\",\n+    ],\n+)\n+\n+cc_library(\n+    name = \"key_value_store\",\n+    srcs = [\"key_value_store.cc\"],\n+    hdrs = [\"key_value_store.h\"],\n+    deps = [\n+        \"//xla/tsl/protobuf:coordination_service_proto_cc\",\n+        \"@com_google_absl//absl/base:core_headers\",\n+        \"@com_google_absl//absl/container:btree\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/functional:any_invocable\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:str_format\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_absl//absl/synchronization\",\n+    ],\n+)\n+\n+xla_cc_test(\n+    name = \"key_value_store_test\",\n+    srcs = [\"key_value_store_test.cc\"],\n+    tags = if_oss([\n+        \"manual\",\n+        \"no_oss\",\n+    ]),  # no status matchers in OSS.\n+    deps = [\n+        \":key_value_store\",\n+        \"//xla/tsl/platform:test\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_absl//absl/types:span\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n+xla_cc_test(\n+    name = \"coordination_service_recoverable_job_test\",\n+    srcs = [\"coordination_service_recoverable_job_test.cc\"],\n+    deps = [\n+        \":coordination_client\",\n+        \":coordination_service\",\n+        \":coordination_service_agent\",\n+        \":grpc_coordination_client\",\n+        \":grpc_coordination_service_impl\",\n+        \"//xla/tsl/distributed_runtime/rpc:async_service_interface\",\n+        \"//xla/tsl/lib/core:status_test_util\",\n+        \"//xla/tsl/platform:env\",\n+        \"//xla/tsl/platform:status\",\n+        \"//xla/tsl/platform:test\",\n+        \"//xla/tsl/protobuf:coordination_config_proto_cc_impl\",\n+        \"@com_github_grpc_grpc//:grpc++\",\n+        \"@com_google_absl//absl/base:core_headers\",\n+        \"@com_google_absl//absl/container:flat_hash_map\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/memory\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/synchronization\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n+xla_cc_test(\n+    name = \"client_server_test\",\n+    size = \"medium\",\n+    srcs = [\"client_server_test.cc\"],\n+    shard_count = 4,\n+    tags = if_oss([\"not_run:arm\"]),\n+    deps = [\n+        \":coordination_client\",\n+        \":coordination_service\",\n+        \":coordination_service_agent\",\n+        \":grpc_coordination_client\",\n+        \":grpc_coordination_service_impl\",\n+        \"//xla/tsl/distributed_runtime/rpc:async_service_interface\",\n+        \"//xla/tsl/lib/core:status_test_util\",\n+        \"//xla/tsl/platform:env\",\n+        \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:status\",\n+        \"//xla/tsl/platform:test\",\n+        \"//xla/tsl/protobuf:coordination_config_proto_cc_impl\",\n+        \"//xla/tsl/protobuf:coordination_service_proto_cc_impl\",\n+        \"@com_github_grpc_grpc//:grpc++\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/status:status_matchers\",\n+        \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_absl//absl/synchronization\",\n+        \"@com_google_absl//absl/time\",\n+        \"@com_google_googletest//:gtest_main\",\n+    ],\n+)\n+\n+filegroup(\n+    name = \"pywrap_required_hdrs\",\n+    srcs = [\n+        \"coordination_client.h\",\n+        \"coordination_service.h\",\n+        \"key_value_store.h\",\n+    ],\n+)\n+\n+cc_library(\n+    name = \"grpc_coordination_client\",\n+    srcs = [\"grpc_coordination_client.cc\"],\n+    hdrs = [\"grpc_coordination_client.h\"],\n+    deps = [\n+        \":coordination_client\",\n+        \"//xla/tsl/distributed_runtime:call_options\",\n+        \"//xla/tsl/distributed_runtime/rpc:grpc_channel\",\n+        \"//xla/tsl/distributed_runtime/rpc:grpc_client_cq_tag\",\n+        \"//xla/tsl/distributed_runtime/rpc:grpc_state\",\n+        \"//xla/tsl/distributed_runtime/rpc:grpc_util\",\n+        \"//xla/tsl/platform:env\",\n+        \"//xla/tsl/platform:status\",\n+        \"//xla/tsl/protobuf:coordination_service_proto_cc\",\n+        \"@com_github_grpc_grpc//:grpc++\",\n+        \"@com_google_absl//absl/base:core_headers\",\n+        \"@com_google_absl//absl/log\",\n+        \"@com_google_absl//absl/synchronization\",\n+        \"@local_tsl//tsl/platform:protobuf\",\n+    ],\n+)\n+\n+cc_library(\n+    name = \"grpc_coordination_service_impl\",\n+    srcs = [\"grpc_coordination_service_impl.cc\"],\n+    hdrs = [\"grpc_coordination_service_impl.h\"],\n+    deps = [\n+        \":coordination_service\",\n+        \":coordination_service_agent\",\n+        \":coordination_service_rpc_handler\",\n+        \"//xla/tsl/distributed_runtime/rpc:async_service_interface\",\n+        \"//xla/tsl/distributed_runtime/rpc:grpc_call\",\n+        \"//xla/tsl/distributed_runtime/rpc:grpc_util\",\n+        \"//xla/tsl/platform:env\",\n+        \"//xla/tsl/protobuf:coordination_service_cc_grpc_proto\",\n+        \"//xla/tsl/protobuf:coordination_service_proto_cc\",\n+        \"@com_github_grpc_grpc//:grpc++\",\n+        \"@com_google_absl//absl/base:core_headers\",\n+        \"@com_google_absl//absl/status\",\n+        \"@com_google_absl//absl/synchronization\",\n+    ],\n+)"
        },
        {
            "sha": "2cc16bd7d9bf7694358dab748843206f02e40e54",
            "filename": "third_party/xla/xla/pjrt/distributed/coordination/client_server_test.cc",
            "status": "added",
            "additions": 1633,
            "deletions": 0,
            "changes": 1633,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fclient_server_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fclient_server_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fclient_server_test.cc?ref=3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a",
            "patch": "@@ -0,0 +1,1633 @@\n+/* Copyright 2020 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <cassert>\n+#include <functional>\n+#include <memory>\n+#include <string>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/log/log.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/status_matchers.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/synchronization/barrier.h\"\n+#include \"absl/synchronization/mutex.h\"\n+#include \"absl/synchronization/notification.h\"\n+#include \"absl/time/clock.h\"\n+#include \"absl/time/time.h\"\n+#include \"grpcpp/channel.h\"\n+#include \"grpcpp/create_channel.h\"\n+#include \"grpcpp/security/credentials.h\"\n+#include \"grpcpp/security/server_credentials.h\"\n+#include \"grpcpp/server.h\"\n+#include \"grpcpp/server_builder.h\"\n+#include \"xla/pjrt/distributed/coordination/coordination_client.h\"\n+#include \"xla/pjrt/distributed/coordination/coordination_service.h\"\n+#include \"xla/pjrt/distributed/coordination/coordination_service_agent.h\"\n+#include \"xla/pjrt/distributed/coordination/grpc_coordination_client.h\"\n+#include \"xla/pjrt/distributed/coordination/grpc_coordination_service_impl.h\"\n+#include \"xla/tsl/distributed_runtime/rpc/async_service_interface.h\"\n+#include \"xla/tsl/lib/core/status_test_util.h\"\n+#include \"xla/tsl/platform/env.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/status.h\"\n+#include \"xla/tsl/platform/test.h\"\n+#include \"xla/tsl/platform/threadpool.h\"\n+#include \"xla/tsl/protobuf/coordination_config.pb.h\"\n+#include \"xla/tsl/protobuf/coordination_service.pb.h\"\n+\n+namespace xla {\n+namespace {\n+using ::tensorflow::CoordinationServiceConfig;\n+using ::testing::AnyOf;\n+using ::testing::ElementsAre;\n+using ::testing::HasSubstr;\n+using ::testing::IsEmpty;\n+using ::testing::UnorderedElementsAre;\n+\n+constexpr absl::Duration kBarrierTimeout = absl::Milliseconds(200);\n+constexpr absl::Duration kHeartbeatTimeout = absl::Seconds(3);\n+constexpr char kBarrierId[] = \"barrier_id\";\n+\n+tensorflow::CoordinatedTask GetTask(int node_id) {\n+  tensorflow::CoordinatedTask task;\n+  task.set_task_id(node_id);\n+  task.set_job_name(\"agent\");\n+  return task;\n+}\n+\n+// Note: b/169705709: no protobuf matchers in OSS.\n+MATCHER_P2(IsKvEntry, key, value, \"\") {\n+  return key == arg.key() && value == arg.value();\n+}\n+\n+class ClientServerTest : public ::testing::Test {\n+ public:\n+  CoordinationServiceConfig GetConfig(\n+      absl::Duration init_and_shutdown_timeout,\n+      bool shutdown_on_destruction = true,\n+      bool cluster_register_with_barrier = true,\n+      bool cluster_shutdown_with_barrier = true) {\n+    // Set config.\n+    tensorflow::CoordinationServiceConfig config;\n+    config.set_service_type(\"standalone\");\n+    config.set_service_leader(\"/job:agent/task:0\");\n+    config.set_cluster_register_timeout_in_ms(\n+        absl::ToInt64Milliseconds(init_and_shutdown_timeout));\n+    config.set_heartbeat_timeout_in_ms(\n+        absl::ToInt64Milliseconds(kHeartbeatTimeout));\n+    if (cluster_shutdown_with_barrier) {\n+      config.set_shutdown_barrier_timeout_in_ms(\n+          absl::ToInt64Milliseconds(init_and_shutdown_timeout));\n+    }\n+    config.set_agent_destruction_without_shutdown(!shutdown_on_destruction);\n+    // TODO(b/369222279): Add more test cases that exercise TF behaviour (no\n+    // barrier).\n+    config.set_cluster_register_with_barrier(cluster_register_with_barrier);\n+    config.set_poll_for_error_from_service_at_startup(true);\n+    return config;\n+  }\n+\n+  CoordinationServiceConfig GetServiceConfig(\n+      int num_nodes, absl::Duration init_and_shutdown_timeout,\n+      bool cluster_register_with_barrier, bool cluster_shutdown_with_barrier) {\n+    auto config =\n+        GetConfig(init_and_shutdown_timeout,\n+                  /*shutdown_on_destruction=*/true,\n+                  cluster_register_with_barrier, cluster_shutdown_with_barrier);\n+    tensorflow::CoordinatedJob* job =\n+        config.mutable_coordinated_job_list()->Add();\n+    job->set_name(\"agent\");\n+    job->set_num_tasks(num_nodes);\n+    auto service = CoordinationService::Create(tsl::Env::Default(), config,\n+                                               /*cache=*/nullptr);\n+    return config;\n+  }\n+\n+  std::unique_ptr<CoordinationServiceAgent> GetClient(\n+      int node_id, absl::Duration init_and_shutdown_timeout = absl::Seconds(3),\n+      bool shutdown_on_destruction = true, bool recoverable = false,\n+      tsl::StatusCallback error_fn = [](const absl::Status& status) {\n+        LOG(ERROR) << \"Agent hit an error: \" << status;\n+      }) {\n+    assert(server_ != nullptr);\n+    std::shared_ptr<grpc::Channel> channel = grpc::CreateChannel(\n+        service_address_, grpc::InsecureChannelCredentials());\n+\n+    std::unique_ptr<CoordinationClient> leader_client;\n+    leader_client.reset(NewGrpcCoordinationClient(channel));\n+\n+    auto coord_agent = CreateCoordinationServiceAgent();\n+    CoordinationServiceConfig config =\n+        GetConfig(init_and_shutdown_timeout, shutdown_on_destruction);\n+    const absl::Status status = coord_agent->Initialize(\n+        tsl::Env::Default(), \"agent\", node_id, config, std::move(leader_client),\n+        std::move(error_fn), recoverable);\n+    if (!status.ok()) {\n+      LOG(ERROR) << \"Coordination agent failed to initialize: \" << status;\n+    }\n+    return coord_agent;\n+  }\n+\n+  void StartService(int num_nodes,\n+                    absl::Duration init_and_shutdown_timeout = absl::Seconds(2),\n+                    bool cluster_register_with_barrier = true,\n+                    bool cluster_shutdown_with_barrier = true) {\n+    auto config = GetServiceConfig(num_nodes, init_and_shutdown_timeout,\n+                                   cluster_register_with_barrier,\n+                                   cluster_shutdown_with_barrier);\n+\n+    int port = tsl::testing::PickUnusedPortOrDie();\n+    grpc::ServerBuilder builder;\n+    service_address_ = absl::StrCat(\"[::]:\", port);\n+    builder.AddListeningPort(service_address_,\n+                             grpc::InsecureServerCredentials());\n+    // Set up the actual coordination service (where all the real logic\n+    // lives).\n+    coord_service_ = CoordinationService::Create(tsl::Env::Default(), config,\n+                                                 /*cache=*/nullptr);\n+    // Set up threads and RPC service.\n+    coord_compute_pool_ = std::make_unique<tsl::thread::ThreadPool>(\n+        tsl::Env::Default(), \"CoordinationServiceRpcHandler\",\n+        /*num_threads=*/4);\n+    coord_rpc_service_ = std::make_unique<GrpcCoordinationServiceImpl>(\n+        coord_compute_pool_.get(), &builder);\n+    auto* grpc_coord_service =\n+        static_cast<GrpcCoordinationServiceImpl*>(coord_rpc_service_.get());\n+    grpc_coord_service->SetCoordinationServiceInstance(coord_service_.get());\n+    // Start the server.\n+    server_ = builder.BuildAndStart();\n+    // Only start RPC loop after the service is live.\n+    coord_rpc_thread_.reset(tsl::Env::Default()->StartThread(\n+        tsl::ThreadOptions(), \"CoordinationServiceHandleRPCsLoop\",\n+        [service = coord_rpc_service_.get()] { service->HandleRPCsLoop(); }));\n+  }\n+\n+  void StopService() {\n+    if (!server_) {\n+      // Service has already stopped.\n+      return;\n+    }\n+    server_->Shutdown(absl::ToChronoTime(absl::Now() + absl::Seconds(5)));\n+    server_->Wait();\n+\n+    // Service object must be destroyed to clear all pending RPCs before\n+    // shutting down the RPC service and the server.\n+    coord_service_ = nullptr;\n+\n+    // Shut down all the service objects.\n+    static_cast<GrpcCoordinationServiceImpl*>(coord_rpc_service_.get())\n+        ->SetCoordinationServiceInstance(nullptr);\n+    coord_rpc_service_->Shutdown();\n+    coord_rpc_thread_ = nullptr;\n+    coord_rpc_service_ = nullptr;\n+    coord_compute_pool_ = nullptr;\n+\n+    // Destroy the server.\n+    server_ = nullptr;\n+  }\n+\n+  void TearDown() override { StopService(); }\n+\n+ private:\n+  std::string service_address_;\n+  std::unique_ptr<grpc::Server> server_;\n+  std::unique_ptr<CoordinationService> coord_service_;\n+  std::unique_ptr<tsl::thread::ThreadPool> coord_compute_pool_;\n+  std::unique_ptr<tsl::AsyncServiceInterface> coord_rpc_service_;\n+  std::unique_ptr<tsl::Thread> coord_rpc_thread_;\n+};\n+\n+struct RecoverableTestParams {\n+  // This determines if the restarted clients will invoke shutdown before going\n+  // away. This checks if the service logic will not be corrupted even if the\n+  // client does not gracefully disconnect (i.e. param set to false).\n+  bool recoverable_node_shutdown_on_destruction;\n+  // This inserts errors to transition the cluster into error states (from the\n+  // service's POV), which may result in different code paths or error codes.\n+  bool inject_heartbeat_errors;\n+  // This exercises barrier counter validation logic. On the agent side, a\n+  // restarted client will have its counter reset. Thus, the service needs to\n+  // let the recoverable node join despite a mismatched counter.\n+  bool pass_multiple_barriers_before_test;\n+};\n+\n+class RecoverableTest\n+    : public ClientServerTest,\n+      public ::testing::WithParamInterface<RecoverableTestParams> {};\n+\n+TEST_F(ClientServerTest, ConnectAndShutdownAreBarriers) {\n+  int num_nodes = 3;\n+  StartService(num_nodes);\n+\n+  absl::Mutex mu;\n+  int connect_count = 0;\n+  int shutdown_count = 0;\n+\n+  absl::Barrier barrier(num_nodes);\n+\n+  auto thread_fn = [&](int node_id) -> absl::Status {\n+    auto client = GetClient(node_id);\n+\n+    // Allow the threads to call Connect one-by-one in order.\n+    auto my_connect_turn = [&]() {\n+      mu.AssertHeld();\n+      return connect_count == node_id;\n+    };\n+    {\n+      absl::MutexLock lock(mu);\n+      mu.Await(absl::Condition(&my_connect_turn));\n+      ++connect_count;\n+    }\n+    TF_RETURN_IF_ERROR(client->Connect());\n+    // Verify that all of the threads have called Connect() by the time we get\n+    // here.\n+    {\n+      absl::MutexLock lock(mu);\n+      if (connect_count != num_nodes) {\n+        return absl::InternalError(absl::StrCat(\n+            \"Connect count is \", connect_count, \" but expected \", num_nodes));\n+      }\n+    }\n+\n+    // Similarly for shutting down.\n+    auto my_shutdown_turn = [&]() {\n+      mu.AssertHeld();\n+      return shutdown_count == node_id;\n+    };\n+    {\n+      absl::MutexLock lock(mu);\n+      mu.Await(absl::Condition(&my_shutdown_turn));\n+      ++shutdown_count;\n+    }\n+    TF_RETURN_IF_ERROR(client->Shutdown());\n+    {\n+      absl::MutexLock lock(mu);\n+      if (shutdown_count != num_nodes) {\n+        return absl::InternalError(absl::StrCat(\n+            \"Shutdown count is \", shutdown_count, \" but expected \", num_nodes));\n+      }\n+    }\n+\n+    return absl::OkStatus();\n+  };\n+\n+  std::vector<absl::Status> statuses(num_nodes);\n+  {\n+    tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"test_threads\",\n+                                        num_nodes);\n+    for (int i = 0; i < num_nodes; ++i) {\n+      thread_pool.Schedule([&, i]() { statuses[i] = thread_fn(i); });\n+    }\n+  }\n+  for (int i = 0; i < num_nodes; ++i) {\n+    TF_EXPECT_OK(statuses[i]);\n+  }\n+}\n+\n+TEST_F(ClientServerTest, ClientsTerminateShutdownIfAnyClientGoesAway) {\n+  int num_nodes = 3;\n+  StartService(num_nodes);\n+\n+  auto thread_fn = [&](int node_id) -> absl::Status {\n+    auto client = GetClient(node_id,\n+                            /*init_and_shutdown_timeout=*/absl::Seconds(3),\n+                            /*shutdown_on_destruction=*/node_id != 0);\n+\n+    TF_RETURN_IF_ERROR(client->Connect());\n+\n+    if (node_id == 0) {\n+      return absl::OkStatus();\n+    }\n+\n+    // The call to Shutdown() should be interrupted if a worker stops issuing\n+    // heartbeats.\n+    return client->Shutdown();\n+  };\n+\n+  std::vector<absl::Status> statuses(num_nodes);\n+  {\n+    tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"test_threads\",\n+                                        num_nodes);\n+    for (int i = 0; i < num_nodes; ++i) {\n+      thread_pool.Schedule([&, i]() { statuses[i] = thread_fn(i); });\n+    }\n+  }\n+  TF_EXPECT_OK(statuses[0]);\n+  for (int i = 1; i < num_nodes; ++i) {\n+    EXPECT_THAT(\n+        statuses[i],\n+        AnyOf(\n+            // Shutdown barrier took too long and failed.\n+            absl_testing::StatusIs(absl::StatusCode::kInternal,\n+                                   HasSubstr(\"timed out\")),\n+            // Heartbeat timeout sets node into error state, failing shutdown\n+            // barrier.\n+            absl_testing::StatusIs(absl::StatusCode::kInternal,\n+                                   HasSubstr(\"heartbeat timeout\")),\n+            // Agent polled error first, and so Shutdown()\n+            // fails because agent is already in error.\n+            absl_testing::StatusIs(absl::StatusCode::kFailedPrecondition)));\n+  }\n+}\n+\n+TEST_F(ClientServerTest, ClientsShutdownSuccessfully) {\n+  int num_nodes = 3;\n+  StartService(num_nodes);\n+\n+  auto thread_fn = [&](int node_id) -> absl::Status {\n+    auto client = GetClient(node_id);\n+\n+    TF_RETURN_IF_ERROR(client->Connect());\n+    return client->Shutdown();\n+    // The error polling request will be cancelled automatically when the\n+    // client is shutting down.\n+  };\n+\n+  std::vector<absl::Status> statuses(num_nodes);\n+  {\n+    tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"test_threads\",\n+                                        num_nodes);\n+    for (int i = 0; i < num_nodes; ++i) {\n+      thread_pool.Schedule([&, i]() { statuses[i] = thread_fn(i); });\n+    }\n+  }\n+  for (int i = 0; i < num_nodes; ++i) {\n+    TF_EXPECT_OK(statuses[i]);\n+  }\n+}\n+\n+TEST_F(ClientServerTest, MissedHeartbeatCallbackIsExecutedIfAnyClientGoesAway) {\n+  int num_nodes = 3;\n+  StartService(num_nodes);\n+\n+  auto thread_fn = [&](int node_id) -> absl::Status {\n+    absl::Notification shutdown;\n+    auto client =\n+        GetClient(node_id,\n+                  /*init_and_shutdown_timeout=*/absl::Seconds(3),\n+                  /*shutdown_on_destruction=*/node_id != 0,\n+                  /*recoverable=*/false,\n+                  /*error_fn=*/[&shutdown](const absl::Status& status) {\n+                    shutdown.Notify();\n+                  });\n+\n+    TF_RETURN_IF_ERROR(client->Connect());\n+\n+    if (node_id == 0) {\n+      return absl::OkStatus();\n+    }\n+    shutdown.WaitForNotification();\n+    return absl::OkStatus();\n+  };\n+\n+  std::vector<absl::Status> statuses(num_nodes);\n+  {\n+    tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"test_threads\",\n+                                        num_nodes);\n+    for (int i = 0; i < num_nodes; ++i) {\n+      thread_pool.Schedule([&, i]() { statuses[i] = thread_fn(i); });\n+    }\n+  }\n+  for (int i = 0; i < num_nodes; ++i) {\n+    TF_EXPECT_OK(statuses[i]);\n+  }\n+}\n+\n+TEST_F(ClientServerTest, ShutdownErrorIsPropagatedToClients) {\n+  int num_nodes = 2;\n+  StartService(num_nodes);\n+  std::vector<absl::Status> statuses = {\n+      absl::UnknownError(\"Uninitialized status.\"),\n+      absl::UnknownError(\"Uninitialized status.\")};\n+  absl::Notification shutdown;\n+\n+  auto thread_fn = [&](int node_id) {\n+    auto client = GetClient(\n+        node_id,\n+        /*init_and_shutdown_timeout=*/absl::Seconds(2),\n+        /*shutdown_on_destruction=*/true,\n+        /*recoverable=*/false,\n+        /*error_fn=*/[&statuses, node_id](const absl::Status& status) {\n+          statuses[node_id] = status;\n+        });\n+\n+    TF_ASSERT_OK(client->Connect());\n+\n+    if (node_id == 0) {\n+      // Shut down early.\n+      client = nullptr;\n+      shutdown.Notify();\n+    } else {\n+      // Block until shutdown barrier times out.\n+      shutdown.WaitForNotification();\n+    }\n+  };\n+\n+  {\n+    tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"test_threads\",\n+                                        num_nodes);\n+    for (int i = 0; i < num_nodes; ++i) {\n+      thread_pool.Schedule([&, i]() { thread_fn(i); });\n+    }\n+  }\n+  EXPECT_THAT(statuses[0], absl_testing::StatusIs(absl::StatusCode::kInternal));\n+  EXPECT_THAT(statuses[1], absl_testing::StatusIs(absl::StatusCode::kInternal));\n+}\n+\n+TEST_F(ClientServerTest, ClientsTerminateIfServiceGoesAway) {\n+#if defined(ADDRESS_SANITIZER)\n+  GTEST_SKIP()\n+      << \"This test is known to produce memory leaks due to ungraceful \"\n+         \"termination of the RPC server despite having pending connections.\";\n+#endif\n+  int num_nodes = 3;\n+  StartService(num_nodes);\n+\n+  absl::Barrier barrier(num_nodes + 1);\n+\n+  auto thread_fn = [&](int node_id) -> absl::Status {\n+    absl::Notification shutdown;\n+    auto client = GetClient(\n+        /*node_id=*/node_id,\n+        /*init_and_shutdown_timeout=*/absl::Seconds(10),\n+        /*shutdown_on_destruction=*/true,\n+        /*recoverable=*/false,\n+        /*error_fn=*/[&shutdown](const absl::Status& status) {\n+          shutdown.Notify();\n+        });\n+\n+    TF_RETURN_IF_ERROR(client->Connect());\n+\n+    barrier.Block();\n+    shutdown.WaitForNotification();\n+\n+    TF_RETURN_IF_ERROR(client->Shutdown());\n+    return absl::OkStatus();\n+  };\n+\n+  std::vector<absl::Status> statuses(num_nodes);\n+  {\n+    tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"test_threads\",\n+                                        num_nodes);\n+    for (int i = 0; i < num_nodes; ++i) {\n+      thread_pool.Schedule([&, i]() { statuses[i] = thread_fn(i); });\n+    }\n+    barrier.Block();\n+    StopService();\n+  }\n+  for (int i = 0; i < num_nodes; ++i) {\n+    EXPECT_EQ(statuses[i].code(), tsl::error::FAILED_PRECONDITION);\n+  }\n+}\n+\n+// We should eventually connect, even if some clients are late to show up.\n+TEST_F(ClientServerTest, LateClientsAreOk) {\n+  int num_nodes = 3;\n+  StartService(num_nodes);\n+\n+  absl::Barrier barrier(num_nodes);\n+\n+  auto thread_fn = [&](int node_id) -> absl::Status {\n+    auto client = GetClient(node_id,\n+                            /*init_and_shutdown_timeout=*/absl::Seconds(20));\n+\n+    barrier.Block();\n+    absl::SleepFor(absl::Milliseconds(200) * node_id);\n+    TF_RETURN_IF_ERROR(client->Connect());\n+    TF_RETURN_IF_ERROR(client->Shutdown());\n+    return absl::OkStatus();\n+  };\n+\n+  std::vector<absl::Status> statuses(num_nodes);\n+  {\n+    tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"test_threads\",\n+                                        num_nodes);\n+    for (int i = 0; i < num_nodes; ++i) {\n+      thread_pool.Schedule([&, i]() { statuses[i] = thread_fn(i); });\n+    }\n+  }\n+  for (int i = 0; i < num_nodes; ++i) {\n+    TF_EXPECT_OK(statuses[i]);\n+  }\n+}\n+\n+// We should eventually time out if a client does not show up.\n+TEST_F(ClientServerTest, ConnectEventuallyTimesOutIfAClientDoesNotShowUp) {\n+  int num_nodes = 3;\n+  StartService(num_nodes);\n+\n+  auto thread_fn = [&](int node_id) -> absl::Status {\n+    auto client = GetClient(node_id);\n+\n+    TF_RETURN_IF_ERROR(client->Connect());\n+    TF_RETURN_IF_ERROR(client->Shutdown());\n+    return absl::OkStatus();\n+  };\n+\n+  // Note: one fewer thread than 'num_nodes'.\n+  std::vector<absl::Status> statuses(num_nodes - 1);\n+  {\n+    tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"test_threads\",\n+                                        num_nodes);\n+    for (int i = 0; i < num_nodes - 1; ++i) {\n+      thread_pool.Schedule([&, i]() { statuses[i] = thread_fn(i); });\n+    }\n+  }\n+  for (int i = 0; i < num_nodes - 1; ++i) {\n+    EXPECT_EQ(statuses[i].code(), tsl::error::DEADLINE_EXCEEDED);\n+  }\n+}\n+\n+// After init, ML program will run. If a client restarts, the ML program will\n+// have an inconsistent state. To recover from this, ALL clients need to\n+// restart.\n+TEST_F(ClientServerTest, ClientRestart_AfterConnect_Fails) {\n+  int num_nodes = 3;\n+  StartService(num_nodes,\n+               /*init_and_shutdown_timeout=*/absl::Seconds(5));\n+  absl::Notification n;\n+\n+  auto thread_fn = [&](int node_id) -> absl::Status {\n+    auto client =\n+        GetClient(node_id, /*init_and_shutdown_timeout=*/absl::Seconds(5));\n+\n+    TF_RETURN_IF_ERROR(client->Connect());\n+    // All clients have successfully connected at this point.\n+    // Simulate client restart by creating a new client.\n+    if (node_id == 2) {\n+      client = nullptr;\n+      auto restarted_client =\n+          GetClient(node_id, /*init_and_shutdown_timeout=*/absl::Seconds(5));\n+      auto status = restarted_client->Connect();\n+      n.Notify();\n+      return status;\n+    }\n+    n.WaitForNotification();\n+    TF_RETURN_IF_ERROR(client->Shutdown());\n+    return absl::OkStatus();\n+  };\n+\n+  std::vector<absl::Status> statuses(num_nodes);\n+  {\n+    tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"test_threads\",\n+                                        num_nodes);\n+    for (int i = 0; i < num_nodes; ++i) {\n+      thread_pool.Schedule([&, i]() { statuses[i] = thread_fn(i); });\n+    }\n+  }\n+  // Errors should have been propagated to the clients, and thus the shutdown\n+  // call will fail with `FailedPrecondition` since the tasks are already in\n+  // error.\n+  EXPECT_THAT(statuses[0],\n+              absl_testing::StatusIs(absl::StatusCode::kFailedPrecondition));\n+  EXPECT_THAT(statuses[1],\n+              absl_testing::StatusIs(absl::StatusCode::kFailedPrecondition));\n+  // This client was restarted, so its connection attempt will be aborted.\n+  EXPECT_THAT(statuses[2], absl_testing::StatusIs(absl::StatusCode::kAborted));\n+}\n+\n+// If a client restarts during init, it can silently reconnect because no\n+// stateful operations have run yet, so the program state is still valid.\n+TEST_F(ClientServerTest, ClientRestart_DuringConnect_Succeeds) {\n+  int num_nodes = 3;\n+  StartService(num_nodes,\n+               /*init_and_shutdown_timeout=*/absl::Seconds(5));\n+  absl::Notification previous_node_2_connecting, node_2_restarted;\n+\n+  std::vector<absl::Status> statuses(num_nodes + 1);\n+  auto thread_fn = [&](int node_id) -> absl::Status {\n+    bool restarted_node_2 = false;\n+    if (node_id == 3) {\n+      restarted_node_2 = true;\n+      node_id = 2;  // This is the restarted client.\n+    }\n+    auto client =\n+        GetClient(node_id, /*init_and_shutdown_timeout=*/absl::Seconds(5));\n+\n+    // Overall timeline:\n+    // 1. Node 0, 2 connects.\n+    // 2. Node 2 restarts and connects.\n+    // 3. Node 1 connects.\n+    // 4. All attempts succeed, except the initial node 2 connection attempt.\n+    if (node_id == 0) {\n+      TF_RETURN_IF_ERROR(client->Connect());\n+      TF_RETURN_IF_ERROR(client->Shutdown());\n+      return absl::OkStatus();\n+    } else if (node_id == 1) {\n+      node_2_restarted.WaitForNotification();\n+      absl::SleepFor(absl::Seconds(1));  // Give time for node 2 to connect.\n+      TF_RETURN_IF_ERROR(client->Connect());\n+      TF_RETURN_IF_ERROR(client->Shutdown());\n+      return absl::OkStatus();\n+    } else if (node_id == 2 && !restarted_node_2) {\n+      previous_node_2_connecting.Notify();\n+      return client->Connect();  // Stale attempt, should fail.\n+    } else {\n+      // Restarted node 2.\n+      previous_node_2_connecting.WaitForNotification();\n+      absl::SleepFor(absl::Seconds(1));  // Give time for node 2 to connect.\n+      node_2_restarted.Notify();\n+      TF_RETURN_IF_ERROR(client->Connect());\n+      TF_RETURN_IF_ERROR(client->Shutdown());\n+      return absl::OkStatus();\n+    }\n+  };\n+  {\n+    tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"test_threads\",\n+                                        num_nodes + 1);\n+\n+    for (int i = 0; i < num_nodes + 1; ++i) {\n+      thread_pool.Schedule([&, i]() { statuses[i] = thread_fn(i); });\n+    }\n+  }\n+  EXPECT_THAT(statuses[0], absl_testing::StatusIs(absl::StatusCode::kOk));\n+  EXPECT_THAT(statuses[1], absl_testing::StatusIs(absl::StatusCode::kOk));\n+  // This was the initial connection attempt that should be aborted.\n+  EXPECT_THAT(statuses[2],\n+              absl_testing::StatusIs(absl::StatusCode::kAlreadyExists));\n+  // This was the restarted client which should silently reconnect.\n+  EXPECT_THAT(statuses[3], absl_testing::StatusIs(absl::StatusCode::kOk));\n+}\n+\n+TEST_F(ClientServerTest, WaitAtBarrier_Succeed) {\n+  int num_nodes = 2;\n+  StartService(num_nodes);\n+\n+  auto thread_fn = [&](int node_id) -> absl::Status {\n+    auto client = GetClient(node_id);\n+    TF_RETURN_IF_ERROR(client->Connect());\n+\n+    TF_RETURN_IF_ERROR(client->WaitAtBarrier(\"barrier_1\", kBarrierTimeout, {}));\n+    TF_RETURN_IF_ERROR(client->WaitAtBarrier(\"barrier_2\", kBarrierTimeout, {}));\n+\n+    TF_RETURN_IF_ERROR(client->Shutdown());\n+    return absl::OkStatus();\n+  };\n+\n+  std::vector<absl::Status> statuses(num_nodes);\n+  {\n+    tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"test_threads\",\n+                                        num_nodes);\n+    for (int i = 0; i < num_nodes; ++i) {\n+      thread_pool.Schedule([&, i]() { statuses[i] = thread_fn(i); });\n+    }\n+  }\n+  for (int i = 0; i < num_nodes; ++i) {\n+    TF_EXPECT_OK(statuses[i]);\n+  }\n+}\n+\n+TEST_F(ClientServerTest, WaitAtBarrier_Timeout) {\n+  int num_nodes = 2;\n+  StartService(num_nodes);\n+  absl::Notification n;\n+\n+  auto thread_fn = [&](int node_id) -> absl::Status {\n+    auto client = GetClient(node_id);\n+    TF_RETURN_IF_ERROR(client->Connect());\n+\n+    // Node 1 waits for barrier to time out before proceeding.\n+    if (node_id == 1) {\n+      n.WaitForNotification();\n+    }\n+    absl::Status barrier_status =\n+        client->WaitAtBarrier(\"barrier_1\", kBarrierTimeout, {});\n+    // Node 0 notifies that barrier has already timed out.\n+    if (node_id == 0) {\n+      n.Notify();\n+    }\n+    TF_RETURN_IF_ERROR(barrier_status);\n+\n+    TF_RETURN_IF_ERROR(client->Shutdown());\n+    return absl::OkStatus();\n+  };\n+\n+  std::vector<absl::Status> statuses(num_nodes);\n+  {\n+    tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"test_threads\",\n+                                        num_nodes);\n+    for (int i = 0; i < num_nodes; ++i) {\n+      thread_pool.Schedule([&, i]() { statuses[i] = thread_fn(i); });\n+    }\n+  }\n+  for (int i = 0; i < num_nodes; ++i) {\n+    // Co-ordination service returns the status of the previous barrier\n+    // failure without waiting for the thread to time out.\n+    EXPECT_EQ(statuses[i].code(), tsl::error::DEADLINE_EXCEEDED)\n+        << \" node id: \" << i;\n+  }\n+}\n+\n+TEST_F(ClientServerTest, WaitAtBarrier_TimeoutWithDifferentBarrierId) {\n+  int num_nodes = 2;\n+  StartService(num_nodes);\n+\n+  auto thread_fn = [&](int node_id) -> absl::Status {\n+    auto client = GetClient(node_id);\n+    TF_RETURN_IF_ERROR(client->Connect());\n+\n+    std::string barrier_id;\n+    if (node_id == 0) {\n+      barrier_id = \"barrier_0\";\n+    } else if (node_id == 1) {\n+      barrier_id = \"barrier_1\";\n+    }\n+    TF_RETURN_IF_ERROR(client->WaitAtBarrier(barrier_id, kBarrierTimeout, {}));\n+\n+    TF_RETURN_IF_ERROR(client->Shutdown());\n+    return absl::OkStatus();\n+  };\n+\n+  std::vector<absl::Status> statuses(num_nodes);\n+  {\n+    tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"test_threads\",\n+                                        num_nodes);\n+    for (int i = 0; i < num_nodes; ++i) {\n+      thread_pool.Schedule([&, i]() { statuses[i] = thread_fn(i); });\n+    }\n+  }\n+  for (int i = 0; i < num_nodes; ++i) {\n+    EXPECT_EQ(statuses[i].code(), tsl::error::DEADLINE_EXCEEDED)\n+        << \" node id: \" << i;\n+  }\n+}\n+\n+TEST_F(ClientServerTest, WaitAtBarrier_ReuseSameId_Succeeds) {\n+  int num_nodes = 2;\n+  StartService(num_nodes);\n+\n+  auto thread_fn = [&](int node_id) -> absl::Status {\n+    auto client = GetClient(node_id);\n+    TF_RETURN_IF_ERROR(client->Connect());\n+\n+    TF_RETURN_IF_ERROR(client->WaitAtBarrier(\"barrier_1\", kBarrierTimeout, {}));\n+    TF_RETURN_IF_ERROR(client->WaitAtBarrier(\"barrier_2\", kBarrierTimeout, {}));\n+    TF_RETURN_IF_ERROR(client->WaitAtBarrier(\"barrier_1\", kBarrierTimeout, {}));\n+    TF_RETURN_IF_ERROR(client->WaitAtBarrier(\"barrier_2\", kBarrierTimeout, {}));\n+\n+    TF_RETURN_IF_ERROR(client->Shutdown());\n+    return absl::OkStatus();\n+  };\n+\n+  std::vector<absl::Status> statuses(num_nodes);\n+  {\n+    tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"test_threads\",\n+                                        num_nodes);\n+    for (int i = 0; i < num_nodes; ++i) {\n+      thread_pool.Schedule([&, i]() { statuses[i] = thread_fn(i); });\n+    }\n+  }\n+  for (int i = 0; i < num_nodes; ++i) {\n+    TF_EXPECT_OK(statuses[i]);\n+  }\n+}\n+\n+TEST_F(ClientServerTest, WaitAtBarrier_RestartAndBarrierAgain_Fails) {\n+  int num_nodes = 2;\n+  // Allow clients to connect by themselves so restarted client can connect and\n+  // try barrier again.\n+  StartService(num_nodes, /*init_and_shutdown_timeout=*/absl::Seconds(2),\n+               /*cluster_register_with_barrier=*/false,\n+               /*cluster_shutdown_with_barrier=*/false);\n+  absl::Status barrier_status;\n+  absl::Notification n;\n+\n+  auto thread_fn = [&](int node_id) {\n+    auto client = GetClient(node_id);\n+    TF_ASSERT_OK(client->Connect());\n+\n+    // Complete barrier 3 times (simulate job progress).\n+    for (int i = 0; i < 3; ++i) {\n+      TF_ASSERT_OK(client->WaitAtBarrier(\"barrier_1\", kBarrierTimeout, {}));\n+    }\n+    if (node_id == 1) {\n+      client = nullptr;  // Simulate client restart.\n+      auto restarted_client = GetClient(1);\n+      TF_ASSERT_OK(restarted_client->Connect());\n+      // This should fail! This variable is checked after the thread pool is\n+      // destroyed.\n+      barrier_status =\n+          restarted_client->WaitAtBarrier(\"barrier_1\", kBarrierTimeout, {});\n+      n.Notify();\n+    }\n+    // Client 0 should only be destroyed after we get the barrier result.\n+    n.WaitForNotification();\n+  };\n+\n+  {\n+    tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"test_threads\",\n+                                        num_nodes);\n+    for (int i = 0; i < num_nodes; ++i) {\n+      thread_pool.Schedule([&, i]() { thread_fn(i); });\n+    }\n+  }\n+  EXPECT_THAT(barrier_status,\n+              absl_testing::StatusIs(absl::StatusCode::kInternal,\n+                                     HasSubstr(\"restarted\")));\n+}\n+\n+TEST_F(ClientServerTest,\n+       WaitAtBarrier_TimeoutThenOkay_StragglingTaskGetsSameError) {\n+  int num_nodes = 2;\n+  StartService(num_nodes);\n+  absl::Notification n, n_2;\n+  absl::Status status_0, status_0_new, status_1, status_1_new;\n+  auto thread_fn = [&](int node_id) {\n+    auto client = GetClient(node_id);\n+    TF_ASSERT_OK(client->Connect());\n+    if (node_id == 0) {\n+      status_0 = client->WaitAtBarrier(\"barrier_1\", kBarrierTimeout, {});\n+      n.Notify();\n+      n_2.WaitForNotification();\n+      status_0_new = client->WaitAtBarrier(\"barrier_1\", kBarrierTimeout, {});\n+    } else {\n+      n.WaitForNotification();  // Block until node 0's barrier times out.\n+      status_1 = client->WaitAtBarrier(\"barrier_1\", kBarrierTimeout, {});\n+      n_2.Notify();\n+      status_1_new = client->WaitAtBarrier(\"barrier_1\", kBarrierTimeout, {});\n+    }\n+  };\n+\n+  {\n+    tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"test_threads\",\n+                                        num_nodes);\n+    for (int i = 0; i < num_nodes; ++i) {\n+      thread_pool.Schedule([&, i]() { thread_fn(i); });\n+    }\n+  }\n+  // Both nodes should get the same error.\n+  EXPECT_THAT(status_0,\n+              absl_testing::StatusIs(absl::StatusCode::kDeadlineExceeded));\n+  EXPECT_THAT(status_1,\n+              absl_testing::StatusIs(absl::StatusCode::kDeadlineExceeded));\n+  // Next barrier call is okay.\n+  TF_EXPECT_OK(status_0_new);\n+  TF_EXPECT_OK(status_1_new);\n+}\n+\n+TEST_F(ClientServerTest,\n+       WaitAtBarrier_QuickTaskStartBarrierTwice_LateTaskGetsSlowError) {\n+  int num_nodes = 2;\n+  StartService(num_nodes);\n+  absl::Notification n;\n+  absl::Status status_0, status_0_new, status_1;\n+  auto thread_fn = [&](int node_id) {\n+    auto client = GetClient(node_id);\n+    TF_ASSERT_OK(client->Connect());\n+    TF_ASSERT_OK(client->WaitAtBarrier(\"barrier_1\", kBarrierTimeout, {}));\n+    TF_ASSERT_OK(client->WaitAtBarrier(\"barrier_1\", kBarrierTimeout, {}));\n+    if (node_id == 0) {\n+      // Let each barrier time out.\n+      status_0 = client->WaitAtBarrier(\"barrier_1\", kBarrierTimeout, {});\n+      status_0_new = client->WaitAtBarrier(\"barrier_1\", kBarrierTimeout, {});\n+      n.Notify();\n+    } else {\n+      // Block until node 0's barriers times out.\n+      n.WaitForNotification();\n+      status_1 = client->WaitAtBarrier(\"barrier_1\", kBarrierTimeout, {});\n+    }\n+  };\n+\n+  {\n+    tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"test_threads\",\n+                                        num_nodes);\n+    for (int i = 0; i < num_nodes; ++i) {\n+      thread_pool.Schedule([&, i]() { thread_fn(i); });\n+    }\n+  }\n+  // Both barriers from node 0 should time out.\n+  EXPECT_THAT(status_0,\n+              absl_testing::StatusIs(absl::StatusCode::kDeadlineExceeded));\n+  EXPECT_THAT(status_0_new,\n+              absl_testing::StatusIs(absl::StatusCode::kDeadlineExceeded));\n+  // Next barrier call from node 1 gets barrier counter mismatch error.\n+  EXPECT_THAT(status_1, absl_testing::StatusIs(absl::StatusCode::kInternal,\n+                                               HasSubstr(\"too quick / slow\")));\n+}\n+\n+TEST_F(ClientServerTest, WaitAtBarrierSubset_Succeeds) {\n+  int num_nodes = 3;\n+  StartService(num_nodes);\n+  absl::Notification n0, n1;\n+\n+  auto thread_fn = [&](int node_id) -> absl::Status {\n+    auto client = GetClient(node_id);\n+    TF_RETURN_IF_ERROR(client->Connect());\n+\n+    if (node_id != 2) {\n+      TF_RETURN_IF_ERROR(client->WaitAtBarrier(\"barrier_1\", kBarrierTimeout,\n+                                               {GetTask(0), GetTask(1)}));\n+    }\n+\n+    TF_RETURN_IF_ERROR(client->Shutdown());\n+    return absl::OkStatus();\n+  };\n+\n+  std::vector<absl::Status> statuses(num_nodes);\n+  {\n+    tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"test_threads\",\n+                                        num_nodes);\n+    for (int i = 0; i < num_nodes; ++i) {\n+      thread_pool.Schedule([&, i]() { statuses[i] = thread_fn(i); });\n+    }\n+    for (int i = 0; i < num_nodes; ++i) {\n+      TF_EXPECT_OK(statuses[i]);\n+    }\n+  }\n+}\n+\n+TEST_F(ClientServerTest, WaitAtBarrier_DifferentSubset_Fails) {\n+  int num_nodes = 2;\n+  StartService(num_nodes);\n+  absl::Notification n;\n+  absl::Status status_0, status_1 = absl::UnknownError(\"Uninitialized error.\");\n+\n+  auto thread_fn = [&](int node_id) {\n+    auto client = GetClient(node_id);\n+    TF_ASSERT_OK(client->Connect());\n+    if (node_id == 0) {\n+      status_0 =\n+          client->WaitAtBarrier(\"barrier_1\", kBarrierTimeout, {GetTask(0)});\n+      n.Notify();\n+    } else {\n+      n.WaitForNotification();\n+      // Same barrier id, but specifies different tasks.\n+      status_1 =\n+          client->WaitAtBarrier(\"barrier_1\", kBarrierTimeout, {GetTask(1)});\n+    }\n+  };\n+\n+  {\n+    tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"test_threads\",\n+                                        num_nodes);\n+    for (int i = 0; i < num_nodes; ++i) {\n+      thread_pool.Schedule([&, i]() { thread_fn(i); });\n+    }\n+  }\n+  // First barrier call succeeds.\n+  TF_EXPECT_OK(status_0);\n+  // Second barrier call with different task args fails.\n+  EXPECT_THAT(status_1,\n+              absl_testing::StatusIs(absl::StatusCode::kInvalidArgument,\n+                                     HasSubstr(\"Conflicting tasks specified\")));\n+}\n+\n+TEST_F(ClientServerTest, CancelNonExistentBarrier_Fails) {\n+  int num_nodes = 1;\n+  StartService(num_nodes);\n+  auto client = GetClient(0);\n+  TF_ASSERT_OK(client->Connect());\n+\n+  EXPECT_THAT(client->CancelBarrier(\"non_existent_barrier\"),\n+              absl_testing::StatusIs(absl::StatusCode::kFailedPrecondition));\n+}\n+\n+TEST_F(ClientServerTest,\n+       WaitAtBarrierSubsetNonParticipatingProcessAttempts_Fails) {\n+  int num_nodes = 3;\n+  StartService(num_nodes);\n+  absl::Notification n;\n+  absl::Barrier barrier(num_nodes + 1);\n+\n+  // Timeline:\n+  // 1. Node 1, 2 joins barrier.\n+  // 2. Barrier fails because node 2 is unexpected.\n+  // 3. Node 0 joins barrier, but fails because barrier already failed.\n+\n+  auto thread_fn = [&](int node_id) -> absl::Status {\n+    auto client = GetClient(node_id);\n+    TF_RETURN_IF_ERROR(client->Connect());\n+\n+    // Node 0 will be notified only after the barrier has failed and will thus\n+    // fail too.\n+    if (node_id == 0) {\n+      n.WaitForNotification();\n+    }\n+    auto status = client->WaitAtBarrier(\"barrier_1\", kBarrierTimeout,\n+                                        {GetTask(0), GetTask(1)});\n+    // Node 1 will fail in the barrier because non-participating node 2 also\n+    // calls it.\n+    if (node_id == 1) {\n+      n.Notify();\n+    }\n+    // Not calling `Shutdown` because the client will have already returned\n+    // error in the previous call to `WaitAtBarrier` for all 3 nodes. In the\n+    // error state, calling `Shutdown` is undefined behavior.\n+    return status;\n+  };\n+\n+  std::vector<absl::Status> statuses(num_nodes);\n+  {\n+    tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"test_threads\",\n+                                        num_nodes);\n+    for (int i = 0; i < num_nodes; ++i) {\n+      thread_pool.Schedule([&, i]() {\n+        statuses[i] = thread_fn(i);\n+        barrier.Block();\n+      });\n+    }\n+\n+    // Block until the threads have finished execution.\n+    barrier.Block();\n+\n+    for (int i = 0; i < num_nodes; ++i) {\n+      EXPECT_EQ(statuses[i].code(), tsl::error::INVALID_ARGUMENT)\n+          << \" node id: \" << i << \" status: \" << statuses[i].message();\n+    }\n+  }\n+}\n+\n+TEST_F(ClientServerTest, GetAliveTasks_Succeed) {\n+  const int num_nodes = 2;\n+  StartService(num_nodes);\n+\n+  auto thread_fn = [&](int node_id) -> absl::Status {\n+    auto client = GetClient(node_id);\n+    TF_RETURN_IF_ERROR(client->Connect());\n+    absl::StatusOr<std::vector<CoordinationServiceAgent::AliveTask>>\n+        alive_tasks = client->GetAliveTasks({GetTask(0), GetTask(1)});\n+    if (!alive_tasks.ok()) {\n+      return alive_tasks.status();\n+    }\n+    TF_RETURN_IF_ERROR(client->Shutdown());\n+    return absl::OkStatus();\n+  };\n+\n+  std::vector<absl::Status> statuses(num_nodes);\n+  {\n+    tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"test_threads\",\n+                                        num_nodes);\n+    for (int i = 0; i < num_nodes; ++i) {\n+      thread_pool.Schedule([&, i]() { statuses[i] = thread_fn(i); });\n+    }\n+  }\n+  for (int i = 0; i < num_nodes; ++i) {\n+    TF_EXPECT_OK(statuses[i]);\n+  }\n+}\n+\n+TEST_F(ClientServerTest, GetKeyValueDir) {\n+  StartService(/*num_nodes=*/1);\n+  auto client = GetClient(/*node_id=*/0);\n+  TF_ASSERT_OK(client->Connect());\n+  TF_ASSERT_OK(client->InsertKeyValue(\"test_dir/sub_dir/1\", \"1\"));\n+  TF_ASSERT_OK(client->InsertKeyValue(\"test_dir/sub_dir/2\", \"2\"));\n+  TF_ASSERT_OK(client->InsertKeyValue(\"test_dir/3\", \"3\"));\n+  TF_ASSERT_OK(client->InsertKeyValue(\"test\", \"4\"));  // Not in a directory.\n+\n+  auto results = client->GetKeyValueDir(\"test_dir/\");\n+\n+  TF_ASSERT_OK(results.status());\n+  auto kvs = results.value();\n+\n+  EXPECT_THAT(kvs, UnorderedElementsAre(IsKvEntry(\"test_dir/sub_dir/1\", \"1\"),\n+                                        IsKvEntry(\"test_dir/sub_dir/2\", \"2\"),\n+                                        IsKvEntry(\"test_dir/3\", \"3\")));\n+}\n+\n+TEST_F(ClientServerTest, InsertKeyValue_Duplicate_Fails) {\n+  StartService(/*num_nodes=*/1);\n+  auto client = GetClient(/*node_id=*/0);\n+  TF_ASSERT_OK(client->Connect());\n+  TF_ASSERT_OK(client->InsertKeyValue(\"test_key\", \"original_value\"));\n+  EXPECT_TRUE(\n+      absl::IsAlreadyExists(client->InsertKeyValue(\"test_key\", \"never_added\")));\n+  auto result = client->GetKeyValue(\"test_key\", absl::Milliseconds(100));\n+  TF_ASSERT_OK(result.status());\n+  EXPECT_EQ(result.value(), \"original_value\");\n+}\n+\n+TEST_F(ClientServerTest, InsertKeyValue_Duplicate_Overwrites) {\n+  StartService(/*num_nodes=*/1);\n+  auto client = GetClient(/*node_id=*/0);\n+  TF_ASSERT_OK(client->Connect());\n+  TF_ASSERT_OK(client->InsertKeyValue(\"test_key\", \"original_value\"));\n+  TF_EXPECT_OK(client->InsertKeyValue(\"test_key\", \"overwritten_value\",\n+                                      /*allow_overwrite=*/true));\n+  auto result = client->GetKeyValue(\"test_key\", absl::Milliseconds(100));\n+  TF_ASSERT_OK(result.status());\n+  EXPECT_EQ(result.value(), \"overwritten_value\");\n+}\n+\n+TEST_F(ClientServerTest, DeleteKeyValue) {\n+  StartService(/*num_nodes=*/1);\n+  auto client = GetClient(/*node_id=*/0);\n+  TF_ASSERT_OK(client->Connect());\n+  TF_ASSERT_OK(client->InsertKeyValue(\"to_be_deleted\", \"deleted\"));\n+  TF_ASSERT_OK(client->InsertKeyValue(\"to_be_kept\", \"kept\"));\n+\n+  auto results = client->DeleteKeyValue(\"to_be_deleted\");\n+\n+  TF_EXPECT_OK(results);\n+  auto deleted_kv =\n+      client->GetKeyValue(\"to_be_deleted\", absl::Milliseconds(200));\n+  // We time out from attempting to retrieve a deleted key.\n+  EXPECT_EQ(deleted_kv.status().code(), tsl::error::DEADLINE_EXCEEDED);\n+  // Other key should still exist.\n+  auto kept_kv = client->GetKeyValue(\"to_be_kept\", absl::Milliseconds(200));\n+  TF_ASSERT_OK(kept_kv.status());\n+  EXPECT_EQ(kept_kv.value(), \"kept\");\n+}\n+\n+TEST_F(ClientServerTest, DeleteKeyValue_Directory) {\n+  StartService(/*num_nodes=*/1);\n+  auto client = GetClient(/*node_id=*/0);\n+  TF_ASSERT_OK(client->Connect());\n+  TF_ASSERT_OK(client->InsertKeyValue(\"test_dir/sub_dir/1\", \"1\"));\n+  TF_ASSERT_OK(client->InsertKeyValue(\"test_dir/sub_dir/2\", \"2\"));\n+  TF_ASSERT_OK(client->InsertKeyValue(\"test_dir/3\", \"3\"));\n+\n+  auto results = client->DeleteKeyValue(\"test_dir/\");\n+\n+  TF_EXPECT_OK(results);\n+  auto kvs = client->GetKeyValueDir(\"test_dir/\");\n+  TF_ASSERT_OK(kvs.status());\n+  EXPECT_THAT(kvs.value(), IsEmpty());\n+}\n+\n+// This prevents a regression found in b/380359918 where original error messages\n+// are hidden because the RPC layer cannot send long error messages.\n+TEST_F(ClientServerTest, BarrierTimeout_ManyLateTasks_ReturnsCorrectError) {\n+  StartService(/*num_nodes=*/100,\n+               /*init_and_shutdown_timeout=*/absl::Seconds(1),\n+               /*cluster_register_with_barrier=*/false);\n+  auto client = GetClient(/*node_id=*/0);\n+  TF_ASSERT_OK(client->Connect());\n+\n+  // Blocks until the barrier times out.\n+  auto status =\n+      client->WaitAtBarrier(\"test_barrier\", absl::Milliseconds(100), {});\n+\n+  EXPECT_THAT(status,\n+              absl_testing::StatusIs(absl::StatusCode::kDeadlineExceeded));\n+}\n+\n+TEST_F(ClientServerTest, Dtor_CancelsOngoingGetKeyValueAndBarrier) {\n+  // Set 2 nodes with no register barrier to allowing pending barrier RPC.\n+  StartService(/*num_nodes=*/2, /*init_and_shutdown_timeout=*/absl::Seconds(2),\n+               /*cluster_register_with_barrier=*/false);\n+  auto client = GetClient(/*node_id=*/0,\n+                          /*init_and_shutdown_timeout=*/absl::Seconds(2),\n+                          /*shutdown_on_destruction=*/false);\n+  TF_ASSERT_OK(client->Connect());\n+  absl::Status barrier_status, get_key_value_status;\n+  client->WaitAtBarrierAsync(\n+      \"barrier\", absl::Seconds(2), {},\n+      [&barrier_status](absl::Status s) { barrier_status = s; });\n+  client->GetKeyValueAsync(\n+      \"test_key\",\n+      [&get_key_value_status](const absl::StatusOr<std::string>& s) {\n+        get_key_value_status = s.status();\n+      });\n+\n+  // Destroy client.\n+  client = nullptr;\n+\n+  // Pending RPCs should be cancelled.\n+  EXPECT_EQ(barrier_status.code(), tsl::error::CANCELLED);\n+  EXPECT_EQ(get_key_value_status.code(), tsl::error::CANCELLED);\n+  // Unsure why, but this avoids tsan races surrounding RPC handler's mutex\n+  // during dtor.\n+  absl::SleepFor(absl::Seconds(1));\n+}\n+\n+TEST_F(ClientServerTest, RecoverableClientNeverJoins_InitialConnectFails) {\n+  int num_nodes = 3;\n+  StartService(num_nodes);\n+  absl::Notification node_2_joins_late;\n+  std::vector<absl::Status> statuses(num_nodes);\n+\n+  auto thread_fn = [&](int node_id) {\n+    auto client = GetClient(node_id,\n+                            /*init_and_shutdown_timeout=*/absl::Seconds(2),\n+                            /*shutdown_on_destruction=*/true,\n+                            // Nodes 1, 2 are recoverable.\n+                            /*recoverable=*/node_id != 0);\n+    if (node_id == 0) {\n+      statuses[0] = client->Connect();\n+    } else if (node_id == 1) {\n+      statuses[1] = client->Connect();\n+      // Connect should time out and fail because node 2 is not connected yet.\n+      node_2_joins_late.Notify();\n+    } else {\n+      node_2_joins_late.WaitForNotification();\n+      statuses[2] = client->Connect();\n+    }\n+  };\n+\n+  {\n+    tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"test_threads\",\n+                                        num_nodes);\n+    for (int i = 0; i < num_nodes; ++i) {\n+      thread_pool.Schedule([&, i]() { thread_fn(i); });\n+    }\n+  }\n+  // Even though node 2 is recoverable, initial connect still fails if it is\n+  // late.\n+  for (int i = 0; i < num_nodes; ++i) {\n+    // 1) Connect() starts a barrier.\n+    // 2) Barrier times out, returning `DeadlineExceeded` error.\n+    // 3) The failed barrier sets the entire cluster to an error state.\n+    // Subsequent connect attempts will return `Aborted` error due to this error\n+    // state.\n+    EXPECT_THAT(statuses[i], absl_testing::StatusIs(\n+                                 AnyOf(absl::StatusCode::kDeadlineExceeded,\n+                                       absl::StatusCode::kAborted)))\n+        << i;\n+  }\n+}\n+\n+TEST_F(ClientServerTest, NonrecoverableClientDies_ErrorPropagated) {\n+  int num_nodes = 3;\n+  StartService(num_nodes);\n+  absl::Notification shutdown;\n+  std::vector<absl::Status> statuses(num_nodes);\n+\n+  auto thread_fn = [&](int node_id) {\n+    auto client = GetClient(node_id,\n+                            /*init_and_shutdown_timeout=*/absl::Seconds(2),\n+                            /*shutdown_on_destruction=*/false,\n+                            // Nodes 1, 2 are recoverable.\n+                            /*recoverable=*/node_id != 0,\n+                            /*error_fn=*/\n+                            [&statuses, node_id](const absl::Status& status) {\n+                              statuses[node_id] = status;\n+                            });\n+    TF_ASSERT_OK(client->Connect());\n+    if (node_id == 0) {\n+      // Non-recoverable node crashed unexpectedly.\n+      return;\n+    }\n+    // Other nodes will get heartbeat timeouts.\n+    absl::SleepFor(kHeartbeatTimeout * 2);\n+  };\n+\n+  {\n+    tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"test_threads\",\n+                                        num_nodes);\n+    for (int i = 0; i < num_nodes; ++i) {\n+      thread_pool.Schedule([&, i]() { thread_fn(i); });\n+    }\n+  }\n+  EXPECT_THAT(statuses,\n+              ElementsAre(\n+                  // Node 0 died unexpectedly, so no error function invoked.\n+                  absl_testing::StatusIs(absl::StatusCode::kOk),\n+                  // Node 1, 2 get error notification that node 0 died.\n+                  absl_testing::StatusIs(absl::StatusCode::kUnavailable),\n+                  absl_testing::StatusIs(absl::StatusCode::kUnavailable)));\n+}\n+\n+TEST_F(ClientServerTest, RecoverableClientDies_NoErrorPropagated) {\n+  int num_nodes = 3;\n+  StartService(num_nodes);\n+  absl::Notification shutdown;\n+  std::vector<absl::Status> statuses(num_nodes);\n+\n+  auto thread_fn = [&](int node_id) {\n+    auto client = GetClient(node_id,\n+                            /*init_and_shutdown_timeout=*/absl::Seconds(2),\n+                            /*shutdown_on_destruction=*/false,\n+                            // Nodes 1, 2 are recoverable.\n+                            /*recoverable=*/node_id != 0,\n+                            /*error_fn=*/\n+                            [&statuses, node_id](const absl::Status& status) {\n+                              statuses[node_id] = status;\n+                            });\n+    TF_ASSERT_OK(client->Connect());\n+    if (node_id == 1) {\n+      // Recoverable node crashed unexpectedly.\n+      return;\n+    }\n+    // Other nodes will get heartbeat timeouts.\n+    absl::SleepFor(kHeartbeatTimeout * 2);\n+  };\n+\n+  {\n+    tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"test_threads\",\n+                                        num_nodes);\n+    for (int i = 0; i < num_nodes; ++i) {\n+      thread_pool.Schedule([&, i]() { thread_fn(i); });\n+    }\n+  }\n+  // Nobody noticed that node 1 (recoverable) died.\n+  TF_EXPECT_OK(statuses[0]);\n+  TF_EXPECT_OK(statuses[1]);\n+  TF_EXPECT_OK(statuses[2]);\n+}\n+\n+TEST_F(ClientServerTest, RecoverableClient_RestartsAndStartsBarrier) {\n+  int num_nodes = 2;\n+  StartService(num_nodes);\n+  absl::Notification node_1_joins_barrier;\n+  absl::Status status_0, status_0_shutdown, status_1, status_1_shutdown;\n+  status_0 = status_0_shutdown = status_1 = status_1_shutdown =\n+      absl::UnknownError(\"Uninitialized error.\");\n+\n+  auto thread_fn = [&](int node_id) {\n+    auto client = GetClient(node_id,\n+                            /*init_and_shutdown_timeout=*/absl::Seconds(2),\n+                            /*shutdown_on_destruction=*/true,\n+                            // Node 0 is recoverable.\n+                            /*recoverable=*/node_id == 0);\n+    TF_ASSERT_OK(client->Connect());\n+    // This increments the internal barrier counter, and checks if the\n+    // recoverable node can start a new barrier later (despite having a reset\n+    // counter on the client-side)\n+    TF_ASSERT_OK(client->WaitAtBarrier(kBarrierId, absl::Seconds(10), {}));\n+    TF_ASSERT_OK(client->WaitAtBarrier(kBarrierId, absl::Seconds(10), {}));\n+    TF_ASSERT_OK(client->WaitAtBarrier(kBarrierId, absl::Seconds(10), {}));\n+    // Timeline:\n+    // 1. Node 0 restarts.\n+    // 2. Node 0 starts a new barrier.\n+    // 3. Node 1 joins barrier.\n+    // 4. End-of-job barrier.\n+    if (node_id == 0) {\n+      // Restart the client.\n+      client = nullptr;\n+      auto restarted_client =\n+          GetClient(/*node_id=*/0,\n+                    /*init_and_shutdown_timeout=*/absl::Seconds(2),\n+                    /*shutdown_on_destruction=*/true,\n+                    /*recoverable=*/true);\n+\n+      restarted_client->WaitAtBarrierAsync(\n+          kBarrierId, absl::Seconds(10), {},\n+          [&status_0](const absl::Status& status) { status_0 = status; });\n+      // This makes sure that the underlying async RPC layer sends the request\n+      // before we let node 1 join the barrier.\n+      absl::SleepFor(absl::Seconds(1));\n+      // Node 1 should join the barrier after the new barrier is initiated.\n+      node_1_joins_barrier.Notify();\n+      // Check that the cluster is healthy at the end of the test.\n+      status_0_shutdown = restarted_client->WaitAtBarrier(\n+          \"before_shutdown\", absl::Seconds(10), {});\n+    } else {\n+      node_1_joins_barrier.WaitForNotification();\n+      status_1 = client->WaitAtBarrier(kBarrierId, absl::Seconds(10), {});\n+      // Check that the cluster is healthy at the end of the test.\n+      status_1_shutdown =\n+          client->WaitAtBarrier(\"before_shutdown\", absl::Seconds(10), {});\n+    }\n+  };\n+\n+  {\n+    tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"test_threads\",\n+                                        num_nodes);\n+    for (int i = 0; i < num_nodes; ++i) {\n+      thread_pool.Schedule([&, i]() { thread_fn(i); });\n+    }\n+  }\n+  TF_EXPECT_OK(status_0);\n+  TF_EXPECT_OK(status_0_shutdown);\n+  TF_EXPECT_OK(status_1);\n+  TF_EXPECT_OK(status_1_shutdown);\n+}\n+\n+TEST_P(RecoverableTest, RecoverableClient_RestartsAndJoinsBarrier) {\n+  const RecoverableTestParams params = GetParam();\n+  int num_nodes = 2;\n+  StartService(num_nodes);\n+  absl::Notification restart_now;\n+  absl::Status status_0, status_0_shutdown, status_1, status_1_shutdown;\n+  status_0 = status_0_shutdown = status_1 = status_1_shutdown =\n+      absl::UnknownError(\"Uninitialized error.\");\n+\n+  auto thread_fn = [&](int node_id) {\n+    auto client = GetClient(\n+        node_id,\n+        /*init_and_shutdown_timeout=*/absl::Seconds(2),\n+        /*shutdown_on_destruction=*/\n+        params.recoverable_node_shutdown_on_destruction ? true : node_id != 1,\n+        // Node 1 is recoverable.\n+        /*recoverable=*/node_id == 1);\n+    TF_ASSERT_OK(client->Connect());\n+    if (params.pass_multiple_barriers_before_test) {\n+      // This increments the internal barrier counter, and checks if the\n+      // recoverable node can join the barrier later (despite having a reset\n+      // counter on the client-side)\n+      TF_ASSERT_OK(client->WaitAtBarrier(kBarrierId, absl::Seconds(10), {}));\n+      TF_ASSERT_OK(client->WaitAtBarrier(kBarrierId, absl::Seconds(10), {}));\n+      TF_ASSERT_OK(client->WaitAtBarrier(kBarrierId, absl::Seconds(10), {}));\n+    }\n+    // Timeline:\n+    // 1. Node 0 joins barrier.\n+    // 2. Node 1 restarts.\n+    // 3. Node 1 joins barrier.\n+    // 4. End-of-job barrier.\n+    if (node_id == 0) {\n+      // Usually, if a non-recoverable node goes away, the barrier will fail\n+      // immediately. However, if the node is recoverable, the barrier will wait\n+      // for the node to restart.\n+      client->WaitAtBarrierAsync(\n+          kBarrierId, absl::Seconds(10), {},\n+          [&status_0](const absl::Status& status) { status_0 = status; });\n+      // This makes sure that the underlying async RPC layer sends the request\n+      // before we let node 1 restart.\n+      absl::SleepFor(absl::Seconds(1));\n+      // Node 1 should restart after the barrier is initiated.\n+      restart_now.Notify();\n+      // Check that the cluster is healthy at the end of the test.\n+      status_0_shutdown =\n+          client->WaitAtBarrier(\"before_shutdown\", absl::Seconds(10), {});\n+    } else {\n+      restart_now.WaitForNotification();\n+      // Restart the client.\n+      client = nullptr;\n+      if (params.inject_heartbeat_errors) {\n+        absl::SleepFor(2 * kHeartbeatTimeout);\n+      }\n+      auto restarted_client =\n+          GetClient(/*node_id=*/1,\n+                    /*init_and_shutdown_timeout=*/absl::Seconds(2),\n+                    /*shutdown_on_destruction=*/true,\n+                    /*recoverable=*/true);\n+      TF_ASSERT_OK(restarted_client->Connect());\n+      status_1 =\n+          restarted_client->WaitAtBarrier(kBarrierId, absl::Seconds(10), {});\n+      // Check that the cluster is healthy at the end of the test.\n+      status_1_shutdown = restarted_client->WaitAtBarrier(\n+          \"before_shutdown\", absl::Seconds(10), {});\n+    }\n+  };\n+\n+  {\n+    tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"test_threads\",\n+                                        num_nodes);\n+    for (int i = 0; i < num_nodes; ++i) {\n+      thread_pool.Schedule([&, i]() { thread_fn(i); });\n+    }\n+  }\n+  TF_EXPECT_OK(status_0);\n+  TF_EXPECT_OK(status_0_shutdown);\n+  TF_EXPECT_OK(status_1);\n+  TF_EXPECT_OK(status_1_shutdown);\n+}\n+\n+TEST_P(RecoverableTest,\n+       RecoverableClient_JoinsBarrierThenRestartsAndJoinsBarrierAgain) {\n+  const RecoverableTestParams params = GetParam();\n+  int num_nodes = 3;\n+  StartService(num_nodes);\n+  absl::Notification node_0_restarts, node_2_joins_barrier_last;\n+  absl::Status status_0_before_restart, status_0_after_restart,\n+      status_0_shutdown, status_1, status_1_shutdown, status_2,\n+      status_2_shutdown;\n+  status_0_before_restart = status_0_after_restart = status_0_shutdown =\n+      status_1 = status_1_shutdown = status_2 = status_2_shutdown =\n+          absl::UnknownError(\"Uninitialized error.\");\n+\n+  auto thread_fn = [&](int node_id) {\n+    auto client = GetClient(\n+        node_id,\n+        /*init_and_shutdown_timeout=*/absl::Seconds(2),\n+        /*shutdown_on_destruction=*/\n+        params.recoverable_node_shutdown_on_destruction ? true : node_id != 0,\n+        // Node 0 is recoverable.\n+        /*recoverable=*/node_id == 0);\n+    TF_ASSERT_OK(client->Connect());\n+    if (params.pass_multiple_barriers_before_test) {\n+      // This increments the internal barrier counter, and checks if the\n+      // recoverable node can join the barrier later (despite having a reset\n+      // counter on the client-side)\n+      TF_ASSERT_OK(client->WaitAtBarrier(kBarrierId, absl::Seconds(10), {}));\n+      TF_ASSERT_OK(client->WaitAtBarrier(kBarrierId, absl::Seconds(10), {}));\n+      TF_ASSERT_OK(client->WaitAtBarrier(kBarrierId, absl::Seconds(10), {}));\n+    }\n+    // Timeline:\n+    // 1. Node 0, 1 joins barrier.\n+    // 2. Node 0 restarts.\n+    // 3. Node 0 joins barrier again.\n+    // 4. Node 2 joins barrier.\n+    // 5. End-of-job barrier.\n+    if (node_id == 0) {\n+      // This is the key difference: client invokes the barrier **first**, then\n+      // restarts and joins barrier again.\n+      // This means that service has to deal with the barrier state from a stale\n+      // client.\n+      client->WaitAtBarrierAsync(\n+          kBarrierId, absl::Seconds(10), {},\n+          [&status_0_before_restart](const absl::Status& status) {\n+            status_0_before_restart = status;\n+          });\n+      node_0_restarts.WaitForNotification();\n+      // Restart the client.\n+      client = nullptr;\n+      if (params.inject_heartbeat_errors) {\n+        absl::SleepFor(2 * kHeartbeatTimeout);\n+      }\n+      auto restarted_client =\n+          GetClient(/*node_id=*/0,\n+                    /*init_and_shutdown_timeout=*/absl::Seconds(2),\n+                    /*shutdown_on_destruction=*/true,\n+                    /*recoverable=*/true);\n+      TF_ASSERT_OK(restarted_client->Connect());\n+      restarted_client->WaitAtBarrierAsync(\n+          kBarrierId, absl::Seconds(10), {},\n+          [&status_0_after_restart](const absl::Status& status) {\n+            status_0_after_restart = status;\n+          });\n+      node_2_joins_barrier_last.Notify();\n+      status_0_shutdown = restarted_client->WaitAtBarrier(\n+          \"before_shutdown\", absl::Seconds(10), {});\n+    } else if (node_id == 1) {\n+      client->WaitAtBarrierAsync(\n+          kBarrierId, absl::Seconds(10), {},\n+          [&status_1](const absl::Status& status) { status_1 = status; });\n+      // This makes sure that the underlying async RPC layer sends the request\n+      // before node 0 restarts.\n+      absl::SleepFor(absl::Seconds(1));\n+      node_0_restarts.Notify();\n+      status_1_shutdown =\n+          client->WaitAtBarrier(\"before_shutdown\", absl::Seconds(10), {});\n+    } else {\n+      node_2_joins_barrier_last.WaitForNotification();\n+      status_2 = client->WaitAtBarrier(kBarrierId, absl::Seconds(10), {});\n+      status_2_shutdown =\n+          client->WaitAtBarrier(\"before_shutdown\", absl::Seconds(10), {});\n+    }\n+  };\n+\n+  {\n+    tsl::thread::ThreadPool thread_pool(tsl::Env::Default(), \"test_threads\",\n+                                        num_nodes);\n+    for (int i = 0; i < num_nodes; ++i) {\n+      thread_pool.Schedule([&, i]() { thread_fn(i); });\n+    }\n+  }\n+  // If node 0 restarts before joining the barrier, the barrier should be\n+  // cancelled.\n+  EXPECT_THAT(status_0_before_restart,\n+              absl_testing::StatusIs(absl::StatusCode::kCancelled));\n+  TF_EXPECT_OK(status_0_after_restart);\n+  TF_EXPECT_OK(status_0_shutdown);\n+  TF_EXPECT_OK(status_1);\n+  TF_EXPECT_OK(status_1_shutdown);\n+  TF_EXPECT_OK(status_2);\n+  TF_EXPECT_OK(status_2_shutdown);\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(\n+    RecoverableTestSuite, RecoverableTest,\n+    // Full matrix of test parameters.\n+    ::testing::Values(\n+        // Basic tests: agent will invoke shutdown on destruction.\n+        RecoverableTestParams{\n+            /*recoverable_node_shutdown_on_destruction=*/true,\n+            /*inject_heartbeat_errors=*/true,\n+            /*pass_multiple_barriers_before_test=*/false,\n+        },\n+        RecoverableTestParams{/*recoverable_node_shutdown_on_destruction=*/true,\n+                              /*inject_heartbeat_errors=*/false,\n+                              /*pass_multiple_barriers_before_test=*/false},\n+        // Check if service can handle restarted clients that do not gracefully\n+        // disconnect.\n+        RecoverableTestParams{\n+            /*recoverable_node_shutdown_on_destruction=*/false,\n+            /*inject_heartbeat_errors=*/true,\n+            /*pass_multiple_barriers_before_test=*/false},\n+        // Hard test case: agent does not invoke shutdown, and doesn't trigger\n+        // heartbeat errors either.\n+        RecoverableTestParams{\n+            /*recoverable_node_shutdown_on_destruction=*/false,\n+            /*inject_heartbeat_errors=*/false,\n+            /*pass_multiple_barriers_before_test=*/false},\n+        // Same as above, but with multiple barriers to exercise counter\n+        // validation logic.\n+        RecoverableTestParams{\n+            /*recoverable_node_shutdown_on_destruction=*/true,\n+            /*inject_heartbeat_errors=*/true,\n+            /*pass_multiple_barriers_before_test=*/true,\n+        },\n+        RecoverableTestParams{/*recoverable_node_shutdown_on_destruction=*/true,\n+                              /*inject_heartbeat_errors=*/false,\n+                              /*pass_multiple_barriers_before_test=*/true},\n+        RecoverableTestParams{\n+            /*recoverable_node_shutdown_on_destruction=*/false,\n+            /*inject_heartbeat_errors=*/true,\n+            /*pass_multiple_barriers_before_test=*/true},\n+        // Hardest test case: agent does not invoke shutdown or trigger\n+        // heartbeat errors. It silently restarts and re-connects.\n+        RecoverableTestParams{\n+            /*recoverable_node_shutdown_on_destruction=*/false,\n+            /*inject_heartbeat_errors=*/false,\n+            /*pass_multiple_barriers_before_test=*/true}));\n+}  // namespace\n+}  // namespace xla"
        },
        {
            "sha": "141fd0d69ae8a2604c8c8699e97688dfa718df6b",
            "filename": "third_party/xla/xla/pjrt/distributed/coordination/coordination_client.h",
            "status": "added",
            "additions": 174,
            "deletions": 0,
            "changes": 174,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_client.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_client.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_client.h?ref=3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a",
            "patch": "@@ -0,0 +1,174 @@\n+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_PJRT_DISTRIBUTED_COORDINATION_COORDINATION_CLIENT_H_\n+#define XLA_PJRT_DISTRIBUTED_COORDINATION_COORDINATION_CLIENT_H_\n+\n+#include <memory>\n+#include <string>\n+\n+#include \"xla/tsl/distributed_runtime/call_options.h\"\n+#include \"xla/tsl/platform/status.h\"\n+#include \"xla/tsl/protobuf/coordination_service.pb.h\"\n+\n+namespace xla {\n+using tensorflow::BarrierRequest;\n+using tensorflow::BarrierResponse;\n+using tensorflow::CancelBarrierRequest;\n+using tensorflow::CancelBarrierResponse;\n+using tensorflow::DeleteKeyValueRequest;\n+using tensorflow::DeleteKeyValueResponse;\n+using tensorflow::GetAliveTasksRequest;\n+using tensorflow::GetAliveTasksResponse;\n+using tensorflow::GetKeyValueDirRequest;\n+using tensorflow::GetKeyValueDirResponse;\n+using tensorflow::GetKeyValueRequest;\n+using tensorflow::GetKeyValueResponse;\n+using tensorflow::GetTaskStateRequest;\n+using tensorflow::GetTaskStateResponse;\n+using tensorflow::HeartbeatRequest;\n+using tensorflow::HeartbeatResponse;\n+using tensorflow::IncrementKeyValueRequest;\n+using tensorflow::IncrementKeyValueResponse;\n+using tensorflow::InsertKeyValueRequest;\n+using tensorflow::InsertKeyValueResponse;\n+using tensorflow::PollForErrorRequest;\n+using tensorflow::PollForErrorResponse;\n+using tensorflow::RegisterTaskRequest;\n+using tensorflow::RegisterTaskResponse;\n+using tensorflow::ReportErrorToServiceRequest;\n+using tensorflow::ReportErrorToServiceResponse;\n+using tensorflow::ReportErrorToTaskRequest;\n+using tensorflow::ReportErrorToTaskResponse;\n+using tensorflow::ResetTaskRequest;\n+using tensorflow::ResetTaskResponse;\n+using tensorflow::ShutdownTaskRequest;\n+using tensorflow::ShutdownTaskResponse;\n+using tensorflow::TryGetKeyValueRequest;\n+using tensorflow::TryGetKeyValueResponse;\n+using tensorflow::WaitForAllTasksRequest;\n+using tensorflow::WaitForAllTasksResponse;\n+using tensorflow::WatchJobStateRequest;\n+using tensorflow::WatchJobStateResponse;\n+\n+// Base class of client interface for communicating with coordination service.\n+// Can be implemented by a variety of transports such as gRPC.\n+class CoordinationClient {\n+ public:\n+  virtual ~CoordinationClient() = default;\n+\n+  virtual void RegisterTaskAsync(tsl::CallOptions* call_opts,\n+                                 const RegisterTaskRequest* request,\n+                                 RegisterTaskResponse* response,\n+                                 tsl::StatusCallback done) = 0;\n+\n+  virtual void HeartbeatAsync(tsl::CallOptions* call_opts,\n+                              const HeartbeatRequest* request,\n+                              HeartbeatResponse* response,\n+                              tsl::StatusCallback done) = 0;\n+\n+  virtual void WaitForAllTasksAsync(const WaitForAllTasksRequest* request,\n+                                    WaitForAllTasksResponse* response,\n+                                    tsl::StatusCallback done) = 0;\n+\n+  virtual void ShutdownTaskAsync(tsl::CallOptions* call_opts,\n+                                 const ShutdownTaskRequest* request,\n+                                 ShutdownTaskResponse* response,\n+                                 tsl::StatusCallback done) = 0;\n+\n+  virtual void ResetTaskAsync(const ResetTaskRequest* request,\n+                              ResetTaskResponse* response,\n+                              tsl::StatusCallback done) = 0;\n+\n+  virtual void ReportErrorToTaskAsync(tsl::CallOptions* call_opts,\n+                                      const ReportErrorToTaskRequest* request,\n+                                      ReportErrorToTaskResponse* response,\n+                                      tsl::StatusCallback done) = 0;\n+\n+  virtual void ReportErrorToServiceAsync(\n+      const ReportErrorToServiceRequest* request,\n+      ReportErrorToServiceResponse* response, tsl::StatusCallback done) = 0;\n+\n+  virtual void GetTaskStateAsync(const GetTaskStateRequest* request,\n+                                 GetTaskStateResponse* response,\n+                                 tsl::StatusCallback done) = 0;\n+\n+  virtual void WatchJobStateAsync(tsl::CallOptions* call_opts,\n+                                  const WatchJobStateRequest* request,\n+                                  WatchJobStateResponse* response,\n+                                  tsl::StatusCallback done) = 0;\n+\n+  virtual void InsertKeyValueAsync(const InsertKeyValueRequest* request,\n+                                   InsertKeyValueResponse* response,\n+                                   tsl::StatusCallback done) = 0;\n+\n+  virtual void GetKeyValueAsync(tsl::CallOptions* call_opts,\n+                                const GetKeyValueRequest* request,\n+                                GetKeyValueResponse* response,\n+                                tsl::StatusCallback done) = 0;\n+\n+  virtual void TryGetKeyValueAsync(const TryGetKeyValueRequest* request,\n+                                   TryGetKeyValueResponse* response,\n+                                   tsl::StatusCallback done) = 0;\n+\n+  virtual void GetKeyValueDirAsync(const GetKeyValueDirRequest* request,\n+                                   GetKeyValueDirResponse* response,\n+                                   tsl::StatusCallback done) = 0;\n+\n+  virtual void IncrementKeyValueAsync(const IncrementKeyValueRequest* request,\n+                                      IncrementKeyValueResponse* response,\n+                                      tsl::StatusCallback done) = 0;\n+\n+  virtual void DeleteKeyValueAsync(const DeleteKeyValueRequest* request,\n+                                   DeleteKeyValueResponse* response,\n+                                   tsl::StatusCallback done) = 0;\n+\n+  virtual void BarrierAsync(tsl::CallOptions* call_opts,\n+                            const BarrierRequest* request,\n+                            BarrierResponse* response,\n+                            tsl::StatusCallback done) = 0;\n+\n+  virtual void CancelBarrierAsync(const CancelBarrierRequest* request,\n+                                  CancelBarrierResponse* response,\n+                                  tsl::StatusCallback done) = 0;\n+\n+  virtual void GetAliveTasksAsync(const GetAliveTasksRequest* request,\n+                                  GetAliveTasksResponse* response,\n+                                  tsl::StatusCallback done) = 0;\n+\n+  virtual void PollForErrorAsync(tsl::CallOptions* call_opts,\n+                                 const PollForErrorRequest* request,\n+                                 PollForErrorResponse* response,\n+                                 tsl::StatusCallback done) = 0;\n+};\n+\n+// Simple wrapper class that can be used to retrieve CoordinationClients.\n+class CoordinationClientCache {\n+ public:\n+  virtual ~CoordinationClientCache() = default;\n+\n+  // If the `target` names a remote task, returns a pointer of the\n+  // CoordinationClient object wrapping that channel to the remote task.\n+  virtual CoordinationClient* GetClient(const std::string& target) = 0;\n+\n+  // If the `target` names a remote task, returns an owned pointer of the\n+  // CoordinationClient object wrapping that channel to the remote task.\n+  virtual std::unique_ptr<CoordinationClient> GetOwnedClient(\n+      const std::string& target) = 0;\n+};\n+\n+}  // namespace xla\n+\n+#endif  // XLA_PJRT_DISTRIBUTED_COORDINATION_COORDINATION_CLIENT_H_"
        },
        {
            "sha": "54a026aefb5b9348983ed5ad546e9578d7f6c218",
            "filename": "third_party/xla/xla/pjrt/distributed/coordination/coordination_service.cc",
            "status": "added",
            "additions": 1915,
            "deletions": 0,
            "changes": 1915,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_service.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_service.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_service.cc?ref=3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a",
            "patch": "@@ -0,0 +1,1915 @@\n+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/pjrt/distributed/coordination/coordination_service.h\"\n+\n+#include <algorithm>\n+#include <cassert>\n+#include <cstddef>\n+#include <cstdint>\n+#include <functional>\n+#include <iterator>\n+#include <memory>\n+#include <optional>\n+#include <string>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/algorithm/container.h\"\n+#include \"absl/container/flat_hash_map.h\"\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/str_format.h\"\n+#include \"absl/strings/str_join.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/synchronization/mutex.h\"\n+#include \"absl/synchronization/notification.h\"\n+#include \"absl/time/clock.h\"\n+#include \"absl/time/time.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/pjrt/distributed/coordination/coordination_client.h\"\n+#include \"xla/pjrt/distributed/coordination/coordination_service_error_util.h\"\n+#include \"xla/tsl/distributed_runtime/call_options.h\"\n+#include \"xla/tsl/platform/env.h\"\n+#include \"xla/tsl/platform/errors.h\"\n+#include \"xla/tsl/platform/status.h\"\n+#include \"xla/tsl/protobuf/coordination_config.pb.h\"\n+#include \"xla/tsl/protobuf/coordination_service.pb.h\"\n+#include \"xla/tsl/util/device_name_utils.h\"\n+\n+// If true, allow the recoverable agent to leave ongoing barriers on restart.\n+// TODO: mwhittaker - Make this a flag.\n+constexpr bool kLeaveBarriersOnRecoverableAgentRestart = false;\n+\n+namespace xla {\n+namespace {\n+using tensorflow::CoordinatedTask;\n+using tensorflow::CoordinatedTaskState;\n+using tensorflow::CoordinatedTaskStateInfo;\n+using tensorflow::CoordinationServiceConfig;\n+using tensorflow::CoordinationServiceError;\n+using tensorflow::DeviceInfo;\n+using tensorflow::KeyValueEntry;\n+\n+constexpr char kClusterRegisterBarrierId[] =\n+    \"[Init]Wait_for_all_tasks_to_register\";\n+constexpr absl::Duration kDevicePropagationTimeout = absl::Hours(1);\n+constexpr int kDefaultHeartbeatTimeoutMs = 10 * 1000;  // 10 seconds\n+constexpr int kServiceToClientTimeoutMs = 10 * 1000;   // 10 seconds\n+constexpr size_t kOngoingBarriersSoftLimit = 20;\n+constexpr char kHealthCheckThread[] = \"CoordinationServiceHealthCheck\";\n+// Limit the number of stragglers we log to avoid `RESOURCE_EXHAUSTED` errors in\n+// the RPC layer from sending overly verbose errors.\n+constexpr int kPendingStragglerLogLimit = 3;\n+constexpr int kUniqueBarrierCounter = 0;\n+\n+std::string GetTaskName(absl::string_view job_name, int task_id) {\n+  return absl::StrCat(\"/job:\", job_name, \"/replica:\", 0, \"/task:\", task_id);\n+}\n+\n+std::string GetTaskName(const CoordinatedTask& task) {\n+  return GetTaskName(task.job_name(), task.task_id());\n+}\n+\n+CoordinatedTask GetTaskFromName(absl::string_view task_name) {\n+  tsl::DeviceNameUtils::ParsedName parsed;\n+  tsl::DeviceNameUtils::ParseFullName(task_name, &parsed);\n+  CoordinatedTask task;\n+  task.set_job_name(parsed.job);\n+  task.set_task_id(parsed.task);\n+  return task;\n+}\n+\n+absl::Status MakeShutdownBarrierError(const absl::Status& error) {\n+  return MakeCoordinationError(absl::InternalError(absl::StrCat(\n+      \"Shutdown barrier has failed.\\nBarrier result: '\", error.ToString())));\n+}\n+\n+}  // namespace\n+\n+void CoordinationService::ErrorPollingState::SetError(\n+    const absl::Status& error) {\n+  if (responded_) return;\n+  responded_ = true;\n+  error_ = error;\n+  for (auto& [_, done_cb] : done_callbacks_) {\n+    done_cb(error_);\n+  }\n+  done_callbacks_.clear();\n+}\n+\n+void CoordinationService::ErrorPollingState::RemoveTask(\n+    const CoordinatedTask& task, absl::string_view reason) {\n+  if (done_callbacks_.contains(task)) {\n+    done_callbacks_[task](MakeCoordinationError(absl::CancelledError(\n+        absl::StrCat(\"Cancelled error polling because: \", reason))));\n+  }\n+  done_callbacks_.erase(task);\n+}\n+\n+void CoordinationService::ErrorPollingState::AddTask(\n+    const CoordinatedTask& task, tsl::StatusCallback&& done) {\n+  // Do not allow to insert a task if the service has already responded.\n+  if (Responded()) return;\n+  polling_task_names_.insert(GetTaskName(task));\n+  RemoveTask(task, \"new request from the same task\");\n+  done_callbacks_[task] = done;\n+}\n+\n+void CoordinationService::TaskState::SetConnected(\n+    IncarnationId task_incarnation) {\n+  state_ = CoordinatedTaskState::TASKSTATE_CONNECTED;\n+  status_ = absl::OkStatus();\n+  task_incarnation_ = task_incarnation;\n+  absl::MutexLock l(last_heartbeat_mu_);\n+  last_heartbeat_us_ = tsl::Env::Default()->NowMicros();\n+}\n+\n+void CoordinationService::TaskState::Disconnect(\n+    uint64_t grace_period_duration_us) {\n+  disconnect_grace_period_us_ =\n+      tsl::Env::Default()->NowMicros() + grace_period_duration_us;\n+  state_ = CoordinatedTaskState::TASKSTATE_DISCONNECTED;\n+  status_ = absl::OkStatus();\n+}\n+\n+bool CoordinationService::TaskState::SetError(const absl::Status& status) {\n+  if (state_ == CoordinatedTaskState::TASKSTATE_ERROR) return false;\n+  state_ = CoordinatedTaskState::TASKSTATE_ERROR;\n+  status_ = status;\n+  return true;\n+}\n+\n+absl::Status CoordinationService::TaskState::RecordHeartbeat(\n+    IncarnationId task_incarnation) {\n+  if (!status_.ok()) return status_;\n+  // Record heartbeat.\n+  if (task_incarnation_ == task_incarnation) {\n+    absl::MutexLock l(last_heartbeat_mu_);\n+    last_heartbeat_us_ = tsl::Env::Default()->NowMicros();\n+    return absl::OkStatus();\n+  }\n+  // Task incarnation mismatch!\n+  if (IsRecoverable()) {\n+    return absl::OkStatus();  // Ignore, but don't record new heartbeat.\n+  } else {\n+    return MakeCoordinationError(absl::AbortedError(absl::StrCat(\n+        task_name_, \" Heartbeat: Incarnation ID mismatch: expecting \",\n+        task_incarnation_.value(), \" but got \", task_incarnation.value(),\n+        \". The task has restarted and likely crashed earlier - check for any \"\n+        \"earlier errors or any scheduler events (e.g. preemption, eviction) to \"\n+        \"debug further.\")));\n+  }\n+}\n+\n+int64_t CoordinationService::TaskState::TimeSinceLastHeartbeatMs() {\n+  absl::MutexLock l(last_heartbeat_mu_);\n+  return (tsl::Env::Default()->NowMicros() - last_heartbeat_us_) / 1000;\n+}\n+\n+absl::flat_hash_set<std::string>\n+CoordinationService::TaskState::GetOngoingBarriers() {\n+  return ongoing_barriers_for_task_;\n+}\n+\n+void CoordinationService::TaskState::JoinBarrier(absl::string_view barrier_id) {\n+  ongoing_barriers_for_task_.emplace(barrier_id);\n+}\n+\n+void CoordinationService::TaskState::ExitBarrier(absl::string_view barrier_id) {\n+  ongoing_barriers_for_task_.erase(barrier_id);\n+}\n+\n+bool CoordinationService::TaskState::IsDisconnectedBeyondGracePeriod() {\n+  return GetState() == CoordinatedTaskState::TASKSTATE_DISCONNECTED &&\n+         tsl::Env::Default()->NowMicros() > disconnect_grace_period_us_;\n+}\n+\n+CoordinationService::CoordinationService(\n+    tsl::Env* env, const CoordinationServiceConfig& config,\n+    std::unique_ptr<CoordinationClientCache> client_cache)\n+    : client_cache_(std::move(client_cache)),\n+      env_(*env),\n+      heartbeat_timeout_ms_([&config]() -> uint64_t {\n+        return config.heartbeat_timeout_in_ms() > 0\n+                   ? config.heartbeat_timeout_in_ms()\n+                   : kDefaultHeartbeatTimeoutMs;\n+      }()),\n+      cluster_register_with_barrier_(config.cluster_register_with_barrier()),\n+      cluster_register_timeout_(\n+          absl::Milliseconds(config.cluster_register_timeout_in_ms())),\n+      shutdown_barrier_timeout_(\n+          absl::Milliseconds(config.shutdown_barrier_timeout_in_ms())),\n+      allow_new_incarnation_to_reconnect_(\n+          config.allow_new_incarnation_to_reconnect()) {\n+  LOG(INFO) << \"Initializing CoordinationService\";\n+  recoverable_jobs_ = absl::flat_hash_set<std::string>(\n+      config.recoverable_jobs().cbegin(), config.recoverable_jobs().cend());\n+  for (const auto& job : config.coordinated_job_list()) {\n+    for (int i = 0; i < job.num_tasks(); ++i) {\n+      const std::string task_name = GetTaskName(job.name(), i);\n+      cluster_state_.emplace(task_name, std::make_unique<TaskState>(task_name));\n+    }\n+  }\n+  StartCheckStaleness();\n+}\n+\n+void CoordinationService::CheckHeartbeatTimeout() {\n+  absl::Status status = absl::OkStatus();\n+  std::vector<absl::string_view> stale_task_names;\n+  absl::MutexLock l(state_mu_);\n+  for (const auto& [task_name, task_state] : cluster_state_) {\n+    // Skip tasks that are not registered or in error state.\n+    if (task_state->GetState() != CoordinatedTaskState::TASKSTATE_CONNECTED) {\n+      continue;\n+    }\n+    const bool is_stale =\n+        task_state->TimeSinceLastHeartbeatMs() > heartbeat_timeout_ms_;\n+    VLOG(10) << \"Checking staleness for \" << task_name\n+             << \" stale?=\" << is_stale;\n+    if (is_stale) {\n+      stale_task_names.push_back(task_name);\n+      status = MakeCoordinationError(absl::UnavailableError(\n+          absl::StrCat(\"Task \", task_name,\n+                       \" heartbeat timeout. This indicates that the \"\n+                       \"remote task has failed, got preempted, or \"\n+                       \"crashed unexpectedly. Check the task logs \"\n+                       \"for an earlier error or scheduler events (e.g. \"\n+                       \"preemption, eviction) to debug further.\")));\n+      SetTaskError(task_name, status);\n+    }\n+  }\n+  // Propagate heartbeat timeout errors to other connected tasks.\n+  if (!stale_task_names.empty()) {\n+    // Show at most n task names in the returned error.\n+    const int n = stale_task_names.size() < kPendingStragglerLogLimit\n+                      ? stale_task_names.size()\n+                      : kPendingStragglerLogLimit;\n+    std::string stale_tasks =\n+        absl::StrJoin(absl::MakeSpan(stale_task_names).first(n), \"\\n\");\n+    absl::Status heartbeat_timeout_error =\n+        MakeCoordinationError(absl::UnavailableError(\n+            absl::StrCat(\"The following tasks are unhealthy (stopped sending \"\n+                         \"heartbeats):\\n\",\n+                         stale_tasks,\n+                         \"\\nThe tasks have crashed. Check the task logs for an \"\n+                         \"earlier error, or scheduler events (e.g. preemption, \"\n+                         \"eviction) to debug further.\")));\n+    PropagateError(heartbeat_timeout_error, stale_task_names);\n+  }\n+}\n+\n+absl::flat_hash_map<std::string, int>\n+CoordinationService::GetCountOfOutOfSyncTasksPerBarrier() {\n+  absl::MutexLock l(state_mu_);\n+  absl::flat_hash_map<std::string, int> out_of_sync_tasks_per_barrier;\n+  for (absl::string_view barrier_id : ongoing_barriers_) {\n+    out_of_sync_tasks_per_barrier[barrier_id] = 0;\n+  }\n+  if (unsynced_recoverable_jobs_.empty()) {\n+    return out_of_sync_tasks_per_barrier;\n+  }\n+  VLOG(1) << \"unsynced_recoverable_jobs_: \"\n+          << absl::StrJoin(unsynced_recoverable_jobs_, \",\");\n+  for (absl::string_view barrier_id : ongoing_barriers_) {\n+    auto* barrier = &barriers_[barrier_id];\n+    for (const auto& [task, at_barrier] : barrier->tasks_at_barrier) {\n+      if (at_barrier) {\n+        continue;\n+      }\n+      const auto& task_name = GetTaskName(task);\n+      if (unsynced_recoverable_jobs_.find(task_name) !=\n+          unsynced_recoverable_jobs_.end()) {\n+        out_of_sync_tasks_per_barrier[barrier_id]++;\n+      }\n+    }\n+  }\n+  if (!out_of_sync_tasks_per_barrier.empty()) {\n+    LOG(INFO) << \"out_of_sync_tasks_per_barrier: \"\n+              << absl::StrJoin(out_of_sync_tasks_per_barrier, \",\",\n+                               absl::PairFormatter(\"=\"));\n+  }\n+\n+  return out_of_sync_tasks_per_barrier;\n+}\n+\n+void CoordinationService::CheckBarrierStatusWithRecoverableTasks() {\n+  if (!kLeaveBarriersOnRecoverableAgentRestart) {\n+    return;\n+  }\n+\n+  // Check if barrier should ignore unsynced recoverable jobs.\n+  // A restarted recoverable task could be out of sync with the rest of the\n+  // cluster. It is possible that the restarted task is waiting on a different\n+  // barrier. In this case, we should let the restarted task wait longer at its\n+  // barrier but ignore the recoverable task in other barriers.\n+  // This is to handle scenarios like below.\n+  // 1. begin loop barrier\n+  // 2. Run training steps\n+  // 3. end loop barrier\n+  // 4. Perform checkpointing\n+  // 5. Go back to 1.\n+  // If a task is restarted, while 2 is in progress, the restarted task will\n+  // wait on begin loop barrier, while other tasks will wait on end loop\n+  // barrier.\n+  // A task can wait only on one barrier at a time. So in this case, to avoid\n+  // deadlock, we should ignore the restarted task in the end_loop barrier so\n+  // that the other tasks can proceed. The restarted task will continue to wait\n+  // on the begin loop barrier. When the other tasks reach begin loop barrier,\n+  // at 5, the restarted task will be synced with the other tasks and thus can\n+  // be removed from the unsynced_tasks set.\n+  auto out_of_sync_tasks_per_barrier = GetCountOfOutOfSyncTasksPerBarrier();\n+\n+  absl::MutexLock l(state_mu_);\n+  // Gather barriers which are ready to pass except for the recoverable tasks\n+  // reconnected during the barrier. When the flag\n+  // leave_barriers_on_recoverable_agent_restart is set, the recoverable tasks\n+  // will be removed from the barrier and the barrier will be passed.\n+  absl::flat_hash_set<BarrierState*> passing_barriers;\n+\n+  for (absl::string_view barrier_id : ongoing_barriers_) {\n+    auto* barrier = &barriers_[barrier_id];\n+    // We should ignore out of sync restarted task in this barrier.\n+    // The restarted task will continue to wait on the other barrier till\n+    // synced.\n+    if (barrier->num_pending_tasks ==\n+        out_of_sync_tasks_per_barrier[barrier_id]) {\n+      LOG(INFO) << \"Barrier \" << barrier_id << \" has no pending tasks, this \"\n+                << \"might be because recoverable tasks have disconnected/\"\n+                << \"restarted and were removed from the barrier.\";\n+      LOG(INFO)\n+          << \"Number of tasks restarted during barrier that were ignored: \"\n+          << barrier->recoverable_tasks_restarted_during_barrier.size();\n+      LOG(INFO) << \"Number of tasks ignored due to being out of sync: \"\n+                << out_of_sync_tasks_per_barrier[barrier_id];\n+      passing_barriers.insert(barrier);\n+    }\n+  }\n+  for (auto* barrier : passing_barriers) {\n+    for (auto& task : barrier->recoverable_tasks_restarted_during_barrier) {\n+      const std::string task_name = GetTaskName(task);\n+      const std::unique_ptr<TaskState>& task_state = cluster_state_[task_name];\n+      if (!barrier->tasks_at_barrier[task]) {\n+        --barrier->num_pending_tasks;\n+      }\n+      barrier->done_callbacks.erase(task);\n+      task_state->ExitBarrier(barrier->id);\n+      LOG(INFO) << \"Removed the recoverable task \" << task_name\n+                << \" from the barrier: \" << barrier->id;\n+    }\n+    PassBarrier(barrier, absl::OkStatus());\n+    barrier->recoverable_tasks_restarted_during_barrier.clear();\n+  }\n+}\n+\n+void CoordinationService::CheckBarrierTimeout() {\n+  absl::flat_hash_map<std::string, BarrierState*> expired_barriers;\n+  uint64_t current_time_micros = tsl::Env::Default()->NowMicros();\n+  absl::MutexLock l(state_mu_);\n+  // Gather barriers which have timed out.\n+  for (absl::string_view barrier_id : ongoing_barriers_) {\n+    auto* barrier = &barriers_[barrier_id];\n+    if (current_time_micros > barrier->deadline_in_micros) {\n+      expired_barriers[barrier_id] = barrier;\n+    }\n+  }\n+  // Pass these barriers with the time out error.\n+  for (const auto& [barrier_id, barrier] : expired_barriers) {\n+    std::string pending_tasks;\n+    int pending_task_count = 0;\n+    // Count and track pending tasks that have not reached the barrier.\n+    for (const auto& [task, at_barrier] : barrier->tasks_at_barrier) {\n+      if (at_barrier) {\n+        continue;\n+      }\n+      ++pending_task_count;\n+      if (pending_task_count < kPendingStragglerLogLimit) {\n+        absl::StrAppend(&pending_tasks, GetTaskName(task), \"\\n\");\n+      }\n+    }\n+    const int64_t tasks_at_barrier =\n+        barrier->tasks_at_barrier.size() - pending_task_count;\n+    std::string error_message = absl::StrFormat(\n+        \"Barrier timed out. Id: %s. This usually happens because a task \"\n+        \"triggered the barrier too early or too slowly. Please look at the \"\n+        \"task logs (both timed out and first task) to debug further.\\n\"\n+        \"# of tasks that reached the barrier: %d/%d.\\nThe first \"\n+        \"task at the barrier: %s. Some timed out task names:\\n%s\",\n+        BarrierName(*barrier), tasks_at_barrier,\n+        barrier->tasks_at_barrier.size(), GetTaskName(barrier->initiating_task),\n+        pending_tasks);\n+    const absl::Status error =\n+        MakeBarrierError(absl::DeadlineExceededError(error_message),\n+                         barrier->id, barrier->counter);\n+    PassBarrier(barrier, error);\n+  }\n+}\n+\n+void CoordinationService::CheckStaleness() {\n+  // Used to store stale tasks and barriers.\n+  while (true) {\n+    {\n+      absl::MutexLock l(state_mu_);\n+      check_staleness_thread_cv_.WaitWithTimeout(&state_mu_, absl::Seconds(1));\n+      if (shutting_down_) {\n+        return;\n+      }\n+    }\n+    CheckHeartbeatTimeout();\n+    CheckBarrierStatusWithRecoverableTasks();\n+    CheckBarrierTimeout();\n+  }\n+}\n+\n+void CoordinationService::StartCheckStaleness() {\n+  check_staleness_thread_.reset(\n+      env_.StartThread({}, kHealthCheckThread, [this]() { CheckStaleness(); }));\n+}\n+\n+void CoordinationService::Stop() {\n+  // Prevent recursion.\n+  if (shutting_down_) {\n+    return;\n+  }\n+  // Indicate that the service is shutting down and stop accepting new RPCs.\n+  shutting_down_ = true;\n+  // Stop the heartbeat thread.\n+  check_staleness_thread_cv_.SignalAll();\n+  // Fail all ongoing barriers.\n+  for (auto& [barrier_id, barrier] : barriers_) {\n+    if (!barrier.passed) {\n+      absl::Status error = MakeBarrierError(\n+          absl::AbortedError(absl::StrCat(\n+              \"Barrier failed because service is shutting down. Barrier_id: \",\n+              BarrierName(barrier))),\n+          barrier.id, barrier.counter);\n+      PassBarrier(&barrier, error);\n+    }\n+  }\n+  barriers_.clear();\n+  // Erase cluster state.\n+  // Note: sequence matters here, this must happen after barrier clean-up as\n+  // the state is used in `PassBarrier`.\n+  cluster_state_.clear();\n+\n+  // TODO(mwhittaker): Provide an error status. For now, we provide an empty\n+  // state and -1 version index. We have to call the callbacks or else some\n+  // memory leaks.\n+  for (auto& [job_name, callback] : watch_job_state_callbacks_) {\n+    callback({}, -1);\n+  }\n+  watch_job_state_callbacks_.clear();\n+\n+  // Cancel all pending PollForErrorAsync() calls.\n+  if (IsClientPollingForError()) {\n+    SendErrorPollingResponse(\n+        absl::CancelledError(\"Coordination service is shutting down. \"\n+                             \"Cancelling PollForErrorAsync()\"));\n+  }\n+}\n+\n+bool CoordinationService::ServiceHasStopped() const { return shutting_down_; }\n+\n+// Helper to log progress to having waited for all tasks.\n+void CoordinationService::LogConnectStatusLocked() const {\n+  const int num_tasks = cluster_state_.size();\n+  int pending_tasks = 0;\n+  std::vector<std::string> task_names;\n+  for (const auto& [task_name, task_state] : cluster_state_) {\n+    if (task_state->GetState() != CoordinatedTaskState::TASKSTATE_CONNECTED) {\n+      pending_tasks++;\n+      if (task_names.size() < kPendingStragglerLogLimit) {\n+        task_names.push_back(task_name);\n+      }\n+    }\n+  }\n+  LOG(INFO) << \"Waiting for \" << pending_tasks << \"/\" << num_tasks\n+            << \" tasks to connect.\";\n+  if (!task_names.empty()) {\n+    LOG(INFO) << \"Example stragglers:\\n\" << absl::StrJoin(task_names, \"\\n\");\n+  }\n+}\n+\n+absl::Status CoordinationService::RegisterTask(const CoordinatedTask& task,\n+                                               IncarnationId incarnation) {\n+  absl::Notification done;\n+  absl::Status status;\n+  RegisterTaskAsync(task, incarnation, [&](absl::Status s) {\n+    status = s;\n+    done.Notify();\n+  });\n+  done.WaitForNotification();\n+  return status;\n+}\n+\n+CoordinationService::BarrierCallback\n+CoordinationService::ConnectAfterBarrierPasses(absl::string_view task_name,\n+                                               IncarnationId incarnation,\n+                                               tsl::StatusCallback done) {\n+  return [this, task = std::string(task_name), incarnation,\n+          done = std::move(done)](absl::Status s,\n+                                  int64_t unused_counter) mutable {\n+    state_mu_.AssertHeld();\n+    const std::unique_ptr<TaskState>& task_state = cluster_state_[task];\n+    if (!s.ok()) {\n+      LOG(WARNING) << \"ConnectAfterBarrierPasses: \" << s;\n+    }\n+    if (incarnation != task_state->GetTaskIncarnation()) {\n+      LOG(WARNING) << \"ConnectAfterBarrierPasses: incarnation=\" << incarnation\n+                   << \", task_state->GetTaskIncarnation()=\"\n+                   << task_state->GetTaskIncarnation();\n+    }\n+    if (s.ok() && incarnation == task_state->GetTaskIncarnation()) {\n+      // Connect task to service.\n+      task_state->Connect();\n+      done(absl::OkStatus());\n+      ClusterStateUpdated();\n+    } else if (s.ok() || absl::IsCancelled(s)) {\n+      // Avoid using `AbortedError` which typically has retry semantics.\n+      done(MakeCoordinationError(\n+          absl::AlreadyExistsError(\"Aborted connect attempt as there is a \"\n+                                   \"request from a newer incarnation.\")));\n+    } else {\n+      // Non-cancellation error.\n+      done(s);\n+    }\n+  };\n+}\n+\n+void CoordinationService::ConnectTask(const CoordinatedTask& task,\n+                                      IncarnationId incarnation) {\n+  const std::string task_name = GetTaskName(task);\n+  const std::unique_ptr<TaskState>& task_state = cluster_state_[task_name];\n+\n+  task_state->SetTaskIncarnation(incarnation);\n+  task_state->Connect();\n+  if (task_state->IsRecoverable()) {\n+    LeaveOngoingBarriers(task, \"recoverable task silently connected again\");\n+    if (kLeaveBarriersOnRecoverableAgentRestart) {\n+      unsynced_recoverable_jobs_.insert(task_name);\n+    }\n+  }\n+  ClusterStateUpdated();\n+}\n+\n+void CoordinationService::RegisterTaskAsync(const CoordinatedTask& task,\n+                                            IncarnationId incarnation,\n+                                            tsl::StatusCallback done) {\n+  const std::string task_name = GetTaskName(task);\n+\n+  std::string error_message;\n+  absl::MutexLock l(state_mu_);\n+  if (ServiceHasStopped()) {\n+    done(MakeCoordinationError(absl::InternalError(absl::StrCat(\n+        \"Coordination service has stopped. RegisterTask() from task: \",\n+        task_name,\n+        \" failed. This usually implies an earlier error that caused \"\n+        \"coordination service to shut down before the workers disconnect \"\n+        \"gracefully. Check the task leader's logs for an earlier error or \"\n+        \"scheduler events (e.g. preemption, eviction) to debug the root \"\n+        \"cause.\"))));\n+    return;\n+  }\n+  if (!cluster_state_.contains(task_name)) {\n+    // Note: return early here as unexpected task register errors should not\n+    // be propagated to other tasks.\n+    done(MakeCoordinationError(absl::InvalidArgumentError(absl::StrCat(\n+        \"Unexpected task registered with task_name=\", task_name))));\n+    return;\n+  }\n+\n+  const std::unique_ptr<TaskState>& task_cluster_state =\n+      cluster_state_[task_name];\n+  task_cluster_state->SetRecoverable(task.recoverable());\n+  const auto task_state = task_cluster_state->GetState();\n+  const auto task_status = task_cluster_state->GetStatus();\n+\n+  if (task_state == CoordinatedTaskState::TASKSTATE_DISCONNECTED ||\n+      ((allow_new_incarnation_to_reconnect_ ||\n+        task_cluster_state->IsRecoverable()) &&\n+       (absl::IsUnavailable(task_status) &&\n+        task_status.GetPayload(CoordinationErrorPayloadKey())))) {\n+    // The task is allowed to register itself if:\n+    // - this task is currently disconnected (registering for the first time\n+    //   or has called ResetTask() previously).\n+    // - this task has lost connection previously which caused it to have\n+    //   an unavailable error state, but has now restarted (possibly with\n+    //   a new incarnation). This is only allowed if configured with\n+    //   `allow_new_incarnation_to_reconnect`.\n+    if (cluster_register_with_barrier_) {\n+      // Impose barrier so that all tasks can register together.\n+      // Note: it is possible that the same task restarts multiple times and\n+      // registers itself with new incarnations.\n+      // That is okay; in this code branch, the tasks are not connected yet,\n+      // and the barrier has not succeeded yet.\n+      // There is no state that needs to be cleaned up.\n+      task_cluster_state->SetTaskIncarnation(incarnation);\n+      // If the task is recoverable and rejoins after the cluster register\n+      // barrier has passed, we want to mark the task unsynced when it connects\n+      // again. When the task passes a barrier with other tasks, it will be\n+      // removed from the unsynced set.\n+      if (task_cluster_state->IsRecoverable() &&\n+          kLeaveBarriersOnRecoverableAgentRestart) {\n+        if (barriers_.contains(kClusterRegisterBarrierId) &&\n+            barriers_[kClusterRegisterBarrierId].passed) {\n+          // We want to mark the task unsynced when it connects again. When the\n+          // task passes a barrier with other tasks, it will be removed from the\n+          // unsynced set.\n+          unsynced_recoverable_jobs_.insert(GetTaskName(task));\n+        }\n+      }\n+      BarrierAsyncLocked(\n+          kClusterRegisterBarrierId, kUniqueBarrierCounter,\n+          cluster_register_timeout_, task, {},\n+          ConnectAfterBarrierPasses(task_name, incarnation, std::move(done)));\n+      ClusterStateUpdated();\n+      return;\n+    }\n+    ConnectTask(task, incarnation);\n+    // TODO(b/369222279): Think about the barrier case - may need periodic\n+    // reporting of stragglers.\n+    LogConnectStatusLocked();\n+    done(absl::OkStatus());\n+    ClusterStateUpdated();\n+    return;\n+  } else if (task_state == CoordinatedTaskState::TASKSTATE_CONNECTED) {\n+    // This may happen if the service processes the initial RegisterTask(),\n+    // but the agent did not receive the response so the agent retries again.\n+    if (task_cluster_state->GetTaskIncarnation() == incarnation ||\n+        task_cluster_state->IsRecoverable()) {\n+      // This should be a no-op, but we update the last heartbeat timestamp\n+      // to give a longer grace period for the agent to start sending\n+      // heartbeats.\n+      /// For recoverable tasks, we also leave any previously ongoing barriers.\n+      ConnectTask(task, incarnation);\n+      LogConnectStatusLocked();\n+      done(absl::OkStatus());\n+      ClusterStateUpdated();\n+      return;\n+    } else {\n+      error_message =\n+          absl::StrCat(task_name,\n+                       \" unexpectedly tried to connect with a different \"\n+                       \"incarnation. It has likely restarted.\");\n+    }\n+  } else {\n+    // This task is already in error, which implies it has registered\n+    // previously.\n+    error_message =\n+        absl::StrCat(task_name,\n+                     \" unexpectedly tried to connect while it is already in \"\n+                     \"error. ResetTask() should be called before a \"\n+                     \"subsequent connect attempt. Existing error: \",\n+                     task_status.ToString());\n+  }\n+  LOG(ERROR) << error_message;\n+  absl::Status error =\n+      MakeCoordinationError(absl::AbortedError(error_message), task);\n+  SetTaskError(task_name, error);\n+  PropagateError(error, {task});\n+  done(error);\n+}\n+\n+void CoordinationService::WaitForAllTasks(const CoordinatedTask& task,\n+                                          const DeviceInfo& devices,\n+                                          tsl::StatusCallback done) {\n+  {\n+    absl::MutexLock l(state_mu_);\n+    if (ServiceHasStopped()) {\n+      done(MakeCoordinationError(absl::InternalError(\n+          \"Coordination service has stopped. WaitForAllTasks() failed.\")));\n+      return;\n+    }\n+    const auto& task_state = cluster_state_.find(GetTaskName(task));\n+    // Collect task device info for the first time that task\n+    // has called WaitForAllTasks(). This will be aggregated when the barrier\n+    // passes.\n+    if (task_state != cluster_state_.end() &&\n+        !task_state->second->DeviceInfoIsCollected()) {\n+      task_state->second->CollectDeviceInfo(devices);\n+    }\n+  }\n+  BarrierAsync(device_propagation_barrier_id_, kUniqueBarrierCounter,\n+               kDevicePropagationTimeout, task, {},\n+               [done = std::move(done)](const absl::Status& s,\n+                                        int64_t unused_counter) { done(s); });\n+}\n+\n+void CoordinationService::ShutdownTaskAsync(const CoordinatedTask& task,\n+                                            tsl::StatusCallback done) {\n+  VLOG(3) << \"Task \" << GetTaskName(task) << \" invoked ShutdownTaskAsync()\";\n+  if (shutdown_barrier_timeout_ > absl::ZeroDuration() && !task.recoverable()) {\n+    // Impose shutdown barrier so that all (non-recoverable) tasks can\n+    // disconnect together.\n+    // Notes:\n+    // 1. Recoverable tasks may disconnect and connect multiple times, thus we\n+    //    should not impose a barrier on them. Otherwise, the shutdown barrier\n+    //    will fail upon the first recoverable task restart.\n+    // 2. But a shutdown barrier across non-recoverable tasks is still needed\n+    //    to propagate any early shutdown errors from a non-recoverable task.\n+    // 3. Users should invoke their own barrier to signify the end of a\n+    //    workload so that there will still be a synchronized shutdown across\n+    //    all tasks.\n+    auto shutdown_tasks = GetTasksForShutdownBarrier();\n+    BarrierAsync(shutdown_barrier_id_, kUniqueBarrierCounter,\n+                 shutdown_barrier_timeout_, task, shutdown_tasks,\n+                 [done = std::move(done)](const absl::Status& s,\n+                                          int64_t unused_counter) {\n+                   if (s.ok()) {\n+                     done(absl::OkStatus());\n+                   } else {\n+                     done(MakeShutdownBarrierError(s));\n+                   }\n+                 });\n+  } else {\n+    absl::Status status;\n+    {\n+      absl::MutexLock l(state_mu_);\n+      if (ServiceHasStopped()) {\n+        status = MakeCoordinationError(absl::InternalError(\n+            \"Coordination service has stopped. ShutdownTaskAsync() failed.\"));\n+      } else {\n+        // Disconnect task from service individually.\n+        status = DisconnectTask(task);\n+      }\n+    }\n+    done(status);\n+  }\n+}\n+\n+absl::Status CoordinationService::ResetTask(const CoordinatedTask& task) {\n+  absl::MutexLock l(state_mu_);\n+  return DisconnectTask(task);\n+}\n+\n+absl::Status CoordinationService::DisconnectTask(const CoordinatedTask& task) {\n+  const std::string task_name = GetTaskName(task);\n+  // Check if task is valid and not already disconnected.\n+  if (ServiceHasStopped()) {\n+    return MakeCoordinationError(absl::InternalError(\n+        absl::StrCat(\"Coordination service has stopped. DisconnectTask() \"\n+                     \"failed for task_name=\",\n+                     task_name)));\n+  } else if (!cluster_state_.contains(task_name)) {\n+    return MakeCoordinationError(absl::InvalidArgumentError(absl::StrCat(\n+        \"Unexpected disconnect request with task_name=\", task_name)));\n+  }\n+  const std::unique_ptr<TaskState>& task_state = cluster_state_[task_name];\n+\n+  if (task_state->GetState() == CoordinatedTaskState::TASKSTATE_DISCONNECTED) {\n+    return MakeCoordinationError(absl::FailedPreconditionError(\n+        absl::StrCat(\"The task is already disconnected: \", task_name)));\n+  }\n+\n+  // Disconnect task.\n+  task_state->Disconnect(\n+      /*grace_period_duration_us=*/heartbeat_timeout_ms_ * 1000);\n+  LeaveOngoingBarriers(task, \"task disconnected\");\n+  RefreshAliveness();\n+  error_polling_state_.RemoveTask(task, \"task has disconnected.\");\n+  LOG(INFO) << task_name << \" has disconnected from coordination service.\";\n+  ClusterStateUpdated();\n+  return absl::OkStatus();\n+}\n+\n+const DeviceInfo& CoordinationService::ListClusterDevices() {\n+  return cluster_devices_;\n+}\n+\n+IncarnationId CoordinationService::GetServiceIncarnation() {\n+  return service_incarnation_;\n+}\n+\n+absl::Status CoordinationService::ReportTaskError(const CoordinatedTask& task,\n+                                                  const absl::Status& error) {\n+  const std::string task_name = GetTaskName(task);\n+  absl::MutexLock l(state_mu_);\n+  if (ServiceHasStopped()) {\n+    return MakeCoordinationError(absl::InternalError(\n+        \"Coordination service has stopped. ReportTaskError() failed.\"));\n+  } else if (!cluster_state_.contains(task_name)) {\n+    return MakeCoordinationError(absl::InvalidArgumentError(\n+        absl::StrCat(\"Unexpected request from task \", task_name)));\n+  } else if (cluster_state_[task_name]->GetState() !=\n+             CoordinatedTaskState::TASKSTATE_CONNECTED) {\n+    return MakeCoordinationError(absl::FailedPreconditionError(\n+        \"The task is not connected or already has an error.\"));\n+  }\n+  SetTaskError(task_name, error);\n+  PropagateError(error, {task}, /*is_reported_by_task=*/true);\n+  return absl::OkStatus();\n+}\n+\n+CoordinatedTaskStateInfo CoordinationService::CreateTaskStateInfo(\n+    const CoordinatedTask& task, const TaskState& state) {\n+  CoordinatedTaskStateInfo info;\n+  info.set_state(state.GetState());\n+  info.set_incarnation(state.GetTaskIncarnation().value());\n+  absl::Status error = state.GetStatus();\n+  *info.mutable_task() = task;\n+  info.set_error_code(error.raw_code());\n+  info.set_error_message(std::string(error.message()));\n+  if (!error.ok()) {\n+    *info.mutable_error_payload()->mutable_source_task() = task;\n+    info.mutable_error_payload()->set_is_reported_error(false);\n+  }\n+  return info;\n+}\n+\n+std::vector<CoordinatedTaskStateInfo> CoordinationService::GetTaskState(\n+    const std::vector<CoordinatedTask>& tasks) {\n+  std::vector<CoordinatedTaskStateInfo> states_info;\n+  states_info.reserve(tasks.size());\n+\n+  absl::MutexLock l(state_mu_);\n+  for (const auto& task : tasks) {\n+    states_info.push_back(\n+        CreateTaskStateInfo(task, *cluster_state_[GetTaskName(task)]));\n+  }\n+  return states_info;\n+}\n+\n+std::vector<CoordinatedTaskStateInfo> CoordinationService::GetJobState(\n+    absl::string_view job_name) {\n+  std::vector<CoordinatedTaskStateInfo> states_info;\n+  for (const auto& [name, task_state] : cluster_state_) {\n+    const CoordinatedTask task = GetTaskFromName(name);\n+    if (task.job_name() != job_name) {\n+      continue;\n+    }\n+    states_info.push_back(CreateTaskStateInfo(task, *cluster_state_[name]));\n+  }\n+  return states_info;\n+}\n+\n+void CoordinationService::NotifyWatchJobStateCallbacks() {\n+  for (auto& [job_name, callback] : watch_job_state_callbacks_) {\n+    callback(GetJobState(job_name), cluster_state_version_number_);\n+  }\n+  watch_job_state_callbacks_.clear();\n+}\n+\n+void CoordinationService::ClusterStateUpdated() {\n+  cluster_state_version_number_++;\n+  NotifyWatchJobStateCallbacks();\n+}\n+\n+void CoordinationService::WatchJobState(absl::string_view job_name,\n+                                        std::optional<int64_t> version_number,\n+                                        WatchJobStateCallback callback) {\n+  absl::MutexLock l(state_mu_);\n+  int64_t v = version_number.value_or(-1);\n+  CHECK_GE(cluster_state_version_number_, v);\n+  if (cluster_state_version_number_ == v) {\n+    // Wait until the cluster state changes before invoking the callback.\n+    watch_job_state_callbacks_.emplace_back(job_name, std::move(callback));\n+  } else {\n+    // Invoke the callback immediately.\n+    callback(GetJobState(job_name), cluster_state_version_number_);\n+  }\n+}\n+\n+absl::Status CoordinationService::RecordHeartbeat(const CoordinatedTask& task,\n+                                                  IncarnationId incarnation) {\n+  const std::string task_name = GetTaskName(task);\n+  absl::Status s = absl::OkStatus();\n+  absl::MutexLock l(state_mu_);\n+  if (ServiceHasStopped()) {\n+    return MakeCoordinationError(absl::InternalError(absl::StrCat(\n+        \"Coordination service has stopped. RecordHeartbeat() from task: \",\n+        task_name,\n+        \" failed. This usually implies an earlier error that caused \"\n+        \"coordination service to shut down before the workers disconnect \"\n+        \"gracefully. Check the task leader's logs for an earlier error or \"\n+        \"scheduler events (e.g. preemption, eviction) to debug the root \"\n+        \"cause.\")));\n+  } else if (!cluster_state_.contains(task_name)) {\n+    return MakeCoordinationError(absl::InvalidArgumentError(\n+        absl::StrCat(\"Unexpected heartbeat request from task: \", task_name,\n+                     \". This usually implies a configuration error.\")));\n+  }\n+  const std::unique_ptr<TaskState>& task_state = cluster_state_[task_name];\n+  if (!task_state->GetStatus().ok()) {\n+    return MakeCoordinationError(absl::AbortedError(absl::StrCat(\n+        \"Unexpected heartbeat request from an already-in-error task: \",\n+        task_name,\n+        \" with existing error: \", task_state->GetStatus().ToString())));\n+  } else if (task_state->IsDisconnectedBeyondGracePeriod()) {\n+    // We accept heartbeats for a short grace period to account for the lag\n+    // time between the service recording the state change and the agent\n+    // stopping heartbeats.\n+    return MakeCoordinationError(absl::InvalidArgumentError(\n+        absl::StrCat(\"Task with task_name=\", task_name,\n+                     \" must be registered before sending heartbeat messages. \"\n+                     \"The service might have restarted, please restart / reset \"\n+                     \"and register again.\")));\n+  }\n+  VLOG(10) << \"Record heartbeat from task: \" << task_name\n+           << \"at incarnation: \" << incarnation << \"at \" << absl::Now();\n+  s = task_state->RecordHeartbeat(incarnation);\n+\n+  // Set and propagate any heartbeat errors.\n+  if (!s.ok()) {\n+    SetTaskError(task_name, s);\n+    PropagateError(s, {task});\n+  }\n+\n+  return s;\n+}\n+\n+bool CoordinationService::AllTasksAreRecoverable(\n+    const std::vector<CoordinatedTask>& tasks) {\n+  for (const auto& task : tasks) {\n+    if (!cluster_state_[GetTaskName(task)]->IsRecoverable() &&\n+        !isRecoverableJob(task.job_name())) {\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n+void CoordinationService::PropagateError(\n+    const absl::Status& error,\n+    const std::vector<absl::string_view>& source_task_names,\n+    bool is_reported_by_task) {\n+  std::vector<CoordinatedTask> source_tasks;\n+  source_tasks.reserve(source_task_names.size());\n+  for (const auto& task : source_task_names) {\n+    source_tasks.push_back(GetTaskFromName(task));\n+  }\n+  return PropagateError(error, source_tasks, is_reported_by_task);\n+}\n+\n+void CoordinationService::PropagateError(\n+    const absl::Status& error, const std::vector<CoordinatedTask>& source_tasks,\n+    bool is_reported_by_task) {\n+  VLOG(3) << \"PropagateError(): \" << error;\n+  assert(!error.ok());\n+  assert(!source_tasks.empty());\n+  if (AllTasksAreRecoverable(source_tasks)) {\n+    VLOG(3) << \"All tasks are recoverable, skip propagating error.\";\n+    return;\n+  }\n+  // If there is no service-to-client connection, use error polling or stop\n+  // the service.\n+  if (client_cache_ == nullptr) {\n+    SendErrorPollingResponseOrFailAllTasks(error);\n+    return;\n+  }\n+\n+  ReportErrorToTaskRequest request;\n+  request.set_error_code(error.raw_code());\n+  request.set_error_message(std::string(error.message()));\n+  CoordinationServiceError* payload = request.mutable_error_payload();\n+  payload->set_is_reported_error(is_reported_by_task);\n+  tsl::CallOptions call_opts;\n+  call_opts.SetTimeout(kServiceToClientTimeoutMs);\n+  // TODO(b/369222279): This logic will be removed shortly, so we don't bother\n+  // adding the full list of source tasks.\n+  if (!source_tasks.empty()) {\n+    *payload->mutable_source_task() = source_tasks[0];\n+  }\n+\n+  std::vector<std::shared_ptr<absl::Notification>> notifications;\n+\n+  for (const auto& pair : cluster_state_) {\n+    // Propagate error only to tasks that are connected\n+    if (pair.second->GetState() != CoordinatedTaskState::TASKSTATE_CONNECTED) {\n+      continue;\n+    }\n+    std::string task = pair.first;\n+\n+    CoordinationClient* client = client_cache_->GetClient(task);\n+    auto response = std::make_shared<ReportErrorToTaskResponse>();\n+    auto n = std::make_shared<absl::Notification>();\n+    client->ReportErrorToTaskAsync(\n+        &call_opts, &request, response.get(),\n+        [response, n, task](const absl::Status& s) {\n+          if (!s.ok()) {\n+            LOG(ERROR) << \"Encountered another error while reporting to \"\n+                       << task << \": \" << s;\n+          }\n+          n->Notify();\n+        });\n+    notifications.push_back(n);\n+  }\n+  for (auto& n : notifications) {\n+    n->WaitForNotification();\n+  }\n+}\n+\n+// Utility for normalizing structured config key string.\n+// The normalized key will not have leading or trailing slashes, and all parts\n+// in the key path are separated by exactly one slack ('/').\n+// E.g., ///a//b/c// --> a/b/c\n+std::string NormalizeKey(absl::string_view orig_key) {\n+  std::string norm_key = std::string(orig_key);\n+  const char* src = norm_key.c_str();\n+  std::string::iterator dst = norm_key.begin();\n+\n+  // Parse all characters\n+  while (*src) {\n+    // Skip leading slashes\n+    while (*src == '/') src++;\n+    // Copy over all non-slash characters\n+    while (*src && *src != '/') {\n+      *dst++ = *src++;\n+    }\n+    // Allow one slash at the end of current directory\n+    if (*src) {\n+      *dst++ = *src++;\n+    }\n+  }\n+  // If ending with slash, remove the trailing slash\n+  if (dst > norm_key.begin() && *(dst - 1) == '/') dst--;\n+  norm_key.resize(dst - norm_key.begin());\n+  return norm_key;\n+}\n+\n+absl::Status CoordinationService::InsertKeyValue(absl::string_view key,\n+                                                 absl::string_view value) {\n+  VLOG(3) << \"CoordinationService::InsertKeyValue(key=\" << key\n+          << \", value=\" << value << \")\";\n+  return store_.Put(NormalizeKey(key), value, /*allow_overwrite=*/false);\n+}\n+\n+absl::Status CoordinationService::InsertKeyValue(absl::string_view key,\n+                                                 absl::string_view value,\n+                                                 bool allow_overwrite) {\n+  VLOG(3) << \"CoordinationService::InsertKeyValue(key=\" << key\n+          << \", value=\" << value << \", allow_overwrite=\" << allow_overwrite\n+          << \")\";\n+  return store_.Put(NormalizeKey(key), value, allow_overwrite);\n+}\n+\n+void CoordinationService::GetKeyValueAsync(absl::string_view key,\n+                                           StatusOrValueCallback done) {\n+  VLOG(3) << \"CoordinationService::GetKeyValueAsync(key=\" << key << \")\";\n+  store_.AddCallbackForKey(NormalizeKey(key), done);\n+}\n+\n+absl::StatusOr<std::string> CoordinationService::TryGetKeyValue(\n+    absl::string_view key) {\n+  VLOG(3) << \"CoordinationService::TryGetKeyValue(key=\" << key << \")\";\n+  std::optional<std::string> s = store_.Get(NormalizeKey(key));\n+  if (!s.has_value()) {\n+    return absl::NotFoundError(absl::StrCat(\"Config key \", key, \" not found.\"));\n+  }\n+  return *std::move(s);\n+}\n+\n+absl::StatusOr<std::string> CoordinationService::IncrementKeyValue(\n+    absl::string_view key, int64_t increment) {\n+  VLOG(3) << \"CoordinationService::IncrementKeyValue(key=\" << key\n+          << \", increment=\" << increment << \")\";\n+  return store_.IncrementBy(NormalizeKey(key), increment);\n+}\n+\n+std::vector<KeyValueEntry> CoordinationService::GetKeyValueDir(\n+    absl::string_view directory_key) {\n+  VLOG(3) << \"CoordinationService::GetKeyValueDir(directory_key=\"\n+          << directory_key << \")\";\n+  return store_.GetPrefix(NormalizeKey(directory_key) + \"/\");\n+}\n+\n+absl::Status CoordinationService::DeleteKeyValue(absl::string_view key) {\n+  VLOG(3) << \"CoordinationService::DeleteKeyValue(key=\" << key << \")\";\n+  const std::string normalized = NormalizeKey(key);\n+  store_.Delete(normalized);\n+  store_.DeletePrefix(normalized + \"/\");\n+  return absl::OkStatus();\n+}\n+\n+void CoordinationService::SetAllTasksError(const absl::Status& error) {\n+  for (const auto& task_state : cluster_state_) {\n+    SetTaskError(task_state.first, error);\n+  }\n+}\n+\n+void CoordinationService::SetTaskError(absl::string_view task_name,\n+                                       const absl::Status& error) {\n+  const CoordinatedTask task = GetTaskFromName(task_name);\n+  const std::unique_ptr<TaskState>& task_state = cluster_state_[task_name];\n+  if (task_state->SetError(error)) {\n+    LeaveOngoingBarriers(\n+        task, absl::StrCat(\"task is set to ERROR: \", error.ToString()));\n+    RefreshAliveness();\n+    ClusterStateUpdated();\n+  }\n+}\n+\n+void CoordinationService::PollForErrorAsync(const CoordinatedTask& task,\n+                                            tsl::StatusCallback done) {\n+  const std::string task_name = GetTaskName(task);\n+  VLOG(3) << \"Task \" << task_name << \" invoked PollForErrorAsync().\";\n+\n+  absl::MutexLock l(state_mu_);\n+  if (ServiceHasStopped()) {\n+    done(MakeCoordinationError(absl::InternalError(\n+        \"PollForError requested after coordination service has shut down.\")));\n+    return;\n+  }\n+\n+  if (client_cache_ != nullptr) {\n+    done(MakeCoordinationError(\n+        absl::InternalError(\"Should not use error polling from service when \"\n+                            \"there is service to client connection.\")));\n+    return;\n+  }\n+\n+  client_polling_for_error_ = true;\n+\n+  if (!cluster_state_.contains(task_name)) {\n+    done(MakeCoordinationError(absl::InvalidArgumentError(\n+        absl::StrCat(\"Unexpected task (\", task_name,\n+                     \") that is not in the cluster polling for errors.\"))));\n+    return;\n+  }\n+\n+  // On the agent side, the error polling thread will only be started when the\n+  // task is connected, but by the time the request is processed by the service,\n+  // the task state may have changed due to actions by the service or the main\n+  // thread on the agent. As a way to handle this, we accept error polling for a\n+  // short grace period. After the grace period, the service will return an\n+  // error to the task.\n+  if (cluster_state_[task_name]->IsDisconnectedBeyondGracePeriod()) {\n+    done(MakeCoordinationError(absl::FailedPreconditionError(\n+        absl::StrCat(\"Task (\", task_name,\n+                     \") that has not been registered or has disconnected \"\n+                     \"polling for errors.\"))));\n+    return;\n+  }\n+\n+  if (cluster_state_[task_name]->GetState() ==\n+      CoordinatedTaskState::TASKSTATE_ERROR) {\n+    done(MakeCoordinationError(absl::FailedPreconditionError(absl::StrCat(\n+        \"Task (\", task_name,\n+        \") that is already in error state polling for errors. Current error: \",\n+        cluster_state_[task_name]->GetStatus().ToString()))));\n+    return;\n+  }\n+\n+  if (error_polling_state_.Responded()) {\n+    done(error_polling_state_.GetError());\n+    return;\n+  }\n+\n+  error_polling_state_.AddTask(task, std::move(done));\n+}\n+\n+// Initializes a new barrier. Returns false if the barrier should fail\n+// immediately.\n+absl::Status CoordinationService::InitializeBarrier(\n+    BarrierState* barrier, absl::string_view barrier_id, int64_t counter,\n+    absl::Duration timeout, const CoordinatedTask& task,\n+    const std::vector<CoordinatedTask>& participating_tasks) {\n+  // Initialize barrier state.\n+  barrier->id = barrier_id;\n+  barrier->counter = counter;\n+  barrier->passed = false;\n+  barrier->result = absl::UnknownError(\"Invalid barrier result.\");\n+  barrier->initiating_task = task;\n+  barrier->done_callbacks.clear();\n+  TF_RETURN_IF_ERROR(InitializeTasksAtBarrier(barrier, participating_tasks));\n+\n+  barrier->num_pending_tasks = barrier->tasks_at_barrier.size();\n+\n+  // Fail the barrier immediately if any tasks are already in error.\n+  for (const auto& pending_task : barrier->tasks_at_barrier) {\n+    const std::string task_name = GetTaskName(pending_task.first);\n+    const std::unique_ptr<TaskState>& task_cluster_state =\n+        cluster_state_[task_name];\n+    if (!task_cluster_state->IsRecoverable() &&\n+        task_cluster_state->GetState() ==\n+            CoordinatedTaskState::TASKSTATE_ERROR) {\n+      absl::Status error = MakeBarrierError(\n+          absl::InternalError(absl::StrCat(\n+              \"Task (\", task_name,\n+              \") is already in error before the barrier \"\n+              \"was called. Barrier Id: \",\n+              BarrierName(*barrier),\n+              \" Task error: \", task_cluster_state->GetStatus().ToString())),\n+          barrier->id, barrier->counter);\n+      PassBarrier(barrier, error);\n+      return error;\n+    }\n+  }\n+  barrier->deadline_in_micros =\n+      tsl::Env::Default()->NowMicros() + (timeout / absl::Microseconds(1));\n+\n+  // Add ongoing barrier to cluster state.\n+  ongoing_barriers_.emplace(barrier_id);\n+  const size_t num_ongoing_barriers = ongoing_barriers_.size();\n+  if (num_ongoing_barriers > kOngoingBarriersSoftLimit) {\n+    LOG(WARNING) << \"There is a high number of ongoing barriers in \"\n+                    \"coordination service: \"\n+                 << num_ongoing_barriers;\n+  }\n+  for (const auto& pending_task : barrier->tasks_at_barrier) {\n+    const CoordinatedTask& task = pending_task.first;\n+    cluster_state_[GetTaskName(task)]->JoinBarrier(barrier_id);\n+  }\n+  return absl::OkStatus();\n+}\n+\n+absl::Status CoordinationService::InitializeTasksAtBarrier(\n+    BarrierState* barrier,\n+    const std::vector<CoordinatedTask>& participating_tasks) {\n+  // Tasks were already specified for a previous barrier counter. Reset state\n+  // and return early.\n+  if (!barrier->tasks_at_barrier.empty()) {\n+    for (auto& it : barrier->tasks_at_barrier) {\n+      it.second = false;\n+    }\n+    return absl::OkStatus();\n+  }\n+  if (participating_tasks.empty()) {\n+    // Assume barrier is for entire cluster if no tasks are specified.\n+    for (const auto& task_state : cluster_state_) {\n+      absl::string_view task_name = task_state.first;\n+      barrier->tasks_at_barrier[GetTaskFromName(task_name)] = false;\n+    }\n+    return absl::OkStatus();\n+  }\n+  // Rely on the caller-specified task args.\n+  for (const auto& task : participating_tasks) {\n+    // Fail the barrier immediately if unexpected task is included in the\n+    // barrier.\n+    const std::string task_name = GetTaskName(task);\n+    if (!cluster_state_.contains(task_name)) {\n+      absl::Status error = MakeBarrierError(\n+          absl::InvalidArgumentError(\n+              absl::StrCat(\"Unexpected task (\", task_name,\n+                           \") that is not in the cluster called the barrier. \"\n+                           \"Barrier Id: \",\n+                           BarrierName(*barrier))),\n+          barrier->id, barrier->counter);\n+      PassBarrier(barrier, error);\n+      return error;\n+    }\n+    barrier->tasks_at_barrier[task] = false;\n+  }\n+  return absl::OkStatus();\n+}\n+\n+void CoordinationService::AddBarrierCallback(BarrierState* barrier,\n+                                             const CoordinatedTask& task,\n+                                             BarrierCallback done) {\n+  auto it = barrier->done_callbacks.find(task);\n+  if (it != barrier->done_callbacks.end()) {\n+    it->second(absl::CancelledError(\n+                   absl::StrCat(\"Cancelled because there's a more recent \"\n+                                \"barrier call for \",\n+                                BarrierName(*barrier), \" from the same task.\")),\n+               barrier->counter);\n+  }\n+  barrier->done_callbacks[task] = std::move(done);\n+}\n+\n+void CoordinationService::BarrierAsync(\n+    // Note: `barrier_id` uses a `std::string` instead of `string_view` as the\n+    // RPC may end (i.e. done callback is invoked) before this handler\n+    // completes, which would invalidate the `string_view`.\n+    std::string barrier_id, int64_t counter, absl::Duration timeout,\n+    const CoordinatedTask& task,\n+    const std::vector<CoordinatedTask>& participating_tasks,\n+    BarrierCallback done) {\n+  absl::MutexLock l(state_mu_);\n+  return BarrierAsyncLocked(barrier_id, counter, timeout, task,\n+                            participating_tasks, std::move(done));\n+};\n+\n+void CoordinationService::BarrierAsyncLocked(\n+    absl::string_view barrier_id, int64_t counter, absl::Duration timeout,\n+    const CoordinatedTask& task,\n+    const std::vector<CoordinatedTask>& participating_tasks,\n+    BarrierCallback done) {\n+  VLOG(3) << \"Task \" << GetTaskName(task) << \" invoked BarrierAsync(\"\n+          << BarrierName(barrier_id, counter) << \").\";\n+\n+  // Check if coordination service has stopped. If so, return an error\n+  // immediately.\n+  if (ServiceHasStopped()) {\n+    done(MakeBarrierError(\n+             absl::InternalError(absl::StrCat(\n+                 \"Barrier \", BarrierName(barrier_id, counter),\n+                 \" requested after coordination service has shut down.\")),\n+             barrier_id, counter),\n+         counter);\n+    return;\n+  }\n+\n+  // For the first barrier counter of a unique id, this will create an empty\n+  // struct.\n+  auto* barrier = &barriers_[barrier_id];\n+\n+  // Maybe create new barrier instance.\n+  bool should_initialize_new_instance =\n+      // First barrier for this id.\n+      (BarrierIsUninitialized(*barrier) && (counter == 0)) ||\n+      // Previous barrier instance has passed. New barrier instance\n+      // requested.\n+      (barrier->passed && counter == (barrier->counter + 1));\n+\n+  // Task just restarted and is starting a new barrier (since previous\n+  // barrier passed).\n+  if (  // Barrier has been passed before.\n+      barrier->passed &&\n+      // Task has likely just restarted.\n+      task.recoverable() && counter == 0 &&\n+      // Not a special once-only barrier.\n+      barrier_id != kClusterRegisterBarrierId &&\n+      barrier_id != device_propagation_barrier_id_ &&\n+      barrier_id != shutdown_barrier_id_) {\n+    should_initialize_new_instance = true;\n+    // Use the service's counter to initialize the new barrier.\n+    counter = barrier->counter + 1;\n+  }\n+\n+  if (should_initialize_new_instance) {\n+    // Initialize new barrier instance state.\n+    absl::Status status = InitializeBarrier(barrier, barrier_id, counter,\n+                                            timeout, task, participating_tasks);\n+    if (!status.ok()) {\n+      LOG(ERROR) << \"Barrier (\" << BarrierName(barrier_id, counter) << \") \"\n+                 << \"failed to initialize with status: \" << status\n+                 << \" for task: \" << GetTaskName(task);\n+      done(status, counter);\n+      return;\n+    }\n+  }\n+\n+  // Add pending callbacks.\n+  AddBarrierCallback(barrier, task, std::move(done));\n+\n+  if (!should_initialize_new_instance && counter != barrier->counter &&\n+      // Recoverable tasks are allowed to use different counters and rejoin the\n+      // barrier.\n+      !(task.recoverable() && counter == 0)) {\n+    // Counter mismatch! This is likely due to a restart.\n+    FailBarrierWithCounterMismatch(barrier, counter);\n+    return;\n+  }\n+\n+  // Check if task args are specified consistently across barrier calls, and if\n+  // caller is involved in the barrier.\n+  if (!ValidateTaskArgs(barrier, task, participating_tasks)) {\n+    return;\n+  }\n+\n+  if (barrier->passed && counter == barrier->counter) {\n+    // Same counter, but barrier has already passed: return previous result.\n+    // This will rarely happen, except if the RPC layer retries the RPC\n+    // multiple times somehow.\n+    RepeatBarrierResult(barrier, task);\n+    return;\n+  }\n+\n+  // Task has reached the barrier.\n+  ReachBarrier(barrier, task);\n+}\n+\n+void CoordinationService::FailBarrierWithCounterMismatch(BarrierState* barrier,\n+                                                         int64_t counter) {\n+  std::string reason;\n+  if (counter == 0 || barrier->counter == 0) {\n+    reason =\n+        \"The service or task probably restarted, check the earlier logs to \"\n+        \"debug further. Usually, a restart on the stale tasks will resolve \"\n+        \"the issue.\";\n+  } else {\n+    reason =\n+        \"One task is probably way too quick / slow in its execution. For \"\n+        \"example, one task started a barrier early, which timed out, and \"\n+        \"started a second barrier before the late task even joined once.\";\n+  }\n+\n+  // Counter mismatch! This is likely due to a restart.\n+  absl::Status error = MakeBarrierError(\n+      absl::InternalError(absl::StrCat(\n+          \"Barrier (\", BarrierName(barrier->id, counter),\n+          \") is invoked by the task, but the service thinks it is the \",\n+          barrier->counter, \"-th instance. \", reason)),\n+      barrier->id, barrier->counter);\n+  PassBarrier(barrier, error);\n+}\n+\n+absl::Status CoordinationService::CancelBarrier(\n+    // Note: `barrier_id` uses a `std::string` instead of `string_view` as the\n+    // RPC may end (i.e. done callback is invoked) before this handler\n+    // completes, which would invalidate the `string_view`.\n+    std::string barrier_id, int64_t counter, const CoordinatedTask& task) {\n+  std::string barrier_name = BarrierName(barrier_id, counter);\n+  absl::MutexLock l(state_mu_);\n+  if (ServiceHasStopped()) {\n+    return MakeBarrierError(\n+        absl::InternalError(absl::StrCat(\n+            \"Coordination service has stopped. CancelBarrier() for \",\n+            barrier_name, \" failed.\")),\n+        barrier_id, counter);\n+  }\n+  auto [it, inserted] = barriers_.try_emplace(barrier_id);\n+  auto* barrier = &it->second;\n+  if (inserted) {\n+    LOG(WARNING) << \"Barrier (\" << barrier_name\n+                 << \") is cancelled before being created by task: \"\n+                 << GetTaskName(task);\n+  }\n+  // Cancelling stale barrier instance.\n+  if (barrier->counter != counter) {\n+    return MakeBarrierError(\n+        absl::FailedPreconditionError(\n+            absl::StrCat(\"Barrier (\", barrier_name,\n+                         \") is cancelled by task, but the service thinks \"\n+                         \"it is the \",\n+                         barrier->counter,\n+                         \"-th instance. This is likely due to a restart in the \"\n+                         \"task or service.\")),\n+        barrier_id, barrier->counter);\n+  }\n+  // Barrier has already been passed.\n+  if (barrier->passed) {\n+    return MakeBarrierError(absl::FailedPreconditionError(absl::StrCat(\n+                                \"Barrier (\", barrier_name,\n+                                \") has already been passed with status code: \",\n+                                barrier->result.code())),\n+                            barrier_id, barrier->counter);\n+  }\n+\n+  // Cancel barrier.\n+  absl::Status cancelled =\n+      MakeBarrierError(absl::CancelledError(absl::StrCat(\n+                           \"Barrier (\", barrier_name,\n+                           \") is cancelled by task: \", GetTaskName(task))),\n+                       barrier_id, barrier->counter);\n+  PassBarrier(barrier, cancelled);\n+\n+  VLOG(3) << \"Barrier (\" << barrier_name << \") is cancelled.\";\n+  return absl::OkStatus();\n+}\n+\n+// Mark barrier as passed.\n+void CoordinationService::PassBarrier(BarrierState* barrier,\n+                                      const absl::Status& result) {\n+  barrier->passed = true;\n+  barrier->result = result;\n+  LOG(INFO) << \"Barrier(\" << BarrierName(*barrier)\n+            << \") has passed with status: \" << result;\n+  // Special hook for device propagation barrier to set global device ids.\n+  if (barrier->id == device_propagation_barrier_id_) {\n+    AggregateClusterDevices();\n+  }\n+  for (const auto& task_at_barrier : barrier->tasks_at_barrier) {\n+    // Clean up task state (used as error hooks).\n+    const CoordinatedTask& task = task_at_barrier.first;\n+    cluster_state_[GetTaskName(task)]->ExitBarrier(barrier->id);\n+  }\n+  ongoing_barriers_.erase(barrier->id);\n+  // Propagate results to participating tasks.\n+  for (const auto& [_, callback] : barrier->done_callbacks) {\n+    callback(result, barrier->counter);\n+  }\n+  barrier->done_callbacks.clear();\n+  if (barrier->id == kClusterRegisterBarrierId && !result.ok()) {\n+    // Set all tasks to error.\n+    absl::Status register_error =\n+        MakeCoordinationError(absl::InternalError(absl::StrCat(\n+            \"Cluster registration failed with error: \", result.ToString())));\n+    SetAllTasksError(register_error);\n+    LOG(ERROR)\n+        << \"Stopping coordination service as cluster registration failed. This \"\n+           \"may be due to 1) some tasks crashed earlier before connecting, 2) \"\n+           \"some tasks were never scheduled, or 3) scheduling delays. Consider \"\n+           \"setting a longer initialization timeout if such delays are \"\n+           \"expected, the timeout is currently set to: \"\n+        << cluster_register_timeout_ << \".\\n\\nOriginal error: \" << result;\n+    return;\n+  }\n+  // Special hook for shutdown barrier to disconnect tasks at the barrier and\n+  // propagate errors to those that have not.\n+  if (barrier->id == shutdown_barrier_id_) {\n+    CompleteShutdownAfterBarrier(result, barrier);\n+  }\n+  if (ServiceHasStopped()) {\n+    return;\n+  }\n+}\n+\n+// Returns true if x is a (non-strict) subset of y.\n+bool TaskSetSubset(const CoordinationService::CoordinatedTaskSet& x,\n+                   const CoordinationService::CoordinatedTaskSet& y) {\n+  return std::all_of(x.begin(), x.end(), [&y](const CoordinatedTask& task) {\n+    return y.contains(task);\n+  });\n+}\n+\n+// Returns true if sets x and y are equal.\n+//\n+// Note that the default equality operator (==) on absl::flat_hash_set invokes\n+// the equal operator on the underlying elements in the sets, but the equal\n+// operator is not defined on protos. Thus, we have to implement our own\n+// equality function.\n+bool TaskSetEqual(const CoordinationService::CoordinatedTaskSet& x,\n+                  const CoordinationService::CoordinatedTaskSet& y) {\n+  return x.size() == y.size() && TaskSetSubset(x, y);\n+}\n+\n+CoordinationService::CoordinatedTaskSet CoordinationService::AliveTasks(\n+    const CoordinatedTaskSet& tasks) const {\n+  CoordinatedTaskSet alive_tasks;\n+  for (const CoordinatedTask& task : tasks) {\n+    auto it = cluster_state_.find(GetTaskName(task));\n+    if (it != cluster_state_.end() &&\n+        it->second->GetState() == CoordinatedTaskState::TASKSTATE_CONNECTED) {\n+      // We consider a task alive if it is CONNECTED.\n+      alive_tasks.insert(task);\n+    }\n+  }\n+  return alive_tasks;\n+}\n+\n+std::vector<IncarnationId> CoordinationService::IncarnationIds(\n+    absl::Span<const CoordinatedTask> tasks) const {\n+  std::vector<IncarnationId> incarnations;\n+  for (const CoordinatedTask& task : tasks) {\n+    auto it = cluster_state_.find(GetTaskName(task));\n+    CHECK(it != cluster_state_.end())\n+        << \"Task \" << GetTaskName(task) << \" not found\";\n+    incarnations.push_back(it->second->GetTaskIncarnation());\n+  }\n+  return incarnations;\n+}\n+\n+void CoordinationService::RefreshAliveness() {\n+  // Try to finish every pending GetAliveTasks call.\n+  auto it = aliveness_states_.begin();\n+  while (it != aliveness_states_.end()) {\n+    CoordinatedTaskSet alive_tasks = AliveTasks(it->tasks);\n+    if (TaskSetSubset(alive_tasks, it->in_barrier)) {\n+      // Every alive task is in the barrier, so the barrier is satisfied. Return\n+      // the same set of alive tasks (alive_tasks) to every task in the barrier.\n+      std::vector<CoordinatedTask> v{alive_tasks.begin(), alive_tasks.end()};\n+      std::vector<IncarnationId> incarnation_ids = IncarnationIds(v);\n+      for (const GetAliveTasksCallback& done : it->dones) {\n+        done(absl::OkStatus(), v, incarnation_ids);\n+      }\n+\n+      // Remove the pending GetAliveTasks call because it is no longer pending.\n+      it = aliveness_states_.erase(it);\n+    } else {\n+      // The pending GetAliveTasks call is still pending.\n+      ++it;\n+    }\n+  }\n+}\n+\n+void CoordinationService::GetAliveTasksAsync(\n+    const tensorflow::CoordinatedTask& requesting_task,\n+    const std::vector<tensorflow::CoordinatedTask>& tasks,\n+    GetAliveTasksCallback done) {\n+  // TODO(mwhittaker): Figure out good timeout semantics and add timeouts.\n+\n+  // Validate that the requesting task is a member of tasks.\n+  CoordinatedTaskSet task_set{tasks.begin(), tasks.end()};\n+  if (!task_set.contains(requesting_task)) {\n+    // TODO(mwhittaker): Consider relaxing the requirement that the requesting\n+    // task is one of the specified tasks.\n+    absl::Status err = absl::InvalidArgumentError(absl::StrCat(\n+        \"Requesting task \", GetTaskName(requesting_task),\n+        \" is not one of the tasks specified in a GetAliveTasks request.\"));\n+    done(err, {}, {});\n+    return;\n+  }\n+\n+  // Find the corresponding AlivenessState, creating a new one if needed.\n+  absl::MutexLock l(state_mu_);\n+  auto it = std::find_if(aliveness_states_.begin(), aliveness_states_.end(),\n+                         [&task_set](const AlivenessState& state) {\n+                           return TaskSetEqual(state.tasks, task_set);\n+                         });\n+  if (it == aliveness_states_.end()) {\n+    aliveness_states_.push_back(AlivenessState{task_set});\n+    it = std::prev(aliveness_states_.end());\n+  }\n+\n+  // Enter the requesting task into the barrier.\n+  it->in_barrier.insert(requesting_task);\n+  it->dones.push_back(std::move(done));\n+\n+  // Finish the barrier, if possible.\n+  CoordinatedTaskSet alive_tasks = AliveTasks(task_set);\n+  if (TaskSetSubset(alive_tasks, it->in_barrier)) {\n+    std::vector<CoordinatedTask> v{alive_tasks.begin(), alive_tasks.end()};\n+    std::vector<IncarnationId> incarnation_ids = IncarnationIds(v);\n+    for (const GetAliveTasksCallback& done : it->dones) {\n+      done(absl::OkStatus(), v, incarnation_ids);\n+    }\n+    aliveness_states_.erase(it);\n+  }\n+}\n+\n+void CoordinationService::SendErrorPollingResponse(const absl::Status& error) {\n+  CHECK(IsClientPollingForError())\n+      << \"`SendErrorPollingResponse` should only be called after agents poll \"\n+         \"errors from the service.\";\n+  if (error_polling_state_.Responded()) {\n+    return;\n+  }\n+  if (!absl::IsCancelled(error)) {\n+    VLOG(2) << \"An error is encountered. Sending the error as a response to \"\n+               \"all error polling requests: \"\n+            << error;\n+  }\n+  std::vector<std::string> missing_tasks;\n+  missing_tasks.reserve(cluster_state_.size());\n+  for (const auto& [task_name, task_state] : cluster_state_) {\n+    if (!error_polling_state_.IsTaskPolling(task_name)) {\n+      missing_tasks.push_back(task_name);\n+    }\n+  }\n+  error_polling_state_.SetError(error);\n+  if (!missing_tasks.empty()) {\n+    LOG(ERROR) << absl::StrFormat(\n+        \"The following %d tasks in the cluster has not sent request to poll \"\n+        \"for error. Error will not be propagated to these tasks: %s\",\n+        missing_tasks.size(), absl::StrJoin(missing_tasks, \",\"));\n+  }\n+}\n+\n+bool CoordinationService::ValidateTaskArgs(\n+    BarrierState* barrier, const CoordinatedTask& task,\n+    const std::vector<CoordinatedTask>& tasks_args) {\n+  // Assume all tasks are participating if no task is specified.\n+  if (tasks_args.empty()) {\n+    if (barrier->tasks_at_barrier.size() != cluster_state_.size()) {\n+      absl::Status error = MakeBarrierError(\n+          absl::InvalidArgumentError(absl::StrCat(\n+              BarrierName(*barrier),\n+              \": No tasks were specified by the client, which implies \"\n+              \"that the entire cluster is participating in the \"\n+              \"barrier. However, the service disagrees. Check that \"\n+              \"the callers are invoking the barrier with \"\n+              \"the same arguments.\")),\n+          barrier->id, barrier->counter);\n+      PassBarrier(barrier, error);\n+      return false;\n+    }\n+    return true;\n+  }\n+  // Otherwise, check that the specified tasks are correct.\n+  if (barrier->tasks_at_barrier.size() != tasks_args.size() ||\n+      absl::c_any_of(tasks_args, [&](const CoordinatedTask& task) {\n+        return !barrier->tasks_at_barrier.contains(task);\n+      })) {\n+    absl::Status error = MakeBarrierError(\n+        absl::InvalidArgumentError(absl::StrCat(\n+            \"Conflicting tasks specified by different processes for the same \"\n+            \"barrier: \",\n+            BarrierName(*barrier),\n+            \". Check that the callers are invoking the barrier with the same \"\n+            \"arguments.\")),\n+        barrier->id, barrier->counter);\n+    PassBarrier(barrier, error);\n+    return false;\n+  }\n+  // Check if the caller task is included in the list of participating tasks.\n+  if (!barrier->tasks_at_barrier.contains(task)) {\n+    absl::Status error =\n+        MakeBarrierError(absl::InvalidArgumentError(absl::StrCat(\n+                             \"A non-participating task (\", GetTaskName(task),\n+                             \") called the barrier: \", BarrierName(*barrier))),\n+                         barrier->id, barrier->counter);\n+    PassBarrier(barrier, error);\n+    return false;\n+  }\n+  return true;\n+}\n+\n+void CoordinationService::RepeatBarrierResult(BarrierState* barrier,\n+                                              const CoordinatedTask& task) {\n+  BarrierCallback done = barrier->done_callbacks[task];\n+  barrier->done_callbacks.erase(task);\n+  // Special hook for shutdown barrier to disconnect task.\n+  if (barrier->id == shutdown_barrier_id_) {\n+    absl::Status s = DisconnectTask(task);\n+    // Return any errors from the disconnect attempt, otherwise return the\n+    // barrier status outside of this hook.\n+    if (!s.ok()) {\n+      done(s, barrier->counter);\n+      return;\n+    }\n+  }\n+  done(barrier->result, barrier->counter);\n+}\n+\n+void CoordinationService::LeaveOngoingBarriers(const CoordinatedTask& task,\n+                                               absl::string_view reason) {\n+  const std::string task_name = GetTaskName(task);\n+  const std::unique_ptr<TaskState>& task_state = cluster_state_[task_name];\n+  // Unregister recoverable task from ongoing barriers.\n+  if (task_state->IsRecoverable()) {\n+    for (const auto& barrier_id : task_state->GetOngoingBarriers()) {\n+      BarrierState* barrier = &barriers_[barrier_id];\n+      // Unregister task from barrier.\n+      if (kLeaveBarriersOnRecoverableAgentRestart) {\n+        if (barrier->tasks_at_barrier.contains(task)) {\n+          // Remove task from barrier.\n+          barrier->recoverable_tasks_restarted_during_barrier.insert(task);\n+        }\n+      } else {\n+        if (barrier->tasks_at_barrier[task]) {\n+          barrier->tasks_at_barrier[task] = false;\n+          ++barrier->num_pending_tasks;\n+        }\n+      }\n+\n+      // Cancel any pending callbacks.\n+      auto it = barrier->done_callbacks.find(task);\n+      if (it != barrier->done_callbacks.end()) {\n+        it->second(\n+            MakeBarrierError(\n+                absl::CancelledError(absl::StrCat(\n+                    \"Stale barrier invocation is cancelled because:  \", reason,\n+                    \". Barrier: \", BarrierName(*barrier), \" Task \", task_name)),\n+                barrier->id, barrier->counter),\n+            barrier->counter);\n+        barrier->done_callbacks.erase(it);\n+      }\n+    }\n+    return;\n+  }\n+  // Fail ongoing barriers since task is not recoverable.\n+  for (const auto& barrier_id : task_state->GetOngoingBarriers()) {\n+    BarrierState* barrier = &barriers_[barrier_id];\n+    absl::Status error = MakeBarrierError(\n+        absl::InternalError(absl::StrCat(\n+            \"Barrier failed because: \", reason,\n+            \". Barrier Id: \", BarrierName(*barrier), \", Task: \", task_name)),\n+        barrier->id, barrier->counter);\n+    PassBarrier(barrier, error);\n+  }\n+}\n+\n+void CoordinationService::ReachBarrier(BarrierState* barrier,\n+                                       const CoordinatedTask& task) {\n+  // Remove pending task.\n+  // We need to check if task made a repeated call after reaching the\n+  // barrier.\n+  if (!barrier->tasks_at_barrier[task]) {\n+    barrier->tasks_at_barrier[task] = true;\n+    --barrier->num_pending_tasks;\n+\n+    if (barrier->num_pending_tasks == 0) {\n+      // Everyone has reached the barrier!\n+      for (const auto& [task, _] : barrier->tasks_at_barrier) {\n+        // All the tasks in the barrier are now synced, so we can remove them\n+        // from the set of unsynced recoverable jobs.\n+        if (unsynced_recoverable_jobs_.contains(GetTaskName(task))) {\n+          LOG(INFO)\n+              << \"Removing task \" << GetTaskName(task)\n+              << \" from unsynced recoverable jobset, since it is synced now \"\n+              << \" with barrier \" << BarrierName(*barrier);\n+          unsynced_recoverable_jobs_.erase(GetTaskName(task));\n+        }\n+      }\n+      PassBarrier(barrier, absl::OkStatus());\n+      return;\n+    }\n+  }\n+};\n+\n+void CoordinationService::AggregateClusterDevices() {\n+  assert(cluster_devices_.device_size() == 0);\n+  std::vector<CoordinatedTask> ordered_tasks;\n+  // Sort by task name to set deterministic order for cluster devices.\n+  ordered_tasks.reserve(cluster_state_.size());\n+  for (const auto& task : cluster_state_) {\n+    ordered_tasks.push_back(GetTaskFromName(task.first));\n+  }\n+  std::sort(ordered_tasks.begin(), ordered_tasks.end(),\n+            [](const CoordinatedTask& task1, const CoordinatedTask& task2) {\n+              if (task1.job_name() != task2.job_name()) {\n+                return task1.job_name() < task2.job_name();\n+              }\n+              return task1.task_id() < task2.task_id();\n+            });\n+\n+  // Aggregate to global device list.\n+  for (const auto& task : ordered_tasks) {\n+    cluster_devices_.MergeFrom(\n+        cluster_state_[GetTaskName(task)]->GetDeviceInfo());\n+  }\n+\n+  if (post_aggregate_device_fn_ != nullptr) {\n+    cluster_devices_ = post_aggregate_device_fn_(cluster_devices_);\n+  }\n+}\n+\n+void CoordinationService::DisconnectAllNonRecoverableTasks() {\n+  for (const auto& [task_name, state] : cluster_state_) {\n+    if (state->IsRecoverable()) {\n+      // Recoverable tasks will disconnect independently without the\n+      // barrier.\n+      continue;\n+    }\n+    auto s = DisconnectTask(GetTaskFromName(task_name));\n+    if (!s.ok()) {\n+      LOG(ERROR) << \"Failed to disconnect task \" << task_name << \": \" << s;\n+    }\n+  }\n+}\n+\n+std::vector<CoordinatedTask> CoordinationService::GetTasksForShutdownBarrier() {\n+  absl::MutexLock l(state_mu_);\n+  if (shutdown_barrier_tasks_.empty()) {\n+    for (const auto& [task_name, task_state] : cluster_state_) {\n+      if (!task_state->IsRecoverable()) {\n+        shutdown_barrier_tasks_.push_back(GetTaskFromName(task_name));\n+      }\n+    }\n+  }\n+  return shutdown_barrier_tasks_;\n+}\n+\n+void CoordinationService::CompleteShutdownAfterBarrier(\n+    const absl::Status& result, BarrierState* barrier) {\n+  if (result.ok()) {\n+    LOG(INFO) << \"Shutdown barrier in coordination service has passed.\";\n+    DisconnectAllNonRecoverableTasks();\n+  } else {\n+    LOG(ERROR) << \"Shutdown barrier in coordination service has failed:\\n\"\n+               << result\n+               << \"\\nThis suggests that the workers are out of sync. Either at \"\n+                  \"least one worker (a) crashed early due to program error or \"\n+                  \"scheduler events (e.g. preemption, eviction), (b) was \"\n+                  \"too fast in its execution, or (c) too slow / hanging. Check \"\n+                  \"the logs (both the program and scheduler events) for an \"\n+                  \"earlier error to identify the root cause.\";\n+    absl::Status shutdown_error = MakeShutdownBarrierError(result);\n+    // Propagate error to all tasks.\n+    // 1. Identify all straggling tasks.\n+    std::vector<CoordinatedTask> straggling_tasks;\n+    for (const auto& [task, at_barrier] : barrier->tasks_at_barrier) {\n+      if (!at_barrier) {\n+        straggling_tasks.push_back(task);\n+      }\n+    }\n+    // 2. Propagate error to everybody.\n+    PropagateError(shutdown_error, straggling_tasks);\n+    // 3. Set all tasks to error, to prevent unexpected connections from\n+    // restarted tasks.\n+    // This is done after (2) so that all tasks get the shutdown error.\n+    // (already-errored tasks don't receive new notifications)\n+    // For restarted tasks that hit this error, they should retry until\n+    // there is a new service instance.\n+    SetAllTasksError(shutdown_error);\n+  }\n+}\n+\n+bool CoordinationService::isRecoverableJob(\n+    const absl::string_view task_name) const {\n+  return recoverable_jobs_.find(task_name) != recoverable_jobs_.end();\n+}\n+\n+void CoordinationService::SendErrorPollingResponseOrFailAllTasks(\n+    const absl::Status& error) {\n+  CHECK(!error.ok()) << \"SendErrorPollingResponseOrFailAllTasks called with OK \"\n+                        \"status. Should always return an error.\";\n+  // Should be called only when there is no service-to-client connection.\n+  assert(client_cache_ == nullptr);\n+  if (IsClientPollingForError()) {\n+    LOG(ERROR)\n+        << \"Use error polling to propagate the following error to all tasks: \"\n+        << error;\n+    SendErrorPollingResponse(error);\n+  } else {\n+    absl::Status unheard_error =\n+        MakeCoordinationError(absl::InternalError(absl::StrCat(\n+            \"All tasks were set to error because coordination service \"\n+            \"encountered an error, but was unable to inform clients. Error: \",\n+            error.ToString())));\n+    LOG(ERROR) << unheard_error;\n+    SetAllTasksError(unheard_error);\n+  }\n+}\n+\n+bool CoordinationService::IsClientPollingForError() const {\n+  return client_polling_for_error_;\n+}\n+\n+}  // namespace xla"
        },
        {
            "sha": "54fef48fb9f8e8ed53f7a12765c7a8c27fe9d4db",
            "filename": "third_party/xla/xla/pjrt/distributed/coordination/coordination_service.h",
            "status": "added",
            "additions": 695,
            "deletions": 0,
            "changes": 695,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_service.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_service.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_service.h?ref=3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a",
            "patch": "@@ -0,0 +1,695 @@\n+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_PJRT_DISTRIBUTED_COORDINATION_COORDINATION_SERVICE_H_\n+#define XLA_PJRT_DISTRIBUTED_COORDINATION_COORDINATION_SERVICE_H_\n+\n+#include <cstdint>\n+#include <functional>\n+#include <memory>\n+#include <string>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/base/thread_annotations.h\"\n+#include \"absl/container/flat_hash_map.h\"\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/functional/any_invocable.h\"\n+#include \"absl/hash/hash.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/synchronization/mutex.h\"\n+#include \"absl/time/time.h\"\n+#include \"xla/pjrt/distributed/coordination/coordination_client.h\"\n+#include \"xla/pjrt/distributed/coordination/key_value_store.h\"\n+#include \"xla/tsl/lib/gtl/int_type.h\"\n+#include \"xla/tsl/platform/env.h\"\n+#include \"xla/tsl/platform/status.h\"\n+#include \"xla/tsl/protobuf/coordination_config.pb.h\"\n+#include \"xla/tsl/protobuf/coordination_service.pb.h\"\n+#include \"tsl/platform/random.h\"\n+\n+namespace xla {\n+\n+TSL_LIB_GTL_DEFINE_INT_TYPE(IncarnationId, uint64_t);\n+\n+// Coordination service is used for controlling and coordinating distributed\n+// execution in a cluster of multiple tasks.\n+//\n+// When enabled, the service keeps track of cluster configurations and the state\n+// of cluster members. TF runtime and libraries can use it to orchestrate\n+// cluster initialization, check the healthiness of tasks, and propagate error\n+// messages to the cluster.\n+//\n+// Normally, the service should first Start(), then perform the supported\n+// coordination operations, and finally Stop(). When service runs into error or\n+// SetError() is called, all subsequent operations will be in error state.\n+//\n+// CoordinationServiceInterface defines the service interface for distributed\n+// coordination. One instance of the service should be deployed in a cluster,\n+// handling various requests and stores configuration key-value data for the\n+// tasks. Each task interacts with the service through CoordinationServiceAgent.\n+class CoordinationService {\n+ public:\n+  using StatusOrValueCallback =\n+      std::function<void(const absl::StatusOr<absl::string_view>&)>;\n+  using BarrierCallback = std::function<void(const absl::Status&, int64_t)>;\n+  using GetAliveTasksCallback = std::function<void(\n+      const absl::Status&, const std::vector<tensorflow::CoordinatedTask>&,\n+      const std::vector<IncarnationId> incarnations)>;\n+\n+  // Convenience structs to allow using CoordinatedTask as container keys.\n+  struct CoordinatedTaskHash {\n+    uint64_t operator()(const tensorflow::CoordinatedTask& task) const {\n+      return absl::HashOf(task.job_name(), task.task_id());\n+    }\n+  };\n+  struct CoordinatedTaskEqual {\n+    bool operator()(const tensorflow::CoordinatedTask& lhs,\n+                    const tensorflow::CoordinatedTask& rhs) const {\n+      return lhs.job_name() == rhs.job_name() && lhs.task_id() == rhs.task_id();\n+    }\n+  };\n+\n+  using CoordinatedTaskSet =\n+      absl::flat_hash_set<tensorflow::CoordinatedTask, CoordinatedTaskHash,\n+                          CoordinatedTaskEqual>;\n+\n+  static std::unique_ptr<CoordinationService> Create(\n+      tsl::Env* env, const tensorflow::CoordinationServiceConfig& config,\n+      std::unique_ptr<CoordinationClientCache> cache) {\n+    return std::make_unique<CoordinationService>(env, config, std::move(cache));\n+  }\n+\n+  CoordinationService(tsl::Env* env,\n+                      const tensorflow::CoordinationServiceConfig& config,\n+                      std::unique_ptr<CoordinationClientCache> client_cache);\n+\n+  ~CoordinationService() {\n+    absl::MutexLock lock(state_mu_);\n+    Stop();\n+  }\n+\n+  // Register a task to the service.\n+  // Possible service errors:\n+  //   - Internal: Service has shut down.\n+  //   - InvalidArgument: Unexpected task request.\n+  //   - Aborted: (1) task is in error state, or (2) task is in connected state\n+  //       with a different incarnation, indicating that it restarted.\n+  //   - DeadlineExceeded: waited too long for straggler tasks to register.\n+  absl::Status RegisterTask(const tensorflow::CoordinatedTask& task,\n+                            IncarnationId incarnation);\n+  void RegisterTaskAsync(const tensorflow::CoordinatedTask& task,\n+                         IncarnationId incarnation, tsl::StatusCallback done);\n+\n+  // Wait for all tasks to be up and running, and register local device\n+  // info. The callback is invoked when all tasks are up and registered, or some\n+  // error occurs.\n+  // Each task's local devices will be appended in a deterministic order, and\n+  // post-processed by the callback in SetDeviceAggregationFunction() (if set).\n+  void WaitForAllTasks(const tensorflow::CoordinatedTask& task,\n+                       const tensorflow::DeviceInfo& devices,\n+                       tsl::StatusCallback done);\n+\n+  // Disconnects task from the service. If `shutdown_barrier_timeout_in_ms` is\n+  // specified in the config, blocks until all tasks reach the barrier before\n+  // disconnecting together.\n+  // Possible service errors:\n+  //   - Internal: Service has shut down.\n+  //   - InvalidArgument: Unexpected task request.\n+  //   - FailedPrecondition: task has already disconnected.\n+  void ShutdownTaskAsync(const tensorflow::CoordinatedTask& task,\n+                         tsl::StatusCallback done);\n+\n+  // Disconnects task from the service and cleans up its internal error state.\n+  // Possible service errors:\n+  //   - Internal: Service has shut down.\n+  //   - InvalidArgument: Unexpected task request.\n+  //   - FailedPrecondition: task has already disconnected.\n+  absl::Status ResetTask(const tensorflow::CoordinatedTask& task);\n+\n+  // Update the heartbeat timestamp of a task. This should only be invoked on\n+  // the leader of the cluster.\n+  //   - Internal: Service has shut down.\n+  absl::Status RecordHeartbeat(const tensorflow::CoordinatedTask& task,\n+                               IncarnationId incarnation);\n+\n+  // Set a task in error state permanently.\n+  absl::Status ReportTaskError(const tensorflow::CoordinatedTask& task,\n+                               const absl::Status& error);\n+\n+  // Get the state and the error status of the tasks.\n+  std::vector<tensorflow::CoordinatedTaskStateInfo> GetTaskState(\n+      const std::vector<tensorflow::CoordinatedTask>& task);\n+\n+  // Watches the state and the error status of the job.\n+  using WatchJobStateCallback = absl::AnyInvocable<void(\n+      std::vector<tensorflow::CoordinatedTaskStateInfo>, int64_t)>;\n+  void WatchJobState(absl::string_view job_name,\n+                     std::optional<int64_t> version_number,\n+                     WatchJobStateCallback);\n+\n+  // Insert a configuration key-value in the coordination service.\n+  // For now, a key-value can only be inserted once and cannot be updated.\n+  // The key-values are not persisted and will be lost if the leader fails.\n+  absl::Status InsertKeyValue(absl::string_view key, absl::string_view value);\n+  absl::Status InsertKeyValue(absl::string_view key, absl::string_view value,\n+                              bool allow_overwrite);\n+\n+  // Get a configuration key-value from the coordination service. The `done`\n+  // callback is invoked when the key-value becomes available.\n+  void GetKeyValueAsync(absl::string_view key, StatusOrValueCallback done);\n+\n+  // Get a configuration key-value from the coordination service. If the key\n+  // does not exist, return NotFound error.\n+  absl::StatusOr<std::string> TryGetKeyValue(absl::string_view key);\n+\n+  // Increment a configuration key-value by the provided increment. If the key\n+  // does not exist, the value is initialized to 0 and then incremented. The\n+  // result after incrementing is returned.\n+  absl::StatusOr<std::string> IncrementKeyValue(absl::string_view key,\n+                                                int64_t increment);\n+\n+  // Gets all values under a directory (key).\n+  // A value is considered to be in the directory if its key is prefixed with\n+  // the directory. This is not a blocking call. Agent does not need to be\n+  // connected to utilize the distributed key-value store.\n+  std::vector<tensorflow::KeyValueEntry> GetKeyValueDir(\n+      absl::string_view directory_key);\n+\n+  // Delete configuration key-value. If key is a directory, recursively clean\n+  // up all key-values under the directory.\n+  absl::Status DeleteKeyValue(absl::string_view key);\n+\n+  // Blocks until all (or a subset of) tasks are at the barrier or the barrier\n+  // fails.\n+  //\n+  // `barrier_id` should be unique across barriers. Once the barrier has passed\n+  // or failed, subsequent calls will not block, and immediately respond with\n+  // the previous response.\n+  //\n+  // The first WaitAtBarrier() call received by the service for a particular\n+  // barrier id is special in that it determines the barrier deadline based on\n+  // timeout duration.\n+  // However, if subsequent calls by different agents specify a different set of\n+  // `participating_tasks` for the same `barrier_id`, the barrier will fail\n+  // instantly.\n+  //\n+  // If no tasks are specified (default), the barrier will block for all the\n+  // connected tasks.\n+  //\n+  // Possible service errors:\n+  //   - DeadlineExceeded: Timed out waiting for specified tasks at the barrier.\n+  //       Deadline is determined by the server timestamp when it receives the\n+  //       first WaitAtBarrier() + timeout duration.\n+  //   - Cancelled: One of the tasks called CancelBarrier().\n+  //   - Aborted: Service is shutting down.\n+  //   - Internal: (1) Any participating task is in ERROR state, (2)\n+  //       coordination service has shut down, or (3) the barrier request has a\n+  //       mismatched counter, indicating that somebody unexpectedly restarted.\n+  //   - InvalidArgument: (1) Conflicting tasks specified by different agents\n+  //       for the same barrier, (2) one of the participating tasks is not in\n+  //       the cluster, or (3) task making the request is not included in the\n+  //       list of participating tasks.\n+  //   - FailedPrecondition: Agent is in UNINITIALIZED or ERROR state.\n+  // TODO(b/342448688): Allow re-use of ids by specifying different counters.\n+  // The counter field is mostly ignored at the moment with no user-facing\n+  // effect.\n+  void BarrierAsync(\n+      std::string barrier_id, int64_t counter, absl::Duration timeout,\n+      const tensorflow::CoordinatedTask& task,\n+      const std::vector<tensorflow::CoordinatedTask>& participating_tasks,\n+      BarrierCallback done);\n+\n+  // Aborts the barrier if it is ongoing.\n+  // Current and future WaitAtBarrier() calls with the same id will return a\n+  // CANCELLED error status.\n+  // Possible service errors:\n+  //   - FailedPrecondition: Barrier has already been passed.\n+  // TODO(b/342448688): Allow re-use of ids by specifying different counters.\n+  // The counter field is mostly ignored at the moment with no user-facing\n+  // effect.\n+  absl::Status CancelBarrier(std::string barrier_id, int64_t counter,\n+                             const tensorflow::CoordinatedTask& task);\n+\n+  // Returns the set of currently alive tasks. More specifically, given a set of\n+  // tasks T, GetAliveTasks(T) returns the subset T of alive tasks. Note that\n+  // `tasks` must include `requesting_task`.\n+  //\n+  // # Barrier Semantics\n+  //\n+  // If multiple tasks call GetAliveTasks concurrently, it's important that they\n+  // all agree on which tasks are alive. Otherwise, the tasks' behavior might\n+  // diverge. For example, imagine a set of tasks trying to run an AllGather,\n+  // but they all disagree on which tasks should be participating in the\n+  // AllGather. This is buggy.\n+  //\n+  // To ensure that every task agrees on which tasks are alive, the\n+  // GetAliveTasks RPC has barrier-like semantics. Consider an invocation\n+  // GetAliveTasks(T) for a set of tasks T. The invocation acts as a barrier,\n+  // waiting for every task in T to call GetAliveTasks(T). Afterwards,\n+  // GetAliveTasks returns the same set of alive tasks A to all the tasks in T.\n+  // This ensures that every task agrees which tasks are alive.\n+  //\n+  // One small correction. GetAliveTasks doesn't act as a barrier for *every*\n+  // task in T. Some tasks in T might have failed, so we should not wait for\n+  // them. Instead, the GetAliveTasks RPC waits only for the returned tasks A.\n+  //\n+  // # An Example\n+  //\n+  // Imagine we have four tasks: A, B, C, and D. Further imagine that task D\n+  // has failed and that every task calls GetAliveTasks([A, B, C, D]). The\n+  // invocation will return tasks [A, B, C]. The GetAliveTasks call acts as a\n+  // barrier across tasks A, B, and C. Task D, which failed, is ignored.\n+  void GetAliveTasksAsync(const tensorflow::CoordinatedTask& requesting_task,\n+                          const std::vector<tensorflow::CoordinatedTask>& tasks,\n+                          GetAliveTasksCallback done);\n+\n+  // Gets error from the coordination service. Block until the service\n+  // returns an error or the task/service is shutdown. This should never be used\n+  // when there is service to client connection (i.e. `CoordinationClientCache`\n+  // is passed in during construction).\n+  //\n+  // The first call to this function will trigger the error polling mode in the\n+  // coordination service, so once an error occurs after the first call, the\n+  // service will use the error polling mode to propagate the error to all\n+  // connected tasks instead of simply shutting down.\n+  void PollForErrorAsync(const tensorflow::CoordinatedTask& task,\n+                         tsl::StatusCallback done);\n+\n+ private:\n+  friend class CoordinationServiceRpcHandler;\n+  friend class CoordinationServiceTest_ListClusterDevices_TfDevice_Test;\n+  friend class CoordinationServiceTest_ListClusterDevices_XlaDevice_Test;\n+  friend class\n+      CoordinationServiceTest_ListClusterDevices_DevicesAreNotAddedTwice_Test;\n+\n+  void LogConnectStatusLocked() const ABSL_EXCLUSIVE_LOCKS_REQUIRED(state_mu_);\n+\n+  const tensorflow::DeviceInfo& ListClusterDevices()\n+      ABSL_EXCLUSIVE_LOCKS_REQUIRED(state_mu_);\n+  IncarnationId GetServiceIncarnation();\n+  void BarrierAsyncLocked(\n+      absl::string_view barrier_id, int64_t counter, absl::Duration timeout,\n+      const tensorflow::CoordinatedTask& task,\n+      const std::vector<tensorflow::CoordinatedTask>& participating_tasks,\n+      BarrierCallback done) ABSL_EXCLUSIVE_LOCKS_REQUIRED(state_mu_);\n+  BarrierCallback ConnectAfterBarrierPasses(absl::string_view task_name,\n+                                            IncarnationId incarnation,\n+                                            tsl::StatusCallback done);\n+  // Connects a task to the service, and leaves any previously ongoing barriers\n+  // for recoverable tasks.\n+  void ConnectTask(const tensorflow::CoordinatedTask& task,\n+                   IncarnationId incarnation)\n+      ABSL_EXCLUSIVE_LOCKS_REQUIRED(state_mu_);\n+  // Checks if any task has stopped sending heartbeats.\n+  void CheckHeartbeatTimeout();\n+  // Checks if any barrier has timed out.\n+  void CheckBarrierTimeout();\n+  // Checks both heartbeat and barrier timeouts. Use a single function so they\n+  // can be run in the same thread as threads are a constrained resource.\n+  void CheckStaleness();\n+  // Starts a thread to check staleness.\n+  void StartCheckStaleness();\n+  void Stop() ABSL_EXCLUSIVE_LOCKS_REQUIRED(state_mu_);\n+  bool ServiceHasStopped() const ABSL_EXCLUSIVE_LOCKS_REQUIRED(state_mu_);\n+  // Report error from a task to all other connected tasks if the task is not\n+  // recoverable.\n+  // Note: SetTaskError() must be called before propagating its error.\n+  void PropagateError(\n+      const absl::Status& error,\n+      const std::vector<tensorflow::CoordinatedTask>& source_tasks,\n+      bool is_reported_by_task = false)\n+      ABSL_EXCLUSIVE_LOCKS_REQUIRED(state_mu_);\n+  void PropagateError(const absl::Status& error,\n+                      const std::vector<absl::string_view>& source_task_names,\n+                      bool is_reported_by_task = false)\n+      ABSL_EXCLUSIVE_LOCKS_REQUIRED(state_mu_);\n+  // Checks if all tasks are from recoverable jobs.\n+  bool AllTasksAreRecoverable(\n+      const std::vector<tensorflow::CoordinatedTask>& tasks)\n+      ABSL_EXCLUSIVE_LOCKS_REQUIRED(state_mu_);\n+  void SetTaskError(absl::string_view task_name, const absl::Status& error)\n+      ABSL_EXCLUSIVE_LOCKS_REQUIRED(state_mu_);\n+  // Used for cluster-wide errors (e.g. register or shutdown barrier fails).\n+  void SetAllTasksError(const absl::Status& error)\n+      ABSL_EXCLUSIVE_LOCKS_REQUIRED(state_mu_);\n+  absl::Status DisconnectTask(const tensorflow::CoordinatedTask& task)\n+      ABSL_EXCLUSIVE_LOCKS_REQUIRED(state_mu_);\n+  void DisconnectAllNonRecoverableTasks()\n+      ABSL_EXCLUSIVE_LOCKS_REQUIRED(state_mu_);\n+  std::vector<tensorflow::CoordinatedTask> GetTasksForShutdownBarrier();\n+\n+  struct BarrierState {\n+    std::string id = \"\";\n+    // Counter is incremented for each new barrier using the same id.\n+    // No two barriers with the same id (and different counters) can be ongoing\n+    // at the same time.\n+    int64_t counter = 0;\n+    bool passed = false;\n+    absl::Status result = absl::UnknownError(\n+        \"Invalid barrier result.\");  // Only valid if `passed` is true.\n+    uint64_t deadline_in_micros = 0;\n+    int num_pending_tasks = 0;\n+    // Specifies which tasks have called the barrier so far.\n+    absl::flat_hash_map<tensorflow::CoordinatedTask, bool, CoordinatedTaskHash,\n+                        CoordinatedTaskEqual>\n+        tasks_at_barrier;\n+    absl::flat_hash_set<tensorflow::CoordinatedTask, CoordinatedTaskHash,\n+                        CoordinatedTaskEqual>\n+        recoverable_tasks_restarted_during_barrier;\n+    absl::flat_hash_map<tensorflow::CoordinatedTask, BarrierCallback,\n+                        CoordinatedTaskHash, CoordinatedTaskEqual>\n+        done_callbacks;\n+    // Specifies the task that initiated the barrier (the first task to call the\n+    // barrier).\n+    tensorflow::CoordinatedTask initiating_task;\n+  };\n+  bool BarrierIsUninitialized(const BarrierState& barrier) {\n+    return barrier.id.empty() && barrier.counter == 0 && !barrier.passed &&\n+           barrier.deadline_in_micros == 0 && barrier.num_pending_tasks == 0;\n+  }\n+  std::string BarrierName(absl::string_view barrier_id, int64_t counter) {\n+    return absl::StrCat(barrier_id, \"::\", counter);\n+  }\n+  std::string BarrierName(const BarrierState& barrier) {\n+    return BarrierName(barrier.id, barrier.counter);\n+  }\n+  // Initializes a new barrier. Returns false if the barrier should fail\n+  // immediately.\n+  absl::Status InitializeBarrier(\n+      BarrierState* barrier, absl::string_view barrier_id, int64_t counter,\n+      absl::Duration timeout, const tensorflow::CoordinatedTask& task,\n+      const std::vector<tensorflow::CoordinatedTask>& participating_tasks)\n+      ABSL_EXCLUSIVE_LOCKS_REQUIRED(state_mu_);\n+  // Initialize `BarrierState`'s tasks_at_barrier map.\n+  absl::Status InitializeTasksAtBarrier(\n+      BarrierState* barrier,\n+      const std::vector<tensorflow::CoordinatedTask>& participating_tasks)\n+      ABSL_EXCLUSIVE_LOCKS_REQUIRED(state_mu_);\n+  // Adds a callback to be called when the barrier is done.\n+  // If there is an existing callback for that task, it will be overwritten,\n+  // cancelling the previous callback.\n+  void AddBarrierCallback(BarrierState* barrier,\n+                          const tensorflow::CoordinatedTask& task,\n+                          BarrierCallback done)\n+      ABSL_EXCLUSIVE_LOCKS_REQUIRED(state_mu_);\n+  // Ends the barrier with a result (ok or error).\n+  void PassBarrier(BarrierState* barrier, const absl::Status& result)\n+      ABSL_EXCLUSIVE_LOCKS_REQUIRED(state_mu_);\n+  // A task reaches the barrier.\n+  void ReachBarrier(BarrierState* barrier,\n+                    const tensorflow::CoordinatedTask& task)\n+      ABSL_EXCLUSIVE_LOCKS_REQUIRED(state_mu_);\n+  void FailBarrierWithCounterMismatch(BarrierState* barrier, int64_t counter)\n+      ABSL_EXCLUSIVE_LOCKS_REQUIRED(state_mu_);\n+  // Propagates same result back to task.\n+  void RepeatBarrierResult(BarrierState* barrier,\n+                           const tensorflow::CoordinatedTask& task)\n+      ABSL_EXCLUSIVE_LOCKS_REQUIRED(state_mu_);\n+  // Leaves any ongoing barriers.\n+  // If the task is non-recoverable, the barrier exits with an error.\n+  // If the task is recoverable, the barrier will 'unregister' a task and allow\n+  // it to join back again later before the timeout.\n+  void LeaveOngoingBarriers(const tensorflow::CoordinatedTask& task,\n+                            absl::string_view reason)\n+      ABSL_EXCLUSIVE_LOCKS_REQUIRED(state_mu_);\n+  // Post-barrier hook to connect all tasks.\n+  void ConnectAllTasks() ABSL_EXCLUSIVE_LOCKS_REQUIRED(state_mu_);\n+  // Post-barrier hook to aggregate device info.\n+  void AggregateClusterDevices() ABSL_EXCLUSIVE_LOCKS_REQUIRED(state_mu_);\n+  // Post-shutdown barrier hook to disconnect tasks that acked and propagate\n+  // errors to those that have not.\n+  void CompleteShutdownAfterBarrier(const absl::Status& result,\n+                                    BarrierState* barrier)\n+      ABSL_EXCLUSIVE_LOCKS_REQUIRED(state_mu_);\n+  // Checks if the participating tasks are specified correctly across barrier\n+  // calls and that the caller task is one of the participating tasks.\n+  bool ValidateTaskArgs(\n+      BarrierState* barrier, const tensorflow::CoordinatedTask& caller_task,\n+      const std::vector<tensorflow::CoordinatedTask>& tasks_args)\n+      ABSL_EXCLUSIVE_LOCKS_REQUIRED(state_mu_);\n+  bool isRecoverableJob(absl::string_view task_name) const;\n+  // Sends responses to error polling requests when an error is encountered.\n+  void SendErrorPollingResponse(const absl::Status& error)\n+      ABSL_EXCLUSIVE_LOCKS_REQUIRED(state_mu_);\n+  // Responds to error polling or fails all tasks when an error is\n+  // encountered. Should only be called when there is no service to client\n+  // connection.\n+  void SendErrorPollingResponseOrFailAllTasks(const absl::Status& error)\n+      ABSL_EXCLUSIVE_LOCKS_REQUIRED(state_mu_);\n+  // Returns whether the clients are polling for error from the service. If the\n+  // clients are not polling for error from the service, the service should stop\n+  // when there is an error. Otherwise, the service should not stop.\n+  bool IsClientPollingForError() const ABSL_EXCLUSIVE_LOCKS_REQUIRED(state_mu_);\n+\n+  // Checks if the barrier can be passed, if recoverable tasks reconnected or\n+  // disconnected to the service while barrier is ongoing.\n+  // This is only applicable if leave_barriers_on_recoverable_agent_restart flag\n+  // is set to true.\n+  void CheckBarrierStatusWithRecoverableTasks();\n+\n+  // Returns a map of ongoing barriers to count of unsynced tasks waiting on\n+  // other barriers.\n+  absl::flat_hash_map<std::string, int> GetCountOfOutOfSyncTasksPerBarrier();\n+\n+  class ErrorPollingState {\n+   public:\n+    // Returns whether the error polling requests have been responded.\n+    bool Responded() const { return responded_; }\n+    // Sets the error and executes the status callbacks.\n+    void SetError(const absl::Status& error);\n+    // Gets the error that is propagated to the agents.\n+    const absl::Status& GetError() const { return error_; }\n+    // Returns true if the task has sent request to poll for error from the\n+    // service.\n+    bool IsTaskPolling(absl::string_view task_name) const {\n+      return polling_task_names_.contains(task_name);\n+    }\n+    // Adds a task to the error polling state.\n+    void AddTask(const tensorflow::CoordinatedTask& task,\n+                 tsl::StatusCallback&& done);\n+\n+    // Removes a task from the error polling state.\n+    // If an existing polling request is present, we will invoke the callback\n+    // with the `reason` argument.\n+    // Note: for disconnected tasks, this does not actually propagate the error\n+    // back, but prevents memory leaks by removing stale callbacks.\n+    void RemoveTask(const tensorflow::CoordinatedTask& task,\n+                    absl::string_view reason);\n+\n+   private:\n+    bool responded_ = false;\n+    absl::Status error_ = absl::OkStatus();\n+    absl::flat_hash_map<tensorflow::CoordinatedTask, tsl::StatusCallback,\n+                        CoordinatedTaskHash, CoordinatedTaskEqual>\n+        done_callbacks_;\n+    absl::flat_hash_set<std::string> polling_task_names_;\n+  };\n+\n+  class TaskState {\n+   public:\n+    // Task state maintained on the coordination service side.\n+    // State transition:\n+    //                Register           Heartbeat\n+    //   DISCONNECTED -------> CONNECTED --------> ERROR (timeout)\n+    //                              |   ReportError\n+    //                              +--------------> ERROR\n+    //\n+    // When task state becomes ERROR, propagate this status to other CONNECTED\n+    // tasks in the cluster.\n+\n+    explicit TaskState(absl::string_view task) { task_name_ = task; }\n+\n+    tensorflow::CoordinatedTaskState GetState() const { return state_; }\n+    absl::Status GetStatus() const { return status_; }\n+    bool IsRecoverable() const { return recoverable_; }\n+    void SetRecoverable(bool recoverable) { recoverable_ = recoverable; }\n+    IncarnationId GetTaskIncarnation() const { return task_incarnation_; }\n+    void SetTaskIncarnation(IncarnationId task_incarnation) {\n+      task_incarnation_ = task_incarnation;\n+    }\n+    void Connect() {\n+      SetConnected(task_incarnation_);\n+      LOG(INFO) << task_name_\n+                << \" has connected to coordination service. Incarnation: \"\n+                << task_incarnation_;\n+    }\n+    void SetConnected(IncarnationId task_incarnation);\n+    void Disconnect(uint64_t grace_period_duration_us);\n+    absl::Status RecordHeartbeat(IncarnationId task_incarnation);\n+    int64_t TimeSinceLastHeartbeatMs();\n+    // Sets the error and returns true if the task state is not ERROR.\n+    // Otherwise, don't overwrite the error and return false.\n+    bool SetError(const absl::Status& status);\n+    tensorflow::DeviceInfo GetDeviceInfo() { return devices_; }\n+    void CollectDeviceInfo(const tensorflow::DeviceInfo& devices) {\n+      devices_ = devices;\n+    }\n+    // Checks if task has called WaitForAllTasks() previously, which gathers the\n+    // local device info.\n+    bool DeviceInfoIsCollected() { return !devices_.device().empty(); }\n+\n+    // This is used to propagate state changes (disconnect, error) to ongoing\n+    // barriers.\n+    absl::flat_hash_set<std::string> GetOngoingBarriers();\n+    // The task has a new ongoing barrier. This does not mean that it has\n+    // reached the barrier.\n+    void JoinBarrier(absl::string_view barrier_id);\n+    // The task has exited a barrier (because a barrier has passed).\n+    void ExitBarrier(absl::string_view barrier_id);\n+    // Returns true if the task has been disconnected beyond the grace period\n+    // and no further agent requests are expected. Note that the grace period\n+    // accounts for the lag time between the service recording the state change\n+    // and the agent stopping heartbeats/error polling.\n+    bool IsDisconnectedBeyondGracePeriod();\n+\n+   private:\n+    std::string task_name_;\n+    // Incarnation ID for CPU:0 on remote task.\n+    IncarnationId task_incarnation_{0};\n+\n+    tensorflow::CoordinatedTaskState state_ =\n+        tensorflow::CoordinatedTaskState::TASKSTATE_DISCONNECTED;\n+    absl::Status status_;\n+    absl::Mutex last_heartbeat_mu_;\n+    uint64_t last_heartbeat_us_ ABSL_GUARDED_BY(last_heartbeat_mu_);\n+    // This denotes the deadline after which we stop accepting heartbeats or\n+    // error polling requests from a disconnected task. This grace period\n+    // accounts for the lag time between the service recording the state change\n+    // and the agent stopping heartbeats/error polling.\n+    uint64_t disconnect_grace_period_us_ = 0;\n+    tensorflow::DeviceInfo devices_;\n+    // For now, we assume there won't be many simultaneous barriers so we simply\n+    // use a set.\n+    absl::flat_hash_set<std::string> ongoing_barriers_for_task_;\n+    // TODO(b/342448688): Re-use config's recoverable jobs instead.\n+    bool recoverable_ = false;\n+  };\n+\n+  // AlivenessState tracks the state of pending GetAliveTasks calls.\n+  struct AlivenessState {\n+    // All tasks that can participate in the GetAliveTasks barrier.\n+    CoordinatedTaskSet tasks;\n+    // All tasks currently blocked on the barrier.\n+    CoordinatedTaskSet in_barrier;\n+    // Done callbacks for the tasks blocked on the barrier.\n+    std::vector<GetAliveTasksCallback> dones;\n+  };\n+\n+  // Returns the set of alive tasks drawn from the provided set of tasks.\n+  CoordinatedTaskSet AliveTasks(const CoordinatedTaskSet& tasks) const\n+      ABSL_EXCLUSIVE_LOCKS_REQUIRED(state_mu_);\n+\n+  // Returns the incarnation ids of the provided tasks, in the same order.\n+  std::vector<IncarnationId> IncarnationIds(\n+      absl::Span<const tensorflow::CoordinatedTask> tasks) const\n+      ABSL_EXCLUSIVE_LOCKS_REQUIRED(state_mu_);\n+\n+  // Refreshes the AlivenessStates of all pending GetAliveTasks call,\n+  // potentially finishing some of the pending calls. The AlivenessStates should\n+  // be refreshed, for example, after a task has failed.\n+  void RefreshAliveness() ABSL_EXCLUSIVE_LOCKS_REQUIRED(state_mu_);\n+\n+  static tensorflow::CoordinatedTaskStateInfo CreateTaskStateInfo(\n+      const tensorflow::CoordinatedTask& task, const TaskState& state);\n+\n+  // Gets the task states for the provided job.\n+  std::vector<tensorflow::CoordinatedTaskStateInfo> GetJobState(\n+      absl::string_view job) ABSL_EXCLUSIVE_LOCKS_REQUIRED(state_mu_);\n+\n+  // Notifies all callbacks registered via WatchJobState.\n+  void NotifyWatchJobStateCallbacks() ABSL_EXCLUSIVE_LOCKS_REQUIRED(state_mu_);\n+\n+  // This method should be called whenever the cluster state changes in a way\n+  // such that NotifyWatchJobStateCallbacks should be called.\n+  void ClusterStateUpdated() ABSL_EXCLUSIVE_LOCKS_REQUIRED(state_mu_);\n+\n+  std::unique_ptr<CoordinationClientCache> client_cache_;\n+  tsl::Env& env_;\n+  const IncarnationId service_incarnation_{tsl::random::New64()};\n+  const uint64_t heartbeat_timeout_ms_;\n+  bool cluster_register_with_barrier_ = false;\n+  const absl::Duration cluster_register_timeout_;\n+  const absl::Duration shutdown_barrier_timeout_;\n+  // If a task restarts with a new incarnation, we may allow it to reconnect\n+  // silently if configured. This is useful when we know that a task can\n+  // immediately resume work upon re-connecting to the service.\n+  bool allow_new_incarnation_to_reconnect_ = false;\n+\n+  std::function<tensorflow::DeviceInfo(const tensorflow::DeviceInfo& devices)>\n+      post_aggregate_device_fn_;\n+\n+  const std::string device_propagation_barrier_id_ =\n+      absl::StrCat(\"WaitForAllTasks::\", service_incarnation_.value());\n+  const std::string shutdown_barrier_id_ =\n+      absl::StrCat(\"Shutdown::\", service_incarnation_.value());\n+  std::vector<tensorflow::CoordinatedTask> shutdown_barrier_tasks_\n+      ABSL_GUARDED_BY(state_mu_);\n+\n+  absl::Mutex state_mu_;\n+  absl::flat_hash_map<std::string, std::unique_ptr<TaskState>> cluster_state_\n+      ABSL_GUARDED_BY(state_mu_);\n+  int64_t cluster_state_version_number_ ABSL_GUARDED_BY(state_mu_) = 0;\n+  std::vector<std::tuple<std::string, WatchJobStateCallback>>\n+      watch_job_state_callbacks_ ABSL_GUARDED_BY(state_mu_);\n+  tensorflow::DeviceInfo cluster_devices_ ABSL_GUARDED_BY(state_mu_);\n+\n+  KeyValueStore store_;\n+\n+  absl::flat_hash_map<std::string, BarrierState> barriers_\n+      ABSL_GUARDED_BY(state_mu_);\n+  // For now, we assume there won't be many simultaneous barriers so we simply\n+  // use a set.\n+  absl::flat_hash_set<std::string> ongoing_barriers_ ABSL_GUARDED_BY(state_mu_);\n+\n+  // The state of all pending GetAliveTasks calls.\n+  std::vector<AlivenessState> aliveness_states_ ABSL_GUARDED_BY(state_mu_);\n+\n+  absl::flat_hash_set<std::string> recoverable_jobs_;\n+\n+  // When the tasks connect to coordination service after cluster initialization\n+  // is done, they will be added to this set.\n+  // Tasks connecting after cluster initialization indicate that they\n+  // reconnected to the service due to preemption or restart.\n+  // Unsynced recoverable tasks will be excluded from the barrier check after\n+  // the first cluster initialization.\n+  // The service will remove them from the set when the tasks pass a\n+  // barrier with other tasks.\n+  absl::flat_hash_set<std::string> unsynced_recoverable_jobs_\n+      ABSL_GUARDED_BY(state_mu_);\n+  // Whether the agents are polling for error from the service. It will be set\n+  // to true when the service sees the first error polling request. Once set to\n+  // true, the value will never change back to false.\n+  bool client_polling_for_error_ ABSL_GUARDED_BY(state_mu_) = false;\n+  ErrorPollingState error_polling_state_ ABSL_GUARDED_BY(state_mu_);\n+\n+  absl::CondVar check_staleness_thread_cv_;\n+  bool shutting_down_ ABSL_GUARDED_BY(state_mu_) = false;\n+  // Note: sequence matters here, we must destroy the staleness thread before\n+  // the other state related to barriers and heartbeats to prevent illegal\n+  // memory access.\n+  std::unique_ptr<tsl::Thread> check_staleness_thread_;\n+\n+  CoordinationService(const CoordinationService&) = delete;\n+  void operator=(const CoordinationService&) = delete;\n+};\n+\n+}  // namespace xla\n+\n+#endif  // XLA_PJRT_DISTRIBUTED_COORDINATION_COORDINATION_SERVICE_H_"
        },
        {
            "sha": "c44247f87a569ae80ad63469e0b418f184a80186",
            "filename": "third_party/xla/xla/pjrt/distributed/coordination/coordination_service_agent.cc",
            "status": "added",
            "additions": 1096,
            "deletions": 0,
            "changes": 1096,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_service_agent.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_service_agent.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_service_agent.cc?ref=3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a",
            "patch": "@@ -0,0 +1,1096 @@\n+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/pjrt/distributed/coordination/coordination_service_agent.h\"\n+\n+#include <algorithm>\n+#include <cassert>\n+#include <cstdint>\n+#include <functional>\n+#include <iterator>\n+#include <map>\n+#include <memory>\n+#include <optional>\n+#include <random>\n+#include <string>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/container/flat_hash_map.h\"\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/functional/bind_front.h\"\n+#include \"absl/log/check.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/strings/numbers.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/str_format.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/strings/substitute.h\"\n+#include \"absl/synchronization/mutex.h\"\n+#include \"absl/synchronization/notification.h\"\n+#include \"absl/time/clock.h\"\n+#include \"absl/time/time.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/pjrt/distributed/coordination/coordination_client.h\"\n+#include \"xla/pjrt/distributed/coordination/coordination_service.h\"\n+#include \"xla/pjrt/distributed/coordination/coordination_service_error_util.h\"\n+#include \"xla/tsl/distributed_runtime/call_options.h\"\n+#include \"xla/tsl/framework/cancellation.h\"\n+#include \"xla/tsl/lib/monitoring/gauge.h\"\n+#include \"xla/tsl/platform/env.h\"\n+#include \"xla/tsl/platform/status.h\"\n+#include \"xla/tsl/protobuf/coordination_config.pb.h\"\n+#include \"xla/tsl/protobuf/coordination_service.pb.h\"\n+\n+namespace xla {\n+using tensorflow::CoordinatedTask;\n+using tensorflow::CoordinatedTaskState;\n+using tensorflow::CoordinatedTaskStateInfo;\n+using tensorflow::CoordinationServiceConfig;\n+using tensorflow::DeviceInfo;\n+using tensorflow::KeyValueEntry;\n+\n+namespace {\n+\n+auto* enabled_usage_metric = tsl::monitoring::Gauge<bool, 0>::New(\n+    \"/coordination_service/agent/enabled\",\n+    \"Tracks usage of coordination service.\");\n+\n+constexpr absl::Duration kDefaultClusterRegisterTimeout = absl::Hours(1);\n+constexpr absl::Duration kDefaultHeartbeatTimeout = absl::Seconds(10);\n+constexpr absl::Duration kDefaultShutdownTimeout = absl::Seconds(10);\n+constexpr char kHeartbeatThread[] = \"CoordinationServiceHeartbeatLoop\";\n+\n+}  // namespace\n+\n+absl::Status CoordinationServiceAgent::Initialize(\n+    tsl::Env* env, absl::string_view job_name, int task_id,\n+    const CoordinationServiceConfig& configs,\n+    std::unique_ptr<CoordinationClient> leader_client,\n+    tsl::StatusCallback error_fn) {\n+  return Initialize(env, job_name, task_id, configs, std::move(leader_client),\n+                    error_fn,\n+                    /*recoverable=*/false);\n+}\n+\n+absl::Status CoordinationServiceAgent::Initialize(\n+    tsl::Env* env, absl::string_view job_name, int task_id,\n+    const CoordinationServiceConfig& configs,\n+    std::unique_ptr<CoordinationClient> leader_client,\n+    tsl::StatusCallback error_fn, bool recoverable) {\n+  CoordinatedTask task;\n+  task.set_job_name(std::string(job_name));\n+  task.set_task_id(task_id);\n+  if (recoverable) {\n+    LOG(WARNING)\n+        << \"Using experimental recoverable task feature. The default shutdown \"\n+           \"barrier will only block non-recoverable tasks. If a synchronized \"\n+           \"shutdown is desired, the user / library should invoke \"\n+           \"`WaitAtBarrier` explicitly at the end of the program.\";\n+    task.set_recoverable(true);\n+  }\n+  return Initialize(env, task, configs, std::move(leader_client), error_fn);\n+}\n+\n+absl::Status CoordinationServiceAgent::Initialize(\n+    tsl::Env* env, const CoordinatedTask& task,\n+    const CoordinationServiceConfig& configs,\n+    std::unique_ptr<CoordinationClient> leader_client,\n+    tsl::StatusCallback error_fn) {\n+  enabled_usage_metric->GetCell()->Set(true);\n+  absl::MutexLock l(state_mu_);\n+  if (state_ != CoordinatedTaskState::TASKSTATE_UNINITIALIZED) {\n+    return MakeCoordinationError(absl::FailedPreconditionError(\n+        \"Coordination service agent has already been initialized.\"));\n+  }\n+\n+  env_ = env;\n+  task_ = task;\n+  configs_ = configs;\n+  if (configs_.service_leader().empty()) {\n+    return MakeCoordinationError(absl::InvalidArgumentError(\n+        \"CoordinationServiceAgent must be initialized with a valid leader.\"));\n+  }\n+  leader_client_ = std::move(leader_client);\n+  if (leader_client_ == nullptr) {\n+    return MakeCoordinationError(absl::InvalidArgumentError(\n+        \"CoordinationServiceAgent must have a valid leader client.\"));\n+  }\n+  error_fn_ = error_fn;\n+  state_ = CoordinatedTaskState::TASKSTATE_DISCONNECTED;\n+  return absl::OkStatus();\n+}\n+\n+bool CoordinationServiceAgent::IsInitialized() {\n+  absl::MutexLock l(state_mu_);\n+  return state_ != CoordinatedTaskState::TASKSTATE_UNINITIALIZED;\n+}\n+\n+bool CoordinationServiceAgent::IsConnected() {\n+  absl::MutexLock l(state_mu_);\n+  return state_ == CoordinatedTaskState::TASKSTATE_CONNECTED;\n+}\n+\n+bool CoordinationServiceAgent::IsError() {\n+  absl::MutexLock l(state_mu_);\n+  return state_ == CoordinatedTaskState::TASKSTATE_ERROR;\n+}\n+\n+void CoordinationServiceAgent::StopHeartbeat() {\n+  {\n+    absl::MutexLock l(shutdown_mu_);\n+    shutting_down_ = true;\n+  }\n+  heartbeat_thread_ = nullptr;\n+}\n+\n+void CoordinationServiceAgent::StopErrorPolling() {\n+  // Cancel pending error polling RPC call.\n+  error_polling_cancellation_manager_->StartCancel();\n+}\n+\n+void CoordinationServiceAgent::ResetCancellationManager() {\n+  error_polling_cancellation_manager_ =\n+      std::make_unique<tsl::CancellationManager>();\n+}\n+\n+absl::Status CoordinationServiceAgent::Connect() {\n+  VLOG(3) << \"Agent has started trying to Connect().\";\n+  {\n+    absl::MutexLock l(state_mu_);\n+    if (state_ != CoordinatedTaskState::TASKSTATE_DISCONNECTED) {\n+      return MakeCoordinationError(absl::FailedPreconditionError(\n+          \"Coordination service agent is not in DISCONNECTED state.\"));\n+    }\n+  }\n+  absl::Status connect_status =\n+      absl::UnknownError(\"Connection not attempted yet.\");\n+  RegisterTaskRequest request;\n+  *request.mutable_source_task() = task_;\n+  request.set_incarnation(incarnation_id_.value());\n+  RegisterTaskResponse response;\n+\n+  const int64_t register_timeout =\n+      configs_.cluster_register_timeout_in_ms() > 0\n+          ? configs_.cluster_register_timeout_in_ms()\n+          : absl::ToInt64Milliseconds(kDefaultClusterRegisterTimeout);\n+  // Give 5 seconds for any service-related timeouts to propagate.\n+  const absl::Time deadline =\n+      absl::Now() + absl::Milliseconds(register_timeout) + absl::Seconds(5);\n+  int attempt = 0;\n+  std::default_random_engine generator;\n+  std::uniform_real_distribution<double> distribution(0.0, 1.0);\n+\n+  do {\n+    ++attempt;\n+    tsl::CallOptions call_opts;\n+    call_opts.SetTimeout(absl::ToInt64Milliseconds(deadline - absl::Now()));\n+    absl::Notification n;\n+    leader_client_->RegisterTaskAsync(\n+        &call_opts, &request, &response, [&](const absl::Status& s) {\n+          if (s.ok()) {\n+            leader_incarnation_ = response.leader_incarnation();\n+            {\n+              absl::MutexLock l(state_mu_);\n+              state_ = CoordinatedTaskState::TASKSTATE_CONNECTED;\n+            }\n+          }\n+          connect_status = s;\n+          n.Notify();\n+        });\n+    n.WaitForNotification();\n+\n+    if (!connect_status.ok()) {\n+      // Exponential backoff with jitter. Note we will retry for `init_timeout`\n+      // time in total; the `14` here corresponds to an ~16s maximum interval\n+      // between connection attempts.\n+      const int backoff = 1 << std::min(14, attempt);\n+      absl::SleepFor(absl::Milliseconds(backoff * distribution(generator)));\n+    }\n+  } while (!connect_status.ok() && absl::Now() < deadline &&\n+           // Retries are attempted for:\n+           // 1. RPC errors.\n+           // 2. aborted duplicate task registration error - this means that\n+           // this task restarted and is trying to reconnect but the service\n+           // has not restarted yet.\n+           // 3. service has not been enabled - this could happen in the single\n+           // client scenario, where the server has been started but the service\n+           // cannot be used yet (nullptr). Presumably the service is in the\n+           // process of being enabled.\n+           (connect_status.GetPayload(CoordinationErrorPayloadKey()) ==\n+                std::nullopt ||\n+            absl::IsAborted(connect_status) ||\n+            absl::IsInternal(connect_status)));\n+  if (!connect_status.ok()) {\n+    SetError(connect_status);\n+    return connect_status;\n+  }\n+\n+  LOG(INFO) << \"Coordination agent has successfully connected.\";\n+  heartbeat_thread_.reset(env_->StartThread(\n+      tsl::ThreadOptions(), kHeartbeatThread,\n+      absl::bind_front(&CoordinationServiceAgent::StartSendingHeartbeats,\n+                       this)));\n+  if (configs_.poll_for_error_from_service_at_startup()) {\n+    StartPollingForError();\n+  }\n+  return absl::OkStatus();\n+}\n+\n+void CoordinationServiceAgent::StartSendingHeartbeats() {\n+  HeartbeatRequest request;\n+  *request.mutable_source_task() = task_;\n+  request.set_incarnation(incarnation_id_.value());\n+  HeartbeatResponse response;\n+  const int64_t heartbeat_interval_ms =\n+      configs_.heartbeat_timeout_in_ms() > 0\n+          ? configs_.heartbeat_timeout_in_ms() / 2\n+          : absl::ToInt64Milliseconds(kDefaultHeartbeatTimeout) / 2;\n+  tsl::CallOptions call_opts;\n+  call_opts.SetTimeout(heartbeat_interval_ms);\n+\n+  while (true) {\n+    absl::Status status;\n+    absl::Notification n;\n+    // Heartbeat RPC implementation automatically retries to tolerate\n+    // transient network failures.\n+    VLOG(10) << \"HeartbeatRequest: \" << request.DebugString();\n+    leader_client_->HeartbeatAsync(&call_opts, &request, &response,\n+                                   [&](const absl::Status& s) {\n+                                     status = s;\n+                                     n.Notify();\n+                                   });\n+    n.WaitForNotification();\n+    VLOG(10) << \"HeartbeatResponse: \" << status;\n+    if (!status.ok()) {\n+      // Ignore heartbeat errors and exit thread if shutting down. For\n+      // example, the agent may send a heartbeat right after Shutdown()\n+      // started, but before StopHeartbeat() and end of Shutdown(). This\n+      // results in an unexpected heartbeat error.\n+      // Waiting for a second allows us to identify if errors are due to\n+      // inflight heartbeats sent during shutdown and can be ignored.\n+      absl::SleepFor(absl::Seconds(1));\n+      {\n+        absl::MutexLock l(shutdown_mu_);\n+\n+        if (shutting_down_) {\n+          return;\n+        }\n+      }\n+      SetError(status);\n+    } else if (response.leader_incarnation() != leader_incarnation_) {\n+      SetError(MakeCoordinationError(absl::AbortedError(\n+          \"Leader incarnation ID mismatch: the coordination  leader \"\n+          \"(usually slice 0 task 0) has restarted. Check for earlier \"\n+          \"errors or any scheduler events (e.g. preemption, eviction) to \"\n+          \"debug further.\")));\n+    }\n+    // Send next heartbeat after an interval.\n+    {\n+      absl::MutexLock l(shutdown_mu_);\n+      shutdown_mu_.AwaitWithTimeout(absl::Condition(&shutting_down_),\n+                                    absl::Milliseconds(heartbeat_interval_ms));\n+      if (shutting_down_) {\n+        return;\n+      }\n+    }\n+  }\n+}\n+\n+void CoordinationServiceAgent::StartPollingForError() {\n+  LOG(INFO) << \"Polling for error from coordination service. This is a \"\n+               \"long-running RPC that will return only if an error is \"\n+               \"encountered or cancelled (e.g. due to shutdown).\";\n+  PollForErrorAsync([&](const absl::Status& status) {\n+    CHECK(!status.ok()) << \"PollForError returned OK status. Should \"\n+                           \"always return an error.\";\n+    if (absl::IsCancelled(status)) {\n+      LOG(INFO)\n+          << \"Cancelling error polling because the service or the agent is \"\n+             \"shutting down.\";\n+      // Return early and there is no need to set error.\n+      return;\n+    }\n+    LOG(ERROR) << \"Polled an error from coordination service (this can be \"\n+                  \"an error from this or another task).\";\n+    SetError(status);\n+  });\n+}\n+\n+void CoordinationServiceAgent::PollForErrorAsync(tsl::StatusCallback done) {\n+  auto call_opts = std::make_shared<tsl::CallOptions>();\n+\n+  absl::Status agent_running_status =\n+      ValidateRunningAgent(/*allow_disconnected=*/true);\n+  if (!agent_running_status.ok()) {\n+    done(agent_running_status);\n+    return;\n+  }\n+  auto request = std::make_shared<PollForErrorRequest>();\n+  auto response = std::make_shared<PollForErrorResponse>();\n+  *request->mutable_source_task() = task_;\n+  VLOG(3) << \"PollForErrorRequest: \" << request->DebugString();\n+\n+  const tsl::CancellationToken token =\n+      error_polling_cancellation_manager_->get_cancellation_token();\n+  const bool already_cancelled =\n+      !error_polling_cancellation_manager_->RegisterCallback(\n+          token, [call_opts]() { call_opts->StartCancel(); });\n+  if (already_cancelled) {\n+    done(absl::CancelledError(\"PollForErrorAsync() was cancelled.\"));\n+    return;\n+  }\n+\n+  leader_client_->PollForErrorAsync(\n+      call_opts.get(), request.get(), response.get(),\n+      [call_opts, request, response, done = std::move(done),\n+       &cm = error_polling_cancellation_manager_,\n+       token](const absl::Status& s) {\n+        // RPC call has completed (no longer needs to be cancelled if agent is\n+        // destroyed).\n+        cm->TryDeregisterCallback(token);\n+        done(s);\n+      });\n+}\n+\n+absl::Status CoordinationServiceAgent::WaitForAllTasks(\n+    const DeviceInfo& local_devices) {\n+  absl::Status agent_running_status = ValidateRunningAgent();\n+  if (!agent_running_status.ok()) {\n+    return agent_running_status;\n+  }\n+  WaitForAllTasksRequest request;\n+  *request.mutable_source_task() = task_;\n+  *request.mutable_device_info() = local_devices;\n+  VLOG(3) << \"WaitForAllTasksRequest: \" << request.DebugString();\n+  WaitForAllTasksResponse response;\n+  absl::Status status;\n+  absl::Notification n;\n+  leader_client_->WaitForAllTasksAsync(&request, &response,\n+                                       [&](const absl::Status& s) {\n+                                         status = s;\n+                                         n.Notify();\n+                                       });\n+  n.WaitForNotification();\n+  if (!status.ok()) {\n+    VLOG(3) << \"WaitForAllTasksResponse: \" << status;\n+    SetError(status);\n+    return status;\n+  }\n+  VLOG(3) << \"WaitForAllTasksResponse: \" << response.DebugString();\n+  cluster_devices_ = response.device_info();\n+  return absl::OkStatus();\n+}\n+\n+const DeviceInfo& CoordinationServiceAgent::GetClusterDeviceInfo() {\n+  return cluster_devices_;\n+}\n+\n+absl::StatusOr<CoordinatedTask> CoordinationServiceAgent::GetOwnTask() {\n+  if (!IsInitialized()) {\n+    return MakeCoordinationError(absl::FailedPreconditionError(\n+        \"Agent has not been initialized; we do not \"\n+        \"know the associated task yet.\"));\n+  }\n+  return task_;\n+}\n+\n+absl::StatusOr<std::vector<CoordinatedTaskStateInfo>>\n+CoordinationServiceAgent::GetTaskState(\n+    const std::vector<CoordinatedTask>& tasks) {\n+  GetTaskStateRequest request;\n+  *request.mutable_source_task() = {tasks.begin(), tasks.end()};\n+  GetTaskStateResponse response;\n+  absl::Notification n;\n+  absl::StatusOr<std::vector<CoordinatedTaskStateInfo>> result;\n+  leader_client_->GetTaskStateAsync(\n+      &request, &response, [&](const absl::Status& s) {\n+        if (s.ok()) {\n+          result = std::vector<CoordinatedTaskStateInfo>(\n+              std::make_move_iterator(response.task_state().begin()),\n+              std::make_move_iterator(response.task_state().end()));\n+        } else {\n+          result = s;\n+        }\n+        n.Notify();\n+      });\n+  n.WaitForNotification();\n+  return result;\n+}\n+\n+std::shared_ptr<tsl::CallOptions> CoordinationServiceAgent::WatchJobStateAsync(\n+    absl::string_view job_name, std::optional<int64_t> version_number,\n+    std::function<void(absl::StatusOr<tensorflow::WatchJobStateResponse>)>\n+        callback) {\n+  auto request = std::make_shared<WatchJobStateRequest>();\n+  auto response = std::make_shared<WatchJobStateResponse>();\n+  auto call_opts = std::make_shared<tsl::CallOptions>();\n+  WatchJobStateRequest* request_ptr = request.get();\n+  WatchJobStateResponse* response_ptr = response.get();\n+  request->set_job_name(job_name);\n+  request->set_version_number(version_number.value_or(-1));\n+\n+  leader_client_->WatchJobStateAsync(\n+      call_opts.get(), request_ptr, response_ptr,\n+      [request = std::move(request), response = std::move(response),\n+       callback = std::move(callback)](const absl::Status& s) mutable {\n+        if (s.ok()) {\n+          callback(*std::move(response));\n+        } else {\n+          callback(s);\n+        }\n+      });\n+  return call_opts;\n+}\n+\n+absl::StatusOr<tensorflow::WatchJobStateResponse>\n+CoordinationServiceAgent::WatchJobState(absl::string_view job_name,\n+                                        std::optional<int64_t> version_number) {\n+  absl::StatusOr<tensorflow::WatchJobStateResponse> response;\n+  absl::Notification done;\n+  WatchJobStateAsync(\n+      job_name, version_number,\n+      [&response, &done](absl::StatusOr<tensorflow::WatchJobStateResponse> r) {\n+        response = std::move(r);\n+        done.Notify();\n+      });\n+  done.WaitForNotification();\n+  return response;\n+}\n+\n+absl::Status CoordinationServiceAgent::ReportError(const absl::Status& error) {\n+  {\n+    absl::MutexLock l(state_mu_);\n+    if (state_ == CoordinatedTaskState::TASKSTATE_UNINITIALIZED) {\n+      return MakeCoordinationError(absl::FailedPreconditionError(\n+          \"Coordination service agent must be initialized first before \"\n+          \"reporting error.\"));\n+    } else if (state_ == CoordinatedTaskState::TASKSTATE_ERROR) {\n+      return MakeCoordinationError(absl::FailedPreconditionError(\n+          \"Coordination service agent is already in error state.\"));\n+    }\n+  }\n+  SetError(MakeCoordinationError(error, task_,\n+                                 /*is_reported_error=*/true));\n+  LOG(INFO) << \"Reporting error to coordination service: \" << error;\n+  ReportErrorToServiceRequest request;\n+  request.set_error_code(error.raw_code());\n+  request.set_error_message(std::string(error.message()));\n+  *request.mutable_error_origin() = task_;\n+  VLOG(5) << \"ReportErrorToServiceRequest: \" << request.DebugString();\n+  ReportErrorToServiceResponse response;\n+\n+  absl::Notification n;\n+  leader_client_->ReportErrorToServiceAsync(\n+      &request, &response, [&](const absl::Status& s) {\n+        VLOG(5) << \"ReportErrorToServiceResponse: \" << s;\n+        if (!s.ok()) {\n+          LOG(ERROR)\n+              << \"Encountered another error when reporting error to \"\n+                 \"coordination service: \"\n+              << s\n+              << \"\\nThis is usually caused by an earlier error during \"\n+                 \"execution. Check the logs of (a) this task, (b) the \"\n+                 \"leader (usually slice 0 task 0) and (c) the scheduler \"\n+                 \"(e.g. preemption, eviction) for an earlier error to debug \"\n+                 \"further.\";\n+        }\n+        n.Notify();\n+      });\n+  n.WaitForNotification();\n+  return absl::OkStatus();\n+}\n+\n+absl::Status CoordinationServiceAgent::Shutdown() { return ShutdownInternal(); }\n+\n+absl::Status CoordinationServiceAgent::ShutdownInternal() {\n+  absl::Status status = absl::OkStatus();\n+  bool is_connected = false;\n+  {\n+    absl::MutexLock l(state_mu_);\n+    is_connected = state_ == CoordinatedTaskState::TASKSTATE_CONNECTED;\n+  }\n+  // Disconnect agent from service.\n+  if (!configs_.agent_destruction_without_shutdown() && is_connected) {\n+    LOG(INFO) << \"Coordination agent has initiated Shutdown().\";\n+    ShutdownTaskRequest request;\n+    *request.mutable_source_task() = task_;\n+    ShutdownTaskResponse response;\n+    tsl::CallOptions call_opts;\n+    const int64_t shutdown_timeout =\n+        (configs_.shutdown_barrier_timeout_in_ms() > 0\n+             ? configs_.shutdown_barrier_timeout_in_ms()\n+             : absl::ToInt64Milliseconds(kDefaultShutdownTimeout)) +\n+        // Add 5s for service-related errors to propagate.\n+        5 * 1000;\n+    call_opts.SetTimeout(shutdown_timeout);\n+\n+    absl::Notification n;\n+    leader_client_->ShutdownTaskAsync(&call_opts, &request, &response,\n+                                      [&status, &n](const absl::Status& s) {\n+                                        status = s;\n+                                        n.Notify();\n+                                      });\n+    n.WaitForNotification();\n+    if (status.ok()) {\n+      LOG(INFO) << \"Coordination agent has successfully shut down.\";\n+    } else {\n+      status = MakeCoordinationError(absl::Status(\n+          status.code(),\n+          absl::StrCat(\n+              \"Failed to disconnect from coordination service with \"\n+              \"status: \",\n+              TrimCoordinationErrorMessage(status).ToString(),\n+              \"Proceeding with agent shutdown anyway. This is usually caused \"\n+              \"by an \"\n+              \"earlier error during execution. Check the logs of (a) this \"\n+              \"task, \"\n+              \"(b) the leader (usually slice 0 task 0) and (c) the scheduler \"\n+              \"(e.g. \"\n+              \"preemption, eviction) for an earlier error to debug further.\")));\n+      SetError(status);\n+    }\n+  }\n+\n+  // Tear down agent.\n+  StopHeartbeat();\n+  StopErrorPolling();\n+  {\n+    absl::MutexLock l(state_mu_);\n+    if (status.ok() && state_ == CoordinatedTaskState::TASKSTATE_ERROR) {\n+      const std::string status_message = absl::StrCat(\n+          \"Shutdown() was called while coordination agent is in error state, \"\n+          \"implying that distributed execution failed. Note: agent will \"\n+          \"still shutdown anyway. Agent status: \",\n+          status_.ToString(),\n+          \"\\nThis is usually caused by an earlier error during execution. \"\n+          \"Check the logs of (a) this task, (b) the leader (usually slice 0 \"\n+          \"task 0) and (c) the scheduler (e.g. preemption, eviction) for an \"\n+          \"earlier error to debug further.\");\n+      status =\n+          MakeCoordinationError(absl::FailedPreconditionError(status_message));\n+      LOG(ERROR) << status_message;\n+    }\n+    state_ = CoordinatedTaskState::TASKSTATE_DISCONNECTED;\n+  }\n+\n+  // Cancel all pending GetKeyValue() and WaitAtBarrier() RPC calls.\n+  cancellation_manager_.StartCancel();\n+\n+  return status;\n+}\n+\n+absl::Status CoordinationServiceAgent::Reset() {\n+  {\n+    absl::MutexLock l(state_mu_);\n+    if (state_ != CoordinatedTaskState::TASKSTATE_ERROR) {\n+      return MakeCoordinationError(absl::FailedPreconditionError(\n+          \"Reset() failed: coordination service agent is not in ERROR state.\"));\n+    }\n+  }\n+\n+  ResetTaskRequest request;\n+  *request.mutable_source_task() = task_;\n+  VLOG(3) << \"ResetTaskRequest: \" << request.DebugString();\n+  ResetTaskResponse response;\n+\n+  absl::Status status;\n+  absl::Notification n;\n+  leader_client_->ResetTaskAsync(&request, &response,\n+                                 [&status, &n](const absl::Status& s) {\n+                                   status = s;\n+                                   n.Notify();\n+                                 });\n+  n.WaitForNotification();\n+  VLOG(3) << \"ResetTaskResponse: \" << status;\n+  if (!status.ok()) {\n+    return status;\n+  }\n+\n+  // Reset agent state.\n+  StopHeartbeat();\n+  StopErrorPolling();\n+  ResetCancellationManager();\n+  {\n+    absl::MutexLock l(state_mu_);\n+    state_ = CoordinatedTaskState::TASKSTATE_DISCONNECTED;\n+  }\n+  {\n+    absl::MutexLock l(shutdown_mu_);\n+    shutting_down_ = false;\n+  }\n+\n+  LOG(INFO) << \"Coordination agent has been reset.\";\n+  return status;\n+}\n+\n+absl::StatusOr<std::string> CoordinationServiceAgent::GetKeyValue(\n+    absl::string_view key) {\n+  return GetKeyValue(key, /*timeout=*/absl::InfiniteDuration());\n+}\n+\n+absl::StatusOr<std::string> CoordinationServiceAgent::GetKeyValue(\n+    absl::string_view key, absl::Duration timeout) {\n+  auto n = std::make_shared<absl::Notification>();\n+  auto result = std::make_shared<absl::StatusOr<std::string>>();\n+  GetKeyValueAsync(\n+      key, [n, result](const absl::StatusOr<std::string>& status_or_value) {\n+        *result = status_or_value;\n+        n->Notify();\n+      });\n+  bool call_completed_before_timeout =\n+      n->WaitForNotificationWithTimeout(timeout);\n+  if (!call_completed_before_timeout) {\n+    VLOG(3) << \"GetKeyValue(\" << key << \") timed out after \" << timeout;\n+    return MakeCoordinationError(absl::DeadlineExceededError(absl::Substitute(\n+        \"GetKeyValue() timed out with key: $0 and duration: $1\", key,\n+        absl::FormatDuration(timeout))));\n+  }\n+  return *result;\n+}\n+\n+std::shared_ptr<tsl::CallOptions> CoordinationServiceAgent::GetKeyValueAsync(\n+    absl::string_view key, StatusOrValueCallback done) {\n+  auto request = std::make_shared<GetKeyValueRequest>();\n+  request->set_key(key.data(), key.size());\n+  VLOG(3) << \"GetKeyValueRequest: \" << request->DebugString();\n+  auto response = std::make_shared<GetKeyValueResponse>();\n+  auto call_opts = std::make_shared<tsl::CallOptions>();\n+\n+  const tsl::CancellationToken token =\n+      cancellation_manager_.get_cancellation_token();\n+  const bool already_cancelled = !cancellation_manager_.RegisterCallback(\n+      token, [call_opts]() { call_opts->StartCancel(); });\n+  if (already_cancelled) {\n+    done(absl::CancelledError(\"GetKeyValueAsync() was cancelled.\"));\n+    return call_opts;\n+  }\n+  leader_client_->GetKeyValueAsync(\n+      call_opts.get(), request.get(), response.get(),\n+      [call_opts, request, response, done = std::move(done),\n+       &cm = cancellation_manager_, token](const absl::Status& s) {\n+        // RPC call has completed (no longer needs to be cancelled if agent is\n+        // destroyed).\n+        cm.TryDeregisterCallback(token);\n+\n+        // Retrieve server response.\n+        if (!s.ok()) {\n+          done(s);\n+          VLOG(3) << \"GetKeyValueResponse: \" << s;\n+        } else {\n+          done(response->kv().value());\n+          VLOG(3) << \"GetKeyValueResponse: \" << response->DebugString();\n+        }\n+      });\n+  return call_opts;\n+}\n+\n+absl::StatusOr<std::string> CoordinationServiceAgent::TryGetKeyValue(\n+    absl::string_view key) {\n+  absl::Notification n;\n+  absl::StatusOr<std::string> result;\n+  TryGetKeyValueRequest request;\n+  request.set_key(key.data(), key.size());\n+  VLOG(3) << \"TryGetKeyValueRequest: \" << request.DebugString();\n+  TryGetKeyValueResponse response;\n+  leader_client_->TryGetKeyValueAsync(\n+      &request, &response, [&](const absl::Status& s) {\n+        if (s.ok()) {\n+          result = response.kv().value();\n+          VLOG(3) << \"TryGetKeyValueResponse: \" << result.value();\n+        } else {\n+          result = s;\n+          VLOG(3) << \"TryGetKeyValueResponse: \" << s;\n+        }\n+        n.Notify();\n+      });\n+  n.WaitForNotification();\n+\n+  return result;\n+}\n+\n+absl::StatusOr<int64_t> CoordinationServiceAgent::IncrementKeyValue(\n+    absl::string_view key, int64_t increment) {\n+  absl::Notification n;\n+  absl::StatusOr<int64_t> result;\n+  IncrementKeyValueRequest request;\n+  request.set_key(key.data(), key.size());\n+  request.set_increment(increment);\n+  VLOG(3) << \"IncrementKeyValueRequest: \" << request.DebugString();\n+  IncrementKeyValueResponse response;\n+  leader_client_->IncrementKeyValueAsync(\n+      &request, &response, [&](const absl::Status& s) {\n+        if (s.ok()) {\n+          int64_t result_value;\n+          if (!absl::SimpleAtoi(response.kv().value(), &result_value)) {\n+            result = absl::InternalError(absl::StrCat(\n+                \"Failed to parse increment key value: \", response.kv().value(),\n+                \" for key: \", key));\n+            return;\n+          }\n+          result = result_value;\n+          VLOG(3) << \"IncrementKeyValueResponse: \" << result.value();\n+        } else {\n+          result = s;\n+          VLOG(3) << \"IncrementKeyValueResponse: \" << s;\n+        }\n+        n.Notify();\n+      });\n+  n.WaitForNotification();\n+  return result;\n+}\n+\n+absl::StatusOr<std::vector<KeyValueEntry>>\n+CoordinationServiceAgent::GetKeyValueDir(absl::string_view key) {\n+  absl::Notification n;\n+  absl::StatusOr<std::vector<KeyValueEntry>> result;\n+  GetKeyValueDirAsync(\n+      key, [&n, &result](\n+               absl::StatusOr<std::vector<KeyValueEntry>> status_or_value) {\n+        result = std::move(status_or_value);\n+        n.Notify();\n+      });\n+\n+  n.WaitForNotification();\n+  return result;\n+}\n+\n+void CoordinationServiceAgent::GetKeyValueDirAsync(\n+    absl::string_view key, StatusOrValueDirCallback done) {\n+  auto request = std::make_shared<GetKeyValueDirRequest>();\n+  request->set_directory_key(key.data(), key.size());\n+  VLOG(3) << \"GetKeyValueDirRequest: \" << request->DebugString();\n+  auto response = std::make_shared<GetKeyValueDirResponse>();\n+  leader_client_->GetKeyValueDirAsync(\n+      request.get(), response.get(),\n+      [request, response, done = std::move(done)](const absl::Status& s) {\n+        if (!s.ok()) {\n+          done(s);\n+          VLOG(3) << \"GetKeyValueDirResponse: \" << s;\n+        } else {\n+          VLOG(3) << \"GetKeyValueDirResponse: \" << response->DebugString();\n+          std::vector<KeyValueEntry> kv_in_directory = {\n+              std::make_move_iterator(response->kv().begin()),\n+              std::make_move_iterator(response->kv().end())};\n+          done(kv_in_directory);\n+        }\n+      });\n+}\n+\n+absl::Status CoordinationServiceAgent::InsertKeyValue(absl::string_view key,\n+                                                      absl::string_view value) {\n+  return InsertKeyValue(key, value, /*allow_overwrite=*/false);\n+}\n+\n+absl::Status CoordinationServiceAgent::InsertKeyValue(absl::string_view key,\n+                                                      absl::string_view value,\n+                                                      bool allow_overwrite) {\n+  InsertKeyValueRequest request;\n+  request.mutable_kv()->set_key(key.data(), key.size());\n+  request.mutable_kv()->set_value(value.data(), value.size());\n+  request.set_allow_overwrite(allow_overwrite);\n+  VLOG(3) << \"InsertKeyValueRequest: \" << request.DebugString();\n+  InsertKeyValueResponse response;\n+\n+  absl::Status status;\n+  absl::Notification n;\n+  leader_client_->InsertKeyValueAsync(&request, &response,\n+                                      [&](const absl::Status& s) {\n+                                        status = s;\n+                                        n.Notify();\n+                                      });\n+  n.WaitForNotification();\n+  VLOG(3) << \"InsertKeyValueResponse: \" << status;\n+  return status;\n+}\n+\n+absl::Status CoordinationServiceAgent::DeleteKeyValue(absl::string_view key) {\n+  DeleteKeyValueRequest request;\n+  request.set_key(key.data(), key.size());\n+  request.set_is_directory(true);\n+  VLOG(3) << \"DeleteKeyValueRequest: \" << request.DebugString();\n+  DeleteKeyValueResponse response;\n+\n+  absl::Status status;\n+  absl::Notification n;\n+  leader_client_->DeleteKeyValueAsync(&request, &response,\n+                                      [&](const absl::Status& s) {\n+                                        status = s;\n+                                        n.Notify();\n+                                      });\n+  n.WaitForNotification();\n+  VLOG(3) << \"DeleteKeyValueResponse \" << status;\n+  return absl::OkStatus();\n+}\n+\n+absl::Status CoordinationServiceAgent::UpdateKeyValue(absl::string_view key,\n+                                                      absl::string_view value) {\n+  return MakeCoordinationError(absl::UnimplementedError(\n+      \"CoordinationServiceAgent::UpdateKeyValue is not implemented.\"));\n+}\n+\n+absl::Status CoordinationServiceAgent::StartWatchKey(\n+    absl::string_view key,\n+    CoordinationServiceAgent::ChangedKeyValuesCallback on_change) {\n+  return MakeCoordinationError(absl::UnimplementedError(\n+      \"CoordinationServiceAgent::StartWatchKey is not implemented.\"));\n+}\n+\n+absl::Status CoordinationServiceAgent::StopWatchKey(absl::string_view key) {\n+  return MakeCoordinationError(absl::UnimplementedError(\n+      \"CoordinationServiceAgent::StopWatchKey is not implemented.\"));\n+}\n+\n+void CoordinationServiceAgent::SetError(const absl::Status& error) {\n+  assert(!error.ok());\n+  absl::MutexLock l(state_mu_);\n+  if (state_ == CoordinatedTaskState::TASKSTATE_ERROR) return;\n+  absl::Status trimmed_error = TrimCoordinationErrorMessage(error);\n+\n+  state_ = CoordinatedTaskState::TASKSTATE_ERROR;\n+  status_ = trimmed_error;\n+  error_fn_(trimmed_error);\n+}\n+\n+absl::Status CoordinationServiceAgent::ActivateWatch(\n+    absl::string_view key, const std::map<std::string, std::string>& kvs) {\n+  return MakeCoordinationError(absl::UnimplementedError(\n+      \"CoordinationServiceAgent::ActivateWatch is not implemented.\"));\n+}\n+\n+absl::Status CoordinationServiceAgent::WaitAtBarrier(\n+    absl::string_view barrier_id, absl::Duration timeout,\n+    const std::vector<CoordinatedTask>& tasks) {\n+  absl::Status status;\n+  absl::Notification n;\n+  WaitAtBarrierAsync(barrier_id, timeout, tasks, [&](const absl::Status& s) {\n+    status = s;\n+    n.Notify();\n+  });\n+  n.WaitForNotification();\n+  return status;\n+}\n+\n+void CoordinationServiceAgent::WaitAtBarrierAsync(\n+    absl::string_view barrier_id, absl::Duration timeout,\n+    const std::vector<CoordinatedTask>& tasks, tsl::StatusCallback done) {\n+  absl::Status agent_running_status =\n+      ValidateRunningAgent(/*allow_disconnected=*/true);\n+  if (!agent_running_status.ok()) {\n+    done(agent_running_status);\n+    return;\n+  }\n+  auto request = std::make_shared<BarrierRequest>();\n+  auto response = std::make_shared<BarrierResponse>();\n+  {\n+    absl::MutexLock l(state_mu_);\n+\n+    // Prevent multiple concurrent invocations with the same id.\n+    // This usually indicates a bug in the user code. They should wait till the\n+    // previous call completes before starting a new one.\n+    if (ongoing_barriers_.contains(barrier_id)) {\n+      done(MakeCoordinationError(absl::FailedPreconditionError(\n+          absl::StrCat(\"Barrier \", barrier_id, \" is already ongoing.\"))));\n+      return;\n+    }\n+    ongoing_barriers_.insert(std::string(barrier_id));\n+\n+    request->set_barrier_id(std::string(barrier_id));\n+    request->set_barrier_timeout_in_ms(timeout / absl::Milliseconds(1));\n+    *request->mutable_source_task() = task_;\n+    *request->mutable_tasks() = {tasks.begin(), tasks.end()};\n+\n+    // Counter is incremented for each unique id's WaitAtBarrier() call.\n+    // Design note: we need agent-side state to fail attempts by restarted tasks\n+    // using the same barrier id (but not the same barrier).\n+    // Consider adding the counter to the barrier response.\n+    if (!barrier_counter_.contains(barrier_id)) {\n+      barrier_counter_[barrier_id] = -1;\n+    }\n+    request->set_counter(barrier_counter_[barrier_id] + 1);\n+    VLOG(3) << \"WaitAtBarrierRequest: \" << request->DebugString();\n+  }\n+\n+  auto call_opts = std::make_shared<tsl::CallOptions>();\n+\n+  const tsl::CancellationToken token =\n+      cancellation_manager_.get_cancellation_token();\n+  const bool already_cancelled = !cancellation_manager_.RegisterCallback(\n+      token, [call_opts]() { call_opts->StartCancel(); });\n+  if (already_cancelled) {\n+    done(absl::CancelledError(\"WaitAtBarrierAsync() was cancelled.\"));\n+    return;\n+  }\n+\n+  leader_client_->BarrierAsync(\n+      call_opts.get(), request.get(), response.get(),\n+      [call_opts, request, response, done = std::move(done), barrier_id, this,\n+       &cm = cancellation_manager_, token](const absl::Status& s) mutable {\n+        absl::MutexLock l(state_mu_);\n+        // Allow the same barrier id to be invoked after this counter's\n+        // completion.\n+        ongoing_barriers_.erase(barrier_id);\n+        // Track completed/errored barrier counters.\n+        if (s.ok()) {\n+          // This would correspond to the request counter.\n+          barrier_counter_[barrier_id] = response->counter();\n+        } else if (s.GetPayload(BarrierErrorPayloadKey()) != std::nullopt) {\n+          // Note that response is discarded if an error is returned, so we need\n+          // to parse from the error message.\n+          barrier_counter_[barrier_id] = GetBarrierCounterFromError(s);\n+        }\n+        // RPC call has completed (no longer needs to be cancelled if agent is\n+        // destroyed).\n+        cm.TryDeregisterCallback(token);\n+        auto status = TrimCoordinationErrorMessage(s);\n+        done(status);\n+        VLOG(3) << \"WaitAtBarrierResponse: \" << status;\n+      });\n+}\n+\n+absl::Status CoordinationServiceAgent::CancelBarrier(\n+    absl::string_view barrier_id) {\n+  absl::Status status;\n+  absl::Notification n;\n+  CancelBarrierAsync(barrier_id, [&](const absl::Status& s) {\n+    status = s;\n+    n.Notify();\n+  });\n+  n.WaitForNotification();\n+  return status;\n+}\n+\n+void CoordinationServiceAgent::CancelBarrierAsync(absl::string_view barrier_id,\n+                                                  tsl::StatusCallback done) {\n+  absl::Status agent_running_status =\n+      ValidateRunningAgent(/*allow_disconnected=*/true);\n+  if (!agent_running_status.ok()) {\n+    done(agent_running_status);\n+    return;\n+  }\n+  absl::MutexLock l(state_mu_);\n+  if (!barrier_counter_.contains(barrier_id)) {\n+    done(MakeCoordinationError(absl::FailedPreconditionError(absl::StrCat(\n+        \"Tried to cancel non-existent barrier \", barrier_id, \".\"))));\n+    return;\n+  }\n+  if (!ongoing_barriers_.contains(barrier_id)) {\n+    done(MakeCoordinationError(absl::FailedPreconditionError(absl::StrCat(\n+        \"Tried to cancel barrier \", barrier_id, \" that is not ongoing.\"))));\n+    return;\n+  }\n+\n+  auto request = std::make_shared<CancelBarrierRequest>();\n+  auto response = std::make_shared<CancelBarrierResponse>();\n+  request->set_barrier_id(std::string(barrier_id));\n+  *request->mutable_source_task() = task_;\n+  VLOG(3) << \"CancelBarrierRequest: \" << request->DebugString();\n+  leader_client_->CancelBarrierAsync(\n+      request.get(), response.get(),\n+      [request, response, done = std::move(done)](const absl::Status& s) {\n+        // Note: barrier state will be cleaned up the original barrier RPC.\n+        done(s);\n+        VLOG(3) << \"CancelBarrierResponse: \" << s;\n+      });\n+}\n+\n+absl::StatusOr<std::vector<CoordinationServiceAgent::AliveTask>>\n+CoordinationServiceAgent::GetAliveTasks(\n+    const std::vector<CoordinatedTask>& tasks) {\n+  // Validate the agent.\n+  if (absl::Status s = ValidateRunningAgent(/*allow_disconnected=*/true);\n+      !s.ok()) {\n+    return s;\n+  }\n+\n+  // Form the request and response.\n+  auto request = std::make_shared<GetAliveTasksRequest>();\n+  auto response = std::make_shared<GetAliveTasksResponse>();\n+  *request->mutable_requesting_task() = task_;\n+  *request->mutable_tasks() = {tasks.begin(), tasks.end()};\n+\n+  // Issue the request and wait for it to finish.\n+  absl::Status status;\n+  absl::Notification n;\n+  auto done = [&status, &n](const absl::Status& s) {\n+    status = s;\n+    n.Notify();\n+  };\n+  leader_client_->GetAliveTasksAsync(request.get(), response.get(), done);\n+  n.WaitForNotification();\n+  if (!status.ok()) {\n+    return status;\n+  }\n+\n+  // Parse the response.\n+  absl::MutexLock lock(incarnations_mu_);\n+  incarnations_.clear();\n+  std::vector<AliveTask> alive_tasks;\n+  for (int i = 0; i < response->alive_tasks_size(); ++i) {\n+    int task_id = response->alive_tasks(i).task_id();\n+    IncarnationId incarnation_id(response->incarnations(i));\n+\n+    alive_tasks.push_back(AliveTask{task_id, incarnation_id});\n+    incarnations_[task_id] = incarnation_id;\n+  }\n+  return alive_tasks;\n+}\n+\n+// Returns an error if agent is not running.\n+absl::Status CoordinationServiceAgent::ValidateRunningAgent(\n+    bool allow_disconnected) {\n+  absl::MutexLock l(state_mu_);\n+  switch (state_) {\n+    case CoordinatedTaskState::TASKSTATE_CONNECTED:\n+      return absl::OkStatus();\n+\n+    case CoordinatedTaskState::TASKSTATE_UNINITIALIZED:\n+      return MakeCoordinationError(absl::FailedPreconditionError(\n+          \"Agent must be in CONNECTED state. It is currently UNINITIALIZED.\"));\n+\n+    case CoordinatedTaskState::TASKSTATE_DISCONNECTED:\n+      if (allow_disconnected) return absl::OkStatus();\n+      return MakeCoordinationError(absl::FailedPreconditionError(\n+          \"Agent must be in CONNECTED state. It is currently DISCONNECTED.\"));\n+\n+    case CoordinatedTaskState::TASKSTATE_ERROR:\n+      return MakeCoordinationError(absl::FailedPreconditionError(\n+          \"Agent must be in CONNECTED state. It is currently in ERROR.\"));\n+\n+    default:\n+      return MakeCoordinationError(absl::FailedPreconditionError(absl::StrCat(\n+          \"Agent is not in CONNECTED state. Current state: \", state_)));\n+  }\n+}\n+\n+absl::StatusOr<tsl::Env*> CoordinationServiceAgent::GetEnv() {\n+  if (!IsInitialized()) {\n+    return MakeCoordinationError(absl::FailedPreconditionError(\n+        \"Coordination service agent has not been initialized.\"));\n+  }\n+  if (env_ == nullptr) {\n+    return MakeCoordinationError(absl::FailedPreconditionError(\n+        \"Coordination service agent was not \"\n+        \"initialized with a valid tsl::Env* object.\"));\n+  }\n+  return env_;\n+}\n+\n+std::unique_ptr<CoordinationServiceAgent> CreateCoordinationServiceAgent() {\n+  return std::make_unique<CoordinationServiceAgent>();\n+}\n+\n+}  // namespace xla"
        },
        {
            "sha": "15231c562b94fee32303e0190c0892b8e62ec13c",
            "filename": "third_party/xla/xla/pjrt/distributed/coordination/coordination_service_agent.h",
            "status": "added",
            "additions": 423,
            "deletions": 0,
            "changes": 423,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_service_agent.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_service_agent.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_service_agent.h?ref=3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a",
            "patch": "@@ -0,0 +1,423 @@\n+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_PJRT_DISTRIBUTED_COORDINATION_COORDINATION_SERVICE_AGENT_H_\n+#define XLA_PJRT_DISTRIBUTED_COORDINATION_COORDINATION_SERVICE_AGENT_H_\n+\n+#include <cstdint>\n+#include <functional>\n+#include <map>\n+#include <memory>\n+#include <optional>\n+#include <string>\n+#include <vector>\n+\n+#include \"absl/base/thread_annotations.h\"\n+#include \"absl/container/flat_hash_map.h\"\n+#include \"absl/container/flat_hash_set.h\"\n+#include \"absl/functional/any_invocable.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/synchronization/mutex.h\"\n+#include \"absl/time/time.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/pjrt/distributed/coordination/coordination_client.h\"\n+#include \"xla/pjrt/distributed/coordination/coordination_service.h\"\n+#include \"xla/tsl/distributed_runtime/call_options.h\"\n+#include \"xla/tsl/framework/cancellation.h\"\n+#include \"xla/tsl/platform/env.h\"\n+#include \"xla/tsl/platform/status.h\"\n+#include \"xla/tsl/protobuf/coordination_config.pb.h\"\n+#include \"xla/tsl/protobuf/coordination_service.pb.h\"\n+#include \"tsl/platform/random.h\"\n+\n+namespace xla {\n+\n+// CoordinationServiceAgent defines the interface for tasks to communicate with\n+// the coordination service instance (which implements\n+// CoordinationService). One instance of the agent should be deployed\n+// on each task for it to send various requests and stores / retrieves config\n+// key-value data to the service.\n+//\n+// See CoordinationService for more details on coordination service.\n+//\n+// All coordination service errors will have an additional\n+// CoordinationServiceError payload to distinguish themselves from RPC failures.\n+// The payload can optionally specify the error origin, and if the error is\n+// reported by the user via `agent->ReportError()`.\n+//\n+// Possible service errors:\n+//    - Internal: Coordination service has shut down or has not been enabled.\n+//    - Aborted: Incarnation mismatch during heartbeat (either remote\n+//                       task or coordination service has restarted).\n+//    - Unavailable: Heartbeat timeout from remote task (failed,\n+//                           crashed or got preempted).\n+//    - InvalidArgument: Unexpected heartbeat from remote task (not\n+//                               registered or wrong config).\n+class CoordinationServiceAgent {\n+ public:\n+  using StatusOrValueCallback =\n+      std::function<void(const absl::StatusOr<std::string>&)>;\n+  // Collection of key-value pairs in the same directory.\n+  using StatusOrValueDirCallback = std::function<void(\n+      const absl::StatusOr<std::vector<tensorflow::KeyValueEntry>>&)>;\n+  using ChangedKeyValuesCallback =\n+      std::function<void(const std::map<std::string, std::string>&)>;\n+\n+  CoordinationServiceAgent() = default;\n+\n+  virtual ~CoordinationServiceAgent() {\n+    absl::Status s = ShutdownInternal();\n+    VLOG(3) << \"Coordination agent dtor failed with status: \" << s;\n+  }\n+\n+  absl::Status Initialize(tsl::Env* env, absl::string_view job_name,\n+                          int task_id,\n+                          const tensorflow::CoordinationServiceConfig& configs,\n+                          std::unique_ptr<CoordinationClient> leader_client,\n+                          tsl::StatusCallback error_fn, bool recoverable);\n+  absl::Status Initialize(tsl::Env* env, absl::string_view job_name,\n+                          int task_id,\n+                          const tensorflow::CoordinationServiceConfig& configs,\n+                          std::unique_ptr<CoordinationClient> leader_client,\n+                          tsl::StatusCallback error_fn);\n+  absl::Status Initialize(tsl::Env* env,\n+                          const tensorflow::CoordinatedTask& task,\n+                          const tensorflow::CoordinationServiceConfig& configs,\n+                          std::unique_ptr<CoordinationClient> leader_client,\n+                          tsl::StatusCallback error_fn);\n+\n+  // Return true if the coordination service agent has been initialized.\n+  bool IsInitialized();\n+\n+  // Return true if the coordination service agent has successfully connected\n+  // with the Coordination Service\n+  bool IsConnected();\n+\n+  // Return true if the coordination service agent has an error state.\n+  bool IsError();\n+\n+  // Connect to coordination service with the following steps:\n+  //   - connect to service address specified in the config of `server_def`\n+  //   - register itself as a task to the service\n+  //   - start a thread to periodically send heartbeat message with the service\n+  // Possible service errors:\n+  //   - Internal: Coordination service has shut down.\n+  //   - FailedPrecondition: Agent is not in DISCONNECTED state.\n+  //   - InvalidArgument: Unexpected task registration\n+  //   - Aborted: Duplicate task registration (agent will retry connecting until\n+  //              the configured timeout)\n+  absl::Status Connect();\n+\n+  // Wait for all tasks to be up and registered. The call blocks until all tasks\n+  // in the cluster are up, or some error occurs.\n+  // Possible service errors:\n+  //   - Internal: Coordination service has shut down.\n+  //   - FailedPrecondition: Agent is not in CONNECTED state.\n+  //   - InvalidArgument: Unexpected task request\n+  absl::Status WaitForAllTasks(const tensorflow::DeviceInfo& local_devices);\n+\n+  // Get the device attributes of tasks from remote tasks in the cluster.\n+  const tensorflow::DeviceInfo& GetClusterDeviceInfo();\n+\n+  // State transition in coordination service agent:\n+  //\n+  //                 Init              Connect           SetError\n+  //   UNINITIALIZED ---> DISCONNECTED ------> CONNECTED -------> ERROR\n+  //                           ^                                  |\n+  //                           |__________________________________|\n+  //                                         Reset\n+\n+  // Get task associated with this agent.\n+  absl::StatusOr<tensorflow::CoordinatedTask> GetOwnTask();\n+\n+  // Get status of a remote task.\n+  absl::StatusOr<std::vector<tensorflow::CoordinatedTaskStateInfo>>\n+  GetTaskState(const std::vector<tensorflow::CoordinatedTask>& task);\n+\n+  // Watches the status of a remote job.\n+  absl::StatusOr<tensorflow::WatchJobStateResponse> WatchJobState(\n+      absl::string_view job_name, std::optional<int64_t> version_number);\n+\n+  // Note: Cancel the underlying RPC call with `call_opts->StartCancel()` and\n+  // `call_opts->ClearCancelCallback()`.\n+  std::shared_ptr<tsl::CallOptions> WatchJobStateAsync(\n+      absl::string_view job_name, std::optional<int64_t> version_number,\n+      std::function<void(absl::StatusOr<tensorflow::WatchJobStateResponse>)>\n+          callback);\n+\n+  // Report error to coordination service. This will invoke the error callback.\n+  // Note that the error payload will set `is_reported_error` to true, to\n+  // distinguish user-specified errors from internal service or RPC failures.\n+  // Possible service errors:\n+  //   - Internal: Coordination service has shut down.\n+  //   - FailedPrecondition: Uninitialized/disconnected/already in error state.\n+  //   - InvalidArgument: Unexpected task request\n+  absl::Status ReportError(const absl::Status& error);\n+\n+  // Shuts down by disconnecting from the service. Should only be called if\n+  // agent is connected and no further agent calls (except the destructor) are\n+  // expected. If `shutdown_barrier_timeout_in_ms` is specified in the config,\n+  // blocks until all tasks reach the barrier before shutting down together. If\n+  // the barrier times out, this agent will still disconnect, while an error is\n+  // reported to other agents that did not reach the barrier on time.\n+  // Possible service errors:\n+  //   - Internal: Coordination service has shut down.\n+  //   - InvalidArgument: Unexpected task request.\n+  //   - FailedPrecondition: Task was in error state (note: agent is still\n+  //                         shut down forcefully).\n+  absl::Status Shutdown();\n+\n+  // Disconnect from the service, and clean up the internal error status.\n+  // Possible service errors:\n+  //   - Internal: Coordination service has shut down.\n+  //   - InvalidArgument: Unexpected task request.\n+  //   - FailedPrecondition: task is not in error state/has already\n+  //       disconnected.\n+  absl::Status Reset();\n+\n+  // Key-value store API.\n+  // The agent does not need to be connected to utilize the key-value store.\n+  // There are no concurrency guarantees. To avoid a race / impose an ordering\n+  // on potentially concurrent ops (e.g. set, delete), use WaitAtBarrier().\n+\n+  // Get config key-value from the service.\n+  // If the key-value is not inserted yet, this is a blocking call that waits\n+  // until the corresponding key is inserted.\n+  //   - DeadlineExceeded: timed out waiting for key.\n+  absl::StatusOr<std::string> GetKeyValue(absl::string_view key);\n+  absl::StatusOr<std::string> GetKeyValue(absl::string_view key,\n+                                          absl::Duration timeout);\n+\n+  // Note: Cancel the underlying RPC call with `call_opts->StartCancel()` and\n+  // `call_opts->ClearCancelCallback()`.\n+  std::shared_ptr<tsl::CallOptions> GetKeyValueAsync(\n+      absl::string_view key, StatusOrValueCallback done);\n+\n+  // Get config key-value from the service.\n+  //   - NotFound: the requested key does not exist.\n+  absl::StatusOr<std::string> TryGetKeyValue(absl::string_view key);\n+\n+  // Increment the value of a key if it is int convertible.\n+  //   - FailedPrecondition: the key is not convertible to an int.\n+  absl::StatusOr<int64_t> IncrementKeyValue(absl::string_view key,\n+                                            int64_t increment);\n+\n+  // Get all values under a directory (key).\n+  // A value is considered to be in the directory if its key is prefixed with\n+  // the directory.\n+  // This is not a blocking call. If no keys are found, an empty vector is\n+  // returned immediately.\n+  absl::StatusOr<std::vector<tensorflow::KeyValueEntry>> GetKeyValueDir(\n+      absl::string_view key);\n+  void GetKeyValueDirAsync(absl::string_view key,\n+                           StatusOrValueDirCallback done);\n+\n+  // Insert config key-value to the service.\n+  //   - AlreadyExists: key is already set.\n+  absl::Status InsertKeyValue(absl::string_view key, absl::string_view value);\n+  absl::Status InsertKeyValue(absl::string_view key, absl::string_view value,\n+                              bool allow_overwrite);\n+\n+  // Delete config keys in the coordination service.\n+  absl::Status DeleteKeyValue(absl::string_view key);\n+\n+  // Update the value of a config key.\n+  absl::Status UpdateKeyValue(absl::string_view key, absl::string_view value);\n+\n+  // Register a callback that will be invoked when the key or keys under the key\n+  // directory are changed (inserted, deleted, or updated).\n+  absl::Status StartWatchKey(absl::string_view key,\n+                             ChangedKeyValuesCallback on_change);\n+  absl::Status StopWatchKey(absl::string_view key);\n+\n+  // Blocks until all (or a subset of) tasks are at the barrier or the barrier\n+  // fails.\n+  //\n+  // `barrier_id` should be unique across barriers.\n+  //\n+  // The first WaitAtBarrier() call received by the service for a particular\n+  // barrier_id is special in that it determines the barrier deadline based on\n+  // timeout duration.\n+  // However, if subsequent calls by different agents specify a different set of\n+  // `tasks` for the same `barrier_id`, the barrier will fail instantly.\n+  // For example,\n+  //   agent_1->WaitAtBarrier(barrier, 10min, <<worker, 1>, <worker, 2>>);\n+  //   agent_2->WaitAtBarrier(barrier, 10min, <<worker, 2>, <worker, 3>>);\n+  // Barrier fails after agent_2s call because it specifies a different set of\n+  // participating tasks.\n+  //\n+  // If no tasks are specified (default), the barrier will block for all the\n+  // connected tasks.\n+  //\n+  // Possible service errors:\n+  //   - DeadlineExceeded: Timed out waiting for specified tasks at the barrier.\n+  //      Deadline is determined by the server timestamp when it receives the\n+  //      first WaitAtBarrier() + timeout duration.\n+  //   - Cancelled: One of the tasks called CancelBarrier().\n+  //   - Aborted: Service is shutting down.\n+  //   - Internal: Any participating task is in ERROR state, or service has shut\n+  //     down.\n+  //   - InvalidArgument: (1) Conflicting tasks specified by different agents\n+  //       for the same barrier, (2) one of the participating tasks is not in\n+  //       the cluster, or (3) task making the request is not included in the\n+  //       list of participating tasks.\n+  //   - FailedPrecondition: Agent is in UNINITIALIZED or ERROR state, or the\n+  //       same barrier id is still being invoked.\n+  virtual absl::Status WaitAtBarrier(\n+      absl::string_view barrier_id, absl::Duration timeout,\n+      const std::vector<tensorflow::CoordinatedTask>& tasks);\n+\n+  void WaitAtBarrierAsync(absl::string_view barrier_id, absl::Duration timeout,\n+                          const std::vector<tensorflow::CoordinatedTask>& tasks,\n+                          tsl::StatusCallback done);\n+\n+  // Aborts the barrier if it is ongoing.\n+  // Current and future WaitAtBarrier() calls with the same id will return a\n+  // CANCELLED error status.\n+  // Possible service errors:\n+  //   - Internal: Coordination service has shut down.\n+  //   - FailedPrecondition: Barrier is non-existent or not ongoing.\n+  virtual absl::Status CancelBarrier(absl::string_view barrier_id);\n+  void CancelBarrierAsync(absl::string_view barrier_id,\n+                          tsl::StatusCallback done);\n+\n+  // Returns the set of currently alive tasks. More specifically, given a set of\n+  // tasks T, GetAliveTasks(T) returns the subset T of alive tasks.\n+  //\n+  // # Barrier Semantics\n+  //\n+  // If multiple tasks call GetAliveTasks concurrently, it's important that they\n+  // all agree on which tasks are alive. Otherwise, the tasks' behavior might\n+  // diverge. For example, imagine a set of tasks trying to run an AllGather,\n+  // but they all disagree on which tasks should be participating in the\n+  // AllGather. This is buggy.\n+  //\n+  // To ensure that every task agrees on which tasks are alive, the\n+  // GetAliveTasks RPC has barrier-like semantics. Consider an invocation\n+  // GetAliveTasks(T) for a set of tasks T. The invocation acts as a barrier,\n+  // waiting for every task in T to call GetAliveTasks(T). Afterwards,\n+  // GetAliveTasks returns the same set of alive tasks A to all the tasks in T.\n+  // This ensures that every task agrees which tasks are alive.\n+  //\n+  // One small correction. GetAliveTasks doesn't act as a barrier for *every*\n+  // task in T. Some tasks in T might have failed, so we should not wait for\n+  // them. Instead, the GetAliveTasks RPC waits only for the returned tasks A.\n+  //\n+  // # An Example\n+  //\n+  // Imagine we have four tasks: A, B, C, and D. Further imagine that task D\n+  // has failed and that every task calls GetAliveTasks([A, B, C, D]). The\n+  // invocation will return tasks [A, B, C]. The GetAliveTasks call acts as a\n+  // barrier across tasks A, B, and C. Task D, which failed, is ignored.\n+  struct AliveTask {\n+    int task_id;\n+    IncarnationId incarnation_id;\n+  };\n+  absl::StatusOr<std::vector<AliveTask>> GetAliveTasks(\n+      const std::vector<tensorflow::CoordinatedTask>& tasks);\n+\n+  // Returns the latest known set of incarnation ids for every task. Incarnation\n+  // ids can be refreshed by calling GetAliveTasks.\n+  //\n+  // When a task starts executing, it generates a random 64 bit incarnation id.\n+  // If a task fails and restarts, for example, it will have a different\n+  // incarnation id before and after it fails. This allows us to distinguish\n+  // different executions of the same task.\n+  absl::flat_hash_map<int, IncarnationId> Incarnations() const {\n+    absl::MutexLock lock(incarnations_mu_);\n+    return incarnations_;\n+  }\n+\n+  // Get unowned tsl::Env* that the agent was initialized with.\n+  absl::StatusOr<tsl::Env*> GetEnv();\n+\n+ protected:\n+  // Set the service agent to error status and invoke the error callback.\n+  // Note: different from ReportError, this does not report the error status to\n+  // remote coordination service.\n+  void SetError(const absl::Status& error);\n+\n+  // Activate the key-value callback watch.\n+  absl::Status ActivateWatch(absl::string_view key,\n+                             const std::map<std::string, std::string>&);\n+\n+  // Returns an error if agent is not running. If `allow_disconnected` is true,\n+  // returns OK even if the agent is in DISCONNECTED state.\n+  absl::Status ValidateRunningAgent(bool allow_disconnected = false);\n+  void StopHeartbeat();\n+\n+ private:\n+  friend class CoordinationServiceRpcHandler;\n+\n+  absl::Status ShutdownInternal();\n+  // Starts sending heartbeats to the coordination service.\n+  void StartSendingHeartbeats();\n+  // Use long polling to get error from the coordination service.\n+  void PollForErrorAsync(tsl::StatusCallback done);\n+\n+  // Starts polling for error from the coordination service.\n+  void StartPollingForError();\n+  // Cancels the error polling request and stops the error polling thread.\n+  void StopErrorPolling();\n+  // Resets the cancellation manager for error polling.\n+  void ResetCancellationManager();\n+\n+  tsl::Env* env_ = nullptr;  // Not owned.\n+  const IncarnationId incarnation_id_{tsl::random::New64()};\n+  tensorflow::CoordinatedTask task_;\n+  tensorflow::CoordinationServiceConfig configs_;\n+  tsl::StatusCallback error_fn_;\n+\n+  mutable absl::Mutex state_mu_;\n+  tensorflow::CoordinatedTaskState state_ ABSL_GUARDED_BY(state_mu_) =\n+      tensorflow::CoordinatedTaskState::TASKSTATE_UNINITIALIZED;\n+  absl::Status status_ ABSL_GUARDED_BY(state_mu_) = absl::OkStatus();\n+  // Tracks the number of times a barrier has been used, keyed by id.\n+  absl::flat_hash_map<std::string, int64_t> barrier_counter_\n+      ABSL_GUARDED_BY(state_mu_);\n+  absl::flat_hash_set<std::string> ongoing_barriers_ ABSL_GUARDED_BY(state_mu_);\n+\n+  IncarnationId leader_incarnation_{0};\n+  tensorflow::DeviceInfo cluster_devices_;\n+\n+  absl::Mutex shutdown_mu_;\n+  bool shutting_down_ ABSL_GUARDED_BY(shutdown_mu_) = false;\n+  std::unique_ptr<tsl::Thread> heartbeat_thread_;\n+\n+  // The latest known incarnation ids for all alive tasks, keyed by task id.\n+  mutable absl::Mutex incarnations_mu_;\n+  absl::flat_hash_map<int, IncarnationId> incarnations_\n+      ABSL_GUARDED_BY(incarnations_mu_);\n+\n+  // Must outlive coordination client which may need to access it within\n+  // GetKeyValueAsync() callbacks.\n+  tsl::CancellationManager cancellation_manager_;\n+  std::unique_ptr<tsl::CancellationManager>\n+      error_polling_cancellation_manager_ =\n+          std::make_unique<tsl::CancellationManager>();\n+  std::unique_ptr<CoordinationClient> leader_client_;\n+\n+  CoordinationServiceAgent(const CoordinationServiceAgent&) = delete;\n+  void operator=(const CoordinationServiceAgent&) = delete;\n+};\n+\n+std::unique_ptr<CoordinationServiceAgent> CreateCoordinationServiceAgent();\n+\n+}  // namespace xla\n+\n+#endif  // XLA_PJRT_DISTRIBUTED_COORDINATION_COORDINATION_SERVICE_AGENT_H_"
        },
        {
            "sha": "92a35738368af4229ab7302828c86f1a372361be",
            "filename": "third_party/xla/xla/pjrt/distributed/coordination/coordination_service_agent_test.cc",
            "status": "added",
            "additions": 752,
            "deletions": 0,
            "changes": 752,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_service_agent_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_service_agent_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_service_agent_test.cc?ref=3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a",
            "patch": "@@ -0,0 +1,752 @@\n+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/pjrt/distributed/coordination/coordination_service_agent.h\"\n+\n+#include <memory>\n+#include <string>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/log/check.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/memory/memory.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/status_matchers.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/time/clock.h\"\n+#include \"absl/time/time.h\"\n+#include \"xla/pjrt/distributed/coordination/coordination_client.h\"\n+#include \"xla/pjrt/distributed/coordination/coordination_service_error_util.h\"\n+#include \"xla/tsl/distributed_runtime/call_options.h\"\n+#include \"xla/tsl/lib/core/status_test_util.h\"\n+#include \"xla/tsl/platform/env.h\"\n+#include \"xla/tsl/platform/status.h\"\n+#include \"xla/tsl/platform/test.h\"\n+#include \"xla/tsl/protobuf/coordination_config.pb.h\"\n+#include \"xla/tsl/protobuf/coordination_service.pb.h\"\n+\n+namespace xla {\n+namespace {\n+using tensorflow::CoordinatedTask;\n+using tensorflow::CoordinationServiceConfig;\n+using tensorflow::KeyValueEntry;\n+\n+using ::testing::_;\n+using ::testing::DoAll;\n+using ::testing::InvokeArgument;\n+using ::testing::Return;\n+using ::testing::SetArgPointee;\n+using ::testing::UnorderedPointwise;\n+using ::testing::WithArgs;\n+\n+MATCHER(KvEq, \"simple KeyValueEntry matcher\") {\n+  const KeyValueEntry& kv0 = std::get<0>(arg);\n+  const KeyValueEntry& kv1 = std::get<1>(arg);\n+  return kv0.key() == kv1.key() && kv0.value() == kv1.value();\n+}\n+\n+// Note: b/169705709: no protobuf matchers in OSS.\n+MATCHER_P2(IsBarrierRequest, id, counter, \"\") {\n+  return id == arg->barrier_id() && counter == arg->counter();\n+}\n+\n+KeyValueEntry CreateKv(const std::string& key, const std::string& value) {\n+  KeyValueEntry kv;\n+  kv.set_key(key);\n+  kv.set_value(value);\n+  return kv;\n+}\n+\n+class TestCoordinationClient : public CoordinationClient {\n+ public:\n+  TestCoordinationClient() = default;\n+  MOCK_METHOD(void, GetKeyValueAsync,\n+              (tsl::CallOptions * call_opts, const GetKeyValueRequest*,\n+               GetKeyValueResponse*, tsl::StatusCallback),\n+              (override));\n+  MOCK_METHOD(void, TryGetKeyValueAsync,\n+              (const TryGetKeyValueRequest*, TryGetKeyValueResponse*,\n+               tsl::StatusCallback),\n+              (override));\n+  MOCK_METHOD(void, IncrementKeyValueAsync,\n+              (const IncrementKeyValueRequest*, IncrementKeyValueResponse*,\n+               tsl::StatusCallback),\n+              (override));\n+  MOCK_METHOD(void, GetKeyValueDirAsync,\n+              (const GetKeyValueDirRequest*, GetKeyValueDirResponse*,\n+               tsl::StatusCallback),\n+              (override));\n+  MOCK_METHOD(void, InsertKeyValueAsync,\n+              (const InsertKeyValueRequest*, InsertKeyValueResponse*,\n+               tsl::StatusCallback),\n+              (override));\n+  MOCK_METHOD(void, DeleteKeyValueAsync,\n+              (const DeleteKeyValueRequest*, DeleteKeyValueResponse*,\n+               tsl::StatusCallback),\n+              (override));\n+  MOCK_METHOD(void, RegisterTaskAsync,\n+              (tsl::CallOptions*, const RegisterTaskRequest*,\n+               RegisterTaskResponse*, tsl::StatusCallback),\n+              (override));\n+  MOCK_METHOD(void, ShutdownTaskAsync,\n+              (tsl::CallOptions*, const ShutdownTaskRequest*,\n+               ShutdownTaskResponse*, tsl::StatusCallback),\n+              (override));\n+  MOCK_METHOD(void, ResetTaskAsync,\n+              (const ResetTaskRequest*, ResetTaskResponse*,\n+               tsl::StatusCallback),\n+              (override));\n+  MOCK_METHOD(void, ReportErrorToServiceAsync,\n+              (const ReportErrorToServiceRequest*,\n+               ReportErrorToServiceResponse*, tsl::StatusCallback),\n+              (override));\n+  MOCK_METHOD(void, BarrierAsync,\n+              (tsl::CallOptions * call_opts, const BarrierRequest*,\n+               BarrierResponse*, tsl::StatusCallback),\n+              (override));\n+  MOCK_METHOD(void, CancelBarrierAsync,\n+              (const CancelBarrierRequest*, CancelBarrierResponse*,\n+               tsl::StatusCallback),\n+              (override));\n+  MOCK_METHOD(void, GetAliveTasksAsync,\n+              (const GetAliveTasksRequest*, GetAliveTasksResponse*,\n+               tsl::StatusCallback),\n+              (override));\n+  MOCK_METHOD(void, GetTaskStateAsync,\n+              (const GetTaskStateRequest*, GetTaskStateResponse*,\n+               tsl::StatusCallback),\n+              (override));\n+  MOCK_METHOD(void, WatchJobStateAsync,\n+              (tsl::CallOptions*, const WatchJobStateRequest*,\n+               WatchJobStateResponse*, tsl::StatusCallback),\n+              (override));\n+  MOCK_METHOD(void, HeartbeatAsync,\n+              (tsl::CallOptions*, const HeartbeatRequest*, HeartbeatResponse*,\n+               tsl::StatusCallback),\n+              (override));\n+  MOCK_METHOD(void, PollForErrorAsync,\n+              (tsl::CallOptions * call_opts, const PollForErrorRequest*,\n+               PollForErrorResponse*, tsl::StatusCallback),\n+              (override));\n+\n+#define UNIMPLEMENTED(method)                                              \\\n+  void method##Async(const method##Request* request,                       \\\n+                     method##Response* response, tsl::StatusCallback done) \\\n+      override {                                                           \\\n+    done(absl::UnimplementedError(#method \"Async\"));                       \\\n+  }\n+\n+  UNIMPLEMENTED(WaitForAllTasks);\n+#undef UNIMPLEMENTED\n+  void ReportErrorToTaskAsync(tsl::CallOptions* call_opts,\n+                              const ReportErrorToTaskRequest* request,\n+                              ReportErrorToTaskResponse* response,\n+                              tsl::StatusCallback done) override {\n+    done(absl::UnimplementedError(\"ReportErrorToTaskAsync\"));\n+  }\n+};\n+\n+class CoordinationServiceAgentTest : public ::testing::Test {\n+ public:\n+  void SetUp() override {\n+    ON_CALL(*client_, RegisterTaskAsync(_, _, _, _))\n+        .WillByDefault(InvokeArgument<3>(absl::OkStatus()));\n+    ON_CALL(*client_, HeartbeatAsync(_, _, _, _))\n+        .WillByDefault(InvokeArgument<3>(absl::OkStatus()));\n+    ON_CALL(*client_, ShutdownTaskAsync(_, _, _, _))\n+        .WillByDefault(InvokeArgument<3>(absl::OkStatus()));\n+    ON_CALL(*client_, ReportErrorToServiceAsync(_, _, _))\n+        .WillByDefault(InvokeArgument<2>(absl::OkStatus()));\n+    ON_CALL(*client_, ResetTaskAsync(_, _, _))\n+        .WillByDefault(InvokeArgument<2>(absl::OkStatus()));\n+    ON_CALL(*client_, BarrierAsync(_, _, _, _))\n+        .WillByDefault(InvokeArgument<3>(absl::OkStatus()));\n+    ON_CALL(*client_, CancelBarrierAsync(_, _, _))\n+        .WillByDefault(InvokeArgument<2>(absl::OkStatus()));\n+    ON_CALL(*client_, GetTaskStateAsync(_, _, _))\n+        .WillByDefault(InvokeArgument<2>(absl::OkStatus()));\n+  }\n+\n+  // Should be called after mocking service responses, before testing the agent.\n+  void InitializeAgent(CoordinationServiceConfig config = {}) {\n+    config.set_service_leader(\"test_leader\");\n+    TF_ASSERT_OK(agent_->Initialize(\n+        tsl::Env::Default(), /*job_name=*/\"test_job\",\n+        /*task_id=*/0, config, std::move(client_),\n+        /*error_fn=*/[](absl::Status s) {\n+          LOG(ERROR) << \"Coordination agent is set to error: \" << s;\n+        }));\n+  }\n+\n+  TestCoordinationClient* GetClient() {\n+    // InitializeAgent() transfers ownership of the coordination client.\n+    CHECK(client_ != nullptr)\n+        << \"GetClient() was called after InitializeAgent()\";\n+    return client_.get();\n+  }\n+\n+ protected:\n+  std::unique_ptr<CoordinationServiceAgent> agent_ =\n+      CreateCoordinationServiceAgent();\n+  std::unique_ptr<TestCoordinationClient> client_ =\n+      std::make_unique<TestCoordinationClient>();\n+};\n+\n+TEST_F(CoordinationServiceAgentTest, GetKeyValue_Simple_Success) {\n+  const std::string& test_key = \"test_key\";\n+  const std::string& test_value = \"test_value\";\n+  // Mock server response: set key-value pair and invoke done callback.\n+  GetKeyValueResponse mocked_response;\n+  auto kv = mocked_response.mutable_kv();\n+  kv->set_key(test_key);\n+  kv->set_value(test_value);\n+  ON_CALL(*GetClient(), GetKeyValueAsync(_, _, _, _))\n+      .WillByDefault(DoAll(SetArgPointee<2>(mocked_response),\n+                           InvokeArgument<3>(absl::OkStatus())));\n+  // Initialize coordination agent.\n+  InitializeAgent();\n+\n+  auto result = agent_->GetKeyValue(test_key);\n+\n+  TF_ASSERT_OK(result.status());\n+  EXPECT_EQ(*result, test_value);\n+}\n+\n+TEST_F(CoordinationServiceAgentTest, GetKeyValue_WithTimeout_Success) {\n+  const std::string& test_key = \"test_key\";\n+  const std::string& test_value = \"test_value\";\n+  // Mock server response: set key-value pair and invoke done callback.\n+  GetKeyValueResponse mocked_response;\n+  auto kv = mocked_response.mutable_kv();\n+  kv->set_key(test_key);\n+  kv->set_value(test_value);\n+  ON_CALL(*GetClient(), GetKeyValueAsync(_, _, _, _))\n+      .WillByDefault(DoAll(SetArgPointee<2>(mocked_response),\n+                           InvokeArgument<3>(absl::OkStatus())));\n+  // Initialize coordination agent.\n+  InitializeAgent();\n+\n+  auto result = agent_->GetKeyValue(test_key, /*timeout=*/absl::Seconds(10));\n+\n+  TF_ASSERT_OK(result.status());\n+  EXPECT_EQ(*result, test_value);\n+}\n+\n+TEST_F(CoordinationServiceAgentTest, GetKeyValue_Timeout_ReturnError) {\n+  const std::string& test_key = \"test_key\";\n+  tsl::StatusCallback owned_done;\n+  ON_CALL(*GetClient(), GetKeyValueAsync(_, _, _, _))\n+      .WillByDefault(WithArgs<3>([&](tsl::StatusCallback done) {\n+        // Copy method argument to prevent deallocation.\n+        owned_done = done;\n+      }));\n+  InitializeAgent();\n+\n+  auto result = agent_->GetKeyValue(test_key, /*timeout=*/absl::Seconds(1));\n+\n+  EXPECT_TRUE(absl::IsDeadlineExceeded(result.status()));\n+  // Needed to tear down test safely since agent dtor would cancel pending\n+  // calls, which would reference deallocated call_opts.\n+  owned_done(absl::CancelledError(\"error\"));\n+}\n+\n+TEST_F(CoordinationServiceAgentTest,\n+       GetKeyValue_DelayedResponse_TimeoutWithoutMemoryError) {\n+  const std::string& test_key = \"test_key\";\n+  const std::string& test_value = \"test_value\";\n+  auto client = std::make_unique<TestCoordinationClient>();\n+  GetKeyValueResponse* owned_response;\n+  tsl::StatusCallback owned_done;\n+  ON_CALL(*GetClient(), GetKeyValueAsync(_, _, _, _))\n+      .WillByDefault(WithArgs<2, 3>(\n+          [&](GetKeyValueResponse* response, tsl::StatusCallback done) {\n+            // Copy method arguments to prevent deallocation before mocking the\n+            // server callback beyond timeout.\n+            owned_response = response;\n+            owned_done = done;\n+          }));\n+  // Initialize coordination service agent.\n+  InitializeAgent();\n+\n+  auto result = agent_->GetKeyValue(test_key, /*timeout=*/absl::Seconds(1));\n+  EXPECT_TRUE(absl::IsDeadlineExceeded(result.status()));\n+  // Delayed server response: set key-value response, and invoke done callback.\n+  auto kv = owned_response->mutable_kv();\n+  kv->set_key(test_key);\n+  kv->set_value(test_value);\n+  owned_done(absl::OkStatus());\n+  // No explicit test, but used to verify there is no stack-use-after-return\n+  // or other memory-related errors.\n+}\n+\n+TEST_F(CoordinationServiceAgentTest,\n+       GetKeyValue_DelayedResponseBeforeTimeout_Success) {\n+  const std::string& test_key = \"test_key\";\n+  const std::string& test_value = \"test_value\";\n+  // Mock delayed server response before timeout: set key-value pair and invoke\n+  // done callback.\n+  auto client = std::make_unique<TestCoordinationClient>();\n+  std::unique_ptr<tsl::Thread> async_thread;\n+  GetKeyValueResponse* owned_response;\n+  tsl::StatusCallback owned_done;\n+  ON_CALL(*GetClient(), GetKeyValueAsync(_, _, _, _))\n+      // Setup async callback to insert key-value after a brief delay (5s)\n+      // before timeout (10s).\n+      .WillByDefault(WithArgs<2, 3>(\n+          [&](GetKeyValueResponse* response, tsl::StatusCallback done) {\n+            // Copy method arguments to prevent deallocation before\n+            //  triggering this async callback.\n+            owned_response = response;\n+            owned_done = done;\n+            async_thread = absl::WrapUnique(tsl::Env::Default()->StartThread(\n+                tsl::ThreadOptions(), \"async_thread\", [&]() {\n+                  // Set brief delay.\n+                  absl::SleepFor(absl::Seconds(5));\n+                  // Set key-value response, and invoke done callback.\n+                  auto kv = owned_response->mutable_kv();\n+                  kv->set_key(test_key);\n+                  kv->set_value(test_value);\n+                  owned_done(absl::OkStatus());\n+                }));\n+          }));\n+  InitializeAgent();\n+\n+  auto result = agent_->GetKeyValue(test_key, /*timeout=*/absl::Seconds(10));\n+\n+  TF_ASSERT_OK(result.status());\n+  EXPECT_EQ(*result, test_value);\n+}\n+\n+TEST_F(CoordinationServiceAgentTest, CancelGetKeyValue_Success) {\n+  const std::string test_key = \"test_key\";\n+  ON_CALL(*GetClient(), GetKeyValueAsync(_, _, _, _))\n+      .WillByDefault(WithArgs<0, 3>(\n+          [](tsl::CallOptions* call_opts, tsl::StatusCallback done) {\n+            // Mock RPC call cancellation.\n+            call_opts->SetCancelCallback([callback = std::move(done)]() {\n+              callback(absl::CancelledError(\"RPC call cancelled.\"));\n+            });\n+          }));\n+  InitializeAgent();\n+\n+  absl::Status status;\n+  std::shared_ptr<tsl::CallOptions> get_kv_call_opts = agent_->GetKeyValueAsync(\n+      test_key, [&status](const absl::StatusOr<std::string>& result) {\n+        status = result.status();\n+      });\n+  get_kv_call_opts->StartCancel();\n+\n+  EXPECT_TRUE(absl::IsCancelled(status)) << status;\n+  // This is to prevent memory leaks due to how we set this particular cancel\n+  // callback. In practice, this should not be necessary.\n+  get_kv_call_opts->ClearCancelCallback();\n+}\n+\n+TEST_F(CoordinationServiceAgentTest, TryGetKeyValue_Simple_Success) {\n+  const std::string& test_key = \"test_key\";\n+  const std::string& test_value = \"test_value\";\n+  // Mock server response: set key-value pair and invoke done callback.\n+  TryGetKeyValueResponse mocked_response;\n+  auto kv = mocked_response.mutable_kv();\n+  kv->set_key(test_key);\n+  kv->set_value(test_value);\n+  ON_CALL(*GetClient(), TryGetKeyValueAsync(_, _, _))\n+      .WillByDefault(DoAll(SetArgPointee<1>(mocked_response),\n+                           InvokeArgument<2>(absl::OkStatus())));\n+\n+  // Initialize coordination agent.\n+  InitializeAgent();\n+  auto result = agent_->TryGetKeyValue(test_key);\n+  TF_ASSERT_OK(result.status());\n+  EXPECT_EQ(*result, test_value);\n+}\n+\n+TEST_F(CoordinationServiceAgentTest, IncrementKeyValue_Simple_Success) {\n+  constexpr absl::string_view test_key = \"test_key\";\n+  constexpr absl::string_view test_value = \"11\";\n+  // Mock server response: set key-value pair and invoke done callback.\n+  IncrementKeyValueResponse mocked_response;\n+  auto kv = mocked_response.mutable_kv();\n+  kv->set_key(test_key);\n+  kv->set_value(test_value);\n+  ON_CALL(*GetClient(), IncrementKeyValueAsync(_, _, _))\n+      .WillByDefault(DoAll(SetArgPointee<1>(mocked_response),\n+                           InvokeArgument<2>(absl::OkStatus())));\n+  // Initialize coordination agent.\n+  InitializeAgent();\n+  auto result = agent_->IncrementKeyValue(test_key, 1);\n+  EXPECT_THAT(result, absl_testing::IsOkAndHolds(11));\n+}\n+\n+TEST_F(CoordinationServiceAgentTest, GetKeyValueDir_Simple_Success) {\n+  const std::string test_key = \"test_key_dir\";\n+  std::vector<KeyValueEntry> test_values;\n+  test_values.push_back(CreateKv(\"test_key_dir/task_0\", \"0\"));\n+  test_values.push_back(CreateKv(\"test_key_dir/task_1\", \"1\"));\n+  // Mock server response: set key-value pair and invoke done callback.\n+  GetKeyValueDirResponse mocked_response;\n+  mocked_response.set_directory_key(test_key);\n+  *mocked_response.mutable_kv() = {test_values.begin(), test_values.end()};\n+  ON_CALL(*GetClient(), GetKeyValueDirAsync(_, _, _))\n+      .WillByDefault(DoAll(SetArgPointee<1>(mocked_response),\n+                           InvokeArgument<2>(absl::OkStatus())));\n+  // Initialize coordination agent.\n+  InitializeAgent();\n+\n+  auto result = agent_->GetKeyValueDir(test_key);\n+\n+  TF_ASSERT_OK(result.status());\n+  EXPECT_THAT(*result, UnorderedPointwise(KvEq(), test_values));\n+}\n+\n+TEST_F(CoordinationServiceAgentTest, ShutdownInErrorShouldReturnError) {\n+  // Connect coordination agent and set it to error.\n+  InitializeAgent();\n+  TF_ASSERT_OK(agent_->Connect());\n+  TF_ASSERT_OK(agent_->ReportError(absl::InternalError(\"Test Error.\")));\n+\n+  // Shutdown should return error.\n+  absl::Status s = agent_->Shutdown();\n+\n+  EXPECT_TRUE(absl::IsFailedPrecondition(s));\n+}\n+\n+TEST_F(CoordinationServiceAgentTest, Reset_ConnectedButNotInError_Fail) {\n+  // Connect agent.\n+  InitializeAgent();\n+  TF_ASSERT_OK(agent_->Connect());\n+\n+  auto status = agent_->Reset();\n+\n+  // Fails because agent is not in ERROR state.\n+  EXPECT_TRUE(absl::IsFailedPrecondition(status));\n+}\n+\n+TEST_F(CoordinationServiceAgentTest, ConnectAfterResetError) {\n+  // Connect coordination agent and set it to error.\n+  InitializeAgent();\n+  TF_ASSERT_OK(agent_->Connect());\n+  TF_ASSERT_OK(agent_->ReportError(absl::InternalError(\"Test Error.\")));\n+\n+  // Reset error.\n+  TF_ASSERT_OK(agent_->Reset());\n+  // Agent should be able to reconnect to the service after resetting.\n+  TF_EXPECT_OK(agent_->Connect());\n+}\n+\n+TEST_F(CoordinationServiceAgentTest, ConnectAfterReset_WithErrorPolling) {\n+  // Connect coordination agent and set it to error.\n+  PollForErrorResponse mocked_response;\n+  EXPECT_CALL(*GetClient(), PollForErrorAsync(_, _, _, _))\n+      .WillOnce(DoAll(SetArgPointee<2>(mocked_response),\n+                      InvokeArgument<3>(absl::UnavailableError(\"Test Error.\"))))\n+      .WillOnce(DoAll(SetArgPointee<2>(mocked_response),\n+                      InvokeArgument<3>(absl::InternalError(\"Test Error.\"))));\n+\n+  CoordinationServiceConfig config;\n+  config.set_poll_for_error_from_service_at_startup(true);\n+  InitializeAgent(config);\n+  // The agent will be in ERROR state after the first call to Connect()\n+  // because the error polling thread will be created and will immediately\n+  // return an error.\n+  TF_ASSERT_OK(agent_->Connect());\n+  // Wait a bit for the error polling thread to start.\n+  absl::SleepFor(absl::Seconds(2));\n+  ASSERT_TRUE(agent_->IsError());\n+\n+  TF_ASSERT_OK(agent_->Reset());\n+  // Agent should be able to reconnect to the service after resetting. The\n+  // error polling thread will be recreated when the agent is connected again.\n+  TF_EXPECT_OK(agent_->Connect());\n+  absl::SleepFor(absl::Seconds(2));\n+  // The agent should again be in ERROR state after Connect().\n+  EXPECT_TRUE(agent_->IsError());\n+}\n+\n+TEST_F(CoordinationServiceAgentTest, CancelledPollForErrorRequest) {\n+  // Connect coordination agent.\n+  PollForErrorResponse mocked_response;\n+  EXPECT_CALL(*GetClient(), PollForErrorAsync(_, _, _, _))\n+      .WillOnce(DoAll(SetArgPointee<2>(mocked_response),\n+                      InvokeArgument<3>(absl::CancelledError(\"Test Error.\"))));\n+\n+  CoordinationServiceConfig config;\n+  config.set_poll_for_error_from_service_at_startup(true);\n+  InitializeAgent(config);\n+  TF_ASSERT_OK(agent_->Connect());\n+  // Wait a bit for the error polling thread to start.\n+  absl::SleepFor(absl::Seconds(2));\n+  // Cancelled error polling request will not set agent to error.\n+  ASSERT_FALSE(agent_->IsError());\n+}\n+\n+TEST_F(CoordinationServiceAgentTest, InvalidPollForErrorRequest) {\n+  // Connect coordination agent.\n+  PollForErrorResponse mocked_response;\n+  EXPECT_CALL(*GetClient(), PollForErrorAsync(_, _, _, _))\n+      .WillOnce(\n+          DoAll(SetArgPointee<2>(mocked_response),\n+                InvokeArgument<3>(absl::InvalidArgumentError(\"Test Error.\"))));\n+\n+  CoordinationServiceConfig config;\n+  config.set_poll_for_error_from_service_at_startup(true);\n+  InitializeAgent(config);\n+  TF_ASSERT_OK(agent_->Connect());\n+  // Wait a bit for the error polling thread to start.\n+  absl::SleepFor(absl::Seconds(2));\n+  ASSERT_TRUE(agent_->IsError());\n+}\n+\n+TEST_F(CoordinationServiceAgentTest,\n+       PollForErrorRequestWithFailedPrecondition) {\n+  // Connect coordination agent.\n+  PollForErrorResponse mocked_response;\n+  EXPECT_CALL(*GetClient(), PollForErrorAsync(_, _, _, _))\n+      .WillOnce(DoAll(\n+          SetArgPointee<2>(mocked_response),\n+          InvokeArgument<3>(absl::FailedPreconditionError(\"Test Error.\"))));\n+\n+  CoordinationServiceConfig config;\n+  config.set_poll_for_error_from_service_at_startup(true);\n+  InitializeAgent(config);\n+  TF_ASSERT_OK(agent_->Connect());\n+  // Wait a bit for the error polling thread to start.\n+  absl::SleepFor(absl::Seconds(2));\n+  ASSERT_TRUE(agent_->IsError());\n+}\n+\n+TEST_F(CoordinationServiceAgentTest, ResetCanBeRetried) {\n+  // Mock reset error failing for the first time.\n+  EXPECT_CALL(*GetClient(), ResetTaskAsync(_, _, _))\n+      .WillOnce(InvokeArgument<2>(absl::InternalError(\"Reset error\")))\n+      .WillOnce(InvokeArgument<2>(absl::OkStatus()));\n+  // Connect coordination agent and set it to error.\n+  InitializeAgent();\n+  TF_ASSERT_OK(agent_->Connect());\n+  TF_ASSERT_OK(agent_->ReportError(absl::InternalError(\"Test Error.\")));\n+\n+  // Reset error fails for the first time.\n+  absl::Status reset_status = agent_->Reset();\n+  EXPECT_TRUE(absl::IsInternal(reset_status));\n+\n+  // Agent should be able to attempt resetting again.\n+  TF_ASSERT_OK(agent_->Reset());\n+  // Agent should be able to reconnect to the service after resetting.\n+  TF_EXPECT_OK(agent_->Connect());\n+}\n+\n+TEST_F(CoordinationServiceAgentTest, GetOwnTask) {\n+  InitializeAgent();\n+\n+  auto result = agent_->GetOwnTask();\n+\n+  TF_ASSERT_OK(result.status());\n+  CoordinatedTask actual_task = *result;\n+  // These fields are from the arguments used in InitializeAgent().\n+  CoordinatedTask expected_task;\n+  expected_task.set_job_name(\"test_job\");\n+  expected_task.set_task_id(0);\n+  EXPECT_EQ(actual_task.job_name(), expected_task.job_name());\n+  EXPECT_EQ(actual_task.task_id(), expected_task.task_id());\n+}\n+\n+TEST_F(CoordinationServiceAgentTest, GetOwnTask_Uninitialized) {\n+  auto result = agent_->GetOwnTask();\n+\n+  EXPECT_TRUE(absl::IsFailedPrecondition(result.status()));\n+}\n+\n+TEST_F(CoordinationServiceAgentTest, GetEnv_SucceedsAfterInit) {\n+  EXPECT_TRUE(absl::IsFailedPrecondition(agent_->GetEnv().status()));\n+  InitializeAgent();\n+\n+  absl::StatusOr<tsl::Env*> result = agent_->GetEnv();\n+\n+  TF_ASSERT_OK(result.status());\n+  EXPECT_EQ(*result, tsl::Env::Default());\n+}\n+\n+TEST_F(CoordinationServiceAgentTest, Connect_AbortedErrorShouldBeRetried) {\n+  // Mock connection failing for the first two times.\n+  EXPECT_CALL(*GetClient(), RegisterTaskAsync(_, _, _, _))\n+      .WillOnce(\n+          InvokeArgument<3>(absl::AbortedError(\"DuplicateTaskRegistration\")))\n+      .WillOnce(\n+          InvokeArgument<3>(absl::AbortedError(\"DuplicateTaskRegistration\")))\n+      .WillOnce(InvokeArgument<3>(absl::OkStatus()));\n+  InitializeAgent();\n+\n+  TF_EXPECT_OK(agent_->Connect());\n+}\n+\n+TEST_F(CoordinationServiceAgentTest, Connect_AbortedErrorShouldFailEventually) {\n+  // Mock connection failing - old incarnation of coordination service never\n+  // restarts.\n+  EXPECT_CALL(*GetClient(), RegisterTaskAsync(_, _, _, _))\n+      .WillRepeatedly(\n+          InvokeArgument<3>(absl::AbortedError(\"DuplicateTaskRegistration\")));\n+  CoordinationServiceConfig config;\n+  // Connect should only be retried for 3 seconds.\n+  config.set_cluster_register_timeout_in_ms(\n+      absl::ToInt64Milliseconds(absl::Seconds(3)));\n+  InitializeAgent(config);\n+\n+  absl::Status s = agent_->Connect();\n+\n+  EXPECT_TRUE(absl::IsAborted(s));\n+}\n+\n+TEST_F(CoordinationServiceAgentTest, Connect_InternalErrorShouldBeRetried) {\n+  EXPECT_CALL(*GetClient(), RegisterTaskAsync(_, _, _, _))\n+      .WillOnce(InvokeArgument<3>(\n+          absl::InternalError(\"Coordination service is not enabled.\")))\n+      .WillOnce(InvokeArgument<3>(\n+          absl::InternalError(\"Coordination service is not enabled.\")))\n+      .WillOnce(InvokeArgument<3>(absl::OkStatus()));\n+  InitializeAgent();\n+\n+  TF_EXPECT_OK(agent_->Connect());\n+}\n+\n+TEST_F(CoordinationServiceAgentTest, WaitAtBarrier_Twice_Success) {\n+  const std::string barrier_id = \"barrier_id\";\n+  // tsl::Call expectations need to be set before the agent is initialized.\n+  EXPECT_CALL(*GetClient(),\n+              BarrierAsync(_, IsBarrierRequest(barrier_id, 0), _, _))\n+      .WillOnce(InvokeArgument<3>(absl::OkStatus()));\n+  EXPECT_CALL(*GetClient(),\n+              // Check that the counter is incremented.\n+              BarrierAsync(_, IsBarrierRequest(barrier_id, 1), _, _))\n+      .WillOnce(InvokeArgument<3>(absl::OkStatus()));\n+\n+  InitializeAgent();\n+  TF_EXPECT_OK(agent_->Connect());\n+\n+  TF_EXPECT_OK(\n+      agent_->WaitAtBarrier(barrier_id, absl::Seconds(1), /*tasks=*/{}));\n+  TF_EXPECT_OK(\n+      agent_->WaitAtBarrier(barrier_id, absl::Seconds(1), /*tasks=*/{}));\n+}\n+\n+TEST_F(CoordinationServiceAgentTest, WaitAtBarrier_Ongoing_Fails) {\n+  const std::string barrier_id = \"barrier_id\";\n+  // tsl::Call expectations need to be set before the agent is initialized.\n+  EXPECT_CALL(*GetClient(),\n+              BarrierAsync(_, IsBarrierRequest(barrier_id, 0), _, _))\n+      // Let the first call hang by not invoking the done callback.\n+      .WillOnce(Return());\n+\n+  InitializeAgent();\n+  TF_EXPECT_OK(agent_->Connect());\n+\n+  agent_->WaitAtBarrierAsync(barrier_id, absl::Seconds(1), /*tasks=*/{},\n+                             [](const absl::Status& s) {});\n+\n+  EXPECT_THAT(agent_->WaitAtBarrier(barrier_id, absl::Seconds(1), /*tasks=*/{}),\n+              absl_testing::StatusIs(absl::StatusCode::kFailedPrecondition));\n+}\n+\n+TEST_F(CoordinationServiceAgentTest,\n+       WaitAtBarrier_FailedWithBarrierError_IncrementCounter) {\n+  const std::string barrier_id = \"barrier_id\";\n+  // tsl::Call expectations need to be set before the agent is initialized.\n+  // First barrier fails with service error (has coordination payload).\n+  EXPECT_CALL(*GetClient(),\n+              BarrierAsync(_, IsBarrierRequest(barrier_id, 0), _, _))\n+      .WillOnce(InvokeArgument<3>(MakeCoordinationError(MakeBarrierError(\n+          absl::InternalError(\"Barrier failed.\"), barrier_id, 0))));\n+  EXPECT_CALL(*GetClient(),\n+              // Second barrier should have incremented counter.\n+              BarrierAsync(_, IsBarrierRequest(barrier_id, 1), _, _))\n+      .WillOnce(InvokeArgument<3>(absl::OkStatus()));\n+\n+  InitializeAgent();\n+  TF_EXPECT_OK(agent_->Connect());\n+  EXPECT_THAT(agent_->WaitAtBarrier(barrier_id, absl::Seconds(1), /*tasks=*/{}),\n+              absl_testing::StatusIs(absl::StatusCode::kInternal));\n+\n+  TF_EXPECT_OK(agent_->WaitAtBarrier(barrier_id, absl::Seconds(1), {}));\n+}\n+\n+TEST_F(CoordinationServiceAgentTest,\n+       WaitAtBarrier_FailedWithRpcError_DoesNotIncrementCounter) {\n+  const std::string barrier_id = \"barrier_id\";\n+  // tsl::Call expectations need to be set before the agent is initialized.\n+  // First barrier fails with RPC error (no coordination payload).\n+  EXPECT_CALL(*GetClient(),\n+              BarrierAsync(_, IsBarrierRequest(barrier_id, 0), _, _))\n+      .WillOnce(InvokeArgument<3>(absl::UnavailableError(\"Connection lost.\")))\n+      // Second call will use the same un-incremented counter.\n+      .WillOnce(InvokeArgument<3>(absl::OkStatus()));\n+\n+  InitializeAgent();\n+  TF_EXPECT_OK(agent_->Connect());\n+  EXPECT_THAT(agent_->WaitAtBarrier(barrier_id, absl::Seconds(1), /*tasks=*/{}),\n+              absl_testing::StatusIs(absl::StatusCode::kUnavailable));\n+\n+  TF_EXPECT_OK(agent_->WaitAtBarrier(barrier_id, absl::Seconds(1), {}));\n+}\n+\n+TEST_F(CoordinationServiceAgentTest, CancelBarrier_OngoingBarrier_Cancelled) {\n+  const std::string barrier_id = \"barrier_id\";\n+  EXPECT_CALL(*GetClient(),\n+              BarrierAsync(_, IsBarrierRequest(barrier_id, 0), _, _))\n+      // Let the first call hang by not invoking the done callback.\n+      .WillOnce(Return());\n+  InitializeAgent();\n+  TF_EXPECT_OK(agent_->Connect());\n+  agent_->WaitAtBarrierAsync(barrier_id, absl::Seconds(1), /*tasks=*/{},\n+                             // Can't test this since this would be invoked on\n+                             // service after cancel invocation.\n+                             [](const absl::Status& s) {});\n+\n+  EXPECT_THAT(agent_->CancelBarrier(barrier_id),\n+              absl_testing::StatusIs(absl::StatusCode::kOk));\n+}\n+\n+TEST_F(CoordinationServiceAgentTest, CancelBarrier_NonExistent_Fails) {\n+  InitializeAgent();\n+  TF_EXPECT_OK(agent_->Connect());\n+\n+  EXPECT_THAT(agent_->CancelBarrier(\"nonexistent_barrier\"),\n+              absl_testing::StatusIs(absl::StatusCode::kFailedPrecondition));\n+}\n+\n+TEST_F(CoordinationServiceAgentTest, CancelBarrier_CompletedBarrier_Fails) {\n+  InitializeAgent();\n+  TF_EXPECT_OK(agent_->Connect());\n+  TF_EXPECT_OK(\n+      agent_->WaitAtBarrier(\"barrier_id\", absl::Seconds(1), /*tasks=*/{}));\n+\n+  EXPECT_THAT(agent_->CancelBarrier(\"barrier_id\"),\n+              absl_testing::StatusIs(absl::StatusCode::kFailedPrecondition));\n+}\n+\n+TEST_F(CoordinationServiceAgentTest, CancelBarrier_ErroredBarrier_Fails) {\n+  EXPECT_CALL(*GetClient(), BarrierAsync(_, _, _, _))\n+      .WillOnce(InvokeArgument<3>(absl::InternalError(\"Test Error.\")));\n+  InitializeAgent();\n+  TF_EXPECT_OK(agent_->Connect());\n+  ASSERT_THAT(\n+      agent_->WaitAtBarrier(\"barrier_id\", absl::Seconds(1), /*tasks=*/{}),\n+      absl_testing::StatusIs(absl::StatusCode::kInternal));\n+\n+  EXPECT_THAT(agent_->CancelBarrier(\"barrier_id\"),\n+              absl_testing::StatusIs(absl::StatusCode::kFailedPrecondition));\n+}\n+\n+}  // namespace\n+}  // namespace xla"
        },
        {
            "sha": "1cb71ca51630add9c7890f2639fe2d80921872a7",
            "filename": "third_party/xla/xla/pjrt/distributed/coordination/coordination_service_error_util.cc",
            "status": "added",
            "additions": 76,
            "deletions": 0,
            "changes": 76,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_service_error_util.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_service_error_util.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_service_error_util.cc?ref=3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a",
            "patch": "@@ -0,0 +1,76 @@\n+/* Copyright 2024 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/pjrt/distributed/coordination/coordination_service_error_util.h\"\n+\n+#include <optional>\n+#include <string>\n+\n+#include \"absl/status/status.h\"\n+#include \"absl/strings/cord.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"tsl/platform/regexp.h\"\n+\n+namespace xla {\n+absl::Status TrimCoordinationErrorMessage(const absl::Status& s) {\n+  if (s.ok()) {\n+    return s;\n+  }\n+  auto status_message = std::string(s.message());\n+  auto additional_info_index = status_message.find(\"Additional GRPC\");\n+  // This error didn't come from gRPC, so we don't need to trim it.\n+  if (additional_info_index == std::string::npos) {\n+    return s;\n+  }\n+\n+  std::optional<absl::Cord> payload =\n+      s.GetPayload(CoordinationErrorPayloadKey());\n+  if (!payload.has_value() && absl::IsUnavailable(s)) {\n+    // This error is not provided by us, so it's probably an RPC layer error.\n+    auto prefix_message =\n+        \"Failed to send RPC to coordination service. Either the leader task \"\n+        \"was preempted/died/restarted unexpectedly or this task is \"\n+        \"experiencing network issues. Check earlier logs from 1) this task, 2) \"\n+        \"the leader (usually slice 0 task 0), and 3) cluster scheduler to debug\"\n+        \" further.\\n\";\n+    status_message = absl::StrCat(\n+        prefix_message,\n+        // Replace the duplicated error message at the start with the prefix.\n+        status_message.substr(additional_info_index));\n+  } else {\n+    // Extract RPC called.\n+    std::string rpc_name;\n+    // Note: it is unfortunate that we have to keep the tensorflow prefix\n+    // because that's the RPC service proto namespace.\n+    RE2::PartialMatch(status_message,\n+                      \"(/tensorflow.CoordinationService/(\\\\w+))\", &rpc_name);\n+    // Erase duplicated error message.\n+    status_message = status_message.substr(0, additional_info_index);\n+    absl::StrAppend(&status_message, \"\\nRPC: \", rpc_name);\n+  }\n+  auto trimmed_status = absl::Status(s.code(), status_message);\n+  // Reattach payload.\n+  if (payload.has_value()) {\n+    trimmed_status.SetPayload(CoordinationErrorPayloadKey(), *payload);\n+  }\n+#if defined(PLATFORM_GOOGLE)\n+  // Reattach source locations.\n+  for (const auto& source_location : s.GetSourceLocations()) {\n+    trimmed_status.AddSourceLocation(source_location);\n+  }\n+#endif\n+  return trimmed_status;\n+}\n+}  // namespace xla"
        },
        {
            "sha": "3faf7ce23fdc39c7465a91f1891dfe21ce9ba8d5",
            "filename": "third_party/xla/xla/pjrt/distributed/coordination/coordination_service_error_util.h",
            "status": "added",
            "additions": 96,
            "deletions": 0,
            "changes": 96,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_service_error_util.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_service_error_util.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_service_error_util.h?ref=3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a",
            "patch": "@@ -0,0 +1,96 @@\n+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#ifndef XLA_PJRT_DISTRIBUTED_COORDINATION_COORDINATION_SERVICE_ERROR_UTIL_H_\n+#define XLA_PJRT_DISTRIBUTED_COORDINATION_COORDINATION_SERVICE_ERROR_UTIL_H_\n+\n+#include <cstdint>\n+#include <optional>\n+#include <string>\n+\n+#include \"absl/status/status.h\"\n+#include \"absl/strings/cord.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"xla/tsl/protobuf/coordination_service.pb.h\"\n+\n+namespace xla {\n+\n+constexpr absl::string_view CoordinationErrorPayloadKey() {\n+  return \"type.googleapis.com/tensorflow.CoordinationServiceError\";\n+}\n+\n+constexpr absl::string_view BarrierErrorPayloadKey() {\n+  return \"type.googleapis.com/tensorflow.BarrierError\";\n+}\n+\n+// Mark error as a coordination service error (as opposed to RPC\n+// errors).\n+inline absl::Status MakeCoordinationError(absl::Status s) {\n+  s.SetPayload(CoordinationErrorPayloadKey(), absl::Cord(\"\"));\n+  return s;\n+}\n+\n+inline absl::Status MakeBarrierError(absl::Status s,\n+                                     absl::string_view barrier_id,\n+                                     int64_t counter) {\n+  tensorflow::BarrierError error;\n+  error.set_barrier_id(std::string(barrier_id));\n+  error.set_counter(counter);\n+  s.SetPayload(BarrierErrorPayloadKey(), absl::Cord(error.SerializeAsString()));\n+  return MakeCoordinationError(s);\n+}\n+\n+inline int64_t GetBarrierCounterFromError(const absl::Status& s) {\n+  if (s.GetPayload(BarrierErrorPayloadKey()) == std::nullopt) {\n+    return -1;\n+  }\n+  tensorflow::BarrierError error;\n+  error.ParseFromString(\n+      std::string(s.GetPayload(BarrierErrorPayloadKey()).value()));\n+  return error.counter();\n+}\n+\n+// Mark error as a coordination service error (as opposed to RPC\n+// errors), and indicate error origin.\n+// Errors reported via the agent API by the user should set `is_reported_error`\n+// to true.\n+inline absl::Status MakeCoordinationError(\n+    absl::Status s, const tensorflow::CoordinatedTask& origin,\n+    bool is_reported_error = false) {\n+  tensorflow::CoordinationServiceError error;\n+  *error.mutable_source_task() = origin;\n+  error.set_is_reported_error(is_reported_error);\n+  s.SetPayload(CoordinationErrorPayloadKey(),\n+               absl::Cord(error.SerializeAsString()));\n+  return s;\n+}\n+\n+// Mark error as a coordination service error with payload.\n+inline absl::Status MakeCoordinationError(\n+    absl::Status s, const tensorflow::CoordinationServiceError& payload) {\n+  s.SetPayload(CoordinationErrorPayloadKey(),\n+               absl::Cord(payload.SerializeAsString()));\n+  return s;\n+}\n+\n+// Trims the error message by replacing the `Additional GRPC error` part.\n+// Note: The duplicated error message is a quirk of the underlying gRPC code\n+// that we are using. Changing the shared code may hide important messages for\n+// other libraries, so we trim the error message for coordination service\n+// instead. See tsl/distributed_runtime/rpc/grpc_state.h for more details.\n+absl::Status TrimCoordinationErrorMessage(const absl::Status& s);\n+\n+}  // namespace xla\n+\n+#endif  // XLA_PJRT_DISTRIBUTED_COORDINATION_COORDINATION_SERVICE_ERROR_UTIL_H_"
        },
        {
            "sha": "b3d732b5bf4e293a418efbb7470f041a528b1056",
            "filename": "third_party/xla/xla/pjrt/distributed/coordination/coordination_service_error_util_test.cc",
            "status": "added",
            "additions": 170,
            "deletions": 0,
            "changes": 170,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_service_error_util_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_service_error_util_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_service_error_util_test.cc?ref=3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a",
            "patch": "@@ -0,0 +1,170 @@\n+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#include \"xla/pjrt/distributed/coordination/coordination_service_error_util.h\"\n+\n+#include <string>\n+\n+#include \"absl/status/status.h\"\n+#include \"absl/strings/match.h\"\n+#include \"xla/tsl/platform/test.h\"\n+#include \"xla/tsl/protobuf/coordination_service.pb.h\"\n+namespace xla {\n+namespace {\n+using ::tensorflow::BarrierError;\n+using ::tensorflow::CoordinatedTask;\n+using ::tensorflow::CoordinationServiceError;\n+\n+TEST(CoordinationServiceErrorUtil, MakeCoordinationErrorWithEmptyPayload) {\n+  absl::Status error = absl::InternalError(\"Test Error\");\n+\n+  absl::Status coordination_error = MakeCoordinationError(error);\n+\n+  EXPECT_EQ(coordination_error.code(), error.code());\n+  EXPECT_EQ(coordination_error.message(), error.message());\n+  // Payload exists but has no value.\n+  EXPECT_EQ(\n+      coordination_error.GetPayload(CoordinationErrorPayloadKey()).value(), \"\");\n+}\n+\n+TEST(CoordinationServiceErrorUtil, MakeCoordinationErrorWithErrorOrigin) {\n+  absl::Status error = absl::InternalError(\"Test Error\");\n+  CoordinatedTask source_task;\n+  source_task.set_job_name(\"test_worker\");\n+  source_task.set_task_id(7);\n+\n+  absl::Status coordination_error = MakeCoordinationError(error, source_task);\n+\n+  EXPECT_EQ(coordination_error.code(), error.code());\n+  EXPECT_EQ(coordination_error.message(), error.message());\n+  CoordinationServiceError payload;\n+  // Explicit string conversion for open source builds.\n+  payload.ParseFromString(std::string(\n+      coordination_error.GetPayload(CoordinationErrorPayloadKey()).value()));\n+  EXPECT_EQ(payload.source_task().job_name(), source_task.job_name());\n+  EXPECT_EQ(payload.source_task().task_id(), source_task.task_id());\n+  EXPECT_EQ(payload.is_reported_error(), false);\n+}\n+\n+TEST(CoordinationServiceErrorUtil, MakeCoordinationErrorWithUserReportedError) {\n+  absl::Status error = absl::InternalError(\"Test Error\");\n+  CoordinatedTask source_task;\n+  source_task.set_job_name(\"test_worker\");\n+  source_task.set_task_id(7);\n+\n+  absl::Status coordination_error =\n+      MakeCoordinationError(error, source_task,\n+                            /*is_reported_error=*/true);\n+\n+  EXPECT_EQ(coordination_error.code(), error.code());\n+  EXPECT_EQ(coordination_error.message(), error.message());\n+  CoordinationServiceError payload;\n+  // Explicit string conversion for open source builds.\n+  payload.ParseFromString(std::string(\n+      coordination_error.GetPayload(CoordinationErrorPayloadKey()).value()));\n+  EXPECT_EQ(payload.source_task().job_name(), source_task.job_name());\n+  EXPECT_EQ(payload.source_task().task_id(), source_task.task_id());\n+  EXPECT_EQ(payload.is_reported_error(), true);\n+}\n+\n+TEST(CoordinationServiceErrorUtil, MakeCoordinationErrorWithPayload) {\n+  absl::Status error = absl::InternalError(\"Test Error\");\n+  CoordinationServiceError payload;\n+  CoordinatedTask* source_task = payload.mutable_source_task();\n+  source_task->set_job_name(\"test_worker\");\n+  source_task->set_task_id(7);\n+  payload.set_is_reported_error(true);\n+\n+  absl::Status coordination_error = MakeCoordinationError(error, payload);\n+\n+  EXPECT_EQ(coordination_error.code(), error.code());\n+  EXPECT_EQ(coordination_error.message(), error.message());\n+  CoordinationServiceError actual_payload;\n+  // Explicit string conversion for open source builds.\n+  actual_payload.ParseFromString(std::string(\n+      coordination_error.GetPayload(CoordinationErrorPayloadKey()).value()));\n+  EXPECT_EQ(actual_payload.source_task().job_name(),\n+            payload.source_task().job_name());\n+  EXPECT_EQ(actual_payload.source_task().task_id(),\n+            payload.source_task().task_id());\n+  EXPECT_EQ(actual_payload.is_reported_error(), payload.is_reported_error());\n+}\n+\n+TEST(CoordinationServiceErrorUtil, MakeBarrierErrorWithPayload) {\n+  absl::Status barrier_error =\n+      MakeBarrierError(absl::InternalError(\"Test Error\"), \"barrier_id\", 8);\n+\n+  BarrierError payload;\n+  payload.ParseFromString(\n+      std::string(barrier_error.GetPayload(BarrierErrorPayloadKey()).value()));\n+  EXPECT_EQ(payload.barrier_id(), \"barrier_id\");\n+  EXPECT_EQ(payload.counter(), 8);\n+  EXPECT_EQ(GetBarrierCounterFromError(barrier_error), 8);\n+  // Payload exists but has no value.\n+  EXPECT_EQ(barrier_error.GetPayload(CoordinationErrorPayloadKey()).value(),\n+            \"\");\n+}\n+\n+TEST(CoordinationServiceErrorUtil,\n+     TrimCoordinationErrorMessage_CoordinationError) {\n+  absl::Status error = MakeCoordinationError(absl::InternalError(\n+      \"Coordination service has stopped. RecordHeartbeat() from task: \"\n+      \"/job:jax_worker/replica:0/task:2 failed. Additional GRPC error \"\n+      \"information from remote target coordination_service while calling \"\n+      \"/tensorflow.CoordinationService/Heartbeat::UNKNOWN:Error received from \"\n+      \"peer  \"\n+      \"{file:'third_party/grpc/src/core/lib/surface/filter_stack_call.cc', \"\n+      \"file_line:464, created_time:'2024-08-05T13:57:51.331198242-07:00', \"\n+      \"grpc_status:13, grpc_message:'Coordination service has stopped. \"\n+      \"RecordHeartbeat() from task: /job:jax_worker/replica:0/task:2 failed. \"\n+      \"'} \"));\n+\n+  absl::Status trimmed_error = TrimCoordinationErrorMessage(error);\n+  EXPECT_EQ(trimmed_error.code(), error.code());\n+  EXPECT_EQ(trimmed_error.message(),\n+            \"Coordination service has stopped. RecordHeartbeat() from task: \"\n+            \"/job:jax_worker/replica:0/task:2 failed. \\nRPC: \"\n+            \"/tensorflow.CoordinationService/Heartbeat\");\n+  // Payload exists but has no value.\n+  EXPECT_EQ(trimmed_error.GetPayload(CoordinationErrorPayloadKey()).value(),\n+            \"\");\n+}\n+\n+TEST(CoordinationServiceErrorUtil, TrimCoordinationErrorMessage_NetworkError) {\n+  absl::Status error = absl::UnavailableError(\n+      \"failed to connect to all addresses; last error: UNKNOWN: \"\n+      \"ipv4:127.0.0.1:10001: Failed to connect to remote host: Connection \"\n+      \"refused. Additional GRPC error information from remote target \"\n+      \"coordination_service while calling \"\n+      \"/tensorflow.CoordinationService/Heartbeat::UNKNOWN:Error received from \"\n+      \"peer \"\n+      \"{file:'third_party/grpc/src/core/lib/surface/filter_stack_call.cc', \"\n+      \"file_line:464, created_time:'2024-08-05T13:57:53.123562608-07:00', \"\n+      \"grpc_status:14, grpc_message:'failed to connect to all addresses; last \"\n+      \"error: UNKNOWN: ipv4:127.0.0.1:10001: Failed to connect to remote host: \"\n+      \"Connection refused'} \");\n+\n+  absl::Status trimmed_error = TrimCoordinationErrorMessage(error);\n+  auto message = trimmed_error.message();\n+  EXPECT_EQ(trimmed_error.code(), error.code());\n+  EXPECT_TRUE(absl::StrContains(message, \"Check earlier logs\"));\n+  EXPECT_TRUE(absl::StrContains(message, \"preempted\"));\n+  // Message is not duplicated.\n+  EXPECT_EQ(message.find(\"failed to connect\"),\n+            message.rfind(\"failed to connect\"))\n+      << trimmed_error;\n+}\n+\n+}  // namespace\n+}  // namespace xla"
        },
        {
            "sha": "8f2dfb02ab135d98998becdec3c0b17f03aa05d9",
            "filename": "third_party/xla/xla/pjrt/distributed/coordination/coordination_service_recoverable_job_test.cc",
            "status": "added",
            "additions": 272,
            "deletions": 0,
            "changes": 272,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_service_recoverable_job_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_service_recoverable_job_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_service_recoverable_job_test.cc?ref=3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a",
            "patch": "@@ -0,0 +1,272 @@\n+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#include <memory>\n+#include <string>\n+#include <utility>\n+\n+#include \"absl/base/thread_annotations.h\"\n+#include \"absl/container/flat_hash_map.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/memory/memory.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/synchronization/mutex.h\"\n+#include \"grpcpp/server.h\"\n+#include \"grpcpp/server_builder.h\"\n+#include \"grpcpp/support/channel_arguments.h\"\n+#include \"xla/pjrt/distributed/coordination/coordination_client.h\"\n+#include \"xla/pjrt/distributed/coordination/coordination_service.h\"\n+#include \"xla/pjrt/distributed/coordination/coordination_service_agent.h\"\n+#include \"xla/pjrt/distributed/coordination/grpc_coordination_client.h\"\n+#include \"xla/pjrt/distributed/coordination/grpc_coordination_service_impl.h\"\n+#include \"xla/tsl/distributed_runtime/rpc/async_service_interface.h\"\n+#include \"xla/tsl/lib/core/status_test_util.h\"\n+#include \"xla/tsl/platform/env.h\"\n+#include \"xla/tsl/platform/status.h\"\n+#include \"xla/tsl/platform/test.h\"\n+#include \"xla/tsl/platform/threadpool.h\"\n+#include \"xla/tsl/protobuf/coordination_config.pb.h\"\n+\n+namespace xla {\n+namespace {\n+using tensorflow::CoordinatedJob;\n+using tensorflow::CoordinationServiceConfig;\n+\n+constexpr char kParameterServerJobName[] = \"parameter_server\";\n+constexpr char kWorkerJobName[] = \"worker\";\n+constexpr char kCoordinationServiceType[] = \"standalone\";\n+constexpr char kServiceLeader[] = \"/job:parameter_server/replica:0/task:0\";\n+\n+class TestCoordinationClientCache : public CoordinationClientCache {\n+ public:\n+  void AddTask(const std::string& target, CoordinationClient* client) {\n+    absl::MutexLock l(clients_mu_);\n+    clients_.emplace(target, client);\n+  }\n+\n+  CoordinationClient* GetClient(const std::string& target) override {\n+    absl::MutexLock l(clients_mu_);\n+    if (auto it = clients_.find(target); it != clients_.end()) {\n+      return it->second;\n+    }\n+    return nullptr;\n+  }\n+\n+  std::unique_ptr<CoordinationClient> GetOwnedClient(\n+      const std::string& target) override {\n+    LOG(ERROR) << \"GetOwnedClient is not supported.\";\n+    return nullptr;\n+  }\n+\n+ private:\n+  absl::Mutex clients_mu_;\n+  absl::flat_hash_map<std::string, CoordinationClient*> clients_\n+      ABSL_GUARDED_BY(clients_mu_);\n+};\n+\n+class TestCoordinationServiceTaskState {\n+ public:\n+  TestCoordinationServiceTaskState() = default;\n+\n+  ~TestCoordinationServiceTaskState() = default;\n+\n+  void Shutdown() {\n+    coord_client_.reset();\n+    coord_agent_.reset();\n+    coord_compute_pool_.reset();\n+    static_cast<GrpcCoordinationServiceImpl*>(coord_rpc_service_.get())\n+        ->SetCoordinationServiceInstance(nullptr);\n+    grpc_server_->Shutdown();\n+    coord_rpc_service_->Shutdown();\n+  }\n+\n+  void StartGrpcServer() {\n+    ::grpc::ServerBuilder builder;\n+    coord_compute_pool_ = std::make_unique<tsl::thread::ThreadPool>(\n+        tsl::Env::Default(), /*name=*/\"CoordinationServiceRpcHandler\",\n+        /*num_threads=*/5);\n+    coord_rpc_service_ = std::make_unique<GrpcCoordinationServiceImpl>(\n+        coord_compute_pool_.get(), &builder);\n+    auto* grpc_coord_service =\n+        static_cast<GrpcCoordinationServiceImpl*>(coord_rpc_service_.get());\n+    grpc_coord_service->SetCoordinationServiceAgentInstance(coord_agent_.get());\n+    grpc_server_ = builder.BuildAndStart();\n+    coord_client_ = absl::WrapUnique(NewGrpcCoordinationClient(\n+        grpc_server_->InProcessChannel(::grpc::ChannelArguments())));\n+    coord_rpc_thread_ = absl::WrapUnique(tsl::Env::Default()->StartThread(\n+        /*thread_options=*/{}, /*name=*/\"CoordinationServiceHandleRPCsLoop\",\n+        [service = coord_rpc_service_.get()]() { service->HandleRPCsLoop(); }));\n+  }\n+\n+  void SetCoordinationService(CoordinationService* service) {\n+    auto* grpc_coord_service =\n+        static_cast<GrpcCoordinationServiceImpl*>(coord_rpc_service_.get());\n+    grpc_coord_service->SetCoordinationServiceInstance(service);\n+  }\n+\n+  void InitializeAndConnectCoordinationAgents(\n+      const std::string& job_name, int task_id,\n+      const CoordinationServiceConfig& coordination_config) {\n+    auto error_fn = [this, job_name](const absl::Status& status) {\n+      this->status_ = status;\n+      LOG(ERROR) << \"Coordination service agent of \" << job_name\n+                 << \" is in error status: \" << status;\n+    };\n+\n+    TF_CHECK_OK(coord_agent_->Initialize(tsl::Env::Default(), job_name, task_id,\n+                                         coordination_config,\n+                                         std::move(coord_client_), error_fn));\n+    TF_CHECK_OK(coord_agent_->Connect());\n+    TF_CHECK_OK(status_);\n+  }\n+\n+  CoordinationClient* GetCoordinationClient() { return coord_client_.get(); }\n+\n+  absl::Status ReportError(const absl::Status& status) {\n+    return coord_agent_->ReportError(status);\n+  }\n+\n+  absl::Status GetStatus() const { return status_; }\n+\n+ private:\n+  std::unique_ptr<::grpc::Server> grpc_server_;\n+  std::unique_ptr<tsl::thread::ThreadPool> coord_compute_pool_;\n+  std::unique_ptr<tsl::AsyncServiceInterface> coord_rpc_service_;\n+  std::unique_ptr<tsl::Thread> coord_rpc_thread_;\n+  std::unique_ptr<CoordinationServiceAgent> coord_agent_ =\n+      CreateCoordinationServiceAgent();\n+  std::unique_ptr<CoordinationClient> coord_client_;\n+  absl::Status status_;\n+};\n+\n+class CoordinationServiceRecoverableJobTest : public ::testing::Test {\n+ public:\n+  void SetUp() override {\n+    state_ps_0_.StartGrpcServer();\n+    state_ps_1_.StartGrpcServer();\n+    state_worker_0_.StartGrpcServer();\n+    state_worker_1_.StartGrpcServer();\n+  }\n+\n+  void TearDown() override {\n+    state_ps_0_.Shutdown();\n+    state_ps_1_.Shutdown();\n+    state_worker_0_.Shutdown();\n+    state_worker_1_.Shutdown();\n+    coord_service_.reset();\n+  }\n+\n+  void Initialize() {\n+    ConfigureCoordinationService();\n+    auto client_cache = std::make_unique<TestCoordinationClientCache>();\n+    client_cache->AddTask(\n+        /*target=*/kServiceLeader, state_ps_0_.GetCoordinationClient());\n+    client_cache->AddTask(\n+        /*target=*/\"/job:parameter_server/replica:0/task:1\",\n+        state_ps_1_.GetCoordinationClient());\n+    client_cache->AddTask(\n+        /*target=*/\"/job:worker/replica:0/task:0\",\n+        state_worker_0_.GetCoordinationClient());\n+    client_cache->AddTask(\n+        /*target=*/\"/job:worker/replica:0/task:1\",\n+        state_worker_1_.GetCoordinationClient());\n+    coord_service_ = CoordinationService::Create(\n+        tsl::Env::Default(), coordination_config_, std::move(client_cache));\n+    // Set the service pointer for all the tasks since it is needed for handling\n+    // error propagations. In reality, every task has its own service pointer.\n+    // To mimic that, we need multi-process tests.\n+    state_ps_0_.SetCoordinationService(coord_service_.get());\n+    state_ps_1_.SetCoordinationService(coord_service_.get());\n+    state_worker_0_.SetCoordinationService(coord_service_.get());\n+    state_worker_1_.SetCoordinationService(coord_service_.get());\n+    state_ps_0_.InitializeAndConnectCoordinationAgents(kParameterServerJobName,\n+                                                       /*task_id=*/0,\n+                                                       coordination_config_);\n+    state_ps_1_.InitializeAndConnectCoordinationAgents(kParameterServerJobName,\n+                                                       /*task_id=*/1,\n+                                                       coordination_config_);\n+    state_worker_0_.InitializeAndConnectCoordinationAgents(\n+        kWorkerJobName,\n+        /*task_id=*/0, coordination_config_);\n+    state_worker_1_.InitializeAndConnectCoordinationAgents(\n+        kWorkerJobName,\n+        /*task_id=*/1, coordination_config_);\n+  }\n+\n+  void ConfigureCoordinationService() {\n+    // Assume the coordination service is deployed in the parameter server.\n+    coordination_config_.set_service_type(kCoordinationServiceType);\n+    coordination_config_.set_service_leader(kServiceLeader);\n+    CoordinatedJob* ps =\n+        coordination_config_.mutable_coordinated_job_list()->Add();\n+    ps->set_name(kParameterServerJobName);\n+    ps->set_num_tasks(2);\n+    CoordinatedJob* worker =\n+        coordination_config_.mutable_coordinated_job_list()->Add();\n+    worker->set_name(kWorkerJobName);\n+    worker->set_num_tasks(2);\n+  }\n+\n+  void AddJobToRecoverableJobs(const std::string& job_name) {\n+    coordination_config_.add_recoverable_jobs(job_name);\n+  }\n+\n+ protected:\n+  CoordinationServiceConfig coordination_config_;\n+  std::unique_ptr<CoordinationService> coord_service_;\n+  TestCoordinationServiceTaskState state_ps_0_;\n+  TestCoordinationServiceTaskState state_ps_1_;\n+  TestCoordinationServiceTaskState state_worker_0_;\n+  TestCoordinationServiceTaskState state_worker_1_;\n+};\n+\n+TEST_F(CoordinationServiceRecoverableJobTest,\n+       UnrecoverableWorkerFailurePropagated) {\n+  Initialize();\n+  TF_ASSERT_OK(state_worker_0_.ReportError(absl::InternalError(\"Test Error.\")));\n+\n+  // For unrecoverable task, error propagates to all connected tasks.\n+  EXPECT_TRUE(absl::IsInternal(state_ps_0_.GetStatus()));\n+  EXPECT_TRUE(absl::IsInternal(state_ps_1_.GetStatus()));\n+  EXPECT_TRUE(absl::IsInternal(state_worker_0_.GetStatus()));\n+  EXPECT_TRUE(absl::IsInternal(state_worker_1_.GetStatus()));\n+}\n+\n+TEST_F(CoordinationServiceRecoverableJobTest,\n+       UnrecoverablePSFailurePropagated) {\n+  Initialize();\n+  TF_ASSERT_OK(state_ps_0_.ReportError(absl::InternalError(\"Test Error.\")));\n+\n+  // For unrecoverable task, error propagates to all connected tasks.\n+  EXPECT_TRUE(absl::IsInternal(state_ps_0_.GetStatus()));\n+  EXPECT_TRUE(absl::IsInternal(state_ps_1_.GetStatus()));\n+  EXPECT_TRUE(absl::IsInternal(state_worker_0_.GetStatus()));\n+  EXPECT_TRUE(absl::IsInternal(state_worker_1_.GetStatus()));\n+}\n+\n+TEST_F(CoordinationServiceRecoverableJobTest,\n+       RecoverableWorkerFailureNotPropagated) {\n+  AddJobToRecoverableJobs(kWorkerJobName);\n+  Initialize();\n+  TF_ASSERT_OK(state_worker_0_.ReportError(absl::InternalError(\"Test Error.\")));\n+\n+  // For recoverable task, error does not propagate.\n+  EXPECT_TRUE(state_ps_0_.GetStatus().ok());\n+  EXPECT_TRUE(state_ps_1_.GetStatus().ok());\n+  EXPECT_TRUE(absl::IsInternal(state_worker_0_.GetStatus()));\n+  EXPECT_TRUE(state_worker_1_.GetStatus().ok());\n+}\n+\n+}  // namespace\n+}  // namespace xla"
        },
        {
            "sha": "1f95a806c7add134497fbdb511f16faeb7e0a84e",
            "filename": "third_party/xla/xla/pjrt/distributed/coordination/coordination_service_rpc_handler.cc",
            "status": "added",
            "additions": 397,
            "deletions": 0,
            "changes": 397,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_service_rpc_handler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_service_rpc_handler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_service_rpc_handler.cc?ref=3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a",
            "patch": "@@ -0,0 +1,397 @@\n+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/pjrt/distributed/coordination/coordination_service_rpc_handler.h\"\n+\n+#include <cstdint>\n+#include <iterator>\n+#include <optional>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/algorithm/container.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/synchronization/mutex.h\"\n+#include \"absl/time/time.h\"\n+#include \"xla/pjrt/distributed/coordination/coordination_service.h\"\n+#include \"xla/pjrt/distributed/coordination/coordination_service_agent.h\"\n+#include \"xla/pjrt/distributed/coordination/coordination_service_error_util.h\"\n+#include \"xla/tsl/platform/status.h\"\n+#include \"xla/tsl/protobuf/coordination_service.pb.h\"\n+#include \"tsl/platform/protobuf.h\"\n+\n+namespace xla {\n+namespace {\n+using tensorflow::CoordinatedTask;\n+using tensorflow::CoordinationServiceError;\n+using tensorflow::KeyValueEntry;\n+}  // namespace\n+\n+void CoordinationServiceRpcHandler::SetAgentInstance(\n+    CoordinationServiceAgent* agent) {\n+  absl::MutexLock l(mu_);\n+  agent_ = agent;\n+}\n+\n+void CoordinationServiceRpcHandler::SetServiceInstance(\n+    CoordinationService* service) {\n+  absl::MutexLock l(mu_);\n+  service_ = service;\n+}\n+\n+void CoordinationServiceRpcHandler::RegisterTaskAsync(\n+    const tensorflow::RegisterTaskRequest* request,\n+    tensorflow::RegisterTaskResponse* response, tsl::StatusCallback done) {\n+  absl::ReaderMutexLock l(mu_);\n+  if (service_ == nullptr) {\n+    done(MakeCoordinationError(\n+        absl::InternalError(\"Coordination service is not enabled.\")));\n+    return;\n+  }\n+  const CoordinatedTask& task = request->source_task();\n+  const IncarnationId incarnation(request->incarnation());\n+  const IncarnationId leader_incarnation = service_->GetServiceIncarnation();\n+  response->set_leader_incarnation(leader_incarnation.value());\n+  service_->RegisterTaskAsync(task, incarnation, done);\n+}\n+\n+void CoordinationServiceRpcHandler::HeartbeatAsync(\n+    const tensorflow::HeartbeatRequest* request,\n+    tensorflow::HeartbeatResponse* response, tsl::StatusCallback done) {\n+  absl::ReaderMutexLock l(mu_);\n+  if (service_ == nullptr) {\n+    done(MakeCoordinationError(\n+        absl::InternalError(\"Coordination service is not enabled.\")));\n+    return;\n+  }\n+  const CoordinatedTask& task = request->source_task();\n+  const IncarnationId incarnation(request->incarnation());\n+  const IncarnationId leader_incarnation = service_->GetServiceIncarnation();\n+  absl::Status s = service_->RecordHeartbeat(task, incarnation);\n+  if (!s.ok()) {\n+    done(s);\n+    return;\n+  }\n+  response->set_leader_incarnation(leader_incarnation.value());\n+  done(absl::OkStatus());\n+}\n+\n+void CoordinationServiceRpcHandler::WaitForAllTasksAsync(\n+    const tensorflow::WaitForAllTasksRequest* request,\n+    tensorflow::WaitForAllTasksResponse* response, tsl::StatusCallback done) {\n+  absl::ReaderMutexLock l(mu_);\n+  if (service_ == nullptr) {\n+    done(MakeCoordinationError(\n+        absl::InternalError(\"Coordination service is not enabled.\")));\n+    return;\n+  }\n+  service_->WaitForAllTasks(\n+      request->source_task(), request->device_info(),\n+      [response, service = service_, done = std::move(done)](absl::Status s) {\n+        if (s.ok()) {\n+          service->state_mu_.AssertHeld();\n+          *response->mutable_device_info() = service->ListClusterDevices();\n+        }\n+        done(s);\n+      });\n+}\n+\n+void CoordinationServiceRpcHandler::ShutdownTaskAsync(\n+    const tensorflow::ShutdownTaskRequest* request,\n+    tensorflow::ShutdownTaskResponse* response, tsl::StatusCallback done) {\n+  absl::ReaderMutexLock l(mu_);\n+  if (service_ == nullptr) {\n+    done(MakeCoordinationError(\n+        absl::InternalError(\"Coordination service is not enabled.\")));\n+    return;\n+  }\n+  service_->ShutdownTaskAsync(request->source_task(),\n+                              [done](absl::Status s) { done(s); });\n+}\n+\n+void CoordinationServiceRpcHandler::ResetTaskAsync(\n+    const tensorflow::ResetTaskRequest* request,\n+    tensorflow::ResetTaskResponse* response, tsl::StatusCallback done) {\n+  absl::ReaderMutexLock l(mu_);\n+  if (service_ == nullptr) {\n+    done(MakeCoordinationError(\n+        absl::InternalError(\"Coordination service is not enabled.\")));\n+    return;\n+  }\n+  done(service_->ResetTask(request->source_task()));\n+}\n+\n+void CoordinationServiceRpcHandler::ReportErrorToTaskAsync(\n+    const tensorflow::ReportErrorToTaskRequest* request,\n+    tensorflow::ReportErrorToTaskResponse* response, tsl::StatusCallback done) {\n+  absl::ReaderMutexLock l(mu_);\n+  if (agent_ == nullptr) {\n+    done(MakeCoordinationError(absl::InternalError(\n+        \"CoordinationServiceAgent is uninitialized or has already shutdown.\")));\n+    return;\n+  }\n+  const CoordinationServiceError& error_payload = request->error_payload();\n+  absl::Status error(\n+      static_cast<absl::StatusCode>(request->error_code()),\n+      absl::StrCat(\n+          \"Error reported from /job:\", error_payload.source_task().job_name(),\n+          \"/task:\", error_payload.source_task().task_id(), \": \",\n+          request->error_message()));\n+  error = MakeCoordinationError(error, error_payload);\n+  agent_->SetError(error);\n+  done(absl::OkStatus());\n+}\n+\n+void CoordinationServiceRpcHandler::ReportErrorToServiceAsync(\n+    const tensorflow::ReportErrorToServiceRequest* request,\n+    tensorflow::ReportErrorToServiceResponse* response,\n+    tsl::StatusCallback done) {\n+  absl::ReaderMutexLock l(mu_);\n+  if (service_ == nullptr) {\n+    done(MakeCoordinationError(\n+        absl::InternalError(\"Coordination service is not enabled.\")));\n+    return;\n+  }\n+  done(service_->ReportTaskError(\n+      request->error_origin(),\n+      MakeCoordinationError(\n+          absl::Status{static_cast<absl::StatusCode>(request->error_code()),\n+                       request->error_message()},\n+          request->error_origin(),\n+          /*is_reported_error=*/true)));\n+}\n+\n+void CoordinationServiceRpcHandler::GetTaskStateAsync(\n+    const tensorflow::GetTaskStateRequest* request,\n+    tensorflow::GetTaskStateResponse* response, tsl::StatusCallback done) {\n+  absl::ReaderMutexLock l(mu_);\n+  if (service_ == nullptr) {\n+    done(MakeCoordinationError(\n+        absl::InternalError(\"Coordination service is not enabled.\")));\n+    return;\n+  }\n+  auto result = service_->GetTaskState(\n+      {request->source_task().begin(), request->source_task().end()});\n+  absl::c_move(result, tsl::protobuf::RepeatedFieldBackInserter(\n+                           response->mutable_task_state()));\n+  done(absl::OkStatus());\n+}\n+\n+void CoordinationServiceRpcHandler::WatchJobStateAsync(\n+    const tensorflow::WatchJobStateRequest* request,\n+    tensorflow::WatchJobStateResponse* response, tsl::StatusCallback done) {\n+  absl::ReaderMutexLock l(mu_);\n+  if (service_ == nullptr) {\n+    done(MakeCoordinationError(\n+        absl::InternalError(\"Coordination service is not enabled.\")));\n+    return;\n+  }\n+\n+  std::optional<int64_t> version_number;\n+  if (request->version_number() >= 0) {\n+    version_number.emplace(request->version_number());\n+  }\n+  service_->WatchJobState(\n+      request->job_name(), version_number,\n+      [response, done](std::vector<tensorflow::CoordinatedTaskStateInfo> info,\n+                       int64_t version_number) {\n+        absl::c_move(info, tsl::protobuf::RepeatedFieldBackInserter(\n+                               response->mutable_task_state()));\n+        response->set_version_number(version_number);\n+        done(absl::OkStatus());\n+      });\n+}\n+\n+void CoordinationServiceRpcHandler::InsertKeyValueAsync(\n+    const tensorflow::InsertKeyValueRequest* request,\n+    tensorflow::InsertKeyValueResponse* response, tsl::StatusCallback done) {\n+  absl::ReaderMutexLock l(mu_);\n+  if (service_ == nullptr) {\n+    done(MakeCoordinationError(\n+        absl::InternalError(\"Coordination service is not enabled.\")));\n+    return;\n+  }\n+  done(service_->InsertKeyValue(request->kv().key(), request->kv().value(),\n+                                request->allow_overwrite()));\n+}\n+\n+void CoordinationServiceRpcHandler::GetKeyValueAsync(\n+    const tensorflow::GetKeyValueRequest* request,\n+    tensorflow::GetKeyValueResponse* response, tsl::StatusCallback done) {\n+  absl::ReaderMutexLock l(mu_);\n+  if (service_ == nullptr) {\n+    done(MakeCoordinationError(\n+        absl::InternalError(\"Coordination service is not enabled.\")));\n+    return;\n+  }\n+  response->mutable_kv()->set_key(request->key());\n+  service_->GetKeyValueAsync(\n+      request->key(),\n+      [response, done = std::move(done)](\n+          const absl::StatusOr<absl::string_view>& status_or_value) {\n+        if (status_or_value.ok()) {\n+          auto value = status_or_value.value();\n+          response->mutable_kv()->set_value(value.data(), value.size());\n+        }\n+        done(status_or_value.status());\n+      });\n+}\n+\n+void CoordinationServiceRpcHandler::TryGetKeyValueAsync(\n+    const tensorflow::TryGetKeyValueRequest* request,\n+    tensorflow::TryGetKeyValueResponse* response, tsl::StatusCallback done) {\n+  absl::ReaderMutexLock l(mu_);\n+  if (service_ == nullptr) {\n+    done(MakeCoordinationError(\n+        absl::InternalError(\"Coordination service is not enabled.\")));\n+    return;\n+  }\n+  auto result = service_->TryGetKeyValue(request->key());\n+  if (!result.ok()) {\n+    done(MakeCoordinationError(result.status()));\n+    return;\n+  }\n+  response->mutable_kv()->set_key(request->key());\n+  response->mutable_kv()->set_value(result.value());\n+  done(absl::OkStatus());\n+}\n+\n+void CoordinationServiceRpcHandler::IncrementKeyValueAsync(\n+    const tensorflow::IncrementKeyValueRequest* request,\n+    tensorflow::IncrementKeyValueResponse* response, tsl::StatusCallback done) {\n+  absl::ReaderMutexLock l(mu_);\n+  if (service_ == nullptr) {\n+    done(MakeCoordinationError(\n+        absl::InternalError(\"Coordination service is not enabled.\")));\n+    return;\n+  }\n+  auto result =\n+      service_->IncrementKeyValue(request->key(), request->increment());\n+  if (!result.ok()) {\n+    done(MakeCoordinationError(result.status()));\n+    return;\n+  }\n+  response->mutable_kv()->set_key(request->key());\n+  response->mutable_kv()->set_value(result.value());\n+  done(absl::OkStatus());\n+}\n+\n+void CoordinationServiceRpcHandler::GetKeyValueDirAsync(\n+    const tensorflow::GetKeyValueDirRequest* request,\n+    tensorflow::GetKeyValueDirResponse* response, tsl::StatusCallback done) {\n+  absl::ReaderMutexLock l(mu_);\n+  if (service_ == nullptr) {\n+    done(MakeCoordinationError(\n+        absl::InternalError(\"Coordination service is not enabled.\")));\n+    return;\n+  }\n+  std::vector<KeyValueEntry> results =\n+      service_->GetKeyValueDir(request->directory_key());\n+  *response->mutable_kv() = {std::make_move_iterator(results.begin()),\n+                             std::make_move_iterator(results.end())};\n+  done(absl::OkStatus());\n+}\n+\n+void CoordinationServiceRpcHandler::DeleteKeyValueAsync(\n+    const tensorflow::DeleteKeyValueRequest* request,\n+    tensorflow::DeleteKeyValueResponse* response, tsl::StatusCallback done) {\n+  absl::ReaderMutexLock l(mu_);\n+  if (service_ == nullptr) {\n+    done(MakeCoordinationError(\n+        absl::InternalError(\"Coordination service is not enabled.\")));\n+    return;\n+  }\n+  done(service_->DeleteKeyValue(request->key()));\n+}\n+\n+void CoordinationServiceRpcHandler::BarrierAsync(\n+    const tensorflow::BarrierRequest* request,\n+    tensorflow::BarrierResponse* response, tsl::StatusCallback done) {\n+  absl::ReaderMutexLock l(mu_);\n+  if (service_ == nullptr) {\n+    done(MakeCoordinationError(\n+        absl::InternalError(\"Coordination service is not enabled.\")));\n+    return;\n+  }\n+  std::vector<CoordinatedTask> tasks = {request->tasks().begin(),\n+                                        request->tasks().end()};\n+  service_->BarrierAsync(request->barrier_id(), request->counter(),\n+                         absl::Milliseconds(request->barrier_timeout_in_ms()),\n+                         request->source_task(), tasks,\n+                         [done = std::move(done), response](\n+                             const absl::Status& status, int64_t counter) {\n+                           response->set_counter(counter);\n+                           done(status);\n+                         });\n+}\n+\n+void CoordinationServiceRpcHandler::CancelBarrierAsync(\n+    const tensorflow::CancelBarrierRequest* request,\n+    tensorflow::CancelBarrierResponse* response, tsl::StatusCallback done) {\n+  absl::ReaderMutexLock l(mu_);\n+  if (service_ == nullptr) {\n+    done(MakeCoordinationError(\n+        absl::InternalError(\"Coordination service is not enabled.\")));\n+    return;\n+  }\n+  done(service_->CancelBarrier(request->barrier_id(), request->counter(),\n+                               request->source_task()));\n+}\n+\n+void CoordinationServiceRpcHandler::GetAliveTasksAsync(\n+    const tensorflow::GetAliveTasksRequest* request,\n+    tensorflow::GetAliveTasksResponse* response, tsl::StatusCallback done) {\n+  absl::ReaderMutexLock l(mu_);\n+  if (service_ == nullptr) {\n+    done(MakeCoordinationError(\n+        absl::InternalError(\"Coordination service is not enabled.\")));\n+    return;\n+  }\n+\n+  std::vector<CoordinatedTask> tasks = {request->tasks().begin(),\n+                                        request->tasks().end()};\n+  service_->GetAliveTasksAsync(\n+      request->requesting_task(), tasks,\n+      [done = std::move(done), response](\n+          const absl::Status& status,\n+          const std::vector<tensorflow::CoordinatedTask>& alive_tasks,\n+          const std::vector<IncarnationId>& incarnations) {\n+        *response->mutable_alive_tasks() = {alive_tasks.begin(),\n+                                            alive_tasks.end()};\n+        for (IncarnationId id : incarnations) {\n+          response->add_incarnations(id.value());\n+        }\n+        done(status);\n+      });\n+}\n+\n+void CoordinationServiceRpcHandler::PollForErrorAsync(\n+    const tensorflow::PollForErrorRequest* request,\n+    tensorflow::PollForErrorResponse* response, tsl::StatusCallback done) {\n+  absl::ReaderMutexLock l(mu_);\n+  if (service_ == nullptr) {\n+    done(MakeCoordinationError(\n+        absl::InternalError(\"Coordination service is not enabled.\")));\n+    return;\n+  }\n+  service_->PollForErrorAsync(\n+      request->source_task(),\n+      [done = std::move(done)](const absl::Status& status) { done(status); });\n+}\n+\n+}  // namespace xla"
        },
        {
            "sha": "7e71594ddfb2845b59d8470957790e4bb42c54b1",
            "filename": "third_party/xla/xla/pjrt/distributed/coordination/coordination_service_rpc_handler.h",
            "status": "added",
            "additions": 122,
            "deletions": 0,
            "changes": 122,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_service_rpc_handler.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_service_rpc_handler.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_service_rpc_handler.h?ref=3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a",
            "patch": "@@ -0,0 +1,122 @@\n+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_PJRT_DISTRIBUTED_COORDINATION_COORDINATION_SERVICE_RPC_HANDLER_H_\n+#define XLA_PJRT_DISTRIBUTED_COORDINATION_COORDINATION_SERVICE_RPC_HANDLER_H_\n+\n+#include \"absl/synchronization/mutex.h\"\n+#include \"xla/pjrt/distributed/coordination/coordination_service.h\"\n+#include \"xla/pjrt/distributed/coordination/coordination_service_agent.h\"\n+#include \"xla/tsl/platform/status.h\"\n+#include \"xla/tsl/protobuf/coordination_service.pb.h\"\n+#include \"tsl/platform/thread_annotations.h\"\n+\n+namespace xla {\n+class CoordinationServiceRpcHandler {\n+ public:\n+  explicit CoordinationServiceRpcHandler() = default;\n+\n+  void SetAgentInstance(CoordinationServiceAgent* agent);\n+\n+  void SetServiceInstance(CoordinationService* service);\n+\n+  void RegisterTaskAsync(const tensorflow::RegisterTaskRequest* request,\n+                         tensorflow::RegisterTaskResponse* response,\n+                         tsl::StatusCallback done);\n+\n+  void HeartbeatAsync(const tensorflow::HeartbeatRequest* request,\n+                      tensorflow::HeartbeatResponse* response,\n+                      tsl::StatusCallback done);\n+\n+  void WaitForAllTasksAsync(const tensorflow::WaitForAllTasksRequest* request,\n+                            tensorflow::WaitForAllTasksResponse* response,\n+                            tsl::StatusCallback done);\n+\n+  void ShutdownTaskAsync(const tensorflow::ShutdownTaskRequest* request,\n+                         tensorflow::ShutdownTaskResponse* response,\n+                         tsl::StatusCallback done);\n+\n+  void ResetTaskAsync(const tensorflow::ResetTaskRequest* request,\n+                      tensorflow::ResetTaskResponse* response,\n+                      tsl::StatusCallback done);\n+\n+  void ReportErrorToTaskAsync(\n+      const tensorflow::ReportErrorToTaskRequest* request,\n+      tensorflow::ReportErrorToTaskResponse* response,\n+      tsl::StatusCallback done);\n+\n+  void ReportErrorToServiceAsync(\n+      const tensorflow::ReportErrorToServiceRequest* request,\n+      tensorflow::ReportErrorToServiceResponse* response,\n+      tsl::StatusCallback done);\n+\n+  void GetTaskStateAsync(const tensorflow::GetTaskStateRequest* request,\n+                         tensorflow::GetTaskStateResponse* response,\n+                         tsl::StatusCallback done);\n+\n+  void WatchJobStateAsync(const tensorflow::WatchJobStateRequest* request,\n+                          tensorflow::WatchJobStateResponse* response,\n+                          tsl::StatusCallback done);\n+\n+  void InsertKeyValueAsync(const tensorflow::InsertKeyValueRequest* request,\n+                           tensorflow::InsertKeyValueResponse* response,\n+                           tsl::StatusCallback done);\n+\n+  void GetKeyValueAsync(const tensorflow::GetKeyValueRequest* request,\n+                        tensorflow::GetKeyValueResponse* response,\n+                        tsl::StatusCallback done);\n+\n+  void IncrementKeyValueAsync(\n+      const tensorflow::IncrementKeyValueRequest* request,\n+      tensorflow::IncrementKeyValueResponse* response,\n+      tsl::StatusCallback done);\n+\n+  void TryGetKeyValueAsync(const tensorflow::TryGetKeyValueRequest* request,\n+                           tensorflow::TryGetKeyValueResponse* response,\n+                           tsl::StatusCallback done);\n+\n+  void GetKeyValueDirAsync(const tensorflow::GetKeyValueDirRequest* request,\n+                           tensorflow::GetKeyValueDirResponse* response,\n+                           tsl::StatusCallback done);\n+\n+  void DeleteKeyValueAsync(const tensorflow::DeleteKeyValueRequest* request,\n+                           tensorflow::DeleteKeyValueResponse* response,\n+                           tsl::StatusCallback done);\n+\n+  void BarrierAsync(const tensorflow::BarrierRequest* request,\n+                    tensorflow::BarrierResponse* response,\n+                    tsl::StatusCallback done);\n+\n+  void CancelBarrierAsync(const tensorflow::CancelBarrierRequest* request,\n+                          tensorflow::CancelBarrierResponse* response,\n+                          tsl::StatusCallback done);\n+\n+  void GetAliveTasksAsync(const tensorflow::GetAliveTasksRequest* request,\n+                          tensorflow::GetAliveTasksResponse* response,\n+                          tsl::StatusCallback done);\n+\n+  void PollForErrorAsync(const tensorflow::PollForErrorRequest* request,\n+                         tensorflow::PollForErrorResponse* response,\n+                         tsl::StatusCallback done);\n+\n+ private:\n+  absl::Mutex mu_;\n+  CoordinationServiceAgent* agent_ TF_GUARDED_BY(mu_) = nullptr;\n+  CoordinationService* service_ TF_GUARDED_BY(mu_) = nullptr;\n+};\n+\n+}  // namespace xla\n+\n+#endif  // XLA_PJRT_DISTRIBUTED_COORDINATION_COORDINATION_SERVICE_RPC_HANDLER_H_"
        },
        {
            "sha": "55ee5a32ed1527a092e69c70648501fe9d8d11a3",
            "filename": "third_party/xla/xla/pjrt/distributed/coordination/coordination_service_test.cc",
            "status": "added",
            "additions": 2700,
            "deletions": 0,
            "changes": 2700,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_service_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_service_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fcoordination_service_test.cc?ref=3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a"
        },
        {
            "sha": "bdcd5d76b579eae384f34bbaf714dae07916ba27",
            "filename": "third_party/xla/xla/pjrt/distributed/coordination/grpc_coordination_client.cc",
            "status": "added",
            "additions": 408,
            "deletions": 0,
            "changes": 408,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fgrpc_coordination_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fgrpc_coordination_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fgrpc_coordination_client.cc?ref=3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a",
            "patch": "@@ -0,0 +1,408 @@\n+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/pjrt/distributed/coordination/grpc_coordination_client.h\"\n+\n+#include <cstddef>\n+#include <memory>\n+#include <string>\n+#include <unordered_map>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/base/thread_annotations.h\"\n+#include \"absl/log/log.h\"\n+#include \"absl/synchronization/mutex.h\"\n+#include \"grpcpp/channel.h\"\n+#include \"grpcpp/completion_queue.h\"\n+#include \"grpcpp/generic/generic_stub.h\"\n+#include \"xla/pjrt/distributed/coordination/coordination_client.h\"\n+#include \"xla/tsl/distributed_runtime/call_options.h\"\n+#include \"xla/tsl/distributed_runtime/rpc/grpc_channel.h\"\n+#include \"xla/tsl/distributed_runtime/rpc/grpc_client_cq_tag.h\"\n+#include \"xla/tsl/distributed_runtime/rpc/grpc_state.h\"\n+#include \"xla/tsl/distributed_runtime/rpc/grpc_util.h\"\n+#include \"xla/tsl/platform/env.h\"\n+#include \"xla/tsl/platform/status.h\"\n+#include \"xla/tsl/protobuf/coordination_service.pb.h\"\n+#include \"tsl/platform/protobuf.h\"\n+\n+namespace xla {\n+namespace {\n+using tensorflow::BarrierRequest;\n+using tensorflow::BarrierResponse;\n+using tensorflow::CancelBarrierRequest;\n+using tensorflow::CancelBarrierResponse;\n+using tensorflow::DeleteKeyValueRequest;\n+using tensorflow::DeleteKeyValueResponse;\n+using tensorflow::GetAliveTasksRequest;\n+using tensorflow::GetAliveTasksResponse;\n+using tensorflow::GetKeyValueDirRequest;\n+using tensorflow::GetKeyValueDirResponse;\n+using tensorflow::GetKeyValueRequest;\n+using tensorflow::GetKeyValueResponse;\n+using tensorflow::GetTaskStateRequest;\n+using tensorflow::GetTaskStateResponse;\n+using tensorflow::HeartbeatRequest;\n+using tensorflow::HeartbeatResponse;\n+using tensorflow::IncrementKeyValueRequest;\n+using tensorflow::IncrementKeyValueResponse;\n+using tensorflow::InsertKeyValueRequest;\n+using tensorflow::InsertKeyValueResponse;\n+using tensorflow::PollForErrorRequest;\n+using tensorflow::PollForErrorResponse;\n+using tensorflow::RegisterTaskRequest;\n+using tensorflow::RegisterTaskResponse;\n+using tensorflow::ReportErrorToServiceRequest;\n+using tensorflow::ReportErrorToServiceResponse;\n+using tensorflow::ReportErrorToTaskRequest;\n+using tensorflow::ReportErrorToTaskResponse;\n+using tensorflow::ResetTaskRequest;\n+using tensorflow::ResetTaskResponse;\n+using tensorflow::ShutdownTaskRequest;\n+using tensorflow::ShutdownTaskResponse;\n+using tensorflow::TryGetKeyValueRequest;\n+using tensorflow::TryGetKeyValueResponse;\n+using tensorflow::WaitForAllTasksRequest;\n+using tensorflow::WaitForAllTasksResponse;\n+using tensorflow::WatchJobStateRequest;\n+using tensorflow::WatchJobStateResponse;\n+\n+class GrpcCoordinationClientThread {\n+ public:\n+  GrpcCoordinationClientThread() {\n+    thread_.reset(tsl::Env::Default()->StartThread(\n+        tsl::ThreadOptions(), \"coordination_client_thread\", [this]() {\n+          void* tag;\n+          bool ok;\n+          while (completion_queue_.Next(&tag, &ok)) {\n+            VLOG(4) << \"GrpcCoordinationClientThread got next tag\";\n+            tsl::GrpcClientCQTag* callback_tag =\n+                static_cast<tsl::GrpcClientCQTag*>(tag);\n+            callback_tag->OnCompleted(ok);\n+            VLOG(4) << \"GrpcCoordinationClientThread blocking for next tag\";\n+          }\n+          VLOG(4) << \"GrpcCoordinationClientThread exiting\";\n+        }));\n+  }\n+\n+  ~GrpcCoordinationClientThread() {\n+    completion_queue_.Shutdown();\n+    thread_.reset();\n+  }\n+\n+  ::grpc::CompletionQueue* completion_queue() { return &completion_queue_; }\n+\n+ private:\n+  ::grpc::CompletionQueue completion_queue_;\n+  std::unique_ptr<tsl::Thread> thread_;\n+};\n+\n+class GrpcCoordinationClient : public CoordinationClient {\n+ public:\n+  GrpcCoordinationClient(tsl::SharedGrpcChannelPtr channel,\n+                         ::grpc::CompletionQueue* cq, const std::string& target)\n+      : stub_(channel), cq_(cq), target_(target) {}\n+  GrpcCoordinationClient(tsl::SharedGrpcChannelPtr channel,\n+                         const std::string& target)\n+      : stub_(channel), target_(target) {\n+    client_thread_ = std::make_unique<GrpcCoordinationClientThread>();\n+    cq_ = client_thread_->completion_queue();\n+  }\n+  ~GrpcCoordinationClient() override = default;\n+\n+  void RegisterTaskAsync(tsl::CallOptions* call_opts,\n+                         const RegisterTaskRequest* request,\n+                         RegisterTaskResponse* response,\n+                         tsl::StatusCallback done) override {\n+    new tsl::RPCState<tsl::protobuf::Message>(\n+        &stub_, cq_, \"/tensorflow.CoordinationService/RegisterTask\", *request,\n+        response, std::move(done), call_opts,\n+        /*threadpool=*/nullptr, /*max_retries=*/0, /*fail_fast=*/false,\n+        &target_);\n+  }\n+\n+  void WaitForAllTasksAsync(const WaitForAllTasksRequest* request,\n+                            WaitForAllTasksResponse* response,\n+                            tsl::StatusCallback done) override {\n+    new tsl::RPCState<tsl::protobuf::Message>(\n+        &stub_, cq_, \"/tensorflow.CoordinationService/WaitForAllTasks\",\n+        *request, response, std::move(done), /*call_opts=*/nullptr,\n+        /*threadpool=*/nullptr, /*max_retries=*/0, /*fail_fast=*/true,\n+        &target_);\n+  }\n+\n+  void ShutdownTaskAsync(tsl::CallOptions* call_opts,\n+                         const ShutdownTaskRequest* request,\n+                         ShutdownTaskResponse* response,\n+                         tsl::StatusCallback done) override {\n+    new tsl::RPCState<tsl::protobuf::Message>(\n+        &stub_, cq_, \"/tensorflow.CoordinationService/ShutdownTask\", *request,\n+        response, std::move(done), call_opts,\n+        /*threadpool=*/nullptr, /*max_retries=*/0, /*fail_fast=*/true,\n+        &target_);\n+  }\n+\n+  void ResetTaskAsync(const ResetTaskRequest* request,\n+                      ResetTaskResponse* response,\n+                      tsl::StatusCallback done) override {\n+    new tsl::RPCState<tsl::protobuf::Message>(\n+        &stub_, cq_, \"/tensorflow.CoordinationService/ResetTask\", *request,\n+        response, std::move(done), /*call_opts=*/nullptr,\n+        /*threadpool=*/nullptr, /*max_retries=*/0, /*fail_fast=*/true,\n+        &target_);\n+  }\n+\n+  void HeartbeatAsync(tsl::CallOptions* call_opts,\n+                      const HeartbeatRequest* request,\n+                      HeartbeatResponse* response,\n+                      tsl::StatusCallback done) override {\n+    // Different from other RPCs which do not retry by default, the Heartbeat\n+    // RPC should retry automatically to tolerate transient network issues.\n+    new tsl::RPCState<tsl::protobuf::Message>(\n+        &stub_, cq_, \"/tensorflow.CoordinationService/Heartbeat\", *request,\n+        response, std::move(done), call_opts, /*threadpool=*/nullptr,\n+        /*max_retries=*/3,\n+        /*fail_fast=*/true, &target_);\n+  }\n+\n+  void ReportErrorToTaskAsync(tsl::CallOptions* call_opts,\n+                              const ReportErrorToTaskRequest* request,\n+                              ReportErrorToTaskResponse* response,\n+                              tsl::StatusCallback done) override {\n+    new tsl::RPCState<tsl::protobuf::Message>(\n+        &stub_, cq_, \"/tensorflow.CoordinationService/ReportErrorToTask\",\n+        *request, response, std::move(done), call_opts,\n+        /*threadpool=*/nullptr, /*max_retries=*/0, /*fail_fast=*/true,\n+        &target_);\n+  }\n+\n+  void ReportErrorToServiceAsync(const ReportErrorToServiceRequest* request,\n+                                 ReportErrorToServiceResponse* response,\n+                                 tsl::StatusCallback done) override {\n+    new tsl::RPCState<tsl::protobuf::Message>(\n+        &stub_, cq_, \"/tensorflow.CoordinationService/ReportErrorToService\",\n+        *request, response, std::move(done), /*call_opts=*/nullptr,\n+        /*threadpool=*/nullptr, /*max_retries=*/0, /*fail_fast=*/true,\n+        &target_);\n+  }\n+\n+  void GetTaskStateAsync(const GetTaskStateRequest* request,\n+                         GetTaskStateResponse* response,\n+                         tsl::StatusCallback done) override {\n+    new tsl::RPCState<tsl::protobuf::Message>(\n+        &stub_, cq_, \"/tensorflow.CoordinationService/GetTaskState\", *request,\n+        response, std::move(done), /*call_opts=*/nullptr,\n+        /*threadpool=*/nullptr, /*max_retries=*/0, /*fail_fast=*/true,\n+        &target_);\n+  }\n+\n+  void WatchJobStateAsync(tsl::CallOptions* call_opts,\n+                          const WatchJobStateRequest* request,\n+                          WatchJobStateResponse* response,\n+                          tsl::StatusCallback done) override {\n+    new tsl::RPCState<tsl::protobuf::Message>(\n+        &stub_, cq_, \"/tensorflow.CoordinationService/WatchJobState\", *request,\n+        response, std::move(done), call_opts,\n+        /*threadpool=*/nullptr, /*max_retries=*/0, /*fail_fast=*/true,\n+        &target_);\n+  }\n+\n+  void InsertKeyValueAsync(const InsertKeyValueRequest* request,\n+                           InsertKeyValueResponse* response,\n+                           tsl::StatusCallback done) override {\n+    new tsl::RPCState<tsl::protobuf::Message>(\n+        &stub_, cq_, \"/tensorflow.CoordinationService/InsertKeyValue\", *request,\n+        response, std::move(done), /*call_opts=*/nullptr,\n+        /*threadpool=*/nullptr, /*max_retries=*/0, /*fail_fast=*/true,\n+        &target_);\n+  }\n+\n+  void GetKeyValueAsync(tsl::CallOptions* call_opts,\n+                        const GetKeyValueRequest* request,\n+                        GetKeyValueResponse* response,\n+                        tsl::StatusCallback done) override {\n+    new tsl::RPCState<tsl::protobuf::Message>(\n+        &stub_, cq_, \"/tensorflow.CoordinationService/GetKeyValue\", *request,\n+        response, std::move(done), call_opts,\n+        /*threadpool=*/nullptr, /*max_retries=*/0, /*fail_fast=*/true,\n+        &target_);\n+  }\n+\n+  void TryGetKeyValueAsync(const TryGetKeyValueRequest* request,\n+                           TryGetKeyValueResponse* response,\n+                           tsl::StatusCallback done) override {\n+    new tsl::RPCState<tsl::protobuf::Message>(\n+        &stub_, cq_, \"/tensorflow.CoordinationService/TryGetKeyValue\", *request,\n+        response, std::move(done), /*call_opts=*/nullptr,\n+        /*threadpool=*/nullptr, /*max_retries=*/0, /*fail_fast=*/true,\n+        &target_);\n+  }\n+\n+  void IncrementKeyValueAsync(const IncrementKeyValueRequest* request,\n+                              IncrementKeyValueResponse* response,\n+                              tsl::StatusCallback done) override {\n+    new tsl::RPCState<tsl::protobuf::Message>(\n+        &stub_, cq_, \"/tensorflow.CoordinationService/IncrementKeyValue\",\n+        *request, response, std::move(done), /*call_opts=*/nullptr,\n+        /*threadpool=*/nullptr, /*max_retries=*/0, /*fail_fast=*/true,\n+        &target_);\n+  }\n+\n+  void GetKeyValueDirAsync(const GetKeyValueDirRequest* request,\n+                           GetKeyValueDirResponse* response,\n+                           tsl::StatusCallback done) override {\n+    new tsl::RPCState<tsl::protobuf::Message>(\n+        &stub_, cq_, \"/tensorflow.CoordinationService/GetKeyValueDir\", *request,\n+        response, std::move(done), /*call_opts=*/nullptr,\n+        /*threadpool=*/nullptr, /*max_retries=*/0, /*fail_fast=*/true,\n+        &target_);\n+  }\n+\n+  void DeleteKeyValueAsync(const DeleteKeyValueRequest* request,\n+                           DeleteKeyValueResponse* response,\n+                           tsl::StatusCallback done) override {\n+    new tsl::RPCState<tsl::protobuf::Message>(\n+        &stub_, cq_, \"/tensorflow.CoordinationService/DeleteKeyValue\", *request,\n+        response, std::move(done), /*call_opts=*/nullptr,\n+        /*threadpool=*/nullptr, /*max_retries=*/0, /*fail_fast=*/true,\n+        &target_);\n+  }\n+\n+  void BarrierAsync(tsl::CallOptions* call_opts, const BarrierRequest* request,\n+                    BarrierResponse* response,\n+                    tsl::StatusCallback done) override {\n+    new tsl::RPCState<tsl::protobuf::Message>(\n+        &stub_, cq_, \"/tensorflow.CoordinationService/Barrier\", *request,\n+        response, std::move(done), call_opts,\n+        /*threadpool=*/nullptr, /*max_retries=*/0, /*fail_fast=*/true,\n+        &target_);\n+  }\n+\n+  void CancelBarrierAsync(const CancelBarrierRequest* request,\n+                          CancelBarrierResponse* response,\n+                          tsl::StatusCallback done) override {\n+    new tsl::RPCState<tsl::protobuf::Message>(\n+        &stub_, cq_, \"/tensorflow.CoordinationService/CancelBarrier\", *request,\n+        response, std::move(done), /*call_opts=*/nullptr,\n+        /*threadpool=*/nullptr, /*max_retries=*/0, /*fail_fast=*/true,\n+        &target_);\n+  }\n+\n+  void GetAliveTasksAsync(const GetAliveTasksRequest* request,\n+                          GetAliveTasksResponse* response,\n+                          tsl::StatusCallback done) override {\n+    new tsl::RPCState<tsl::protobuf::Message>(\n+        &stub_, cq_, \"/tensorflow.CoordinationService/GetAliveTasks\", *request,\n+        response, std::move(done), /*call_opts=*/nullptr,\n+        /*threadpool=*/nullptr, /*max_retries=*/0, /*fail_fast=*/true,\n+        &target_);\n+  }\n+\n+  void PollForErrorAsync(tsl::CallOptions* call_opts,\n+                         const PollForErrorRequest* request,\n+                         PollForErrorResponse* response,\n+                         tsl::StatusCallback done) override {\n+    new tsl::RPCState<tsl::protobuf::Message>(\n+        &stub_, cq_, \"/tensorflow.CoordinationService/PollForError\", *request,\n+        response, std::move(done), call_opts,\n+        /*threadpool=*/nullptr, /*max_retries=*/0, /*fail_fast=*/true,\n+        &target_);\n+  }\n+\n+ private:\n+  ::grpc::GenericStub stub_;\n+  ::grpc::CompletionQueue* cq_;\n+  const std::string target_;\n+  std::unique_ptr<GrpcCoordinationClientThread> client_thread_;\n+};\n+\n+class GrpcCoordinationClientCache : public CoordinationClientCache {\n+ public:\n+  explicit GrpcCoordinationClientCache(\n+      std::shared_ptr<tsl::GrpcChannelCache> channel_cache)\n+      : next_round_robin_assignment_(0),\n+        channel_cache_(channel_cache),\n+        threads_(4) {}\n+\n+  ~GrpcCoordinationClientCache() override = default;\n+\n+  CoordinationClient* GetClient(const std::string& target) override {\n+    absl::MutexLock l(clients_mu_);\n+    auto it = clients_.find(target);\n+    if (it == clients_.end()) {\n+      tsl::SharedGrpcChannelPtr channel =\n+          channel_cache_->FindWorkerChannel(target);\n+      if (channel == nullptr) {\n+        VLOG(2) << \"Coordination client for target \" << target << \" not found.\";\n+      }\n+      int assigned_index = AssignClientToThread(target);\n+      auto coord_client = std::make_unique<GrpcCoordinationClient>(\n+          channel, threads_[assigned_index].completion_queue(), target);\n+      it = clients_.emplace(target, std::move(coord_client)).first;\n+    }\n+    return it->second.get();\n+  }\n+\n+  std::unique_ptr<CoordinationClient> GetOwnedClient(\n+      const std::string& target) override {\n+    tsl::SharedGrpcChannelPtr channel =\n+        channel_cache_->FindWorkerChannel(target);\n+    if (channel == nullptr) {\n+      VLOG(2) << \"Coordination client for target \" << target << \" not found.\";\n+    }\n+    return std::make_unique<GrpcCoordinationClient>(channel, target);\n+  }\n+\n+ private:\n+  absl::Mutex assignment_mu_;\n+  std::unordered_map<std::string, size_t> target_assignments_\n+      ABSL_GUARDED_BY(assignment_mu_);\n+  size_t next_round_robin_assignment_ ABSL_GUARDED_BY(assignment_mu_);\n+\n+  size_t AssignClientToThread(const std::string& target) {\n+    // Round-robin target assignment, but keeps the same target on the same\n+    // polling thread always, as this is important for gRPC performance\n+    absl::MutexLock l(assignment_mu_);\n+    auto it = target_assignments_.find(target);\n+    if (it == target_assignments_.end()) {\n+      it = target_assignments_\n+               .insert(std::make_pair(\n+                   target, (next_round_robin_assignment_++) % threads_.size()))\n+               .first;\n+    }\n+    return it->second;\n+  }\n+\n+  std::shared_ptr<tsl::GrpcChannelCache> channel_cache_;\n+  mutable absl::Mutex clients_mu_;\n+  std::unordered_map<std::string, std::unique_ptr<CoordinationClient>> clients_\n+      ABSL_GUARDED_BY(clients_mu_);\n+  std::vector<GrpcCoordinationClientThread> threads_;\n+};\n+\n+}  // namespace\n+\n+CoordinationClientCache* NewGrpcCoordinationClientCache(\n+    std::shared_ptr<tsl::GrpcChannelCache> channel_cache) {\n+  return new GrpcCoordinationClientCache(channel_cache);\n+}\n+\n+CoordinationClient* NewGrpcCoordinationClient(\n+    std::shared_ptr<::grpc::Channel> channel) {\n+  return new GrpcCoordinationClient(channel, /*target=*/\"coordination_service\");\n+}\n+\n+}  // namespace xla"
        },
        {
            "sha": "0661b3a7b18df8108cc1ec1c8b2cf1314562e662",
            "filename": "third_party/xla/xla/pjrt/distributed/coordination/grpc_coordination_client.h",
            "status": "added",
            "additions": 34,
            "deletions": 0,
            "changes": 34,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fgrpc_coordination_client.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fgrpc_coordination_client.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fgrpc_coordination_client.h?ref=3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a",
            "patch": "@@ -0,0 +1,34 @@\n+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_PJRT_DISTRIBUTED_COORDINATION_GRPC_COORDINATION_CLIENT_H_\n+#define XLA_PJRT_DISTRIBUTED_COORDINATION_GRPC_COORDINATION_CLIENT_H_\n+\n+#include <memory>\n+\n+#include \"xla/pjrt/distributed/coordination/coordination_client.h\"\n+#include \"xla/tsl/distributed_runtime/rpc/grpc_channel.h\"\n+\n+namespace xla {\n+\n+CoordinationClientCache* NewGrpcCoordinationClientCache(\n+    std::shared_ptr<tsl::GrpcChannelCache> channel);\n+\n+CoordinationClient* NewGrpcCoordinationClient(\n+    std::shared_ptr<::grpc::Channel> channel);\n+\n+}  // namespace xla\n+\n+#endif  // XLA_PJRT_DISTRIBUTED_COORDINATION_GRPC_COORDINATION_CLIENT_H_"
        },
        {
            "sha": "27af5e9104ffb45f2d93a773874027fc328a673b",
            "filename": "third_party/xla/xla/pjrt/distributed/coordination/grpc_coordination_service_impl.cc",
            "status": "added",
            "additions": 96,
            "deletions": 0,
            "changes": 96,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fgrpc_coordination_service_impl.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fgrpc_coordination_service_impl.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fgrpc_coordination_service_impl.cc?ref=3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a",
            "patch": "@@ -0,0 +1,96 @@\n+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/pjrt/distributed/coordination/grpc_coordination_service_impl.h\"\n+\n+#include \"absl/synchronization/mutex.h\"\n+#include \"xla/tsl/platform/threadpool.h\"\n+\n+namespace xla {\n+\n+GrpcCoordinationServiceImpl::GrpcCoordinationServiceImpl(\n+    tsl::thread::ThreadPool* compute_pool,\n+    ::grpc::ServerBuilder* server_builder)\n+    : compute_pool_(*compute_pool), shutdown_(false) {\n+  server_builder->RegisterService(&service_);\n+  cq_ = server_builder->AddCompletionQueue();\n+}\n+\n+void GrpcCoordinationServiceImpl::HandleRPCsLoop() {\n+#define ENQUEUE_REQUEST(method)                                               \\\n+  do {                                                                        \\\n+    absl::ReaderMutexLock l(&shutdown_mu_);                                   \\\n+    if (shutdown_) {                                                          \\\n+      continue;                                                               \\\n+    }                                                                         \\\n+    tsl::Call<GrpcCoordinationServiceImpl,                                    \\\n+              tensorflow::grpc::CoordinationService::AsyncService,            \\\n+              tensorflow::method##Request, tensorflow::method##Response>::    \\\n+        EnqueueRequest(&service_, cq_.get(),                                  \\\n+                       &tensorflow::grpc::CoordinationService::AsyncService:: \\\n+                           Request##method,                                   \\\n+                       &GrpcCoordinationServiceImpl::method##Handler, false); \\\n+  } while (0)\n+  ENQUEUE_REQUEST(RegisterTask);\n+  ENQUEUE_REQUEST(WaitForAllTasks);\n+  ENQUEUE_REQUEST(ShutdownTask);\n+  ENQUEUE_REQUEST(ResetTask);\n+  ENQUEUE_REQUEST(Heartbeat);\n+  ENQUEUE_REQUEST(ReportErrorToTask);\n+  ENQUEUE_REQUEST(ReportErrorToService);\n+  ENQUEUE_REQUEST(GetTaskState);\n+  ENQUEUE_REQUEST(WatchJobState);\n+  ENQUEUE_REQUEST(InsertKeyValue);\n+  ENQUEUE_REQUEST(GetKeyValue);\n+  ENQUEUE_REQUEST(TryGetKeyValue);\n+  ENQUEUE_REQUEST(IncrementKeyValue);\n+  ENQUEUE_REQUEST(GetKeyValueDir);\n+  ENQUEUE_REQUEST(DeleteKeyValue);\n+  ENQUEUE_REQUEST(Barrier);\n+  ENQUEUE_REQUEST(CancelBarrier);\n+  ENQUEUE_REQUEST(GetAliveTasks);\n+  ENQUEUE_REQUEST(PollForError);\n+#undef ENQUEUE_REQUEST\n+\n+  void* tag;  // Matches the operation started against this cq_.\n+  bool ok;\n+\n+  while (true) {\n+    if (!cq_->Next(&tag, &ok)) {\n+      // The queue is shutting down.\n+      break;\n+    }\n+    tsl::GrpcCallTag<GrpcCoordinationServiceImpl>* callback_tag =\n+        static_cast<tsl::GrpcCallTag<GrpcCoordinationServiceImpl>*>(tag);\n+\n+    if (callback_tag) {\n+      callback_tag->OnCompleted(this, ok);\n+    } else {\n+      cq_->Shutdown();\n+      break;\n+    }\n+  }\n+}\n+\n+void GrpcCoordinationServiceImpl::Shutdown() {\n+  absl::MutexLock l(shutdown_mu_);\n+  shutdown_ = true;\n+  // This enqueues a special event (with a null tag) that causes the completion\n+  // queue to be shut down on the polling thread.\n+  shutdown_alarm_ = std::make_unique<::grpc::Alarm>(\n+      cq_.get(), gpr_now(GPR_CLOCK_MONOTONIC), nullptr);\n+}\n+\n+}  // namespace xla"
        },
        {
            "sha": "3f699619273755072c02ab25e3d01426ace3838d",
            "filename": "third_party/xla/xla/pjrt/distributed/coordination/grpc_coordination_service_impl.h",
            "status": "added",
            "additions": 124,
            "deletions": 0,
            "changes": 124,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fgrpc_coordination_service_impl.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fgrpc_coordination_service_impl.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fgrpc_coordination_service_impl.h?ref=3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a",
            "patch": "@@ -0,0 +1,124 @@\n+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_PJRT_DISTRIBUTED_COORDINATION_GRPC_COORDINATION_SERVICE_IMPL_H_\n+#define XLA_PJRT_DISTRIBUTED_COORDINATION_GRPC_COORDINATION_SERVICE_IMPL_H_\n+\n+#include <memory>\n+\n+#include \"absl/base/thread_annotations.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/synchronization/mutex.h\"\n+#include \"grpcpp/alarm.h\"\n+#include \"grpcpp/completion_queue.h\"\n+#include \"grpcpp/server_builder.h\"\n+#include \"xla/pjrt/distributed/coordination/coordination_service.h\"\n+#include \"xla/pjrt/distributed/coordination/coordination_service_agent.h\"\n+#include \"xla/pjrt/distributed/coordination/coordination_service_rpc_handler.h\"\n+#include \"xla/tsl/distributed_runtime/rpc/async_service_interface.h\"\n+#include \"xla/tsl/distributed_runtime/rpc/grpc_call.h\"\n+#include \"xla/tsl/distributed_runtime/rpc/grpc_util.h\"\n+#include \"xla/tsl/platform/threadpool.h\"\n+#include \"xla/tsl/protobuf/coordination_service.grpc.pb.h\"\n+#include \"xla/tsl/protobuf/coordination_service.pb.h\"\n+\n+namespace xla {\n+\n+class GrpcCoordinationServiceImpl : public tsl::AsyncServiceInterface {\n+ public:\n+  template <class RequestMessage, class ResponseMessage>\n+  using CoordCall =\n+      tsl::Call<GrpcCoordinationServiceImpl,\n+                tensorflow::grpc::CoordinationService::AsyncService,\n+                RequestMessage, ResponseMessage>;\n+\n+  GrpcCoordinationServiceImpl(tsl::thread::ThreadPool* compute_pool,\n+                              ::grpc::ServerBuilder* server_builder);\n+  ~GrpcCoordinationServiceImpl() override {}\n+\n+  void HandleRPCsLoop() override;\n+  void Shutdown() override;\n+  void SetCoordinationServiceAgentInstance(CoordinationServiceAgent* agent) {\n+    rpc_handler_.SetAgentInstance(agent);\n+  }\n+  void SetCoordinationServiceInstance(CoordinationService* service) {\n+    rpc_handler_.SetServiceInstance(service);\n+  }\n+  CoordinationServiceRpcHandler* GetRpcHandler() { return &rpc_handler_; }\n+\n+ private:\n+#define HANDLER(method)                                                       \\\n+  void method##Handler(CoordCall<tensorflow::method##Request,                 \\\n+                                 tensorflow::method##Response>* call) {       \\\n+    absl::ReaderMutexLock l(&shutdown_mu_);                                   \\\n+    if (shutdown_) {                                                          \\\n+      call->SendResponse(tsl::ToGrpcStatus(                                   \\\n+          absl::InternalError(\"Coordination service has been shut down.\")));  \\\n+      return;                                                                 \\\n+    }                                                                         \\\n+    compute_pool_.Schedule([this, call]() {                                   \\\n+      rpc_handler_.method##Async(&call->request, &call->response,             \\\n+                                 [call](const absl::Status& s) {              \\\n+                                   call->ClearCancelCallback();               \\\n+                                   call->SendResponse(tsl::ToGrpcStatus(s));  \\\n+                                 });                                          \\\n+    });                                                                       \\\n+    tsl::Call<GrpcCoordinationServiceImpl,                                    \\\n+              tensorflow::grpc::CoordinationService::AsyncService,            \\\n+              tensorflow::method##Request, tensorflow::method##Response>::    \\\n+        EnqueueRequest(&service_, cq_.get(),                                  \\\n+                       &tensorflow::grpc::CoordinationService::AsyncService:: \\\n+                           Request##method,                                   \\\n+                       &GrpcCoordinationServiceImpl::method##Handler,         \\\n+                       /*supports_cancel=*/false);                            \\\n+  }\n+  HANDLER(RegisterTask);\n+  HANDLER(WaitForAllTasks);\n+  HANDLER(ShutdownTask);\n+  HANDLER(ResetTask);\n+  HANDLER(Heartbeat);\n+  HANDLER(ReportErrorToTask);\n+  HANDLER(ReportErrorToService);\n+  HANDLER(GetTaskState);\n+  HANDLER(WatchJobState);\n+  HANDLER(InsertKeyValue);\n+  HANDLER(GetKeyValue);\n+  HANDLER(TryGetKeyValue);\n+  HANDLER(IncrementKeyValue);\n+  HANDLER(GetKeyValueDir);\n+  HANDLER(DeleteKeyValue);\n+  HANDLER(Barrier);\n+  HANDLER(CancelBarrier);\n+  HANDLER(GetAliveTasks);\n+  HANDLER(PollForError);\n+#undef HANDLER\n+\n+  tsl::thread::ThreadPool& compute_pool_;\n+  CoordinationServiceRpcHandler rpc_handler_;\n+\n+  absl::Mutex shutdown_mu_;\n+  bool shutdown_ ABSL_GUARDED_BY(shutdown_mu_);\n+  std::unique_ptr<::grpc::Alarm> shutdown_alarm_;\n+\n+  std::unique_ptr<::grpc::ServerCompletionQueue> cq_;\n+  tensorflow::grpc::CoordinationService::AsyncService service_;\n+\n+  GrpcCoordinationServiceImpl(const GrpcCoordinationServiceImpl&) = delete;\n+  void operator=(const GrpcCoordinationServiceImpl&) = delete;\n+};\n+\n+}  // namespace xla\n+\n+#endif  // XLA_PJRT_DISTRIBUTED_COORDINATION_GRPC_COORDINATION_SERVICE_IMPL_H_"
        },
        {
            "sha": "b42d75b987c441767661c0a1a9bb5920e3a9e770",
            "filename": "third_party/xla/xla/pjrt/distributed/coordination/key_value_store.cc",
            "status": "added",
            "additions": 146,
            "deletions": 0,
            "changes": 146,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fkey_value_store.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fkey_value_store.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fkey_value_store.cc?ref=3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a",
            "patch": "@@ -0,0 +1,146 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/pjrt/distributed/coordination/key_value_store.h\"\n+\n+#include <cstdint>\n+#include <optional>\n+#include <string>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/match.h\"\n+#include \"absl/strings/numbers.h\"\n+#include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/str_format.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/synchronization/mutex.h\"\n+\n+namespace xla {\n+\n+KeyValueStore::~KeyValueStore() {\n+  absl::MutexLock l(mu_);\n+\n+  absl::Status cancelled = absl::CancelledError(\"KeyValueStore destructed\");\n+  for (auto& [key, callbacks] : callbacks_) {\n+    for (Callback& callback : callbacks) {\n+      callback(cancelled);\n+    }\n+  }\n+}\n+\n+absl::Status KeyValueStore::Put(absl::string_view key, absl::string_view value,\n+                                bool allow_overwrite) {\n+  absl::MutexLock l(mu_);\n+\n+  if (allow_overwrite) {\n+    data_[key] = value;\n+    NotifyCallbacksForKey(key, value);\n+    return absl::OkStatus();\n+  }\n+\n+  auto [it, inserted] = data_.try_emplace(key, value);\n+  if (!inserted) {\n+    return absl::AlreadyExistsError(\n+        absl::StrCat(\"key \", key, \" already exists.\"));\n+  }\n+  NotifyCallbacksForKey(key, value);\n+  return absl::OkStatus();\n+}\n+\n+std::optional<std::string> KeyValueStore::Get(absl::string_view key) {\n+  absl::MutexLock l(mu_);\n+  auto it = data_.find(key);\n+  if (it == data_.end()) {\n+    return std::nullopt;\n+  }\n+  return it->second;\n+}\n+\n+absl::StatusOr<std::string> KeyValueStore::IncrementBy(absl::string_view key,\n+                                                       int64_t increment) {\n+  absl::MutexLock l(mu_);\n+  auto [it, inserted] = data_.try_emplace(key, \"0\");\n+  int val;\n+  if (!absl::SimpleAtoi(it->second, &val)) {\n+    return absl::FailedPreconditionError(absl::StrFormat(\n+        \"Failed to parse value \\\"%s\\\" as an integer.\", it->second));\n+  }\n+  it->second = absl::StrCat(val + increment);\n+  NotifyCallbacksForKey(key, it->second);\n+  return it->second;\n+}\n+\n+std::vector<tensorflow::KeyValueEntry> KeyValueStore::GetPrefix(\n+    absl::string_view prefix) {\n+  absl::MutexLock l(mu_);\n+\n+  std::vector<tensorflow::KeyValueEntry> entries;\n+  for (auto it = data_.lower_bound(prefix); it != data_.end(); ++it) {\n+    const auto& [key, value] = *it;\n+    if (!absl::StartsWith(key, prefix)) {\n+      break;\n+    }\n+    tensorflow::KeyValueEntry entry;\n+    entry.set_key(key);\n+    entry.set_value(value);\n+    entries.push_back(std::move(entry));\n+  }\n+  return entries;\n+}\n+\n+void KeyValueStore::Delete(absl::string_view key) {\n+  absl::MutexLock l(mu_);\n+  data_.erase(key);\n+}\n+\n+void KeyValueStore::DeletePrefix(absl::string_view prefix) {\n+  absl::MutexLock l(mu_);\n+\n+  auto begin = data_.lower_bound(prefix);\n+  auto it = begin;\n+  for (; it != data_.end(); ++it) {\n+    const auto& [key, value] = *it;\n+    if (!absl::StartsWith(key, prefix)) {\n+      break;\n+    }\n+  }\n+  data_.erase(begin, it);\n+}\n+\n+void KeyValueStore::AddCallbackForKey(absl::string_view key,\n+                                      Callback callback) {\n+  absl::MutexLock l(mu_);\n+\n+  if (auto it = data_.find(key); it != data_.end()) {\n+    callback(it->second);\n+    return;\n+  }\n+  callbacks_[key].push_back(std::move(callback));\n+}\n+\n+void KeyValueStore::NotifyCallbacksForKey(\n+    absl::string_view key, const absl::StatusOr<absl::string_view>& value) {\n+  if (auto it = callbacks_.find(key); it != callbacks_.end()) {\n+    for (Callback& callback : it->second) {\n+      callback(value);\n+    }\n+    callbacks_.erase(it);\n+  }\n+}\n+\n+}  // namespace xla"
        },
        {
            "sha": "7eb8949cc8e39191c767ed8dc0467778d44883da",
            "filename": "third_party/xla/xla/pjrt/distributed/coordination/key_value_store.h",
            "status": "added",
            "additions": 97,
            "deletions": 0,
            "changes": 97,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fkey_value_store.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fkey_value_store.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fkey_value_store.h?ref=3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a",
            "patch": "@@ -0,0 +1,97 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_PJRT_DISTRIBUTED_COORDINATION_KEY_VALUE_STORE_H_\n+#define XLA_PJRT_DISTRIBUTED_COORDINATION_KEY_VALUE_STORE_H_\n+\n+#include <cstdint>\n+#include <optional>\n+#include <string>\n+#include <vector>\n+\n+#include \"absl/base/thread_annotations.h\"\n+#include \"absl/container/btree_map.h\"\n+#include \"absl/container/flat_hash_map.h\"\n+#include \"absl/functional/any_invocable.h\"\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/synchronization/mutex.h\"\n+#include \"xla/tsl/protobuf/coordination_service.pb.h\"\n+\n+namespace xla {\n+\n+// A thread-safe in-memory key-value store.\n+class KeyValueStore {\n+ public:\n+  using Callback =\n+      absl::AnyInvocable<void(const absl::StatusOr<absl::string_view>&)>;\n+\n+  KeyValueStore() = default;\n+  ~KeyValueStore();\n+\n+  // KeyValueStore is not copyable or movable.\n+  KeyValueStore(const KeyValueStore&) = delete;\n+  KeyValueStore(KeyValueStore&&) = delete;\n+  KeyValueStore& operator=(const KeyValueStore&) = delete;\n+  KeyValueStore& operator=(KeyValueStore&&) = delete;\n+\n+  // Inserts a key-value pair. If allow_overwrite is false, then Put returns an\n+  // error if the provided key is already in the store.\n+  absl::Status Put(absl::string_view key, absl::string_view value,\n+                   bool allow_overwrite);\n+\n+  // Returns the value associated with the provided key, if one exists.\n+  std::optional<std::string> Get(absl::string_view key);\n+\n+  // Increments the value associated with the provided key by the provided\n+  // increment. If the key does not exist, the value is initialized to 0. And\n+  // then incremented.\n+  //\n+  // The value string is interpreted as a big-endian 64-bit integer. An error is\n+  // returned if the value string is not exactly 8 bytes.\n+  absl::StatusOr<std::string> IncrementBy(absl::string_view key,\n+                                          int64_t increment);\n+\n+  // Returns all key-value pairs where the key has the provided prefix.\n+  //\n+  // The empty string \"\" is a prefix of every key, so GetPrefix(\"\") can be used\n+  // to retrieve every element in the store.\n+  std::vector<tensorflow::KeyValueEntry> GetPrefix(absl::string_view prefix);\n+\n+  // Adds a callback that is called when the provided key exists in the map.\n+  void AddCallbackForKey(absl::string_view key, Callback callback);\n+\n+  // Deletes the provided key.\n+  void Delete(absl::string_view key);\n+\n+  // Deletes all key-value pairs where the key has the provided prefix.\n+  void DeletePrefix(absl::string_view prefix);\n+\n+ private:\n+  // Notifies all callbacks registered for the provided key.\n+  void NotifyCallbacksForKey(absl::string_view key,\n+                             const absl::StatusOr<absl::string_view>& value)\n+      ABSL_EXCLUSIVE_LOCKS_REQUIRED(mu_);\n+\n+  absl::Mutex mu_;\n+  absl::btree_map<std::string, std::string> data_ ABSL_GUARDED_BY(mu_);\n+  absl::flat_hash_map<std::string, std::vector<Callback>> callbacks_\n+      ABSL_GUARDED_BY(mu_);\n+};\n+\n+}  // namespace xla\n+\n+#endif  // XLA_PJRT_DISTRIBUTED_COORDINATION_KEY_VALUE_STORE_H_"
        },
        {
            "sha": "1425fc3a827dd46bf73f1e0ce12ef878f0c43578",
            "filename": "third_party/xla/xla/pjrt/distributed/coordination/key_value_store_test.cc",
            "status": "added",
            "additions": 232,
            "deletions": 0,
            "changes": 232,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fkey_value_store_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fkey_value_store_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Fkey_value_store_test.cc?ref=3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a",
            "patch": "@@ -0,0 +1,232 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/pjrt/distributed/coordination/key_value_store.h\"\n+\n+#include <optional>\n+#include <string>\n+#include <utility>\n+#include <vector>\n+\n+#include \"absl/status/status.h\"\n+#include \"absl/status/statusor.h\"\n+#include \"absl/strings/string_view.h\"\n+#include \"absl/types/span.h\"\n+#include \"xla/tsl/platform/test.h\"\n+\n+namespace xla {\n+namespace {\n+\n+using ::testing::ElementsAre;\n+using ::testing::Eq;\n+using ::testing::IsEmpty;\n+using ::testing::Optional;\n+using ::testing::Pair;\n+using ::testing::status::IsOkAndHolds;\n+using ::testing::status::StatusIs;\n+\n+// Converts a list of KeyValueEntries into a list of pairs.\n+std::vector<std::pair<std::string, std::string>> AsPairs(\n+    absl::Span<const tensorflow::KeyValueEntry> entries) {\n+  std::vector<std::pair<std::string, std::string>> pairs;\n+  for (const tensorflow::KeyValueEntry& entry : entries) {\n+    pairs.push_back({entry.key(), entry.value()});\n+  }\n+  return pairs;\n+}\n+\n+TEST(KeyValueStore, OverwritingInsertSucceeds) {\n+  KeyValueStore store;\n+  ASSERT_OK(store.Put(\"foo\", \"bar\", /*allow_overwrite=*/true));\n+  EXPECT_THAT(store.Get(\"foo\"), Optional(Eq(\"bar\")));\n+  ASSERT_OK(store.Put(\"foo\", \"moo\", /*allow_overwrite=*/true));\n+  EXPECT_THAT(store.Get(\"foo\"), Optional(Eq(\"moo\")));\n+}\n+\n+TEST(KeyValueStore, OverwritingInsertFails) {\n+  KeyValueStore store;\n+  ASSERT_OK(store.Put(\"foo\", \"bar\", /*allow_overwrite=*/false));\n+  EXPECT_THAT(store.Put(\"foo\", \"bar\", /*allow_overwrite=*/false),\n+              StatusIs(absl::StatusCode::kAlreadyExists));\n+}\n+\n+TEST(KeyValueStore, GetExistingKey) {\n+  KeyValueStore store;\n+  ASSERT_OK(store.Put(\"foo\", \"bar\", /*allow_overwrite=*/false));\n+  EXPECT_THAT(store.Get(\"foo\"), Optional(Eq(\"bar\")));\n+}\n+\n+TEST(KeyValueStore, GetMissingKey) {\n+  KeyValueStore store;\n+  EXPECT_EQ(store.Get(\"foo\"), std::nullopt);\n+}\n+\n+TEST(KeyValueStore, GetPrefixNoMatch) {\n+  KeyValueStore store;\n+  ASSERT_OK(store.Put(\"a\", \"\", /*allow_overwrite=*/true));\n+  ASSERT_OK(store.Put(\"c\", \"\", /*allow_overwrite=*/true));\n+  EXPECT_THAT(AsPairs(store.GetPrefix(\"b\")), IsEmpty());\n+}\n+\n+TEST(KeyValueStore, GetPrefixExactMatch) {\n+  KeyValueStore store;\n+  ASSERT_OK(store.Put(\"a\", \"\", /*allow_overwrite=*/true));\n+  ASSERT_OK(store.Put(\"b\", \"\", /*allow_overwrite=*/true));\n+  ASSERT_OK(store.Put(\"bb\", \"\", /*allow_overwrite=*/true));\n+  ASSERT_OK(store.Put(\"bbb\", \"\", /*allow_overwrite=*/true));\n+  ASSERT_OK(store.Put(\"c\", \"\", /*allow_overwrite=*/true));\n+  EXPECT_THAT(AsPairs(store.GetPrefix(\"b\")),\n+              ElementsAre(Pair(\"b\", \"\"), Pair(\"bb\", \"\"), Pair(\"bbb\", \"\")));\n+}\n+\n+TEST(KeyValueStore, GetPrefixNoExactMatch) {\n+  KeyValueStore store;\n+  ASSERT_OK(store.Put(\"a\", \"\", /*allow_overwrite=*/true));\n+  ASSERT_OK(store.Put(\"bb\", \"\", /*allow_overwrite=*/true));\n+  ASSERT_OK(store.Put(\"bbb\", \"\", /*allow_overwrite=*/true));\n+  ASSERT_OK(store.Put(\"c\", \"\", /*allow_overwrite=*/true));\n+  EXPECT_THAT(AsPairs(store.GetPrefix(\"b\")),\n+              ElementsAre(Pair(\"bb\", \"\"), Pair(\"bbb\", \"\")));\n+}\n+\n+TEST(KeyValueStore, GetPrefixWithDirectoryFormat) {\n+  KeyValueStore store;\n+  ASSERT_OK(store.Put(\"a/1\", \"\", /*allow_overwrite=*/true));\n+  ASSERT_OK(store.Put(\"b/1\", \"\", /*allow_overwrite=*/true));\n+  ASSERT_OK(store.Put(\"b/2\", \"\", /*allow_overwrite=*/true));\n+  ASSERT_OK(store.Put(\"b/3/4\", \"\", /*allow_overwrite=*/true));\n+  ASSERT_OK(store.Put(\"b/3/5\", \"\", /*allow_overwrite=*/true));\n+  ASSERT_OK(store.Put(\"c/1\", \"\", /*allow_overwrite=*/true));\n+  EXPECT_THAT(AsPairs(store.GetPrefix(\"b/\")),\n+              ElementsAre(Pair(\"b/1\", \"\"), Pair(\"b/2\", \"\"), Pair(\"b/3/4\", \"\"),\n+                          Pair(\"b/3/5\", \"\")));\n+}\n+\n+TEST(KeyValueStore, AddCallbackAndThenKey) {\n+  KeyValueStore store;\n+  bool callback_called = false;\n+  store.AddCallbackForKey(\"foo\",\n+                          [&](const absl::StatusOr<absl::string_view>& s) {\n+                            ASSERT_OK(s);\n+                            ASSERT_THAT(s, IsOkAndHolds(\"bar\"));\n+                            callback_called = true;\n+                          });\n+  ASSERT_FALSE(callback_called);\n+  ASSERT_OK(store.Put(\"foo\", \"bar\", /*allow_overwrite=*/true));\n+  EXPECT_TRUE(callback_called);\n+}\n+\n+TEST(KeyValueStore, AddKeyThenCallback) {\n+  KeyValueStore store;\n+  ASSERT_OK(store.Put(\"foo\", \"bar\", /*allow_overwrite=*/true));\n+  bool callback_called = false;\n+  store.AddCallbackForKey(\"foo\",\n+                          [&](const absl::StatusOr<absl::string_view>& s) {\n+                            ASSERT_OK(s);\n+                            ASSERT_THAT(s, IsOkAndHolds(\"bar\"));\n+                            callback_called = true;\n+                          });\n+  EXPECT_TRUE(callback_called);\n+}\n+\n+TEST(KeyValueStore, DeleteKey) {\n+  KeyValueStore store;\n+  ASSERT_OK(store.Put(\"a\", \"\", /*allow_overwrite=*/true));\n+  store.Delete(\"a\");\n+  EXPECT_EQ(store.Get(\"a\"), std::nullopt);\n+}\n+\n+TEST(KeyValueStore, DeleteMissingKey) {\n+  KeyValueStore store;\n+  ASSERT_OK(store.Put(\"a\", \"\", /*allow_overwrite=*/true));\n+  store.Delete(\"b\");\n+}\n+\n+TEST(KeyValueStore, DeletePrefixNoMatch) {\n+  KeyValueStore store;\n+  ASSERT_OK(store.Put(\"a\", \"\", /*allow_overwrite=*/true));\n+  ASSERT_OK(store.Put(\"c\", \"\", /*allow_overwrite=*/true));\n+  store.DeletePrefix(\"b\");\n+  EXPECT_THAT(AsPairs(store.GetPrefix(\"\")),\n+              ElementsAre(Pair(\"a\", \"\"), Pair(\"c\", \"\")));\n+}\n+\n+TEST(KeyValueStore, DeletePrefixExactMatch) {\n+  KeyValueStore store;\n+  ASSERT_OK(store.Put(\"a\", \"\", /*allow_overwrite=*/true));\n+  ASSERT_OK(store.Put(\"b\", \"\", /*allow_overwrite=*/true));\n+  ASSERT_OK(store.Put(\"bb\", \"\", /*allow_overwrite=*/true));\n+  ASSERT_OK(store.Put(\"bbb\", \"\", /*allow_overwrite=*/true));\n+  ASSERT_OK(store.Put(\"c\", \"\", /*allow_overwrite=*/true));\n+  store.DeletePrefix(\"b\");\n+  EXPECT_THAT(AsPairs(store.GetPrefix(\"\")),\n+              ElementsAre(Pair(\"a\", \"\"), Pair(\"c\", \"\")));\n+}\n+\n+TEST(KeyValueStore, DeletePrefixNoExactMatch) {\n+  KeyValueStore store;\n+  ASSERT_OK(store.Put(\"a\", \"\", /*allow_overwrite=*/true));\n+  ASSERT_OK(store.Put(\"bb\", \"\", /*allow_overwrite=*/true));\n+  ASSERT_OK(store.Put(\"bbb\", \"\", /*allow_overwrite=*/true));\n+  ASSERT_OK(store.Put(\"c\", \"\", /*allow_overwrite=*/true));\n+  store.DeletePrefix(\"b\");\n+  EXPECT_THAT(AsPairs(store.GetPrefix(\"\")),\n+              ElementsAre(Pair(\"a\", \"\"), Pair(\"c\", \"\")));\n+}\n+\n+TEST(KeyValueStore, CallbacksCalledOnDestruction) {\n+  bool callback_called = false;\n+  {\n+    KeyValueStore store;\n+    store.AddCallbackForKey(\n+        \"foo\", [&](const absl::StatusOr<absl::string_view>& s) {\n+          ASSERT_THAT(s, StatusIs(absl::StatusCode::kCancelled));\n+          callback_called = true;\n+        });\n+  }\n+  EXPECT_TRUE(callback_called);\n+}\n+\n+TEST(KeyValueStore, IncrementByPositiveValueSucceeds) {\n+  KeyValueStore store;\n+  ASSERT_OK(store.Put(\"foo\", \"3\",\n+                      /*allow_overwrite=*/true));\n+  EXPECT_THAT(store.IncrementBy(\"foo\", 2), IsOkAndHolds(\"5\"));\n+}\n+\n+TEST(KeyValueStore, IncrementByNegativeValueSucceeds) {\n+  KeyValueStore store;\n+  ASSERT_OK(store.Put(\"foo\", \"3\",\n+                      /*allow_overwrite=*/true));\n+  // Result will be the two's complement of the positive value.\n+  EXPECT_THAT(store.IncrementBy(\"foo\", -5), IsOkAndHolds(\"-2\"));\n+}\n+\n+TEST(KeyValueStore, IncrementByWithoutExistingValueSucceeds) {\n+  // Result will be the increment value. This behavior is consistent with\n+  // other KV store implementations.\n+  KeyValueStore store;\n+  EXPECT_THAT(store.IncrementBy(\"foo\", 2), IsOkAndHolds(\"2\"));\n+}\n+\n+TEST(KeyValueStore, IncrementByWithInvalidValueFails) {\n+  KeyValueStore store;\n+  ASSERT_OK(store.Put(\"foo\", \"invalid\", /*allow_overwrite=*/true));\n+  EXPECT_THAT(store.IncrementBy(\"foo\", 2),\n+              StatusIs(absl::StatusCode::kFailedPrecondition));\n+}\n+\n+}  // namespace\n+}  // namespace xla"
        },
        {
            "sha": "b0312391f252d17b89ae56d4c37a7bda0edaaeba",
            "filename": "third_party/xla/xla/pjrt/distributed/coordination/test_device.proto",
            "status": "added",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Ftest_device.proto",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Ftest_device.proto",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fdistributed%2Fcoordination%2Ftest_device.proto?ref=3c206f2a2a29bf1203c6856a16ff7b059b5b0e6a",
            "patch": "@@ -0,0 +1,13 @@\n+syntax = \"proto3\";\n+\n+package xla;\n+\n+message TestDevice {\n+  string name = 1;\n+  int64 local_id = 2;\n+  int64 global_id = 3;\n+}\n+\n+message TestDeviceList {\n+  repeated TestDevice device = 1;\n+}"
        }
    ],
    "stats": {
        "total": 12027,
        "additions": 12027,
        "deletions": 0
    }
}