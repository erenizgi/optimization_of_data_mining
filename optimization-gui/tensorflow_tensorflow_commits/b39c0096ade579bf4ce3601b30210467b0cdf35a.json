{
    "author": "jcai19",
    "message": "[XLA][Numerics][HLO Value Tracking] Propagte original values in HLO parameters through the StableHLO round trip\n\nThis saves HLO original values of parameters in MLIR function argument attributes.\n\nPiperOrigin-RevId: 814947158",
    "sha": "b39c0096ade579bf4ce3601b30210467b0cdf35a",
    "files": [
        {
            "sha": "33c63a107fea2fdd78dd1107f5e086e1e4621e7a",
            "filename": "third_party/xla/xla/hlo/translate/hlo_to_mhlo/hlo_function_importer.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b39c0096ade579bf4ce3601b30210467b0cdf35a/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fhlo_function_importer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b39c0096ade579bf4ce3601b30210467b0cdf35a/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fhlo_function_importer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Fhlo_function_importer.cc?ref=b39c0096ade579bf4ce3601b30210467b0cdf35a",
            "patch": "@@ -567,9 +567,18 @@ absl::StatusOr<Value> HloFunctionImporter::ImportInstructionsImpl(\n   // Setup the input parameters.\n   const int num_parameters = computation.num_parameters();\n \n+  FuncOp func = llvm::dyn_cast<FuncOp>(builder->getBlock()->getParentOp());\n   for (int i = 0; i < num_parameters; i++) {\n     auto* hlo_parameter = computation.parameter_instruction(i);\n     instruction_value_map_[hlo_parameter] = arguments[i];\n+    // Only add original value attributes to parameters in functions. Skip\n+    // regions.\n+    if (hlo_parameter->original_value() && func) {\n+      func.setArgAttr(\n+          i, kMhloOriginalValueAttr,\n+          builder_->getStringAttr(\n+              \"{\" + hlo_parameter->original_value()->ToString() + \"}\"));\n+    }\n   }\n \n   for (auto instruction : computation.MakeInstructionPostOrder()) {"
        },
        {
            "sha": "6602b160319b4e156c3e3e8c0423afea5704f6dd",
            "filename": "third_party/xla/xla/hlo/translate/hlo_to_mhlo/tests/import.hlo",
            "status": "modified",
            "additions": 19,
            "deletions": 5,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b39c0096ade579bf4ce3601b30210467b0cdf35a/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Ftests%2Fimport.hlo",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b39c0096ade579bf4ce3601b30210467b0cdf35a/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Ftests%2Fimport.hlo",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fhlo_to_mhlo%2Ftests%2Fimport.hlo?ref=b39c0096ade579bf4ce3601b30210467b0cdf35a",
            "patch": "@@ -2130,9 +2130,23 @@ add {\n // FLATTEN-CHECK-SAME:  ([[ARG:%.*]]: tensor<4x4xf32>) -> (tensor<4x2xf32>, tensor<4x2xi32>)\n \n // Test HLO original value\n-// CHECK-LABEL:  func private @test_original_value\n-%test_original_value (Arg_0: f32[192]) -> f32[1,17,17,192] {\n-  %Arg_0 = f32[192]{0} parameter(0)\n-  // CHECK: \"mhlo.broadcast_in_dim\"(%arg0) <{broadcast_dimensions = dense<3> : tensor<1xi64>}> {mhlo.original_value = \"{{[{][{]}}\\22broadcast.2342\\22{{[}][}]}}\"} : (tensor<192xf32>) -> tensor<1x17x17x192xf32>\n-  ROOT %broadcast.2342 = f32[1,17,17,192]{3,2,1,0} broadcast(f32[192]{0} %Arg_0), dimensions={3}, origin={{\"broadcast.2342\"}}\n+// CHECK-LABEL:  func private @add_original_value\n+// CHECK-SAME:  {mhlo.original_value = \"{{[{][{]}}\\22a\\22{{[}][}]}}\"}\n+add_original_value {\n+  lhs = f32[] parameter(0), origin={{\"a\"}}\n+  rhs = f32[] parameter(1)\n+  ROOT add = f32[] add(lhs, rhs)\n+}\n+\n+// CHECK-LABEL:  func private @test_orignal_value\n+// CHECK-SAME:  ([[INPUT:%.*]]: tensor<8xf32>\n+// CHECK-SAME:  {mhlo.original_value = \"{{[{][{]}}\\22b\\22{{[}][}]}}\"})\n+%test_orignal_value {\n+  input = f32[8] parameter(0), origin={{\"b\"}}\n+  // CHECK-NEXT:  \"mhlo.all_reduce\"([[INPUT]]) <{\n+  // CHECK:  ^bb0([[ARG0:%.*]]: tensor<f32>, [[ARG1:%.*]]: tensor<f32>):\n+  // CHECK:    [[ADD:%.*]] = mhlo.add [[ARG0]], [[ARG1]]\n+  // CHECK:    mhlo.return [[ADD]] : tensor<f32>\n+  // CHECK:  }) {mhlo.original_value = \"{{[{][{]}}\\22c\\22{{[}][}]}}\"}\n+  ROOT result = f32[8] all-reduce(input), channel_id=1, replica_groups={{0,1,2,3}, {4,5,6,7}}, to_apply=add_original_value, origin={{\"c\"}}\n }"
        },
        {
            "sha": "017f199a396e81e4074174ed121692f0e73c246d",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b39c0096ade579bf4ce3601b30210467b0cdf35a/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b39c0096ade579bf4ce3601b30210467b0cdf35a/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2FBUILD?ref=b39c0096ade579bf4ce3601b30210467b0cdf35a",
            "patch": "@@ -291,6 +291,7 @@ cc_library(\n     srcs = [\"translate.cc\"],\n     hdrs = [\"translate.h\"],\n     deps = [\n+        \":attribute_exporter\",\n         \":mlir_hlo_to_hlo\",\n         \":type_to_shape\",\n         \"//xla:debug_options_flags\","
        },
        {
            "sha": "d7d1a4386af636cf9a55ce648e7c50ecfc3c9a4d",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/mlir_hlo_to_hlo.cc",
            "status": "modified",
            "additions": 54,
            "deletions": 13,
            "changes": 67,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b39c0096ade579bf4ce3601b30210467b0cdf35a/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fmlir_hlo_to_hlo.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b39c0096ade579bf4ce3601b30210467b0cdf35a/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fmlir_hlo_to_hlo.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fmlir_hlo_to_hlo.cc?ref=b39c0096ade579bf4ce3601b30210467b0cdf35a",
            "patch": "@@ -916,6 +916,20 @@ static void ExtractFrontendAttributesFromFunction(\n     }\n }\n \n+static void ExtractOriginalValuesFromFunction(\n+    mlir::func::FuncOp function,\n+    llvm::SmallVectorImpl<std::optional<xla::OriginalValueProto>>*\n+        original_value_protos) {\n+  original_value_protos->resize(function.getNumArguments(), std::nullopt);\n+  for (int i = 0, end = function.getNumArguments(); i < end; ++i) {\n+    if (auto original_value_attr = function.getArgAttrOfType<mlir::StringAttr>(\n+            i, xla::kMhloOriginalValueAttr)) {\n+      (*original_value_protos)[i] =\n+          xla::ConvertOriginalValue(original_value_attr.getValue());\n+    }\n+  }\n+}\n+\n static bool SomeOptionalShardingsAreSet(\n     llvm::ArrayRef<std::optional<xla::OpSharding>> shardings) {\n   return llvm::any_of(shardings,\n@@ -1114,8 +1128,10 @@ class ConvertToHloModule {\n       bool ensure_single_arg,\n       const std::vector<bool>& entry_args_same_across_replicas,\n       llvm::ArrayRef<std::optional<xla::OpSharding>> arg_shardings,\n+      llvm::ArrayRef<std::optional<xla::FrontendAttributes>> arg_fe_attrs,\n+      llvm::ArrayRef<std::optional<xla::OriginalValueProto>>\n+          arg_original_value_protos,\n       llvm::ArrayRef<std::optional<xla::OpSharding>> ret_shardings,\n-      llvm::ArrayRef<std::optional<xla::FrontendAttributes>> fe_attrs,\n       xla::XlaComputationId& computation,\n       llvm::ArrayRef<mlir::Value> implicit_operands = {},\n       llvm::ArrayRef<mlir::Value> implicit_results = {});\n@@ -5493,8 +5509,9 @@ LogicalResult ConvertToHloModule::LowerStablehloCompositeCall(\n           /*is_entry_function=*/false,\n           /*ensure_single_arg=*/false,\n           /*entry_args_same_across_replicas=*/{},\n-          /*arg_shardings=*/{}, /*ret_shardings=*/{},\n-          /*fe_attrs=*/{}, /*computation=*/computation,\n+          /*arg_shardings=*/{}, /*arg_fe_attrs=*/{},\n+          /*arg_original_value_protos=*/{}, /*ret_shardings=*/{},\n+          /*computation=*/computation,\n           /*implicit_operands=*/{}))) {\n     return failure();\n   }\n@@ -5552,8 +5569,9 @@ LogicalResult ConvertToHloModule::LowerCompositeCall(\n           /*is_entry_function=*/false,\n           /*ensure_single_arg=*/false,\n           /*entry_args_same_across_replicas=*/{},\n-          /*arg_shardings=*/{}, /*ret_shardings=*/{},\n-          /*fe_attrs=*/{}, /*computation=*/computation,\n+          /*arg_shardings=*/{}, /*arg_fe_attrs=*/{},\n+          /*arg_original_value_protos=*/{}, /*ret_shardings=*/{},\n+          /*computation=*/computation,\n           /*implicit_operands=*/{}))) {\n     return failure();\n   }\n@@ -5908,6 +5926,8 @@ LogicalResult ConvertToHloModule::RunOnFunction(mlir::func::FuncOp f) {\n   llvm::SmallVector<std::optional<xla::OpSharding>, 4> arg_shardings;\n   llvm::SmallVector<std::optional<xla::OpSharding>, 4> ret_shardings;\n   llvm::SmallVector<std::optional<xla::FrontendAttributes>, 4> arg_fe_attrs;\n+  llvm::SmallVector<std::optional<xla::OriginalValueProto>, 4>\n+      arg_original_value_protos;\n   if (entry_function) {\n     bool any_arg_replicated = false;\n     entry_args_same_across_replicas.reserve(f.getNumArguments());\n@@ -5954,13 +5974,14 @@ LogicalResult ConvertToHloModule::RunOnFunction(mlir::func::FuncOp f) {\n     if (!any_arg_replicated) entry_args_same_across_replicas.clear();\n   }\n   ExtractFrontendAttributesFromFunction(f, &arg_fe_attrs);\n+  ExtractOriginalValuesFromFunction(f, &arg_original_value_protos);\n   ExtractShardingsFromFunction(f, &arg_shardings, &ret_shardings,\n                                entry_function);\n   xla::XlaComputationId computation;\n   if (failed(LowerBasicBlockAsFunction(\n           &f.front(), builder.get(), entry_function, false,\n-          entry_args_same_across_replicas, arg_shardings, ret_shardings,\n-          arg_fe_attrs, computation))) {\n+          entry_args_same_across_replicas, arg_shardings, arg_fe_attrs,\n+          arg_original_value_protos, ret_shardings, computation))) {\n     return failure();\n   }\n   if (auto execution_thread =\n@@ -6102,12 +6123,14 @@ LogicalResult ConvertToHloModule::LowerBasicBlockAsFunction(\n     bool ensure_single_arg,\n     const std::vector<bool>& entry_args_same_across_replicas,\n     llvm::ArrayRef<std::optional<xla::OpSharding>> arg_shardings,\n+    llvm::ArrayRef<std::optional<xla::FrontendAttributes>> arg_fe_attrs,\n+    llvm::ArrayRef<std::optional<xla::OriginalValueProto>>\n+        arg_original_value_protos,\n     llvm::ArrayRef<std::optional<xla::OpSharding>> ret_shardings,\n-    llvm::ArrayRef<std::optional<xla::FrontendAttributes>> fe_attrs,\n     xla::XlaComputationId& computation,\n     llvm::ArrayRef<mlir::Value> implicit_operands,\n     llvm::ArrayRef<mlir::Value> implicit_results) {\n-  // Mapping from the Value to lowered XlaOp.\n+  //  Mapping from the Value to lowered XlaOp.\n   ValueLoweringMap lowering;\n \n   // If using tuples as input, then there is only one input parameter that is a\n@@ -6137,6 +6160,10 @@ LogicalResult ConvertToHloModule::LowerBasicBlockAsFunction(\n       xla::XlaScopedShardingAssignment scoped_sharding(\n           builder, arg_shardings.empty() ? std::nullopt\n                                          : arg_shardings[arg.getArgNumber()]);\n+      xla::XlaScopedOriginalValueAssignment original_value(\n+          builder, arg_original_value_protos.empty()\n+                       ? std::nullopt\n+                       : arg_original_value_protos[arg.getArgNumber()]);\n       lowering[arg] = xla::GetTupleElement(tuple, arg.getArgNumber());\n     }\n   } else {\n@@ -6172,6 +6199,10 @@ LogicalResult ConvertToHloModule::LowerBasicBlockAsFunction(\n           xla::XlaScopedShardingAssignment scoped_sharding(\n               builder,\n               arg_shardings.empty() ? std::nullopt : arg_shardings[num]);\n+          xla::XlaScopedOriginalValueAssignment original_value(\n+              builder, arg_original_value_protos.empty()\n+                           ? std::nullopt\n+                           : arg_original_value_protos[num]);\n           lowering[arg] = xla::GetTupleElement(tuple, num);\n         }\n         for (auto [implicit_index, implicit_operand] :\n@@ -6188,6 +6219,10 @@ LogicalResult ConvertToHloModule::LowerBasicBlockAsFunction(\n         xla::XlaScopedShardingAssignment scoped_sharding(\n             builder,\n             arg_shardings.empty() ? std::nullopt : arg_shardings.front());\n+        xla::XlaScopedOriginalValueAssignment original_value(\n+            builder, arg_original_value_protos.empty()\n+                         ? std::nullopt\n+                         : arg_original_value_protos.front());\n         mlir::Value arg = implicit_operands.empty() ? block->getArgument(0)\n                                                     : implicit_operands.front();\n         xla::XlaScopedOpMetadataAssignment op_metadata(\n@@ -6214,10 +6249,14 @@ LogicalResult ConvertToHloModule::LowerBasicBlockAsFunction(\n         xla::Shape shape = xla::TypeToShape(arg.getType());\n         xla::XlaScopedShardingAssignment scoped_sharding(\n             builder, arg_shardings.empty() ? std::nullopt : arg_shardings[num]);\n-        if (!fe_attrs.empty() && fe_attrs[num]) {\n+        xla::XlaScopedOriginalValueAssignment original_value(\n+            builder, arg_original_value_protos.empty()\n+                         ? std::nullopt\n+                         : arg_original_value_protos[num]);\n+        if (!arg_fe_attrs.empty() && arg_fe_attrs[num]) {\n           // Populates frontend attributes for parameters only for the entry\n           // functions with no tuple args.\n-          builder->SetFrontendAttributes(*fe_attrs[num]);\n+          builder->SetFrontendAttributes(*arg_fe_attrs[num]);\n         }\n         // Save the location information as a name. For example JAX will set the\n         // name of the function argument of these. Want to preserve these for\n@@ -6277,8 +6316,10 @@ LogicalResult ConvertToHloModule::LowerRegionAsComputation(\n           &region->front(), builder.get(),\n           /*is_entry_function=*/false,\n           /*ensure_single_arg*/ ensure_single_arg,\n-          /*entry_args_same_across_replicas=*/{}, arg_shardings, ret_shardings,\n-          /*fe_attrs=*/{}, func, implicit_operands, implicit_results))) {\n+          /*entry_args_same_across_replicas=*/{}, arg_shardings,\n+          /*arg_fe_attrs=*/{},\n+          /*arg_original_value_protos=*/{}, ret_shardings, func,\n+          implicit_operands, implicit_results))) {\n     return failure();\n   }\n   return success();"
        },
        {
            "sha": "dfde609083ed7d6f0d050340e3e90f966b18682b",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/tests/export.mlir",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b39c0096ade579bf4ce3601b30210467b0cdf35a/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fexport.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b39c0096ade579bf4ce3601b30210467b0cdf35a/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fexport.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fexport.mlir?ref=b39c0096ade579bf4ce3601b30210467b0cdf35a",
            "patch": "@@ -3172,10 +3172,10 @@ func.func @main(%input0: tensor<16x16xf32>, %input1: tensor<16x16xi32>) {\n // -----\n // CHECK: HloModule\n // CHECK: ENTRY\n-// CHECK: %[[ARG0:.*]] = f32[192] parameter(0)\n+// CHECK: %[[ARG0:.*]] = f32[192] parameter(0), origin={{[{][{]}}\"a\"{{[}][}]}}\n // CHECK: ROOT %[[RESULT:.*]] = f32[1,17,17,192] broadcast(%[[ARG0]]), dimensions={3}, origin={{[{][{]}}\"broadcast.2342\"{{[}][}]}}\n \n-func.func @main(%arg0: tensor<192xf32>) -> tensor<1x17x17x192xf32> {\n+func.func @main(%arg0: tensor<192xf32> {mhlo.original_value = \"{{\\22a\\22}}\"}) -> tensor<1x17x17x192xf32> {\n   %0 = \"mhlo.broadcast_in_dim\"(%arg0) <{broadcast_dimensions = dense<3> : tensor<1xi64>}> {mhlo.original_value = \"{{\\22broadcast.2342\\22}}\"} : (tensor<192xf32>) -> tensor<1x17x17x192xf32>\n   return %0 : tensor<1x17x17x192xf32>\n }"
        },
        {
            "sha": "2f0c301340e89971956c58a41f23cc42725ba349",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/translate.cc",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b39c0096ade579bf4ce3601b30210467b0cdf35a/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftranslate.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b39c0096ade579bf4ce3601b30210467b0cdf35a/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftranslate.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftranslate.cc?ref=b39c0096ade579bf4ce3601b30210467b0cdf35a",
            "patch": "@@ -42,6 +42,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_input_output_alias_config.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n+#include \"xla/hlo/translate/mhlo_to_hlo/attribute_exporter.h\"\n #include \"xla/hlo/translate/mhlo_to_hlo/mlir_hlo_to_hlo.h\"\n #include \"xla/hlo/translate/mhlo_to_hlo/type_to_shape.h\"\n #include \"xla/hlo/translate/register.h\"\n@@ -139,6 +140,18 @@ absl::Status ConvertMlirHloToHloViaBuilder(\n       }\n     }\n   }\n+\n+  for (int i = 0; i < main.getNumArguments(); ++i) {\n+    if (auto original_value_attr = main.getArgAttrOfType<mlir::StringAttr>(\n+            i, xla::kMhloOriginalValueAttr)) {\n+      *computation.mutable_proto()\n+           ->mutable_computations(0)\n+           ->mutable_instructions(i)\n+           ->mutable_original_value() =\n+          *ConvertOriginalValue(original_value_attr);\n+    }\n+  }\n+\n   auto hlo_module = computation.proto();\n   mlir::StringRef module_name = module.getName() ? *module.getName() : \"main\";\n   hlo_module.set_name(module_name.str());"
        },
        {
            "sha": "c2109b31f0d8117de0d631eed8ef01cefd0acb10",
            "filename": "third_party/xla/xla/hlo/translate/tests/stablehlo.mlir",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/b39c0096ade579bf4ce3601b30210467b0cdf35a/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Ftests%2Fstablehlo.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/b39c0096ade579bf4ce3601b30210467b0cdf35a/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Ftests%2Fstablehlo.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Ftests%2Fstablehlo.mlir?ref=b39c0096ade579bf4ce3601b30210467b0cdf35a",
            "patch": "@@ -1953,11 +1953,11 @@ module {\n // CHECK-LABEL: HloModule main\n \n // CHECK: ENTRY\n-// CHECK: %[[ARG0:.*]] = f32[192] parameter(0)\n+// CHECK: %[[ARG0:.*]] = f32[192] parameter(0), origin={{[{][{]}}\"a\"{{[}][}]}}\n // CHECK: ROOT %[[RESULT:.*]] = f32[1,17,17,192] broadcast(%[[ARG0]]), dimensions={3}, origin={{[{][{]}}\"broadcast.2342\"{{[}][}]}}\n \n module {\n-  func.func @main(%arg0: tensor<192xf32>) -> tensor<1x17x17x192xf32> {\n+  func.func @main(%arg0: tensor<192xf32> {mhlo.original_value = \"{{\\22a\\22}}\"}) -> tensor<1x17x17x192xf32> {\n     %0 = stablehlo.broadcast_in_dim %arg0, dims = [3] {mhlo.original_value = \"{{\\22broadcast.2342\\22}}\"} : (tensor<192xf32>) -> tensor<1x17x17x192xf32>\n     return %0 : tensor<1x17x17x192xf32>\n   }"
        }
    ],
    "stats": {
        "total": 122,
        "additions": 100,
        "deletions": 22
    }
}