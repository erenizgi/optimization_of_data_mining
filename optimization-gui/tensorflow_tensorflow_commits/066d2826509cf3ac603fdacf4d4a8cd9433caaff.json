{
    "author": "chsigg",
    "message": "Refactor ConcatenateTest to remove boolean parameter.\n\nThe `ConcatenateTest` now uses `TritonSupportTestWithTypeAndDeviceParam`, removing the boolean parameter for nested GEMM fusions. The test now always assumes nested GEMM fusions are used. The `TopKTest` class definition is updated to reflect the removal of the aliased type.\n\nPiperOrigin-RevId: 840074814",
    "sha": "066d2826509cf3ac603fdacf4d4a8cd9433caaff",
    "files": [
        {
            "sha": "c4c3d2db82353bd9fdcc52cbcef7f62ce14e5834",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/support_test.cc",
            "status": "modified",
            "additions": 30,
            "deletions": 63,
            "changes": 93,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/066d2826509cf3ac603fdacf4d4a8cd9433caaff/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/066d2826509cf3ac603fdacf4d4a8cd9433caaff/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fsupport_test.cc?ref=066d2826509cf3ac603fdacf4d4a8cd9433caaff",
            "patch": "@@ -376,8 +376,7 @@ ENTRY triton_computation {\n })\";\n   TF_ASSERT_OK_AND_ASSIGN(\n       TestedInstruction ti,\n-      ParseTemplateAndGetInstruction(kHloTestTemplate, data_type, opcode,\n-                                     /*use_nested_gemm_fusions=*/true));\n+      ParseTemplateAndGetInstruction(kHloTestTemplate, data_type, opcode));\n   RunSupportTest(std::move(ti), /*output_tile_sizes=*/{4, 4}, cc);\n }\n \n@@ -391,8 +390,7 @@ ENTRY triton_computation {\n })\";\n   TF_ASSERT_OK_AND_ASSIGN(\n       TestedInstruction ti,\n-      ParseTemplateAndGetInstruction(kHloTestTemplate, data_type, opcode,\n-                                     /*use_nested_gemm_fusions=*/true));\n+      ParseTemplateAndGetInstruction(kHloTestTemplate, data_type, opcode));\n   RunSupportTest(std::move(ti), /*output_tile_sizes=*/{4}, cc);\n }\n \n@@ -406,8 +404,7 @@ ENTRY triton_computation {\n })\";\n   TF_ASSERT_OK_AND_ASSIGN(\n       TestedInstruction ti,\n-      ParseTemplateAndGetInstruction(kHloTestTemplate, data_type, opcode,\n-                                     /*use_nested_gemm_fusions=*/true));\n+      ParseTemplateAndGetInstruction(kHloTestTemplate, data_type, opcode));\n   RunSupportTest(std::move(ti), /*output_tile_sizes=*/{4}, cc);\n }\n \n@@ -1054,35 +1051,17 @@ ENTRY triton_computation {\n })\";\n   TF_ASSERT_OK_AND_ASSIGN(TestedInstruction ti,\n                           ParseTemplateAndGetInstruction(\n-                              kHloTestTemplate, F32, HloOpcode::kConcatenate,\n-                              /*use_nested_gemm_fusions=*/true));\n+                              kHloTestTemplate, F32, HloOpcode::kConcatenate));\n   RunSupportTest(std::move(ti), /*output_tile_sizes=*/{1, 64, 1}, cc);\n }\n \n INSTANTIATE_TEST_SUITE_P(ConcatenateTestSuite, ConcatenateDeviceTest,\n                          ::testing::ValuesIn(AllDevicesToTest()));\n \n-// TODO(b/393299275): remove the boolean parameter once the migration is\n-// complete.\n-class TritonSupportTestWithTypeAndDeviceAndBoolParam\n-    : public TritonSupportTest,\n-      public ::testing::WithParamInterface<\n-          std::tuple<PrimitiveType, se::GpuComputeCapability, bool>> {\n- public:\n-  static std::string ParamToString(\n-      const ::testing::TestParamInfo<ParamType>& info) {\n-    auto [data_type, cc, use_nested_gemm_fusions] = info.param;\n-    return absl::StrCat(PrimitiveType_Name(data_type), \"_\",\n-                        ComputeCapabilityToString(cc), \"_\",\n-                        use_nested_gemm_fusions ? \"nested_gemm_fusions\"\n-                                                : \"no_nested_gemm_fusions\");\n-  }\n-};\n-\n-using ConcatenateTest = TritonSupportTestWithTypeAndDeviceAndBoolParam;\n+using ConcatenateTest = TritonSupportTestWithTypeAndDeviceParam;\n \n TEST_P(ConcatenateTest, IsTritonSupportedConcatenate) {\n-  auto [data_type, cc, use_nested_gemm_fusions] = GetParam();\n+  auto [data_type, cc] = GetParam();\n   const std::string kHloTestTemplate = R\"(\n nest0 {\n   ROOT p0 = $0[128] parameter(0)\n@@ -1114,8 +1093,7 @@ ENTRY triton_computation {\n })\";\n   TF_ASSERT_OK_AND_ASSIGN(TestedInstruction ti, ParseTemplateAndGetInstruction(\n                                                     kHloTestTemplate, data_type,\n-                                                    HloOpcode::kConcatenate,\n-                                                    use_nested_gemm_fusions));\n+                                                    HloOpcode::kConcatenate));\n   RunSupportTest(std::move(ti), /*output_tile_sizes=*/{64}, cc);\n }\n \n@@ -1124,9 +1102,8 @@ constexpr std::array kTestedOpsConcatenate = {HloOpcode::kConcatenate};\n INSTANTIATE_TEST_SUITE_P(\n     ConcatenateTestSuite, ConcatenateTest,\n     ::testing::Combine(::testing::ValuesIn(AllXlaDataTypes()),\n-                       ::testing::ValuesIn(AllDevicesToTest()),\n-                       ::testing::Bool()),\n-    ConcatenateTest::ParamToString);\n+                       ::testing::ValuesIn(AllDevicesToTest())),\n+    TritonSupportTestTypeAndDeviceToString);\n \n using CollectiveTest = TritonSupportTestWithTypeAndDeviceParam;\n \n@@ -1917,10 +1894,10 @@ ENTRY triton_computation {\n       hlo_text, primitive_util::LowercasePrimitiveTypeName(input_type),\n       primitive_util::LowercasePrimitiveTypeName(result_type));\n \n-  TF_ASSERT_OK_AND_ASSIGN(TestedInstruction ti,\n-                          ParseTemplateAndGetInstruction(\n-                              hlo_text, PRIMITIVE_TYPE_INVALID, HloOpcode::kDot,\n-                              /*use_nested_gemm_fusions=*/true));\n+  TF_ASSERT_OK_AND_ASSIGN(\n+      TestedInstruction ti,\n+      ParseTemplateAndGetInstruction(hlo_text, PRIMITIVE_TYPE_INVALID,\n+                                     HloOpcode::kDot));\n   RunSupportTest(std::move(ti), /*output_tile_sizes=*/{16, 32}, cc, fail_mode);\n }\n \n@@ -1954,8 +1931,7 @@ ENTRY triton_computation {\n )\";\n   TF_ASSERT_OK_AND_ASSIGN(\n       TestedInstruction ti,\n-      ParseTemplateAndGetInstruction(kHloTestTemplate, F32, HloOpcode::kDot,\n-                                     /* use_nested_gemm_fusions=*/true));\n+      ParseTemplateAndGetInstruction(kHloTestTemplate, F32, HloOpcode::kDot));\n   RunSupportTest(std::move(ti), /*output_tile_sizes=*/{16, 32},\n                  se::CudaComputeCapability::Ampere());\n }\n@@ -1982,8 +1958,7 @@ ENTRY triton_computation {\n )\";\n   TF_ASSERT_OK_AND_ASSIGN(\n       TestedInstruction ti,\n-      ParseTemplateAndGetInstruction(kHloTestTemplate, F32, HloOpcode::kDot,\n-                                     /* use_nested_gemm_fusions=*/true));\n+      ParseTemplateAndGetInstruction(kHloTestTemplate, F32, HloOpcode::kDot));\n   RunSupportTest(std::move(ti), /*output_tile_sizes=*/{16, 32},\n                  se::CudaComputeCapability::Ampere());\n }\n@@ -2022,8 +1997,7 @@ ENTRY triton_computation {\n )\";\n   TF_ASSERT_OK_AND_ASSIGN(\n       TestedInstruction ti,\n-      ParseTemplateAndGetInstruction(kHloTestTemplate, F32, HloOpcode::kDot,\n-                                     /* use_nested_gemm_fusions=*/true));\n+      ParseTemplateAndGetInstruction(kHloTestTemplate, F32, HloOpcode::kDot));\n   RunSupportTest(std::move(ti), /*output_tile_sizes=*/{1, 16, 32},\n                  se::CudaComputeCapability::Ampere());\n }\n@@ -2061,8 +2035,7 @@ ENTRY triton_computation {\n )\";\n   TF_ASSERT_OK_AND_ASSIGN(\n       TestedInstruction ti,\n-      ParseTemplateAndGetInstruction(kHloTestTemplate, F32, HloOpcode::kDot,\n-                                     /* use_nested_gemm_fusions=*/true));\n+      ParseTemplateAndGetInstruction(kHloTestTemplate, F32, HloOpcode::kDot));\n   RunSupportTest(std::move(ti), /*output_tile_sizes=*/{1, 16, 1, 32},\n                  se::CudaComputeCapability::Ampere());\n }\n@@ -2101,8 +2074,7 @@ ENTRY triton_computation {\n )\";\n   TF_ASSERT_OK_AND_ASSIGN(\n       TestedInstruction ti,\n-      ParseTemplateAndGetInstruction(kHloTestTemplate, F32, HloOpcode::kDot,\n-                                     /* use_nested_gemm_fusions=*/true));\n+      ParseTemplateAndGetInstruction(kHloTestTemplate, F32, HloOpcode::kDot));\n   RunSupportTest(std::move(ti), /*output_tile_sizes=*/{16, 32},\n                  se::CudaComputeCapability::Ampere());\n }\n@@ -2142,8 +2114,7 @@ ENTRY triton_computation {\n )\";\n   TF_ASSERT_OK_AND_ASSIGN(\n       TestedInstruction ti,\n-      ParseTemplateAndGetInstruction(kHloTestTemplate, F32, HloOpcode::kDot,\n-                                     /* use_nested_gemm_fusions=*/true));\n+      ParseTemplateAndGetInstruction(kHloTestTemplate, F32, HloOpcode::kDot));\n   RunSupportTest(std::move(ti), /*output_tile_sizes=*/{16, 32},\n                  se::CudaComputeCapability::Ampere());\n }\n@@ -2183,8 +2154,7 @@ ENTRY triton_computation {\n )\";\n   TF_ASSERT_OK_AND_ASSIGN(\n       TestedInstruction ti,\n-      ParseTemplateAndGetInstruction(kHloTestTemplate, F32, HloOpcode::kDot,\n-                                     /* use_nested_gemm_fusions=*/true));\n+      ParseTemplateAndGetInstruction(kHloTestTemplate, F32, HloOpcode::kDot));\n   RunSupportTest(std::move(ti), /*output_tile_sizes=*/{16, 32},\n                  se::CudaComputeCapability::Ampere());\n }\n@@ -2252,8 +2222,7 @@ ENTRY triton_computation {\n   TF_ASSERT_OK_AND_ASSIGN(\n       TestedInstruction ti,\n       ParseTemplateAndGetInstruction(\n-          hlo_text, PrimitiveType::PRIMITIVE_TYPE_INVALID, HloOpcode::kDot,\n-          /* use_nested_gemm_fusions=*/true));\n+          hlo_text, PrimitiveType::PRIMITIVE_TYPE_INVALID, HloOpcode::kDot));\n   RunSupportTest(std::move(ti), /*output_tile_sizes=*/{16, 32}, cc, fail_mode);\n }\n \n@@ -2336,8 +2305,7 @@ ENTRY triton_computation {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       TestedInstruction ti,\n-      ParseTemplateAndGetInstruction(hlo_text, F32, HloOpcode::kDot,\n-                                     /* use_nested_gemm_fusions=*/true));\n+      ParseTemplateAndGetInstruction(hlo_text, F32, HloOpcode::kDot));\n   ExpectedFailMode fail_mode = ExpectedFailMode::kFail;\n   if (absl::c_linear_search(std::vector{F8E5M2, F8E4M3FN, F8E4M3, S8},\n                             data_type)) {\n@@ -2374,8 +2342,7 @@ ENTRY triton_computation {\n   TF_ASSERT_OK_AND_ASSIGN(\n       TestedInstruction ti,\n       ParseTemplateAndGetInstruction(kHloTestTemplate, GetParam(),\n-                                     HloOpcode::kScaledDot,\n-                                     /*use_nested_gemm_fusions=*/true));\n+                                     HloOpcode::kScaledDot));\n   RunSupportTest(std::move(ti), /*output_tile_sizes=*/{16, 16},\n                  se::CudaComputeCapability::Hopper());\n }\n@@ -2421,8 +2388,7 @@ ENTRY triton_computation {\n \n   TF_ASSERT_OK_AND_ASSIGN(\n       TestedInstruction ti,\n-      ParseTemplateAndGetInstruction(hlo_text, F32, HloOpcode::kFusion,\n-                                     /* use_nested_gemm_fusions=*/true));\n+      ParseTemplateAndGetInstruction(hlo_text, F32, HloOpcode::kFusion));\n   RunSupportTest(std::move(ti), /*output_tile_sizes=*/{16, 32}, cc);\n }\n \n@@ -2461,8 +2427,7 @@ ENTRY triton_computation {\n       kind);\n   TF_ASSERT_OK_AND_ASSIGN(\n       TestedInstruction ti,\n-      ParseTemplateAndGetInstruction(hlo_text, F32, HloOpcode::kFusion,\n-                                     /* use_nested_gemm_fusions=*/true));\n+      ParseTemplateAndGetInstruction(hlo_text, F32, HloOpcode::kFusion));\n   RunSupportTest(std::move(ti), /*output_tile_sizes=*/{64}, cc);\n }\n \n@@ -2512,8 +2477,7 @@ ENTRY triton_computation {\n )\";\n   TF_ASSERT_OK_AND_ASSIGN(\n       TestedInstruction ti,\n-      ParseTemplateAndGetInstruction(hlo_text, F32, HloOpcode::kFusion,\n-                                     /*use_nested_gemm_fusions=*/true));\n+      ParseTemplateAndGetInstruction(hlo_text, F32, HloOpcode::kFusion));\n   se::GpuComputeCapability cc = se::CudaComputeCapability::Ampere();\n   ASSERT_FALSE(IsTritonSupportedInstruction(ti.Instruction(), cc));\n   RunSupportTest(std::move(ti), /*output_tile_sizes=*/{64, 32}, cc);\n@@ -3205,7 +3169,10 @@ INSTANTIATE_TEST_SUITE_P(\n         ::testing::ValuesIn(AllDevicesToTest())),\n     TritonSupportTestTwoTypesAndDeviceToString);\n \n-using TopKTest = TritonSupportTestWithTypeAndDeviceAndBoolParam;\n+class TopKTest\n+    : public TritonSupportTest,\n+      public ::testing::WithParamInterface<\n+          std::tuple<PrimitiveType, se::GpuComputeCapability, bool>> {};\n \n std::string ParamToStringTopK(\n     const ::testing::TestParamInfo<TopKTest::ParamType>& info) {"
        },
        {
            "sha": "c02705a9e07ff2d8c3c3e7498e26f1680bc55665",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/test_utils.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 11,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/066d2826509cf3ac603fdacf4d4a8cd9433caaff/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/066d2826509cf3ac603fdacf4d4a8cd9433caaff/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.cc?ref=066d2826509cf3ac603fdacf4d4a8cd9433caaff",
            "patch": "@@ -304,8 +304,7 @@ namespace {\n // computation whose root is a fusion. Otherwise, creates a new entry\n // computation whose root is a fusion instruction that calls the original entry\n // computation. The new fusion instruction uses the generic Triton backend kind.\n-absl::Status ConvertEntryToTritonFusion(HloModule* module,\n-                                        bool use_nested_gemm_fusions) {\n+absl::Status ConvertEntryToTritonFusion(HloModule* module) {\n   if (module->entry_computation()->root_instruction()->opcode() ==\n       HloOpcode::kFusion) {\n     return absl::OkStatus();\n@@ -327,12 +326,8 @@ absl::Status ConvertEntryToTritonFusion(HloModule* module,\n       module->entry_computation()));\n \n   gpu::GpuBackendConfig gpu_config;\n-  if (use_nested_gemm_fusions) {\n-    gpu_config.mutable_fusion_backend_config()->set_kind(\n-        kTritonNestedGemmFusionKind);\n-  } else {\n-    gpu_config.mutable_fusion_backend_config()->set_kind(kTritonFusionKind);\n-  }\n+  gpu_config.mutable_fusion_backend_config()->set_kind(\n+      kTritonNestedGemmFusionKind);\n   TF_RETURN_IF_ERROR(fusion->set_backend_config(gpu_config));\n \n   auto new_entry =\n@@ -355,14 +350,13 @@ DebugOptions TritonSupportTestBase::GetDebugOptionsForTest() const {\n absl::StatusOr<TritonSupportTestBase::TestedInstruction>\n TritonSupportTestBase::ParseTemplateAndGetInstruction(\n     absl::string_view hlo_template, xla::PrimitiveType data_type,\n-    xla::HloOpcode opcode, bool use_nested_gemm_fusions) {\n+    xla::HloOpcode opcode) {\n   const std::string hlo_text = absl::Substitute(\n       hlo_template, primitive_util::LowercasePrimitiveTypeName(data_type),\n       HloOpcodeString(opcode));\n   TF_ASSIGN_OR_RETURN(std::unique_ptr<HloModule> module,\n                       ParseAndReturnVerifiedModule(hlo_text));\n-  TF_RETURN_IF_ERROR(\n-      ConvertEntryToTritonFusion(module.get(), use_nested_gemm_fusions));\n+  TF_RETURN_IF_ERROR(ConvertEntryToTritonFusion(module.get()));\n   const HloComputation* computation =\n       module->GetComputationWithName(\"triton_computation\");\n   if (computation == module->entry_computation()) {"
        },
        {
            "sha": "0d32097bf4cd88152107560fd32ad870eaaf96aa",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/test_utils.h",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/066d2826509cf3ac603fdacf4d4a8cd9433caaff/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/066d2826509cf3ac603fdacf4d4a8cd9433caaff/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftest_utils.h?ref=066d2826509cf3ac603fdacf4d4a8cd9433caaff",
            "patch": "@@ -158,12 +158,9 @@ class TritonSupportTestBase : public HloTestBase {\n   // `triton_computation` with the generic Triton emitter. Tests that need\n   // the `__triton_gemm` backend kind should provide their own ENTRY\n   // computation.\n-  //\n-  // TODO(b/393299275): remove `use_nested_gemm_fusions` once the migration is\n-  // complete.\n   absl::StatusOr<TestedInstruction> ParseTemplateAndGetInstruction(\n       absl::string_view hlo_template, xla::PrimitiveType data_type,\n-      xla::HloOpcode opcode, bool use_nested_gemm_fusions = false);\n+      xla::HloOpcode opcode);\n \n   llvm::LLVMContext llvm_ctx_;\n   llvm::Triple target_triple_;"
        }
    ],
    "stats": {
        "total": 114,
        "additions": 36,
        "deletions": 78
    }
}