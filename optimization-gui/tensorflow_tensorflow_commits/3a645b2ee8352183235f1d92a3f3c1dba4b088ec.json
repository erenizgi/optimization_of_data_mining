{
    "author": "mmakevic-amd",
    "message": "PR #31764: [ROCm] Fix CI build break\n\nImported from GitHub PR https://github.com/openxla/xla/pull/31764\n\nüìù Summary of Changes\nMoved `//xla/service/gpu/llvm_gpu_backend:nvptx_backend` dependency of `xla/backends/gpu/codegen/triton:fusion_emitter` behind `if_cuda_is_configured` guard.\n\nüéØ Justification\nROCm CI was failing to build with:\n```\nERROR: /root/xla/xla/service/gpu/llvm_gpu_backend/BUILD:83:11: Compiling xla/service/gpu/llvm_gpu_backend/nvptx_backend.cc failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing CppCompile command (from target //xla/service/gpu/llvm_gpu_backend:nvptx_backend) external/local_config_rocm/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer ... (remaining 359 arguments skipped)\nxla/service/gpu/llvm_gpu_backend/nvptx_backend.cc:32:10: fatal error: 'third_party/gpus/cuda/include/cuda.h' file not found\n   32 | #include \"third_party/gpus/cuda/include/cuda.h\"\n      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n1 error generated.\n```\n\nüöÄ Kind of Contribution\nüêõ Bug Fix\n\nüìä Benchmark (for Performance Improvements)\n/\n\nüß™ Unit Tests:\n/\n\nüß™ Execution Tests:\n/\n\nCopybara import of the project:\n\n--\nc4332f9b12329dc9c39ed5fc2dda915a29f22edd by Milica Makevic <Milica.Makevic@amd.com>:\n\nMake nvptx_backend cuda only dep\n\nMerging this change closes #31764\n\nPiperOrigin-RevId: 811254313",
    "sha": "3a645b2ee8352183235f1d92a3f3c1dba4b088ec",
    "files": [
        {
            "sha": "1523b489222e7cd3ccfcf3969d4edaa3b5e0b859",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/3a645b2ee8352183235f1d92a3f3c1dba4b088ec/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/3a645b2ee8352183235f1d92a3f3c1dba4b088ec/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=3a645b2ee8352183235f1d92a3f3c1dba4b088ec",
            "patch": "@@ -7,6 +7,10 @@ load(\n )\n load(\"//xla/tests:build_defs.bzl\", \"xla_test\")\n load(\"//xla/tsl:tsl.bzl\", \"if_google\")\n+load(\n+    \"//xla/tsl/platform/default:cuda_build_defs.bzl\",\n+    \"if_cuda_is_configured\",\n+)\n \n package(\n     # copybara:uncomment default_applicable_licenses = [\"//tensorflow:license\"],\n@@ -292,6 +296,8 @@ cc_library(\n         \"@local_tsl//tsl/platform:path\",\n         \"@local_tsl//tsl/platform:statusor\",\n         \"@triton//:TritonTransforms\",\n+    ]) + if_cuda_is_configured([\n+        \"//xla/service/gpu/llvm_gpu_backend:nvptx_backend\",\n     ]),\n )\n "
        }
    ],
    "stats": {
        "total": 6,
        "additions": 6,
        "deletions": 0
    }
}