{
    "author": "sohaibiftikhar",
    "message": "[XLA:GPU] Plug in the all reduce codegen into thunk emitter.\n\nThis is the last in the chain to enable all-reduce codegen through triton.\nThis switches to the codegen all-reduce for the one-shot variant.\n\nSome changes were made to the collective emitter to make it work e2e:\n  - Add casts for PRED types.\n  - Fix issues with double buffering index calculation.\n  - tt.divisibility was added to remote buffer loads to remove the layout conversion during lowering.\n\nPiperOrigin-RevId: 838808339",
    "sha": "1eac4fbc9d04c4b1249d42d544fe85050941f6df",
    "files": [
        {
            "sha": "c309c1ff0d77c513bafbb746dc0156d8c5599d82",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1eac4fbc9d04c4b1249d42d544fe85050941f6df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1eac4fbc9d04c4b1249d42d544fe85050941f6df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2FBUILD?ref=1eac4fbc9d04c4b1249d42d544fe85050941f6df",
            "patch": "@@ -1084,6 +1084,7 @@ xla_cc_test(\n         \"@com_google_absl//absl/memory\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/strings:string_view\",\n         \"@com_google_googletest//:gtest_main\","
        },
        {
            "sha": "25b6c1d8f3834cdcab3f835ee35007d54bed8cf5",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/collective_emitter.cc",
            "status": "modified",
            "additions": 69,
            "deletions": 28,
            "changes": 97,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1eac4fbc9d04c4b1249d42d544fe85050941f6df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1eac4fbc9d04c4b1249d42d544fe85050941f6df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter.cc?ref=1eac4fbc9d04c4b1249d42d544fe85050941f6df",
            "patch": "@@ -30,8 +30,8 @@ limitations under the License.\n #include \"llvm/Support/MathExtras.h\"\n #include \"mlir/Dialect/Arith/IR/Arith.h\"\n #include \"mlir/Dialect/LLVMIR/NVVMDialect.h\"\n+#include \"mlir/IR/Attributes.h\"\n #include \"mlir/IR/Builders.h\"\n-#include \"mlir/IR/BuiltinTypeInterfaces.h\"\n #include \"mlir/IR/BuiltinTypes.h\"\n #include \"mlir/IR/TypeUtilities.h\"\n #include \"mlir/IR/Types.h\"\n@@ -66,7 +66,6 @@ limitations under the License.\n namespace xla::gpu {\n namespace {\n \n-using ::mlir::ShapedType;\n using ::mlir::Value;\n using ::xla::se::gpu::AllReduceStrategy;\n using ::xla::xtile::TensorValue;\n@@ -191,6 +190,8 @@ absl::StatusOr<TensorValue> EmitAllReduce(\n     const BlockLevelParameters& block_level_parameters,\n     mlir::FunctionOpInterface fn, mlir::Value pid,\n     absl::flat_hash_map<const TiledHloInstruction*, TensorValue>& values) {\n+  const int64_t num_elements =\n+      ShapeUtil::ElementsIn(computation->root_instruction()->shape());\n   const TiledHloInstruction* tiled_input_hlo = tiled_hlo_reduce.operand(0);\n   TensorValue input_tile = values[tiled_input_hlo];\n \n@@ -223,26 +224,59 @@ absl::StatusOr<TensorValue> EmitAllReduce(\n       ttir::PointerType::get(b.getI64Type(), kGlobalAddressSpace);\n   auto remote_input_buffers_i64 =\n       ttir::BitcastOp::create(b, ptr_to_i64_type, remote_input_buffers);\n-  Value remote_buf_ptr_addr = ttir::AddPtrOp::create(\n-      b, ptr_to_i64_type, remote_input_buffers_i64, device_rank);\n-  Value remote_buf_i64 = ttir::LoadOp::create(b, remote_buf_ptr_addr,\n-                                              ttir::CacheModifier::NONE,     //\n-                                              ttir::EvictionPolicy::NORMAL,  //\n-                                              false);  // isVolatile\n-  const auto elem_type =\n-      mlir::cast<ShapedType>(input_tile.getType()).getElementType();\n+\n+  const mlir::Type i64_type = b.getI64Type();\n+  const mlir::Type elem_type = input_tile.getType().getElementType();\n+  const mlir::Type elem_storage_type = xtile::StorageType(elem_type);\n   const auto ptr_to_elem_type =\n-      ttir::PointerType::get(elem_type, kGlobalAddressSpace);\n-  Value remote_buf_ptr =\n-      ttir::IntToPtrOp::create(b, ptr_to_elem_type, remote_buf_i64);\n+      ttir::PointerType::get(elem_storage_type, kGlobalAddressSpace);\n+  constexpr int32_t kBitsPerByte = 8;\n+  const int64_t remote_buffer_size =\n+      num_elements * (elem_storage_type.getIntOrFloatBitWidth() / kBitsPerByte);\n+  Value buffer_index = arith::AndIOp::create(\n+      b, i64_type, arith::ExtSIOp::create(b, i64_type, signal_value),\n+      arith::ConstantOp::create(b, i64_type, b.getI64IntegerAttr(1)));\n+  Value buffer_offset = arith::MulIOp::create(\n+      b, i64_type, buffer_index,\n+      arith::ConstantOp::create(b, i64_type,\n+                                b.getI64IntegerAttr(remote_buffer_size)));\n+  // Helper function to get the buffer pointer for a given signal value.\n+  const auto get_buffer_ptr = [&](mlir::Value buffer_ptr_base) -> mlir::Value {\n+    return ttir::AddPtrOp::create(b, ptr_to_elem_type, buffer_ptr_base,\n+                                  buffer_offset);\n+  };\n+\n   mlir::ArrayRef<int64_t> remote_shape = tile_info.original_shape();\n   const mlir::MemRefType remote_memref_type =\n-      mlir::MemRefType::get(remote_shape, elem_type);\n-  mlir::Value remote_buf_memref =\n-      mtx::PtrToMemrefOp::create(b, remote_memref_type, remote_buf_ptr);\n-  xtile::InsertTileOp::create(\n-      b, input_tile, remote_buf_memref, tile_info.offsets(),\n-      tile_info.padded_tile_sizes(), tile_info.tile_strides());\n+      mlir::MemRefType::get(remote_shape, elem_storage_type);\n+  // Scoped to reuse variable names during reduction phase.\n+  {\n+    Value remote_buf_ptr_addr = ttir::AddPtrOp::create(\n+        b, ptr_to_i64_type, remote_input_buffers_i64, device_rank);\n+    Value remote_buf_i64 =\n+        ttir::LoadOp::create(b, remote_buf_ptr_addr,\n+                             ttir::CacheModifier::NONE,     //\n+                             ttir::EvictionPolicy::NORMAL,  //\n+                             false);                        // isVolatile\n+    Value remote_buf_ptr_base = ttir::IntToPtrOp::create(\n+        b, ptr_to_elem_type, remote_buf_i64,\n+        llvm::ArrayRef<mlir::NamedAttribute>{xtile::GetDivisibilityAttr(b)});\n+    Value remote_buf_ptr = get_buffer_ptr(remote_buf_ptr_base);\n+    mlir::Value remote_buf_memref =\n+        mtx::PtrToMemrefOp::create(b, remote_memref_type, remote_buf_ptr);\n+    // Workaround(i1_to_i8_workaround) as in fusion_emitter.\n+    // The parameter extraction casts the storage type to the logical type.\n+    // But for copying to the remote buffer we need to cast it back to the\n+    // storage type. Downstream passes should be able to optimize this away.\n+    TensorValue storage_tile = input_tile;\n+    if (elem_storage_type != elem_type) {\n+      storage_tile = mlir::cast<TensorValue>(\n+          xtile::Cast(b, input_tile, elem_storage_type));\n+    }\n+    xtile::InsertTileOp::create(\n+        b, storage_tile, remote_buf_memref, tile_info.offsets(),\n+        tile_info.padded_tile_sizes(), tile_info.tile_strides());\n+  }\n \n   // 2. Synchronization phase: Wait for all ranks to complete the scatter.\n   int64_t world_size = all_reduce.device_list().num_devices_per_group();\n@@ -258,12 +292,8 @@ absl::StatusOr<TensorValue> EmitAllReduce(\n       to_emit.push_back(instr);\n     }\n   }\n-  // Set accumulator zero.\n-  mlir::Value accumulator_zero =\n-      arith::ConstantOp::create(b, elem_type, b.getZeroAttr(elem_type));\n-  TensorValue accumulator =\n-      xtile::Splat(b, accumulator_zero, input_tile.getType().getShape());\n-  for (int rank = 0; rank < world_size; ++rank) {\n+\n+  const auto load_tile_for_rank = [&](int64_t rank) {\n     Value rank_idx =\n         arith::ConstantOp::create(b, b.getI64Type(), b.getI64IntegerAttr(rank));\n     Value remote_buf_ptr_addr = ttir::AddPtrOp::create(\n@@ -273,13 +303,23 @@ absl::StatusOr<TensorValue> EmitAllReduce(\n                              ttir::CacheModifier::NONE,     //\n                              ttir::EvictionPolicy::NORMAL,  //\n                              false);                        // isVolatile\n-    Value remote_buf_ptr =\n+    Value remote_buf_ptr_base =\n         ttir::IntToPtrOp::create(b, ptr_to_elem_type, remote_buf_i64);\n+    Value remote_buf_ptr = get_buffer_ptr(remote_buf_ptr_base);\n     Value remote_buf_memref =\n         mtx::PtrToMemrefOp::create(b, remote_memref_type, remote_buf_ptr);\n     TensorValue next_tile =\n         EmitParameterExtract(b, tile_info, remote_buf_memref);\n-\n+    // # Workaround(i1_to_i8_workaround) as in fusion_emitter.\n+    // See fusion emitter for more details.\n+    if (elem_storage_type != elem_type) {\n+      next_tile = mlir::cast<TensorValue>(xtile::Cast(b, next_tile, elem_type));\n+    }\n+    return next_tile;\n+  };\n+  TensorValue accumulator = load_tile_for_rank(0);\n+  for (int rank = 1; rank < world_size; ++rank) {\n+    TensorValue next_tile = load_tile_for_rank(rank);\n     absl::flat_hash_map<const HloInstruction*, TensorValue> region_values;\n     region_values[reduction_computation->parameter_instruction(0)] =\n         accumulator;\n@@ -395,7 +435,8 @@ absl::StatusOr<int32_t> AddCollectiveMetadataArguments(\n     // Also add the remote/scratch buffers for collectives.\n     // !tt.ptr<!tt.ptr<type>>\n     fn_arg_types.push_back(ttir::PointerType::get(\n-        ttir::PointerType::get(ir_type, kGlobalAddressSpace),\n+        ttir::PointerType::get(xtile::StorageType(ir_type),\n+                               kGlobalAddressSpace),\n         kGlobalAddressSpace));\n   }\n   // num_metadata_args ="
        },
        {
            "sha": "8e406de216c1f7dbf9a468295cea872f1a1a8aa2",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/collective_emitter_test.cc",
            "status": "modified",
            "additions": 37,
            "deletions": 11,
            "changes": 48,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1eac4fbc9d04c4b1249d42d544fe85050941f6df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1eac4fbc9d04c4b1249d42d544fe85050941f6df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Fcollective_emitter_test.cc?ref=1eac4fbc9d04c4b1249d42d544fe85050941f6df",
            "patch": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_format.h\"\n+#include \"absl/strings/str_join.h\"\n #include \"absl/strings/string_view.h\"\n #include \"llvm/IR/Module.h\"\n #include \"mlir/IR/MLIRContext.h\"\n@@ -40,6 +41,7 @@ limitations under the License.\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/testlib/hlo_hardware_independent_test_base.h\"\n #include \"xla/hlo/utils/hlo_query.h\"\n+#include \"xla/primitive_util.h\"\n #include \"xla/service/gpu/gpu_device_info_for_tests.h\"\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n #include \"xla/service/hlo_creation_utils.h\"\n@@ -102,12 +104,18 @@ class CollectiveBlockLevelConfigTest : public HloHardwareIndependentTestBase {\n  protected:\n   static std::string GetModuleStr(const Shape& shape,\n                                   absl::string_view replica_groups = \"{0,1}\") {\n+    absl::string_view type_str =\n+        xla::primitive_util::LowercasePrimitiveTypeName(shape.element_type());\n+    absl::string_view reduction_kind = \"add\";\n+    if (shape.element_type() == PRED) {\n+      reduction_kind = \"or\";\n+    }\n     return absl::StrFormat(R\"(\n       HloModule test\n       apply_op {\n-        x = f32[] parameter(0)\n-        y = f32[] parameter(1)\n-        ROOT apply_op = f32[] add(x, y)\n+        x = %3$s[] parameter(0)\n+        y = %3$s[] parameter(1)\n+        ROOT apply_op = %3$s[] %4$s(x, y)\n       }\n \n       ENTRY test_computation {\n@@ -116,7 +124,8 @@ class CollectiveBlockLevelConfigTest : public HloHardwareIndependentTestBase {\n         ROOT all-reduce-done = %1$s all-reduce-done(all-reduce-start)\n       }\n     )\",\n-                           shape.ToString(), replica_groups);\n+                           shape.ToString(), replica_groups, type_str,\n+                           reduction_kind);\n   }\n \n   const se::DeviceDescription device_info_;\n@@ -167,12 +176,12 @@ struct AllReduceBlockLevelConfigTestCase {\n   }\n };\n \n-class CollectiveEmitterParameterizedTest\n+class CollectiveBlockLevelConfigParameterizedTest\n     : public CollectiveBlockLevelConfigTest,\n       public ::testing::WithParamInterface<AllReduceBlockLevelConfigTestCase> {\n };\n \n-TEST_P(CollectiveEmitterParameterizedTest, AllReduceBlockLevelConfig) {\n+TEST_P(CollectiveBlockLevelConfigParameterizedTest, AllReduceBlockLevelConfig) {\n   const auto& param = GetParam();\n   TF_ASSERT_OK_AND_ASSIGN(const auto module_with_fusion,\n                           BuildModuleWithFusion(GetModuleStr(param.shape)));\n@@ -184,7 +193,7 @@ TEST_P(CollectiveEmitterParameterizedTest, AllReduceBlockLevelConfig) {\n \n INSTANTIATE_TEST_SUITE_P(\n     CollectiveEmitterParameterizedTestInstantiation,\n-    CollectiveEmitterParameterizedTest,\n+    CollectiveBlockLevelConfigParameterizedTest,\n     ::testing::Values(AllReduceBlockLevelConfigTestCase{\n                           /* .test_name = */ \"F32_65536\",\n                           /* .shape = */ ShapeUtil::MakeShape(F32, {65536}),\n@@ -204,7 +213,7 @@ INSTANTIATE_TEST_SUITE_P(\n                             output_tiles { sizes: 256 sizes: 16 }\n                           )pb\"}),\n     [](const ::testing::TestParamInfo<\n-        CollectiveEmitterParameterizedTest::ParamType>& info) {\n+        CollectiveBlockLevelConfigParameterizedTest::ParamType>& info) {\n       return info.param.test_name;\n     });\n \n@@ -252,11 +261,15 @@ TEST_F(CollectiveEmitterTest, AllReduceWithTritonGetLaunchConfig) {\n   EXPECT_EQ(launch_config->launch_dimensions.num_threads_per_block(), 512);\n }\n \n-TEST_F(CollectiveEmitterTest, AllReduceWithTritonGenerateTritonKernel) {\n+class CollectiveEmitterParameterizedTest\n+    : public CollectiveEmitterTest,\n+      public ::testing::WithParamInterface<Shape> {};\n+\n+TEST_P(CollectiveEmitterParameterizedTest,\n+       AllReduceWithTritonGenerateTritonKernelSanity) {\n   TF_ASSERT_OK_AND_ASSIGN(\n       std::unique_ptr<ModuleWithEmitter> result,\n-      BuildModuleWithEmitter(GetModuleStr(ShapeUtil::MakeShape(F32, {65536})),\n-                             device_info_));\n+      BuildModuleWithEmitter(GetModuleStr(GetParam()), device_info_));\n   const TritonFusion* triton_fusion = result->emitter.get();\n   ASSERT_NE(triton_fusion, nullptr);\n   TF_ASSERT_OK_AND_ASSIGN(\n@@ -266,6 +279,19 @@ TEST_F(CollectiveEmitterTest, AllReduceWithTritonGenerateTritonKernel) {\n           &result->llvm_module, &result->mlir_context));\n }\n \n+INSTANTIATE_TEST_SUITE_P(\n+    CollectiveEmitterParameterizedTestInstantiation,\n+    CollectiveEmitterParameterizedTest,\n+    ::testing::Values(ShapeUtil::MakeShape(F32, {65536}),\n+                      ShapeUtil::MakeShape(BF16, {200, 100}),\n+                      ShapeUtil::MakeShape(PRED, {200, 64})),\n+    [](const ::testing::TestParamInfo<\n+        CollectiveEmitterParameterizedTest::ParamType>& info) {\n+      return primitive_util::LowercasePrimitiveTypeName(\n+                 info.param.element_type()) +\n+             \"__\" + absl::StrJoin(info.param.dimensions(), \"_\");\n+    });\n+\n }  // namespace\n \n }  // namespace xla::gpu"
        },
        {
            "sha": "89ed1ef978bb52206c8a83dafda7dc1bb69c9105",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/emitter_helpers.h",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1eac4fbc9d04c4b1249d42d544fe85050941f6df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1eac4fbc9d04c4b1249d42d544fe85050941f6df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Femitter_helpers.h?ref=1eac4fbc9d04c4b1249d42d544fe85050941f6df",
            "patch": "@@ -28,6 +28,7 @@ limitations under the License.\n #include \"llvm/ADT/SmallVector.h\"\n #include \"llvm/Support/raw_ostream.h\"\n #include \"mlir/Dialect/Arith/IR/Arith.h\"\n+#include \"mlir/IR/Attributes.h\"\n #include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n #include \"mlir/IR/BuiltinTypeInterfaces.h\"\n@@ -47,6 +48,7 @@ limitations under the License.\n namespace xla::xtile {\n \n using TensorValue = mlir::TypedValue<mlir::RankedTensorType>;\n+static constexpr auto kTritonDivisibilityAttr = \"tt.divisibility\";\n \n // Returns a string representation of the given MLIR entity.\n template <typename T>\n@@ -244,6 +246,13 @@ TensorValue BroadcastInDims(mlir::ImplicitLocOpBuilder& b, TensorValue value,\n TensorValue Splat(mlir::ImplicitLocOpBuilder& b, ::mlir::Value value,\n                   ::mlir::ArrayRef<int64_t> output_shape);\n \n+// Returns a named attribute for divisibility of triton pointer function\n+// arguments.\n+inline mlir::NamedAttribute GetDivisibilityAttr(mlir::ImplicitLocOpBuilder& b) {\n+  return b.getNamedAttr(kTritonDivisibilityAttr,\n+                        b.getIntegerAttr(b.getI32Type(), 16));\n+}\n+\n }  // namespace xla::xtile\n \n #endif  // XLA_BACKENDS_GPU_CODEGEN_TRITON_EMITTER_HELPERS_H_"
        },
        {
            "sha": "056bbfb3e8fbf7a4e0b59f51af2ab685992eeb1b",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion.cc",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1eac4fbc9d04c4b1249d42d544fe85050941f6df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1eac4fbc9d04c4b1249d42d544fe85050941f6df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.cc?ref=1eac4fbc9d04c4b1249d42d544fe85050941f6df",
            "patch": "@@ -110,10 +110,15 @@ TritonFusion::GenerateTritonKernelAndWrapper(\n absl::StatusOr<FusionEmissionResult> TritonFusion::Emit(\n     IrEmitterContext& ir_emitter_context,\n     const HloFusionInstruction& fusion) const {\n-  return Emit(ir_emitter_context, fusion, nullptr, {});\n+  TF_ASSIGN_OR_RETURN(EmitResult kernel_and_module,\n+                      Emit(ir_emitter_context, fusion, nullptr, {}));\n+  FusionEmissionResult result;\n+  result.thunks.push_back(std::move(kernel_and_module.kernel_thunk));\n+  result.module = std::move(kernel_and_module.llvm_module);\n+  return result;\n }\n \n-absl::StatusOr<FusionEmissionResult> TritonFusion::Emit(\n+absl::StatusOr<TritonFusion::EmitResult> TritonFusion::Emit(\n     IrEmitterContext& ir_emitter_context, const HloFusionInstruction& fusion,\n     const HloInstruction* instr_override,\n     absl::Span<const Shape> unmanaged_arguments) const {\n@@ -186,7 +191,6 @@ absl::StatusOr<FusionEmissionResult> TritonFusion::Emit(\n         RemoveUnusedTritonAbiArguments(local_module.get(), ir_emitter_context,\n                                        sanitized_kernel_name, launch_dimensions,\n                                        kernel_arguments));\n-\n     PopulateNvvmAnnotations(local_module.get(), kernel, triton_wrapper_result);\n \n     return {{kernel->getName().str(), launch_dimensions,\n@@ -200,17 +204,13 @@ absl::StatusOr<FusionEmissionResult> TritonFusion::Emit(\n           hlo_computation, kernel_arguments.args(),\n           /*discriminator=*/\"\", generate);\n   TF_ASSIGN_OR_RETURN(const KernelReuseCache::Entry* entry, status_or_entry);\n-\n-  FusionEmissionResult result;\n-  result.thunks.emplace_back(std::make_unique<KernelThunk>(\n-      Thunk::ThunkInfo::WithProfileAnnotation(\n-          &fusion, ir_emitter_context.GetNextThunkId()),\n-      entry->kernel_name, kernel_arguments, entry->launch_dimensions,\n-      entry->cluster_dim, entry->shmem_bytes, entry->tma_metadata));\n-  if (!was_cached) {\n-    result.module = std::move(local_module);\n-  }\n-  return result;\n+  return EmitResult{\n+      std::make_unique<KernelThunk>(\n+          Thunk::ThunkInfo::WithProfileAnnotation(\n+              &fusion, ir_emitter_context.GetNextThunkId()),\n+          entry->kernel_name, kernel_arguments, entry->launch_dimensions,\n+          entry->cluster_dim, entry->shmem_bytes, entry->tma_metadata),\n+      was_cached ? nullptr : std::move(local_module)};\n }\n \n namespace {"
        },
        {
            "sha": "20745b6fd7c39b46f0d63814564b998881f5b3e4",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion.h",
            "status": "modified",
            "additions": 14,
            "deletions": 2,
            "changes": 16,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1eac4fbc9d04c4b1249d42d544fe85050941f6df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1eac4fbc9d04c4b1249d42d544fe85050941f6df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion.h?ref=1eac4fbc9d04c4b1249d42d544fe85050941f6df",
            "patch": "@@ -1,4 +1,3 @@\n-#include \"xla/backends/gpu/codegen/triton/xtile_compiler.h\"\n /* Copyright 2024 The OpenXLA Authors.\n \n Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -16,13 +15,17 @@ limitations under the License.\n #ifndef XLA_BACKENDS_GPU_CODEGEN_TRITON_FUSION_H_\n #define XLA_BACKENDS_GPU_CODEGEN_TRITON_FUSION_H_\n \n+#include <memory>\n #include <optional>\n+#include <utility>\n \n #include \"absl/status/statusor.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n #include \"llvm/IR/Module.h\"\n #include \"xla/backends/gpu/codegen/fusion_emitter.h\"\n+#include \"xla/backends/gpu/codegen/triton/xtile_compiler.h\"\n+#include \"xla/backends/gpu/runtime/kernel_thunk.h\"\n #include \"xla/hlo/ir/hlo_instruction.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/service/gpu/hlo_fusion_analysis.h\"\n@@ -49,6 +52,13 @@ class TritonFusion : public FusionInterface {\n       IrEmitterContext& ir_emitter_context,\n       const HloFusionInstruction& fusion) const final;\n \n+  // A kernel thunk and the corresponding LLVM module if it was not cached.\n+  // This is a more concrete version of FusionEmissionResult that can be used in\n+  // places where we know we are dealing with Triton fusions.\n+  struct EmitResult {\n+    std::unique_ptr<KernelThunk> kernel_thunk;\n+    std::unique_ptr<llvm::Module> llvm_module;\n+  };\n   // Overload of [Emit] that allows passing overrides for instructions\n   // and unmanaged arguments.\n   // - Instruction overloads are required when we forcibly form fusions for\n@@ -63,7 +73,9 @@ class TritonFusion : public FusionInterface {\n   //\n   // TODO(b/461717780): Remove the instruction override once we form collective\n   // based fusions earlier in the compiler pipeline.\n-  absl::StatusOr<FusionEmissionResult> Emit(\n+  // Returns a pair of the kernel thunk and an llvm module. The local module\n+  // is only returned if the kernel was not cached.\n+  absl::StatusOr<EmitResult> Emit(\n       IrEmitterContext& ir_emitter_context, const HloFusionInstruction& fusion,\n       const HloInstruction* instr_override,\n       absl::Span<const Shape> unmanaged_arguments) const;"
        },
        {
            "sha": "677d7ccbac6070e3f39cd65a8caeb014947ea015",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/fusion_emitter.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1eac4fbc9d04c4b1249d42d544fe85050941f6df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1eac4fbc9d04c4b1249d42d544fe85050941f6df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ffusion_emitter.cc?ref=1eac4fbc9d04c4b1249d42d544fe85050941f6df",
            "patch": "@@ -1124,6 +1124,7 @@ absl::StatusOr<TensorValue> EmitTiledHloInstruction(\n     TensorValue parameter =\n         EmitParameterExtract(b, tile_info, fn.getArgument(arg_index));\n \n+    // Workaround(i1_to_i8_workaround)\n     // Some types are stored using different types, e.g. i1 is stored in memory\n     // as i8. It's important to type checking that we perform a conversion after\n     // loading if the type of the loaded parameter does not match what is\n@@ -1436,6 +1437,7 @@ absl::Status EmitGeneric(\n   for (auto [root, result, arg] :\n        llvm::zip(tiled_hlo_computation.GetRoots(), results,\n                  fn.getArguments().drop_front(computation->num_parameters()))) {\n+    // Workaround(i1_to_i8_workaround)\n     // Some types are stored using different types, e.g. i1 is stored in memory\n     // as i8. It's important to check converted types before storing if the type\n     // of the result does not match the type of the output pointer."
        },
        {
            "sha": "9ce2658877f5c1ab6555bbd63638abc88cdf2b2f",
            "filename": "third_party/xla/xla/backends/gpu/codegen/triton/transforms/triton_xla_extract_insert_to_triton_pass.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1eac4fbc9d04c4b1249d42d544fe85050941f6df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_extract_insert_to_triton_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1eac4fbc9d04c4b1249d42d544fe85050941f6df/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_extract_insert_to_triton_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Ftriton%2Ftransforms%2Ftriton_xla_extract_insert_to_triton_pass.cc?ref=1eac4fbc9d04c4b1249d42d544fe85050941f6df",
            "patch": "@@ -361,8 +361,8 @@ class RewriteFuncOp : public mlir::OpRewritePattern<func::FuncOp> {\n       if (op.getArgAttr(i, \"tt.nv_tma_desc\")) {\n         continue;\n       }\n-      new_func.setArgAttr(i, \"tt.divisibility\",\n-                          builder.getIntegerAttr(builder.getI32Type(), 16));\n+      const mlir::NamedAttribute attr = xtile::GetDivisibilityAttr(builder);\n+      new_func.setArgAttr(i, attr.getName(), attr.getValue());\n     }\n \n     rewriter.inlineRegionBefore(op.getRegion(), new_func.getFunctionBody(),"
        },
        {
            "sha": "c2b1db8f6c7737934e6e45f388907e4d32e696a3",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1eac4fbc9d04c4b1249d42d544fe85050941f6df/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1eac4fbc9d04c4b1249d42d544fe85050941f6df/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=1eac4fbc9d04c4b1249d42d544fe85050941f6df",
            "patch": "@@ -476,6 +476,8 @@ cc_library(\n         \"//xla/backends/gpu/codegen:fusion_emitter\",\n         \"//xla/backends/gpu/codegen:fusions\",\n         \"//xla/backends/gpu/codegen/llvm:llvm_emitter\",\n+        \"//xla/backends/gpu/codegen/triton:collective_emitter\",\n+        \"//xla/backends/gpu/codegen/triton:fusion\",\n         \"//xla/backends/gpu/codegen/triton:fusion_emitter\",\n         \"//xla/backends/gpu/codegen/triton:xtile_compiler\",\n         \"//xla/backends/gpu/collectives:gpu_clique_key\","
        },
        {
            "sha": "a3966bb8c2c103d637b0733b4843d42d391bd3a9",
            "filename": "third_party/xla/xla/service/gpu/thunk_emitter.cc",
            "status": "modified",
            "additions": 60,
            "deletions": 19,
            "changes": 79,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/1eac4fbc9d04c4b1249d42d544fe85050941f6df/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/1eac4fbc9d04c4b1249d42d544fe85050941f6df/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2Fthunk_emitter.cc?ref=1eac4fbc9d04c4b1249d42d544fe85050941f6df",
            "patch": "@@ -57,6 +57,8 @@ limitations under the License.\n #include \"xla/backends/gpu/codegen/fusion_emitter.h\"\n #include \"xla/backends/gpu/codegen/fusions.h\"\n #include \"xla/backends/gpu/codegen/llvm/llvm_emitter.h\"\n+#include \"xla/backends/gpu/codegen/triton/collective_emitter.h\"\n+#include \"xla/backends/gpu/codegen/triton/fusion.h\"\n #include \"xla/backends/gpu/codegen/triton/xtile_compiler.h\"\n #include \"xla/backends/gpu/runtime/all_gather_thunk.h\"\n #include \"xla/backends/gpu/runtime/all_reduce_thunk.h\"\n@@ -144,6 +146,7 @@ limitations under the License.\n #include \"xla/service/gpu/stream_executor_util.h\"\n #include \"xla/service/gpu/transforms/collectives/collective_ops_utils.h\"\n #include \"xla/service/gpu/triton_call.h\"\n+#include \"xla/service/hlo_creation_utils.h\"\n #include \"xla/service/llvm_ir/buffer_assignment_util.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n@@ -165,6 +168,11 @@ limitations under the License.\n namespace xla::gpu {\n namespace {\n \n+struct EmitCollectiveResult {\n+  std::unique_ptr<CollectiveKernelThunk> thunk;\n+  std::unique_ptr<llvm::Module> llvm_module;\n+};\n+\n // TODO: move into a host_execute specific file.\n bool IsHostExecuteCustomCall(const HloInstruction& hlo) {\n   return hlo.opcode() == HloOpcode::kCustomCall &&\n@@ -187,23 +195,53 @@ static constexpr bool kRequiresCollectiveKernelThunk =\n // As it stands now the collective kernel thunk is wrapped inside other\n // collective thunks such as AllReduceStart. So this function is only\n // responsible for emitting the collective kernel thunk and its dependencies.\n-// If nullptr is returned it means that the collective kernel thunk could not be\n-// emitted. This is not an error.\n-absl::StatusOr<std::unique_ptr<CollectiveKernelThunk>>\n-EmitCollectiveKernelThunk(IrEmitterContext* ir_emitter_context,\n-                          const CallGraph* call_graph,\n-                          Thunk::ThunkInfo thunk_info,\n-                          std::vector<CollectiveThunk::Buffer> buffers,\n-                          const HloAllReduceInstruction* instr,\n-                          const AllReduceConfig& config) {\n-  return std::make_unique<CollectiveKernelThunk>(\n-      thunk_info, config.config, config.reduction_kind,\n-      /*is_async=*/!IsGPUSyncCollective(*instr), std::move(buffers),\n-      /*is_collective_kernel_enabled=*/\n-      instr->GetModule()\n-          ->config()\n-          .debug_options()\n-          .xla_gpu_unsupported_use_all_reduce_one_shot_kernel());\n+absl::StatusOr<EmitCollectiveResult> EmitCollectiveKernelThunk(\n+    IrEmitterContext* ir_emitter_context, const CallGraph* call_graph,\n+    Thunk::ThunkInfo thunk_info, std::vector<CollectiveThunk::Buffer> buffers,\n+    const HloAllReduceInstruction* instr, const AllReduceConfig& config) {\n+  std::unique_ptr<HloModule> fused_module =\n+      NewModuleWithFusion(instr, HloInstruction::FusionKind::kLoop);\n+  HloFusionInstruction* fusion_instr = Cast<HloFusionInstruction>(\n+      fused_module->entry_computation()->root_instruction());\n+  const se::DeviceDescription& device_info =\n+      ir_emitter_context->gpu_device_info();\n+  const auto make_thunk = [&](absl::string_view kernel_name,\n+                              int32_t shmem_bytes,\n+                              std::unique_ptr<llvm::Module> local_module) {\n+    return EmitCollectiveResult{\n+        std::make_unique<CollectiveKernelThunk>(\n+            thunk_info, config.config, config.reduction_kind,\n+            /*is_async=*/!IsGPUSyncCollective(*instr), std::move(buffers),\n+            /*is_collective_kernel_enabled=*/\n+            instr->GetModule()\n+                ->config()\n+                .debug_options()\n+                .xla_gpu_unsupported_use_all_reduce_one_shot_kernel(),\n+            /*kernel_name=*/kernel_name,\n+            /*shmem_bytes=*/shmem_bytes,\n+            /*is_multimem_enabled=*/false),\n+        std::move(local_module)};\n+  };\n+  TF_ASSIGN_OR_RETURN(bool did_set_config, TrySetGpuBackendConfigForCollective(\n+                                               device_info, fusion_instr));\n+  if (!did_set_config) {\n+    return make_thunk(/*kernel_name=*/\"\", 0, nullptr);\n+  }\n+  const HloFusionAnalysis fusion_analysis =\n+      HloFusionAnalysis::Create(*fusion_instr, device_info);\n+  auto emitter = std::make_unique<TritonFusion>(fusion_analysis);\n+  TritonFusion::EmitResult result;\n+  {\n+    XLA_SCOPED_LOGGING_TIMER(\"Emit collective kernel thunk\");\n+    TF_ASSIGN_OR_RETURN(std::vector<Shape> unmanaged_arguments,\n+                        GetCollectiveUnmanagedKernelArguments(fusion_instr));\n+    TF_ASSIGN_OR_RETURN(\n+        result, emitter->Emit(*ir_emitter_context, *fusion_instr,\n+                              /*instr_override=*/instr, unmanaged_arguments));\n+  }\n+  return make_thunk(result.kernel_thunk->kernel_name(),\n+                    result.kernel_thunk->shmem_bytes(),\n+                    std::move(result.llvm_module));\n }\n \n // If the fusion instruction is a dynamic-slice-fusion instruction,\n@@ -1790,13 +1828,16 @@ absl::StatusOr<ThunkSequence> ThunkEmitter::EmitCollectiveThunk(\n   // lifted out of the all reduce thunk.\n   if constexpr (kRequiresCollectiveKernelThunk<CollectiveThunkType>) {\n     TF_ASSIGN_OR_RETURN(\n-        auto collective_kernel_thunk,\n+        auto emit_result,\n         EmitCollectiveKernelThunk(\n             ir_emitter_context_, call_graph_.get(), thunk_info, buffers,\n             Cast<HloAllReduceInstruction>(inst), GetAllReduceConfigInst(inst)));\n+    if (emit_result.llvm_module != nullptr) {\n+      kernel_modules_.push_back(std::move(emit_result.llvm_module));\n+    }\n     thunk = std::make_unique<CollectiveThunkType>(\n         thunk_info, inst, /*buffers=*/std::move(buffers),\n-        std::move(collective_kernel_thunk),\n+        std::move(emit_result.thunk),\n         ir_emitter_context_->debug_options().xla_gpu_use_memcpy_local_p2p());\n   } else {\n     thunk = std::make_unique<CollectiveThunkType>("
        }
    ],
    "stats": {
        "total": 286,
        "additions": 210,
        "deletions": 76
    }
}