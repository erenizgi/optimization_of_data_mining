{
    "author": "junwhanahn",
    "message": "Add an option to use a custom host memory allocator for PjRt GPU\n\nPiperOrigin-RevId: 844863896",
    "sha": "46151f082e5d165c998639a1187d9eb04447e900",
    "files": [
        {
            "sha": "6383fc562d55f2da89d3fda201d4384aa130e36b",
            "filename": "tensorflow/core/common_runtime/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/46151f082e5d165c998639a1187d9eb04447e900/tensorflow%2Fcore%2Fcommon_runtime%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/46151f082e5d165c998639a1187d9eb04447e900/tensorflow%2Fcore%2Fcommon_runtime%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fgpu%2FBUILD?ref=46151f082e5d165c998639a1187d9eb04447e900",
            "patch": "@@ -205,6 +205,7 @@ tf_cuda_library(\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/synchronization\",\n         \"@local_xla//xla:shape_util\",\n+        \"@local_xla//xla/pjrt:host_memory_allocator\",\n         \"@local_xla//xla/stream_executor/gpu:gpu_init_impl\",\n         \"@local_xla//xla/stream_executor/integrations:stream_executor_allocator\",\n         \"@local_xla//xla/tsl/framework:device_id_utils\","
        },
        {
            "sha": "f40fd04472700cac40d31e89cd073a72854f6e71",
            "filename": "tensorflow/core/common_runtime/gpu/gpu_device.cc",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/46151f082e5d165c998639a1187d9eb04447e900/tensorflow%2Fcore%2Fcommon_runtime%2Fgpu%2Fgpu_device.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/46151f082e5d165c998639a1187d9eb04447e900/tensorflow%2Fcore%2Fcommon_runtime%2Fgpu%2Fgpu_device.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Fcommon_runtime%2Fgpu%2Fgpu_device.cc?ref=46151f082e5d165c998639a1187d9eb04447e900",
            "patch": "@@ -15,6 +15,7 @@ limitations under the License.\n \n // TODO(opensource): Use a more generic sounding preprocessor name than\n // GOOGLE_CUDA\n+#include \"xla/pjrt/host_memory_allocator.h\"\n #if (defined(GOOGLE_CUDA) && GOOGLE_CUDA) || \\\n     (defined(TENSORFLOW_USE_ROCM) && TENSORFLOW_USE_ROCM)\n \n@@ -1880,8 +1881,10 @@ Status BaseGPUDeviceFactory::CreateDevices(\n     // TODO(chuanhao): Use the correct NUMA_NODE.\n     const int64_t numa_node = 0;\n \n-    std::unique_ptr<tsl::Allocator> pjrt_gpu_host_allocator(\n-        process_state->GetGpuHostAllocator(/*options=*/{}, numa_node));\n+    auto pjrt_gpu_host_allocator =\n+        std::make_unique<xla::BasicHostMemoryAllocator>(\n+            std::unique_ptr<tsl::Allocator>(\n+                process_state->GetGpuHostAllocator(/*options=*/{}, numa_node)));\n \n     if (populate_pjrt_gpu_client_creation_info &&\n         !should_create_new_pjrt_client) {"
        },
        {
            "sha": "571caba934dfe74452e6c7afdcd7a2c9f5fdd3b0",
            "filename": "tensorflow/core/tfrt/common/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/46151f082e5d165c998639a1187d9eb04447e900/tensorflow%2Fcore%2Ftfrt%2Fcommon%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/46151f082e5d165c998639a1187d9eb04447e900/tensorflow%2Fcore%2Ftfrt%2Fcommon%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftfrt%2Fcommon%2FBUILD?ref=46151f082e5d165c998639a1187d9eb04447e900",
            "patch": "@@ -106,6 +106,7 @@ cc_library(\n         \"@com_google_absl//absl/synchronization\",\n         \"@local_tsl//tsl/platform:statusor\",\n         \"@local_xla//xla/client:local_client\",\n+        \"@local_xla//xla/pjrt:host_memory_allocator\",\n         \"@local_xla//xla/pjrt:local_device_state\",\n         \"@local_xla//xla/pjrt:pjrt_client\",\n         \"@local_xla//xla/pjrt:tf_pjrt_client\","
        },
        {
            "sha": "3da5fb930a9e1b5a126076f25c27c1dc15a3a539",
            "filename": "tensorflow/core/tfrt/common/pjrt_state.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/46151f082e5d165c998639a1187d9eb04447e900/tensorflow%2Fcore%2Ftfrt%2Fcommon%2Fpjrt_state.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/46151f082e5d165c998639a1187d9eb04447e900/tensorflow%2Fcore%2Ftfrt%2Fcommon%2Fpjrt_state.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcore%2Ftfrt%2Fcommon%2Fpjrt_state.h?ref=46151f082e5d165c998639a1187d9eb04447e900",
            "patch": "@@ -25,6 +25,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/synchronization/mutex.h\"\n #include \"xla/client/local_client.h\"\n+#include \"xla/pjrt/host_memory_allocator.h\"\n #include \"xla/pjrt/local_device_state.h\"\n #include \"xla/pjrt/pjrt_client.h\"\n #include \"xla/stream_executor/integrations/tf_allocator_adapter.h\"\n@@ -44,7 +45,7 @@ using PjRtClientsMap = std::map<DeviceType, std::unique_ptr<xla::PjRtClient>>;\n struct PjRtGpuClientCreationInfo {\n   std::set<int> allowed_devices;\n   std::unique_ptr<se::MultiDeviceAdapter> allocator;\n-  std::unique_ptr<tsl::Allocator> host_memory_allocator;\n+  std::unique_ptr<xla::HostMemoryAllocator> host_memory_allocator;\n   std::map<int, std::unique_ptr<xla::LocalDeviceState>> local_device_states;\n   xla::LocalClient* local_client;\n };"
        },
        {
            "sha": "1821bf0aff705729b52564dd5ff3953bb35018fd",
            "filename": "third_party/xla/xla/pjrt/BUILD",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/46151f082e5d165c998639a1187d9eb04447e900/third_party%2Fxla%2Fxla%2Fpjrt%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/46151f082e5d165c998639a1187d9eb04447e900/third_party%2Fxla%2Fxla%2Fpjrt%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2FBUILD?ref=46151f082e5d165c998639a1187d9eb04447e900",
            "patch": "@@ -671,6 +671,7 @@ cc_library(\n         \":device_event\",\n         \":event_pool\",\n         \":host_callback\",\n+        \":host_memory_allocator\",\n         \":host_memory_spaces\",\n         \":layout_mode\",\n         \":local_device_state\",\n@@ -1400,6 +1401,17 @@ cc_library(\n     ],\n )\n \n+cc_library(\n+    name = \"host_memory_allocator\",\n+    srcs = [\"host_memory_allocator.cc\"],\n+    hdrs = [\"host_memory_allocator.h\"],\n+    deps = [\n+        \"//xla/tsl/framework:allocator\",\n+        \"@com_google_absl//absl/functional:any_invocable\",\n+        \"@com_google_absl//absl/status\",\n+    ],\n+)\n+\n xla_cc_test(\n     name = \"errors_test\",\n     srcs = [\"errors_test.cc\"],"
        },
        {
            "sha": "be2ae8bb7e80e1560b6397365fc5bc960b4201bc",
            "filename": "third_party/xla/xla/pjrt/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/46151f082e5d165c998639a1187d9eb04447e900/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/46151f082e5d165c998639a1187d9eb04447e900/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2FBUILD?ref=46151f082e5d165c998639a1187d9eb04447e900",
            "patch": "@@ -85,6 +85,7 @@ cc_library(\n         \"//xla/pjrt:common_pjrt_client\",\n         \"//xla/pjrt:device_event\",\n         \"//xla/pjrt:event_pool\",\n+        \"//xla/pjrt:host_memory_allocator\",\n         \"//xla/pjrt:host_memory_spaces\",\n         \"//xla/pjrt:local_device_state\",\n         \"//xla/pjrt:mlir_to_hlo\","
        },
        {
            "sha": "b12dcf160cfdbc19aa7a3069e377b857c38af51b",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client.cc",
            "status": "modified",
            "additions": 33,
            "deletions": 4,
            "changes": 37,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/46151f082e5d165c998639a1187d9eb04447e900/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/46151f082e5d165c998639a1187d9eb04447e900/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.cc?ref=46151f082e5d165c998639a1187d9eb04447e900",
            "patch": "@@ -76,6 +76,7 @@ limitations under the License.\n #include \"xla/pjrt/gpu/gpu_topology.h\"\n #include \"xla/pjrt/gpu/gpu_topology.pb.h\"\n #include \"xla/pjrt/gpu/se_gpu_topology_description.h\"\n+#include \"xla/pjrt/host_memory_allocator.h\"\n #include \"xla/pjrt/host_memory_spaces.h\"\n #include \"xla/pjrt/host_to_device_transfer_manager.h\"\n #include \"xla/pjrt/local_device_state.h\"\n@@ -206,7 +207,7 @@ StreamExecutorGpuClient::StreamExecutorGpuClient(\n     std::string platform_name, LocalClient* client,\n     std::vector<std::unique_ptr<PjRtStreamExecutorDevice>> devices,\n     int process_index, std::unique_ptr<se::DeviceAddressAllocator> allocator,\n-    std::unique_ptr<tsl::Allocator> host_memory_allocator,\n+    std::unique_ptr<HostMemoryAllocator> host_memory_allocator,\n     bool should_stage_host_to_device_transfers,\n     std::unique_ptr<gpu::GpuExecutableRunOptions> gpu_run_options,\n     std::shared_ptr<KeyValueStoreInterface> kv_store,\n@@ -1790,9 +1791,37 @@ absl::StatusOr<std::unique_ptr<PjRtClient>> GetStreamExecutorGpuClient(\n                       GetStreamExecutorGpuDeviceAllocator(\n                           xla_client->platform(), options.allocator_config,\n                           local_device_states));\n-  TF_ASSIGN_OR_RETURN(\n-      auto host_memory_allocator,\n-      GetGpuHostAllocator(local_device_states.begin()->second->executor()));\n+  std::unique_ptr<HostMemoryAllocator> host_memory_allocator;\n+  if (options.host_memory_allocator_factory != nullptr) {\n+    stream_executor::StreamExecutor* const stream_executor =\n+        local_device_states.begin()->second->compute_stream()->parent();\n+    HostMemoryAllocator::Options allocator_options;\n+    allocator_options.alignment = tsl::Allocator::kAllocatorAlignment;\n+    allocator_options.map_fn = [stream_executor](void* data, size_t size) {\n+      bool success = stream_executor->HostMemoryRegister(data, size);\n+      if (!success) {\n+        return absl::InternalError(absl::StrFormat(\n+            \"Failed to register host memory at address: %ps\", data));\n+      }\n+      return absl::OkStatus();\n+    };\n+    allocator_options.unmap_fn = [stream_executor](void* data) {\n+      bool success = stream_executor->HostMemoryUnregister(data);\n+      if (!success) {\n+        return absl::InternalError(absl::StrFormat(\n+            \"Failed to unregister host memory at address: %ps\", data));\n+      }\n+      return absl::OkStatus();\n+    };\n+    host_memory_allocator =\n+        options.host_memory_allocator_factory(allocator_options);\n+  } else {\n+    TF_ASSIGN_OR_RETURN(\n+        auto allocator,\n+        GetGpuHostAllocator(local_device_states.begin()->second->executor()));\n+    host_memory_allocator = std::make_unique<BasicHostMemoryAllocator>(\n+        std::move(allocator), tsl::Allocator::kAllocatorAlignment);\n+  }\n \n   auto gpu_run_options = std::make_unique<gpu::GpuExecutableRunOptions>();\n   if (options.enable_mock_nccl) {"
        },
        {
            "sha": "b65f9a7f4af02a2cfb6abde9830097e81830d4e8",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_client.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/46151f082e5d165c998639a1187d9eb04447e900/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/46151f082e5d165c998639a1187d9eb04447e900/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_client.h?ref=46151f082e5d165c998639a1187d9eb04447e900",
            "patch": "@@ -45,6 +45,7 @@ limitations under the License.\n #include \"xla/pjrt/gpu/gpu_topology.h\"\n #include \"xla/pjrt/gpu/gpu_topology.pb.h\"\n #include \"xla/pjrt/gpu/se_gpu_topology_description.h\"\n+#include \"xla/pjrt/host_memory_allocator.h\"\n #include \"xla/pjrt/local_device_state.h\"\n #include \"xla/pjrt/pjrt_client.h\"\n #include \"xla/pjrt/pjrt_compiler.h\"\n@@ -110,7 +111,7 @@ class StreamExecutorGpuClient : public xla::PjRtStreamExecutorClient {\n       std::string platform_name, LocalClient* client,\n       std::vector<std::unique_ptr<PjRtStreamExecutorDevice>> devices,\n       int process_index, std::unique_ptr<se::DeviceAddressAllocator> allocator,\n-      std::unique_ptr<tsl::Allocator> host_memory_allocator,\n+      std::unique_ptr<HostMemoryAllocator> host_memory_allocator,\n       bool should_stage_host_to_device_transfers,\n       std::unique_ptr<gpu::GpuExecutableRunOptions> gpu_run_options,\n       std::shared_ptr<KeyValueStoreInterface> kv_store,"
        },
        {
            "sha": "04d720161c86b57739c30b4719de118e0c8725d5",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 9,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/46151f082e5d165c998639a1187d9eb04447e900/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/46151f082e5d165c998639a1187d9eb04447e900/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2FBUILD?ref=46151f082e5d165c998639a1187d9eb04447e900",
            "patch": "@@ -37,7 +37,6 @@ cc_library(\n     visibility = internal_visibility([\"//xla/pjrt/gpu:legacy_gpu_client_users\"]),\n     deps = [\n         \":gpu_event\",\n-        \":host_memory_allocator\",\n         \":tracked_gpu_device_buffer\",\n         \"//xla:debug_options_flags\",\n         \"//xla:executable_run_options\",\n@@ -64,6 +63,7 @@ cc_library(\n         \"//xla/pjrt:abstract_tracked_device_buffer\",\n         \"//xla/pjrt:device_event\",\n         \"//xla/pjrt:host_callback\",\n+        \"//xla/pjrt:host_memory_allocator\",\n         \"//xla/pjrt:host_memory_spaces\",\n         \"//xla/pjrt:layout_mode\",\n         \"//xla/pjrt:mlir_to_hlo\",\n@@ -356,11 +356,3 @@ xla_cc_test(\n         \"@local_tsl//tsl/platform:casts\",\n     ],\n )\n-\n-cc_library(\n-    name = \"host_memory_allocator\",\n-    hdrs = [\"host_memory_allocator.h\"],\n-    deps = [\n-        \"//xla/tsl/framework:allocator\",\n-    ],\n-)"
        },
        {
            "sha": "cef01b496e48ee399c5e52411d23e0360c70ac20",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/host_memory_allocator.h",
            "status": "removed",
            "additions": 0,
            "deletions": 46,
            "changes": 46,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/f3ec010de9577c6f6709d96e7a3270b3f10568eb/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Fhost_memory_allocator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/f3ec010de9577c6f6709d96e7a3270b3f10568eb/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Fhost_memory_allocator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Fhost_memory_allocator.h?ref=f3ec010de9577c6f6709d96e7a3270b3f10568eb",
            "patch": "@@ -1,46 +0,0 @@\n-/* Copyright 2025 The OpenXLA Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#ifndef XLA_PJRT_GPU_TFRT_HOST_MEMORY_ALLOCATOR_H_\n-#define XLA_PJRT_GPU_TFRT_HOST_MEMORY_ALLOCATOR_H_\n-\n-#include <cstddef>\n-#include <functional>\n-#include <memory>\n-#include <utility>\n-\n-#include \"xla/tsl/framework/allocator.h\"\n-\n-namespace xla {\n-class HostMemoryAllocator {\n- public:\n-  explicit HostMemoryAllocator(std::unique_ptr<tsl::Allocator> allocator)\n-      : allocator_(std::move(allocator)) {}\n-\n-  // Uses tsl::Allocator destructor as the deleter for owned pointer.\n-  using OwnedPtr = std::unique_ptr<void, std::function<void(void*)>>;\n-  OwnedPtr Allocate(size_t size) {\n-    if (size == 0) return OwnedPtr(nullptr, [](void* ptr) {});\n-    return OwnedPtr(\n-        allocator_->AllocateRaw(tsl::Allocator::kAllocatorAlignment, size),\n-        [this](void* ptr) { allocator_->DeallocateRaw(ptr); });\n-  }\n-\n- private:\n-  std::unique_ptr<tsl::Allocator> allocator_;\n-};\n-}  // namespace xla\n-\n-#endif  // XLA_PJRT_GPU_TFRT_HOST_MEMORY_ALLOCATOR_H_"
        },
        {
            "sha": "3f987409f0b21f4286028575127374613f2c763a",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tfrt_gpu_async_host_to_device_transfer_manager.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/46151f082e5d165c998639a1187d9eb04447e900/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_async_host_to_device_transfer_manager.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/46151f082e5d165c998639a1187d9eb04447e900/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_async_host_to_device_transfer_manager.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_async_host_to_device_transfer_manager.cc?ref=46151f082e5d165c998639a1187d9eb04447e900",
            "patch": "@@ -40,11 +40,11 @@ limitations under the License.\n #include \"xla/pjrt/distributed/protocol.pb.h\"\n #include \"xla/pjrt/gpu/gpu_topology.pb.h\"\n #include \"xla/pjrt/gpu/tfrt/gpu_event.h\"\n-#include \"xla/pjrt/gpu/tfrt/host_memory_allocator.h\"\n #include \"xla/pjrt/gpu/tfrt/tfrt_gpu_client.h\"\n #include \"xla/pjrt/gpu/tfrt/tfrt_gpu_device.h\"\n #include \"xla/pjrt/gpu/tfrt/tracked_gpu_device_buffer.h\"\n #include \"xla/pjrt/gpu/tfrt/utils.h\"\n+#include \"xla/pjrt/host_memory_allocator.h\"\n #include \"xla/pjrt/pjrt_client.h\"\n #include \"xla/pjrt/pjrt_compiler.h\"\n #include \"xla/pjrt/pjrt_executable.h\""
        },
        {
            "sha": "1aceca82c59b844a392fd9c1b582f22332dec8fb",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tfrt_gpu_buffer.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/46151f082e5d165c998639a1187d9eb04447e900/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_buffer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/46151f082e5d165c998639a1187d9eb04447e900/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_buffer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_buffer.cc?ref=46151f082e5d165c998639a1187d9eb04447e900",
            "patch": "@@ -41,10 +41,10 @@ limitations under the License.\n #include \"xla/pjrt/distributed/protocol.pb.h\"\n #include \"xla/pjrt/gpu/gpu_topology.pb.h\"\n #include \"xla/pjrt/gpu/tfrt/gpu_event.h\"\n-#include \"xla/pjrt/gpu/tfrt/host_memory_allocator.h\"\n #include \"xla/pjrt/gpu/tfrt/tfrt_gpu_client.h\"\n #include \"xla/pjrt/gpu/tfrt/tracked_gpu_device_buffer.h\"\n #include \"xla/pjrt/gpu/tfrt/utils.h\"\n+#include \"xla/pjrt/host_memory_allocator.h\"\n #include \"xla/pjrt/host_memory_spaces.h\"\n #include \"xla/pjrt/pjrt_client.h\"\n #include \"xla/pjrt/pjrt_compiler.h\""
        },
        {
            "sha": "d36b6acc5510ed14866cc517e6ffcdea6d8d826d",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tfrt_gpu_client.cc",
            "status": "modified",
            "additions": 31,
            "deletions": 7,
            "changes": 38,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/46151f082e5d165c998639a1187d9eb04447e900/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/46151f082e5d165c998639a1187d9eb04447e900/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client.cc?ref=46151f082e5d165c998639a1187d9eb04447e900",
            "patch": "@@ -63,12 +63,12 @@ limitations under the License.\n #include \"xla/pjrt/gpu/gpu_topology.h\"\n #include \"xla/pjrt/gpu/gpu_topology.pb.h\"\n #include \"xla/pjrt/gpu/tfrt/gpu_event.h\"\n-#include \"xla/pjrt/gpu/tfrt/host_memory_allocator.h\"\n #include \"xla/pjrt/gpu/tfrt/tfrt_gpu_async_host_to_device_transfer_manager.h\"\n #include \"xla/pjrt/gpu/tfrt/tfrt_gpu_device.h\"\n #include \"xla/pjrt/gpu/tfrt/tfrt_gpu_executable.h\"\n #include \"xla/pjrt/gpu/tfrt/tracked_gpu_device_buffer.h\"\n #include \"xla/pjrt/gpu/tfrt/utils.h\"\n+#include \"xla/pjrt/host_memory_allocator.h\"\n #include \"xla/pjrt/host_memory_spaces.h\"\n #include \"xla/pjrt/layout_mode.h\"\n #include \"xla/pjrt/mlir_to_hlo.h\"\n@@ -149,7 +149,7 @@ TfrtGpuClient::TfrtGpuClient(\n     bool should_stage_host_to_device_transfers,\n     bool abort_collectives_on_failure,\n     MaybeOwning<se::DeviceAddressAllocator> allocator,\n-    std::unique_ptr<tsl::Allocator> host_memory_allocator,\n+    std::shared_ptr<HostMemoryAllocator> host_memory_allocator,\n     std::unique_ptr<gpu::GpuExecutableRunOptions> gpu_run_options,\n     std::shared_ptr<KeyValueStoreInterface> kv_store,\n     std::shared_ptr<const GpuTopology> gpu_topology)\n@@ -160,8 +160,7 @@ TfrtGpuClient::TfrtGpuClient(\n           should_stage_host_to_device_transfers),\n       abort_collectives_on_failure_(abort_collectives_on_failure),\n       allocator_(std::move(allocator)),\n-      host_memory_allocator_(std::make_unique<HostMemoryAllocator>(\n-          std::move(host_memory_allocator))),\n+      host_memory_allocator_(std::move(host_memory_allocator)),\n       devices_(InitializeDevices(this, devices)),\n       id_to_device_(GetIdToDeviceMap(devices)),\n       addressable_devices_(GetAddressableDevicePointers(devices)),\n@@ -1189,11 +1188,36 @@ absl::StatusOr<std::unique_ptr<PjRtClient>> GetTfrtGpuClient(\n       GetGpuXlaClient(options.platform_name, options.allowed_devices));\n   EnablePeerAccess(xla_client->backend().stream_executors());\n \n-  std::unique_ptr<tsl::Allocator> host_memory_allocator;\n-  if (!xla_client->backend().stream_executors().empty()) {\n+  std::shared_ptr<HostMemoryAllocator> host_memory_allocator;\n+  if (options.host_memory_allocator_factory != nullptr) {\n+    stream_executor::StreamExecutor* const stream_executor =\n+        xla_client->backend().stream_executors().front();\n+    HostMemoryAllocator::Options allocator_options;\n+    allocator_options.alignment = tsl::Allocator::kAllocatorAlignment;\n+    allocator_options.map_fn = [stream_executor](void* data, size_t size) {\n+      bool success = stream_executor->HostMemoryRegister(data, size);\n+      if (!success) {\n+        return absl::InternalError(absl::StrFormat(\n+            \"Failed to register host memory at address: %ps\", data));\n+      }\n+      return absl::OkStatus();\n+    };\n+    allocator_options.unmap_fn = [stream_executor](void* data) {\n+      bool success = stream_executor->HostMemoryUnregister(data);\n+      if (!success) {\n+        return absl::InternalError(absl::StrFormat(\n+            \"Failed to unregister host memory at address: %ps\", data));\n+      }\n+      return absl::OkStatus();\n+    };\n+    host_memory_allocator =\n+        options.host_memory_allocator_factory(allocator_options);\n+  } else if (!xla_client->backend().stream_executors().empty()) {\n     TF_ASSIGN_OR_RETURN(\n-        host_memory_allocator,\n+        std::unique_ptr<tsl::Allocator> allocator,\n         GetGpuHostAllocator(xla_client->backend().stream_executors().front()));\n+    host_memory_allocator = std::make_shared<BasicHostMemoryAllocator>(\n+        std::move(allocator), tsl::Allocator::kAllocatorAlignment);\n   }\n \n   auto gpu_run_options = std::make_unique<gpu::GpuExecutableRunOptions>();"
        },
        {
            "sha": "f08c7d9076c9d808ef3814dd5166234ef51c40b4",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tfrt_gpu_client.h",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/46151f082e5d165c998639a1187d9eb04447e900/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/46151f082e5d165c998639a1187d9eb04447e900/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client.h?ref=46151f082e5d165c998639a1187d9eb04447e900",
            "patch": "@@ -48,10 +48,10 @@ limitations under the License.\n #include \"xla/pjrt/distributed/key_value_store_interface.h\"\n #include \"xla/pjrt/gpu/gpu_topology.h\"\n #include \"xla/pjrt/gpu/se_gpu_topology_description.h\"\n-#include \"xla/pjrt/gpu/tfrt/host_memory_allocator.h\"\n #include \"xla/pjrt/gpu/tfrt/tfrt_gpu_buffer.h\"\n #include \"xla/pjrt/gpu/tfrt/tfrt_gpu_device.h\"\n #include \"xla/pjrt/gpu/tfrt/tracked_gpu_device_buffer.h\"\n+#include \"xla/pjrt/host_memory_allocator.h\"\n #include \"xla/pjrt/pjrt_client.h\"\n #include \"xla/pjrt/pjrt_common.h\"\n #include \"xla/pjrt/pjrt_compiler.h\"\n@@ -120,7 +120,7 @@ class TfrtGpuClient final : public PjRtClient {\n                 bool should_stage_host_to_device_transfers,\n                 bool abort_collectives_on_failure,\n                 MaybeOwning<se::DeviceAddressAllocator> allocator,\n-                std::unique_ptr<tsl::Allocator> host_memory_allocator,\n+                std::shared_ptr<HostMemoryAllocator> host_memory_allocator,\n                 std::unique_ptr<gpu::GpuExecutableRunOptions> gpu_run_options,\n                 std::shared_ptr<KeyValueStoreInterface> kv_store,\n                 std::shared_ptr<const GpuTopology> gpu_topology);\n@@ -339,7 +339,7 @@ class TfrtGpuClient final : public PjRtClient {\n   // complete.\n   MaybeOwning<se::DeviceAddressAllocator> allocator_;\n   // Allocator to be used for staging memory transfers to devices.\n-  std::unique_ptr<HostMemoryAllocator> host_memory_allocator_;\n+  std::shared_ptr<HostMemoryAllocator> host_memory_allocator_;\n \n   // Pointers to `owned_devices_`.\n   std::vector<PjRtDevice*> devices_;"
        },
        {
            "sha": "a0a9448a2527adbe0af5720baf7094eced61831e",
            "filename": "third_party/xla/xla/pjrt/host_memory_allocator.cc",
            "status": "added",
            "additions": 45,
            "deletions": 0,
            "changes": 45,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/46151f082e5d165c998639a1187d9eb04447e900/third_party%2Fxla%2Fxla%2Fpjrt%2Fhost_memory_allocator.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/46151f082e5d165c998639a1187d9eb04447e900/third_party%2Fxla%2Fxla%2Fpjrt%2Fhost_memory_allocator.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fhost_memory_allocator.cc?ref=46151f082e5d165c998639a1187d9eb04447e900",
            "patch": "@@ -0,0 +1,45 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"xla/pjrt/host_memory_allocator.h\"\n+\n+#include <cstddef>\n+#include <cstdint>\n+#include <memory>\n+#include <utility>\n+\n+#include \"xla/tsl/framework/allocator.h\"\n+\n+namespace xla {\n+\n+BasicHostMemoryAllocator::BasicHostMemoryAllocator(\n+    std::unique_ptr<tsl::Allocator> allocator, size_t alignment)\n+    : allocator_(std::move(allocator)), alignment_(alignment) {}\n+\n+HostMemoryAllocator::OwnedPtr BasicHostMemoryAllocator::Allocate(size_t size) {\n+  if (size == 0) {\n+    return nullptr;\n+  }\n+  return OwnedPtr(\n+      reinterpret_cast<uint8_t*>(allocator_->AllocateRaw(alignment_, size)),\n+      {\n+          +[](void* ptr, void* arg) {\n+            reinterpret_cast<tsl::Allocator*>(arg)->DeallocateRaw(ptr);\n+          },\n+          allocator_.get(),\n+      });\n+}\n+\n+}  // namespace xla"
        },
        {
            "sha": "8123ade19c7887a43b2806b483c245fef9567118",
            "filename": "third_party/xla/xla/pjrt/host_memory_allocator.h",
            "status": "added",
            "additions": 76,
            "deletions": 0,
            "changes": 76,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/46151f082e5d165c998639a1187d9eb04447e900/third_party%2Fxla%2Fxla%2Fpjrt%2Fhost_memory_allocator.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/46151f082e5d165c998639a1187d9eb04447e900/third_party%2Fxla%2Fxla%2Fpjrt%2Fhost_memory_allocator.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fhost_memory_allocator.h?ref=46151f082e5d165c998639a1187d9eb04447e900",
            "patch": "@@ -0,0 +1,76 @@\n+/* Copyright 2025 The OpenXLA Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef XLA_PJRT_HOST_MEMORY_ALLOCATOR_H_\n+#define XLA_PJRT_HOST_MEMORY_ALLOCATOR_H_\n+\n+#include <cstddef>\n+#include <cstdint>\n+#include <functional>\n+#include <memory>\n+\n+#include \"absl/functional/any_invocable.h\"\n+#include \"absl/status/status.h\"\n+#include \"xla/tsl/framework/allocator.h\"\n+\n+namespace xla {\n+\n+// An interface for host memory allocation.\n+class HostMemoryAllocator {\n+ public:\n+  struct Options {\n+    // Minimum alignment of the allocated memory.\n+    size_t alignment = tsl::Allocator::kAllocatorAlignment;\n+\n+    // Functions for mapping and unmapping the allocated memory.\n+    absl::AnyInvocable<absl::Status(void*, size_t)> map_fn;\n+    absl::AnyInvocable<absl::Status(void*)> unmap_fn;\n+  };\n+\n+  using Factory = std::function<std::unique_ptr<HostMemoryAllocator>(\n+      const Options& options)>;\n+\n+  struct Deleter {\n+    void operator()(void* ptr) { deleter(ptr, arg); }\n+    void (*deleter)(void* ptr, void* arg);\n+    void* arg;\n+  };\n+  using OwnedPtr = std::unique_ptr<uint8_t[], Deleter>;\n+\n+  virtual ~HostMemoryAllocator() = default;\n+\n+  // Allocates `size` bytes of memory. The returned pointer is guaranteed to be\n+  // aligned to `options_.alignment`.\n+  virtual OwnedPtr Allocate(size_t size) = 0;\n+};\n+\n+// `HostMemoryAllocator` implementation that uses a `tsl::Allocator` to back\n+// allocations.\n+class BasicHostMemoryAllocator : public HostMemoryAllocator {\n+ public:\n+  explicit BasicHostMemoryAllocator(\n+      std::unique_ptr<tsl::Allocator> allocator,\n+      size_t alignment = tsl::Allocator::kAllocatorAlignment);\n+\n+  OwnedPtr Allocate(size_t size) override;\n+\n+ private:\n+  const std::unique_ptr<tsl::Allocator> allocator_;\n+  const size_t alignment_;\n+};\n+\n+}  // namespace xla\n+\n+#endif  // XLA_PJRT_HOST_MEMORY_ALLOCATOR_H_"
        },
        {
            "sha": "391ed56fc386c08c48ee1714202bc9cd27a04a73",
            "filename": "third_party/xla/xla/pjrt/pjrt_stream_executor_client.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/46151f082e5d165c998639a1187d9eb04447e900/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/46151f082e5d165c998639a1187d9eb04447e900/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.cc?ref=46151f082e5d165c998639a1187d9eb04447e900",
            "patch": "@@ -115,6 +115,7 @@ limitations under the License.\n #include \"xla/pjrt/dump/dump.h\"\n #include \"xla/pjrt/event_pool.h\"\n #include \"xla/pjrt/host_callback.h\"\n+#include \"xla/pjrt/host_memory_allocator.h\"\n #include \"xla/pjrt/host_memory_spaces.h\"\n #include \"xla/pjrt/layout_mode.h\"\n #include \"xla/pjrt/local_device_state.h\"\n@@ -276,7 +277,7 @@ PjRtStreamExecutorClient::PjRtStreamExecutorClient(\n     int process_index,\n     std::vector<std::unique_ptr<PjRtMemorySpace>> memory_spaces,\n     std::unique_ptr<se::DeviceAddressAllocator> allocator,\n-    std::unique_ptr<tsl::Allocator> host_memory_allocator,\n+    std::unique_ptr<HostMemoryAllocator> host_memory_allocator,\n     bool should_stage_host_to_device_transfers,\n     std::unique_ptr<gpu::GpuExecutableRunOptions> gpu_run_options)\n     : platform_id_(tsl::Fingerprint64(platform_name)),\n@@ -302,7 +303,8 @@ PjRtStreamExecutorClient::PjRtStreamExecutorClient(\n   }\n \n   if (!host_memory_allocator_) {\n-    host_memory_allocator_ = std::make_unique<CpuAllocator>();\n+    host_memory_allocator_ = std::make_unique<BasicHostMemoryAllocator>(\n+        std::make_unique<CpuAllocator>());\n   }\n \n   for (const std::unique_ptr<PjRtStreamExecutorDevice>& device :\n@@ -669,12 +671,8 @@ PjRtStreamExecutorClient::LinearizeHostBufferInto(\n   if (must_use_staging_buffer || (!IsDmaMapped(data, packed_size) &&\n                                   (should_stage_host_to_device_transfers() &&\n                                    packed_size < (int64_t{1} << 30)))) {\n-    void* ptr = host_memory_allocator()->AllocateRaw(\n-        tsl::Allocator::kAllocatorAlignment, transpose ? size : packed_size);\n-    staging_buffer = std::shared_ptr<void>(\n-        ptr, [host_memory_allocator = host_memory_allocator()](void* ptr) {\n-          host_memory_allocator->DeallocateRaw(ptr);\n-        });\n+    staging_buffer =\n+        host_memory_allocator()->Allocate(transpose ? size : packed_size);\n   }\n \n   // Copy the buffer into a staging buffer before returning control to the"
        },
        {
            "sha": "c00be14ba842958df7cfc9434ddb54b1098b6ea1",
            "filename": "third_party/xla/xla/pjrt/pjrt_stream_executor_client.h",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/46151f082e5d165c998639a1187d9eb04447e900/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/46151f082e5d165c998639a1187d9eb04447e900/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.h?ref=46151f082e5d165c998639a1187d9eb04447e900",
            "patch": "@@ -49,6 +49,7 @@ limitations under the License.\n #include \"xla/literal.h\"\n #include \"xla/pjrt/abstract_tracked_device_buffer.h\"\n #include \"xla/pjrt/common_pjrt_client.h\"\n+#include \"xla/pjrt/host_memory_allocator.h\"\n #include \"xla/pjrt/local_device_state.h\"\n #include \"xla/pjrt/pjrt_client.h\"\n #include \"xla/pjrt/pjrt_common.h\"\n@@ -238,7 +239,7 @@ class PjRtStreamExecutorClient : public CommonPjRtClient {\n       int process_index,\n       std::vector<std::unique_ptr<PjRtMemorySpace>> memory_spaces,\n       std::unique_ptr<se::DeviceAddressAllocator> allocator,\n-      std::unique_ptr<tsl::Allocator> host_memory_allocator,\n+      std::unique_ptr<HostMemoryAllocator> host_memory_allocator,\n       bool should_stage_host_to_device_transfers,\n       std::unique_ptr<gpu::GpuExecutableRunOptions> gpu_run_options);\n   ~PjRtStreamExecutorClient() override = default;\n@@ -341,7 +342,7 @@ class PjRtStreamExecutorClient : public CommonPjRtClient {\n   }\n   LocalClient* client() const { return client_; }\n   se::DeviceAddressAllocator* allocator() const { return allocator_; }\n-  tsl::Allocator* host_memory_allocator() const {\n+  HostMemoryAllocator* host_memory_allocator() const {\n     return host_memory_allocator_.get();\n   }\n   bool should_stage_host_to_device_transfers() const {\n@@ -483,7 +484,7 @@ class PjRtStreamExecutorClient : public CommonPjRtClient {\n   LocalClient* client_;\n \n   // Allocator to be used for staging memory transfers to devices.\n-  std::unique_ptr<tsl::Allocator> host_memory_allocator_;\n+  std::unique_ptr<HostMemoryAllocator> host_memory_allocator_;\n \n   // Device memory allocator. If owned, the allocator must outlive the devices,\n   // because it is the device destructor that waits for any outstanding work to"
        },
        {
            "sha": "84fa6ae199757745437f02aa391fd9424754f99e",
            "filename": "third_party/xla/xla/pjrt/plugin/xla_gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/46151f082e5d165c998639a1187d9eb04447e900/third_party%2Fxla%2Fxla%2Fpjrt%2Fplugin%2Fxla_gpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/46151f082e5d165c998639a1187d9eb04447e900/third_party%2Fxla%2Fxla%2Fpjrt%2Fplugin%2Fxla_gpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fplugin%2Fxla_gpu%2FBUILD?ref=46151f082e5d165c998639a1187d9eb04447e900",
            "patch": "@@ -29,6 +29,7 @@ cc_library(\n     hdrs = [\"xla_gpu_client_options.h\"],\n     deps = [\n         \":xla_gpu_allocator_config\",\n+        \"//xla/pjrt:host_memory_allocator\",\n         \"//xla/pjrt/distributed:key_value_store_interface\",\n     ],\n )"
        },
        {
            "sha": "771506c9fecf2cc4b663acb5fc7b3858154b4323",
            "filename": "third_party/xla/xla/pjrt/plugin/xla_gpu/xla_gpu_client_options.h",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/46151f082e5d165c998639a1187d9eb04447e900/third_party%2Fxla%2Fxla%2Fpjrt%2Fplugin%2Fxla_gpu%2Fxla_gpu_client_options.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/46151f082e5d165c998639a1187d9eb04447e900/third_party%2Fxla%2Fxla%2Fpjrt%2Fplugin%2Fxla_gpu%2Fxla_gpu_client_options.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fplugin%2Fxla_gpu%2Fxla_gpu_client_options.h?ref=46151f082e5d165c998639a1187d9eb04447e900",
            "patch": "@@ -22,6 +22,7 @@ limitations under the License.\n #include <string>\n \n #include \"xla/pjrt/distributed/key_value_store_interface.h\"\n+#include \"xla/pjrt/host_memory_allocator.h\"\n #include \"xla/pjrt/plugin/xla_gpu/xla_gpu_allocator_config.h\"\n \n namespace xla {\n@@ -40,6 +41,10 @@ struct GpuClientOptions {\n \n   bool should_stage_host_to_device_transfers = true;\n \n+  // Optional factory for a host memory allocator to use for transfer. Used only\n+  // if `should_stage_host_to_device_transfers` is true.\n+  HostMemoryAllocator::Factory host_memory_allocator_factory;\n+\n   // kv_store must be non-null if num_nodes > 1.\n   std::shared_ptr<KeyValueStoreInterface> kv_store = nullptr;\n "
        },
        {
            "sha": "95a5e499f8e3bc2b1611f69830e1a430ff7b9f10",
            "filename": "third_party/xla/xla/pjrt/se_raw_buffer.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 19,
            "changes": 27,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/46151f082e5d165c998639a1187d9eb04447e900/third_party%2Fxla%2Fxla%2Fpjrt%2Fse_raw_buffer.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/46151f082e5d165c998639a1187d9eb04447e900/third_party%2Fxla%2Fxla%2Fpjrt%2Fse_raw_buffer.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fse_raw_buffer.cc?ref=46151f082e5d165c998639a1187d9eb04447e900",
            "patch": "@@ -151,12 +151,8 @@ PjRtStreamExecutorRawBuffer::CopyRawHostToDeviceAndReturnEvent(\n                 \"host_memory_allocator should be initialized for \"\n                 \"staging buffer transfer.\");\n           }\n-          void* ptr = client->host_memory_allocator()->AllocateRaw(\n-              tsl::Allocator::kAllocatorAlignment, transfer_size);\n-          staging_buffer = std::shared_ptr<void>(\n-              ptr,\n-              [host_memory_allocator = client->host_memory_allocator()](\n-                  void* ptr) { host_memory_allocator->DeallocateRaw(ptr); });\n+          staging_buffer =\n+              client->host_memory_allocator()->Allocate(transfer_size);\n           auto copy_to_staging_buffer = [src, transfer_size,\n                                          staging_buffer]() mutable {\n             std::memcpy(staging_buffer.get(), src, transfer_size);\n@@ -210,12 +206,8 @@ PjRtStreamExecutorRawBuffer::CopyRawDeviceToHostAndReturnEvent(\n                 \"host_memory_allocator should be initialized for \"\n                 \"staging buffer transfer.\");\n           }\n-          void* ptr = client->host_memory_allocator()->AllocateRaw(\n-              tsl::Allocator::kAllocatorAlignment, transfer_size);\n-          std::shared_ptr<void> staging_buffer = std::shared_ptr<void>(\n-              ptr,\n-              [host_memory_allocator = client->host_memory_allocator()](\n-                  void* ptr) { host_memory_allocator->DeallocateRaw(ptr); });\n+          std::shared_ptr<void> staging_buffer =\n+              client->host_memory_allocator()->Allocate(transfer_size);\n           TF_RETURN_IF_ERROR(\n               stream->Memcpy(staging_buffer.get(), sub_buffer, transfer_size));\n           auto copy_from_staging_buffer = [dst, transfer_size,\n@@ -496,13 +488,10 @@ void PjRtStreamExecutorRawBuffer::CopyTo(\n     src_usage_event_promise->Set(*std::move(d2h_event));\n     return;\n   } else {\n-    void* ptr = client_->host_memory_allocator()->AllocateRaw(\n-        tsl::Allocator::kAllocatorAlignment, GetOnDeviceSizeInBytes());\n-    std::shared_ptr<void> staging_buffer = std::shared_ptr<void>(\n-        ptr, [host_memory_allocator = client_->host_memory_allocator()](\n-                 void* ptr) { host_memory_allocator->DeallocateRaw(ptr); });\n-    auto d2h_event =\n-        CopyRawDeviceToHostAndReturnEvent(ptr, 0, GetOnDeviceSizeInBytes());\n+    std::shared_ptr<void> staging_buffer =\n+        client_->host_memory_allocator()->Allocate(GetOnDeviceSizeInBytes());\n+    auto d2h_event = CopyRawDeviceToHostAndReturnEvent(\n+        staging_buffer.get(), 0, GetOnDeviceSizeInBytes());\n     if (!d2h_event.ok()) {\n       definition_event_promise->SetError(d2h_event.status());\n       src_usage_event_promise->SetError(d2h_event.status());"
        }
    ],
    "stats": {
        "total": 344,
        "additions": 239,
        "deletions": 105
    }
}