{
    "author": "bixia1",
    "message": "Implement the conversion between StableHLO and HLO for buffer related ops.\n\nPiperOrigin-RevId: 797121260",
    "sha": "8aa4ddaec4c85400fa1517c6c3205585a8483fb3",
    "files": [
        {
            "sha": "a95ae55e4e6c787c35c97854649cfd6e18e43e12",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/BUILD",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8aa4ddaec4c85400fa1517c6c3205585a8483fb3/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8aa4ddaec4c85400fa1517c6c3205585a8483fb3/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2FBUILD?ref=8aa4ddaec4c85400fa1517c6c3205585a8483fb3",
            "patch": "@@ -336,6 +336,8 @@ cc_library(\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/mlir/utils:type_util\",\n         \"//xla/mlir_hlo\",\n+        \"@com_google_absl//absl/algorithm:container\",\n+        \"@com_google_absl//absl/status:statusor\",\n         \"@llvm-project//llvm:Support\",\n         \"@llvm-project//mlir:IR\",\n         \"@llvm-project//mlir:SparseTensorDialect\","
        },
        {
            "sha": "566d74f06f7f099917de905b3b000e18b1781779",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/mlir_hlo_to_hlo.cc",
            "status": "modified",
            "additions": 22,
            "deletions": 0,
            "changes": 22,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8aa4ddaec4c85400fa1517c6c3205585a8483fb3/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fmlir_hlo_to_hlo.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8aa4ddaec4c85400fa1517c6c3205585a8483fb3/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fmlir_hlo_to_hlo.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fmlir_hlo_to_hlo.cc?ref=8aa4ddaec4c85400fa1517c6c3205585a8483fb3",
            "patch": "@@ -79,6 +79,7 @@ limitations under the License.\n #include \"xla/hlo/builder/xla_builder.h\"\n #include \"xla/hlo/builder/xla_computation.h\"\n #include \"xla/hlo/ir/dynamic_parameter_binding.h\"\n+#include \"xla/hlo/ir/hlo_instructions.h\"\n #include \"xla/hlo/ir/hlo_module.h\"\n #include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/hlo/ir/hlo_sharding.h\"\n@@ -2622,6 +2623,19 @@ LogicalResult ExportXlaOp(CustomCallOp op, OpLoweringContext ctx) {\n \n   auto aliasInfo = xla::ConvertOutputOperandAliasing<\n       mlir::stablehlo::OutputOperandAliasAttr>(op.getOutputOperandAliases());\n+  // Pin and Unpin are the boundary to transition into or out of a buffer\n+  // chain and their operands and results are not different types. XLA/HLO\n+  // requires alias info for Pin and Unpin custom calls, such as to support\n+  // copy insertion to add the needed copies of the Pin operand and the Unpin\n+  // result. We keep this detail within XLA/HLO and do not require StableHLO\n+  // users to add alias of different types to theses custom calls.\n+  if (absl::string_view(op.getCallTargetName()) ==\n+          xla::kUnpinCustomCallTarget ||\n+      absl::string_view(op.getCallTargetName()) == xla::kPinCustomCallTarget) {\n+    aliasInfo = {std::make_pair(\n+        xla::ShapeIndex(),\n+        std::make_pair(static_cast<int64_t>(0), xla::ShapeIndex()))};\n+  }\n   auto output_operand_aliasing = absl::MakeSpan(*aliasInfo);\n \n   auto custom_call_schedule = xla::SCHEDULE_NONE;\n@@ -4311,6 +4325,14 @@ LogicalResult ExportXlaOp(CustomCallOp op, OpLoweringContext ctx) {\n   auto aliasInfo =\n       xla::ConvertOutputOperandAliasing<mlir::mhlo::OutputOperandAliasAttr>(\n           op.getOutputOperandAliases());\n+  // XLA/HLO requires alias info for Pin and Unpin custom calls.\n+  if (absl::string_view(op.getCallTargetName()) ==\n+          xla::kUnpinCustomCallTarget ||\n+      absl::string_view(op.getCallTargetName()) == xla::kPinCustomCallTarget) {\n+    aliasInfo = {std::make_pair(\n+        xla::ShapeIndex(),\n+        std::make_pair(static_cast<int64_t>(0), xla::ShapeIndex()))};\n+  }\n   auto output_operand_aliasing = absl::MakeSpan(*aliasInfo);\n   auto custom_call_schedule =\n       xla::ConvertCustomCallSchedule(op.getCustomCallSchedule());"
        },
        {
            "sha": "ff0f0ac5fee9d4a435ad05b7faec333e21318a40",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/tests/export.mlir",
            "status": "modified",
            "additions": 104,
            "deletions": 0,
            "changes": 104,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8aa4ddaec4c85400fa1517c6c3205585a8483fb3/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fexport.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8aa4ddaec4c85400fa1517c6c3205585a8483fb3/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fexport.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fexport.mlir?ref=8aa4ddaec4c85400fa1517c6c3205585a8483fb3",
            "patch": "@@ -3179,3 +3179,107 @@ func.func @main(%arg0: tensor<192xf32>) -> tensor<1x17x17x192xf32> {\n   %0 = \"mhlo.broadcast_in_dim\"(%arg0) <{broadcast_dimensions = dense<3> : tensor<1xi64>}> {mhlo.original_value = \"{{\\22broadcast.2342\\22}}\"} : (tensor<192xf32>) -> tensor<1x17x17x192xf32>\n   return %0 : tensor<1x17x17x192xf32>\n }\n+\n+// -----\n+\n+// CHECK: HloModule\n+// CHECK: ENTRY\n+func.func @main() -> memref<2xf32> {\n+  // CHECK: custom-call(), custom_call_target=\"CreateBuffer\"\n+  %0 = \"mhlo.custom_call\"() {\n+    call_target_name = \"CreateBuffer\",\n+    api_version = 4 : i32\n+  } : () -> memref<2xf32>\n+  func.return %0 : memref<2xf32>\n+}\n+\n+// -----\n+\n+// CHECK: HloModule main\n+// CHECK: ENTRY\n+func.func @main(%arg0: tensor<2xf32>) -> memref<2xf32> {\n+  //               CHECK: custom-call({{.*}}), custom_call_target=\"Pin\",\n+  // CHECK-SAME{LITERAL}: output_to_operand_aliasing={{}: (0, {})}\n+  %0 = \"mhlo.custom_call\"(%arg0) {\n+    call_target_name = \"Pin\",\n+    api_version = 4 : i32\n+  } : (tensor<2xf32>) -> memref<2xf32>\n+  func.return %0 : memref<2xf32>\n+}\n+\n+// -----\n+\n+// CHECK: HloModule\n+// CHECK: ENTRY\n+func.func @main(%arg0: memref<2xf32>) -> tensor<2xf32> {\n+  //               CHECK: custom-call({{.*}}), custom_call_target=\"Unpin\",\n+  // CHECK-SAME{LITERAL}: output_to_operand_aliasing={{}: (0, {})}\n+  %0 = \"mhlo.custom_call\"(%arg0) {\n+    call_target_name = \"Unpin\",\n+    api_version = 4 : i32\n+  } : (memref<2xf32>) -> tensor<2xf32>\n+  func.return %0 : tensor<2xf32>\n+}\n+\n+// -----\n+\n+// CHECK: HloModule main\n+// CHECK: ENTRY\n+func.func @main(%arg0: memref<2x4xf32>) -> memref<2x4xf32> {\n+  //               CHECK: custom-call({{.*}}), custom_call_target=\"foo\",\n+  // CHECK-SAME{LITERAL}: output_to_operand_aliasing={{}: (0, {})}\n+  %0 = \"mhlo.custom_call\"(%arg0) {\n+    call_target_name = \"foo\",\n+    api_version = 4 : i32,\n+    output_operand_aliases = [\n+      #mhlo.output_operand_alias<output_tuple_indices = [],\n+        operand_index = 0,\n+        operand_tuple_indices = []>]\n+  } : (memref<2x4xf32>) -> memref<2x4xf32>\n+  func.return %0 : memref<2x4xf32>\n+}\n+\n+// -----\n+\n+// CHECK: HloModule\n+// CHECK: ENTRY\n+func.func @main(%arg0: tensor<2xf32>, %arg1: memref<2xf32>) -> tuple<tensor<2xf32>, memref<2xf32>> {\n+  // CHECK: %{{.*}} = (f32[2], b(f32[2])) tuple(%{{.*}}, %{{.*}}),\n+  %0 = \"mhlo.tuple\"(%arg0, %arg1) : (tensor<2xf32>, memref<2xf32>) -> tuple<tensor<2xf32>, memref<2xf32>>\n+  func.return %0 : tuple<tensor<2xf32>, memref<2xf32>>\n+}\n+\n+// -----\n+\n+// CHECK: HloModule main\n+// CHECK: ENTRY\n+func.func @main(%arg0: tuple<tensor<2xf32>, memref<2xf32>>) -> memref<2xf32> {\n+  // CHECK: %{{.*}} = b(f32[2]) get-tuple-element(%{{.*}})\n+  %0 = \"mhlo.get_tuple_element\"(%arg0) {\n+    index = 1 : i32\n+  } : (tuple<tensor<2xf32>, memref<2xf32>>) -> memref<2xf32>\n+  func.return %0 : memref<2xf32>\n+}\n+\n+// -----\n+\n+// CHECK: HloModule\n+// CHECK: ENTRY\n+func.func @main(%arg0: tensor<i1>, %arg1: memref<2xf32>) -> memref<2xf32> {\n+  // CHECK: %{{.*}} = (pred[], b(f32[2])) while(%{{.*}}), condition=%{{.*}}, body=%{{.*}}\n+  %0:2 = mhlo.while(%iterArg0 = %arg0, %iterArg1 = %arg1) : tensor<i1>, memref<2xf32>\n+    cond {\n+      mhlo.return %iterArg0 : tensor<i1>\n+    } do {\n+      %1 = \"mhlo.custom_call\"(%iterArg1) {\n+        call_target_name = \"foo\",\n+        api_version = 4 : i32,\n+        output_operand_aliases = [\n+          #mhlo.output_operand_alias<output_tuple_indices = [],\n+            operand_index = 0,\n+            operand_tuple_indices = []>]\n+      } : (memref<2xf32>) -> memref<2xf32>\n+      mhlo.return %iterArg0, %1 : tensor<i1>, memref<2xf32>\n+    }\n+  func.return %0#1: memref<2xf32>\n+}"
        },
        {
            "sha": "586c3483cd354e0f3f9111610d873a45aa28c29a",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/tests/export_and_check_layouts.mlir",
            "status": "modified",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8aa4ddaec4c85400fa1517c6c3205585a8483fb3/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fexport_and_check_layouts.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8aa4ddaec4c85400fa1517c6c3205585a8483fb3/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fexport_and_check_layouts.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Fexport_and_check_layouts.mlir?ref=8aa4ddaec4c85400fa1517c6c3205585a8483fb3",
            "patch": "@@ -27,3 +27,31 @@ module @jit_f {\n     return %4 : tensor<0xi1>\n   }\n }\n+\n+// -----\n+\n+// Testing custom-call ops with buffer types and layouts.\n+// CHECK: HloModule\n+// CHECK: ENTRY\n+func.func @main(%arg0: tensor<2x4xf32>) -> tensor<2x4xf32> {\n+  // CHECK: %{{.*}} = b(f32[2,4]{0,1}) custom-call(%{{.*}}), custom_call_target=\"Pin\"\n+  %0 = \"mhlo.custom_call\"(%arg0) {\n+    call_target_name = \"Pin\",\n+    api_version = 4 : i32\n+  } : (tensor<2x4xf32>) -> memref<2x4xf32, strided<[1, 2], offset:0>>\n+  // CHECK: %{{.*}} = b(f32[2,4]{0,1}) custom-call(%{{.*}}), custom_call_target=\"foo\"\n+  %1 = \"mhlo.custom_call\"(%0) {\n+    call_target_name = \"foo\",\n+    api_version = 4 : i32,\n+    output_operand_aliases = [\n+      #mhlo.output_operand_alias<output_tuple_indices = [],\n+        operand_index = 0,\n+        operand_tuple_indices = []>]\n+  } : (memref<2x4xf32, strided<[1, 2], offset:0>>) -> memref<2x4xf32, strided<[1, 2], offset:0>>\n+  // CHECK: %{{.*}} = f32[2,4]{1,0} custom-call(%{{.*}}), custom_call_target=\"Unpin\"\n+  %2 = \"mhlo.custom_call\"(%1) {\n+    call_target_name = \"Unpin\",\n+    api_version = 4 : i32\n+  } : (memref<2x4xf32, strided<[1, 2], offset:0>>) -> tensor<2x4xf32>\n+  func.return %2 : tensor<2x4xf32>\n+}"
        },
        {
            "sha": "8301b7629d1c819c90f44761fd2741aa439cce7b",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/type_to_shape.cc",
            "status": "modified",
            "additions": 50,
            "deletions": 2,
            "changes": 52,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8aa4ddaec4c85400fa1517c6c3205585a8483fb3/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftype_to_shape.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8aa4ddaec4c85400fa1517c6c3205585a8483fb3/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftype_to_shape.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftype_to_shape.cc?ref=8aa4ddaec4c85400fa1517c6c3205585a8483fb3",
            "patch": "@@ -18,8 +18,11 @@ limitations under the License.\n #include <cstdint>\n #include <numeric>\n #include <optional>\n+#include <utility>\n #include <vector>\n \n+#include \"absl/algorithm/container.h\"\n+#include \"absl/status/statusor.h\"\n #include \"llvm/ADT/ArrayRef.h\"\n #include \"llvm/ADT/STLExtras.h\"\n #include \"llvm/ADT/SmallVector.h\"\n@@ -161,10 +164,55 @@ Shape TypeToShape(mlir::Type type) {\n     auto tuple_type =\n         mlir::TupleType::get(type.getContext(), bundle_type.getTypes());\n     return TypeToShape(tuple_type);\n+  } else if (auto m = mlir::dyn_cast<mlir::MemRefType>(type)) {\n+    llvm::SmallVector<int64_t, 6> span(m.getShape().begin(),\n+                                       m.getShape().end());\n+    mlir::Type element_type = m.getElementType();\n+    PrimitiveType primitive_type = ConvertMlirTypeToPrimitiveType(element_type);\n+    if (m.getLayout().isIdentity()) {\n+      absl::StatusOr<Shape> shape =\n+          ShapeUtil::MakeValidatedBufferShape(primitive_type, span);\n+      if (!shape.ok()) {\n+        return {};\n+      }\n+      return shape.value();\n+    }\n+\n+    llvm::SmallVector<int64_t, 4> strides;\n+    int64_t offset;\n+    if (failed(m.getStridesAndOffset(strides, offset))) {\n+      return {};\n+    }\n+\n+    llvm::SmallVector<std::pair<int64_t, int>, 4> strides_with_indices;\n+    for (const auto& e : llvm::enumerate(strides)) {\n+      strides_with_indices.push_back({e.value(), e.index()});\n+    }\n+    absl::c_stable_sort(strides_with_indices);\n+\n+    llvm::SmallVector<int64_t, 4> minor_to_major;\n+    int64_t stride = 1;\n+    for (const auto& pr : strides_with_indices) {\n+      minor_to_major.push_back(pr.second);\n+\n+      if (stride != pr.first && m.getShape()[pr.second] != 1) {\n+        return {};\n+      }\n+\n+      stride *= m.getShape()[pr.second];\n+    }\n+\n+    llvm::SmallVector<int64_t, 4> dimensions(m.getShape().begin(),\n+                                             m.getShape().end());\n+    absl::StatusOr<Shape> shape = ShapeUtil::MakeValidatedBufferShape(\n+        ::xla::ShapeUtil::MakeShapeWithDenseLayout(primitive_type, dimensions,\n+                                                   minor_to_major));\n+    if (!shape.ok()) {\n+      return {};\n+    }\n+    return shape.value();\n   }\n \n-  // Return empty XLA shape to signify error. No MLIR Type maps to a empty\n-  // Shape.\n   return {};\n }\n "
        },
        {
            "sha": "9cb984ac919c55c2ad395fa320246c56391f6cbf",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/type_to_shape.h",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8aa4ddaec4c85400fa1517c6c3205585a8483fb3/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftype_to_shape.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8aa4ddaec4c85400fa1517c6c3205585a8483fb3/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftype_to_shape.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftype_to_shape.h?ref=8aa4ddaec4c85400fa1517c6c3205585a8483fb3",
            "patch": "@@ -23,7 +23,8 @@ limitations under the License.\n \n namespace xla {\n \n-// Returns a XLA Shape equivalent of a MLIR Type, else returns empty shape.\n+// Returns a XLA Shape equivalent of a MLIR Type, else returns empty shape to\n+// signify an error, as no MLIR Type maps to an empty Shape.\n Shape TypeToShape(mlir::Type type);\n \n }  // namespace xla"
        },
        {
            "sha": "81f02c01f3c6bdeb5bed3d178767c6d16def726f",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/type_to_shape_test.cc",
            "status": "modified",
            "additions": 24,
            "deletions": 0,
            "changes": 24,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8aa4ddaec4c85400fa1517c6c3205585a8483fb3/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftype_to_shape_test.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8aa4ddaec4c85400fa1517c6c3205585a8483fb3/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftype_to_shape_test.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftype_to_shape_test.cc?ref=8aa4ddaec4c85400fa1517c6c3205585a8483fb3",
            "patch": "@@ -35,6 +35,7 @@ limitations under the License.\n #include \"tsl/platform/protobuf.h\"\n \n using mlir::Builder;\n+using mlir::MemRefType;\n using mlir::MLIRContext;\n using mlir::RankedTensorType;\n using mlir::UnrankedTensorType;\n@@ -141,5 +142,28 @@ TEST(TypeToShapeTest, ConvertTensorTypeToTypes) {\n       EqualsProto(Shape().ToProto()));\n }\n \n+TEST(TypeToShapeTest, ConvertBufferTypeToTypes) {\n+  MLIRContext context;\n+  Builder builder(&context);\n+\n+  Shape shape1 = ShapeUtil::MakeShapeWithDenseLayout(PrimitiveType::F32,\n+                                                     {10, 20, 30}, {1, 0, 2});\n+\n+  EXPECT_THAT(\n+      TypeToShape(ConvertShapeToType<MemRefType>(shape1, builder).value())\n+          .ToProto(),\n+      EqualsProto(\n+          ShapeUtil::MakeValidatedBufferShape(shape1).value().ToProto()));\n+\n+  Shape shape2 = ShapeUtil::MakeShapeWithDenseLayout(PrimitiveType::F32,\n+                                                     {10, 20, 30}, {2, 1, 0});\n+\n+  EXPECT_THAT(\n+      TypeToShape(ConvertShapeToType<MemRefType>(shape2, builder).value())\n+          .ToProto(),\n+      EqualsProto(\n+          ShapeUtil::MakeValidatedBufferShape(shape2).value().ToProto()));\n+}\n+\n }  // namespace\n }  // namespace xla"
        },
        {
            "sha": "a18781bb1068fb08a872820a5dca510bd3746958",
            "filename": "third_party/xla/xla/hlo/translate/tests/stablehlo.mlir",
            "status": "modified",
            "additions": 104,
            "deletions": 2,
            "changes": 106,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8aa4ddaec4c85400fa1517c6c3205585a8483fb3/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Ftests%2Fstablehlo.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8aa4ddaec4c85400fa1517c6c3205585a8483fb3/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Ftests%2Fstablehlo.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Ftests%2Fstablehlo.mlir?ref=8aa4ddaec4c85400fa1517c6c3205585a8483fb3",
            "patch": "@@ -1966,7 +1966,6 @@ module {\n // -----\n \n // CHECK-LABEL: HloModule main\n-\n // CHECK: ENTRY\n func.func @main() -> tensor<128x2048xf32> {\n   // CHECK-NEXT: %after-all.1 = token[] after-all(), sharding={manual}\n@@ -1984,7 +1983,6 @@ func.func @main() -> tensor<128x2048xf32> {\n // -----\n \n // CHECK-LABEL: HloModule main\n-\n //       CHECK: ENTRY\n func.func @main(%arg0: tensor<2x4xf32>) -> tensor<2x4xf32> {\n   //               CHECK: custom-call({{.*}}), custom_call_target=\"foo\",\n@@ -1999,3 +1997,107 @@ func.func @main(%arg0: tensor<2x4xf32>) -> tensor<2x4xf32> {\n   } : (tensor<2x4xf32>) -> tensor<2x4xf32>\n   func.return %0 : tensor<2x4xf32>\n }\n+\n+// -----\n+\n+// CHECK-LABEL: HloModule main, entry_computation_layout\n+//       CHECK: ENTRY\n+func.func @main() -> memref<2xf32> {\n+  // CHECK: custom-call(), custom_call_target=\"CreateBuffer\"\n+  %0 = \"stablehlo.custom_call\"() {\n+    call_target_name = \"CreateBuffer\",\n+    api_version = 4 : i32\n+  } : () -> memref<2xf32>\n+  func.return %0 : memref<2xf32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: HloModule main, entry_computation_layout\n+//       CHECK: ENTRY\n+func.func @main(%arg0: tensor<2xf32>) -> memref<2xf32> {\n+  //               CHECK: custom-call({{.*}}), custom_call_target=\"Pin\",\n+  // CHECK-SAME{LITERAL}: output_to_operand_aliasing={{}: (0, {})}\n+  %0 = \"stablehlo.custom_call\"(%arg0) {\n+    call_target_name = \"Pin\",\n+    api_version = 4 : i32\n+  } : (tensor<2xf32>) -> memref<2xf32>\n+  func.return %0 : memref<2xf32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: HloModule main, entry_computation_layout\n+//       CHECK: ENTRY\n+func.func @main(%arg0: memref<2xf32>) -> tensor<2xf32> {\n+  //               CHECK: custom-call({{.*}}), custom_call_target=\"Unpin\",\n+  // CHECK-SAME{LITERAL}: output_to_operand_aliasing={{}: (0, {})}\n+  %0 = \"stablehlo.custom_call\"(%arg0) {\n+    call_target_name = \"Unpin\",\n+    api_version = 4 : i32\n+  } : (memref<2xf32>) -> tensor<2xf32>\n+  func.return %0 : tensor<2xf32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: HloModule main, entry_computation_layout\n+//       CHECK: ENTRY\n+func.func @main(%arg0: memref<2x4xf32>) -> memref<2x4xf32> {\n+  //               CHECK: custom-call({{.*}}), custom_call_target=\"foo\",\n+  // CHECK-SAME{LITERAL}: output_to_operand_aliasing={{}: (0, {})}\n+  %0 = \"stablehlo.custom_call\"(%arg0) {\n+    call_target_name = \"foo\",\n+    api_version = 4 : i32,\n+    output_operand_aliases = [\n+      #stablehlo.output_operand_alias<output_tuple_indices = [],\n+        operand_index = 0,\n+        operand_tuple_indices = []>]\n+  } : (memref<2x4xf32>) -> memref<2x4xf32>\n+  func.return %0 : memref<2x4xf32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: HloModule main, entry_computation_layout\n+//       CHECK: ENTRY\n+func.func @main(%arg0: tensor<2xf32>, %arg1: memref<2xf32>) -> tuple<tensor<2xf32>, memref<2xf32>> {\n+  // CHECK: %{{.*}} = (f32[2], b(f32[2])) tuple(%{{.*}}, %{{.*}}),\n+  %0 = \"stablehlo.tuple\"(%arg0, %arg1) : (tensor<2xf32>, memref<2xf32>) -> tuple<tensor<2xf32>, memref<2xf32>>\n+  func.return %0 : tuple<tensor<2xf32>, memref<2xf32>>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: HloModule main, entry_computation_layout\n+//       CHECK: ENTRY\n+func.func @main(%arg0: tuple<tensor<2xf32>, memref<2xf32>>) -> memref<2xf32> {\n+  // CHECK: %{{.*}} = b(f32[2]) get-tuple-element(%{{.*}})\n+  %0 = \"stablehlo.get_tuple_element\"(%arg0) {\n+    index = 1 : i32\n+  } : (tuple<tensor<2xf32>, memref<2xf32>>) -> memref<2xf32>\n+  func.return %0 : memref<2xf32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: HloModule main, entry_computation_layout\n+//       CHECK: ENTRY\n+func.func @main(%arg0: tensor<i1>, %arg1: memref<2xf32>) -> memref<2xf32> {\n+  // CHECK: %{{.*}} = (pred[], b(f32[2])) while(%{{.*}}), condition=%{{.*}}, body=%{{.*}}\n+  %0:2 = stablehlo.while(%iterArg0 = %arg0, %iterArg1 = %arg1) : tensor<i1>, memref<2xf32>\n+    cond {\n+      stablehlo.return %iterArg0 : tensor<i1>\n+    } do {\n+      %1 = \"stablehlo.custom_call\"(%iterArg1) {\n+        call_target_name = \"foo\",\n+        api_version = 4 : i32,\n+        output_operand_aliases = [\n+          #stablehlo.output_operand_alias<output_tuple_indices = [],\n+            operand_index = 0,\n+            operand_tuple_indices = []>]\n+      } : (memref<2xf32>) -> memref<2xf32>\n+      stablehlo.return %iterArg0, %1 : tensor<i1>, memref<2xf32>\n+    }\n+  func.return %0#1: memref<2xf32>\n+}"
        },
        {
            "sha": "573e4fe80e485c720657138ac72a6f0d44096832",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/IR/hlo_base.td",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8aa4ddaec4c85400fa1517c6c3205585a8483fb3/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2FIR%2Fhlo_base.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8aa4ddaec4c85400fa1517c6c3205585a8483fb3/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2FIR%2Fhlo_base.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2FIR%2Fhlo_base.td?ref=8aa4ddaec4c85400fa1517c6c3205585a8483fb3",
            "patch": "@@ -80,12 +80,16 @@ defvar MHLO_AnyTensor = HLO_AnyTensor;\n \n defvar MHLO_ComplexTensor = HLO_ComplexTensor;\n \n+defvar MHLO_Buffer = HLO_Buffer;\n+\n defvar MHLO_Tuple = HLO_Tuple;\n \n defvar MHLO_TensorOrToken = HLO_TensorOrPerAxisQuantizedTensorOrToken;\n+defvar MHLO_TensorOrTokenOrBuffer = AnyTypeOf<[MHLO_TensorOrToken, MHLO_Buffer]>;\n defvar MHLO_TensorOrAnyToken = AnyTypeOf<[MHLO_TensorOrToken, StableHLO_TokenType]>;\n \n defvar MHLO_TensorOrTokenOrTuple = AnyTypeOf<[MHLO_Tensor, MHLO_Token, MHLO_Tuple]>;\n+defvar MHLO_TensorOrTokenOrTupleOrBuffer = AnyTypeOf<[MHLO_TensorOrTokenOrTuple, MHLO_Buffer]>;\n \n defvar MHLO_DimensionValue = HLO_DimensionValue;\n "
        },
        {
            "sha": "c0c13b3acf82ba2a2eda5149d7b8f51e3ff2593b",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/IR/hlo_ops.td",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8aa4ddaec4c85400fa1517c6c3205585a8483fb3/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2FIR%2Fhlo_ops.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8aa4ddaec4c85400fa1517c6c3205585a8483fb3/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2FIR%2Fhlo_ops.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2FIR%2Fhlo_ops.td?ref=8aa4ddaec4c85400fa1517c6c3205585a8483fb3",
            "patch": "@@ -1494,11 +1494,11 @@ def MHLO_WhileOp: MHLO_Op<\"while\", [\n     }) : (tensor<i32>, tensor<i32>) -> (tensor<i32>, tensor<i32>)\n     ```\n   }];\n-  let arguments = (ins Variadic<MHLO_TensorOrToken>:$operand);\n+  let arguments = (ins Variadic<MHLO_TensorOrTokenOrBuffer>:$operand);\n \n   let regions = (region SizedRegion<1>:$cond, SizedRegion<1>:$body);\n \n-  let results = (outs Variadic<MHLO_TensorOrToken>);\n+  let results = (outs Variadic<MHLO_TensorOrTokenOrBuffer>);\n \n   let extraClassDeclaration = [{\n     // Method of OpAsmOpInterface used during custom printing to name the block\n@@ -1800,7 +1800,7 @@ def MHLO_GetTupleElementOp: MHLO_Op<\"get_tuple_element\", [Pure,\n     ConfinedAttr<I32Attr, [IntNonNegative]>:$index\n   );\n \n-  let results = (outs MHLO_TensorOrTokenOrTuple);\n+  let results = (outs MHLO_TensorOrTokenOrTupleOrBuffer);\n \n   let hasFolder = 1;\n \n@@ -1823,7 +1823,7 @@ def MHLO_TupleOp : MHLO_Op<\"tuple\", [Pure,\n     %result = mhlo.tuple %val0, %val1 : tuple<tensor<2xf32>, tuple<tensor<i32>>>\n     ```\n    }];\n-  let arguments = (ins Variadic<MHLO_TensorOrTokenOrTuple>:$val);\n+  let arguments = (ins Variadic<MHLO_TensorOrTokenOrTupleOrBuffer>:$val);\n   let results = (outs MHLO_Tuple:$result);\n \n   let hasCanonicalizer = 1;\n@@ -3467,7 +3467,7 @@ def MHLO_ReturnOp : MHLO_Op<\"return\", [Pure, Terminator]> {\n   }];\n \n   let arguments = (ins\n-    Variadic<MHLO_TensorOrTokenOrTuple >:$results\n+    Variadic<MHLO_TensorOrTokenOrTupleOrBuffer >:$results\n   );\n \n   // Disable conversion operator for return op as the op is not an actual XLA"
        },
        {
            "sha": "70289af25d86f79d3233a5400a4302af5c38a8b5",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/utils/type_conversion.h",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8aa4ddaec4c85400fa1517c6c3205585a8483fb3/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Futils%2Ftype_conversion.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8aa4ddaec4c85400fa1517c6c3205585a8483fb3/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Futils%2Ftype_conversion.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Futils%2Ftype_conversion.h?ref=8aa4ddaec4c85400fa1517c6c3205585a8483fb3",
            "patch": "@@ -53,6 +53,7 @@ namespace stablehlo {\n //   * Index types (index).\n //   * Tensor types.\n //   * Tuple types.\n+//   * Buffer types.\n // Types which are specific to individual dialects like !stablehlo.token\n // and !mhlo.token are handled in subclasses.\n class HloTypeConverter : public TypeConverter {"
        },
        {
            "sha": "53cfbe55d27ebbde6b1833ee23288790c283d4f3",
            "filename": "third_party/xla/xla/mlir_hlo/tests/Dialect/mhlo/hlo-legalize-to-stablehlo.mlir",
            "status": "modified",
            "additions": 131,
            "deletions": 1,
            "changes": 132,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8aa4ddaec4c85400fa1517c6c3205585a8483fb3/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2FDialect%2Fmhlo%2Fhlo-legalize-to-stablehlo.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8aa4ddaec4c85400fa1517c6c3205585a8483fb3/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2FDialect%2Fmhlo%2Fhlo-legalize-to-stablehlo.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2FDialect%2Fmhlo%2Fhlo-legalize-to-stablehlo.mlir?ref=8aa4ddaec4c85400fa1517c6c3205585a8483fb3",
            "patch": "@@ -1871,7 +1871,7 @@ func.func @bounded_dynamism_broadcast_in_dim(%arg0: tensor<1x?xf32, #mhlo.type_e\n // CHECK-LABEL: bounded_dynamism_with_unknown_op\n func.func @bounded_dynamism_with_unknown_op(%arg0: tensor<1x4xi32>, %arg1: tensor<i32>) -> tensor<1x4xi32> {\n   %0 = \"mhlo.set_dimension_size\"(%arg0, %arg1) <{dimension = 1 : i64}> : (tensor<1x4xi32>, tensor<i32>) -> tensor<1x?xi32, #mhlo.type_extensions<bounds = [?, 4]>>\n-  // CHECK: \"tensor.cast\"({{.*}}) : (tensor<1x?xi32, #stablehlo.bounds<?, 4>>) -> tensor<1x4xi32> \n+  // CHECK: \"tensor.cast\"({{.*}}) : (tensor<1x?xi32, #stablehlo.bounds<?, 4>>) -> tensor<1x4xi32>\n   %cast = tensor.cast %0 : tensor<1x?xi32, #mhlo.type_extensions<bounds = [?, 4]>> to tensor<1x4xi32>\n   return %cast : tensor<1x4xi32>\n }\n@@ -2285,3 +2285,133 @@ func.func @op_xla_rng_get_and_update_state() -> tensor<2xui64> {\n   } : () -> tensor<2xui64>\n   func.return %0 : tensor<2xui64>\n }\n+\n+// -----\n+\n+// CHECK-LABEL: \"custom_call_op_create_buffer\"\n+func.func @custom_call_op_create_buffer() -> memref<2xf32> {\n+  //      CHECK: \"stablehlo.custom_call\"\n+  // CHECK-SAME: \"CreateBuffer\"\n+  // CHECK-SAME: () -> memref<2xf32>\n+  %0 = \"mhlo.custom_call\"() {\n+    call_target_name = \"CreateBuffer\",\n+    api_version = 4 : i32\n+  } : () -> memref<2xf32>\n+  func.return %0 : memref<2xf32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: \"custom_call_op_pin\"\n+func.func @custom_call_op_pin(%arg0: tensor<2xf32>) -> memref<2xf32> {\n+  //      CHECK: \"stablehlo.custom_call\"\n+  // CHECK-SAME: \"Pin\"\n+  // CHECK-SAME: (tensor<2xf32>) -> memref<2xf32>\n+  %0 = \"mhlo.custom_call\"(%arg0) {\n+    call_target_name = \"Pin\",\n+    api_version = 4 : i32\n+  } : (tensor<2xf32>) -> memref<2xf32>\n+  func.return %0 : memref<2xf32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: \"custom_call_op_unpin\"\n+func.func @custom_call_op_unpin(%arg0: memref<2xf32>) -> tensor<2xf32> {\n+  //      CHECK: \"stablehlo.custom_call\"\n+  // CHECK-SAME: \"Unpin\"\n+  // CHECK-SAME: (memref<2xf32>) -> tensor<2xf32>\n+  %0 = \"mhlo.custom_call\"(%arg0) {\n+    call_target_name = \"Unpin\",\n+    api_version = 4 : i32\n+  } : (memref<2xf32>) -> tensor<2xf32>\n+  func.return %0 : tensor<2xf32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: \"custom_call_op_with_buffer_type\"\n+func.func @custom_call_op_with_buffer_type(%arg0: memref<2x4xf32>) -> memref<2x4xf32> {\n+  //      CHECK: \"stablehlo.custom_call\"\n+  // CHECK-SAME: \"foo\"\n+  // CHECK-SAME: #stablehlo.output_operand_alias<output_tuple_indices = [], operand_index = 0, operand_tuple_indices = []>\n+  // CHECK-SAME: (memref<2x4xf32>) -> memref<2x4xf32>\n+  %0 = \"mhlo.custom_call\"(%arg0) {\n+    call_target_name = \"foo\",\n+    api_version = 4 : i32,\n+    output_operand_aliases = [\n+      #mhlo.output_operand_alias<output_tuple_indices = [],\n+        operand_index = 0,\n+        operand_tuple_indices = []>]\n+  } : (memref<2x4xf32>) -> memref<2x4xf32>\n+  func.return %0 : memref<2x4xf32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: \"custom_call_op_with_buffer_type_with_layout\"\n+#map = affine_map<(d0, d1) -> (d1, d0)>\n+func.func @custom_call_op_with_buffer_type_with_layout(%arg0: memref<2x4xf32, #map>) -> memref<2x4xf32, #map> {\n+  //      CHECK: \"stablehlo.custom_call\"\n+  // CHECK-SAME: \"foo\"\n+  // CHECK-SAME: #stablehlo.output_operand_alias<output_tuple_indices = [], operand_index = 0, operand_tuple_indices = []>\n+  // CHECK-SAME: (memref<2x4xf32, #map>) -> memref<2x4xf32, #map>\n+  %0 = \"mhlo.custom_call\"(%arg0) {\n+    call_target_name = \"foo\",\n+    api_version = 4 : i32,\n+    output_operand_aliases = [\n+      #mhlo.output_operand_alias<output_tuple_indices = [],\n+        operand_index = 0,\n+        operand_tuple_indices = []>]\n+  } : (memref<2x4xf32, #map>) -> memref<2x4xf32, #map>\n+  func.return %0 : memref<2x4xf32, #map>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: \"tuple_op_with_buffer_type\"\n+func.func @tuple_op_with_buffer_type(%arg0: tensor<2xf32>, %arg1: memref<2xf32>) -> tuple<tensor<2xf32>, memref<2xf32>> {\n+  // CHECK: \"stablehlo.tuple\"(%{{.*}}, %{{.*}}) : (tensor<2xf32>, memref<2xf32>) -> tuple<tensor<2xf32>, memref<2xf32>>\n+  %0 = \"mhlo.tuple\"(%arg0, %arg1) : (tensor<2xf32>, memref<2xf32>) -> tuple<tensor<2xf32>, memref<2xf32>>\n+  func.return %0 : tuple<tensor<2xf32>, memref<2xf32>>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: \"get_tuple_element_op_with_buffer_type\"\n+func.func @get_tuple_element_op_with_buffer_type(%arg0: tuple<tensor<2xf32>, memref<2xf32>>) -> memref<2xf32> {\n+  // CHECK: \"stablehlo.get_tuple_element\"(%{{.*}}) {{.*}}: (tuple<tensor<2xf32>, memref<2xf32>>) -> memref<2xf32>\n+  %0 = \"mhlo.get_tuple_element\"(%arg0) {\n+    index = 1 : i32\n+  } : (tuple<tensor<2xf32>, memref<2xf32>>) -> memref<2xf32>\n+  func.return %0 : memref<2xf32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: \"while_op_with_buffer_type\"\n+func.func @while_op_with_buffer_type(%arg0: tensor<i1>, %arg1: memref<2xf32>) -> memref<2xf32> {\n+  //      CHECK: \"stablehlo.while\"({{.*}}, {{.*}}) ({\n+  // CHECK-NEXT:   ^bb{{.*}}({{.*}}, {{.*}}):\n+  // CHECK-NEXT:     \"stablehlo.return\"\n+  // CHECK-NEXT:   }, {\n+  // CHECK-NEXT:   ^bb{{.*}}({{.*}}, {{.*}}):\n+  // CHECK-NEXT:     \"stablehlo.custom_call\"\n+  // CHECK-NEXT:     \"stablehlo.return\"\n+  // CHECK-NEXT: }) : (tensor<i1>, memref<2xf32>) -> (tensor<i1>, memref<2xf32>)\n+  %0:2 = mhlo.while(%iterArg0 = %arg0, %iterArg1 = %arg1) : tensor<i1>, memref<2xf32>\n+    cond {\n+      mhlo.return %iterArg0 : tensor<i1>\n+    } do {\n+      %1 = \"mhlo.custom_call\"(%iterArg1) {\n+        call_target_name = \"foo\",\n+        api_version = 4 : i32,\n+        output_operand_aliases = [\n+          #mhlo.output_operand_alias<output_tuple_indices = [],\n+            operand_index = 0,\n+            operand_tuple_indices = []>]\n+      } : (memref<2xf32>) -> memref<2xf32>\n+      mhlo.return %iterArg0, %1 : tensor<i1>, memref<2xf32>\n+    }\n+  func.return %0#1: memref<2xf32>\n+}"
        },
        {
            "sha": "fac2fc366e06c2a723773d5fdeafaca1e102c9ee",
            "filename": "third_party/xla/xla/mlir_hlo/tests/Dialect/mhlo/stablehlo-legalize-to-hlo.mlir",
            "status": "modified",
            "additions": 130,
            "deletions": 0,
            "changes": 130,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/8aa4ddaec4c85400fa1517c6c3205585a8483fb3/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2FDialect%2Fmhlo%2Fstablehlo-legalize-to-hlo.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/8aa4ddaec4c85400fa1517c6c3205585a8483fb3/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2FDialect%2Fmhlo%2Fstablehlo-legalize-to-hlo.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2FDialect%2Fmhlo%2Fstablehlo-legalize-to-hlo.mlir?ref=8aa4ddaec4c85400fa1517c6c3205585a8483fb3",
            "patch": "@@ -2539,3 +2539,133 @@ func.func @op_unary_einsum_deprecated(%arg0: tensor<8x16xf32>) -> tensor<8xf32>\n   } : (tensor<8x16xf32>) -> tensor<8xf32>\n   func.return %0 : tensor<8xf32>\n }\n+\n+// -----\n+\n+// CHECK-LABEL: \"custom_call_op_create_buffer\"\n+func.func @custom_call_op_create_buffer() -> memref<2xf32> {\n+  //      CHECK: \"mhlo.custom_call\"\n+  // CHECK-SAME: \"CreateBuffer\"\n+  // CHECK-SAME: () -> memref<2xf32>\n+  %0 = \"stablehlo.custom_call\"() {\n+    call_target_name = \"CreateBuffer\",\n+    api_version = 4 : i32\n+  } : () -> memref<2xf32>\n+  func.return %0 : memref<2xf32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: \"custom_call_op_pin\"\n+func.func @custom_call_op_pin(%arg0: tensor<2xf32>) -> memref<2xf32> {\n+  //      CHECK: \"mhlo.custom_call\"\n+  // CHECK-SAME: \"Pin\"\n+  // CHECK-SAME: (tensor<2xf32>) -> memref<2xf32>\n+  %0 = \"stablehlo.custom_call\"(%arg0) {\n+    call_target_name = \"Pin\",\n+    api_version = 4 : i32\n+  } : (tensor<2xf32>) -> memref<2xf32>\n+  func.return %0 : memref<2xf32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: \"custom_call_op_unpin\"\n+func.func @custom_call_op_unpin(%arg0: memref<2xf32>) -> tensor<2xf32> {\n+  //      CHECK: \"mhlo.custom_call\"\n+  // CHECK-SAME: \"Unpin\"\n+  // CHECK-SAME: (memref<2xf32>) -> tensor<2xf32>\n+  %0 = \"stablehlo.custom_call\"(%arg0) {\n+    call_target_name = \"Unpin\",\n+    api_version = 4 : i32\n+  } : (memref<2xf32>) -> tensor<2xf32>\n+  func.return %0 : tensor<2xf32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: \"custom_call_op_with_buffer_type\"\n+func.func @custom_call_op_with_buffer_type(%arg0: memref<2x4xf32>) -> memref<2x4xf32> {\n+  //      CHECK: \"mhlo.custom_call\"\n+  // CHECK-SAME: \"foo\"\n+  // CHECK-SAME: #mhlo.output_operand_alias<output_tuple_indices = [], operand_index = 0, operand_tuple_indices = []>\n+  // CHECK-SAME: (memref<2x4xf32>) -> memref<2x4xf32>\n+  %0 = \"stablehlo.custom_call\"(%arg0) {\n+    call_target_name = \"foo\",\n+    api_version = 4 : i32,\n+    output_operand_aliases = [\n+      #stablehlo.output_operand_alias<output_tuple_indices = [],\n+        operand_index = 0,\n+        operand_tuple_indices = []>]\n+  } : (memref<2x4xf32>) -> memref<2x4xf32>\n+  func.return %0 : memref<2x4xf32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: \"custom_call_op_with_buffer_type_with_layout\"\n+#map = affine_map<(d0, d1) -> (d1, d0)>\n+func.func @custom_call_op_with_buffer_type_with_layout(%arg0: memref<2x4xf32, #map>) -> memref<2x4xf32, #map> {\n+  //      CHECK: \"mhlo.custom_call\"\n+  // CHECK-SAME: \"foo\"\n+  // CHECK-SAME: #mhlo.output_operand_alias<output_tuple_indices = [], operand_index = 0, operand_tuple_indices = []>\n+  // CHECK-SAME: (memref<2x4xf32, #map>) -> memref<2x4xf32, #map>\n+  %0 = \"stablehlo.custom_call\"(%arg0) {\n+    call_target_name = \"foo\",\n+    api_version = 4 : i32,\n+    output_operand_aliases = [\n+      #stablehlo.output_operand_alias<output_tuple_indices = [],\n+        operand_index = 0,\n+        operand_tuple_indices = []>]\n+  } : (memref<2x4xf32, #map>) -> memref<2x4xf32, #map>\n+  func.return %0 : memref<2x4xf32, #map>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: \"tuple_op_with_buffer_type\"\n+func.func @tuple_op_with_buffer_type(%arg0: tensor<2xf32>, %arg1: memref<2xf32>) -> tuple<tensor<2xf32>, memref<2xf32>> {\n+  // CHECK: \"mhlo.tuple\"(%{{.*}}, %{{.*}}) : (tensor<2xf32>, memref<2xf32>) -> tuple<tensor<2xf32>, memref<2xf32>>\n+  %0 = \"stablehlo.tuple\"(%arg0, %arg1) : (tensor<2xf32>, memref<2xf32>) -> tuple<tensor<2xf32>, memref<2xf32>>\n+  func.return %0 : tuple<tensor<2xf32>, memref<2xf32>>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: \"get_tuple_element_op_with_buffer_type\"\n+func.func @get_tuple_element_op_with_buffer_type(%arg0: tuple<tensor<2xf32>, memref<2xf32>>) -> memref<2xf32> {\n+  // CHECK: \"mhlo.get_tuple_element\"(%{{.*}}) {{.*}}: (tuple<tensor<2xf32>, memref<2xf32>>) -> memref<2xf32>\n+  %0 = \"stablehlo.get_tuple_element\"(%arg0) {\n+    index = 1 : i32\n+  } : (tuple<tensor<2xf32>, memref<2xf32>>) -> memref<2xf32>\n+  func.return %0 : memref<2xf32>\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: \"while_op_with_buffer_type\"\n+func.func @while_op_with_buffer_type(%arg0: tensor<i1>, %arg1: memref<2xf32>) -> memref<2xf32> {\n+  //      CHECK: \"mhlo.while\"({{.*}}, {{.*}}) ({\n+  // CHECK-NEXT:   ^bb{{.*}}({{.*}}, {{.*}}):\n+  // CHECK-NEXT:     \"mhlo.return\"\n+  // CHECK-NEXT:   }, {\n+  // CHECK-NEXT:   ^bb{{.*}}({{.*}}, {{.*}}):\n+  // CHECK-NEXT:     \"mhlo.custom_call\"\n+  // CHECK-NEXT:     \"mhlo.return\"\n+  // CHECK-NEXT: }) : (tensor<i1>, memref<2xf32>) -> (tensor<i1>, memref<2xf32>)\n+  %0:2 = stablehlo.while(%iterArg0 = %arg0, %iterArg1 = %arg1) : tensor<i1>, memref<2xf32>\n+    cond {\n+      stablehlo.return %iterArg0 : tensor<i1>\n+    } do {\n+      %1 = \"stablehlo.custom_call\"(%iterArg1) {\n+        call_target_name = \"foo\",\n+        api_version = 4 : i32,\n+        output_operand_aliases = [\n+          #stablehlo.output_operand_alias<output_tuple_indices = [],\n+            operand_index = 0,\n+            operand_tuple_indices = []>]\n+      } : (memref<2xf32>) -> memref<2xf32>\n+      stablehlo.return %iterArg0, %1 : tensor<i1>, memref<2xf32>\n+    }\n+  func.return %0#1: memref<2xf32>\n+}"
        }
    ],
    "stats": {
        "total": 618,
        "additions": 607,
        "deletions": 11
    }
}