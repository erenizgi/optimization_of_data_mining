{
    "author": "olegshyshkov",
    "message": "[XLA:GPU] Use StreamState as rendezvous value.\n\nWe can add output pointer to StreamState and it will have all the information for rendezvour. No need to have a separate RendezvousValue struct.\n\nPiperOrigin-RevId: 846313928",
    "sha": "c549ee47f87dc9083b4891b7f6dafa2063ddf12a",
    "files": [
        {
            "sha": "d6c17f5e6bd4bf6efaa2711079feb20686ff5736",
            "filename": "third_party/xla/xla/backends/gpu/runtime/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c549ee47f87dc9083b4891b7f6dafa2063ddf12a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c549ee47f87dc9083b4891b7f6dafa2063ddf12a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2FBUILD?ref=c549ee47f87dc9083b4891b7f6dafa2063ddf12a",
            "patch": "@@ -1504,6 +1504,7 @@ cc_library(\n         \"//xla/stream_executor:memory_allocation\",\n         \"//xla/stream_executor:stream\",\n         \"//xla/tsl/platform:errors\",\n+        \"//xla/tsl/platform:status_macros\",\n         \"//xla/tsl/platform:statusor\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/base:core_headers\","
        },
        {
            "sha": "f89a39e29bc3e9df43f5a312e804978d877f6b20",
            "filename": "third_party/xla/xla/backends/gpu/runtime/ragged_all_to_all_thunk.cc",
            "status": "modified",
            "additions": 46,
            "deletions": 70,
            "changes": 116,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c549ee47f87dc9083b4891b7f6dafa2063ddf12a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c549ee47f87dc9083b4891b7f6dafa2063ddf12a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.cc?ref=c549ee47f87dc9083b4891b7f6dafa2063ddf12a",
            "patch": "@@ -59,6 +59,7 @@ limitations under the License.\n #include \"xla/util.h\"\n #include \"xla/xla_data.pb.h\"\n #include \"tsl/platform/casts.h\"\n+#include \"xla/tsl/platform/status_macros.h\"\n \n namespace xla {\n namespace gpu {\n@@ -215,99 +216,75 @@ absl::Status RunRaggedAllToAll(\n   return future.Await();\n }\n \n-// Contains the values that are passed between host threads with rendezvous.\n-struct RendezvousValue {\n-  RankId rank;\n-  se::DeviceAddressBase output_buffer;\n-  se::Event* start_event;\n-  se::Event* end_event;\n+}  // namespace\n \n-  bool operator<(const RendezvousValue& other) const {\n-    return rank < other.rank;\n-  }\n-};\n-\n-// Executes the rendezvous before the kernel start.\n-// Inserts CUDA events into the stream to ensure that all devices have reached\n-// the start event before the kernel starts.\n-absl::StatusOr<std::shared_ptr<std::vector<RendezvousValue>>>\n-RendezvousBeforeKernelStart(absl::string_view name,\n-                            const GpuCliqueKey& clique_key, RankId rank,\n-                            int64_t num_ranks,\n-                            const se::DeviceAddressBase& output_buffer,\n-                            se::Stream& stream, se::Event* start_event,\n-                            se::Event* end_event) {\n-  RendezvousValue rendezvous_value;\n-  rendezvous_value.rank = rank;\n-  rendezvous_value.output_buffer = output_buffer;\n-  rendezvous_value.start_event = start_event;\n-  rendezvous_value.end_event = end_event;\n+absl::StatusOr<\n+    std::shared_ptr<std::vector<const RaggedAllToAllStartThunk::StreamState*>>>\n+RaggedAllToAllStartThunk::RendezvousBeforeKernelStart(\n+    const GpuCliqueKey& clique_key, se::Stream& stream,\n+    const StreamState& state) {\n+  int64_t num_ranks = clique_key.num_local_participants();\n \n   // Record that this device has started the memcpy ragged-all-to-all. We do\n   // this before the rendezvous to make sure that RecordEvent is called before\n   // WaitFor on another stream.\n-  TF_RETURN_IF_ERROR(stream.RecordEvent(start_event));\n+  RETURN_IF_ERROR(stream.RecordEvent(state.start_event.get()));\n \n-  auto rendezvous_fn = [](absl::Span<const RendezvousValue* const> values) {\n-    std::vector<RendezvousValue> values_copy;\n+  auto rendezvous_fn = [](absl::Span<const StreamState* const> values) {\n+    std::vector<const StreamState*> values_copy;\n     for (const auto& value : values) {\n-      values_copy.push_back(*value);\n+      values_copy.push_back(value);\n     }\n     // Sort to make sure that values are in the same order as the devices are\n     // ordered in the communicator.\n-    absl::c_sort(values_copy);\n+    absl::c_sort(values_copy, [](const StreamState* a, const StreamState* b) {\n+      return a->rank < b->rank;\n+    });\n     return values_copy;\n   };\n \n   std::string start_rendezvous_key =\n-      absl::StrFormat(\"start %s ragged-all-to-all for rank %d, clique %s\", name,\n-                      rank.value(), clique_key.ToString());\n-  TF_ASSIGN_OR_RETURN(\n-      std::shared_ptr<std::vector<RendezvousValue>> rendezvous_values,\n-      Rendezvous<std::vector<RendezvousValue>>(\n-          /*name=*/\n-          start_rendezvous_key, /*key=*/clique_key,\n-          /*value=*/rendezvous_value, /*num_threads=*/num_ranks,\n-          rendezvous_fn));\n+      absl::StrFormat(\"start one-shot ragged-all-to-all for rank %d, clique %s\",\n+                      state.rank.value(), clique_key.ToString());\n+  ASSIGN_OR_RETURN(\n+      std::shared_ptr<std::vector<const StreamState*>> rendezvous_values,\n+      Rendezvous<std::vector<const StreamState*>>(\n+          start_rendezvous_key, clique_key, state, num_ranks, rendezvous_fn));\n \n   // Wait for all devices to reach the start event. This indicates that all\n   // output buffers are ready for transfer.\n-  for (auto& value : *rendezvous_values) {\n-    TF_RETURN_IF_ERROR(stream.WaitFor(value.start_event));\n+  for (const StreamState* remote_stream_state : *rendezvous_values) {\n+    RETURN_IF_ERROR(stream.WaitFor(remote_stream_state->start_event.get()));\n   }\n \n   return rendezvous_values;\n }\n \n-// Executes the rendezvous after the kernel finish. Waits for all devices to\n-// reach the end event.\n-absl::Status RendezvousAfterKernelFinish(\n-    absl::string_view name, const GpuCliqueKey& clique_key, RankId rank,\n-    int64_t num_ranks, se::Stream& stream, se::Event* end_event,\n-    const std::shared_ptr<std::vector<RendezvousValue>>& rendezvous_values) {\n+absl::Status RaggedAllToAllStartThunk::RendezvousAfterKernelFinish(\n+    const GpuCliqueKey& clique_key, se::Stream& stream,\n+    const StreamState& state,\n+    absl::Span<const StreamState* const> remote_stream_states) {\n+  int64_t num_ranks = clique_key.num_local_participants();\n+\n   // Record that this device has finished the memcpy ragged-all-to-all.\n-  TF_RETURN_IF_ERROR(stream.RecordEvent(end_event));\n+  RETURN_IF_ERROR(stream.RecordEvent(state.end_event.get()));\n \n   // Do another rendezvous to make sure that we call RecordEvent for end_event\n   // before WaitFor on another stream.\n-  std::string finish_rendezvous_key =\n-      absl::StrFormat(\"finish %s ragged-all-to-all for rank %d, clique %s\",\n-                      name, rank.value(), clique_key.ToString());\n-  TF_RETURN_IF_ERROR(Rendezvous(/*name=*/finish_rendezvous_key,\n-                                /*key=*/clique_key,\n-                                /*num_threads=*/num_ranks));\n+  std::string finish_rendezvous_key = absl::StrFormat(\n+      \"finish one-shot ragged-all-to-all for rank %d, clique %s\",\n+      state.rank.value(), clique_key.ToString());\n+  RETURN_IF_ERROR(Rendezvous(finish_rendezvous_key, clique_key, num_ranks));\n \n   // Wait for all devices to reach the end event. This indicates that all\n   // updates from other devices have arrived.\n-  for (auto& value : *rendezvous_values) {\n-    TF_RETURN_IF_ERROR(stream.WaitFor(value.end_event));\n+  for (const StreamState* remote_stream_state : remote_stream_states) {\n+    RETURN_IF_ERROR(stream.WaitFor(remote_stream_state->end_event.get()));\n   }\n \n   return absl::OkStatus();\n }\n \n-}  // namespace\n-\n absl::Status RaggedAllToAllStartThunk::RunOneShotRaggedAllToAll(\n     const GpuCliqueKey& clique_key, se::Stream& stream,\n     const StreamState& state, absl::Span<DeviceBufferPair const> buffers) {\n@@ -322,19 +299,16 @@ absl::Status RaggedAllToAllStartThunk::RunOneShotRaggedAllToAll(\n   PrimitiveType element_type = buffers[0].element_type;\n \n   se::DeviceAddressBase input_buffer = buffers[0].source_buffer;\n-  se::DeviceAddressBase output_buffer = buffers[1].destination_buffer;\n \n   TF_ASSIGN_OR_RETURN(\n-      std::shared_ptr<std::vector<RendezvousValue>> rendezvous_values,\n-      RendezvousBeforeKernelStart(\n-          /*name=*/\"one-shot\", clique_key, rank, num_ranks, output_buffer,\n-          stream, state.start_event.get(), state.end_event.get()));\n+      std::shared_ptr<std::vector<const StreamState*>> remote_stream_states,\n+      RendezvousBeforeKernelStart(clique_key, stream, state));\n \n   const int64_t num_updates_per_replica = config_.num_total_updates / num_ranks;\n \n   absl::InlinedVector<se::DeviceAddressBase, 4> output_ptrs;\n-  for (auto& value : *rendezvous_values) {\n-    output_ptrs.push_back(value.output_buffer);\n+  for (const StreamState* remote_stream_state : *remote_stream_states) {\n+    output_ptrs.push_back(remote_stream_state->local_output_buffer);\n   }\n \n   TF_RETURN_IF_ERROR(RunRaggedAllToAllKernel(\n@@ -343,9 +317,8 @@ absl::Status RaggedAllToAllStartThunk::RunOneShotRaggedAllToAll(\n       buffers[4].source_buffer, num_ranks, num_updates_per_replica,\n       config_.num_input_rows, config_.num_row_elements));\n \n-  return RendezvousAfterKernelFinish(\n-      /*name=*/\"one-shot\", clique_key, rank, num_ranks, stream,\n-      state.end_event.get(), rendezvous_values);\n+  return RendezvousAfterKernelFinish(clique_key, stream, state,\n+                                     *remote_stream_states);\n }\n \n RaggedAllToAllStartThunk::RaggedAllToAllStartThunk(\n@@ -440,6 +413,9 @@ absl::Status RaggedAllToAllStartThunk::Initialize(\n     return absl::InternalError(\"Failed to allocate output offsets buffer.\");\n   }\n \n+  state->local_output_buffer = params.buffer_allocations->GetDeviceAddress(\n+      buffers_[1].destination_buffer);\n+\n   if (is_local()) {\n     TF_ASSIGN_OR_RETURN(state->start_event, executor->CreateEvent());\n     TF_ASSIGN_OR_RETURN(state->end_event, executor->CreateEvent());\n@@ -493,7 +469,7 @@ absl::StatusOr<bool> RaggedAllToAllStartThunk::RunCollective(\n                                       device_buffers[0].element_type);\n \n   if (should_use_one_shot_kernel) {\n-    TF_RETURN_IF_ERROR(\n+    RETURN_IF_ERROR(\n         RunOneShotRaggedAllToAll(clique_key, stream, *state, device_buffers));\n     return false;\n   }"
        },
        {
            "sha": "b3354b814fda60efcba856c4391e442aa6e13deb",
            "filename": "third_party/xla/xla/backends/gpu/runtime/ragged_all_to_all_thunk.h",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/c549ee47f87dc9083b4891b7f6dafa2063ddf12a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/c549ee47f87dc9083b4891b7f6dafa2063ddf12a/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fruntime%2Fragged_all_to_all_thunk.h?ref=c549ee47f87dc9083b4891b7f6dafa2063ddf12a",
            "patch": "@@ -32,6 +32,7 @@ limitations under the License.\n #include \"xla/core/collectives/communicator.h\"\n #include \"xla/core/collectives/rank_id.h\"\n #include \"xla/hlo/ir/hlo_instructions.h\"\n+#include \"xla/stream_executor/device_address.h\"\n #include \"xla/stream_executor/device_address_handle.h\"\n #include \"xla/stream_executor/event.h\"\n #include \"xla/stream_executor/memory_allocation.h\"\n@@ -91,6 +92,9 @@ class RaggedAllToAllStartThunk : public CollectiveThunk {\n     // Device memory buffer for output offsets.\n     se::DeviceAddressHandle output_offsets_device_buffer;\n \n+    // Device memory buffer for local output.\n+    se::DeviceAddressBase local_output_buffer;\n+\n     // Event to synchronize streams on different devices at the start of the\n     // kernel.\n     std::unique_ptr<se::Event> start_event;\n@@ -103,6 +107,20 @@ class RaggedAllToAllStartThunk : public CollectiveThunk {\n         : device_ordinal(device_ordinal), rank(rank) {}\n   };\n \n+  // Executes the rendezvous before the kernel start.\n+  // Inserts CUDA events into the stream to ensure that all devices have reached\n+  // the start event before the kernel starts.\n+  absl::StatusOr<std::shared_ptr<std::vector<const StreamState*>>>\n+  RendezvousBeforeKernelStart(const GpuCliqueKey& clique_key,\n+                              se::Stream& stream, const StreamState& state);\n+\n+  // Executes the rendezvous after the kernel finish. Waits for all devices to\n+  // reach the end event.\n+  absl::Status RendezvousAfterKernelFinish(\n+      const GpuCliqueKey& clique_key, se::Stream& stream,\n+      const StreamState& state,\n+      absl::Span<const StreamState* const> remote_stream_states);\n+\n   absl::Status RunOneShotRaggedAllToAll(\n       const GpuCliqueKey& clique_key, se::Stream& stream,\n       const StreamState& state, absl::Span<DeviceBufferPair const> buffers);"
        }
    ],
    "stats": {
        "total": 135,
        "additions": 65,
        "deletions": 70
    }
}