{
    "author": "pifon2a",
    "message": "[XLA:GPU] Enable chlo.acosh -> kAcosh HloInstruction lowering.\n\nPiperOrigin-RevId: 807623829",
    "sha": "70003f77e0a12f293ad61b7562539421c0c86151",
    "files": [
        {
            "sha": "64dbb72a27c5d81f0f5fcd04b3b4ff522660e0bd",
            "filename": "tensorflow/compiler/mlir/tf2xla/transforms/xla_legalize_tf.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/70003f77e0a12f293ad61b7562539421c0c86151/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftransforms%2Fxla_legalize_tf.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/70003f77e0a12f293ad61b7562539421c0c86151/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftransforms%2Fxla_legalize_tf.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow%2Fcompiler%2Fmlir%2Ftf2xla%2Ftransforms%2Fxla_legalize_tf.cc?ref=70003f77e0a12f293ad61b7562539421c0c86151",
            "patch": "@@ -49,6 +49,7 @@ limitations under the License.\n #include \"tensorflow/compiler/mlir/tf2xla/transforms/passes.h\"\n #include \"tensorflow/compiler/mlir/tf2xla/transforms/xla_legalize_targets.h\"\n #include \"xla/mlir_hlo/mhlo/IR/hlo_ops.h\"  // IWYU pragma: keep, dependent dialect\n+#include \"xla/mlir_hlo/mhlo/transforms/passes.h\"\n #include \"xla/mlir_hlo/mhlo/transforms/rewriters.h\"\n #include \"xla/mlir_hlo/mhlo/utils/type_conversion.h\"\n #include \"tensorflow/core/lib/monitoring/counter.h\""
        },
        {
            "sha": "654f538658ae0419460a845750225b42d1f1d238",
            "filename": "third_party/xla/xla/backends/gpu/codegen/emitters/transforms/optimize_loops.cc",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftransforms%2Foptimize_loops.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftransforms%2Foptimize_loops.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fbackends%2Fgpu%2Fcodegen%2Femitters%2Ftransforms%2Foptimize_loops.cc?ref=70003f77e0a12f293ad61b7562539421c0c86151",
            "patch": "@@ -46,6 +46,11 @@ namespace gpu {\n \n namespace {\n \n+bool IsExpensiveToUnroll(mlir::Operation* op) {\n+  return mlir::isa<mlir::func::CallOp, mlir::scf::ForOp, mlir::math::AcoshOp>(\n+      op);\n+}\n+\n int GetUnrollingFactor(mlir::scf::ForOp op) {\n   // We only unroll loops with a step of 1 and a lower bound of 0. That's the\n   // only type we generate.\n@@ -68,7 +73,7 @@ int GetUnrollingFactor(mlir::scf::ForOp op) {\n   int64_t size = 0;\n   bool can_unroll = true;\n   op.getBodyRegion().walk([&](mlir::Operation* op) {\n-    if (mlir::isa<mlir::func::CallOp, mlir::scf::ForOp>(op)) {\n+    if (IsExpensiveToUnroll(op)) {\n       can_unroll = false;\n       return;\n     }"
        },
        {
            "sha": "54e78cfcdaa7ac7a855396d544e51e59ab869e52",
            "filename": "third_party/xla/xla/hlo/builder/lib/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fhlo%2Fbuilder%2Flib%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fhlo%2Fbuilder%2Flib%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fbuilder%2Flib%2FBUILD?ref=70003f77e0a12f293ad61b7562539421c0c86151",
            "patch": "@@ -222,6 +222,7 @@ cc_library(\n         \"//xla:util\",\n         \"//xla:xla_data_proto_cc\",\n         \"//xla/hlo/builder:xla_builder\",\n+        \"//xla/hlo/ir:hlo\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/status\","
        },
        {
            "sha": "8ec0942c30c1b61cba042390c0b802da93791b8e",
            "filename": "third_party/xla/xla/hlo/builder/lib/math.cc",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fhlo%2Fbuilder%2Flib%2Fmath.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fhlo%2Fbuilder%2Flib%2Fmath.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fbuilder%2Flib%2Fmath.cc?ref=70003f77e0a12f293ad61b7562539421c0c86151",
            "patch": "@@ -21,6 +21,7 @@ limitations under the License.\n #include <cstdint>\n #include <functional>\n #include <limits>\n+#include <optional>\n #include <type_traits>\n #include <vector>\n \n@@ -35,6 +36,7 @@ limitations under the License.\n #include \"xla/hlo/builder/lib/loops.h\"\n #include \"xla/hlo/builder/lib/math_impl.h\"\n #include \"xla/hlo/builder/xla_builder.h\"\n+#include \"xla/hlo/ir/hlo_opcode.h\"\n #include \"xla/primitive_util.h\"\n #include \"xla/shape.h\"\n #include \"xla/status_macros.h\"\n@@ -1223,7 +1225,11 @@ XlaOp Atan(XlaOp x) { return Atan2(x, ScalarLike(x, 1.0)); }\n // If x^2 will overflow, we approximate sqrt(x^2 - 1) == x and compute as\n // log(2*x) = log(2) + log(x).  (Note this works because negative x never\n // overflows; x < -1 simply yields nan.  This is quite different than asinh!)\n-XlaOp Acosh(XlaOp x) {\n+XlaOp Acosh(XlaOp x, const std::optional<ResultAccuracy>& result_accuracy,\n+            bool expand) {\n+  if (!expand) {\n+    return x.builder()->UnaryOp(HloOpcode::kAcosh, x, result_accuracy);\n+  }\n   XlaBuilder* b = x.builder();\n   return b->ReportErrorOrReturn([&]() -> absl::StatusOr<XlaOp> {\n     TF_ASSIGN_OR_RETURN(auto shape, b->GetShape(x));"
        },
        {
            "sha": "5f9f784d263c9222f41326a6fa361460d42374f6",
            "filename": "third_party/xla/xla/hlo/builder/lib/math.h",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fhlo%2Fbuilder%2Flib%2Fmath.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fhlo%2Fbuilder%2Flib%2Fmath.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fbuilder%2Flib%2Fmath.h?ref=70003f77e0a12f293ad61b7562539421c0c86151",
            "patch": "@@ -16,6 +16,8 @@ limitations under the License.\n #ifndef XLA_HLO_BUILDER_LIB_MATH_H_\n #define XLA_HLO_BUILDER_LIB_MATH_H_\n \n+#include <optional>\n+\n #include \"xla/hlo/builder/xla_builder.h\"\n \n namespace xla {\n@@ -93,7 +95,9 @@ XlaOp Atan(XlaOp x);\n // Hyperbolic trigonometric functions\n \n // Computes the inverse hyperbolic cosine of 'x'.\n-XlaOp Acosh(XlaOp x);\n+XlaOp Acosh(XlaOp x,\n+            const std::optional<ResultAccuracy>& result_accuracy = std::nullopt,\n+            bool expand = true);\n \n // Computes the inverse hyperbolic sine of 'x'.\n XlaOp Asinh(XlaOp x);"
        },
        {
            "sha": "4893c6e7b37179bcac096e9b728d5aee201e2e07",
            "filename": "third_party/xla/xla/hlo/builder/xla_builder.h",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fhlo%2Fbuilder%2Fxla_builder.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fhlo%2Fbuilder%2Fxla_builder.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Fbuilder%2Fxla_builder.h?ref=70003f77e0a12f293ad61b7562539421c0c86151",
            "patch": "@@ -1731,6 +1731,9 @@ class XlaBuilder {\n       absl::Span<const std::pair<int64_t, int64_t>> padding, XlaOp source,\n       XlaOp init_value, XlaComputationId scatter);\n   friend XlaOp Abs(XlaOp operand);\n+  friend XlaOp Acosh(XlaOp x,\n+                     const std::optional<ResultAccuracy>& result_accuracy,\n+                     bool expand);\n   friend XlaOp Atan2(XlaOp y, XlaOp x,\n                      absl::Span<const int64_t> broadcast_dimensions);\n   friend XlaOp Erf(XlaOp operand,"
        },
        {
            "sha": "de2251b788a24962ecfca041d019817d7e11b326",
            "filename": "third_party/xla/xla/hlo/translate/BUILD",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2FBUILD?ref=70003f77e0a12f293ad61b7562539421c0c86151",
            "patch": "@@ -131,6 +131,5 @@ cc_library(\n         \"@llvm-project//mlir:Transforms\",\n         \"@llvm-project//mlir:UBDialect\",\n         \"@stablehlo//:stablehlo_passes\",\n-        \"@stablehlo//:stablehlo_passes_optimization\",\n     ],\n )"
        },
        {
            "sha": "a4957df8ed397a310b75b6e096a10214325eff48",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/gen_hlo_op_writer.td",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fgen_hlo_op_writer.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fgen_hlo_op_writer.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fgen_hlo_op_writer.td?ref=70003f77e0a12f293ad61b7562539421c0c86151",
            "patch": "@@ -197,6 +197,7 @@ defvar CustomHloConverterOps = [\n \n   // MHLO ops.\n   // go/keep-sorted start\n+  MHLO_AcoshOp,\n   MHLO_AddDependencyOp,\n   MHLO_AllGatherOp,\n   MHLO_AllReduceOp,"
        },
        {
            "sha": "d6f93420326b814c83c45416ecc21b4d5d1de13d",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/mlir_hlo_to_hlo.cc",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fmlir_hlo_to_hlo.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fmlir_hlo_to_hlo.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Fmlir_hlo_to_hlo.cc?ref=70003f77e0a12f293ad61b7562539421c0c86151",
            "patch": "@@ -5177,6 +5177,17 @@ LogicalResult ExportXlaOp(UniformDequantizeOp op, OpLoweringContext ctx) {\n   return failure();\n }\n \n+LogicalResult ExportXlaOp(AcoshOp op, OpLoweringContext ctx) {\n+  auto& value_map = *ctx.values;\n+  xla::XlaOp operand;\n+  if (failed(GetXlaOp(op.getOperand(), value_map, &operand, op))) {\n+    return failure();\n+  }\n+  value_map[op] =\n+      xla::Acosh(operand, /*result_accuracy=*/std::nullopt, /*expand=*/false);\n+  return success();\n+}\n+\n LogicalResult ExportXlaOp(TopKOp op, OpLoweringContext ctx) {\n   auto& value_map = *ctx.values;\n   xla::XlaOp operand;"
        },
        {
            "sha": "9795682255426feb5647ec7e33e679e9a5550970",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/tests/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2FBUILD?ref=70003f77e0a12f293ad61b7562539421c0c86151",
            "patch": "@@ -10,6 +10,7 @@ lit_test_suite(\n     name = \"all_tests\",\n     srcs = enforce_glob(\n         [\n+            \"acosh.mlir\",\n             \"add.mlir\",\n             \"attributes.mlir\",\n             \"call.mlir\","
        },
        {
            "sha": "b0e64798be2b1912a84594a34625bc5cda372dfd",
            "filename": "third_party/xla/xla/hlo/translate/mhlo_to_hlo/tests/acosh.mlir",
            "status": "added",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Facosh.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Facosh.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fhlo%2Ftranslate%2Fmhlo_to_hlo%2Ftests%2Facosh.mlir?ref=70003f77e0a12f293ad61b7562539421c0c86151",
            "patch": "@@ -0,0 +1,7 @@\n+// RUN: xla-translate -mlir-hlo-to-hlo-text %s | FileCheck %s\n+\n+func.func @main(%arg0: tensor<4xf32>) -> tensor<4xf32> {\n+  // CHECK: f32[4] acosh\n+  %0 = \"mhlo.acosh\"(%arg0) : (tensor<4xf32>) -> tensor<4xf32>\n+  func.return %0 : tensor<4xf32>\n+}"
        },
        {
            "sha": "f8adcdb88de58d16254a1f8be4256ae245f772b4",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/transforms/chlo_legalize_to_hlo/chlo_legalize_to_hlo_pass.cc",
            "status": "modified",
            "additions": 116,
            "deletions": 69,
            "changes": 185,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fchlo_legalize_to_hlo%2Fchlo_legalize_to_hlo_pass.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fchlo_legalize_to_hlo%2Fchlo_legalize_to_hlo_pass.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fchlo_legalize_to_hlo%2Fchlo_legalize_to_hlo_pass.cc?ref=70003f77e0a12f293ad61b7562539421c0c86151",
            "patch": "@@ -13,11 +13,13 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n+#include <memory>\n #include <optional>\n #include <utility>\n #include <vector>\n \n #include \"mhlo/IR/hlo_ops.h\"\n+#include \"mhlo/transforms/passes.h\"\n #include \"mhlo/transforms/rewriters.h\"\n #include \"mhlo/utils/type_conversion.h\"\n #include \"mlir/Dialect/Arith/IR/Arith.h\"\n@@ -27,8 +29,11 @@ limitations under the License.\n #include \"mlir/IR/Attributes.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n+#include \"mlir/IR/BuiltinTypeInterfaces.h\"\n+#include \"mlir/IR/BuiltinTypes.h\"\n #include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/IR/TypeUtilities.h\"\n #include \"mlir/Pass/PassManager.h\"\n #include \"mlir/Support/LLVM.h\"\n #include \"mlir/Support/LogicalResult.h\"\n@@ -46,22 +51,39 @@ namespace mhlo {\n \n namespace {\n \n+ChloLegalizeToHighLevelMhloPassOptions FromPassOptions(bool enableAcosh) {\n+  ChloLegalizeToHighLevelMhloPassOptions options;\n+  options.enable_acosh_ = enableAcosh;\n+  return options;\n+}\n+\n+bool isLegalAcosh(chlo::AcoshOp op) {\n+  return !llvm::isa<FloatType>(getElementTypeOrSelf(op.getType()));\n+}\n+\n struct ChloLegalizeToHighLevelMhloPass\n     : public impl::ChloLegalizeToHighLevelMhloPassBase<\n           ChloLegalizeToHighLevelMhloPass> {\n-  using ChloLegalizeToHighLevelMhloPassBase::\n-      ChloLegalizeToHighLevelMhloPassBase;\n+  ChloLegalizeToHighLevelMhloPass() = default;\n+  explicit ChloLegalizeToHighLevelMhloPass(\n+      ChloLegalizeToHighLevelMhloPassOptions options)\n+      : impl::ChloLegalizeToHighLevelMhloPassBase<\n+            ChloLegalizeToHighLevelMhloPass>(options) {}\n \n   void runOnOperation() override {\n     MLIRContext &context = getContext();\n     ConversionTarget conversionTarget(context);\n     RewritePatternSet conversionPatterns(&context);\n \n-    chlo::populateChloToHighLevelMhloOpPatterns(&context, &conversionPatterns);\n+    chlo::populateChloToHighLevelMhloOpPatterns(&context, &conversionPatterns,\n+                                                FromPassOptions(enable_acosh_));\n \n     // Consider the mhlo dialect legal for tests. Also add helper dialects\n     // that are needed by the patterns.\n     conversionTarget.addLegalDialect<chlo::ChloDialect, mhlo::MhloDialect>();\n+    if (enable_acosh_) {\n+      conversionTarget.addDynamicallyLegalOp<chlo::AcoshOp>(isLegalAcosh);\n+    }\n     conversionTarget\n         .addIllegalOp<chlo::TopKOp, chlo::ErfOp, chlo::RaggedDotOp>();\n \n@@ -100,71 +122,86 @@ struct ChloLegalizeToHloPass\n   }\n };\n \n-struct RaggedDotChloToMhlo : public OpRewritePattern<chlo::RaggedDotOp> {\n-  using OpRewritePattern<chlo::RaggedDotOp>::OpRewritePattern;\n-\n-  LogicalResult matchAndRewrite(chlo::RaggedDotOp raggedDotOp,\n-                                PatternRewriter &rewriter) const override {\n-    auto moduleOp = raggedDotOp->getParentOfType<ModuleOp>();\n-\n-    OpBuilder builder(moduleOp.getBodyRegion());\n-    builder.setInsertionPointToStart(&moduleOp.getBodyRegion().front());\n-\n-    auto chloRaggedDotDimNums = raggedDotOp.getRaggedDotDimensionNumbers();\n-    auto dotDimNums = mhlo::DotDimensionNumbersAttr::get(\n-        builder.getContext(), chloRaggedDotDimNums.getLhsBatchingDimensions(),\n-        chloRaggedDotDimNums.getRhsBatchingDimensions(),\n-        chloRaggedDotDimNums.getLhsContractingDimensions(),\n-        chloRaggedDotDimNums.getRhsContractingDimensions());\n-    auto raggedDotDimNums = mhlo::RaggedDotDimensionNumbersAttr::get(\n-        builder.getContext(), dotDimNums,\n-        chloRaggedDotDimNums.getLhsRaggedDimensions(),\n-        chloRaggedDotDimNums.getRhsGroupDimensions());\n-\n-    auto mhloPrecision =\n-        [](chlo::Precision precision) -> std::optional<mhlo::Precision> {\n-      switch (precision) {\n-        case chlo::Precision::DEFAULT:\n-          return mhlo::Precision::DEFAULT;\n-        case chlo::Precision::HIGH:\n-          return mhlo::Precision::HIGH;\n-        case chlo::Precision::HIGHEST:\n-          return mhlo::Precision::HIGHEST;\n-      }\n-    };\n-    ArrayAttr precisionConfig = rewriter.getArrayAttr({});\n-    if (raggedDotOp.getPrecisionConfig().has_value()) {\n-      SmallVector<Attribute> vector;\n-      for (auto configValue : raggedDotOp.getPrecisionConfig()\n-                                  .value()\n-                                  .getAsRange<chlo::PrecisionAttr>()) {\n-        vector.push_back(\n-            PrecisionAttr::get(raggedDotOp.getContext(),\n-                               mhloPrecision(configValue.getValue()).value()));\n-      }\n-      precisionConfig = rewriter.getArrayAttr(vector);\n+LogicalResult convertRaggedDotChloToMhlo(chlo::RaggedDotOp raggedDotOp,\n+                                         PatternRewriter& rewriter) {\n+  auto moduleOp = raggedDotOp->getParentOfType<ModuleOp>();\n+\n+  OpBuilder builder(moduleOp.getBodyRegion());\n+  builder.setInsertionPointToStart(&moduleOp.getBodyRegion().front());\n+\n+  auto chloRaggedDotDimNums = raggedDotOp.getRaggedDotDimensionNumbers();\n+  auto dotDimNums = mhlo::DotDimensionNumbersAttr::get(\n+      builder.getContext(), chloRaggedDotDimNums.getLhsBatchingDimensions(),\n+      chloRaggedDotDimNums.getRhsBatchingDimensions(),\n+      chloRaggedDotDimNums.getLhsContractingDimensions(),\n+      chloRaggedDotDimNums.getRhsContractingDimensions());\n+  auto raggedDotDimNums = mhlo::RaggedDotDimensionNumbersAttr::get(\n+      builder.getContext(), dotDimNums,\n+      chloRaggedDotDimNums.getLhsRaggedDimensions(),\n+      chloRaggedDotDimNums.getRhsGroupDimensions());\n+\n+  auto mhloPrecision =\n+      [](chlo::Precision precision) -> std::optional<mhlo::Precision> {\n+    switch (precision) {\n+      case chlo::Precision::DEFAULT:\n+        return mhlo::Precision::DEFAULT;\n+      case chlo::Precision::HIGH:\n+        return mhlo::Precision::HIGH;\n+      case chlo::Precision::HIGHEST:\n+        return mhlo::Precision::HIGHEST;\n     }\n-\n-    mhlo::RaggedDotOp mhloOp = rewriter.create<mhlo::RaggedDotOp>(\n-        raggedDotOp.getLoc(), raggedDotOp.getResult().getType(),\n-        raggedDotOp.getLhs(), raggedDotOp.getRhs(), raggedDotOp.getGroupSizes(),\n-        raggedDotDimNums, precisionConfig);\n-    std::optional<NamedAttribute> frontendAttributes =\n-        raggedDotOp->getAttrDictionary().getNamed(\"mhlo.frontend_attributes\");\n-    if (frontendAttributes.has_value()) {\n-      std::vector<NamedAttribute> attributes =\n-          mhloOp->getDiscardableAttrDictionary().getValue().vec();\n-      attributes.push_back(frontendAttributes.value());\n-      mhloOp->setDiscardableAttrs(rewriter.getDictionaryAttr(attributes));\n+  };\n+  ArrayAttr precisionConfig = rewriter.getArrayAttr({});\n+  if (raggedDotOp.getPrecisionConfig().has_value()) {\n+    SmallVector<Attribute> vector;\n+    for (auto configValue : raggedDotOp.getPrecisionConfig()\n+                                .value()\n+                                .getAsRange<chlo::PrecisionAttr>()) {\n+      vector.push_back(mhlo::PrecisionAttr::get(\n+          raggedDotOp.getContext(),\n+          mhloPrecision(configValue.getValue()).value()));\n     }\n+    precisionConfig = rewriter.getArrayAttr(vector);\n+  }\n \n-    rewriter.replaceOp(raggedDotOp, mhloOp.getOperation());\n-    return success();\n+  auto mhloOp = mhlo::RaggedDotOp::create(\n+      rewriter, raggedDotOp.getLoc(), raggedDotOp.getResult().getType(),\n+      raggedDotOp.getLhs(), raggedDotOp.getRhs(), raggedDotOp.getGroupSizes(),\n+      raggedDotDimNums, precisionConfig);\n+  std::optional<NamedAttribute> frontendAttributes =\n+      raggedDotOp->getAttrDictionary().getNamed(\"mhlo.frontend_attributes\");\n+  if (frontendAttributes.has_value()) {\n+    std::vector<NamedAttribute> attributes =\n+        mhloOp->getDiscardableAttrDictionary().getValue().vec();\n+    attributes.push_back(frontendAttributes.value());\n+    mhloOp->setDiscardableAttrs(rewriter.getDictionaryAttr(attributes));\n   }\n-};\n+\n+  rewriter.replaceOp(raggedDotOp, mhloOp.getOperation());\n+  return success();\n+}\n+\n+LogicalResult convertAcoshChloToMhlo(chlo::AcoshOp op,\n+                                     PatternRewriter& rewriter) {\n+  if (mhlo::isLegalAcosh(op)) {\n+    return failure();\n+  }\n+  rewriter.replaceOpWithNewOp<mhlo::AcoshOp>(op, op->getOperands());\n+  return success();\n+}\n \n }  // namespace\n \n+ChloLegalizeToHighLevelMhloPassOptions getDefaultChloToHighLevelMhloOptions() {\n+  return ChloLegalizeToHighLevelMhloPassOptions();\n+}\n+\n+ChloLegalizeToHighLevelMhloPassOptions getGpuChloToHighLevelMhloOptions() {\n+  ChloLegalizeToHighLevelMhloPassOptions opts;\n+  opts.enable_acosh_ = true;\n+  return opts;\n+}\n+\n }  // namespace mhlo\n \n namespace chlo {\n@@ -173,17 +210,27 @@ namespace {\n \n }  // namespace\n \n-void populateChloToHighLevelMhloOpPatterns(MLIRContext *,\n-                                           RewritePatternSet *patterns) {\n-  patterns->add<mhlo::RaggedDotChloToMhlo>(patterns->getContext(),\n-                                           /*benefit=*/10);\n+void populateChloToHighLevelMhloOpPatterns(\n+    MLIRContext*, RewritePatternSet* patterns,\n+    const mhlo::ChloLegalizeToHighLevelMhloPassOptions& options) {\n+  constexpr unsigned kBenefit = 10;\n+  if (options.enable_acosh_) {\n+    patterns->add(mhlo::convertAcoshChloToMhlo, kBenefit);\n+  }\n+  patterns->add(mhlo::convertRaggedDotChloToMhlo, kBenefit);\n   populateWithGenerated(*patterns);\n }\n \n-void populateChloToHloPatterns(MLIRContext *context,\n-                               TypeConverter *typeConverter,\n-                               RewritePatternSet *patterns) {\n-  chlo::populateChloToHighLevelMhloOpPatterns(context, patterns);\n+void populateChloToHighLevelMhloOpPatterns(MLIRContext* context,\n+                                           RewritePatternSet* patterns) {\n+  populateChloToHighLevelMhloOpPatterns(\n+      context, patterns, mhlo::ChloLegalizeToHighLevelMhloPassOptions());\n+}\n+\n+void populateChloToHloPatterns(MLIRContext* context,\n+                               TypeConverter* typeConverter,\n+                               RewritePatternSet* patterns) {\n+  populateChloToHighLevelMhloOpPatterns(context, patterns);\n   stablehlo::populateChloToStablehloPatterns(context, patterns);\n   stablehlo::populateStablehloToHloPatterns(patterns, typeConverter, context);\n }"
        },
        {
            "sha": "603a8cbdeeeb4933ffff0f0cd75535a43d6e43de",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/transforms/hlo_legalize_to_arithmetic/hlo_legalize_to_arithmetic.cc",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fhlo_legalize_to_arithmetic%2Fhlo_legalize_to_arithmetic.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fhlo_legalize_to_arithmetic%2Fhlo_legalize_to_arithmetic.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fhlo_legalize_to_arithmetic%2Fhlo_legalize_to_arithmetic.cc?ref=70003f77e0a12f293ad61b7562539421c0c86151",
            "patch": "@@ -194,6 +194,7 @@ void populateScalarHloToArithmeticConversionPatterns(\n   // clang-format off\n   patterns->add<\n       ScalarHloToArithmeticPattern<mhlo::AbsOp>,\n+      ScalarHloToArithmeticPattern<mhlo::AcoshOp>,\n       ScalarHloToArithmeticPattern<mhlo::AddOp>,\n       ScalarHloToArithmeticPattern<mhlo::AndOp>,\n       ScalarHloToArithmeticPattern<mhlo::Atan2Op>,"
        },
        {
            "sha": "b607ba4b8f3a7806bfad7265e0faedbe46ad7ef5",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/transforms/hlo_legalize_to_stablehlo/hlo_legalize_to_stablehlo.cc",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fhlo_legalize_to_stablehlo%2Fhlo_legalize_to_stablehlo.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fhlo_legalize_to_stablehlo%2Fhlo_legalize_to_stablehlo.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fhlo_legalize_to_stablehlo%2Fhlo_legalize_to_stablehlo.cc?ref=70003f77e0a12f293ad61b7562539421c0c86151",
            "patch": "@@ -113,6 +113,11 @@ std::optional<int64_t> getPublicFeaturesNotInStablehlo(HloOpTy hloOp) {\n     // Version 1: Initial version for ErfOp.\n     return 1;\n   }\n+  // StableHLO doesn't support Acosh yet.\n+  if constexpr (std::is_same<HloOpTy, mhlo::AcoshOp>::value) {\n+    // Version 1: Initial version for AcoshOp.\n+    return 1;\n+  }\n   return std::nullopt;\n }\n \n@@ -439,7 +444,8 @@ LogicalResult convertAttributes(ConversionPatternRewriter& rewriter,\n     }\n \n     // Handle DenseElements --> DenseArray for certain StableHLO ops\n-    if constexpr (!std::is_same<HloOpTy, mhlo::ErfOp>::value &&\n+    if constexpr (!std::is_same<HloOpTy, mhlo::AcoshOp>::value &&\n+                  !std::is_same<HloOpTy, mhlo::ErfOp>::value &&\n                   !std::is_same<HloOpTy, mhlo::TopKOp>::value) {\n       if (!stablehloAttr)\n         stablehloAttr = convertDenseArray<HloToStablehloOp<HloOpTy>>(\n@@ -707,7 +713,8 @@ void populateHloToStablehloPatterns(RewritePatternSet* patterns,\n #include \"stablehlo/dialect/StablehloOps.cpp.inc\"\n       >(patterns, converter, context, allowExperimentalFeatures);\n \n-  populateHloToStablehloCustomCallPatterns<mhlo::TopKOp, mhlo::ErfOp>(\n+  populateHloToStablehloCustomCallPatterns<mhlo::AcoshOp, mhlo::ErfOp,\n+                                           mhlo::TopKOp>(\n       patterns, converter, context, allowExperimentalFeatures);\n }\n "
        },
        {
            "sha": "36f4450f5647762946eade74e6f7243a1a3d0b02",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/transforms/mhlo_passes.td",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fmhlo_passes.td",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fmhlo_passes.td",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fmhlo_passes.td?ref=70003f77e0a12f293ad61b7562539421c0c86151",
            "patch": "@@ -22,6 +22,10 @@ def ChloLegalizeToHighLevelMhloPass : Pass<\"chlo-legalize-to-high-level-mhlo\", \"\n     ops with XLA support. These are MHLO ops that directly model the CHLO op,\n     such as TopK and Erf.\n   }];\n+  let options = [\n+    Option<\"enable_acosh_\", \"enable-acosh\", \"bool\", /*default=*/\"false\",\n+           \"Enable chlo.acosh to mhlo.acosh lowering.\">\n+  ];\n   let dependentDialects = [\"mhlo::MhloDialect\"];\n }\n "
        },
        {
            "sha": "60ad87f297524266d860c3211d2a53598f255a6b",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/transforms/passes.h",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fpasses.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fpasses.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Fpasses.h?ref=70003f77e0a12f293ad61b7562539421c0c86151",
            "patch": "@@ -37,6 +37,13 @@ namespace mhlo {\n #define GEN_PASS_DECL\n #include \"mhlo/transforms/mhlo_passes.h.inc\"\n \n+/// Returns the default options for the ChloLegalizeToHighLevelMhloPass. These\n+/// options specify the ops that are supported by all XLA backends.\n+ChloLegalizeToHighLevelMhloPassOptions getDefaultChloToHighLevelMhloOptions();\n+\n+/// Returns options for the ChloLegalizeToHighLevelMhloPass for the GPU backend.\n+ChloLegalizeToHighLevelMhloPassOptions getGpuChloToHighLevelMhloOptions();\n+\n /// Lowers from HLO dialect to Arithmetic dialect.\n std::unique_ptr<OperationPass<ModuleOp>> createLegalizeToArithmeticPass();\n "
        },
        {
            "sha": "bdc733ef413577907d8ce01cb52642427b93858a",
            "filename": "third_party/xla/xla/mlir_hlo/mhlo/transforms/rewriters.h",
            "status": "modified",
            "additions": 11,
            "deletions": 6,
            "changes": 17,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Frewriters.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Frewriters.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fmhlo%2Ftransforms%2Frewriters.h?ref=70003f77e0a12f293ad61b7562539421c0c86151",
            "patch": "@@ -16,7 +16,6 @@ limitations under the License.\n #ifndef MLIR_HLO_MHLO_TRANSFORMS_REWRITERS_H\n #define MLIR_HLO_MHLO_TRANSFORMS_REWRITERS_H\n \n-#include <functional>\n #include <memory>\n \n #include \"mlir/IR/MLIRContext.h\"\n@@ -26,6 +25,8 @@ limitations under the License.\n namespace mlir {\n namespace mhlo {\n \n+struct ChloLegalizeToHighLevelMhloPassOptions;\n+\n // Collection of rewrite patterns for lowering a general dot product.\n void populateGeneralDotOpLoweringPatterns(RewritePatternSet *patterns,\n                                           MLIRContext *ctx);\n@@ -106,20 +107,24 @@ void populateTrigonometricToApproximationPatterns(MLIRContext *context,\n void populateGroupReductionDimensionsPatterns(MLIRContext *context,\n                                               RewritePatternSet *patterns,\n                                               bool preferColumnsReductions);\n+\n }  // namespace mhlo\n \n namespace chlo {\n \n // Populates direct translations between CHLO and MHLO ops for higher level\n // MHLO ops like TopK and Erf.\n-void populateChloToHighLevelMhloOpPatterns(MLIRContext *context,\n-                                           RewritePatternSet *patterns);\n+void populateChloToHighLevelMhloOpPatterns(\n+    MLIRContext* context, RewritePatternSet* patterns,\n+    const mhlo::ChloLegalizeToHighLevelMhloPassOptions& options);\n+void populateChloToHighLevelMhloOpPatterns(MLIRContext* context,\n+                                           RewritePatternSet* patterns);\n \n // Populates direct translations between CHLO->MHLO high level ops\n // and CHLO->StableHLO->MHLO patterns.\n-void populateChloToHloPatterns(MLIRContext *context,\n-                               TypeConverter *typeConverter,\n-                               RewritePatternSet *patterns);\n+void populateChloToHloPatterns(MLIRContext* context,\n+                               TypeConverter* typeConverter,\n+                               RewritePatternSet* patterns);\n \n }  // namespace chlo\n "
        },
        {
            "sha": "b6c1283396b1754f050fbc7782ce682865f7b9f3",
            "filename": "third_party/xla/xla/mlir_hlo/stablehlo_ext/transforms/chlo_preserve_high_level_ops.cpp",
            "status": "modified",
            "additions": 30,
            "deletions": 6,
            "changes": 36,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2Fchlo_preserve_high_level_ops.cpp",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2Fchlo_preserve_high_level_ops.cpp",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2Fchlo_preserve_high_level_ops.cpp?ref=70003f77e0a12f293ad61b7562539421c0c86151",
            "patch": "@@ -230,6 +230,15 @@ struct ErfOpToCustomCallPattern : public OpRewritePattern<chlo::ErfOp> {\n   }\n };\n \n+struct AcoshOpToCustomCallPattern : public OpRewritePattern<chlo::AcoshOp> {\n+  using OpRewritePattern::OpRewritePattern;\n+  LogicalResult matchAndRewrite(chlo::AcoshOp op,\n+                                PatternRewriter& rewriter) const override {\n+    return wrapChloOperationInCustomCall(rewriter, op, \"mhlo.acosh\",\n+                                         /*version=*/1);\n+  }\n+};\n+\n ///////\n // CHLO to CompositeOp Patterns\n ///////\n@@ -273,6 +282,14 @@ struct ErfOpToCompositePattern : public OpRewritePattern<chlo::ErfOp> {\n   }\n };\n \n+struct AcoshOpToCompositePattern : public OpRewritePattern<chlo::AcoshOp> {\n+  using OpRewritePattern::OpRewritePattern;\n+  LogicalResult matchAndRewrite(chlo::AcoshOp op,\n+                                PatternRewriter& rewriter) const override {\n+    return wrapChloOpInComposite(op, /*version=*/1, rewriter);\n+  }\n+};\n+\n }  // namespace\n \n struct ChloPreserveHighLevelOpsPass\n@@ -290,17 +307,24 @@ struct ChloPreserveHighLevelOpsPass\n         .setMaxNumRewrites(GreedyRewriteConfig::kNoLimit)\n         .setStrictness(GreedyRewriteStrictness::ExistingOps);\n \n+    auto* ctx = &getContext();\n     RewritePatternSet patterns(&getContext());\n+    // clang-format off\n     if (useDeprecatedCustomCallEncoding) {\n       // Deprecated CustomCall encoding.\n-      patterns.add<RaggedDotOpToCustomCallPattern>(patterns.getContext());\n-      patterns.add<TopKOpToCustomCallPattern>(&getContext());\n-      patterns.add<ErfOpToCustomCallPattern>(&getContext());\n+      patterns.add<\n+        AcoshOpToCustomCallPattern,\n+        ErfOpToCustomCallPattern,\n+        RaggedDotOpToCustomCallPattern,\n+        TopKOpToCustomCallPattern>(ctx);\n     } else {\n-      patterns.add<RaggedDotOpToCompositePattern>(patterns.getContext());\n-      patterns.add<TopKOpToCompositePattern>(&getContext());\n-      patterns.add<ErfOpToCompositePattern>(&getContext());\n+      patterns.add<\n+        AcoshOpToCompositePattern,\n+        ErfOpToCompositePattern,\n+        RaggedDotOpToCompositePattern,\n+        TopKOpToCompositePattern>(ctx);\n     }\n+    // clang-format on\n \n     // Only apply to CustomCallOps\n     auto moduleOp = getOperation();"
        },
        {
            "sha": "186976efb7eacfff7b289cbfb6e3300f2a7c3c25",
            "filename": "third_party/xla/xla/mlir_hlo/stablehlo_ext/transforms/chlo_recompose_ops.cpp",
            "status": "modified",
            "additions": 51,
            "deletions": 13,
            "changes": 64,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2Fchlo_recompose_ops.cpp",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2Fchlo_recompose_ops.cpp",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Fstablehlo_ext%2Ftransforms%2Fchlo_recompose_ops.cpp?ref=70003f77e0a12f293ad61b7562539421c0c86151",
            "patch": "@@ -208,8 +208,9 @@ LogicalResult recomposeChloOpFromCompositeOp(stablehlo::CompositeOp op,\n   // Convert encoded attributes to CHLO attrs.\n   auto attrs =\n       deserializeChloAttributes(op, op.getName(), op.getCompositeAttributes());\n-  if (failed(attrs))\n+  if (failed(attrs)) {\n     return rewriter.notifyMatchFailure(op, \"failed to deserialize attributes\");\n+  }\n   rewriter.replaceOpWithNewOp<OpType>(op, op->getResultTypes(),\n                                       op->getOperands(), attrs.value());\n   return success();\n@@ -220,11 +221,13 @@ struct RaggedDotOpRecomposePattern\n   using OpRewritePattern::OpRewritePattern;\n   LogicalResult matchAndRewrite(stablehlo::CompositeOp op,\n                                 PatternRewriter& rewriter) const override {\n-    if (op.getName() != \"chlo.ragged_dot\")\n+    if (op.getName() != \"chlo.ragged_dot\") {\n       return rewriter.notifyMatchFailure(op, \"not a chlo.ragged_dot\");\n-    if (op.getVersion() != 1)\n+    }\n+    if (op.getVersion() != 1) {\n       return rewriter.notifyMatchFailure(\n           op, \"unsupported version for chlo.ragged_dot composite\");\n+    }\n     return recomposeChloOpFromCompositeOp<chlo::RaggedDotOp>(op, rewriter);\n   }\n };\n@@ -243,15 +246,33 @@ struct TopKOpRecomposePattern\n   }\n };\n \n+struct AcoshOpRecomposePattern\n+    : public OpRewritePattern<stablehlo::CompositeOp> {\n+  using OpRewritePattern::OpRewritePattern;\n+  LogicalResult matchAndRewrite(stablehlo::CompositeOp op,\n+                                PatternRewriter& rewriter) const override {\n+    if (op.getName() != \"chlo.acosh\") {\n+      return rewriter.notifyMatchFailure(op, \"not a chlo.acosh\");\n+    }\n+    if (op.getVersion() != 1) {\n+      return rewriter.notifyMatchFailure(\n+          op, \"unsupported version for chlo.acosh composite\");\n+    }\n+    return recomposeChloOpFromCompositeOp<chlo::AcoshOp>(op, rewriter);\n+  }\n+};\n+\n struct ErfOpRecomposePattern : public OpRewritePattern<stablehlo::CompositeOp> {\n   using OpRewritePattern::OpRewritePattern;\n   LogicalResult matchAndRewrite(stablehlo::CompositeOp op,\n                                 PatternRewriter& rewriter) const override {\n-    if (op.getName() != \"chlo.erf\")\n+    if (op.getName() != \"chlo.erf\") {\n       return rewriter.notifyMatchFailure(op, \"not a chlo.erf\");\n-    if (op.getVersion() != 1)\n+    }\n+    if (op.getVersion() != 1) {\n       return rewriter.notifyMatchFailure(\n           op, \"unsupported version for chlo.erf composite\");\n+    }\n     return recomposeChloOpFromCompositeOp<chlo::ErfOp>(op, rewriter);\n   }\n };\n@@ -330,6 +351,16 @@ struct ErfOpCustomCallRecomposePattern\n   }\n };\n \n+struct AcoshOpCustomCallRecomposePattern\n+    : public OpRewritePattern<stablehlo::CustomCallOp> {\n+  using OpRewritePattern::OpRewritePattern;\n+  LogicalResult matchAndRewrite(stablehlo::CustomCallOp op,\n+                                PatternRewriter& rewriter) const override {\n+    return recomposeChloOpFromCustomCall<chlo::AcoshOp>(\n+        op, {\"mhlo.acosh\", \"chlo.acosh\"}, rewriter);\n+  }\n+};\n+\n }  // namespace\n \n struct ChloRecomposeOpsPass\n@@ -345,17 +376,24 @@ struct ChloRecomposeOpsPass\n         .setMaxNumRewrites(GreedyRewriteConfig::kNoLimit)\n         .setStrictness(GreedyRewriteStrictness::ExistingOps);\n \n-    RewritePatternSet patterns(&getContext());\n+    auto* ctx = &getContext();\n+    RewritePatternSet patterns(ctx);\n+    // clang-format off\n     // CustomCall Patterns\n-    patterns.add<ErfOpCustomCallRecomposePattern>(&getContext());\n-    patterns.add<RaggedDotOpCustomCallRecomposePattern>(&getContext());\n-    patterns.add<TanOpCustomCallRecomposePattern>(&getContext());\n-    patterns.add<TopKOpCustomCallRecomposePattern>(&getContext());\n+    patterns.add<\n+      AcoshOpCustomCallRecomposePattern,\n+      ErfOpCustomCallRecomposePattern,\n+      RaggedDotOpCustomCallRecomposePattern,\n+      TanOpCustomCallRecomposePattern,\n+      TopKOpCustomCallRecomposePattern>(ctx);\n \n     // Composite Patterns\n-    patterns.add<ErfOpRecomposePattern>(&getContext());\n-    patterns.add<RaggedDotOpRecomposePattern>(&getContext());\n-    patterns.add<TopKOpRecomposePattern>(&getContext());\n+    patterns.add<\n+      AcoshOpRecomposePattern,\n+      ErfOpRecomposePattern,\n+      RaggedDotOpRecomposePattern,\n+      TopKOpRecomposePattern>(ctx);\n+    // clang-format on\n \n     // Only apply to CustomCallOps\n     auto moduleOp = getOperation();"
        },
        {
            "sha": "6d68d03f5d28ac094e7fe2e50a9563b3ef3eee8e",
            "filename": "third_party/xla/xla/mlir_hlo/tests/Dialect/chlo/chlo_legalize_to_mhlo.mlir",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2FDialect%2Fchlo%2Fchlo_legalize_to_mhlo.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2FDialect%2Fchlo%2Fchlo_legalize_to_mhlo.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2FDialect%2Fchlo%2Fchlo_legalize_to_mhlo.mlir?ref=70003f77e0a12f293ad61b7562539421c0c86151",
            "patch": "@@ -1,5 +1,5 @@\n // RUN: mlir-hlo-opt --chlo-legalize-to-hlo --split-input-file -verify-diagnostics %s | FileCheck %s --dump-input-context=20\n-// RUN: mlir-hlo-opt --chlo-legalize-to-high-level-mhlo --split-input-file -verify-diagnostics %s | FileCheck %s --check-prefix=CHECK-HIGH-LEVEL\n+// RUN: mlir-hlo-opt --chlo-legalize-to-high-level-mhlo=\"enable-acosh\" --split-input-file -verify-diagnostics %s | FileCheck %s --check-prefix=CHECK-HIGH-LEVEL\n \n // CHECK-LABEL: func.func @asin_bf16(\n // CHECK-SAME:    %[[TMP_arg0:.*]]: tensor<bf16>\n@@ -613,6 +613,7 @@ func.func @acosh_complex_f32(%arg : tensor<complex<f32>>) -> tensor<complex<f32>\n   %result = \"chlo.acosh\"(%arg) : (tensor<complex<f32>>) -> tensor<complex<f32>>\n   func.return %result : tensor<complex<f32>>\n }\n+// CHECK-HIGH-LEVEL-NOT: mhlo.acosh\n \n // -----\n \n@@ -728,6 +729,7 @@ func.func @acosh(%arg: tensor<f16>) -> tensor<f16> {\n   %1 = \"chlo.acosh\"(%arg) : (tensor<f16>) -> tensor<f16>\n   func.return %1 : tensor<f16>\n }\n+// CHECK-HIGH-LEVEL: mhlo.acosh\n \n // -----\n \n@@ -878,6 +880,7 @@ func.func @acosh_complex_f32(%arg : tensor<complex<f32>>) -> tensor<complex<f32>\n   %result = \"chlo.acosh\"(%arg) : (tensor<complex<f32>>) -> tensor<complex<f32>>\n   func.return %result : tensor<complex<f32>>\n }\n+// CHECK-HIGH-LEVEL-NOT: mhlo.acosh\n \n // -----\n "
        },
        {
            "sha": "deba0175f9b7dec4ceca4668ff8979ed7ec7951a",
            "filename": "third_party/xla/xla/mlir_hlo/tests/stablehlo_ext/chlo_preserve_high_level_ops.mlir",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2Fstablehlo_ext%2Fchlo_preserve_high_level_ops.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2Fstablehlo_ext%2Fchlo_preserve_high_level_ops.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2Fstablehlo_ext%2Fchlo_preserve_high_level_ops.mlir?ref=70003f77e0a12f293ad61b7562539421c0c86151",
            "patch": "@@ -73,6 +73,16 @@ func.func @erf_preserve(%arg0: tensor<3x20x20xbf16>) -> tensor<?x20x20xbf16> {\n \n // -----\n \n+// CHECK-LABEL: func @acosh_preserve\n+func.func @acosh_preserve(%arg0: tensor<3x20x20xbf16>) -> tensor<?x20x20xbf16> {\n+  // CHECK-CC: stablehlo.custom_call @mhlo.acosh(%arg0) {mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<3x20x20xbf16>) -> tensor<?x20x20xbf16>\n+  // CHECK: stablehlo.composite \"chlo.acosh\" %arg0 {decomposition = @chlo.acosh.impl, version = 1 : i32}\n+  %0 = chlo.acosh %arg0 : tensor<3x20x20xbf16> -> tensor<?x20x20xbf16>\n+  return %0 : tensor<?x20x20xbf16>\n+}\n+\n+// -----\n+\n // CHECK-LABEL: func @tan_no_preserve\n func.func @tan_no_preserve(%arg0: tensor<16xf32>) -> tensor<?xf32> {\n   // CHECK: chlo.tan"
        },
        {
            "sha": "a460f3c547eaaed14b24005e49c50e36a3d6c6ea",
            "filename": "third_party/xla/xla/mlir_hlo/tests/stablehlo_ext/chlo_recompose_ops.mlir",
            "status": "modified",
            "additions": 30,
            "deletions": 0,
            "changes": 30,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2Fstablehlo_ext%2Fchlo_recompose_ops.mlir",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2Fstablehlo_ext%2Fchlo_recompose_ops.mlir",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fmlir_hlo%2Ftests%2Fstablehlo_ext%2Fchlo_recompose_ops.mlir?ref=70003f77e0a12f293ad61b7562539421c0c86151",
            "patch": "@@ -18,6 +18,22 @@ func.func private @chlo.erf.impl(%arg0: tensor<3x20x20xbf16>) -> tensor<?x20x20x\n \n // -----\n \n+// CHECK-LABEL: func @acosh_recompose_composite\n+func.func @acosh_recompose_composite(%arg0: tensor<3x20x20xbf16>) -> tensor<?x20x20xbf16> {\n+  // CHECK-NEXT: chlo.acosh\n+  // CHECK-NOT: stablehlo.composite\n+  %0 = stablehlo.composite \"chlo.acosh\" %arg0 {decomposition = @chlo.acosh.impl, version = 1 : i32} : (tensor<3x20x20xbf16>) -> tensor<?x20x20xbf16>\n+  return %0 : tensor<?x20x20xbf16>\n+}\n+// CHECK-NOT: @chlo.acosh.imp\n+func.func private @chlo.acosh.impl(%arg0: tensor<3x20x20xbf16>) -> tensor<?x20x20xbf16> {\n+  %0 = chlo.acosh %arg0 : tensor<3x20x20xbf16> -> tensor<?x20x20xbf16>\n+  return %0 : tensor<?x20x20xbf16>\n+}\n+\n+// -----\n+\n+\n // CHECK-LABEL: func @ragged_dot_recompose_composite\n func.func @ragged_dot_recompose_composite(%arg0: tensor<2x11x5xf32>, %arg1: tensor<3x2x5x7xf32>, %arg2: tensor<3xi64>) -> tensor<2x11x7xf32> {\n   // CHECK: \"chlo.ragged_dot\"(%arg0, %arg1, %arg2) <{precision_config = [#chlo<precision DEFAULT>, #chlo<precision DEFAULT>], ragged_dot_dimension_numbers = #chlo.ragged_dot<lhs_batching_dimensions = [0], rhs_batching_dimensions = [1], lhs_contracting_dimensions = [2], rhs_contracting_dimensions = [2], lhs_ragged_dimensions = [1], rhs_group_dimensions = [0]>}> : (tensor<2x11x5xf32>, tensor<3x2x5x7xf32>, tensor<3xi64>) -> tensor<2x11x7xf32>\n@@ -65,6 +81,20 @@ func.func @erf_recompose_cc(%arg0: tensor<3x20x20xbf16>) -> tensor<?x20x20xbf16>\n \n // -----\n \n+// CHECK-LABEL: @acosh_recompose_cc\n+func.func @acosh_recompose_cc(%arg0: tensor<3x20x20xbf16>) -> tensor<?x20x20xbf16> {\n+  // CHECK: %0 = chlo.acosh %arg0 : tensor<3x20x20xbf16> -> tensor<?x20x20xbf16>\n+  %0 = \"stablehlo.custom_call\"(%arg0) {\n+    backend_config = \"\",\n+    call_target_name = \"mhlo.acosh\",\n+    mhlo.attributes = {},\n+    mhlo.version = 1 : i64\n+  } : (tensor<3x20x20xbf16>) -> tensor<?x20x20xbf16>\n+  func.return %0 : tensor<?x20x20xbf16>\n+}\n+\n+// -----\n+\n // CHECK-LABEL: func @ragged_dot_recompose_cc\n func.func @ragged_dot_recompose_cc(%arg0: tensor<2x11x5xf32>, %arg1: tensor<3x2x5x7xf32>, %arg2: tensor<3xi64>) -> tensor<2x11x7xf32> {\n   // CHECK: \"chlo.ragged_dot\"(%arg0, %arg1, %arg2) <{precision_config = [#chlo<precision DEFAULT>, #chlo<precision DEFAULT>], ragged_dot_dimension_numbers = #chlo.ragged_dot<lhs_batching_dimensions = [0], rhs_batching_dimensions = [1], lhs_contracting_dimensions = [2], rhs_contracting_dimensions = [2], lhs_ragged_dimensions = [1], rhs_group_dimensions = [0]>}> : (tensor<2x11x5xf32>, tensor<3x2x5x7xf32>, tensor<3xi64>) -> tensor<2x11x7xf32>"
        },
        {
            "sha": "69d3d4a02bd6795e2fcb0c033fe748e73616638f",
            "filename": "third_party/xla/xla/pjrt/gpu/se_gpu_pjrt_compiler.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_compiler.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_compiler.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Fse_gpu_pjrt_compiler.cc?ref=70003f77e0a12f293ad61b7562539421c0c86151",
            "patch": "@@ -212,7 +212,8 @@ StreamExecutorGpuCompiler::Compile(CompileOptions options,\n       module, xla_computation,\n       /*use_tuple_args=*/options.parameter_is_tupled_arguments,\n       /*return_tuple=*/false,\n-      /*exec_build_options=*/&input_options.executable_build_options));\n+      /*exec_build_options=*/&input_options.executable_build_options,\n+      mlir::mhlo::getGpuChloToHighLevelMhloOptions()));\n   return Compile(std::move(input_options), xla_computation, topology, client);\n }\n }  // namespace xla"
        },
        {
            "sha": "b526ff052d8045ed45a0cb28fd0a946d5ec23f84",
            "filename": "third_party/xla/xla/pjrt/gpu/tfrt/tfrt_gpu_client.cc",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fgpu%2Ftfrt%2Ftfrt_gpu_client.cc?ref=70003f77e0a12f293ad61b7562539421c0c86151",
            "patch": "@@ -343,7 +343,8 @@ absl::StatusOr<std::unique_ptr<PjRtExecutable>> TfrtGpuClient::Compile(\n   TF_RETURN_IF_ERROR(MlirToXlaComputation(\n       module, xla_computation,\n       /*use_tuple_args=*/options.parameter_is_tupled_arguments,\n-      /*return_tuple=*/false, &exec_build_options));\n+      /*return_tuple=*/false, &exec_build_options,\n+      mlir::mhlo::getGpuChloToHighLevelMhloOptions()));\n \n   // If the compile options specify argument layout, then let's\n   // fall back to using the options to determine layouts."
        },
        {
            "sha": "840c27ef48a252883e8dc999771394c8d1bc6237",
            "filename": "third_party/xla/xla/pjrt/mlir_to_hlo.cc",
            "status": "modified",
            "additions": 8,
            "deletions": 5,
            "changes": 13,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fpjrt%2Fmlir_to_hlo.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fpjrt%2Fmlir_to_hlo.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fmlir_to_hlo.cc?ref=70003f77e0a12f293ad61b7562539421c0c86151",
            "patch": "@@ -78,6 +78,9 @@ limitations under the License.\n #include \"xla/util.h\"\n \n namespace xla {\n+namespace {\n+using mlir::mhlo::ChloLegalizeToHighLevelMhloPassOptions;\n+}\n \n void RegisterAllHloDialects(mlir::DialectRegistry& registry) {\n   registry.insert<mlir::arith::ArithDialect>();\n@@ -90,10 +93,10 @@ void RegisterAllHloDialects(mlir::DialectRegistry& registry) {\n   mlir::stablehlo::registerAllDialects(registry);\n }\n \n-absl::Status MlirToXlaComputation(mlir::ModuleOp module,\n-                                  XlaComputation& xla_computation,\n-                                  bool use_tuple_args, bool return_tuple,\n-                                  ExecutableBuildOptions* exec_build_options) {\n+absl::Status MlirToXlaComputation(\n+    mlir::ModuleOp module, XlaComputation& xla_computation, bool use_tuple_args,\n+    bool return_tuple, ExecutableBuildOptions* exec_build_options,\n+    const ChloLegalizeToHighLevelMhloPassOptions& chlo_opts) {\n   mlir::MLIRContext* context = module->getContext();\n   mlir::BaseScopedDiagnosticHandler diagnostic_handler(context);\n   {\n@@ -123,7 +126,7 @@ absl::Status MlirToXlaComputation(mlir::ModuleOp module,\n         mlir::stablehlo_ext::createChloRecomposeOpsPass());\n     pm.addPass(mlir::createSymbolDCEPass());\n     pm.addNestedPass<mlir::func::FuncOp>(\n-        mlir::mhlo::createChloLegalizeToHighLevelMhloPass());\n+        mlir::mhlo::createChloLegalizeToHighLevelMhloPass(chlo_opts));\n     pm.addNestedPass<mlir::func::FuncOp>(\n         mlir::stablehlo::createChloLegalizeToStablehloPass());\n "
        },
        {
            "sha": "f21c0c213576647e777f3dd53ffc2dac59aa2d73",
            "filename": "third_party/xla/xla/pjrt/mlir_to_hlo.h",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fpjrt%2Fmlir_to_hlo.h",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fpjrt%2Fmlir_to_hlo.h",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fmlir_to_hlo.h?ref=70003f77e0a12f293ad61b7562539421c0c86151",
            "patch": "@@ -30,6 +30,7 @@ limitations under the License.\n #include \"mlir/Support/LLVM.h\"\n #include \"xla/client/executable_build_options.h\"\n #include \"xla/hlo/builder/xla_computation.h\"\n+#include \"xla/mlir_hlo/mhlo/transforms/passes.h\"\n \n namespace xla {\n \n@@ -43,10 +44,11 @@ absl::StatusOr<mlir::OwningOpRef<mlir::ModuleOp>> ParseMlirModuleString(\n // Converts an CHLO/MHLO module to XLA HLO.\n // TODO(b/345414638): Delete `use_shardy` when we move Shardy as the first pass\n // in the XLA pipeline.\n-absl::Status MlirToXlaComputation(mlir::ModuleOp module,\n-                                  XlaComputation& xla_computation,\n-                                  bool use_tuple_args, bool return_tuple,\n-                                  ExecutableBuildOptions* exec_build_options);\n+absl::Status MlirToXlaComputation(\n+    mlir::ModuleOp module, XlaComputation& xla_computation, bool use_tuple_args,\n+    bool return_tuple, ExecutableBuildOptions* exec_build_options,\n+    const mlir::mhlo::ChloLegalizeToHighLevelMhloPassOptions& chlo_opts =\n+        mlir::mhlo::getDefaultChloToHighLevelMhloOptions());\n \n // Converts an MHLO/CHLO module string to an XLA computation.\n absl::Status ParseMlirModuleStringAndConvertToXlaComputation("
        },
        {
            "sha": "78088bac8f0beeec95505239be7297ffccd0e497",
            "filename": "third_party/xla/xla/pjrt/pjrt_stream_executor_client.cc",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.cc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.cc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fpjrt%2Fpjrt_stream_executor_client.cc?ref=70003f77e0a12f293ad61b7562539421c0c86151",
            "patch": "@@ -3887,10 +3887,13 @@ PjRtStreamExecutorClient::Compile(mlir::ModuleOp module, CompileOptions options,\n \n   XlaComputation xla_computation;\n   ExecutableBuildOptions& exec_build_options = options.executable_build_options;\n+  auto chlo_opts = gpu_run_options_ == nullptr\n+                       ? mlir::mhlo::getDefaultChloToHighLevelMhloOptions()\n+                       : mlir::mhlo::getGpuChloToHighLevelMhloOptions();\n   TF_RETURN_IF_ERROR(MlirToXlaComputation(\n       module, xla_computation,\n       /*use_tuple_args=*/options.parameter_is_tupled_arguments,\n-      /*return_tuple=*/false, &exec_build_options));\n+      /*return_tuple=*/false, &exec_build_options, chlo_opts));\n \n   // If the compile options specify argument layout, then let's\n   // fall back to using the options to determine layouts."
        },
        {
            "sha": "ab5e3a921661fc9a524a670d111488094d1e4616",
            "filename": "third_party/xla/xla/service/gpu/BUILD",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Fservice%2Fgpu%2FBUILD?ref=70003f77e0a12f293ad61b7562539421c0c86151",
            "patch": "@@ -2588,6 +2588,7 @@ cc_library(\n         \"//xla:shape_util\",\n         \"//xla:side_effect_util\",\n         \"//xla:util\",\n+        \"//xla/codegen:ir_emission_utils\",\n         \"//xla/hlo/analysis:hlo_dataflow_analysis\",\n         \"//xla/hlo/ir:hlo\",\n         \"//xla/hlo/ir:hlo_instruction_utils\","
        },
        {
            "sha": "fbcf1c1195a37428aa08068fa0756ab69fa9c518",
            "filename": "third_party/xla/xla/tests/exhaustive/exhaustive_unary_test_ops.inc",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/tensorflow/tensorflow/blob/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Ftests%2Fexhaustive%2Fexhaustive_unary_test_ops.inc",
            "raw_url": "https://github.com/tensorflow/tensorflow/raw/70003f77e0a12f293ad61b7562539421c0c86151/third_party%2Fxla%2Fxla%2Ftests%2Fexhaustive%2Fexhaustive_unary_test_ops.inc",
            "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/third_party%2Fxla%2Fxla%2Ftests%2Fexhaustive%2Fexhaustive_unary_test_ops.inc?ref=70003f77e0a12f293ad61b7562539421c0c86151",
            "patch": "@@ -197,7 +197,9 @@ DEFINE_UNARY_TEST_OP(\n     SqrtOp, { return [](XlaOp x) { return Sqrt(x); }; }, { return std::sqrt; });\n DEFINE_UNARY_TEST_OP(\n     CbrtOp, { return [](XlaOp x) { return Cbrt(x); }; }, { return std::cbrt; });\n-DEFINE_UNARY_TEST_OP(AcoshOp, { return Acosh; }, { return std::acosh; });\n+DEFINE_UNARY_TEST_OP(\n+    AcoshOp, { return [](XlaOp x) { return Acosh(x); }; },\n+    { return std::acosh; });\n DEFINE_UNARY_TEST_OP(AsinhOp, { return Asinh; }, { return std::asinh; });\n DEFINE_UNARY_TEST_OP(AtanhOp, { return Atanh; }, { return std::atanh; });\n DEFINE_UNARY_TEST_OP(AcosOp, { return Acos; }, { return std::acos; });"
        }
    ],
    "stats": {
        "total": 456,
        "additions": 342,
        "deletions": 114
    }
}